<?xml version='1.0' encoding='UTF-8'?>
<collection id="L10">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (<fixed-case>LREC</fixed-case>'10)</booktitle>
      <editor><first>Nicoletta</first><last>Calzolari</last></editor>
      <editor><first>Khalid</first><last>Choukri</last></editor>
      <editor><first>Bente</first><last>Maegaard</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Jan</first><last>Odijk</last></editor>
      <editor><first>Stelios</first><last>Piperidis</last></editor>
      <editor><first>Mike</first><last>Rosner</last></editor>
      <editor><first>Daniel</first><last>Tapias</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Valletta, Malta</address>
      <month>May</month>
      <year>2010</year>
      <venue>lrec</venue>
    </meta>
    <frontmatter>
      <bibkey>lrec-2010-international</bibkey>
    </frontmatter>
    <paper id="1">
      <author><first>Hercules</first><last>Dalianis</last></author>
      <author><first>Hao-chun</first><last>Xing</last></author>
      <author><first>Xin</first><last>Zhang</last></author>
      <title>Creating a Reusable <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>hinese Parallel Corpus for Bilingual Dictionary Construction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/13_Paper.pdf</url>
      <abstract>This paper first describes an experiment to construct an English-Chinese parallel corpus, then applying the Uplug word alignment tool on the corpus and finally produce and evaluate an English-Chinese word list. The Stockholm English-Chinese Parallel Corpus (SEC) was created by downloading English-Chinese parallel corpora from a Chinese web site containing law texts that have been manually translated from Chinese to English. The parallel corpus contains 104 563 Chinese characters equivalent to 59 918 Chinese words, and the corresponding English corpus contains 75 766 English words. However Chinese writing does not utilize any delimiters to mark word boundaries so we had to carry out word segmentation as a preprocessing step on the Chinese corpus. Moreover since the parallel corpus is downloaded from Internet the corpus is noisy regarding to alignment between corresponding translated sentences. Therefore we used 60 hours of manually work to align the sentences in the English and Chinese parallel corpus before performing automatic word alignment using Uplug. The word alignment with Uplug was carried out from English to Chinese. Nine respondents evaluated the resulting English-Chinese word list with frequency equal to or above three and we obtained an accuracy of 73.1 percent.</abstract>
      <bibkey>dalianis-etal-2010-creating</bibkey>
    </paper>
    <paper id="2">
      <author><first>Lluís</first><last>Padró</last></author>
      <author><first>Miquel</first><last>Collado</last></author>
      <author><first>Samuel</first><last>Reese</last></author>
      <author><first>Marina</first><last>Lloberes</last></author>
      <author><first>Irene</first><last>Castellón</last></author>
      <title><fixed-case>F</fixed-case>ree<fixed-case>L</fixed-case>ing 2.1: Five Years of Open-source Language Processing Tools</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/14_Paper.pdf</url>
      <abstract>FreeLing is an open-source multilingual language processing library providing a wide range of language analyzers for several languages. It offers text processing and language annotation facilities to natural language processing application developers, simplifying the task of building those applications. FreeLing is customizable and extensible. Developers can use the default linguistic resources (dictionaries, lexicons, grammars, etc.) directly, or extend them, adapt them to specific domains, or even develop new ones for specific languages. This paper overviews the recent history of this tool, summarizes the improvements and extensions incorporated in the latest version, and depicts the architecture of the library. Special focus is brought to the fact and consequences of the library being open-source: After five years and over 35,000 downloads, a growing user community has extended the initial threelanguages (English, Spanish and Catalan) to eight (adding Galician, Italian, Welsh, Portuguese, and Asturian), proving that the collaborative open model is a productive approach for the development of NLP tools and resources.</abstract>
      <bibkey>padro-etal-2010-freeling</bibkey>
    </paper>
    <paper id="3">
      <author><first>Amit</first><last>Kirschenbaum</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <title>A General Method for Creating a Bilingual Transliteration Dictionary</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/16_Paper.pdf</url>
      <abstract>Transliteration is the rendering in one language of terms from another language (and, possibly, another writing system), approximating spelling and/or phonetic equivalents between the two languages. A transliteration dictionary is a crucial resource for a variety of natural language applications, most notably machine translation. We describe a general method for creating bilingual transliteration dictionaries from Wikipedia article titles. The method can be applied to any language pair with Wikipedia presence, independently of the writing systems involved, and requires only a single simple resource that can be provided by any literate bilingual speaker. It was successfully applied to extract a Hebrew-English transliteration dictionary which, when incorporated in a machine translation system, indeed improved its performance.</abstract>
      <bibkey>kirschenbaum-wintner-2010-general</bibkey>
    </paper>
    <paper id="4">
      <author><first>Huan-An</first><last>Kao</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <title>Comment Extraction from Blog Posts and Its Applications to Opinion Mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/17_Paper.pdf</url>
      <abstract>Blog posts containing many personal experiences or perspectives toward specific subjects are useful. Blogs allow readers to interact with bloggers by placing comments on specific blog posts. The comments carry viewpoints of readers toward the targets described in the post, or supportive/non-supportive attitude toward the post. Comment extraction is challenging due to that there does not exist a unique template among all blog service providers. This paper proposes methods to deal with this problem. Firstly, the repetitive patterns and their corresponding blocks are extracted from input posts by pattern identification algorithm. Secondly, three filtering strategies, i.e., tag pattern loop filtering, rule overlap filtering, and longest rule first, are used to remove non-comment blocks. Finally, a comment/non-comment classifier is learned to distinguish comment blocks from non-comment blocks with 14 block-level features and 5 rule-level features. In the experiments, we randomly select 600 blog posts from 12 blog service providers. F-measure, recall, and precision are 0.801, 0.855, and 0.780, respectively, by using all of the three filtering strategies together with some selected features. The application of comment extraction to blog mining is also illustrated. We show how to identify the relevant opinionated objects ― say, opinion holders, opinions, and targets, from posts.</abstract>
      <bibkey>kao-chen-2010-comment</bibkey>
    </paper>
    <paper id="5">
      <author><first>Thomas</first><last>Schmidt</last></author>
      <author><first>Wilfried</first><last>Schütte</last></author>
      <title><fixed-case>FOLKER</fixed-case>: An Annotation Tool for Efficient Transcription of Natural, Multi-party Interaction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/18_Paper.pdf</url>
      <abstract>This paper presents FOLKER, an annotation tool developed for the efficient transcription of natural, multi-party interaction in a conversation analysis framework. FOLKER is being developed at the Institute for German Language in and for the FOLK project, whose aim is the construction of a large corpus of spoken present-day German, to be used for research and teaching purposes. FOLKER builds on the experience gained with multi-purpose annotation tools like ELAN and EXMARaLDA, but attempts to improve transcription efficiency by restricting and optimizing both data model and tool functionality to a single, well-defined purpose. The tools most important features in this respect are the possibility to freely switch between several editable views according to the requirements of different steps in the annotation process, and an automatic syntax check of annotations during input for their conformance to the GAT transcription convention. This paper starts with a description of the GAT transcription conventions and the data model underlying the tool. It then gives an overview of the tool functionality and compares this functionality to that of other widely used tools.</abstract>
      <bibkey>schmidt-schutte-2010-folker</bibkey>
    </paper>
    <paper id="6">
      <author><first>Roberto</first><last>Navigli</last></author>
      <author><first>Paola</first><last>Velardi</last></author>
      <author><first>Juana Maria</first><last>Ruiz-Martínez</last></author>
      <title>An Annotated Dataset for Extracting Definitions and Hypernyms from the Web</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/20_Paper.pdf</url>
      <abstract>This paper presents and analyzes an annotated corpus of definitions, created to train an algorithm for the automatic extraction of definitions and hypernyms from web documents. As an additional resource, we also include a corpus of non-definitions with syntactic patterns similar to those of definition sentences, e.g.: ""An android is a robot"" vs. ""Snowcap is unmistakable"". Domain and style independence is obtained thanks to the annotation of a large and domain-balanced corpus and to a novel pattern generalization algorithm based on word-class lattices (WCL). A lattice is a directed acyclic graph (DAG), a subclass of nondeterministic finite state automata (NFA). The lattice structure has the purpose of preserving the salient differences among distinct sequences, while eliminating redundant information. The WCL algorithm will be integrated into an improved version of the GlossExtractor Web application (Velardi et al., 2008). This paper is mostly concerned with a description of the corpus, the annotation strategy, and a linguistic analysis of the data. A summary of the WCL algorithm is also provided for the sake of completeness.</abstract>
      <bibkey>navigli-etal-2010-annotated</bibkey>
    </paper>
    <paper id="7">
      <author><first>Maria</first><last>Khokhlova</last></author>
      <author><first>Victor</first><last>Zakharov</last></author>
      <title>Studying Word Sketches for <fixed-case>R</fixed-case>ussian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/21_Paper.pdf</url>
      <abstract>Without any doubt corpora are vital tools for linguistic studies and solution for applied tasks. Although corpora opportunities are very useful, there is a need of another kind of software for further improvement of linguistic research as it is impossible to process huge amount of linguistic data manually. The Sketch Engine representing itself a corpus tool which takes as input a corpus of any language and corresponding grammar patterns. The paper describes the writing of Sketch grammar for the Russian language as a part of the Sketch Engine system. The system gives information about a words collocability on concrete dependency models, and generates lists of the most frequent phrases for a given word based on appropriate models. The paper deals with two different approaches to writing rules for the grammar, based on morphological information, and also with applying word sketches to the Russian language. The data evidences that such results may find an extensive use in various fields of linguistics, such as dictionary compiling, language learning and teaching, translation (including machine translation), phraseology, information retrieval etc.</abstract>
      <bibkey>khokhlova-zakharov-2010-studying</bibkey>
    </paper>
    <paper id="8">
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <title>Using Linear Interpolation and Weighted Reordering Hypotheses in the <fixed-case>M</fixed-case>oses System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/23_Paper.pdf</url>
      <abstract>This paper proposes to introduce a novel reordering model in the open-source Moses toolkit. The main idea is to provide weighted reordering hypotheses to the SMT decoder. These hypotheses are built using a first-step Ngram-based SMT translation from a source language into a third representation that is called reordered source language. Each hypothesis has its own weight provided by the Ngram-based decoder. This proposed reordering technique offers a better and more efficient translation when compared to both the distance-based and the lexicalized reordering. In addition to this reordering approach, this paper describes a domain adaptation technique which is based on a linear combination of an specific in-domain and an extra out-domain translation models. Results for both approaches are reported in the Arabic-to-English 2008 IWSLT task. When implementing the weighted reordering hypotheses and the domain adaptation technique in the final translation system, translation results reach improvements up to 2.5 BLEU compared to a standard state-of-the-art Moses baseline system.</abstract>
      <bibkey>costa-jussa-fonollosa-2010-using</bibkey>
    </paper>
    <paper id="9">
      <author><first>Onno</first><last>Crasborn</last></author>
      <title>The Sign Linguistics Corpora Network: Towards Standards for Signed Language Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/25_Paper.pdf</url>
      <abstract>The Sign Linguistics Corpora Network is a three-year network initiative that aims to collect existing knowledge and practices on the creation and use of signed language resources. The concrete goals are to organise a series of four workshops in 2009 and 2010, create a stable Internet location for such knowledge, and generate new ideas for employing the most recent technologies for the study of signed languages. The network covers a wide range of subjects: data collection, metadata, annotation, and exploitation; these are the topics of the four workshops. The outcomes of the first two workshops are summarised in this paper; both workshops demonstrated that the need for dedicated knowledge on sign language corpora is especially salient in countries where researchers work alone or in small groups, which is still quite common in many places in Europe. While the original goal of the network was primarily to focus on corpus linguistics and language documentation, human language technology has gradually been incorporated as a user group of signed language resources.</abstract>
      <bibkey>crasborn-2010-sign</bibkey>
    </paper>
    <paper id="10">
      <author><first>Antoinette</first><last>Hawayek</last></author>
      <author><first>Riccardo</first><last>Del Gratta</last></author>
      <author><first>Giuseppe</first><last>Cappelli</last></author>
      <title>A Bilingual Dictionary <fixed-case>M</fixed-case>exican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage-<fixed-case>S</fixed-case>panish/<fixed-case>S</fixed-case>panish-<fixed-case>M</fixed-case>exican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/27_Paper.pdf</url>
      <abstract>We present a three-part bilingual specialized dictionary Mexican Sign Language-Spanish / Spanish-Mexican Sign Language. This dictionary will be the outcome of a three-years agreement between the Italian Consiglio Nazionale delle Ricerche and the Mexican Conacyt. Although many other sign language dictionaries have been provided to deaf communities, there are no Mexican Sign Language dictionaries in Mexico, yet. We want to stress on the specialized feature of the proposed dictionary: the bilingual dictionary will contain frequently used general Spanish forms along with scholastic course specific specialized words whose meanings warrant comprehension of school curricula. We emphasize that this aspect of the bilingual dictionary can have a deep social impact, since we will furnish to deaf people the possibility to get competence in official language, which is necessary to ensure access to school curriculum and to become full-fledged citizens. From a technical point of view, the dictionary consists of a relational database, where we have saved the sign parameters and a graphical user interface especially designed to allow deaf children to retrieve signs using the relevant parameters and,thus, the meaning of the sign in Spanish.</abstract>
      <bibkey>hawayek-etal-2010-bilingual</bibkey>
    </paper>
    <paper id="11">
      <author><first>Serge</first><last>Sharoff</last></author>
      <author><first>Zhili</first><last>Wu</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <title>The Web Library of Babel: evaluating genre collections</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/28_Paper.pdf</url>
      <abstract>We present experiments in automatic genre classification on web corpora, comparing a wide variety of features on several different genreannotated datasets (HGC, I-EN, KI-04, KRYS-I, MGC and SANTINIS).We investigate the performance of several types of features (POS n-grams, character n-grams and word n-grams) and show that simple character n-grams perform best on current collections because of their ability to generalise both lexical and syntactic phenomena related to genres. However, we also show that these impressive results might not be transferrable to the wider web due to the lack of comparability between different annotation labels (many webpages cannot be described in terms of the genre labels in individual collections), lack of representativeness of existing collections (many genres are represented by webpages coming from a small number of sources) as well as problems in the reliability of genre annotation (many pages from the web are difficult to interpret in terms of the labels available). This suggests that more research is needed to understand genres on the Web.</abstract>
      <bibkey>sharoff-etal-2010-web</bibkey>
    </paper>
    <paper id="12">
      <author><first>Hans-Ulrich</first><last>Krieger</last></author>
      <title>A General Methodology for Equipping Ontologies with Time</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/29_Paper.pdf</url>
      <abstract>In the first part of this paper, we present a framework for enriching arbitrary upper or domain-specific ontologies with a concept of time. To do so, we need the notion of a time slice. Contrary to other approaches, we directly interpret the original entities as time slices in order to (i) avoid a duplication of the original ontology and (ii) to prevent a knowledge engineer from ontology rewriting. The diachronic representation of time is complemented by a sophisticated time ontology that supports underspecification and an arbitrarily fine granularity of time. As a showcase, we describe how the time ontology has been interfaced with the PROTON upper ontology. The second part investigates a temporal extension of RDF that replaces the usual triple notation by a more general tuple representation. In this setting, Hayes/ter Horst-like entailment rules are replaced by their temporal counterparts. Our motivation to move towards this direction is twofold: firstly, extending binary relation instances with time leads to a massive proliferation of useless objects (independently of the encoding); secondly, reasoning and querying with such extended relations is extremely complex, expensive, and error-prone.</abstract>
      <bibkey>krieger-2010-general</bibkey>
    </paper>
    <paper id="13">
      <author><first>Ting</first><last>Qian</last></author>
      <author><first>Kristy</first><last>Hollingshead</last></author>
      <author><first>Su-youn</first><last>Yoon</last></author>
      <author><first>Kyoung-young</first><last>Kim</last></author>
      <author><first>Richard</first><last>Sproat</last></author>
      <title>A Python Toolkit for Universal Transliteration</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/30_Paper.pdf</url>
      <abstract>We describe ScriptTranscriber, an open source toolkit for extracting transliterations in comparable corpora from languages written in different scripts. The system includes various methods for extracting potential terms of interest from raw text, for providing guesses on the pronunciations of terms, and for comparing two strings as possible transliterations using both phonetic and temporal measures. The system works with any script in the Unicode Basic Multilingual Plane and is easily extended to include new modules. Given comparable corpora, such as newswire text, in a pair of languages that use different scripts, ScriptTranscriber provides an easy way to mine transliterations from the comparable texts. This is particularly useful for underresourced languages, where training data for transliteration may be lacking, and where it is thus hard to train good transliterators. ScriptTranscriber provides an open source package that allows for ready incorporation of more sophisticated modules ― e.g. a trained transliteration model for a particular language pair. ScriptTranscriber is available as part of the nltk contrib source tree at http://code.google.com/p/nltk/.</abstract>
      <bibkey>qian-etal-2010-python</bibkey>
    </paper>
    <paper id="14">
      <author><first>K. Bretonnel</first><last>Cohen</last></author>
      <author><first>Christophe</first><last>Roeder</last></author>
      <author><first>William A.</first><last>Baumgartner Jr.</last></author>
      <author><first>Lawrence E.</first><last>Hunter</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <title>Test Suite Design for Biomedical Ontology Concept Recognition Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/31_Paper.pdf</url>
      <abstract>Systems that locate mentions of concepts from ontologies in free text are known as ontology concept recognition systems. This paper describes an approach to the evaluation of the workings of ontology concept recognition systems through use of a structured test suite and presents a publicly available test suite for this purpose. It is built using the principles of descriptive linguistic fieldwork and of software testing. More broadly, we also seek to investigate what general principles might inform the construction of such test suites. The test suite was found to be effective in identifying performance errors in an ontology concept recognition system. The system could not recognize 2.1% of all canonical forms and no non-canonical forms at all. Regarding the question of general principles of test suite construction, we compared this test suite to a named entity recognition test suite constructor. We found that they had twenty features in total and that seven were shared between the two models, suggesting that there is a core of feature types that may be applicable to test suite construction for any similar type of application.</abstract>
      <bibkey>cohen-etal-2010-test</bibkey>
    </paper>
    <paper id="15">
      <author><first>Els</first><last>Lefever</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Construction of a Benchmark Data Set for Cross-lingual Word Sense Disambiguation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/34_Paper.pdf</url>
      <abstract>Given the recent trend to evaluate the performance of word sense disambiguation systems in a more application-oriented set-up, we report on the construction of a multilingual benchmark data set for cross-lingual word sense disambiguation. The data set was created for a lexical sample of 25 English nouns, for which translations were retrieved in 5 languages, namely Dutch, German, French, Italian and Spanish. The corpus underlying the sense inventory was the parallel data set Europarl. The gold standard sense inventory was based on the automatic word alignments of the parallel corpus, which were manually verified. The resulting word alignments were used to perform a manual clustering of the translations over all languages in the parallel corpus. The inventory then served as input for the annotators of the sentences, who were asked to provide a maximum of three contextually relevant translations per language for a given focus word. The data set was released in the framework of the SemEval-2010 competition.</abstract>
      <bibkey>lefever-hoste-2010-construction</bibkey>
    </paper>
    <paper id="16">
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <title>Corpus and Evaluation Measures for Automatic Plagiarism Detection</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/35_Paper.pdf</url>
      <abstract>The simple access to texts on digital libraries and the World Wide Web has led to an increased number of plagiarism cases in recent years, which renders manual plagiarism detection infeasible at large. Various methods for automatic plagiarism detection have been developed whose objective is to assist human experts in the analysis of documents for plagiarism. The methods can be divided into two main approaches: intrinsic and external. Unlike other tasks in natural language processing and information retrieval, it is not possible to publish a collection of real plagiarism cases for evaluation purposes since they cannot be properly anonymized. Therefore, current evaluations found in the literature are incomparable and, very often not even reproducible. Our contribution in this respect is a newly developed large-scale corpus of artificial plagiarism useful for the evaluation of intrinsic as well as external plagiarism detection. Additionally, new detection performance measures tailored to the evaluation of plagiarism detection algorithms are proposed.</abstract>
      <bibkey>barron-cedeno-etal-2010-corpus</bibkey>
    </paper>
    <paper id="17">
      <author><first>Claus</first><last>Zinn</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Jacquelijn</first><last>Ringersma</last></author>
      <title>An Evolving e<fixed-case>S</fixed-case>cience Environment for Research Data in Linguistics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/36_Paper.pdf</url>
      <abstract>The amount of research data in the Humanities is increasing at fast speed. Metadata helps describing and making accessible this data to interested researchers within and across institutions. While metadata interoperability is an issue that is being recognised and addressed, the systematic and user-driven provision of annotations and the linking together of resources into new organisational layers have received much less attention. This paper gives an overview of our evolving technological eScience environment to support such functionality. It describes two tools, ADDIT and ViCoS, which enable researchers, rather than archive managers, to organise and reorganise research data to fit their particular needs. The two tools, which are embedded into our institute's existing software landscape, are an initial step towards an eScience environment that gives our scientists easy access to (multimodal) research data of their interest, and empowers them to structure, enrich, link together, and share such data as they wish.</abstract>
      <bibkey>zinn-etal-2010-evolving</bibkey>
    </paper>
    <paper id="18">
      <author><first>Simon</first><last>Scerri</last></author>
      <author><first>Gerhard</first><last>Gossen</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <title>Classifying Action Items for Semantic Email</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/39_Paper.pdf</url>
      <abstract>Email can be considered as a virtual working environment in which users are constantly struggling to manage the vast amount of exchanged data. Although most of this data belongs to well-defined workflows, these are implicit and largely unsupported by existing email clients. Semanta provides this support by enabling Semantic Email ― email enhanced with machine-processable metadata about specific types of email Action Items (e.g. Task Assignment, Meeting Proposal). In the larger picture, these items form part of ad-hoc workflows (e.g. Task Delegation, Meeting Scheduling). Semanta is faced with a knowledge-acquisition bottleneck, as users cannot be expected to annotate each action item, and their automatic recognition proves difficult. This paper focuses on applying computationally treatable aspects of speech act theory for the classification of email action items. A rule-based classification model is employed, based on the presence or form of a number of linguistic features. The technologys evaluation suggests that whereas full automation is not feasible, the results are good enough to be presented as suggestions for the user to review. In addition the rule-based system will bootstrap a machine learning system that is currently in development, to generate the initial training sets which are then improved through the users reviewing.</abstract>
      <bibkey>scerri-etal-2010-classifying</bibkey>
    </paper>
    <paper id="19">
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <title>Automatic Acquisition of Parallel Corpora from Websites with Dynamic Content</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/40_Paper.pdf</url>
      <abstract>Parallel corpora are indispensable resources for a variety of multilingual natural language processing tasks. This paper presents a technique for fully automatic construction of constantly growing parallel corpora. We propose a simple and effective dictionary-based algorithm to extract parallel document pairs from a large collection of articles retrieved from the Internet, potentially containing manually translated texts. This algorithm was implemented and tested on Hebrew-English parallel texts. With properly selected thresholds, precision of 100% can be obtained.</abstract>
      <bibkey>tsvetkov-wintner-2010-automatic</bibkey>
    </paper>
    <paper id="20">
      <author><first>Vassiliki</first><last>Rentoumi</last></author>
      <author><first>Stefanos</first><last>Petrakis</last></author>
      <author><first>Manfred</first><last>Klenner</last></author>
      <author><first>George A.</first><last>Vouros</last></author>
      <author><first>Vangelis</first><last>Karkaletsis</last></author>
      <title>United we Stand: Improving Sentiment Analysis by Joining Machine Learning and Rule Based Methods</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/41_Paper.pdf</url>
      <abstract>In the past, we have succesfully used machine learning approaches for sentiment analysis. In the course of those experiments, we observed that our machine learning method, although able to cope well with figurative language could not always reach a certain decision about the polarity orientation of sentences, yielding erroneous evaluations. We support the conjecture that these cases bearing mild figurativeness could be better handled by a rule-based system. These two systems, acting complementarily, could bridge the gap between machine learning and rule-based approaches. Experimental results using the corpus of the Affective Text Task of SemEval 07, provide evidence in favor of this direction.</abstract>
      <bibkey>rentoumi-etal-2010-united</bibkey>
    </paper>
    <paper id="21">
      <author><first>Núria</first><last>Bel</last></author>
      <title>Handling of Missing Values in Lexical Acquisition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/45_Paper.pdf</url>
      <abstract>In this work we propose a strategy to reduce the impact of the sparse data problem in the tasks of lexical information acquisition based on the observation of linguistic cues. We propose a way to handle the uncertainty created by missing values, that is, when a zero value could mean either that the cue has not been observed because the word in question does not belong to the class, i.e. negative evidence, or that the word in question has just not been observed in the context sought by chance, i.e. lack of evidence. This uncertainty creates problems to the learner, because zero values for incompatible labelled examples make the cue lose its predictive capacity and even though some samples display the sought context, it is not taken into account. In this paper we present the results of our experiments to try to reduce this uncertainty by, as other authors do (Joanis et al. 2007, for instance), substituting zero values for pre-processed estimates. Here we present a first round of experiments that have been the basis for the estimates of linguistic information motivated by lexical classes. We obtained experimental results that show a clear benefit of the proposed approach.</abstract>
      <bibkey>bel-2010-handling</bibkey>
    </paper>
    <paper id="22">
      <author><first>Elin</first><last>Carlsson</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <title>Influence of Module Order on Rule-Based De-identification of Personal Names in Electronic Patient Records Written in <fixed-case>S</fixed-case>wedish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/46_Paper.pdf</url>
      <abstract>Electronic patient records (EPRs) are a valuable resource for research but for confidentiality reasons they cannot be used freely. In order to make EPRs available to a wider group of researchers, sensitive information such as personal names has to be removed. De-identification is a process that makes this possible. Both rule-based as well as statistical and machine learning based methods exist to perform de-identification, but the second method requires annotated training material which exists only very sparsely for patient names. It is therefore necessary to use rule-based methods for de-identification of EPRs. Not much is known, however, about the order in which the various rules should be applied and how the different rules influence precision and recall. This paper aims to answer this research question by implementing and evaluating four common rules for de-identification of personal names in EPRs written in Swedish: (1) dictionary name matching, (2) title matching, (3) common words filtering and (4) learning from previous modules. The results show that to obtain the highest recall and precision, the rules should be applied in the following order: title matching, common words filtering and dictionary name matching.</abstract>
      <bibkey>carlsson-dalianis-2010-influence</bibkey>
    </paper>
    <paper id="23">
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Mireia</first><last>Farrús</last></author>
      <author><first>José B.</first><last>Mariño</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <title>Automatic and Human Evaluation Study of a Rule-based and a Statistical <fixed-case>C</fixed-case>atalan-<fixed-case>S</fixed-case>panish Machine Translation Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/47_Paper.pdf</url>
      <abstract>Machine translation systems can be classified into rule-based and corpus-based approaches, in terms of their core technology. Since both paradigms have largely been used during the last years, one of the aims in the research community is to know how these systems differ in terms of translation quality. To this end, this paper reports a study and comparison of a rule-based and a corpus-based (particularly, statistical) Catalan-Spanish machine translation systems, both of them freely available in the web. The translation quality analysis is performed under two different domains: journalistic and medical. The systems are evaluated by using standard automatic measures, as well as by native human evaluators. Automatic results show that the statistical system performs better than the rule-based system. Human judgements show that in the Spanish-to-Catalan direction the statistical system also performs better than the rule-based system, while in the Catalan-to-Spanish direction is the other way round. Although the statistical system obtains the best automatic scores, its errors tend to be more penalized by human judgements than the errors of the rule-based system. This can be explained because statistical errors are usually unexpected and they do not follow any pattern.</abstract>
      <bibkey>costa-jussa-etal-2010-automatic</bibkey>
    </paper>
    <paper id="24">
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <author><first>Bharat Ram</first><last>Ambati</last></author>
      <title>An Integrated Digital Tool for Accessing Language Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/48_Paper.pdf</url>
      <abstract>Language resources can be classified under several categories. To be able to query and operate on all (or most of) these categories using a single digital tool would be very helpful for a large number of researchers working on languages. We describe such a tool in this paper. It is different from other such tools in that it allows querying and transformation on different kinds of resources (such as corpora, lexicon and language models) with the same framework. Search options can be given based on the kind of resource being queried. It is possible to select a matched resource and open it for editing in the specialized interfaces with which that resource is associated. The tool also allows the extracted or modified data to be saved separately, apart from having the usual facilities like displaying the results in KeyWord-In-Context (KWIC) format. We also present the notation used for querying and transformation, which is comparable to but different from the Corpus Query Language (CQL).</abstract>
      <bibkey>singh-ambati-2010-integrated</bibkey>
    </paper>
    <paper id="25">
      <author><first>Jakob Schou</first><last>Pedersen</last></author>
      <author><first>Lars Bo</first><last>Larsen</last></author>
      <title>A Speech Corpus for Dyslexic Reading Training</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/52_Paper.pdf</url>
      <abstract>Traditional Danish reading training for dyslexic readers typically involves the presence of a professional reading therapist for guidance, advice and evaluation. Allowing dyslexic readers to train their reading skills on their own could not only benefit the dyslexics themselves in terms of increased flexibility but could also allow professional therapists to increase the amount of dyslexic readers to whom they have a professional contact. It is envisioned that an automated reading training tool operating on the basis of ASR could provide dyslexic users with such independence. However, only limited experience in handling dyslexic input (in Danish) by a speech recognizer exists currently. This paper reports on the establishment of a speech corpus of Danish dyslexic speech along with an annotation hereof and the setup of a proof-of-concept training tool allowing dyslexic users to improve their reading skills on their own. Despite relatively limited ASR performance, a usability evaluation by dyslexic users shows an unconditional belief in the fairness of the system and indicates furthermore willingness for using such a training tool.</abstract>
      <bibkey>pedersen-larsen-2010-speech</bibkey>
    </paper>
    <paper id="26">
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Imed</first><last>Zitouni</last></author>
      <title><fixed-case>A</fixed-case>rabic Word Segmentation for Better Unit of Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/54_Paper.pdf</url>
      <abstract>The Arabic language has a very rich morphology where a word is composed of zero or more prefixes, a stem and zero or more suffixes. This makes Arabic data sparse compared to other languages, such as English, and consequently word segmentation becomes very important for many Natural Language Processing tasks that deal with the Arabic language. We present in this paper two segmentation schemes that are morphological segmentation and Arabic TreeBank segmentation and we show their impact on an important natural language processing task that is mention detection. Experiments on Arabic TreeBank corpus show 98.1% accuracy on morphological segmentation and 99.4% on morphological segmentation. We also discuss the importance of segmenting the text; experiments show up to 6F points improvement of the mention detection system performance when morphological segmentation is used instead of not segmenting the text. Obtained results also show up to 3F points improvement is achieved when the appropriate segmentation style is used.</abstract>
      <bibkey>benajiba-zitouni-2010-arabic</bibkey>
    </paper>
    <paper id="27">
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Kiyong</first><last>Lee</last></author>
      <author><first>Harry</first><last>Bunt</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <title><fixed-case>ISO</fixed-case>-<fixed-case>T</fixed-case>ime<fixed-case>ML</fixed-case>: An International Standard for Semantic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/55_Paper.pdf</url>
      <abstract>In this paper, we present ISO-TimeML, a revised and interoperable version of the temporal markup language, TimeML. We describe the changes and enrichments made, while framing the effort in a more general methodology of semantic annotation. In particular, we assume a principled distinction between the annotation of an expression and the representation which that annotation denotes. This involves not only the specification of an annotation language for a particular phenomenon, but also the development of a meta-model that allows one to interpret the syntactic expressions of the specification semantically.</abstract>
      <bibkey>pustejovsky-etal-2010-iso</bibkey>
    </paper>
    <paper id="28">
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Ivan</first><last>Obradović</last></author>
      <author><first>Olivera</first><last>Kitanović</last></author>
      <title><fixed-case>GIS</fixed-case> Application Improvement with Multilingual Lexical and Terminological Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/57_Paper.pdf</url>
      <abstract>This paper introduces the results of integration of lexical and terminological resources, most of them developed within the Human Language Technology (HLT) Group at the University of Belgrade, with the Geological information system of Serbia (GeolISS) developed at the Faculty of Mining and Geology and funded by the Ministry of the Environmental protection. The approach to GeolISS development, which is aimed at the integration of existing geologic archives, data from published maps on different scales, newly acquired field data, and intranet and internet publishing of geologic is given, followed by the description of the geologic multilingual vocabulary and other lexical and terminological resources used. Two basic results are outlined: multilingual map annotation and improvement of queries for the GeolISS geodatabase. Multilingual labelling and annotation of maps for their graphic display and printing have been tested with Serbian, which describes regional information in the local language, and English, used for sharing geographic information with the world, although the geological vocabulary offers the possibility for integration of other languages as well. The resources also enable semantic and morphological expansion of queries, the latter being very important in highly inflective languages, such as Serbian.</abstract>
      <bibkey>stankovic-etal-2010-gis</bibkey>
    </paper>
    <paper id="29">
      <author><first>Nathanael</first><last>Chambers</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <title>A Database of Narrative Schemas</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/58_Paper.pdf</url>
      <abstract>This paper describes a new language resource of events and semantic roles that characterize real-world situations. Narrative schemas contain sets of related events (edit and publish), a temporal ordering of the events (edit before publish), and the semantic roles of the participants (authors publish books). This type of world knowledge was central to early research in natural language understanding, scripts being one of the main formalisms, they represented common sequences of events that occur in the world. Unfortunately, most of this knowledge was hand-coded and time consuming to create. Current machine learning techniques, as well as a new approach to learning through coreference chains, has allowed us to automatically extract rich event structure from open domain text in the form of narrative schemas. The narrative schema resource described in this paper contains approximately 5000 unique events combined into schemas of varying sizes. We describe the resource, how it is learned, and a new evaluation of the coverage of these schemas over unseen documents.</abstract>
      <bibkey>chambers-jurafsky-2010-database</bibkey>
    </paper>
    <paper id="30">
      <author><first>Dimitrios</first><last>Kokkinakis</last></author>
      <author><first>Ulla</first><last>Gerdin</last></author>
      <title>A <fixed-case>S</fixed-case>wedish Scientific Medical Corpus for Terminology Management and Linguistic Exploration</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/60_Paper.pdf</url>
      <abstract>This paper describes the development of a new Swedish scientific medical corpus. We provide a detailed description of the characteristics of this new collection as well results of an application of the corpus on term management tasks, including terminology validation and terminology extraction. Although the corpus is representative for the scientific medical domain it still covers in detail a lot of specialised sub-disciplines such as diabetes and osteoporosis which makes it suitable for facilitating the production of smaller but more focused sub-corpora. We address this issue by making explicit some features of the corpus in order to demonstrate the usability of the corpus particularly for the quality assessment of subsets of official terminologies such as the Systematized NOmenclature of MEDicine - Clinical Terms (SNOMED CT). Domain-dependent language resources, labelled or not, are a crucial key components for progressing R&amp;D in the human language technology field since such resources are an indispensable, integrated part for terminology management, evaluation, software prototyping and design validation and a prerequisite for the development and evaluation of a number of sublanguage dependent applications including information extraction, text mining and information retrieval.</abstract>
      <bibkey>kokkinakis-gerdin-2010-swedish</bibkey>
    </paper>
    <paper id="31">
      <author><first>Thomas</first><last>Proisl</last></author>
      <author><first>Besim</first><last>Kabashi</last></author>
      <title>Using High-Quality Resources in <fixed-case>NLP</fixed-case>: The Valency Dictionary of <fixed-case>E</fixed-case>nglish as a Resource for Left-Associative Grammars</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/62_Paper.pdf</url>
      <abstract>In Natural Language Processing (NLP), the quality of a system depends to a great extent on the quality of the linguistic resources it uses. One area where precise information is particularly needed is valency. The unpredictable character of valency properties requires a reliable source of information for syntactic and semantic analysis. There are several (electronic) dictionaries that provide the necessary information. One such dictionary that contains especially detailed valency descriptions is the Valency Dictionary of English. We will discuss how the Valency Dictionary of English in machine-readable form can be used as a resource for NLP. We will use valency descriptions that are freely available online via the Erlangen Valency Pattern Bank which contains most of the information from the printed dictionary. We will show that the valency data can be used for accurately parsing natural language with a rule-based approach by integrating it into a Left-Associative Grammar. The Valency Dictionary of English can therefore be regarded as being well suited for NLP purposes.</abstract>
      <bibkey>proisl-kabashi-2010-using</bibkey>
    </paper>
    <paper id="32">
      <author><first>Xabier</first><last>Saralegi</last></author>
      <author><first>Maddalen</first><last>Lopez de Lacalle</last></author>
      <title>Dictionary and Monolingual Corpus-based Query Translation for <fixed-case>B</fixed-case>asque-<fixed-case>E</fixed-case>nglish <fixed-case>CLIR</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/63_Paper.pdf</url>
      <abstract>This paper deals with the main problems that arise in the query translation process in dictionary-based Cross-lingual Information Retrieval (CLIR): translation selection, presence of Out-Of-Vocabulary (OOV) terms and translation of Multi-Word Expressions (MWE). We analyse to what extent each problem affects the retrieval performance for the Basque-English pair of languages, and the improvement obtained when using parallel corpora free methods to address them. To tackle the translation selection problem we provide novel extensions of an already existing monolingual target co-occurrence-based method, the Out-Of Vocabulary terms are dealt with by means of a cognate detection-based method and finally, for the Multi-Word Expression translation problem, a naïve matching technique is applied. The error analysis shows significant differences in the deterioration of the performance depending on the problem, in terms of Mean Average Precision (MAP), the translation selection problem being the cause of most of the errors. Otherwise, the proposed combined strategy shows a good performance to tackle the three above-mentioned main problems.</abstract>
      <bibkey>saralegi-lopez-de-lacalle-2010-dictionary</bibkey>
    </paper>
    <paper id="33">
      <author><first>Véronika</first><last>Lux-Pogodalla</last></author>
      <author><first>Dominique</first><last>Besagni</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <title><fixed-case>F</fixed-case>ast<fixed-case>K</fixed-case>wic, an “Intelligent“ Concordancer Using <fixed-case>FASTR</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/65_Paper.pdf</url>
      <abstract>In this paper, we introduce the FastKwic (Key Word In Context using FASTR), a new concordancer for French and English that does not require users to learn any particular request language. Built on FASTR, it shows them not only occurrences of the searched term but also of several morphological, morpho-syntactic and syntactic variants (for example, image enhancement, enhancement of image, enhancement of fingerprint image, image texture enhancement). Fastkwic is freely available. It consists of two UTF-8 compliant Perl modules that depend on several external tools and resources : FASTR, TreeTagger, Flemm (for French). Licenses of theses tools and resources permitting, the FastKwic package is nevertheless self-sufficient. FastKwic first modules is for terminological resource compilation. Its input is a list of terms - as required by FASTR. FastKwic second module is for processing concordances. It relies on FASTR again for indexing the input corpus with terms and their variants. Its output is a concordancer: for each term and its variants, the context of occurrence is provided.</abstract>
      <bibkey>lux-pogodalla-etal-2010-fastkwic</bibkey>
    </paper>
    <paper id="34">
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Duško</first><last>Vitas</last></author>
      <title>A Description of Morphological Features of <fixed-case>S</fixed-case>erbian: a Revision using Feature System Declaration</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/66_Paper.pdf</url>
      <abstract>In this paper we discuss some well-known morphological descriptions used in various projects and applications (most notably MULTEXT-East and Unitex) and illustrate the encountered problems on Serbian. We have spotted four groups of problems: the lack of a value for an existing category, the lack of a category, the interdependence of values and categories lacking some description, and the lack of a support for some types of categories. At the same time, various descriptions often describe exactly the same morphological property using different approaches. We propose a new morphological description for Serbian following the feature structure representation defined by the ISO standard. In this description we try do incorporate all characteristics of Serbian that need to be specified for various applications. We have developed several XSLT scripts that transform our description into descriptions needed for various applications. We have developed the first version of this new description, but we treat it as an ongoing project because for some properties we have not yet found the satisfactory solution.</abstract>
      <bibkey>krstev-etal-2010-description</bibkey>
    </paper>
    <paper id="35">
      <author><first>Plaban Kr.</first><last>Bhowmick</last></author>
      <author><first>Anupam</first><last>Basu</last></author>
      <author><first>Pabitra</first><last>Mitra</last></author>
      <title>Determining Reliability of Subjective and Multi-label Emotion Annotation through Novel Fuzzy Agreement Measure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/67_Paper.pdf</url>
      <abstract>The paper presents a new fuzzy agreement measure $\gamma_f$ for determining the agreement in multi-label and subjective annotation task. In this annotation framework, one data item may belong to a category or a class with a belief value denoting the degree of confidence of an annotator in assigning the data item to that category. We have provided a notion of disagreement based on the belief values provided by the annotators with respect to a category. The fuzzy agreement measure $\gamma_f$ has been proposed by defining different fuzzy agreement sets based on the distribution of difference of belief values provided by the annotators. The fuzzy agreement has been computed by studying the average agreement over all the data items and annotators. Finally, we elaborate on the computation $\gamma_f$ measure with a case study on emotion text data where a data item (sentence) may belong to more than one emotion category with varying belief values.</abstract>
      <bibkey>bhowmick-etal-2010-determining</bibkey>
    </paper>
    <paper id="36">
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <title><fixed-case>FIDJI</fixed-case>: Web Question-Answering at Quaero 2009</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/68_Paper.pdf</url>
      <abstract>This paper presents the participation of FIDJI system to the Web Question-Answering evaluation campaign organized by Quaero in 2009. FIDJI is an open-domain question-answering system which combines syntactic information with traditional QA techniques such as named entity recognition and term weighting in order to validate answers through multiple documents. It was originally designed to process ``clean'' document collections. Overall results are significantly lower than in traditional campaigns but results (for French evaluation) are quite good compared to other state-of-the-art systems. They show that a syntax-based strategy, applied on uncleaned Web data, can still obtain good results. Moreover, we obtain much higher scores on ``complex'' questions, i.e. `how' and `why' questions, which are more representative of real user needs. These results show that questioning the Web with advanced linguistic techniques can be done without heavy pre-processing and with results that come near to best systems that use strong resources and large structured indexes.</abstract>
      <bibkey>tannier-moriceau-2010-fidji</bibkey>
    </paper>
    <paper id="37">
      <author><first>Kai</first><last>Wörner</last></author>
      <title>A Tool for Feature-Structure Stand-Off-Annotation on Transcriptions of Spoken Discourse</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/69_Paper.pdf</url>
      <abstract>Annotation Science, a discipline dedicated to developing and maturing methodology for the annotation of language resources, is playing a prominent role in the fields of computational and corpus linguistics. While progress in the search for the right annotation model and format is undeniable, these results only sparsely become manifest in actual solutions (i.e. software tools) that could be used by researchers wishing to annotate their resources right away, even less so for resources of spoken language transcriptions. The paper presents a solution consisting of a data model and an annotation tool that tries to fill this gap between âannotation science and the practice of transcribing spoken language in the area of discourse analysis and pragmatics, where the lack of ready-to-use annotation solutions is especially remarkable. The chosen model combines feature structures in standoff-annotation and a data model based on annotation graphs, combining their advantages. It is ideally fitted for the transcription of spoken language by centering on the temporal relations of the speakers utterances and is implemented in reliable tools that support an iterative workflow. The standoff annotation allows for more complex annotations and relies on an established and well documented model.</abstract>
      <bibkey>worner-2010-tool</bibkey>
    </paper>
    <paper id="38">
      <author><first>Jan</first><last>Odijk</last></author>
      <title>The <fixed-case>CLARIN</fixed-case>-<fixed-case>NL</fixed-case> Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/70_Paper.pdf</url>
      <abstract>In this paper I present the CLARIN-NL project, the Dutch national project that aims to play a central role in the European CLARIN infrastructure, not only for the preparatory phase, but also for the implementation and exploitation phases. I argue that the way the CLARIN-NL project has been set-up can serve as an excellent example for other national CLARIN projects, for the following reasons: (1) it is a mix between a programme and a project; (2) it offers opportunities to seriously test standards and protocols currently proposed by CLARIN, thus providing evidence-based requirements and desiderata for the CLARIN infrastructure and ensuring compatibility of CLARIN with national data and tools; (3) it brings the intended users (humanities researchers) and the technology providers (infrastructure specialists and language and speech technology researchers) together in concrete cooperation projects, with a central role for the users research questions,, thus ensuring that the infrastructure will provide functionality that is needed by its intended users.</abstract>
      <bibkey>odijk-2010-clarin</bibkey>
    </paper>
    <paper id="39">
      <author><first>Virach</first><last>Sornlertlamvanich</last></author>
      <author><first>Thatsanee</first><last>Charoenporn</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Language Resource Management System for <fixed-case>A</fixed-case>sian <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Collaboration and Its Web Service Application</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/71_Paper.pdf</url>
      <abstract>This paper presents the language resource management system for the development and dissemination of Asian WordNet (AWN) and its web service application. We develop the platform to establish a network for the cross language WordNet development. Each node of the network is designed for maintaining the WordNet for a language. Via the table that maps between each language WordNet and the Princeton WordNet (PWN), the Asian WordNet is realized to visualize the cross language WordNet between the Asian languages. We propose a language resource management system, called WordNet Management System (WNMS), as a distributed management system that allows the server to perform the cross language WordNet retrieval, including the fundamental web service applications for editing, visualizing and language processing. The WNMS is implemented on a web service protocol therefore each node can be independently maintained, and the service of each language WordNet can be called directly through the web service API. In case of cross language implementation, the synset ID (or synset offset) defined by PWN is used to determined the linkage between the languages.</abstract>
      <bibkey>sornlertlamvanich-etal-2010-language</bibkey>
    </paper>
    <paper id="40">
      <author><first>Jinho D.</first><last>Choi</last></author>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title><fixed-case>P</fixed-case>ropbank Frameset Annotation Guidelines Using a Dedicated Editor, Cornerstone</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/73_Paper.pdf</url>
      <abstract>This paper gives guidelines of how to create and update Propbank frameset files using a dedicated editor, Cornerstone. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Thus, for each predicate in Propbank, there exists a corresponding frameset file showing the expected predicate argument structure of each sense related to the predicate. Since most Propbank annotations are based on the predicate argument structure defined in the frameset files, it is important to keep the files consistent, simple to read as well as easy to update. The frameset files are written in XML, which can be difficult to edit when using a simple text editor. Therefore, it is helpful to develop a user-friendly editor such as Cornerstone, specifically customized to create and edit frameset files. Cornerstone runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.</abstract>
      <bibkey>choi-etal-2010-propbank</bibkey>
    </paper>
    <paper id="41">
      <author><first>Johannes</first><last>Handl</last></author>
      <author><first>Carsten</first><last>Weber</last></author>
      <title>A Multilayered Declarative Approach to Cope with Morphotactics and Allomorphy in Derivational Morphology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/76_Paper.pdf</url>
      <abstract>This paper deals with the derivational morphology of automatic word form recognition. It presents a set of declarative rules which augment lexical entries with information governing the allomorphic changes of derivation in addition to the existing allomorphy rules for inflection. The resulting component generates a single lexicon for derivational and inflectional allomorphy from an elementary base-form lexicon. Thereby our focus lies both on avoiding redundant allomorph entries and on the suitability of the resulting lexical entries for morphological analysis. We prove the usability of our approach by using the generated allomorphs as the lexicon for automatic wordform recognition.</abstract>
      <bibkey>handl-weber-2010-multilayered</bibkey>
    </paper>
    <paper id="42">
      <author><first>Olga</first><last>Lyashevskaya</last></author>
      <title>Bank of <fixed-case>R</fixed-case>ussian Constructions and Valencies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/77_Paper.pdf</url>
      <abstract>The Bank of Russian Constructions and Valencies (Russian FrameBank) is an annotation project that takes as input samples from the Russian National Corpus (http://www.ruscorpora.ru). Since Russian verbs and predicates from other POS classes have their particular and not always predictable case pattern, these words and their argument structures are to be described as lexical constructions. The slots of partially filled phrasal constructions (e.g. vzjal i uexal he suddenly (lit. took and) went away) are also under analysis. Thus, the notion of construction is understood in the sense of Fillmores Construction Grammar and is not limited to that of argument structure of verbs. FrameBank brings together the dictionary of constructions and the annotated collection of examples. Our goal is to mark the set of arguments and adjuncts of a certain construction. The main focus is on realization of the elements in the running text, to facilitate searches through pattern realizations by a certain combination of features. The relevant dataset involves lexical, POS and other morphosyntactic tags, semantic classes, as well as grammatical constructions that introduce or license the use of elements within a given construction.</abstract>
      <bibkey>lyashevskaya-2010-bank</bibkey>
    </paper>
    <paper id="43">
      <author><first>Zareen</first><last>Syed</last></author>
      <author><first>Evelyne</first><last>Viegas</last></author>
      <author><first>Savas</first><last>Parastatidis</last></author>
      <title>Automatic Discovery of Semantic Relations using <fixed-case>M</fixed-case>ind<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/78_Paper.pdf</url>
      <abstract>Information extraction deals with extracting entities (such as people, organizations or locations) and named relations between entities (such as ""People born-in Country"") from text documents. An important challenge in information extraction is the labeling of training data which is usually done manually and is therefore very laborious and in certain cases impractical. This paper introduces a new model to extract semantic relations fully automatically from text using the Encarta encyclopedia and lexical-semantic relations discovered by MindNet. MindNet is a lexical knowledge base that can be constructed fully automatically from a given text corpus without any human intervention. Encarta articles are categorized and linked to related articles by experts. We demonstrate how the structured data available in Encarta and the lexical semantic relations between words in MindNet can be used to enrich MindNet with semantic relations between entities. With a slight trade off of accuracy a semantically enriched MindNet can be used to extract relations from a text corpus without any human intervention.</abstract>
      <bibkey>syed-etal-2010-automatic</bibkey>
    </paper>
    <paper id="44">
      <author><first>Adam</first><last>Kilgarriff</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Jan</first><last>Pomikálek</last></author>
      <author><first>Avinesh</first><last>PVS</last></author>
      <title>A Corpus Factory for Many Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/79_Paper.pdf</url>
      <abstract>For many languages there are no large, general-language corpora available. Until the web, all but the institutions could do little but shake their heads in dismay as corpus-building was long, slow and expensive. But with the advent of the Web it can be highly automated and thereby fast and inexpensive. We have developed a corpus factory where we build large corpora. In this paper we describe the method we use, and how it has worked, and how various problems were solved, for eight languages: Dutch, Hindi, Indonesian, Norwegian, Swedish, Telugu, Thai and Vietnamese. We use the BootCaT method: we take a set of 'seed words' for the language from Wikipedia. Then, several hundred times over, we * randomly select three or four of the seed words * send as a query to Google or Yahoo or Bing, which returns a 'search hits' page * gather the pages that Google or Yahoo point to and save the text. This forms the corpus, which we then * 'clean' (to remove navigation bars, advertisements etc) * remove duplicates * tokenise and (if tools are available) lemmatise and part-of-speech tag * load into our corpus query tool, the Sketch Engine The corpora we have developed are available for use in the Sketch Engine corpus query tool.</abstract>
      <bibkey>kilgarriff-etal-2010-corpus</bibkey>
    </paper>
    <paper id="45">
      <author><first>Richard</first><last>Johansson</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <title>A Flexible Representation of Heterogeneous Annotation Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/80_Paper.pdf</url>
      <abstract>This paper describes a new flexible representation for the annotation of complex structures of metadata over heterogeneous data collections containing text and other types of media such as images or audio files. We argue that existing frameworks are not suitable for this purpose, most importantly because they do not easily generalize to multi-document and multimodal corpora, and because they often require the use of particular software frameworks. In the paper, we define a data model to represent such structured data over multimodal collections. Furthermore, we define a surface realization of the data structure as a simple and readable XML format. We present two examples of annotation tasks to illustrate how the representation and format work for complex structures involving multimodal annotation and cross-document links. The representation described here has been used in a large-scale project focusing on the annotation of a wide range of information ― from low-level features to high-level semantics ― in a multimodal data collection containing both text and images.</abstract>
      <bibkey>johansson-moschitti-2010-flexible</bibkey>
    </paper>
    <paper id="46">
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Fanny</first><last>Grandry</last></author>
      <title>Hybrid Citation Extraction from Patents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/81_Paper.pdf</url>
      <abstract>The Quaero project organized a set of evaluations of Named Entity recognition systems in 2009. One of the sub-tasks consists in extracting citations from patents, i.e. references to other documents, either other patents or general literature from English-language patents. We present in this paper the participation of LIMSI in this evaluation, with a complete system description and the evaluation results. The corpus shown that patent and non-patent citations have a very different nature. We then separated references to other patents and to general literature papers and we created a hybrid system. For patent citations, the system used rule-based expert knowledge on the form of regular expressions. The system for detecting non-patent citations, on the other hand, is purely stochastic (machine learning with CRF++). Then we mixed both approaches to provide a single output. 4 teams participated to this task and our system obtained the best results of this evaluation campaign, even if the difference between the first two systems is poorly significant.</abstract>
      <bibkey>galibert-etal-2010-hybrid</bibkey>
    </paper>
    <paper id="47">
      <author><first>Luca</first><last>Dini</last></author>
      <author><first>Giampaolo</first><last>Mazzini</last></author>
      <title>The Impact of Grammar Enhancement on Semantic Resources Induction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/82_Paper.pdf</url>
      <abstract>In this paper describes the effects of the evolution of an Italian dependency grammar on a task of multilingual FrameNet acquisition. The task is based on the creation of virtual English/Italian parallel annotation corpora, which are then aligned at dependency level by using two manually encoded grammar based dependency parsers. We show how the evolution of the LAS (Labeled Attachment Score) metric for the considered grammar has a direct impact on the quality of the induced FrameNet, thus proving that the evolution of the quality of syntactic resources is mirrored by an analogous evolution in semantic ones. In particular we show that an improvement of 30% in LAS causes an improvement of precision for the induced resource ranging from 5% to 10%, depending on the type of evaluation.</abstract>
      <bibkey>dini-mazzini-2010-impact</bibkey>
    </paper>
    <paper id="48">
      <author><first>Yiou</first><last>Wang</last></author>
      <author><first>Kiyotaka</first><last>Uchimoto</last></author>
      <author><first>Jun’ichi</first><last>Kazama</last></author>
      <author><first>Canasai</first><last>Kruengkrai</last></author>
      <author><first>Kentaro</first><last>Torisawa</last></author>
      <title>Adapting <fixed-case>C</fixed-case>hinese Word Segmentation for Machine Translation Based on Short Units</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/83_Paper.pdf</url>
      <abstract>In Chinese texts, words composed of single or multiple characters are not separated by spaces, unlike most western languages. Therefore Chinese word segmentation is considered an important first step in machine translation (MT) and its performance impacts MT results. Many factors affect Chinese word segmentations, including the segmentation standards and segmentation strategies. The performance of a corpus-based word segmentation model depends heavily on the quality and the segmentation standard of the training corpora. However, we observed that existing manually annotated Chinese corpora tend to have low segmentation granularity and provide poor morphological information due to the present segmentation standards. In this paper, we introduce a short-unit standard of Chinese word segmentation, which is particularly suitable for machine translation, and propose a semi-automatic method of transforming the existing corpora into the ones that can satisfy our standards. We evaluate the usefulness of our approach on the basis of translation tasks from the technology newswire domain and the scientific paper domain, and demonstrate that it significantly improves the performance of Chinese-Japanese machine translation (over 1.0 BLEU increase).</abstract>
      <bibkey>wang-etal-2010-adapting</bibkey>
    </paper>
    <paper id="49">
      <author><first>Ekaterina</first><last>Ovchinnikova</last></author>
      <author><first>Laure</first><last>Vieu</last></author>
      <author><first>Alessandro</first><last>Oltramari</last></author>
      <author><first>Stefano</first><last>Borgo</last></author>
      <author><first>Theodore</first><last>Alexandrov</last></author>
      <title>Data-Driven and Ontological Analysis of <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et for Natural Language Reasoning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/84_Paper.pdf</url>
      <abstract>This paper focuses on the improvement of the conceptual structure of FrameNet (FN) for the sake of applying this resource to knowledge-intensive NLP tasks requiring reasoning, such as question answering, information extraction etc. In this paper we show that in addition to coverage incompleteness, the current version of FN suffers from conceptual inconsistency and lacks axiomatization which can prevent appropriate inferences. For the sake of discovering and classifying conceptual problems in FN we investigate the FrameNet-Annotated corpus for Textual Entailment. Then we propose a methodology for improving the conceptual organization of FN. The main issue we focus on in our study is enriching, axiomatizing and cleaning up frame relations. Our methodology includes a data-driven analysis of frames resulting in discovering new frame relations and an ontological analysis of frames and frame relations resulting in axiomatizing relations and formulating constraints on them. In this paper, frames and frame relations are analyzed in terms of the DOLCE formal ontology. Additionally, we have described a case study aiming at demonstrating how the proposed methodology works in practice as well as investigating the impact of the restructured and axiomatized frame relations on recognizing textual entailment.</abstract>
      <bibkey>ovchinnikova-etal-2010-data</bibkey>
    </paper>
    <paper id="50">
      <author><first>Samira</first><last>Shaikh</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>Aaron</first><last>Broadwell</last></author>
      <author><first>Jennifer</first><last>Stromer-Galley</last></author>
      <author><first>Sarah</first><last>Taylor</last></author>
      <author><first>Nick</first><last>Webb</last></author>
      <title><fixed-case>MPC</fixed-case>: A Multi-Party Chat Corpus for Modeling Social Phenomena in Discourse</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/85_Paper.pdf</url>
      <abstract>In this paper, we describe our experience with collecting and creating an annotated corpus of multi-party online conversations in a chat-room environment. This effort is part of a larger project to develop computational models of social phenomena such as agenda control, influence, and leadership in on-line interactions. Such models will help capturing the dialogue dynamics that are essential for developing, among others, realistic human-machine dialogue systems, including autonomous virtual chat agents. In this paper we describe data collection method used and the characteristics of the initial dataset of English chat. We have devised a multi-tiered collection process in which the subjects start from simple, free-flowing conversations and progress towards more complex and structured interactions. In this paper, we report on the first two stages of this process, which were recently completed. The third, large-scale collection effort is currently being conducted. All English dialogue has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics. Some details of these annotations will be discussed later in this paper, although a full description is impossible within the scope of this article.</abstract>
      <bibkey>shaikh-etal-2010-mpc</bibkey>
    </paper>
    <paper id="51">
      <author><first>Yanli</first><last>Sun</last></author>
      <title>Mining the Correlation between Human and Automatic Evaluation at Sentence Level</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/87_Paper.pdf</url>
      <abstract>Automatic evaluation metrics are fast and cost-effective measurements of the quality of a Machine Translation (MT) system. However, as humans are the end-user of MT output, human judgement is the benchmark to assess the usefulness of automatic evaluation metrics. While most studies report the correlation between human evaluation and automatic evaluation at corpus level, our study examines their correlation at sentence level. In addition to the statistical correlation scores, such as Spearman's rank-order correlation coefficient, a finer-grained and detailed examination of the sensitivity of automatic metrics compared to human evaluation is also reported in this study. The results show that the threshold for human evaluators to agree with the judgements of automatic metrics varies with the automatic metrics at sentence level. While the automatic scores for two translations are greatly different, human evaluators may consider the translations to be qualitatively similar and vice versa. The detailed analysis of the correlation between automatic and human evaluation allows us determine with increased confidence whether an increase in the automatic scores will be agreed by human evaluators or not.</abstract>
      <bibkey>sun-2010-mining</bibkey>
    </paper>
    <paper id="52">
      <author><first>Alberto</first><last>Simões</last></author>
      <author><first>José João</first><last>Almeida</last></author>
      <author><first>Rita</first><last>Farinha</last></author>
      <title>Processing and Extracting Data from Dicionário Aberto</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/90_Paper.pdf</url>
      <abstract>Synonyms dictionaries are useful resources for natural language processing. Unfortunately their availability in digital format is limited, as publishing companies do not release their dictionaries in open digital formats. Dicionário-Aberto (Simões and Farinha, 2010) is an open and free digital synonyms dictionary for the Portuguese language. It is under public domain and in textual digital format, which makes it usable for any task. Synonyms dictionaries are commonly used for the extraction of relations between words, the construction of complex structures like ontologies or thesaurus (comparable to WordNet (Miller et al., 1990)), or just the extraction of lists of words of specific type. This article will present Dicionário-Aberto, discussing how it was created, its main characteristics, the type of information present on it and the formats in which it is available. Follows the description of an API designed specifically to help Dicionário-Aberto processing without the need to tackle with the dictionary format. Finally, we will analyze the results on some data extraction experiments, extracting lists of words from a specific class, and extracting relationships between words.</abstract>
      <bibkey>simoes-etal-2010-processing</bibkey>
    </paper>
    <paper id="53">
      <author><first>Ulli</first><last>Waltinger</last></author>
      <title><fixed-case>G</fixed-case>erman<fixed-case>P</fixed-case>olarity<fixed-case>C</fixed-case>lues: A Lexical Resource for <fixed-case>G</fixed-case>erman Sentiment Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/91_Paper.pdf</url>
      <abstract>In this paper, we propose GermanPolarityClues, a new publicly available lexical resource for sentiment analysis for the German language. While sentiment analysis and polarity classification has been extensively studied at different document levels (e.g. sentences and phrases), only a few approaches explored the effect of a polarity-based feature selection and subjectivity resources for the German language. This paper evaluates four different English and three different German sentiment resources in a comparative manner by combining a polarity-based feature selection with SVM-based machine learning classifier. Using a semi-automatic translation approach, we were able to construct three different resources for a German sentiment analysis. The manually finalized GermanPolarityClues dictionary offers thereby a number of 10, 141 polarity features, associated to three numerical polarity scores, determining the positive, negative and neutral direction of specific term features. While the results show that the size of dictionaries clearly correlate to polarity-based feature coverage, this property does not correlate to classification accuracy. Using a polarity-based feature selection, considering a minimum amount of prior polarity features, in combination with SVM-based machine learning methods exhibits for both languages the best performance (F1: 0.83-0.88).</abstract>
      <bibkey>waltinger-2010-germanpolarityclues</bibkey>
    </paper>
    <paper id="54">
      <author><first>Antonio</first><last>Pareja-Lora</last></author>
      <author><first>Guadalupe Aguado</first><last>de Cea</last></author>
      <title>Ontology-based Interoperation of Linguistic Tools for an Improved Lemma Annotation in <fixed-case>S</fixed-case>panish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/92_Paper.pdf</url>
      <abstract>In this paper, we present an ontology-based methodology and architecture for the comparison, assessment, combination (and, to some extent, also contrastive evaluation) of the results of different linguistic tools. More specifically, we describe an experiment aiming at the improvement of the correctness of lemma tagging for Spanish. This improvement was achieved by means of the standardisation and combination of the results of three different linguistic annotation tools (Bitexts DataLexica, Connexors FDG Parser and LACELLs POS tagger), using (1) ontologies, (2) a set of lemma tagging correction rules, determined empirically during the experiment, and (3) W3C standard languages, such as XML, RDF(S) and OWL. As we show in the results of the experiment, the interoperation of these tools by means of ontologies and the correction rules applied in the experiment improved significantly the quality of the resulting lemma tagging (when compared to the separate lemma tagging performed by each of the tools that we made interoperate).</abstract>
      <bibkey>pareja-lora-de-cea-2010-ontology</bibkey>
    </paper>
    <paper id="55">
      <author><first>Torsten</first><last>Zesch</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <title>The More the Better? Assessing the Influence of <fixed-case>W</fixed-case>ikipedia’s Growth on Semantic Relatedness Measures</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/93_Paper.pdf</url>
      <abstract>Wikipedia has been used as a knowledge source in many areas of natural language processing. As most studies only use a certain Wikipedia snapshot, the influence of Wikipedias massive growth on the results is largely unknown. For the first time, we perform an in-depth analysis of this influence using semantic relatedness as an example application that tests a wide range of Wikipedias properties. We find that the growth of Wikipedia has almost no effect on the correlation of semantic relatedness measures with human judgments, while the coverage steadily increases.</abstract>
      <bibkey>zesch-gurevych-2010-better</bibkey>
    </paper>
    <paper id="56">
      <author><first>Nick</first><last>Campbell</last></author>
      <author><first>Akiko</first><last>Tabata</last></author>
      <title>A Software Toolkit for Viewing Annotated Multimodal Data Interactively over the Web</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/96_Paper.pdf</url>
      <abstract>This paper describes a software toolkit for the interactive display and analysis of automatically extracted or manually derived annotation features of visual and audio data. It has been extensively tested with material collected as part of the FreeTalk Multimodal Conversation Corpus. Both the corpus and the software are available for download from sites in Europe and Japan. The corpus consists of several hours of video and audio recordings from a variety of capture devices, and includes subjective annotations of the content, along with derived data obtained from image processing. Because of the large size of the corpus, it is unrealistic to expect researchers to download all the material before deciding whether it will be useful to them in their research. We have therefore devised a means for interactive browsing of the content and for viewing at different levels of granularity. This has resulted in a simple set of tools that can be added to any website to allow similar browsing of audio- video recordings and their related data and annotations.</abstract>
      <bibkey>campbell-tabata-2010-software</bibkey>
    </paper>
    <paper id="57">
      <author><first>Ana Cristina</first><last>Mendes</last></author>
      <author><first>Luísa</first><last>Coheur</last></author>
      <author><first>Paula Vaz</first><last>Lobo</last></author>
      <title>Named Entity Recognition in Questions: Towards a Golden Collection</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/97_Paper.pdf</url>
      <abstract>Named Entity Recognition (NER) plays a relevant role in several Natural Language Processing tasks. Question-Answering (QA) is an example of such, since answers are frequently named entities in agreement with the semantic category expected by a given question. In this context, the recognition of named entities is usually applied in free text data. NER in natural language questions can also aid QA and, thus, should not be disregarded. Nevertheless, it has not yet been given the necessary importance. In this paper, we approach the identification and classification of named entities in natural language questions. We hypothesize that NER results can benefit with the inclusion of previously labeled questions in the training corpus. We present a broad study addressing that hypothesis, focusing on the balance to be achieved between the amount of free text and questions in order to build a suitable training corpus. This work also contributes by providing a set of nearly 5,500 annotated questions with their named entities, freely available for research purposes.</abstract>
      <bibkey>mendes-etal-2010-named</bibkey>
    </paper>
    <paper id="58">
      <author><first>Patrizia</first><last>Paggio</last></author>
      <author><first>Jens</first><last>Allwood</last></author>
      <author><first>Elisabeth</first><last>Ahlsén</last></author>
      <author><first>Kristiina</first><last>Jokinen</last></author>
      <author><first>Costanza</first><last>Navarretta</last></author>
      <title>The <fixed-case>NOMCO</fixed-case> Multimodal <fixed-case>N</fixed-case>ordic Resource - Goals and Characteristics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/98_Paper.pdf</url>
      <abstract>This paper presents the multimodal corpora that are being collected and annotated in the Nordic NOMCO project. The corpora will be used to study communicative phenomena such as feedback, turn management and sequencing. They already include video material for Swedish, Danish, Finnish and Estonian, and several social activities are represented. The data will make it possible to verify empirically how gestures (head movements, facial displays, hand gestures and body postures) and speech interact in all the three mentioned aspects of communication. The data are being annotated following the MUMIN annotation scheme, which provides attributes concerning the shape and the communicative functions of head movements, face expressions, body posture and hand gestures. After having described the corpora, the paper discusses how they will be used to study the way feedback is expressed in speech and gestures, and reports results from two pilot studies where we investigated the function of head gestures ― both single and repeated ― in combination with feedback expressions. The annotated corpora will be valuable sources for research on intercultural communication as well as for interaction in the individual languages.</abstract>
      <bibkey>paggio-etal-2010-nomco</bibkey>
    </paper>
    <paper id="59">
      <author><first>Kikuo</first><last>Maekawa</last></author>
      <author><first>Makoto</first><last>Yamazaki</last></author>
      <author><first>Takehiko</first><last>Maruyama</last></author>
      <author><first>Masaya</first><last>Yamaguchi</last></author>
      <author><first>Hideki</first><last>Ogura</last></author>
      <author><first>Wakako</first><last>Kashino</last></author>
      <author><first>Toshinobu</first><last>Ogiso</last></author>
      <author><first>Hanae</first><last>Koiso</last></author>
      <author><first>Yasuharu</first><last>Den</last></author>
      <title>Design, Compilation, and Preliminary Analyses of <fixed-case>B</fixed-case>alanced <fixed-case>C</fixed-case>orpus of <fixed-case>C</fixed-case>ontemporary <fixed-case>W</fixed-case>ritten <fixed-case>J</fixed-case>apanese</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/99_Paper.pdf</url>
      <abstract>Compilation of a 100 million words balanced corpus called the Balanced Corpus of Contemporary Written Japanese (or BCCWJ) is underway at the National Institute for Japanese Language and Linguistics. The corpus covers a wide range of text genres including books, magazines, newspapers, governmental white papers, textbooks, minutes of the National Diet, internet text (bulletin board and blogs) and so forth, and when possible, samples are drawn from the rigidly defined statistical populations by means of random sampling. All texts are dually POS-analyzed based upon two different, but mutually related, definitions of word. Currently, more than 90 million words have been sampled and XML annotated with respect to text-structure and lexical and character information. A preliminary linear discriminant analysis of text genres using the data of POS frequencies and sentence length revealed it was possible to classify the text genres with a correct identification rate of 88% as far as the samples of books, newspapers, whitepapers, and internet bulletin boards are concerned. When the samples of blogs were included in this data set, however, the identification rate went down to 68%, suggesting the considerable variance of the blog texts in terms of the textual register and style.</abstract>
      <bibkey>maekawa-etal-2010-design</bibkey>
    </paper>
    <paper id="60">
      <author><first>Lieve</first><last>Macken</last></author>
      <title>An Annotation Scheme and Gold Standard for <fixed-case>D</fixed-case>utch-<fixed-case>E</fixed-case>nglish Word Alignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/100_Paper.pdf</url>
      <abstract>The importance of sentence-aligned parallel corpora has been widely acknowledged. Reference corpora in which sub-sentential translational correspondences are indicated manually are more labour-intensive to create, and hence less wide-spread. Such manually created reference alignments -- also called Gold Standards -- have been used in research projects to develop or test automatic word alignment systems. In most translations, translational correspondences are rather complex; for example word-by-word correspondences can be found only for a limited number of words. A reference corpus in which those complex translational correspondences are aligned manually is therefore also a useful resource for the development of translation tools and for translation studies. In this paper, we describe how we created a Gold Standard for the Dutch-English language pair. We present the annotation scheme, annotation guidelines, annotation tool and inter-annotator results. To cover a wide range of syntactic and stylistic phenomena that emerge from different writing and translation styles, our Gold Standard data set contains texts from different text types. The Gold Standard will be publicly available as part of the Dutch Parallel Corpus.</abstract>
      <bibkey>macken-2010-annotation</bibkey>
    </paper>
    <paper id="61">
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Ingo</first><last>Siegert</last></author>
      <author><first>Lutz</first><last>Bigalke</last></author>
      <author><first>Sascha</first><last>Meudt</last></author>
      <title>Developing an Expressive Speech Labeling Tool Incorporating the Temporal Characteristics of Emotion</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/101_Paper.pdf</url>
      <abstract>A lot of research effort has been spent on the development of emotion theories and modeling, however, their suitability and applicability to expressions in human computer interaction has not exhaustively been evaluated. Furthermore, investigations concerning the ability of the annotators to map certain expressions onto the developed emotion models is lacking proof. The proposed annotation tool, which incorporates the standard Geneva Emotional Wheel developed by Klaus Scherer and a novel temporal characteristic description feature, is aiming towards enabling the annotator to label expressions recorded in human computer interaction scenarios on an utterance level. Further, it is respecting key features of realistic and natural emotional expressions, such as their sequentiality, temporal characteristics, their mixed occurrences, and their expressivity or clarity of perception. Additionally, first steps towards evaluating the proposed tool, by analyzing utterance annotations taken from two expressive speech corpora, are undertaken and some future goals including the open source accessibility of the tool are given.</abstract>
      <bibkey>scherer-etal-2010-developing</bibkey>
    </paper>
    <paper id="62">
      <author><first>Ahmet</first><last>Aker</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Model Summaries for Location-related Images</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/102_Paper.pdf</url>
      <abstract>At present there is no publicly available data set to evaluate the performance of different summarization systems on the task of generating location-related extended image captions. In this paper we describe a corpus of human generated model captions in English and German. We have collected 932 model summaries in English from existing image descriptions and machine translated these summaries into German. We also performed post-editing on the translated German summaries to ensure high quality. Both English and German summaries are evaluated using a readability assessment as in DUC and TAC to assess their quality. Our model summaries performed similar to the ones reported in Dang (2005) and thus are suitable for evaluating automatic summarization systems on the task of generating image descriptions for location related images. In addition, we also investigated whether post-editing of machine-translated model summaries is necessary for automated ROUGE evaluations. We found a high correlation in ROUGE scores between post-edited and non-post-edited model summaries which indicates that the expensive process of post-editing is not necessary.</abstract>
      <bibkey>aker-gaizauskas-2010-model</bibkey>
    </paper>
    <paper id="63">
      <author><first>Justus</first><last>Roux</last></author>
      <author><first>Pieter</first><last>Scholtz</last></author>
      <author><first>Daleen</first><last>Klop</last></author>
      <author><first>Claus</first><last>Povlsen</last></author>
      <author><first>Bart</first><last>Jongejan</last></author>
      <author><first>Asta</first><last>Magnusdottir</last></author>
      <title>Incorporating Speech Synthesis in the Development of a Mobile Platform for e-learning.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/103_Paper.pdf</url>
      <abstract>This presentation and accompanying demonstration focuses on the development of a mobile platform for e-learning purposes with enhanced text-to-speech capabilities. It reports on an international consortium project entitled Mobile E-learning for Africa (MELFA), which includes a reading and literacy training component, particularly focusing on an African language, isiXhosa. The high penetration rate of mobile phones within the African continent has created new opportunities for delivering various kinds of information, including e-learning material to communities that have not had appropriate infrastructures. Aspects of the mobile platform development are described paying attention to basic functionalities of the user interface, as well as to the underlying web technologies involved. Some of the main features of the literacy training module are described, such as grapheme-sound correspondence, syllabification-sound relationships, varying tempo of presentation. A particular point is made for using HMM (HTS) synthesis in this case, as it seems to be very appropriate for less resourced languages.</abstract>
      <bibkey>roux-etal-2010-incorporating</bibkey>
    </paper>
    <paper id="64">
      <author><first>Bernard</first><last>Jacquemin</last></author>
      <title>A Derivational Rephrasing Experiment for Question Answering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/104_Paper.pdf</url>
      <abstract>In Knowledge Management, variations in information expressions have proven a real challenge. In particular, classical semantic relations (e.g. synonymy) do not connect words with different parts-of-speech. The method proposed tries to address this issue. It consists in building a derivational resource from a morphological derivation tool together with derivational guidelines from a dictionary in order to store only correct derivatives. This resource, combined with a syntactic parser, a semantic disambiguator and some derivational patterns, helps to reformulate an original sentence while keeping the initial meaning in a convincing manner This approach has been evaluated in three different ways: the precision of the derivatives produced from a lemma; its ability to provide well-formed reformulations from an original sentence, preserving the initial meaning; its impact on the results coping with a real issue, \textit{ie} a question answering task . The evaluation of this approach through a question answering system shows the pros and cons of this system, while foreshadowing some interesting future developments.</abstract>
      <bibkey>jacquemin-2010-derivational</bibkey>
    </paper>
    <paper id="65">
      <author><first>Sherri</first><last>Condon</last></author>
      <author><first>Dan</first><last>Parvaz</last></author>
      <author><first>John</first><last>Aberdeen</last></author>
      <author><first>Christy</first><last>Doran</last></author>
      <author><first>Andrew</first><last>Freeman</last></author>
      <author><first>Marwan</first><last>Awad</last></author>
      <title>Evaluation of Machine Translation Errors in <fixed-case>E</fixed-case>nglish and Iraqi <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/106_Paper.pdf</url>
      <abstract>Errors in machine translations of English-Iraqi Arabic dialogues were analyzed at two different points in the systems? development using HTER methods to identify errors and human annotations to refine TER annotations. The analyses were performed on approximately 100 translations into each language from 4 translation systems collected at two annual evaluations. Although the frequencies of errors in the more mature systems were lower, the proportions of error types exhibited little change. Results include high frequencies of pronoun errors in translations to English, high frequencies of subject person inflection in translations to Iraqi Arabic, similar frequencies of word order errors in both translation directions, and very low frequencies of polarity errors. The problems with many errors can be generalized as the need to insert lexemes not present in the source or vice versa, which includes errors in multi-word expressions. Discourse context will be required to resolve some problems with deictic elements like pronouns.</abstract>
      <bibkey>condon-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="66">
      <author><first>Mahdi</first><last>Mohseni</last></author>
      <author><first>Behrouz</first><last>Minaei-bidgoli</last></author>
      <title>A <fixed-case>P</fixed-case>ersian Part-Of-Speech Tagger Based on Morphological Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/107_Paper.pdf</url>
      <abstract>This paper describes a method based on morphological analysis of words for a Persian Part-Of-Speech (POS) tagging system. This is a main part of a process for expanding a large Persian corpus called Peyekare (or Textual Corpus of Persian Language). Peykare is arranged into two parts: annotated and unannotated parts. We use the annotated part in order to create an automatic morphological analyzer, a main segment of the system. Morphosyntactic features of Persian words cause two problems: the number of tags is increased in the corpus (586 tags) and the form of the words is changed. This high number of tags debilitates any taggers to work efficiently. From other side the change of word forms reduces the frequency of words with the same lemma; and the number of words belonging to a specific tag reduces as well. This problem also has a bad effect on statistical taggers. The morphological analyzer by removing the problems helps the tagger to cover a large number of tags in the corpus. Using a Markov tagger the method is evaluated on the corpus. The experiments show the efficiency of the method in Persian POS tagging.</abstract>
      <bibkey>mohseni-minaei-bidgoli-2010-persian</bibkey>
    </paper>
    <paper id="67">
      <author><first>Olga</first><last>Babko-Malaya</last></author>
      <author><first>Dan</first><last>Hunter</last></author>
      <author><first>Connie</first><last>Fournelle</last></author>
      <author><first>Jim</first><last>White</last></author>
      <title>Evaluation of Document Citations in Phase 2 Gale Distillation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/108_Paper.pdf</url>
      <abstract>The focus of information retrieval evaluations, such as NISTs TREC evaluations (e.g. Voorhees 2003), is on evaluation of the information content of system responses. On the other hand, retrieval tasks usually involve two different dimensions: reporting relevant information and providing sources of information, including corroborating evidence and alternative documents. Under the DARPA Global Autonomous Language Exploitation (GALE) program, Distillation provides succinct, direct responses to the formatted queries using the outputs of automated transcription and translation technologies. These responses are evaluated in two dimensions: information content, which measures the amount of relevant and non-redundant information, and document support, which measures the number of alternative sources provided in support of reported information. The final metric in the overall GALE distillation evaluation combines the results of scoring of both query responses and document citations. In this paper, we describe our evaluation framework with emphasis on the scoring of document citations and an analysis of how systems perform at providing sources of information.</abstract>
      <bibkey>babko-malaya-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="68">
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <title>A Freely Available Morphological Analyzer for <fixed-case>T</fixed-case>urkish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/109_Paper.pdf</url>
      <abstract>This paper presents TRmorph, a two-level morphological analyzer for Turkish. TRmorph is a fairly complete and accurate morphological analyzer for Turkish. However, strength of TRmorph is neither in its performance, nor in its novelty. The main feature of this analyzer is its availability. It has completely been implemented using freely available tools and resources, and the two-level description is also distributed with a license that allows others to use and modify it freely for different applications. To our knowledge, TRmorph is the first freely available morphological analyzer for Turkish. This makes TRmorph particularly suitable for applications where the analyzer has to be changed in some way, or as a starting point for morphological analyzers for similar languages. TRmorph's specification of Turkish morphology is relatively complete, and it is distributed with a large lexicon. Along with the description of how the analyzer is implemented, this paper provides an evaluation of the analyzer on two large corpora.</abstract>
      <bibkey>coltekin-2010-freely</bibkey>
    </paper>
    <paper id="69">
      <author><first>Martin</first><last>Volk</last></author>
      <author><first>Noah</first><last>Bubenhofer</last></author>
      <author><first>Adrian</first><last>Althaus</last></author>
      <author><first>Maya</first><last>Bangerter</last></author>
      <author><first>Lenz</first><last>Furrer</last></author>
      <author><first>Beni</first><last>Ruef</last></author>
      <title>Challenges in Building a Multilingual Alpine Heritage Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/110_Paper.pdf</url>
      <abstract>This paper describes our efforts to build a multilingual heritage corpus of alpine texts. Currently we digitize the yearbooks of the Swiss Alpine Club which contain articles in French, German, Italian and Romansch. Articles comprise mountaineering reports from all corners of the earth, but also scientific topics such as topography, geology or glacierology as well as occasional poetry and lyrics. We have already scanned close to 70,000 pages which has resulted in a corpus of 25 million words, 10% of which is a parallel French-German corpus. We have solved a number of challenges in automatic language identification and text structure recognition. Our next goal is to identify the great variety of toponyms (e.g. names of mountains and valleys, glaciers and rivers, trails and cabins) in this corpus, and we sketch how a large gazetteer of Swiss topographical names can be exploited for this purpose. Despite the size of the resource, exact matching leads to a low recall because of spelling variations, language mixtures and partial repetitions.</abstract>
      <bibkey>volk-etal-2010-challenges</bibkey>
    </paper>
    <paper id="70">
      <author><first>Silvia</first><last>Pareti</last></author>
      <author><first>Irina</first><last>Prodanof</last></author>
      <title>Annotating Attribution Relations: Towards an <fixed-case>I</fixed-case>talian Discourse Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/111_Paper.pdf</url>
      <abstract>In this paper we describe the development of a schema for the annotation of attribution relations and present the first findings and some relevant issues concerning this phenomenon. Following the D-LTAG approach to discourse, we have developed a lexically anchored description of attribution, considering this relation, contrary to the approach in the PDTB, independently from other discourse relations. This approach has allowed us to deal with the phenomenon in a broader perspective than previous studies, reaching therefore a more accurate description of it and making it possible to raise some still unaddressed issues. Following this analysis, we propose an annotation schema and discuss the first results concerning its applicability. The schema has been applied to a pilot portion of the ISST corpus of Italian and represents the initial phase of a project aiming at the creation of an Italian Discourse Treebank. We believe this work will raise some awareness concerning the fundamental importance of attribution relations. The identification of the source has in fact strong implications for the attributed material. Moreover, it will make overt the complexity of a phenomenon for long underestimated.</abstract>
      <bibkey>pareti-prodanof-2010-annotating</bibkey>
    </paper>
    <paper id="71">
      <author><first>Nick</first><last>Webb</last></author>
      <author><first>David</first><last>Benyon</last></author>
      <author><first>Preben</first><last>Hansen</last></author>
      <author><first>Oil</first><last>Mival</last></author>
      <title>Evaluating Human-Machine Conversation for Appropriateness</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/115_Paper.pdf</url>
      <abstract>Evaluation of complex, collaborative dialogue systems is a difficult task. Traditionally, developers have relied upon subjective feedback from the user, and parametrisation over observable metrics. However, both models place some reliance on the notion of a task; that is, the system is helping to user achieve some clearly defined goal, such as book a flight or complete a banking transaction. It is not clear that such metrics are as useful when dealing with a system that has a more complex task, or even no definable task at all, beyond maintain and performing a collaborative dialogue. Working within the EU funded COMPANIONS program, we investigate the use of appropriateness as a measure of conversation quality, the hypothesis being that good companions need to be good conversational partners . We report initial work in the direction of annotating dialogue for indicators of good conversation, including the annotation and comparison of the output of two generations of the same dialogue system.</abstract>
      <bibkey>webb-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="72">
      <author><first>Beáta</first><last>Megyesi</last></author>
      <author><first>Bengt</first><last>Dahlqvist</last></author>
      <author><first>Éva Á.</first><last>Csató</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <title>The <fixed-case>E</fixed-case>nglish-<fixed-case>S</fixed-case>wedish-<fixed-case>T</fixed-case>urkish Parallel Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/116_Paper.pdf</url>
      <abstract>We describe a syntactically annotated parallel corpus containing typologically partly different languages, namely English, Swedish and Turkish. The corpus consists of approximately 300 000 tokens in Swedish, 160 000 in Turkish and 150 000 in English, containing both fiction and technical documents. We build the corpus by using the Uplug toolkit for automatic structural markup, such as tokenization and sentence segmentation, as well as sentence and word alignment. In addition, we use basic language resource kits for the linguistic analysis of the languages involved. The annotation is carried on various layers from morphological and part of speech analysis to dependency structures. The tools used for linguistic annotation, e.g.,\ HunPos tagger and MaltParser, are freely available data-driven resources, trained on existing corpora and treebanks for each language. The parallel treebank is used in teaching and linguistic research to study the relationship between the structurally different languages. In order to study the treebank, several tools have been developed for the visualization of the annotation and alignment, allowing search for linguistic patterns.</abstract>
      <bibkey>megyesi-etal-2010-english</bibkey>
    </paper>
    <paper id="73">
      <author><first>Pradeep</first><last>Dantuluri</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <title>A Use Case for Controlled Languages as Interfaces to Semantic Web Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/117_Paper.pdf</url>
      <abstract>Although the Semantic web is steadily gaining in popularity, it remains a mystery to a large percentage of Internet users. This can be attributed to the complexity of the technologies that form its core. Creating intuitive interfaces which completely abstract the technologies underneath, is one way to solve this problem. A contrasting approach is to ease the user into understanding the technologies. We propose a solution which anchors on using controlled languages as interfaces to semantic web applications. This paper describes one such approach for the domain of meeting minutes, status reports and other project specific documents. A controlled language is developed along with an ontology to handle semi-automatic knowledge extraction. The contributions of this paper include an ontology designed for the domain of meeting minutes and status reports, and a controlled language grammar tailored for the above domain to perform the semi-automatic knowledge acquisition and generate RDF triples. This paper also describes two grammar prototypes, which were developed and evaluated prior to the development of the final grammar, as well as the Link grammar, which was the grammar formalism of choice.</abstract>
      <bibkey>dantuluri-etal-2010-use</bibkey>
    </paper>
    <paper id="74">
      <author><first>Charles</first><last>Teissèdre</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <title>Resources for Calendar Expressions Semantic Tagging and Temporal Navigation through Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/118_Paper.pdf</url>
      <abstract>The linguistic resources presented in this paper are designed for the recognition and semantic tagging of calendar expressions in French. While existing resources generally put the emphasis on describing calendar bases pointed out by calendar expressions (which are considered as named entities), our approach tries to explicit how references to calendar are linguistically built up, taking into account not only the calendar bases but as well the prepositions and units that operate on them, as they provide valuable information on how texts refer to the calendar. The modelling of these expressions led us to consider calendar expressions as a conjunction of operators interacting with temporal references. Though the resources aim to be generic and easily reusable, we illustrate the interest of our approach by using the resources output to feed a text navigation tool that is currently being improved, in order to offer users a way of temporally progressing or navigating in texts.</abstract>
      <bibkey>teissedre-etal-2010-resources</bibkey>
    </paper>
    <paper id="75">
      <author><first>Oscar</first><last>Saz</last></author>
      <author><first>Eduardo</first><last>Lleida</last></author>
      <author><first>Carlos</first><last>Vaquero</last></author>
      <author><first>W.-Ricardo</first><last>Rodríguez</last></author>
      <title>The Alborada-<fixed-case>I</fixed-case>3<fixed-case>A</fixed-case> Corpus of Disordered Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/119_Paper.pdf</url>
      <abstract>This paper describes the Alborada-I3A corpus of disordered speech, acquired during the recent years for the research in different speech technologies for the handicapped like Automatic Speech Recognition or pronunciation assessment. It contains more than 2 hours of speech from 14 young impaired speakers and nearly 9 hours from 232 unimpaired age-matched peers whose collaboration was possible by the joint work with different educational and assistive institutions. Furthermore, some extra resources are provided with the corpus, including the results of a perceptual human-based labeling of the lexical mispronunciations made by the impaired speakers. The corpus has been used to achieve results in different tasks like analyses on the speech production in impaired children, acoustic and lexical adaptation for ASR and studies on the speech proficiency of the impaired speakers. Finally, the full corpus is freely available for the research community with the only restrictions of maintaining all its data and resources for research purposes only and keeping the privacy of the speakers and their speech data</abstract>
      <bibkey>saz-etal-2010-alborada</bibkey>
    </paper>
    <paper id="76">
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <author><first>John</first><last>McNaught</last></author>
      <author><first>James</first><last>Thomas</last></author>
      <author><first>Mark</first><last>Rickinson</last></author>
      <author><first>Sandy</first><last>Oliver</last></author>
      <title>Evaluating a Text Mining Based Educational Search Portal</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/121_Paper.pdf</url>
      <abstract>In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user-centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features.</abstract>
      <bibkey>ananiadou-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="77">
      <author><first>Jennifer</first><last>Pedler</last></author>
      <author><first>Roger</first><last>Mitton</last></author>
      <title>A Large List of Confusion Sets for Spellchecking Assessed Against a Corpus of Real-word Errors</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/122_Paper.pdf</url>
      <abstract>One of the methods that has been proposed for dealing with real-word errors (errors that occur when a correctly spelled word is substituted for the one intended) is the ""confusion-set"" approach - a confusion set being a small group of words that are likely to be confused with one another. Using a list of confusion sets drawn up in advance, a spellchecker, on finding one of these words in a text, can assess whether one of the other members of its set would be a better fit and, if it appears to be so, propose that word as a correction. Much of the research using this approach has suffered from two weaknesses. The first is the small number of confusion sets used. The second is that systems have largely been tested on artificial errors. In this paper we address these two weaknesses. We describe the creation of a realistically sized list of confusion sets, then the assembling of a corpus of real-word errors, and then we assess the potential of that list in relation to that corpus.</abstract>
      <bibkey>pedler-mitton-2010-large</bibkey>
    </paper>
    <paper id="78">
      <author><first>Alexander</first><last>Schmitt</last></author>
      <author><first>Gregor</first><last>Bertrand</last></author>
      <author><first>Tobias</first><last>Heinroth</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Jackson</first><last>Liscombe</last></author>
      <title><fixed-case>WIT</fixed-case>c<fixed-case>HCR</fixed-case>af<fixed-case>T</fixed-case>: A Workbench for Intelligent explora<fixed-case>T</fixed-case>ion of Human <fixed-case>C</fixed-case>ompute<fixed-case>R</fixed-case> conversa<fixed-case>T</fixed-case>ions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/123_Paper.pdf</url>
      <abstract>We present Witchcraft, an open-source framework for the evaluation of prediction models for spoken dialogue systems based on interaction logs and audio recordings. The use of Witchcraft is two fold: first, it provides an adaptable user interface to easily manage and browse thousands of logged dialogues (e.g. calls). Second, with help of the underlying models and the connected machine learning framework RapidMiner the workbench is able to display at each dialogue turn the probability of the task being completed based on the dialogue history. It estimates the emotional state, gender and age of the user. While browsing through a logged conversation, the user can directly observe the prediction result of the models at each dialogue step. By that, Witchcraft allows for spotting problematic dialogue situations and demonstrates where the current system and the prediction models have design flaws. Witchcraft will be made publically available to the community and will be deployed as open-source project.</abstract>
      <bibkey>schmitt-etal-2010-witchcraft</bibkey>
    </paper>
    <paper id="79">
      <author><first>Svetlana</first><last>Stoyanchev</last></author>
      <author><first>Paul</first><last>Piwek</last></author>
      <title>Constructing the <fixed-case>CODA</fixed-case> Corpus: A Parallel Corpus of Monologues and Expository Dialogues</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/127_Paper.pdf</url>
      <abstract>We describe the construction of the CODA corpus, a parallel corpus of monologues and expository dialogues. The dialogue part of the corpus consists of expository, i.e., information-delivering rather than dramatic, dialogues written by several acclaimed authors. The monologue part of the corpus is a paraphrase in monologue form of these dialogues by a human annotator. The annotator-written monologue preserves all information present in the original dialogue and does not introduce any new information that is not present in the original dialogue. The corpus was constructed as a resource for extracting rules for automated generation of dialogue from monologue. Using authored dialogues allows us to analyse the techniques used by accomplished writers for presenting information in the form of dialogue. The dialogues are annotated with dialogue acts and the monologues with rhetorical structure. We developed annotation and translation guidelines together with a custom-developed tool for carrying out translation, alignment and annotation of the dialogues. The final parallel CODA corpus consists of 1000 dialogue turns that are tagged with dialogue acts and aligned with monologue that expresses the same information and has been annotated with rhetorical structure relations.</abstract>
      <bibkey>stoyanchev-piwek-2010-constructing</bibkey>
    </paper>
    <paper id="80">
      <author><first>Dain</first><last>Kaplan</last></author>
      <author><first>Ryu</first><last>Iida</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <title>Annotation Process Management Revisited</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/129_Paper.pdf</url>
      <abstract>Proper annotation process management is crucial to the construction of corpora, which are in turn indispensable to the data-driven techniques that have come to the forefront in NLP during the last two decades. It is still common to see ad-hoc tools created for a specific annotation project, but it is time this changed; creation of such tools is labor and time expensive, and is secondary to corpus creation. In addition, such tools likely lack proper annotation process management, increasingly more important as corpora sizes grow in size and complexity. This paper first raises a list of ten needs that any general purpose annotation system should address moving forward, such as user &amp; role management, delegation &amp; monitoring of work, diffing &amp; merging annotators work, versioning of corpora, multilingual support, import/export format flexibility, and so on. A framework to address these needs is then proposed, and how having proper annotation process management can be beneficial to the creation and maintenance of corpora explained. The paper then introduces SLATE (Segment and Link-based Annotation Tool Enhanced), the second iteration of a web-based annotation tool, which is being rewritten to implement the proposed framework.</abstract>
      <bibkey>kaplan-etal-2010-annotation</bibkey>
    </paper>
    <paper id="81">
      <author><first>Giulio</first><last>Paci</last></author>
      <author><first>Giorgio</first><last>Pedrazzi</last></author>
      <author><first>Roberta</first><last>Turra</last></author>
      <title><fixed-case>W</fixed-case>ikipedia-based Approach for Linking Ontology Concepts to their Realisations in Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/132_Paper.pdf</url>
      <abstract>A novel method to automatically associate ontological concepts to their realisations in texts is presented. The method has been developed in the context of the Papyrus project to annotate texts and audio transcripts with a set of relevant concepts from the Papyrus News Ontology. To avoid strong dependency on a specific ontology, the annotation process starts by performing a Wikipedia-based annotation of news items: the most relevant keywords are detected and the Wikipedia pages that best describe their actual meaning are identified. In a later step this annotation is translated into an Ontology-based one: keywords are connected to the most appropriate ontology classes on the basis of a relatedness measure that relies on Wikipedia knowledge. Wikipedia-annotation provides a domain independent abstraction layer that simplify the adaptation of the approach to other domains and ontologies. Evaluation has been performed on a set of manually annotated news, resulting in 58% F1 score for relevant Wikipedia pages and 64% for relevant ontology concepts identification.</abstract>
      <bibkey>paci-etal-2010-wikipedia</bibkey>
    </paper>
    <paper id="82">
      <author><first>Marianne</first><last>Laurent</last></author>
      <author><first>Philippe</first><last>Bretier</last></author>
      <author><first>Carole</first><last>Manquillet</last></author>
      <title>Ad-hoc Evaluations Along the Lifecycle of Industrial Spoken Dialogue Systems: Heading to Harmonisation?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/133_Paper.pdf</url>
      <abstract>With a view to rationalise the evaluation process within the Orange Labs spoken dialogue system projects, a field audit has been realised among the various related professionals. The article presents the study's main conclusions and draws work perspectives to enhance the evaluation process in such a complex organisation. We first present the typical spoken dialogue system project lifecycle and the involved communities of stakeholders. We then sketch a map of indicators used across the teams. It shows that each professional category designs its evaluation metrics according to a case-by-case strategy, each one targeting different goals and methodologies. And last, we identify weaknesses in the evaluation process is handled by the various teams. Among others, we mention: the dependency on the design and exploitation tools that may not be suitable for an adequate collection of relevant indicators, the need to refine some indicators' definition and analysis to obtain valuable information for system enhancement, the sharing issue that advocates for a common definition of indicators across the teams and, as a consequence, the need for shared applications that support and encourage such a rationalisation.</abstract>
      <bibkey>laurent-etal-2010-ad</bibkey>
    </paper>
    <paper id="83">
      <author><first>Masahiro</first><last>Nakano</last></author>
      <author><first>Hideyuki</first><last>Shibuki</last></author>
      <author><first>Rintaro</first><last>Miyazaki</last></author>
      <author><first>Madoka</first><last>Ishioroshi</last></author>
      <author><first>Koichi</first><last>Kaneko</last></author>
      <author><first>Tatsunori</first><last>Mori</last></author>
      <title>Construction of Text Summarization Corpus for the Credibility of Information on the Web</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/135_Paper.pdf</url>
      <abstract>Recently, the credibility of information on the Web has become an important issue. In addition to telling about content of source documents, indicating how to interpret the content, especially showing interpretation of the relation between statements appeared to contradict each other, is important for helping a user judge the credibility of information. In this paper, we will describe the purpose and the way in the construction of a text summarization corpus. Our purpose in the construction of the corpus includes the following three points; to collect Web documents relevant to several query sentences, to prepare gold standard data to evaluate smaller sub-processes in the extraction process and the summary generation process, to investigate the summaries made by human summarizers. The constructed corpus contains six query sentences, 24 manually-constructed summaries, and 24 collections of source Web documents. We also investigated how the descriptions of interpretation, which help a user judge the credibility of other descriptions in the summary, appear in the corpus. As a result, we confirmed that showing interpretation on conflicts is important for helping a user judge the credibility of information.</abstract>
      <bibkey>nakano-etal-2010-construction</bibkey>
    </paper>
    <paper id="84">
      <author><first>João</first><last>Silva</last></author>
      <author><first>António</first><last>Branco</last></author>
      <author><first>Patricia</first><last>Gonçalves</last></author>
      <title>Top-Performing Robust Constituency Parsing of <fixed-case>P</fixed-case>ortuguese: Freely Available in as Many Ways as you Can Get it</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/136_Paper.pdf</url>
      <abstract>In this paper we present LX-Parser, a probabilistic, robust constituency parser for Portuguese. This parser achieves ca. 88% f-score in the labeled bracketing task, thus reaching a state-of-the-art performance score that is in line with those that are currently obtained by top-ranking parsers for English, the most studied natural language. To the best of our knowledge, LX-Parser is the first state-of-the-art, robust constituency parser for Portuguese that is made freely available. This parser is being distributed in a variety of ways, each suited for a different type of usage. More specifically, LX-Parser is being made available (i) as a downloadable, stand-alone parsing tool that can be run locally by its users; (ii) as a Web service that exposes an interface that can be invoked remotely and transparently by client applications; and finally (iii) as an on-line parsing service, aimed at human users, that can be accessed through any common Web browser.</abstract>
      <bibkey>silva-etal-2010-top</bibkey>
    </paper>
    <paper id="85">
      <author><first>Sylviane</first><last>Cardey</last></author>
      <author><first>Krzysztof</first><last>Bogacki</last></author>
      <author><first>Xavier</first><last>Blanco</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <title>Resources for Controlled Languages for Alert Messages and Protocols in the <fixed-case>E</fixed-case>uropean Perspective</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/137_Paper.pdf</url>
      <abstract>This paper is concerned with resources for controlled languages for alert messages and protocols in the European perspective. These resources have been produced as the outcome of a project (Alert Messages and Protocols: MESSAGE) which has been funded with the support of the European Commission - Directorate-General Justice, Freedom and Security, and with the specific objective of 'promoting and supporting the development of security standards, and an exchange of know-how and experience on protection of people'. The MESSAGE project involved the development and transfer of a methodology for writing safe and safely translatable alert messages and protocols created by Centre Tesnière in collaboration with the aircraft industry, the health profession, and emergency services by means of a consortium of four partners to their four European member states in their languages (ES, FR (Coordinator), GB, PL). The paper describes alert messages and protocols, controlled languages for safety and security, the target groups involved, controlled language evaluation, dissemination, the resources that are available, both Freely available and From Owner, together with illustrations of the resources, and the potential transferability to other sectors and users.</abstract>
      <bibkey>cardey-etal-2010-resources</bibkey>
    </paper>
    <paper id="86">
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <title><fixed-case>MULTEXT</fixed-case>-East Version 4: Multilingual Morphosyntactic Specifications, Lexicons and Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/138_Paper.pdf</url>
      <abstract>The paper presents the fourth, ``Mondilex'' edition of the MULTEXT-East language resources, a multilingual dataset for language engineering research and development, focused on the morphosyntactic level of linguistic description. This standardised and linked set of resources covers a large number of mainly Central and Eastern European languages and includes the EAGLES-based morphosyntactic specifications; morphosyntactic lexica; and annotated parallel, comparable, and speech corpora. The fourth release of these resources introduces XML-encoded morphosyntactic specifications and adds six new languages, bringing the total to 16: to Bulgarian, Croatian, Czech, Estonian, English, Hungarian, Romanian, Serbian, Slovene, and the Resian dialect of Slovene it adds Macedonian, Persian, Polish, Russian, Slovak, and Ukrainian. This dataset, unique in terms of languages covered and the wealth of encoding, is extensively documented, and freely available for research purposes at http://nl.ijs.si/ME/V4/.</abstract>
      <bibkey>erjavec-2010-multext</bibkey>
    </paper>
    <paper id="87">
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Darja</first><last>Fišer</last></author>
      <author><first>Simon</first><last>Krek</last></author>
      <author><first>Nina</first><last>Ledinek</last></author>
      <title>The <fixed-case>JOS</fixed-case> Linguistically Tagged Corpus of <fixed-case>S</fixed-case>lovene</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/139_Paper.pdf</url>
      <abstract>The JOS language resources are meant to facilitate developments of HLT and corpus linguistics for the Slovene language and consist of the morphosyntactic specifications, defining the Slovene morphosyntactic features and tagset; two annotated corpora (jos100k and jos1M); and two web services (a concordancer and text annotation tool). The paper introduces these components, and concentrates on jos100k, a 100,000 word sampled balanced monolingual Slovene corpus, manually annotated for three levels of linguistic description. On the morphosyntactic level, each word is annotated with its morphosyntactic description and lemma; on the syntactic level the sentences are annotated with dependency links; on the semantic level, all the occurrences of 100 top nouns in the corpus are annotated with their wordnet synset from the Slovene semantic lexicon sloWNet. The JOS corpora and specifications have a standardised encoding (Text Encoding Initiative Guidelines TEI P5) and are available for research from http://nl.ijs.si/jos/ under the Creative Commons licence.</abstract>
      <bibkey>erjavec-etal-2010-jos</bibkey>
    </paper>
    <paper id="88">
      <author><first>Livio</first><last>Robaldo</last></author>
      <author><first>Eleni</first><last>Miltsakaki</last></author>
      <author><first>Alessia</first><last>Bianchini</last></author>
      <title>Corpus-based Semantics of Concession: Where do Expectations Come from?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/140_Paper.pdf</url>
      <abstract>In this paper, we discuss our analysis and resulting new annotations of Penn Discourse Treebank (PDTB) data tagged as Concession. Concession arises whenever one of the two arguments creates an expectation, and the other ones denies it. In Natural Languages, typical discourse connectives conveying Concession are 'but', 'although', 'nevertheless', etc. Extending previous theoretical accounts, our corpus analysis reveals that concessive interpretations are due to different sources of expectation, each giving rise to critical inferences about the relationship of the involved eventualities. We identify four different sources of expectation: Causality, Implication, Correlation, and Implicature. The reliability of these categories is supported by a high inter-annotator agreement score, computed over a sample of one thousand tokens of explicit connectives annotated as Concession in PDTB. Following earlier work of (Hobbs, 1998) and (Davidson, 1967) notion of reification, we extend the logical account of Concession originally proposed in (Robaldo et al., 2008) to provide refined formal descriptions for the first three mentioned sources of expectations in Concessive relations.</abstract>
      <bibkey>robaldo-etal-2010-corpus</bibkey>
    </paper>
    <paper id="89">
      <author><first>Darja</first><last>Fišer</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Špela</first><last>Vintar</last></author>
      <title>Learning to Mine Definitions from <fixed-case>S</fixed-case>lovene Structured and Unstructured Knowledge-Rich Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/141_Paper.pdf</url>
      <abstract>The paper presents an innovative approach to extract Slovene definition candidates from domain-specific corpora using morphosyntactic patterns, automatic terminology recognition and semantic tagging with wordnet senses. First, a classification model was trained on examples from Slovene Wikipedia which was then used to find well-formed definitions among the extracted candidates. The results of the experiment are encouraging, with accuracy ranging from 67% to 71%. The paper also addresses some drawbacks of the approach and suggests ways to overcome them in future work.</abstract>
      <bibkey>fiser-etal-2010-learning</bibkey>
    </paper>
    <paper id="90">
      <author><first>Dan</first><last>Tufiş</last></author>
      <author><first>Dan</first><last>Ştefănescu</last></author>
      <title>A Differential Semantics Approach to the Annotation of Synsets in <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/142_Paper.pdf</url>
      <abstract>We describe a new method for sentiment load annotation of the synsets of a wordnet, along the principles of Osgoods Semantic Differential theory and extending the Kamp and Marx calculus, by taking into account not only the WordNet structure but also the SUMO/MILO (Niles &amp; Pease, 2001) and DOMAINS (Bentivogli et al., 2004) knowledge sources. We discuss the method to annotate all the synsets in PWN2.0, irrespective of their part of speech. As the number of possible factors (semantic oppositions, along which the synsets are ranked) is very large, we developed also an application allowing the text analyst to select the most discriminating factors for the type of text to be analyzed. Once the factors have been selected, the underlying wordnet is marked-up on the fly and it can be used for the intended textual analysis. We anticipate that these annotations can be imported in other language wordnets, provided they are aligned to PWN2.0. The method for the synsets annotation generalizes the usual subjectivity mark-up (positive, negative and objective) according to a user-based multi-criteria differential semantics model.</abstract>
      <bibkey>tufis-stefanescu-2010-differential</bibkey>
    </paper>
    <paper id="91">
      <author><first>Elena</first><last>Grishina</last></author>
      <title>Multimodal <fixed-case>R</fixed-case>ussian Corpus (<fixed-case>MURCO</fixed-case>): First Steps</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/143_Paper.pdf</url>
      <abstract>The paper introduces the Multimodal Russian Corpus (MURCO), which has been created in the framework of the Russian National Corpus (RNC). The MURCO provides the users with the great amount of phonetic, orthoepic, intonational information related to Russian. Moreover, the deeply annotated part of the MURCO contains the data concerning Russian gesticulation, speech act system, types of vocal gestures and interjections in Russian, and so on. The Corpus is on free access. The paper describes the main types of annotation and the interface structure of the MURCO. The MURCO consists of two parts, the second part being the subset of the first: 1) the whole Corpus, which is annotated from the lexical (lemmatization), morphological, semantic, accentological, metatextual, socioligical point of view (these types of annotation are standard for the RNC), and also from the point of view of phonetics (the orthoepic annotation and the mark-up of accentological word structure), 2) the deeply annotated MURCO, which is annotated in addition from the point of view of gesticulation and speech act structure.</abstract>
      <bibkey>grishina-2010-multimodal</bibkey>
    </paper>
    <paper id="92">
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <title>Lingua-Align: An Experimental Toolbox for Automatic Tree-to-Tree Alignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/144_Paper.pdf</url>
      <abstract>In this paper we present an experimental toolbox for automatic tree-to-tree alignment based on a binary classification model. The aligner implements a recurrent architecture for structural prediction using history features and a sequential classification procedure. The discriminative base classifier uses a log-linear model in the current setup which enables simple integration of various features extracted from the data. The Lingua-Align toolbox provides a flexible framework for feature extraction including contextual properties and implements several alignment inference procedures. Various settings and constraints can be controlled via a simple frontend or called from external scripts. Lingua-Align supports different treebank formats and includes additional tools for conversion and evaluation. In our experiments we can show that our tree aligner produces results with high quality and outperforms unsupervised techniques proposed otherwise. It also integrates well with another existing tool for manual tree alignment which makes it possible to quickly integrate additional training material and to run semi-automatic alignment strategies.</abstract>
      <bibkey>tiedemann-2010-lingua</bibkey>
    </paper>
    <paper id="93">
      <author><first>Cécile</first><last>Grivaz</last></author>
      <title>Human Judgements on Causation in <fixed-case>F</fixed-case>rench Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/145_Paper.pdf</url>
      <abstract>The annotation of causal relations in natural language texts can lead to a low inter-annotator agreement. A French corpus annotated with causal relations would be helpful for the evaluation of programs that extract causal knowledge, as well as for the study of the expression of causation. As previous theoretical work provides no necessary and sufficient condition that would allow an annotator to easily identify causation, we explore features that are associated with causation in human judgements. We present an experiment that allows us to elicit intuitive features of causation. We test the statistical association of features of causation from theoretical previous work with causation itself in human judgements in an annotation experiment. We then establish guidelines based on these features for annotating a French corpus. We argue that our approach leads to coherent annotation guidelines, since it allows us to obtain a κ = 0.84 agreement between the majority of the annotators answers and our own educated judgements. We present these annotation instructions in detail.</abstract>
      <bibkey>grivaz-2010-human</bibkey>
    </paper>
    <paper id="94">
      <author><first>Rita</first><last>Marinelli</last></author>
      <author><first>Adriana</first><last>Roventini</last></author>
      <author><first>Giovanni</first><last>Spadoni</last></author>
      <author><first>Sebastiana</first><last>Cucurullo</last></author>
      <title>Lexical Semantic Resources in a Terminological Network</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/147_Paper.pdf</url>
      <abstract>A research has been carried on and is still in progress aimed at the construction of three specialized lexicons organized as databases of relational type. The three databases contain terms belonging to the specialized knowledge fields of maritime terminology (technical-nautical and maritime transport domain), taxation law, and labour law with union labour rules, respectively. The EuroWordNet/ItalWordNet model was firstly used to structure the terminological database of maritime domain. The methodology experimented for its construction was applied to construct the next databases. It consists in i) the management of corpora of specialized languages and ii) the use of generic databases to identify and extract a set of candidate terms to be codified in the terminological databases. The three specialized resources are described highlighting the various kinds of lexical semantic relations linking each term to the others within the single terminological database and to the generic resources WordNet and ItalWordNet. The construction of these specialized lexicons was carried on in the framework of different projects; but they can be seen as a first nucleus of an organized network of generic and specialized lexicons with the purpose of making the meaning of each term clearer from a cognitive point of view.</abstract>
      <bibkey>marinelli-etal-2010-lexical</bibkey>
    </paper>
    <paper id="95">
      <author><first>Aleksander</first><last>Wawer</last></author>
      <title>Is Sentiment a Property of Synsets? Evaluating Resources for Sentiment Classification using Machine Learning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/149_Paper.pdf</url>
      <abstract>Existing approaches to classifying documents by sentiment include machine learning with features created from n-grams and part of speech. This paper explores a different approach and examines performance of one selected machine learning algorithm, Support Vector Machines, with features computed using existing lexical resources. Special attention has been paid to fine tuning of the algorithm regarding number of features. The immediate purpose of this experiment is to evaluate lexical and sentiment resources in document-level sentiment classification task. Results described in the paper are also useful to indicate how lexicon design, different language dimensions and semantic categories contribute to document-level sentiment recognition. In a less direct way (through the examination of evaluated resources), the experiment analyzes adequacy of lexemes, word senses and synsets as different possible layers for ascribing sentiment, or as candidates for sentiment carriers. The proposed approach of machine learning word category frequencies instead of n-grams and part of speech features can potentially exhibit improvements in domain independency, but this hypothesis has to be verified in future works.</abstract>
      <bibkey>wawer-2010-sentiment</bibkey>
    </paper>
    <paper id="96">
      <author><first>Iñaki</first><last>Alegria</last></author>
      <author><first>Garbiñe</first><last>Aranbarri</last></author>
      <author><first>Klara</first><last>Ceberio</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Bittor</first><last>Laskurain</last></author>
      <author><first>Ruben</first><last>Urizar</last></author>
      <title>A Morphological Processor Based on <fixed-case>F</fixed-case>oma for <fixed-case>B</fixed-case>iscayan (a <fixed-case>B</fixed-case>asque dialect)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/150_Paper.pdf</url>
      <abstract>We present a new morphological processor for Biscayan, a dialect of Basque, developed on the description of the morphology of standard Basque. The database for the standard morphology has been extended for dialects and an open-source tool for morphological description named foma is used for building the processor. Biscayan is a dialect of the Basque language spoken mainly in Biscay, a province on the western of the Basque Country. The description of the lexicon and the morphotactics (or word grammar) for the standard Basque was carried out using a relational database and the database has been extended in order to include dialectal variants linked to the standard entries. XuxenB, a spelling checker/corrector for this dialect, is the first application of this work. Additionally to the basic analyzer used for spelling, a new transducer is included. It is an enhanced analyzer for linking standard form with the corresponding standard ones. It is used in correction for generation of proposals when in the input text appear standard forms which we want to replace with dialectal forms.</abstract>
      <bibkey>alegria-etal-2010-morphological</bibkey>
    </paper>
    <paper id="97">
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <author><first>Rafał L.</first><last>Górski</last></author>
      <author><first>Marek</first><last>Łaziński</last></author>
      <author><first>Piotr</first><last>Pęzik</last></author>
      <title>Recent Developments in the <fixed-case>N</fixed-case>ational <fixed-case>C</fixed-case>orpus of <fixed-case>P</fixed-case>olish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/152_Paper.pdf</url>
      <abstract>The aim of the paper is to present recent ― as of March 2010 ― developments in the construction of the National Corpus of Polish (NKJP). The NKJP project was launched at the very end of 2007 and it is aimed at compiling a large, linguistically annotated corpus of contemporary Polish by the end of 2010. Out of the total pool of 1 billion words of text data collected in the project, a 300 million word balanced corpus will be selected to match a set of predefined representativeness criteria. This present paper outlines a number of recent developments in the NKJP project, including: 1) the design of text encoding XML schemata for various levels of linguistic information, 2) a new tool for manual annotation at various levels, 3) numerous improvements in search tools. As the work on NKJP progresses, it becomes clear that this project serves as an important testbed for linguistic annotation and interoperability standards. We believe that our recent experiences will prove relevant to future large-scale language resource compilation efforts.</abstract>
      <bibkey>przepiorkowski-etal-2010-recent</bibkey>
    </paper>
    <paper id="98">
      <author><first>António</first><last>Branco</last></author>
      <author><first>Francisco</first><last>Costa</last></author>
      <author><first>João</first><last>Silva</last></author>
      <author><first>Sara</first><last>Silveira</last></author>
      <author><first>Sérgio</first><last>Castro</last></author>
      <author><first>Mariana</first><last>Avelãs</last></author>
      <author><first>Clara</first><last>Pinto</last></author>
      <author><first>João</first><last>Graça</last></author>
      <title>Developing a Deep Linguistic Databank Supporting a Collection of Treebanks: the <fixed-case>CINTIL</fixed-case> <fixed-case>D</fixed-case>eep<fixed-case>G</fixed-case>ram<fixed-case>B</fixed-case>ank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/154_Paper.pdf</url>
      <abstract>Corpora of sentences annotated with grammatical information have been deployed by extending the basic lexical and morphological data with increasingly complex information, such as phrase constituency, syntactic functions, semantic roles, etc. As these corpora grow in size and the linguistic information to be encoded reaches higher levels of sophistication, the utilization of annotation tools and, above all, supporting computational grammars appear no longer as a matter of convenience but of necessity. In this paper, we report on the design features, the development conditions and the methodological options of a deep linguistic databank, the CINTIL DeepGramBank. In this corpus, sentences are annotated with fully fledged linguistically informed grammatical representations that are produced by a deep linguistic processing grammar, thus consistently integrating morphological, syntactic and semantic information. We also report on how such corpus permits to straightforwardly obtain a whole range of past generation annotated corpora (POS, NER and morphology), current generation treebanks (constituency treebanks, dependency banks, propbanks) and next generation databanks (logical form banks) simply by means of a very residual selection/extraction effort to get the appropriate ""views"" exposing the relevant layers of information.</abstract>
      <bibkey>branco-etal-2010-developing</bibkey>
    </paper>
    <paper id="99">
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Markus</first><last>Forsberg</last></author>
      <author><first>Dimitrios</first><last>Kokkinakis</last></author>
      <title><fixed-case>D</fixed-case>iabase: Towards a Diachronic <fixed-case>BLARK</fixed-case> in Support of Historical Studies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/156_Paper.pdf</url>
      <abstract>We present our ongoing work on language technology-based e-science in the humanities, social sciences and education, with a focus on text-based research in the historical sciences. An important aspect of language technology is the research infrastructure known by the acronym BLARK (Basic LAnguage Resource Kit). A BLARK as normally presented in the literature arguably reflects a modern standard language, which is topic- and genre-neutral, thus abstracting away from all kinds of language variation. We argue that this notion could fruitfully be extended along any of the three axes implicit in this characterization (the social, the topical and the temporal), in our case the temporal axis, towards a diachronic BLARK for Swedish, which can be used to develop e-science tools in support of historical studies.</abstract>
      <bibkey>borin-etal-2010-diabase</bibkey>
    </paper>
    <paper id="100">
      <author><first>Anne</first><last>Abeillé</last></author>
      <author><first>Danièle</first><last>Godard</last></author>
      <title>The Grande Grammaire du Français Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/157_Paper.pdf</url>
      <abstract>We present a new reference Grammar of French (La Grande Grammaire du français), which is a collective project (gathering around fifty contributors), producing a book (about 2200 pages, to be published en 2011) and associated databases. Like the recent reference grammars of the other Romance Languages, it takes into account the important results of the linguistic research of the past thrity years, while aiming at a non specialist audience and avoiding formalization. We differ from existing French grammar by being focused on contemporary French from a purely descriptive point of view, and by taking spoken data into account. We include a description of all the syntactic phenomena, as well as lexical, semantic, pragmatic and prosodic insights, specially as they interact with syntax. The analysis concerns the data from contemporary written French, but also includes data from spoken corpora and regional or non standard French (when accessible). Throughout the grammar, a simple phrase structure grammar is used, in order to maintain a common representation. The analyses are modular with a strict division of labor between morphology, syntax and semantics. From the syntactic point of view, POS are also distinguished from grammatical relations (or functions). The databases include a terminological glossary, different lexical databases for certain POS, certain valence frames and certain semantic classes, and a bibliographical database.</abstract>
      <bibkey>abeille-godard-2010-grande</bibkey>
    </paper>
    <paper id="101">
      <author><first>Satoshi</first><last>Sekine</last></author>
      <author><first>Kapil</first><last>Dalwani</last></author>
      <title>Ngram Search Engine with Patterns Combining Token, <fixed-case>POS</fixed-case>, Chunk and <fixed-case>NE</fixed-case> Information</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/158_Paper.pdf</url>
      <abstract>We developed a search tool for ngrams extracted from a very large corpus (the current system uses the entire Wikipedia, which has 1.7 billion tokens). The tool supports queries with an arbitrary number of wildcards and/or specification by a combination of token, POS, chunk (such as NP, VP, PP) and Named Entity (NE). The previous system (Sekine 08) can only handle tokens and unrestricted wildcards in the query, such as * was established in *. However, being able to constrain the wildcards by POS, chunk or NE is quite useful to filter out noise. For example, the new system can search for NE=COMPANY was established in POS=CD. This finer specification reduces the number of outputs to less than half and avoids the ngrams which have a comma or a common noun at the first position or location information at the last position. It outputs the matched ngrams with their frequencies as well as all the contexts (i.e. sentences, KWIC lists and document ID information) where the matched ngrams occur in the corpus. It takes a fraction of a second for a search on a single CPU Linux-PC (1GB memory and 500GB disk) environment.</abstract>
      <bibkey>sekine-dalwani-2010-ngram</bibkey>
    </paper>
    <paper id="102">
      <author><first>Alexander</first><last>Schmitt</last></author>
      <author><first>Tim</first><last>Polzehl</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Jackson</first><last>Liscombe</last></author>
      <title>The Influence of the Utterance Length on the Recognition of Aged Voices</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/159_Paper.pdf</url>
      <abstract>This paper addresses the recognition of elderly callers based on short and narrow-band utterances, which are typical for Interactive Voice Response (IVR) systems. Our study is based on 2308 short utterances from a deployed IVR application. We show that features such as speaking rate, jitter and shimmer that are considered as most meaningful ones for determining elderly users underperform when used in the IVR context while pitch and intensity features seem to gain importance. We further demonstrate the influence of the utterance length on the classifiers performance: for both humans and classifier, the distinction between aged and non-aged voices becomes increasingly difficult the shorter the utterances get. Our setup based on a Support Vector Machine (SVM) with linear kernel reaches a comparably poor performance of 58% accuracy, which can be attributed to an average utterance length of only 1.6 seconds. The automatic distinction between aged and non-aged utterances drops to random when the utterance length falls below 1.2 seconds.</abstract>
      <bibkey>schmitt-etal-2010-influence</bibkey>
    </paper>
    <paper id="103">
      <author><first>Marta</first><last>Recasens</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>M. Antònia</first><last>Martí</last></author>
      <title>A Typology of Near-Identity Relations for Coreference (<fixed-case>NIDENT</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/160_Paper.pdf</url>
      <abstract>The task of coreference resolution requires people or systems to decide when two referring expressions refer to the 'same' entity or event. In real text, this is often a difficult decision because identity is never adequately defined, leading to contradictory treatment of cases in previous work. This paper introduces the concept of 'near-identity', a middle ground category between identity and non-identity, to handle such cases systematically. We present a typology of Near-Identity Relations (NIDENT) that includes fifteen types―grouped under four main families―that capture a wide range of ways in which (near-)coreference relations hold between discourse entities. We validate the theoretical model by annotating a small sample of real data and showing that inter-annotator agreement is high enough for stability (K=0.58, and up to K=0.65 and K=0.84 when leaving out one and two outliers, respectively). This work enables subsequent creation of the first internally consistent language resource of this type through larger annotation efforts.</abstract>
      <bibkey>recasens-etal-2010-typology</bibkey>
    </paper>
    <paper id="104">
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <author><first>Paola</first><last>Monachesi</last></author>
      <title>Interacting Semantic Layers of Annotation in <fixed-case>S</fixed-case>o<fixed-case>N</fixed-case>a<fixed-case>R</fixed-case>, a Reference Corpus of Contemporary Written <fixed-case>D</fixed-case>utch</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/162_Paper.pdf</url>
      <abstract>This paper reports on the annotation of a corpus of 1 million words with four semantic annotation layers, including named entities, co- reference relations, semantic roles and spatial and temporal expressions. These semantic annotation layers can benefit from the manually verified part of speech tagging, lemmatization and syntactic analysis (dependency tree) information layers which resulted from an earlier project (Van Noord et al., 2006) and will thus result in a deeply syntactically and semantically annotated corpus. This annotation effort is carried out in the framework of a larger project which aims at the collection of a 500-million word corpus of contemporary Dutch, covering the variants used in the Netherlands and Flanders, the Dutch speaking part of Belgium. All the annotation schemes used were (co-)developed by the authors within the Flemish-Dutch STEVIN-programme as no previous schemes for Dutch were available. They were created taking into account standards (either de facto or official (like ISO)) used elsewhere.</abstract>
      <bibkey>schuurman-etal-2010-interacting</bibkey>
    </paper>
    <paper id="105">
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Marc</first><last>Kemps-Snijders</last></author>
      <author><first>Dieter</first><last>Van Uytvanck</last></author>
      <author><first>Menzo</first><last>Windhouwer</last></author>
      <author><first>Peter</first><last>Withers</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Claus</first><last>Zinn</last></author>
      <title>A Data Category Registry- and Component-based Metadata Framework</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/163_Paper.pdf</url>
      <abstract>We describe our computer-supported framework to overcome the rule of metadata schism. It combines the use of controlled vocabularies, managed by a data category registry, with a component-based approach, where the categories can be combined to yield complex metadata structures. A metadata scheme devised in this way will thus be grounded in its use of categories. Schema designers will profit from existing prefabricated larger building blocks, motivating re-use at a larger scale. The common base of any two metadata schemes within this framework will solve, at least to a good extent, the semantic interoperability problem, and consequently, further promote systematic use of metadata for existing resources and tools to be shared.</abstract>
      <bibkey>broeder-etal-2010-data</bibkey>
    </paper>
    <paper id="106">
      <author><first>Isa</first><last>Maks</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Annotation Scheme and Gold Standard for <fixed-case>D</fixed-case>utch Subjective Adjectives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/165_Paper.pdf</url>
      <abstract>Many techniques are developed to derive automatically lexical resources for opinion mining. In this paper we present a gold standard for Dutch adjectives developed for the evaluation of these techniques. In the first part of the paper we introduce our annotation guidelines. They are based upon guidelines recently developed for English which annotate subjectivity and polarity at word sense level. In addition to subjectivity and polarity we propose a third annotation category: that of the attitude holder. The identity of the attitude holder is partly implied by the word itself and may provide useful information for opinion mining systems. In the second part of paper we present the criteria adopted for the selection of items which should be included in this gold standard. Our design is aimed at an equal representation of all dimensions of the lexicon , like frequency and polysemy, in order to create a gold standard which can be used not only for benchmarking purposes but also may help to improve in a systematic way, the methods which derive the word lists. Finally we present the results of the annotation task including annotator agreement rates and disagreement analysis.</abstract>
      <bibkey>maks-vossen-2010-annotation</bibkey>
    </paper>
    <paper id="107">
      <author><first>Mark</first><last>Arehart</last></author>
      <title>Indexing Methods for Faster and More Effective Person Name Search</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/166_Paper.pdf</url>
      <abstract>This paper compares several indexing methods for person names extracted from text, developed for an information retrieval system with requirements for fast approximate matching of noisy and multicultural Romanized names. Such matching algorithms are computationally expensive and unacceptably slow when used without an indexing or blocking step. The goal is to create a small candidate pool containing all the true matches that can be exhaustively searched by a more effective but slower name comparison method. In addition to dramatically faster search, some of the methods evaluated here led to modest gains in effectiveness by eliminating false positives. Four indexing techniques using either phonetic keys or substrings of name segments, with and without name segment stopword lists, were combined with three name matching algorithms. On a test set of 700 queries run against 70K noisy and multicultural names, the best-performing technique took just 2.1% as long as a naive exhaustive search and increased F1 by 3 points, showing that an appropriate indexing technique can increase both speed and effectiveness.</abstract>
      <bibkey>arehart-2010-indexing</bibkey>
    </paper>
    <paper id="108">
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <author><first>Minoru</first><last>Sasaki</last></author>
      <title>Detection of Peculiar Examples using <fixed-case>LOF</fixed-case> and One Class <fixed-case>SVM</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/167_Paper.pdf</url>
      <abstract>This paper proposes the method to detect peculiar examples of the target word from a corpus. In this paper we regard following examples as peculiar examples: (1) a meaning of the target word in the example is new, (2) a compound word consisting of the target word in the example is new or very technical. The peculiar example is regarded as an outlier in the given example set. Therefore we can apply many methods proposed in the data mining domain to our task. In this paper, we propose the method to combine the density based method, Local Outlier Factor (LOF), and One Class SVM, which are representative outlier detection methods in the data mining domain. In the experiment, we use the Whitepaper text in BCCWJ as the corpus, and 10 noun words as target words. Our method improved precision and recall of LOF and One Class SVM. And we show that our method can detect new meanings by using the noun `midori (green)'. The main reason of un-detections and wrong detection is that similarity measure of two examples is inadequacy. In future, we must improve it.</abstract>
      <bibkey>shinnou-sasaki-2010-detection</bibkey>
    </paper>
    <paper id="109">
      <author><first>Naushad</first><last>UzZaman</last></author>
      <author><first>James</first><last>Allen</last></author>
      <title><fixed-case>TRIOS</fixed-case>-<fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ank Corpus: Extended <fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ank Corpus with Help of Deep Understanding of Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/169_Paper.pdf</url>
      <abstract>TimeBank (Pustejovsky et al, 2003a), a reference for TimeML (Pustejovsky et al, 2003b) compliant annotation, is widely used temporally annotated corpus in the community. It captures time expressions, events, and relations between events and event and temporal expression; but there is room for improvements in this hand-annotated widely used TimeBank corpus. This work is one such effort to extend the TimeBank corpus. Our first goal is to suggest missing TimeBank events and temporal expressions, i.e. events and temporal expressions that were missed by TimeBank annotators. Along with that this paper also suggests some additions to TimeML language by adding new event features (ontology type), some more SLINKs and also relations between events with their arguments, which we call RLINK (relation link). With our new suggestions we present the TRIOS-TimeBank corpus, an extended TimeBank corpus. We conclude by suggesting our future work to clean the TimeBank corpus even more and automatically generating larger temporally annotated corpus for the community.</abstract>
      <bibkey>uzzaman-allen-2010-trios</bibkey>
    </paper>
    <paper id="110">
      <author><first>Adam</first><last>Funk</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <title>Ontology-Based Categorization of Web Services with Machine Learning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/170_Paper.pdf</url>
      <abstract>We present the problem of categorizing web services according to a shallow ontology for presentation on a specialist portal, using their WSDL and associated textual documents found by a crawler. We treat this as a text classification problem and apply first information extraction (IE) techniques (voting using keywords weight according to their context), then machine learning (ML), and finally a combined approach in which ML has priority over weighted keywords, but the latter can still make up categorizations for services for which ML does not produce enough. We evaluate the techniques (using data manually annotated through the portal, which we also use as the training data for ML) according to standard IE measures for flat categorization as well as the Balanced Distance Metric (more suitable for ontological classification) and compare them with related work in web service categorization. The ML and combined categorization results are good and the system is designed to take users' contributions through the portal's Web 2.0 features as additional training data.</abstract>
      <bibkey>funk-bontcheva-2010-ontology</bibkey>
    </paper>
    <paper id="111">
      <author><first>Yugo</first><last>Murawaki</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>Online <fixed-case>J</fixed-case>apanese Unknown Morpheme Detection using Orthographic Variation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/171_Paper.pdf</url>
      <abstract>To solve the unknown morpheme problem in Japanese morphological analysis, we previously proposed a novel framework of online unknown morpheme acquisition and its implementation. This framework poses a previously unexplored problem, online unknown morpheme detection. Online unknown morpheme detection is a task of finding morphemes in each sentence that are not listed in a given lexicon. Unlike in English, it is a non-trivial task because Japanese does not delimit words by white space. We first present a baseline method that simply uses the output of the morphological analyzer. We then show that it fails to detect some unknown morphemes because they are over-segmented into shorter registered morphemes. To cope with this problem, we present a simple solution, the use of orthographic variation of Japanese. Under the assumption that orthographic variants behave similarly, each over-segmentation candidate is checked against its counterparts. Experiments show that the proposed method improves the recall of detection and contributes to improving unknown morpheme acquisition.</abstract>
      <bibkey>murawaki-kurohashi-2010-online</bibkey>
    </paper>
    <paper id="112">
      <author><first>Bracha</first><last>Nir</last></author>
      <author><first>Brian</first><last>MacWhinney</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <title>A Morphologically-Analyzed <fixed-case>CHILDES</fixed-case> Corpus of <fixed-case>H</fixed-case>ebrew</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/172_Paper.pdf</url>
      <abstract>We present a corpus of transcribed spoken Hebrew that forms an integral part of a comprehensive data system that has been developed to suit the specific needs and interests of child language researchers: CHILDES (Child Language Data Exchange System). We introduce a dedicated transcription scheme for the spoken Hebrew data that is aware both of the phonology and of the standard orthography of the language. We also introduce a morphological analyzer that was specifically developed for this corpus.</abstract>
      <bibkey>nir-etal-2010-morphologically</bibkey>
    </paper>
    <paper id="113">
      <author><first>Kristiina</first><last>Jokinen</last></author>
      <title>Non-verbal Signals for Turn-taking and Feedback</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/173_Paper.pdf</url>
      <abstract>This paper concerns non-verbal communication, and describes especially the use of eye-gaze to signal turn-taking and feedback in conversational settings. Eye-gaze supports smooth interaction by providing signals that the interlocutors interpret with respect to such conversational functions as taking turns and giving feedback. New possibilities to study the effect of eye-gaze on the interlocutors communicative behaviour have appeared with the eye-tracking technology which in the past years has matured to the level where its use to study naturally occurring dialogues have become easier and more reliable to conduct. It enables the tracking of eye-fixations and gaze-paths, and thus allows analysis of the persons turn-taking and feedback behaviour through the analysis of their focus of attention. In this paper, experiments on the interlocutors non-verbal communication in conversational settings using the eye-tracker are reported, and results of classifying turn-taking using eye-gaze and gesture information are presented. Also the hybrid method that combines signal level analysis with human interpretation is discussed.</abstract>
      <bibkey>jokinen-2010-non</bibkey>
    </paper>
    <paper id="114">
      <author><first>Alejandro</first><last>Abejón</last></author>
      <author><first>Doroteo T.</first><last>Toledano</last></author>
      <author><first>Danilo</first><last>Spada</last></author>
      <author><first>González</first><last>Victor</last></author>
      <author><first>Daniel Hernández</first><last>López</last></author>
      <title>A Study of the Influence of Speech Type on Automatic Language Recognition Performance</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/174_Paper.pdf</url>
      <abstract>Automatic language recognition on spontaneous speech has experienced a rapid development in the last few years. This development has been in part due to the competitive technological Language Recognition Evaluations (LRE) organized by the National Institute of Standards and Technology (NIST). Until now, the need to have clearly defined and consistent evaluations has kept some real-life application issues out of these evaluations. In particular, all past NIST LREs have used exclusively conversational telephone speech (CTS) for development and test. Fortunately this has changed in the current NIST LRE since it includes also broadcast speech. However, for testing only the telephone speech found in broadcast data will be used. In real-life applications, there could be several more types of speech and systems could be forced to use a mix of different types of data for training and development and recognition. In this article, we have defined a test-bed including several types of speech data and have analyzed how a typical language recognition system works using different types of speech, and also a combination of different types of speech, for training and testing.</abstract>
      <bibkey>abejon-etal-2010-study</bibkey>
    </paper>
    <paper id="115">
      <author><first>Gosse</first><last>Bouma</last></author>
      <title>Cross-lingual Ontology Alignment using <fixed-case>E</fixed-case>uro<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et and <fixed-case>W</fixed-case>ikipedia</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/175_Paper.pdf</url>
      <abstract>This paper describes a system for linking the thesaurus of the Netherlands Institute for Sound and Vision to English WordNet and dbpedia. The thesaurus contains subject (concept) terms, and names of persons, locations, and miscalleneous names. We used EuroWordNet, a multilingual wordnet, and Dutch Wikipedia as intermediaries for the two alignments. EuroWordNet covers most of the subject terms in the thesaurus, but the organization of the cross-lingual links makes selection of the most appropriate English target term almost impossible. Precision and recall of the automatic alignment with WordNet for subject terms is 0.59. Using page titles, redirects, disambiguation pages, and anchor text harvested from Dutch Wikipedia gives reasonable performance on subject terms and geographical terms. Many person and miscalleneous names in the thesaurus could not be located in (Dutch or English) Wikipedia. Precision for miscellaneous names, subjects, persons and locations for the alignment with Wikipedia ranges from 0.63 to 0.94, while recall for subject terms is 0.62.</abstract>
      <bibkey>bouma-2010-cross</bibkey>
    </paper>
    <paper id="116">
      <author><first>François</first><last>Lefebvre-Albaret</last></author>
      <author><first>Patrice</first><last>Dalle</last></author>
      <title>Video Retrieval in Sign Language Videos : How to Model and Compare Signs?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/176_Paper.pdf</url>
      <abstract>This paper deals with the problem of finding sign occurrences in a sign language (SL) video. It begins with an analysis of sign models and the way they can take into account the sign variability. Then, we review the most popular technics dedicated to automatic sign language processing and we focus on their adaptation to model sign variability. We present a new method to provide a parametric description of the sign as a set of continuous and discrete parameters. Signs are classified according to there categories (ballistic movements, circles ...), the symmetry between the hand movements, hand absolute and relative locations. Membership grades to sign categories and continuous parameter comparisons can be combined to estimate the similarity between two signs. We set out our system and we evaluate how much time can be saved when looking for a sign in a french sign language video. By now, our formalism only uses hand 2D locations, we finally discuss about the way of integrating other parameters as hand shape or facial expression in our framework.</abstract>
      <bibkey>lefebvre-albaret-dalle-2010-video</bibkey>
    </paper>
    <paper id="117">
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Alejandra</first><last>Lorenzo</last></author>
      <title>Identifying Sources of Weakness in Syntactic Lexicon Extraction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/177_Paper.pdf</url>
      <abstract>Previous work has shown that large scale subcategorisation lexicons could be extracted from parsed corpora with reasonably high precision. In this paper, we apply a standard extraction procedure to a 100 millions words parsed corpus of french and obtain rather poor results. We investigate different factors likely to improve performance such as in particular, the specific extraction procedure and the parser used; the size of the input corpus; and the type of frames learned. We try out different ways of interleaving the output of several parsers with the lexicon extraction process and show that none of them improves the results. Conversely, we show that increasing the size of the input corpus and modifying the extraction procedure to better differentiate prepositional arguments from prepositional modifiers improves performance. In conclusion, we suggest that a more sophisticated approach to parser combination and better probabilistic models of the various types of prepositional objects in French are likely ways to get better results.</abstract>
      <bibkey>gardent-lorenzo-2010-identifying</bibkey>
    </paper>
    <paper id="118">
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <title>Improvements in Parsing the Index <fixed-case>T</fixed-case>homisticus Treebank. Revision, Combination and a Feature Model for Medieval <fixed-case>L</fixed-case>atin</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/178_Paper.pdf</url>
      <abstract>The creation of language resources for less-resourced languages like the historical ones benefits from the exploitation of language-independent tools and methods developed over the years by many projects for modern languages. Along these lines, a number of treebanks for historical languages started recently to arise, including treebanks for Latin. Among the Latin treebanks, the Index Thomisticus Treebank is a 68,000 token dependency treebank based on the Index Thomisticus by Roberto Busa SJ, which contains the opera omnia of Thomas Aquinas (118 texts) as well as 61 texts by other authors related to Thomas, for a total of approximately 11 million tokens. In this paper, we describe a number of modifications that we applied to the dependency parser DeSR, in order to improve the parsing accuracy rates on the Index Thomisticus Treebank. First, we adapted the parser to the specific processing of Medieval Latin, defining an ad-hoc configuration of its features. Then, in order to improve the accuracy rates provided by DeSR, we applied a revision parsing method and we combined the outputs produced by different algorithms. This allowed us to improve accuracy rates substantially, reaching results that are well beyond the state of the art of parsing for Latin.</abstract>
      <bibkey>passarotti-dellorletta-2010-improvements</bibkey>
    </paper>
    <paper id="119">
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <title>Cultural Aspects of Spatiotemporal Analysis in Multilingual Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/180_Paper.pdf</url>
      <abstract>In this paper we want to point out some issues arising when a natural language processing task involves several languages (like multi- lingual, multidocument summarization and the machine translation aspects involved) which are often neglected. These issues are of a more cultural nature, and may even come into play when several documents in a single language are involved. We pay special attention to those aspects dealing with the spatiotemporal characteristics of a text. Correct automatic selection of (parts of) texts such as handling the same eventuality, presupposes spatiotemporal disambiguation at a rather specific level. The same holds for the analysis of the query. For generation and translation purposes, spatiotemporal aspects may be relevant as well. At the moment English (both the British and American variants) and Dutch (the Flemish and Dutch variant) are covered, all taking into account the perspective of a contemporary, Flemish user. In our approach the cultural aspects associated with for example the language of publication and the language used by the user play a crucial role.</abstract>
      <bibkey>schuurman-vandeghinste-2010-cultural</bibkey>
    </paper>
    <paper id="120">
      <author><first>Takehiro</first><last>Teraoka</last></author>
      <author><first>Jun</first><last>Okamoto</last></author>
      <author><first>Shun</first><last>Ishizaki</last></author>
      <title>An Associative Concept Dictionary for Verbs and its Application to Elliptical Word Estimation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/181_Paper.pdf</url>
      <abstract>Natural language processing technology has developed remarkably, but it is still difficult for computers to understand contextual meanings as humans do. The purpose of our work has been to construct an associative concept dictionary for Japanese verbs and make computers understand contextual meanings with a high degree of accuracy. We constructed an automatic system that can be used to estimate elliptical words. We present the result of comparing words that were estimated both by our proposed system (VNACD) and three baseline systems (VACD, NACD, and CF). We then calculated the mean reciprocal rank (MRR), top N accuracy (top 1, top 5, and top 10), and the mean average precision (MAP). Finally, we showed the effectiveness of our method for which both an associative concept dictionary for verbs (Verb-ACD) and one for nouns (Noun-ACD) were used. From the results, we conclude that both the Verb-ACD and the Noun-ACD play a key role in estimating elliptical words.</abstract>
      <bibkey>teraoka-etal-2010-associative</bibkey>
    </paper>
    <paper id="121">
      <author><first>Sowmya V.</first><last>B.</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <author><first>Anupam</first><last>Basu</last></author>
      <title>Resource Creation for Training and Testing of Transliteration Systems for <fixed-case>I</fixed-case>ndian Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/182_Paper.pdf</url>
      <abstract>Machine transliteration is used in a number of NLP applications ranging from machine translation and information retrieval to input mechanisms for non-roman scripts. Many popular Input Method Editors for Indian languages, like Baraha, Akshara, Quillpad etc, use back-transliteration as a mechanism to allow users to input text in a number of Indian language. The lack of a standard dataset to evaluate these systems makes it difficult to make any meaningful comparisons of their relative accuracies. In this paper, we describe the methodology for the creation of a dataset of ~2500 transliterated sentence pairs each in Bangla, Hindi and Telugu. The data was collected across three different modes from a total of 60 users. We believe that this dataset will prove useful not only for the evaluation and training of back-transliteration systems but also help in the linguistic analysis of the process of transliterating Indian languages from native scripts to Roman.</abstract>
      <bibkey>b-etal-2010-resource</bibkey>
    </paper>
    <paper id="122">
      <author><first>Antonio</first><last>Balvet</last></author>
      <author><first>Lucie</first><last>Barque</last></author>
      <author><first>Rafael</first><last>Marín</last></author>
      <title>Building a Lexicon of <fixed-case>F</fixed-case>rench Deverbal Nouns from a Semantically Annotated Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/183_Paper.pdf</url>
      <abstract>This paper presents project Nomage, which aims at describing the aspectual properties of deverbal nouns in an empirical way. It is centered on the development of two resources: a semantically annotated corpus of deverbal nouns, and an electronic lexicon. They are both presented in this paper, and emphasize how the semantic annotations of the corpus allow the lexicographic description of deverbal nouns to be validated, in particular their polysemy. Nominalizations have occupied a central place in grammatical analysis, with a focus on morphological and syntactic aspects. More recently, researchers have begun to address a specific issue often neglected before, i.e. the semantics of nominalizations, and its implications for Natural Language Processing applications such as electronic ontologies or Information Retrieval. We focus on precisely this issue in the research project NOMAGE, funded by the French National Research Agency (ANR-07-JCJC-0085-01). In this paper, we present the Nomage corpus and the annotations we make on deverbal nouns (section 2). We then show how we build our lexicon with the semantically annotated corpus and illustrate the kind of generalizations we can make from such data (section 3).</abstract>
      <bibkey>balvet-etal-2010-building</bibkey>
    </paper>
    <paper id="123">
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Giuseppe</first><last>Riccardi</last></author>
      <author><first>Rashmi</first><last>Prasad</last></author>
      <author><first>Aravind</first><last>Joshi</last></author>
      <title>Annotation of Discourse Relations for Conversational Spoken Dialogs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/184_Paper.pdf</url>
      <abstract>In this paper, we make a qualitative and quantitative analysis of discourse relations within the LUNA conversational spoken dialog corpus. In particular, we first describe the Penn Discourse Treebank (PDTB) and then we detail the adaptation of its annotation scheme to the LUNA corpus of Italian task-oriented dialogs in the domain of software/hardware assistance. We discuss similarities and differences between our approach and the PDTB paradigm and point out the peculiarities of spontaneous dialogs w.r.t. written text, which motivated some changes in the annotation strategy. In particular, we introduced the annotation of relations between non-contiguous arguments and we modified the sense hierarchy in order to take into account the important role of pragmatics in dialogs. In the final part of the paper, we present a comparison between the sense and connective frequency in a representative subset of the LUNA corpus and in the PDTB. Such analysis confirmed the differences between the two corpora and corroborates our choice to introduce dialog-specific adaptations.</abstract>
      <bibkey>tonelli-etal-2010-annotation</bibkey>
    </paper>
    <paper id="124">
      <author><first>Sandra</first><last>Williams</last></author>
      <author><first>Richard</first><last>Power</last></author>
      <title>A Fact-aligned Corpus of Numerical Expressions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/185_Paper.pdf</url>
      <abstract>We describe a corpus of numerical expressions, developed as part of the NUMGEN project. The corpus contains newspaper articles and scientific papers in which exactly the same numerical facts are presented many times (both within and across texts). Some annotations of numerical facts are original: for example, numbers are automatically classified as round or non-round by an algorithm derived from Jansen and Pollmann (2001); also, numerical hedges such as about or a little under are marked up and classified semantically using arithmetical relations. Through explicit alignment of phrases describing the same fact, the corpus can support research on the influence of various contextual factors (e.g., document position, intended readership) on the way in which numerical facts are expressed. As an example we present results from an investigation showing that when a fact is mentioned more than once in a text, there is a clear tendency for precision to increase from first to subsequent mentions, and for mathematical level either to remain constant or to increase.</abstract>
      <bibkey>williams-power-2010-fact</bibkey>
    </paper>
    <paper id="125">
      <author><first>Ludovic</first><last>Quintard</last></author>
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Dominique</first><last>Laurent</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <title>Question Answering on Web Data: The <fixed-case>QA</fixed-case> Evaluation in Quæro</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/186_Paper.pdf</url>
      <abstract>In the QA and information retrieval domains progress has been assessed via evaluation campaigns(Clef, Ntcir, Equer, Trec).In these evaluations, the systems handle independent questions and should provide one answer to each question, extracted from textual data, for both open domain and restricted domain. Quæro is a program promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Among the many research areas concerned by Quæro. The Quaero project organized a series of evaluations of Question Answering on Web Data systems in 2008 and 2009. For each language, English and French the full corpus has a size of around 20Gb for 2.5M documents. We describe the task and corpora, and especially the methodologies used in 2008 to construct the test of question and a new one in the 2009 campaign. Six types of questions were addressed, factual, Non-factual(How, Why, What), List, Boolean. A description of the participating systems and the obtained results is provided. We show the difficulty for a question-answering system to work with complex data and questions.</abstract>
      <bibkey>quintard-etal-2010-question</bibkey>
    </paper>
    <paper id="126">
      <author><first>Silvia</first><last>Quarteroni</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <title>A Comprehensive Resource to Evaluate Complex Open Domain Question Answering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/188_Paper.pdf</url>
      <abstract>Complex Question Answering is a discipline that involves a deep understanding of question/answer relations, such as those characterizing definition and procedural questions and their answers. To contribute to the improvement of this technology, we deliver two question and answer corpora for complex questions, WEB-QA and TREC-QA, extracted by the same Question Answering system, YourQA, from the Web and from the AQUAINT-6 data collection respectively. We believe that such corpora can be useful resources to address a type of QA that is far from being efficiently solved. WEB-QA and TREC-QA are available in two formats: judgment files and training/testing files. Judgment files contain a ranked list of candidate answers to TREC-10 complex questions, extracted using YourQA as a baseline system and manually labelled according to a Likert scale from 1 (completely incorrect) to 5 (totally correct). Training and testing files contain learning instances compatible with SVM-light; these are useful for experimenting with shallow and complex structural features such as parse trees and semantic role labels. Our experiments with the above corpora have allowed to prove that structured information representation is useful to improve the accuracy of complex QA systems and to re-rank answers.</abstract>
      <bibkey>quarteroni-moschitti-2010-comprehensive</bibkey>
    </paper>
    <paper id="127">
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Ludovic</first><last>Quintard</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Claire</first><last>Nédellec</last></author>
      <author><first>Sophie</first><last>Aubin</last></author>
      <author><first>Laurent</first><last>Gillard</last></author>
      <author><first>Jean-Pierre</first><last>Raysz</last></author>
      <author><first>Delphine</first><last>Pois</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Dominique</first><last>Laurent</last></author>
      <title>Named and Specific Entity Detection in Varied Data: The Quæro Named Entity Baseline Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/191_Paper.pdf</url>
      <abstract>The Quæro program that promotes research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within its context a set of evaluations of Named Entity recognition systems was held in 2009. Four tasks were defined. The first two concerned traditional named entities in French broadcast news for one (a rerun of ESTER 2) and of OCR-ed old newspapers for the other. The third was a gene and protein name extraction in medical abstracts. The last one was the detection of references in patents. Four different partners participated, giving a total of 16 systems. We provide a synthetic descriptions of all of them classifying them by the main approaches chosen (resource-based, rules-based or statistical), without forgetting the fact that any modern system is at some point hybrid. The metric (the relatively standard Slot Error Rate) and the results are also presented and discussed. Finally, a process is ongoing with preliminary acceptance of the partners to ensure the availability for the community of all the corpora used with the exception of the non-Quæro produced ESTER 2 one.</abstract>
      <bibkey>galibert-etal-2010-named</bibkey>
    </paper>
    <paper id="128">
      <author><first>Kyota</first><last>Tsutsumida</last></author>
      <author><first>Jun</first><last>Okamoto</last></author>
      <author><first>Shun</first><last>Ishizaki</last></author>
      <author><first>Makoto</first><last>Nakatsuji</last></author>
      <author><first>Akimichi</first><last>Tanaka</last></author>
      <author><first>Tadasu</first><last>Uchiyama</last></author>
      <title>Study of Word Sense Disambiguation System that uses Contextual Features - Approach of Combining Associative Concept Dictionary and Corpus -</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/192_Paper.pdf</url>
      <abstract>We propose a Word Sense Disambiguation (WSD) method that accurately classifies ambiguous words to concepts in the Associative Concept Dictionary (ACD) even when the test corpus and the training corpus for WSD are acquired from different domains. Many WSD studies determine the context of the target ambiguous word by analyzing sentences containing the target word. However, they offer poor performance when they are applied to a corpus that differs from the training corpus. One solution is to use associated words that are domain-independently assigned to the concept in ACD; i.e. many users commonly imagine those words against a given concept. Furthermore, by using the associated words of a concept as search queries for a training corpus, our method extracts relevant words, those that are computationally judged to be related to that concept. By checking the frequency of associated words and relevant words that appear near to the target word in a sentence in the test corpus, our method classifies the target word to the concept in ACD. Our evaluation using two different types of corpus demonstrates its good accuracy.</abstract>
      <bibkey>tsutsumida-etal-2010-study</bibkey>
    </paper>
    <paper id="129">
      <author><first>Lars</first><last>Ahrenberg</last></author>
      <title>Alignment-based Profiling of <fixed-case>E</fixed-case>uroparl Data in an <fixed-case>E</fixed-case>nglish-<fixed-case>S</fixed-case>wedish Parallel Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/193_Paper.pdf</url>
      <abstract>This paper profiles the Europarl part of an English-Swedish parallel corpus and compares it with three other subcorpora of the same parallel corpus. We first describe our method for comparison which is based on manually reviewed word alignments. We investigate relative frequences of different types of correspondence, including null alignments, many-to-one correspondences and crossings. In addition, both halves of the parallel corpus have been annotated with morpho-syntactic information. The syntactic annotation uses labelled dependency relations. Thus, we can see how different types of correspondences are distributed on different parts-of-speech and compute correspondences at the structural level. In spite of the fact that two of the other subcorpora contains fiction, it is found that the Europarl part is the one having the highest proportion of many types of restructurings, including additions, deletions, long distance reorderings and dependency reversals. We explain this by the fact that the majority of Europarl segments are parallel translations rather than source texts and their translations.</abstract>
      <bibkey>ahrenberg-2010-alignment</bibkey>
    </paper>
    <paper id="130">
      <author><first>Muhammad Kamran</first><last>Malik</last></author>
      <author><first>Tafseer</first><last>Ahmed</last></author>
      <author><first>Sebastian</first><last>Sulger</last></author>
      <author><first>Tina</first><last>Bögel</last></author>
      <author><first>Atif</first><last>Gulzar</last></author>
      <author><first>Ghulam</first><last>Raza</last></author>
      <author><first>Sarmad</first><last>Hussain</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <title>Transliterating <fixed-case>U</fixed-case>rdu for a Broad-Coverage <fixed-case>U</fixed-case>rdu/<fixed-case>H</fixed-case>indi <fixed-case>LFG</fixed-case> Grammar</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/194_Paper.pdf</url>
      <abstract>In this paper, we present a system for transliterating the Arabic-based script of Urdu to a Roman transliteration scheme. The system is integrated into a larger system consisting of a morphology module, implemented via finite state technologies, and a computational LFG grammar of Urdu that was developed with the grammar development platform XLE (Crouch et al. 2008). Our long-term goal is to handle Hindi alongside Urdu; the two languages are very similar with respect to syntax and lexicon and hence, one grammar can be used to cover both languages. However, they are not similar concerning the script -- Hindi is written in Devanagari, while Urdu uses an Arabic-based script. By abstracting away to a common Roman transliteration scheme in the respective transliterators, our system can be enabled to handle both languages in parallel. In this paper, we discuss the pipeline architecture of the Urdu-Roman transliterator, mention several linguistic and orthographic issues and present the integration of the transliterator into the LFG parsing system.</abstract>
      <bibkey>malik-etal-2010-transliterating</bibkey>
    </paper>
    <paper id="131">
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Harry</first><last>Bunt</last></author>
      <title>Towards an Integrated Scheme for Semantic Annotation of Multimodal Dialogue Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/195_Paper.pdf</url>
      <abstract>Recent years witness a growing interest in the use of multimodal data for modelling of communicative behaviour in dialogue. Dybkjaer and Bernsen (2002), point out that coding schemes for multimodal data are used solely by their creators. Standardisation has been achieved to some extent for coding behavioural features for certain nonverbal expressions, e.g. for facial expression, however, for the semantic annotation of such expressions combined with other modalities such as speech there is still a long way to go. The majority of existing dialogue act annotation schemes that are designed to code semantic and pragmatic dialogue information are limited to analysis of spoken modality. This paper investigates the applicability of existing dialogue act annotation schemes to the semantic annotation of multimodal data, and the way a dialogue act annotation scheme can be extended to cover dialogue phenomena from multiple modalities. The general conclusion of our explorative study is that a multidimensional dialogue act taxonomy is usable for this purpose when some adjustments are made. We proposed a solution for adding these aspects to a dialogue act annotation scheme without changing its set of communicative functions, in the form of qualifiers that can be attached to communicative function tags.</abstract>
      <bibkey>petukhova-bunt-2010-towards</bibkey>
    </paper>
    <paper id="132">
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <author><first>Alessandro</first><last>Mazzei</last></author>
      <author><first>Vincenzo</first><last>Lombardo</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Leonardo</first><last>Lesmo</last></author>
      <author><first>Giuseppe</first><last>Attardi</last></author>
      <author><first>Maria</first><last>Simi</last></author>
      <author><first>Alberto</first><last>Lavelli</last></author>
      <author><first>Johan</first><last>Hall</last></author>
      <author><first>Jens</first><last>Nilsson</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <title>Comparing the Influence of Different Treebank Annotations on Dependency Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/196_Paper.pdf</url>
      <abstract>As the interest of the NLP community grows to develop several treebanks also for languages other than English, we observe efforts towards evaluating the impact of different annotation strategies used to represent particular languages or with reference to particular tasks. This paper contributes to the debate on the influence of resources used for the training and development on the performance of parsing systems. It presents a comparative analysis of the results achieved by three different dependency parsers developed and tested with respect to two treebanks for the Italian language, namely TUT and ISST--TANL, which differ significantly at the level of both corpus composition and adopted dependency representations.</abstract>
      <bibkey>bosco-etal-2010-comparing</bibkey>
    </paper>
    <paper id="133">
      <author><first>Christian</first><last>Federmann</last></author>
      <title><fixed-case>A</fixed-case>ppraise: An Open-Source Toolkit for Manual Phrase-Based Evaluation of Translations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/197_Paper.pdf</url>
      <abstract>We describe a focused effort to investigate the performance of phrase-based, human evaluation of machine translation output achieving a high annotator agreement. We define phrase-based evaluation and describe the implementation of Appraise, a toolkit that supports the manual evaluation of machine translation results. Phrase ranking can be done using either a fine-grained six-way scoring scheme that allows to differentiate between ""much better"" and ""slightly better"", or a reduced subset of ranking choices. Afterwards we discuss kappa values for both scoring models from several experiments conducted with human annotators. Our results show that phrase-based evaluation can be used for fast evaluation obtaining significant agreement among annotators. The granularity of ranking choices should, however, not be too fine-grained as this seems to confuse annotators and thus reduces the overall agreement. The work reported in this paper confirms previous work in the field and illustrates that the usage of human evaluation in machine translation should be reconsidered. The Appraise toolkit is available as open-source and can be downloaded from the author's website.</abstract>
      <bibkey>federmann-2010-appraise</bibkey>
    </paper>
    <paper id="134">
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Chunyu</first><last>Kit</last></author>
      <title>How Large a Corpus Do We Need: Statistical Method Versus Rule-based Method</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/199_Paper.pdf</url>
      <abstract>We investigate the impact of input data scale in corpus-based learning using a study style of Zipfs law. In our research, Chinese word segmentation is chosen as the study case and a series of experiments are specially conducted for it, in which two types of segmentation techniques, statistical learning and rule-based methods, are examined. The empirical results show that a linear performance improvement in statistical learning requires an exponential increasing of training corpus size at least. As for the rule-based method, an approximate negative inverse relationship between the performance and the size of the input lexicon can be observed.</abstract>
      <bibkey>zhao-etal-2010-large</bibkey>
    </paper>
    <paper id="135">
      <author><first>Bolette S.</first><last>Pedersen</last></author>
      <author><first>Sanni</first><last>Nimb</last></author>
      <author><first>Anna</first><last>Braasch</last></author>
      <title>Merging Specialist Taxonomies and Folk Taxonomies in Wordnets - A case Study of Plants, Animals and Foods in the <fixed-case>D</fixed-case>anish <fixed-case>W</fixed-case>ordnet</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/200_Paper.pdf</url>
      <abstract>In this paper we investigate the problem of merging specialist taxonomies with the more intuitive folk taxonomies in lexical-semantic resources like wordnets; and we focus in particular on plants, animals and foods. We show that a traditional dictionary like Den Danske Ordbog (DDO) survives well with several inconsistencies between different taxonomies of the vocabulary and that a restructuring is therefore necessary in order to compile a consistent wordnet resource on its basis. To this end, we apply Cruses definitions for hyponymies, namely those of natural kinds (such as plants and animals) on the one hand and functional kinds (such as foods) on the other. We pursue this distinction in the development of the Danish wordnet, DanNet, which has recently been built on the basis of DDO and is made open source for all potential users at www.wordnet.dk. Not surprisingly, we conclude that cultural background influences the structure of folk taxonomies quite radically, and that wordnet builders must therefore consider these carefully in order to capture their central characteristics in a systematic way.</abstract>
      <bibkey>pedersen-etal-2010-merging</bibkey>
    </paper>
    <paper id="136">
      <author><first>Marta</first><last>Tatu</last></author>
      <author><first>Dan</first><last>Moldovan</last></author>
      <title>Inducing Ontologies from Folksonomies using Natural Language Understanding</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/203_Paper.pdf</url>
      <abstract>Folksonomies are unsystematic, unsophisticated collections of keywords associated by social bookmarking users to web content and, despite their inconsistency problems (typographical errors, spelling variations, use of space or punctuation as delimiters, same tag applied in different context, synonymy of concepts, etc.), their popularity is increasing among Web 2.0 application developers. In this paper, in addition to eliminating folksonomic irregularities existing at the lexical, syntactic or semantic understanding levels, we propose an algorithm that automatically builds a semantic representation of the folksonomy by exploiting the tags, their social bookmarking associations (co-occuring tags) and, more importantly, the content of labeled documents. We derive the semantics of each tag, discover semantic links between the folksonomic tags and expose the underlying semantic structure of the folksonomy, thus, enabling a number of information discovery and ontology-based reasoning applications.</abstract>
      <bibkey>tatu-moldovan-2010-inducing</bibkey>
    </paper>
    <paper id="137">
      <author><first>Orphée</first><last>De Clercq</last></author>
      <author><first>Maribel Montero</first><last>Perez</last></author>
      <title>Data Collection and <fixed-case>IPR</fixed-case> in Multilingual Parallel Corpora. <fixed-case>D</fixed-case>utch Parallel Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/204_Paper.pdf</url>
      <abstract>After three years of work the Dutch Parallel Corpus (DPC) project has reached an end. The finalized corpus is a ten-million-word high-quality sentence-aligned bidirectional parallel corpus of Dutch, English and French, with Dutch as central language. In this paper we present the corpus and try to formulate some basic data collection principles, based on the work that was carried out for the project. Building a corpus is a difficult and time-consuming task, especially when every text sample included has to be cleared from copyrights. The DPC is balanced according to five text types (literature, journalistic texts, instructive texts, administrative texts and texts treating external communication) and four translation directions (Dutch-English, English-Dutch, Dutch-French and French-Dutch). All the text material was cleared from copyrights. The data collection process necessitated the involvement of different text providers, which resulted in drawing up four different licence agreements. Problems such as an unknown source language, copyright issues and changes to the corpus design are discussed in close detail and illustrated with examples so as to be of help to future corpus compilers.</abstract>
      <bibkey>de-clercq-perez-2010-data</bibkey>
    </paper>
    <paper id="138">
      <author><first>Agata</first><last>Cybulska</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Event Models for Historical Perspectives: Determining Relations between High and Low Level Events in Text, Based on the Classification of Time, Location and Participants.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/205_Paper.pdf</url>
      <abstract>In this paper, we report on a study that was performed within the Semantics of History project on how descriptions of historical events are realized in different types of text and what the implications are for modeling the event information. We believe that different historical perspectives of writers correspond in some degree with genre distinction and correlate with variation in language use. To capture differences between event representations in diverse text types and thus to identify relations between historical events, we defined an event model. We observed clear relations between particular parts of event descriptions - actors, time and location modifiers. Texts, written shortly after an event happened, use more specific and uniquely occurring event descriptions than texts describing the same events but written from a longer time perspective. We carried out some statistical corpus research to confirm this hypothesis. The ability to automatically determine relations between historical events and their sub-events over textual data, based on the relations between event participants, time markers and locations, will have important repercussions for the design of historical information retrieval systems.</abstract>
      <bibkey>cybulska-vossen-2010-event</bibkey>
    </paper>
    <paper id="139">
      <author><first>Christian</first><last>Scheible</last></author>
      <title>An Evaluation of Predicate Argument Clustering using Pseudo-Disambiguation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/206_Paper.pdf</url>
      <abstract>Schulte im Walde et al. (2008) presented a novel approach to semantic verb classication. The predicate argument model (PAC) presented in their paper models selectional preferences by using soft clustering that incorporates the Expectation Maximization (EM) algorithm and the MDL principle. In this paper, I will show how the model handles the task of differentiating between plausible and implau- sible combinations of verbs, subcategorization frames and arguments by applying the pseudo-disambiguation evaluation method. The predicate argument clustering model will be evaluated in comparison with the latent semantic clustering model by Rooth et al. (1999). In particular, the influences of the model parameters, data frequency, and the individual components of the predicate argument model are examined. The results of these experiments show that (i) the selectional preference model overgeneralizes over arguments for the purpose of a pseudo-disambiguation task and that (ii) pseudo-disambiguation should not be used as a universal indicator for the quality of a model.</abstract>
      <bibkey>scheible-2010-evaluation</bibkey>
    </paper>
    <paper id="140">
      <author><first>Fabienne</first><last>Venant</last></author>
      <title>Meaning Representation: From Continuity to Discreteness</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/207_Paper.pdf</url>
      <abstract>This paper presents a geometric approach to meaning representation within the framework of continuous mathematics. Meaning representation is a central issue in Natural Language Processing, in particular for tasks like word sense disambiguation or information extraction. We want here to discuss the relevance of using continuous models in semantics. We dont want to argue the continuous or discrete nature of lexical meaning. We use continuity as a tool to access and manipulate lexical meaning. Following Victorri (1994), we assume that continuity or discreteness are not properties of phenomena but characterizations of theories upon phenomena. We briefly describe our theoretical framework, the dynamical construction of meaning (Victorri and Fuchs, 1996), then present the way we automatically build continuous semantic spaces from a graph of synonymy and discuss their relevance and utility. We also think that discreteness and continuity can collaborate. We show here how we can complete our geometric representations with informations from discrete descriptions of meaning.</abstract>
      <bibkey>venant-2010-meaning</bibkey>
    </paper>
    <paper id="141">
      <author><first>Matthieu</first><last>Vernier</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <title>Learning Subjectivity Phrases missing from Resources through a Large Set of Semantic Tests</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/208_Paper.pdf</url>
      <abstract>In recent years, blogs and social networks have particularly boosted interests for opinion mining research. In order to satisfy real-scale applicative needs, a main task is to create or to enhance lexical and semantic resources on evaluative language. Classical resources of the area are mostly built for english, they contain simple opinion word markers and are far to cover the lexical richness of this linguistic phenomenon. In particular, infrequent subjective words, idiomatic expressions, and cultural stereotypes are missing from resources. We propose a new method, applied on french, to enhance automatically an opinion word lexicon. This learning method relies on linguistic uses of internet users and on semantic tests to infer the degree of subjectivity of many new adjectives, nouns, verbs, noun phrases, verbal phrases which are usually forgotten by other resources. The final appraisal lexicon contains 3,456 entries. We evaluate the lexicon enhancement with and without textual context.</abstract>
      <bibkey>vernier-etal-2010-learning</bibkey>
    </paper>
    <paper id="142">
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Oliviero</first><last>Stock</last></author>
      <title>Evaluation Metrics for Persuasive <fixed-case>NLP</fixed-case> with <fixed-case>G</fixed-case>oogle <fixed-case>A</fixed-case>d<fixed-case>W</fixed-case>ords</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/209_Paper.pdf</url>
      <abstract>Evaluating systems and theories about persuasion represents a bottleneck for both theoretical and applied fields: experiments are usually expensive and time consuming. Still, measuring the persuasive impact of a message is of paramount importance. In this paper we present a new ``cheap and fast'' methodology for measuring the persuasiveness of communication. This methodology allows conducting experiments with thousands of subjects for a few dollars in a few hours, by tweaking and using existing commercial tools for advertising on the web, such as Google AdWords. The central idea is to use AdWords features for defining message persuasiveness metrics. Along with a description of our approach we provide some pilot experiments, conducted both with text and image based ads, that confirm the effectiveness of our ideas. We also discuss the possible application of research on persuasive systems to Google AdWords in order to add more flexibility in the wearing out of persuasive messages.</abstract>
      <bibkey>guerini-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="143">
      <author><first>Bart</first><last>Desmet</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Towards a Balanced Named Entity Corpus for <fixed-case>D</fixed-case>utch</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/210_Paper.pdf</url>
      <abstract>This paper introduces a new named entity corpus for Dutch. State-of-the-art named entity recognition systems require a substantial annotated corpus to be trained on. Such corpora exist for English, but not for Dutch. The STEVIN-funded SoNaR project aims to produce a diverse 500-million-word reference corpus of written Dutch, with four semantic annotation layers: named entities, coreference relations, semantic roles and spatiotemporal expressions. A 1-million-word subset will be manually corrected. Named entity annotation guidelines for Dutch were developed, adapted from the MUC and ACE guidelines. Adaptations include the annotation of products and events, the classification into subtypes, and the markup of metonymic usage. Inter-annotator agreement experiments were conducted to corroborate the reliability of the guidelines, which yielded satisfactory results (Kappa scores above 0.90). We are building a NER system, trained on the 1-million-word subcorpus, to automatically classify the remainder of the SoNaR corpus. To this end, experiments with various classification algorithms (MBL, SVM, CRF) and features have been carried out and evaluated.</abstract>
      <bibkey>desmet-hoste-2010-towards</bibkey>
    </paper>
    <paper id="144">
      <author><first>Grégory</first><last>Senay</last></author>
      <author><first>Georges</first><last>Linarès</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Stanislas</first><last>Oger</last></author>
      <author><first>Thierry</first><last>Michel</last></author>
      <title>Transcriber Driving Strategies for Transcription Aid System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/211_Paper.pdf</url>
      <abstract>Speech recognition technology suffers from a lack of robustness which limits its usability for fully automated speech-to-text transcription, and manual correction is generally required to obtain perfect transcripts. In this paper, we propose a general scheme for semi-automatic transcription, in which the system and the transcriptionist contribute jointly to the speech transcription. The proposed system relies on the editing of confusion networks and on reactive decoding, the latter one being supposed to take benefits from the manual correction and improve the error rates. In order to reduce the correction time, we evaluate various strategies aiming to guide the transcriptionist towards the critical areas of transcripts. These strategies are based on graph density-based criterion and two semantic consistency criterion; using a corpus-based method and a web-search engine. They allow to indicate to the user the areas which present severe lacks of understandability. We evaluate these driving strategies by simulating the correction process of French broadcast news transcriptions. Results show that interactive decoding improves the correction act efficiency with all driving strategies and semantic information must be integrated into the interactive decoding process.</abstract>
      <bibkey>senay-etal-2010-transcriber</bibkey>
    </paper>
    <paper id="145">
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Tomislava</first><last>Lauc</last></author>
      <author><first>Damir</first><last>Boras</last></author>
      <title>Building a Gold Standard for Event Detection in <fixed-case>C</fixed-case>roatian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/213_Paper.pdf</url>
      <abstract>This paper describes the process of building a newspaper corpus annotated with events described in specific documents. The main difference to the corpora built as part of the TDT initiative is that documents are not annotated by topics, but by specific events they describe. Additionally, documents are gathered from sixteen sources and all documents in the corpus are annotated with the corresponding event. The annotation process consists of a browsing and a searching step. Experiments are performed with a threshold that could be used in the browsing step yielding the result of having to browse through only 1% of document pairs for a 2% loss of relevant document pairs. A statistical analysis of the annotated corpus is undertaken showing that most events are described by few documents while just some events are reported by many documents. The inter-annotator agreement measures show high agreement concerning grouping documents into event clusters, but show a much lower agreement concerning the number of events the documents are organized into. An initial experiment is described giving a baseline for further research on this corpus.</abstract>
      <bibkey>ljubesic-etal-2010-building</bibkey>
    </paper>
    <paper id="146">
      <author><first>Ziqi</first><last>Zhang</last></author>
      <author><first>José</first><last>Iria</last></author>
      <author><first>Fabio</first><last>Ciravegna</last></author>
      <title>Improving Domain-specific Entity Recognition with Automatic Term Recognition and Feature Extraction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/214_Paper.pdf</url>
      <abstract>Domain specific entity recognition often relies on domain-specific knowledge to improve system performance. However, such knowledge often suffers from limited domain portability and is expensive to build and maintain. Therefore, obtaining it in a generic and unsupervised manner would be a desirable feature for domain-specific entity recognition systems. In this paper, we introduce an approach that exploits domain-specificity of words as a form of domain-knowledge for entity-recognition tasks. Compared to prior work in the field, our approach is generic and completely unsupervised. We empirically show an improvement in entity extraction accuracy when features derived by our unsupervised method are used, with respect to baseline methods that do not employ domain knowledge. We also compared the results against those of existing systems that use manually crafted domain knowledge, and found them to be competitive.</abstract>
      <bibkey>zhang-etal-2010-improving</bibkey>
    </paper>
    <paper id="147">
      <author><first>Jana</first><last>Straková</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <title><fixed-case>C</fixed-case>zech Information Retrieval with Syntax-based Language Models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/215_Paper.pdf</url>
      <abstract>In recent years, considerable attention has been dedicated to language modeling methods in information retrieval. Although these approaches generally allow exploitation of any type of language model, most of the published experiments were conducted with a classical n-gram model, usually limited only to unigrams. A few works exploiting syntax in information retrieval can be cited in this context, but significant contribution of syntax based language modeling for information retrieval is yet to be proved. In this paper, we propose, implement, and evaluate an enrichment of language model employing syntactic dependency information acquired automatically from both documents and queries. Our experiments are conducted on Czech which is a morphologically rich language and has a considerably free word order, therefore a syntactic language model is expected to contribute positively to the unigram and bigram language model based on surface word order. By testing our model on the Czech test collection from Cross Language Evaluation Forum 2007 Ad-Hoc track, we show positive contribution of using dependency syntax in this context.</abstract>
      <bibkey>strakova-pecina-2010-czech</bibkey>
    </paper>
    <paper id="148">
      <author><first>Giuseppe</first><last>Attardi</last></author>
      <author><first>Stefano Dei</first><last>Rossi</last></author>
      <author><first>Giulia</first><last>Di Pietro</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <author><first>Maria</first><last>Simi</last></author>
      <title>A Resource and Tool for Super-sense Tagging of <fixed-case>I</fixed-case>talian Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/216_Paper.pdf</url>
      <abstract>A SuperSense Tagger is a tool for the automatic analysis of texts that associates to each noun, verb, adjective and adverb a semantic category within a general taxonomy. The developed tagger, based on a statistical model (Maximum Entropy), required the creation of an Italian annotated corpus, to be used as a training set, and the improvement of various existing tools. The obtained results significantly improved the current state-of-the art for this particular task.</abstract>
      <bibkey>attardi-etal-2010-resource</bibkey>
    </paper>
    <paper id="149">
      <author><first>Izaskun</first><last>Aldezabal</last></author>
      <author><first>María Jesús</first><last>Aranzabe</last></author>
      <author><first>Arantza</first><last>Díaz de Ilarraza</last></author>
      <author><first>Ainara</first><last>Estarrona</last></author>
      <title>Building the <fixed-case>B</fixed-case>asque <fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/217_Paper.pdf</url>
      <abstract>This paper presents the work that has been carried out to annotate semantic roles in the Basque Dependency Treebank (BDT). We will describe the resources we have used and the way the annotation of 100 verbs has been done. We decide to follow the model proposed in the PropBank project that has been deployed in other languages, such as Chinese, Spanish, Catalan and Russian. The resources used are: an in-house database with syntactic/semantic subcategorization frames for Basque verbs, an English-Basque verb mapping based on Levins classification and the BDT itself. Detailed guidelines for human taggers have been established as a result of this annotation process. In addition, we have characterized the information associated to the semantic tag. Besides, and based on this study, we will define semi-automatic procedures that will facilitate the task of manual annotation for the rest of the verbs of the Treebank. We have also adapted AbarHitz, a tool used in the construction of the BDT, for the task of annotating semantic roles according to the proposed characterization.</abstract>
      <bibkey>aldezabal-etal-2010-building</bibkey>
    </paper>
    <paper id="150">
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Christine</first><last>Meunier</last></author>
      <author><first>Irina</first><last>Nesterenko</last></author>
      <author><first>Roxane</first><last>Bertrand</last></author>
      <title>Automatic Detection of Syllable Boundaries in Spontaneous Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/219_Paper.pdf</url>
      <abstract>This paper presents the outline and performance of an automatic syllable boundary detection system. The syllabification of phonemes is performed with a rule-based system, implemented in a Java program. Phonemes are categorized into 6 classes. A set of specific rules are developed and categorized as general rules which can be applied in all cases, and exception rules which are applied in some specific situations. These rules deal with a French spontaneous speech corpus. Moreover, the proposed phonemes, classes and rules are listed in an external configuration file of the tool (under GPL licence) that make the tool very easy to adapt to a specific corpus by adding or modifying rules, phoneme encoding or phoneme classes, by the use of a new configuration file. Finally, performances are evaluated and compared to 3 other French syllabification systems and show significant improvements. Automatic system output and expert's syllabification are in agreement for most of syllable boundaries in our corpus.</abstract>
      <bibkey>bigi-etal-2010-automatic</bibkey>
    </paper>
    <paper id="151">
      <author><first>Nikos</first><last>Tsourakis</last></author>
      <author><first>Agnes</first><last>Lisowska</last></author>
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <title>Examining the Effects of Rephrasing User Input on Two Mobile Spoken Language Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/220_Paper.pdf</url>
      <abstract>During the construction of a spoken dialogue system much effort is spent on improving the quality of speech recognition as possible. However, even if an application perfectly recognizes the input, its understanding may be far from what the user originally meant. The user should be informed about what the system actually understood so that an error will not have a negative impact in the later stages of the dialogue. One important aspect that this work tries to address is the effect of presenting the systems understanding during interaction with users. We argue that for specific kinds of applications its important to confirm the understanding of the system before obtaining the output. In this way the user can avoid misconceptions and problems occurring in the dialogue flow and he can enhance his confidence in the system. Nevertheless this has an impact on the interaction, as the mental workload increases, and the users behavior may adapt to the systems coverage. We focus on two applications that implement the notion of rephrasing users input in a different way. Our study took place among 14 subjects that used both systems on a Nokia N810 Internet Tablet.</abstract>
      <bibkey>tsourakis-etal-2010-examining</bibkey>
    </paper>
    <paper id="152">
      <author><first>Samuel</first><last>Reese</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>Lluís</first><last>Padró</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <title><fixed-case>W</fixed-case>ikicorpus: A Word-Sense Disambiguated Multilingual <fixed-case>W</fixed-case>ikipedia Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/222_Paper.pdf</url>
      <abstract>This article presents a new freely available trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia and has been automatically enriched with linguistic information. To our knowledge, this is the largest such corpus that is freely available to the community: In its present version, it contains over 750 million words. The corpora have been annotated with lemma and part of speech information using the open source library FreeLing. Also, they have been sense annotated with the state of the art Word Sense Disambiguation algorithm UKB. As UKB assigns WordNet senses, and WordNet has been aligned across languages via the InterLingual Index, this sort of annotation opens the way to massive explorations in lexical semantics that were not possible before. We present a first attempt at creating a trilingual lexical resource from the sense-tagged Wikipedia corpora, namely, WikiNet. Moreover, we present two by-products of the project that are of use for the NLP community: An open source Java-based parser for Wikipedia pages developed for the construction of the corpus, and the integration of the WSD algorithm UKB in FreeLing.</abstract>
      <bibkey>reese-etal-2010-wikicorpus</bibkey>
    </paper>
    <paper id="153">
      <author><first>Markus</first><last>Dickinson</last></author>
      <author><first>Charles</first><last>Jochim</last></author>
      <title>Evaluating Distributional Properties of Tagsets</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/227_Paper.pdf</url>
      <abstract>We investigate which distributional properties should be present in a tagset by examining different mappings of various current part-of-speech tagsets, looking at English, German, and Italian corpora. Given the importance of distributional information, we present a simple model for evaluating how a tagset mapping captures distribution, specifically by utilizing a notion of frames to capture the local context. In addition to an accuracy metric capturing the internal quality of a tagset, we introduce a way to evaluate the external quality of tagset mappings so that we can ensure that the mapping retains linguistically important information from the original tagset. Although most of the mappings we evaluate are motivated by linguistic concerns, we also explore an automatic, bottom-up way to define mappings, to illustrate that better distributional mappings are possible. Comparing our initial evaluations to POS tagging results, we find that more distributional tagsets can sometimes result in worse accuracy, underscring the need to carefully define the properties of a tagset.</abstract>
      <bibkey>dickinson-jochim-2010-evaluating</bibkey>
    </paper>
    <paper id="154">
      <author><first>Maxim</first><last>Khalilov</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <author><first>Inguna</first><last>Skadin̨a</last></author>
      <author><first>Edgars</first><last>Brālītis</last></author>
      <author><first>Lauma</first><last>Pretkalnin̨a</last></author>
      <title>Towards Improving <fixed-case>E</fixed-case>nglish-<fixed-case>L</fixed-case>atvian Translation: A System Comparison and a New Rescoring Feature</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/228_Paper.pdf</url>
      <abstract>Translation into the languages with relatively free word order has received a lot less attention than translation into fixed word order languages (English), or into analytical languages (Chinese). At the same time this translation task is found among the most difficult challenges for machine translation (MT), and intuitively it seems that there is some space in improvement intending to reflect the free word order structure of the target language. This paper presents a comparative study of two alternative approaches to statistical machine translation (SMT) and their application to a task of English-to-Latvian translation. Furthermore, a novel feature intending to reflect the relatively free word order scheme of the Latvian language is proposed and successfully applied on the n-best list rescoring step. Moving beyond classical automatic scores of translation quality that are classically presented in MT research papers, we contribute presenting a manual error analysis of MT systems output that helps to shed light on advantages and disadvantages of the SMT systems under consideration.</abstract>
      <bibkey>khalilov-etal-2010-towards</bibkey>
    </paper>
    <paper id="155">
      <author><first>Grigori</first><last>Sidorov</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>S</fixed-case>panish Large Statistical Dictionary of Inflectional Forms</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/229_Paper.pdf</url>
      <abstract>The paper presents an approach for constructing a weighted bilingual dictionary of inflectional forms using as input data a traditional bilingual dictionary, and not parallel corpora. An algorithm is developed that generates all possible morphological (inflectional) forms and weights them using information on distribution of corresponding grammar sets (grammar information) in large corpora for each language. The algorithm also takes into account the compatibility of grammar sets in a language pair; for example, verb in past tense in language L normally is expected to be translated by verb in past tense in Language L'. We consider that the developed method is universal, i.e. can be applied to any pair of languages. The obtained dictionary is freely available. It can be used in several NLP tasks, for example, statistical machine translation.</abstract>
      <bibkey>sidorov-etal-2010-english</bibkey>
    </paper>
    <paper id="156">
      <author><first>Fernando</first><last>Fernández-Martínez</last></author>
      <author><first>Juan Manuel</first><last>Lucas-Cuesta</last></author>
      <author><first>Roberto Barra</first><last>Chicote</last></author>
      <author><first>Javier</first><last>Ferreiros</last></author>
      <author><first>Javier</first><last>Macías-Guarasa</last></author>
      <title><fixed-case>HIFI</fixed-case>-<fixed-case>AV</fixed-case>: An Audio-visual Corpus for Spoken Language Human-Machine Dialogue Research in <fixed-case>S</fixed-case>panish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/230_Paper.pdf</url>
      <abstract>In this paper, we describe a new multi-purpose audio-visual database on the context of speech interfaces for controlling household electronic devices. The database comprises speech and video recordings of 19 speakers interacting with a HIFI audio box by means of a spoken dialogue system. Dialogue management is based on Bayesian Networks and the system is provided with contextual information handling strategies. Each speaker was requested to fulfil different sets of specific goals following predefined scenarios, according to both different complexity levels and degrees of freedom or initiative allowed to the user. Due to a careful design and its size, the recorded database allows comprehensive studies on speech recognition, speech understanding, dialogue modeling and management, microphone array based speech processing, and both speech and video-based acoustic source localisation. The database has been labelled for quality and efficiency studies on dialogue performance. The whole database has been validated through both objective and subjective tests.</abstract>
      <bibkey>fernandez-martinez-etal-2010-hifi</bibkey>
    </paper>
    <paper id="157">
      <author><first>Joana</first><last>Hois</last></author>
      <title>Inter-Annotator Agreement on a Linguistic Ontology for Spatial Language - A Case Study for <fixed-case>GUM</fixed-case>-Space</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/231_Paper.pdf</url>
      <abstract>In this paper, we present a case study for measuring inter-annotator agreement on a linguistic ontology for spatial language, namely the spatial extension of the Generalized Upper Model. This linguistic ontology specifies semantic categories, and it is used in dialogue systems for natural language of space in the context of human-computer interaction and spatial assistance systems. Its core representation for spatial language distinguishes how sentences can be structured and categorized into units that contribute certain meanings to the expression. This representation is here evaluated in terms of inter-annotator agreement: four uninformed annotators were instructed by a manual how to annotate sentences with the linguistic ontology. They have been assigned to annotate 200 sentences with varying length and complexity. Their resulting agreements are calculated together with our own 'expert annotation' of the same sentences. We show that linguistic ontologies can be evaluated with respect to inter-annotator agreement, and we present encouraging results of calculating agreements for the spatial extension of the Generalized Upper Model.</abstract>
      <bibkey>hois-2010-inter</bibkey>
    </paper>
    <paper id="158">
      <author><first>Dekang</first><last>Lin</last></author>
      <author><first>Kenneth</first><last>Church</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <author><first>Shane</first><last>Bergsma</last></author>
      <author><first>Kailash</first><last>Patil</last></author>
      <author><first>Emily</first><last>Pitler</last></author>
      <author><first>Rachel</first><last>Lathbury</last></author>
      <author><first>Vikram</first><last>Rao</last></author>
      <author><first>Kapil</first><last>Dalwani</last></author>
      <author><first>Sushant</first><last>Narsale</last></author>
      <title>New Tools for Web-Scale N-grams</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/233_Paper.pdf</url>
      <abstract>While the web provides a fantastic linguistic resource, collecting and processing data at web-scale is beyond the reach of most academic laboratories. Previous research has relied on search engines to collect online information, but this is hopelessly inefficient for building large-scale linguistic resources, such as lists of named-entity types or clusters of distributionally similar words. An alternative to processing web-scale text directly is to use the information provided in an N-gram corpus. An N-gram corpus is an efficient compression of large amounts of text. An N-gram corpus states how often each sequence of words (up to length N) occurs. We propose tools for working with enhanced web-scale N-gram corpora that include richer levels of source annotation, such as part-of-speech tags. We describe a new set of search tools that make use of these tags, and collectively lower the barrier for lexical learning and ambiguity resolution at web-scale. They will allow novel sources of information to be applied to long-standing natural language challenges.</abstract>
      <bibkey>lin-etal-2010-new</bibkey>
    </paper>
    <paper id="159">
      <author><first>Eric</first><last>Auer</last></author>
      <author><first>Albert</first><last>Russel</last></author>
      <author><first>Han</first><last>Sloetjes</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Oliver</first><last>Schreer</last></author>
      <author><first>S.</first><last>Masnieri</last></author>
      <author><first>Daniel</first><last>Schneider</last></author>
      <author><first>Sebastian</first><last>Tschöpel</last></author>
      <title><fixed-case>ELAN</fixed-case> as Flexible Annotation Framework for Sound and Image Processing Detectors</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/234_Paper.pdf</url>
      <abstract>Annotation of digital recordings in humanities research still is, to a large extend, a process that is performed manually. This paper describes the first pattern recognition based software components developed in the AVATecH project and their integration in the annotation tool ELAN. AVATecH (Advancing Video/Audio Technology in Humanities Research) is a project that involves two Max Planck Institutes (Max Planck Institute for Psycholinguistics, Nijmegen, Max Planck Institute for Social Anthropology, Halle) and two Fraunhofer Institutes (Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme IAIS, Sankt Augustin, Fraunhofer Heinrich-Hertz-Institute, Berlin) and that aims to develop and implement audio and video technology for semi-automatic annotation of heterogeneous media collections as they occur in multimedia based research. The highly diverse nature of the digital recordings stored in the archives of both Max Planck Institutes, poses a huge challenge to most of the existing pattern recognition solutions and is a motivation to make such technology available to researchers in the humanities.</abstract>
      <bibkey>auer-etal-2010-elan</bibkey>
    </paper>
    <paper id="160">
      <author><first>Damjan</first><last>Vlaj</last></author>
      <author><first>Aleksandra Zögling</first><last>Markuš</last></author>
      <author><first>Marko</first><last>Kos</last></author>
      <author><first>Zdravko</first><last>Kačič</last></author>
      <title>Acquisition and Annotation of <fixed-case>S</fixed-case>lovenian <fixed-case>L</fixed-case>ombard Speech Database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/235_Paper.pdf</url>
      <abstract>This paper presents the acquisition and annotation of Slovenian Lombard Speech Database, the recording of which started in the year 2008. The database was recorded at the University of Maribor, Slovenia. The goal of this paper is to describe the hardware platform used for the acquisition of speech material, recording scenarios and tools used for the annotation of Slovenian Lombard Speech Database. The database consists of recordings of 10 Slovenian native speakers. Five males and five females were recorded. Each speaker pronounced a set of eight corpuses in two recording sessions with at least one week pause between recordings. The structure of the corpus is similar to SpeechDat II database. Approximately 30 minutes of speech material per speaker and per session was recorded. The manual annotation of speech material is performed with the LombardSpeechLabel tool developed at the University of Maribor. The speech and annotation material was saved on 10 DVDs (one speaker on one DVD).</abstract>
      <bibkey>vlaj-etal-2010-acquisition</bibkey>
    </paper>
    <paper id="161">
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>Marie-Aude</first><last>Lefer</last></author>
      <title>The <fixed-case>M</fixed-case>u<fixed-case>L</fixed-case>e<fixed-case>XF</fixed-case>o<fixed-case>R</fixed-case> Database: Representing Word-Formation Processes in a Multilingual Lexicographic Environment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/236_Paper.pdf</url>
      <abstract>This paper introduces a new lexicographic resource, the MuLeXFoR database, which aims to present word-formation processes in a multilingual environment. Morphological items represent a real challenge for lexicography, especially for the development of multilingual tools. Affixes can take part in several word-formation rules and, conversely, rules can be realised by means of a variety of affixes. Consequently, it is often difficult to provide enough information to help users understand the meaning(s) of an affix or familiarise with the most frequent strategies used to translate the meaning(s) conveyed by affixes. In fact, traditional dictionaries often fail to achieve this goal. The database introduced in this paper tries to take advantage of recent advances in electronic implementation and morphological theory. Word-formation is presented as a set of multilingual rules that users can access via different indexes (affixes, rules and constructed words). MuLeXFoR entries contain, among other things, detailed descriptions of morphological constraints and productivity notes, which are sorely lacking in currently available tools such as bilingual dictionaries.</abstract>
      <bibkey>cartoni-lefer-2010-mulexfor</bibkey>
    </paper>
    <paper id="162">
      <author><first>Fang</first><last>Xu</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <title>Paragraph Acquisition and Selection for List Question Using <fixed-case>A</fixed-case>mazon’s <fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/241_Paper.pdf</url>
      <abstract>Creating more fine-grained annotated data than previously relevent document sets is important for evaluating individual components in automatic question answering systems. In this paper, we describe using the Amazon's Mechanical Turk (AMT) to judge whether paragraphs in relevant documents answer corresponding list questions in TREC QA track 2004. Based on AMT results, we build a collection of 1300 gold-standard supporting paragraphs for list questions. Our online experiments suggested that recruiting more people per task assures better annotation quality. In order to learning true labels from AMT annotations, we investigated three approaches on two datasets with different levels of annotation errors. Experimental studies show that the Naive Bayesian model and EM-based GLAD model can generate results highly agreeing with gold-standard annotations, and dominate significantly over the majority voting method for true label learning. We also suggested setting higher HIT approval rate to assure better online annotation quality, which leads to better performance of learning methods.</abstract>
      <bibkey>xu-klakow-2010-paragraph</bibkey>
    </paper>
    <paper id="163">
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <title>Construction of a <fixed-case>C</fixed-case>hinese Opinion Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/242_Paper.pdf</url>
      <abstract>In this paper, we base on the syntactic structural Chinese Treebank corpus, construct the Chinese Opinon Treebank for the research of opinion analysis. We introduce the tagging scheme and develop a tagging tool for constructing this corpus. Annotated samples are described. Information including opinions (yes or no), their polarities (positive, neutral or negative), types (expression, status, or action), is defined and annotated. In addition, five structure trios are introduced according to the linguistic relations between two Chinese words. Four of them that are possibly related to opinions are also annotated in the constructed corpus to provide the linguistic cues. The number of opinion sentences together with the number of their polarities, opinion types, and trio types are calculated. These statistics are compared and discussed. To know the quality of the annotations in this corpus, the kappa values of the annotations are calculated. The substantial agreement between annotations ensures the applicability and reliability of the constructed corpus.</abstract>
      <bibkey>ku-etal-2010-construction</bibkey>
    </paper>
    <paper id="164">
      <author><first>Takeshi</first><last>Abekawa</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Kyo</first><last>Kageura</last></author>
      <title>Community-based Construction of Draft and Final Translation Corpus Through a Translation Hosting Site Minna no Hon’yaku (<fixed-case>MNH</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/243_Paper.pdf</url>
      <abstract>In this paper we report a way of constructing a translation corpus that contains not only source and target texts, but draft and final versions of target texts, through the translation hosting site Minna no Hon'yaku (MNH). We made MNH publicly available on April 2009. Since then, more than 1,000 users have registered and over 3,500 documents have been translated, as of February 2010, from English to Japanese and from Japanese to English. MNH provides an integrated translation-aid environment, QRedit, which enables translators to look up high-quality dictionaries and Wikipedia as well as to search Google seamlessly. As MNH keeps translation logs, a corpus consisting of source texts, draft translations in several versions, and final translations is constructed naturally through MNH. As of 7 February, 764 documents with multiple translation versions are accumulated, of which 110 are edited by more than one translators. This corpus can be used for self-learning by inexperienced translators on MNH, and potentially for improving machine translation.</abstract>
      <bibkey>abekawa-etal-2010-community</bibkey>
    </paper>
    <paper id="165">
      <author><first>Vamshi</first><last>Ambati</last></author>
      <author><first>Stephan</first><last>Vogel</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <title>Active Learning and Crowd-Sourcing for Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/244_Paper.pdf</url>
      <abstract>Large scale parallel data generation for new language pairs requires intensive human effort and availability of experts. It becomes immensely difficult and costly to provide Statistical Machine Translation (SMT) systems for most languages due to the paucity of expert translators to provide parallel data. Even if experts are present, it appears infeasible due to the impending costs. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.</abstract>
      <bibkey>ambati-etal-2010-active</bibkey>
    </paper>
    <paper id="166">
      <author><first>Hercules</first><last>Dalianis</last></author>
      <author><first>Sumithra</first><last>Velupillai</last></author>
      <title>How Certain are Clinical Assessments? Annotating <fixed-case>S</fixed-case>wedish Clinical Text for (Un)certainties, Speculations and Negations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/245_Paper.pdf</url>
      <abstract>Clinical texts contain a large amount of information. Some of this information is embedded in contexts where e.g. a patient status is reasoned about, which may lead to a considerable amount of statements that indicate uncertainty and speculation. We believe that distinguishing such instances from factual statements will be very beneficial for automatic information extraction. We have annotated a subset of the Stockholm Electronic Patient Record Corpus for certain and uncertain expressions as well as speculative and negation keywords, with the purpose of creating a resource for the development of automatic detection of speculative language in Swedish clinical text. We have analyzed the results from the initial annotation trial by means of pairwise Inter-Annotator Agreement (IAA) measured with F-score. Our main findings are that IAA results for certain expressions and negations are very high, but for uncertain expressions and speculative keywords results are less encouraging. These instances need to be defined in more detail. With this annotation trial, we have created an important resource that can be used to further analyze the properties of speculative language in Swedish clinical text. Our intention is to release this subset to other research groups in the future after removing identifiable information.</abstract>
      <bibkey>dalianis-velupillai-2010-certain</bibkey>
    </paper>
    <paper id="167">
      <author><first>Winston</first><last>Anderson</last></author>
      <author><first>Laurette</first><last>Pretorius</last></author>
      <author><first>Albert</first><last>Kotzé</last></author>
      <title>Base Concepts in the <fixed-case>A</fixed-case>frican Languages Compared to Upper Ontologies and the <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Top Ontology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/247_Paper.pdf</url>
      <abstract>Ontologies, and in particular upper ontologies, are foundational to the establishment of the Semantic Web. Upper ontologies are used as equivalence formalisms between domain specific ontologies. Multilingualism brings one of the key challenges to the development of these ontologies. Fundamental to the challenges of defining upper ontologies is the assumption that concepts are universally shared. The approach to developing linguistic ontologies aligned to upper ontologies, particularly in the non-Indo-European language families, has highlighted these challenges. Previously two approaches to developing new linguistic ontologies and the influence of these approaches on the upper ontologies have been well documented. These approaches are examined in a unique new context: the African, and in particular, the Bantu languages. In particular, we address the following two questions: Which approach is better for the alignment of the African languages to upper ontologies? Can the concepts that are linguistically shared amongst the African languages be aligned easily with upper ontology concepts claimed to be universally shared?</abstract>
      <bibkey>anderson-etal-2010-base</bibkey>
    </paper>
    <paper id="168">
      <author><first>Keyan</first><last>Zhou</last></author>
      <author><first>Aijun</first><last>Li</last></author>
      <author><first>Zhigang</first><last>Yin</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <title><fixed-case>CASIA</fixed-case>-<fixed-case>CASSIL</fixed-case>: a <fixed-case>C</fixed-case>hinese Telephone Conversation Corpus in Real Scenarios with Multi-leveled Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/248_Paper.pdf</url>
      <abstract>CASIA-CASSIL is a large-scale corpus base of Chinese human-human naturally-occurring telephone conversations in restricted domains. The first edition consists of 792 90-second conversations belonging to tourism domain, which are selected from 7,639 spontaneous telephone recordings in real scenarios. The corpus is now being annotated with wide range of linguistic and paralinguistic information in multi-levels. The annotations include Turns, Speaker Gender, Orthographic Transcription, Chinese Syllable, Chinese Phonetic Transcription, Prosodic Boundary, Stress of Sentence, Non-Speech Sounds, Voice Quality, Topic, Dialog-act and Adjacency Pairs, Ill-formedness, and Expressive Emotion as well, 13 levels in total. The abundant annotation will be effective especially for studying Chinese spoken language phenomena. This paper describes the whole process to build the conversation corpus, including collecting and selecting the original data, and the follow-up process such as transcribing, annotating, and so on. CASIA-CASSIL is being extended to a large scale corpus base of annotated Chinese dialogs for spoken Chinese study.</abstract>
      <bibkey>zhou-etal-2010-casia</bibkey>
    </paper>
    <paper id="169">
      <author><first>Kwanchiva</first><last>Saykham</last></author>
      <author><first>Ananlada</first><last>Chotimongkol</last></author>
      <author><first>Chai</first><last>Wutiwiwatchai</last></author>
      <title>Online Temporal Language Model Adaptation for a <fixed-case>T</fixed-case>hai Broadcast News Transcription System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/249_Paper.pdf</url>
      <abstract>This paper investigates the effectiveness of online temporal language model adaptation when applied to a Thai broadcast news transcription task. Our adaptation scheme works as follow: first an initial language model is trained with broadcast news transcription available during the development period. Then the language model is adapted over time with more recent broadcast news transcription and online news articles available during deployment especially the data from the same time period as the broadcast news speech being recognized. We found that the data that are closer in time are more similar in terms of perplexity and are more suitable for language model adaptation. The LMs that are adapted over time with more recent news data are better, both in terms of perplexity and WER, than the static LM trained from only the initial set of broadcast news data. Adaptation data from broadcast news transcription improved perplexity by 38.3% and WER by 7.1% relatively. Though, online news articles achieved less improvement, it is still a useful resource as it can be obtained automatically. Better data pre-processing techniques and data selection techniques based on text similarity could be applied to the news articles to obtain further improvement from this promising result.</abstract>
      <bibkey>saykham-etal-2010-online</bibkey>
    </paper>
    <paper id="170">
      <author><first>Ruud</first><last>Koolen</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <title>The <fixed-case>D</fixed-case>-<fixed-case>TUNA</fixed-case> Corpus: A <fixed-case>D</fixed-case>utch Dataset for the Evaluation of Referring Expression Generation Algorithms</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/251_Paper.pdf</url>
      <abstract>We present the D-TUNA corpus, which is the first semantically annotated corpus of referring expressions in Dutch. Its primary function is to evaluate and improve the performance of REG algorithms. Such algorithms are computational models that automatically generate referring expressions by computing how a specific target can be identified to an addressee by distinguishing it from a set of distractor objects. We performed a large-scale production experiment, in which participants were asked to describe furniture items and people, and provided all descriptions with semantic information regarding the target and the distractor objects. Besides being useful for evaluating REG algorithms, the corpus addresses several other research goals. Firstly, the corpus contains both written and spoken referring expressions uttered in the direction of an addressee, which enables systematic analyses of how modality (text or speech) influences the human production of referring expressions. Secondly, due to its comparability with the English TUNA corpus, our Dutch corpus can be used to explore the differences between Dutch and English speakers regarding the production of referring expressions.</abstract>
      <bibkey>koolen-krahmer-2010-tuna</bibkey>
    </paper>
    <paper id="171">
      <author><first>Aina</first><last>Peris</last></author>
      <author><first>Mariona</first><last>Taulé</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <author><first>Horacio</first><last>Rodríguez</last></author>
      <title><fixed-case>ADN</fixed-case>-Classifier:Automatically Assigning Denotation Types to Nominalizations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/252_Paper.pdf</url>
      <abstract>This paper presents the ADN-Classifier, an Automatic classification system of Spanish Deverbal Nominalizations aimed at identifying its semantic denotation (i.e. event, result, underspecified, or lexicalized). The classifier can be used for NLP tasks such as coreference resolution or paraphrase detection. To our knowledge, the ADN-Classifier is the first effort in acquisition of denotations for nominalizations using Machine Learning.We compare the results of the classifier when using a decreasing number of Knowledge Sources, namely (1) the complete nominal lexicon (AnCora-Nom) that includes sense distictions, (2) the nominal lexicon (AnCora-Nom) removing the sense-specific information, (3) nominalizations context information obtained from a treebank corpus (AnCora-Es) and (4) the combination of the previous linguistic resources. In a realistic scenario, that is, without sense distinction, the best results achieved are those taking into account the information declared in the lexicon (89.40% accuracy). This shows that the lexicon contains crucial information (such as argument structure) that corpus-derived features cannot substitute for.</abstract>
      <bibkey>peris-etal-2010-adn</bibkey>
    </paper>
    <paper id="172">
      <author><first>Lene</first><last>Antonsen</last></author>
      <author><first>Trond</first><last>Trosterud</last></author>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <title>Reusing Grammatical Resources for New Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/254_Paper.pdf</url>
      <abstract>Grammatical approaches to language technology are often considered less optimal than statistical approaches in multilingual settings, where large-scale portability becomes an important issue. The present paper argues that there is a notable gain in reusing grammatical resources when porting technology to new languages. The pivot language is North Sámi, and the paper discusses portability with respect to the closely related Lule and South Sámi, and to the unrelated Faroese and Greenlandic languages.</abstract>
      <bibkey>antonsen-etal-2010-reusing</bibkey>
    </paper>
    <paper id="173">
      <author><first>Line</first><last>Adde</last></author>
      <author><first>Torbjørn</first><last>Svendsen</last></author>
      <title><fixed-case>N</fixed-case>ame<fixed-case>D</fixed-case>at: A Database of <fixed-case>E</fixed-case>nglish Proper Names Spoken by Native Norwegians</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/255_Paper.pdf</url>
      <abstract>This paper describes the design and collection of NameDat, a database containing English proper names spoken by native Norwegians. The database was designed to cover the typical acoustic and phonetic variations that appear when Norwegians pronounce English names. The intended use of the database is acoustic and lexical modeling of these phonetic variations. The English names in the database have been enriched with several annotation tiers. The recorded names were selected according to three selection criteria: the familiarity of the name, the expected recognition performance and the coverage of non-native phonemes. The validity of the manual annotations was verified by means of an automatic recognition experiment of non-native names. The experiment showed that the use of the manual transcriptions from NameDat yields an increase in recognition performance over automatically generated transcriptions.</abstract>
      <bibkey>adde-svendsen-2010-namedat</bibkey>
    </paper>
    <paper id="174">
      <author><first>Natalie D.</first><last>Snoeren</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <title>The Study of Writing Variants in an Under-resourced Language: Some Evidence from Mobile N-Deletion in <fixed-case>L</fixed-case>uxembourgish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/258_Paper.pdf</url>
      <abstract>The national language of the Grand-Duchy of Luxembourg, Luxembourgish, has often been characterized as one of Europe's under-described and under-resourced languages. Because of a limited written production of Luxembourgish, poorly observed writing standardization (as compared to other languages such as English and French) and a large diversity of spoken varieties, the study of Luxembourgish poses many interesting challenges to automatic speech processing studies as well as to linguistic enquiries. In the present paper, we make use of large corpora to focus on typical writing and derived pronunciation variants in Luxembourgish, elicited by mobile -n deletion (hereafter shortened to MND). Using transcriptions from the House of Parliament debates and 10k words from news reports, we examine the reality of MND variants in written transcripts of speech. The goal of this study is manyfold: quantify the potential of variation due to MND in written Luxembourgish, check the mandatory status of the MND rule and discuss the arising problems for automatic spoken Luxembourgish processing.</abstract>
      <bibkey>snoeren-etal-2010-study</bibkey>
    </paper>
    <paper id="175">
      <author><first>Katarzyna</first><last>Głowińska</last></author>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <title>The Design of Syntactic Annotation Levels in the <fixed-case>N</fixed-case>ational <fixed-case>C</fixed-case>orpus of <fixed-case>P</fixed-case>olish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/259_Paper.pdf</url>
      <abstract>The paper presents the procedure of syntactic annotation of the National Corpus of Polish. The paper concentrates on the delimitation of syntactic words (analytical forms, reflexive verbs, discontinuous conjunctions, etc.) and syntactic groups, as well as on problems encountered during the annotation process: syntactic group boundaries, multiword entities, abbreviations, discontinuous phrases and syntactic words. It includes the complete tagset for syntactic words and the list of syntactic groups recognized in NKJP. The tagset defines grammatical classes and categories according to morphosyntactic and syntactic criteria only. Syntactic annotation in the National Corpus of Polish is limited to making constituents of combinations of words. Annotation depends on shallow parsing and manual post-editing of the results by annotators. Manual annotation is performed by two independents annotators, with a referee in cases of disagreement. The manually constructed grammar, both for syntactic words and for syntactic groups, is encoded in the shallow parsing system Spejd.</abstract>
      <bibkey>glowinska-przepiorkowski-2010-design</bibkey>
    </paper>
    <paper id="176">
      <author><first>Yuki</first><last>Kamiya</last></author>
      <author><first>Tomohiro</first><last>Ohno</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <author><first>Hideki</first><last>Kashioka</last></author>
      <title>Construction of Back-Channel Utterance Corpus for Responsive Spoken Dialogue System Development</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/260_Paper.pdf</url>
      <abstract>In spoken dialogues, if a spoken dialogue system does not respond at all during users utterances, the user might feel uneasy because the user does not know whether or not the system has recognized the utterances. In particular, back-channel utterances, which the system outputs as voices such as yeah and uh huh in English have important roles for a driver in in-car speech dialogues because the driver does not look owards a listener while driving. This paper describes construction of a back-channel utterance corpus and its analysis to develop the system which can output back-channel utterances at the proper timing in the responsive in-car speech dialogue. First, we constructed the back-channel utterance corpus by integrating the back-channel utterances that four subjects provided for the drivers utterances in 60 dialogues in the CIAIR in-car speech dialogue corpus. Next, we analyzed the corpus and revealed the relation between back-channel utterance timings and information on bunsetsu, clause, pause and rate of speech. Based on the analysis, we examined the possibility of detecting back-channel utterance timings by machine learning technique. As the result of the experiment, we confirmed that our technique achieved as same detection capability as a human.</abstract>
      <bibkey>kamiya-etal-2010-construction</bibkey>
    </paper>
    <paper id="177">
      <author><first>Marina B.</first><last>Ruiter</last></author>
      <author><first>Toni C. M.</first><last>Rietveld</last></author>
      <author><first>Catia</first><last>Cucchiarini</last></author>
      <author><first>Emiel J.</first><last>Krahmer</last></author>
      <author><first>Helmer</first><last>Strik</last></author>
      <title>Human Language Technology and Communicative Disabilities: Requirements and Possibilities for the Future</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/261_Paper.pdf</url>
      <abstract>For some years now, the Nederlandse Taalunie (Dutch Language Union) has been active in promoting the development of human language technology (HLT) applications for users of Dutch with communication disabilities. The reason is that HLT products and services may enable these users to improve their verbal autonomy and communication skills. We sought to identify a minimum common set of HLT resources that is required to develop tools for a wide range of communication disabilities. In order to reach this goal, we investigated the specific HLT needs of communicatively disabled people and related these needs to the underlying HLT software components. By analysing the availability and quality of these essential HLT resources, we were able to identify which of the crucial elements need further research and development to become usable for developing applications for communicatively disabled users of Dutch. The results obtained in the current survey can be used to inform policy institutions on how they can stimulate the development of HLT resources for this target group. In the current study results were obtained for Dutch, but a similar approach can also be used for other languages.</abstract>
      <bibkey>ruiter-etal-2010-human</bibkey>
    </paper>
    <paper id="178">
      <author><first>Felix</first><last>Burkhardt</last></author>
      <author><first>Martin</first><last>Eckert</last></author>
      <author><first>Wiebke</first><last>Johannsen</last></author>
      <author><first>Joachim</first><last>Stegmann</last></author>
      <title>A Database of Age and Gender Annotated Telephone Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/262_Paper.pdf</url>
      <abstract>This article describes an age-annotated database of German telephone speech. All in all 47 hours of prompted and free text was recorded, uttered by 954 paid participants in a style typical for automated voice services. The participants were selected based on an equal distribution of males and females within four age cluster groups; children, youth, adults and seniors. Within the children, gender is not distinguished, because it doesnt have a strong enough effect on the voice. The textual content was designed to be typical for automated voice services and consists mainly of short commands, single words and numbers. An additional database consists of 659 speakers (368 female and 291 male) that called an automated voice portal server and answered freely on one of the two questions What is your favourite dish? and What would you take to an island? (island set, 422 speakers). This data might be used for out-of domain testing. The data will be used to tune an age-detecting automated voice service and might be released to research institutes under controlled conditions as part of an open age and gender detection challenge.</abstract>
      <bibkey>burkhardt-etal-2010-database</bibkey>
    </paper>
    <paper id="179">
      <author><first>Maarten</first><last>Marx</last></author>
      <author><first>Anne</first><last>Schuth</last></author>
      <title><fixed-case>D</fixed-case>utch<fixed-case>P</fixed-case>arl. The Parliamentary Documents in <fixed-case>D</fixed-case>utch</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/263_Paper.pdf</url>
      <abstract>A corpus called DutchParl is created which aims to contain all digitally available parliamentary documents written in the Dutch language. The first version of DutchParl contains documents from the parliaments of The Netherlands, Flanders and Belgium. The corpus is divided along three dimensions: per parliament, scanned or digital documents, written recordings of spoken text and others. The digital collection contains more than 800 million tokens, the scanned collection more than 1 billion. All documents are available as UTF-8 encoded XML files with extensive metadata in Dublin Core standard. The text itself is divided into pages which are divided into paragraphs. Every document, page and paragraph has a unique URN which resolves to a web page. Every page element in the XML files is connected to a facsimile image of that page in PDF or JPEG format. We created a viewer in which both versions can be inspected simultaneously. The corpus is available for download in several formats. The corpus can be used for corpus-linguistic and political science research, and is suitable for performing scalability tests for XML information systems.</abstract>
      <bibkey>marx-schuth-2010-dutchparl</bibkey>
    </paper>
    <paper id="180">
      <author><first>Verena</first><last>Henrich</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <title><fixed-case>G</fixed-case>ern<fixed-case>E</fixed-case>di<fixed-case>T</fixed-case> - The <fixed-case>G</fixed-case>erma<fixed-case>N</fixed-case>et Editing Tool</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf</url>
      <abstract>This paper introduces GernEdiT (short for: GermaNet Editing Tool), a new graphical user interface for the lexicographers and developers of GermaNet, the German version of the Princeton WordNet. GermaNet is a lexical-semantic net that relates German nouns, verbs, and adjectives. Traditionally, lexicographic work for extending the coverage of GermaNet utilized the Princeton WordNet development environment of lexicographer files. Due to a complex data format and no opportunity of automatic consistency checks, this process was very error prone and time consuming. The GermaNet Editing Tool GernEdiT was developed to overcome these shortcomings. The main purposes of the GernEdiT tool are, besides supporting lexicographers to access, modify, and extend GermaNet data in an easy and adaptive way, as follows: Replace the standard editing tools by a more user-friendly tool, use a relational database as data storage, support export formats in the form of XML, and facilitate internal consistency and correctness of the linguistic resource. All these core functionalities of GernEdiT along with the main aspects of the underlying lexical resource GermaNet and its current database format are presented in this paper.</abstract>
      <bibkey>henrich-hinrichs-2010-gernedit</bibkey>
    </paper>
    <paper id="181">
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <author><first>Verena</first><last>Henrich</last></author>
      <author><first>Thomas</first><last>Zastrow</last></author>
      <title>Sustainability of Linguistic Data and Analysis in the Context of a Collaborative e<fixed-case>S</fixed-case>cience Environment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/266_Paper.pdf</url>
      <abstract>For researchers, it is especially important that primary research data are preserved and made available on a long-term basis and to a wide variety of researchers. In order to ensure long-term availability of the archived data, it is imperative that the data to be stored is conformant with standardized data formats and best practices followed by the relevant research communities. Storing, managing, and accessing such standard-conformant data requires a repository-based infrastructure. Two projects at the University of Tübingen are realizing a collaborative eScience research environment with the help of eSciDoc for the university that supports long-term preservation of all kinds of data as well as a fine-grained and contextualized data management: the INF project and the BW-eSci(T) project. The task of the infrastructure (INF) project within the collaborative research centre âEmergence of Meaning (SFB 833) is to guarantee the long-term availability of the SFBs data. BW-eSci(T) is a joint project of the University of Tübingen and the Fachinformationszentrums (FIZ) Karlsruhe. The goal of this project is to develop a prototypical eScience research environment for the University of Tübingen.</abstract>
      <bibkey>hinrichs-etal-2010-sustainability</bibkey>
    </paper>
    <paper id="182">
      <author><first>Fabienne</first><last>Fritzinger</last></author>
      <author><first>Frank</first><last>Richter</last></author>
      <author><first>Marion</first><last>Weller</last></author>
      <title>Pattern-Based Extraction of Negative Polarity Items from Dependency-Parsed Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/267_Paper.pdf</url>
      <abstract>We describe a new method for extracting Negative Polarity Item candidates (NPI candidates) from dependency-parsed German text corpora. Semi-automatic extraction of NPIs is a challenging task since NPIs do not have uniform categorical or other syntactic properties that could be used for detecting them; they occur as single words or as multi-word expressions of almost any syntactic category. Their defining property is of a semantic nature, they may only occur in the scope of negation and related semantic operators. In contrast to an earlier approach to NPI extraction from corpora, we specifically target multi-word expressions. Besides applying statistical methods to measure the co-occurrence of our candidate expressions with negative contexts, we also apply linguistic criteria in an attempt to determine to which degree they are idiomatic. Our method is evaluated by comparing the set of NPIs we found with the most comprehensive electronic list of German NPIs, which currently contains 165 entries. Our method retrieved 142 NPIs, 114 of which are new.</abstract>
      <bibkey>fritzinger-etal-2010-pattern</bibkey>
    </paper>
    <paper id="183">
      <author><first>Attila</first><last>Görög</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Computer Assisted Semantic Annotation in the <fixed-case>D</fixed-case>utch<fixed-case>S</fixed-case>em<fixed-case>C</fixed-case>or Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/269_Paper.pdf</url>
      <abstract>The goal of this paper is to describe the annotation protocols and the Semantic Annotation Tool (SAT) used in the DutchSemCor project. The DutchSemCor project is aiming at aligning the Cornetto lexical database with the Dutch language corpus SoNaR. 250K corpus occurrences of the 3,000 most frequent and most ambiguous Dutch nouns, adjectives and verbs are being annotated manually using the SAT. This data is then used for bootstrapping 750K extra occurrences which in turn will be checked manually. Our main focus in this paper is the methodology applied in the project to attain the envisaged Inter-annotator Agreement (IA) of =80%. We will also discuss one of the main objectives of DutchSemCor i.e. to provide semantically annotated language data with high scores for quantity, quality and diversity. Sample data with high scores for these three features can yield better results for co-training WSD systems. Finally, we will take a brief look at our annotation tool.</abstract>
      <bibkey>gorog-vossen-2010-computer</bibkey>
    </paper>
    <paper id="184">
      <author><first>Marie</first><last>Hinrichs</last></author>
      <author><first>Thomas</first><last>Zastrow</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <title><fixed-case>W</fixed-case>eb<fixed-case>L</fixed-case>icht: Web-based <fixed-case>LRT</fixed-case> Services in a Distributed e<fixed-case>S</fixed-case>cience Infrastructure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/270_Paper.pdf</url>
      <abstract>eScience - enhanced science - is a new paradigm of scientific work and research. In the humanities, eScience environments can be helpful in establishing new workflows and lifecycles of scientific data. WebLicht is such an eScience environment for linguistic analysis, making linguistic tools and resources available network-wide. Today, most digital language resources and tools (LRT) are available by download only. This is inconvenient for someone who wants to use and combine several tools because these tools are normally not compatible with each other. To overcome this restriction, WebLicht makes the functionality of linguistic tools and the resources themselves available via the internet as web services. In WebLicht, several kinds of linguistic tools are available which cover the basic functionality of automatic and incremental creation of annotated text corpora. To make use of the more than 70 tools and resources currently available, the end user needs nothing more than just a common web browser.</abstract>
      <bibkey>hinrichs-etal-2010-weblicht</bibkey>
    </paper>
    <paper id="185">
      <author><first>Francisco</first><last>Torreira</last></author>
      <author><first>Mirjam</first><last>Ernestus</last></author>
      <title>The Nijmegen Corpus of Casual <fixed-case>S</fixed-case>panish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/271_Paper.pdf</url>
      <abstract>This article describes the preparation, recording and orthographic transcription of a new speech corpus, the Nijmegen Corpus of Casual Spanish (NCCSp). The corpus contains around 30 hours of recordings of 52 Madrid Spanish speakers engaged in conversations with friends. Casual speech was elicited during three different parts, which together provided around ninety minutes of speech from every group of speakers. While Parts 1 and 2 did not require participants to perform any specific task, in Part 3 participants negotiated a common answer to general questions about society. Information about how to obtain a copy of the corpus can be found online at http://mirjamernestus.ruhosting.nl/Ernestus/NCCSp</abstract>
      <bibkey>torreira-ernestus-2010-nijmegen</bibkey>
    </paper>
    <paper id="186">
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Luís Miguel</first><last>Cabral</last></author>
      <author><first>Corina</first><last>Forascu</last></author>
      <author><first>Pamela</first><last>Forner</last></author>
      <author><first>Fredric</first><last>Gey</last></author>
      <author><first>Katrin</first><last>Lamm</last></author>
      <author><first>Thomas</first><last>Mandl</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Anselmo</first><last>Peñas</last></author>
      <author><first>Álvaro</first><last>Rodrigo</last></author>
      <author><first>Julia</first><last>Schulz</last></author>
      <author><first>Yvonne</first><last>Skalban</last></author>
      <author><first>Erik</first><last>Tjong Kim Sang</last></author>
      <title><fixed-case>G</fixed-case>iki<fixed-case>CLEF</fixed-case>: Crosscultural Issues in Multilingual Information Access</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/272_Paper.pdf</url>
      <abstract>In this paper we describe GikiCLEF, the first evaluation contest that, to our knowledge, was specifically designed to expose and investigate cultural and linguistic issues involved in structured multimedia collections and searching, and which was organized under the scope of CLEF 2009. GikiCLEF evaluated systems that answered hard questions for both human and machine, in ten different Wikipedia collections, namely Bulgarian, Dutch, English, German, Italian, Norwegian (Bokmäl and Nynorsk), Portuguese, Romanian, and Spanish. After a short historical introduction, we present the task, together with its motivation, and discuss how the topics were chosen. Then we provide another description from the point of view of the participants. Before disclosing their results, we introduce the SIGA management system explaining the several tasks which were carried out behind the scenes. We quantify in turn the GIRA resource, offered to the community for training and further evaluating systems with the help of the 50 topics gathered and the solutions identified. We end the paper with a critical discussion of what was learned, advancing possible ways to reuse the data.</abstract>
      <bibkey>santos-etal-2010-gikiclef</bibkey>
    </paper>
    <paper id="187">
      <author><first>Dieter</first><last>Van Uytvanck</last></author>
      <author><first>Claus</first><last>Zinn</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Mariano</first><last>Gardellini</last></author>
      <title>Virtual Language Observatory: The Portal to the Language Resources and Technology Universe</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/273_Paper.pdf</url>
      <abstract>Over the years, the field of Language Resources and Technology (LRT) has developed a tremendous amount of resources and tools. However, there is no ready-to-use map that researchers could use to gain a good overview and steadfast orientation when searching for, say corpora or software tools to support their studies. It is rather the case that information is scattered across project- or organisation-specific sites, which makes it hard if not impossible for less-experienced researchers to gather all relevant material. Clearly, the provision of metadata is central to resource and software exploration. However, in the LRT field, metadata comes in many forms, tastes and qualities, and therefore substantial harmonization and curation efforts are required to provide researchers with metadata-based guidance. To address this issue a broad alliance of LRT providers (CLARIN, the Linguist List, DOBES, DELAMAN, DFKI, ELRA) have initiated the Virtual Language Observatory portal to provide a low-barrier, easy-to-follow entry point to language resources and tools; it can be accessed via http://www.clarin.eu/vlo</abstract>
      <bibkey>van-uytvanck-etal-2010-virtual</bibkey>
    </paper>
    <paper id="188">
      <author><first>Pavel</first><last>Skrelin</last></author>
      <author><first>Nina</first><last>Volskaya</last></author>
      <author><first>Daniil</first><last>Kocharov</last></author>
      <author><first>Karina</first><last>Evgrafova</last></author>
      <author><first>Olga</first><last>Glotova</last></author>
      <author><first>Vera</first><last>Evdokimova</last></author>
      <title>A Fully Annotated Corpus of <fixed-case>R</fixed-case>ussian Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/274_Paper.pdf</url>
      <abstract>The paper introduces CORPRES ― a fully annotated Russian speech corpus developed at the Department of Phonetics, St. Petersburg State University as a result of a three-year project. The corpus includes samples of different speaking styles produced by 4 male and 4 female speakers. Six levels of annotation cover all phonetic and prosodic information about the recorded speech data, including labels for pitch marks, phonetic events, narrow and wide phonetic transcription, orthographic and prosodic transcription. Precise phonetic transcription of the data provides an especially valuable resource for both research and development purposes. Overall corpus size is 528 458 running words and contains 60 hours of speech made up of 7.5 hours from each speaker. 40% of the corpus was manually segmented and fully annotated on all six levels. 60% of the corpus was partly annotated; there are labels for pitch period and phonetic event labels. Orthographic, prosodic and ideal phonetic transcription for this part was generated and stored as text files. The fully annotated part of the corpus covers all speaking styles included in the corpus and all speakers. The paper contains information about CORPRES design and annotation principles, overall data description and some speculation about possible use of the corpus.</abstract>
      <bibkey>skrelin-etal-2010-fully</bibkey>
    </paper>
    <paper id="189">
      <author><first>Werner</first><last>Spiegl</last></author>
      <author><first>Korbinian</first><last>Riedhammer</last></author>
      <author><first>Stefan</first><last>Steidl</last></author>
      <author><first>Elmar</first><last>Nöth</last></author>
      <title><fixed-case>FAU</fixed-case> <fixed-case>IISAH</fixed-case> Corpus – A <fixed-case>G</fixed-case>erman Speech Database Consisting of Human-Machine and Human-Human Interaction Acquired by Close-Talking and Far-Distance Microphones</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/275_Paper.pdf</url>
      <abstract>In this paper the FAU IISAH corpus and its recording conditions are described: a new speech database consisting of human-machine and human-human interaction recordings. Beside close-talking microphones for the best possible audio quality of the recorded speech, far-distance microphones were used to acquire the interaction and communication. The recordings took place during a Wizard-of-Oz experiment in the intelligent, senior-adapted house (ISA-House). That is a living room with a speech controlled home assistance system for elderly people, based on a dialogue system, which is able to process spontaneous speech. During the studies in the ISA-House more than eight hours of interaction data were recorded including 3 hours and 27 minutes of spontaneous speech. The data were annotated in terms of human-human (off-talk) and human-machine (on-talk) interaction. The test persons used 2891 turns of off-talk and 2752 turns of on-talk including 1751 different words. Still in progress is the analysis under statistical and linguistical aspects.</abstract>
      <bibkey>spiegl-etal-2010-fau</bibkey>
    </paper>
    <paper id="190">
      <author><first>Kais</first><last>Dukes</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <title>Morphological Annotation of <fixed-case>Q</fixed-case>uranic <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/276_Paper.pdf</url>
      <abstract>The Quranic Arabic Corpus (http://corpus.quran.com) is an annotated linguistic resource with multiple layers of annotation including morphological segmentation, part-of-speech tagging, and syntactic analysis using dependency grammar. The motivation behind this work is to produce a resource that enables further analysis of the Quran, the 1,400 year old central religious text of Islam. This paper describes a new approach to morphological annotation of Quranic Arabic, a genre difficult to compare with other forms of Arabic. Processing Quranic Arabic is a unique challenge from a computational point of view, since the vocabulary and spelling differ from Modern Standard Arabic. The Quranic Arabic Corpus differs from other Arabic computational resources in adopting a tagset that closely follows traditional Arabic grammar. We made this decision in order to leverage a large body of existing historical grammatical analysis, and to encourage online collaborative annotation. In this paper, we discuss how the unique challenge of morphological annotation of Quranic Arabic is solved using a multi-stage approach. The different stages include automatic morphological tagging using diacritic edit-distance, two-pass manual verification, and online collaborative annotation. This process is evaluated to validate the appropriateness of the chosen methodology.</abstract>
      <bibkey>dukes-habash-2010-morphological</bibkey>
    </paper>
    <paper id="191">
      <author><first>Florian</first><last>Schiel</last></author>
      <title><fixed-case>BAS</fixed-case>tat : New Statistical Resources at the <fixed-case>B</fixed-case>avarian Archive for Speech Signals</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/277_Paper.pdf</url>
      <abstract>A new type of language resource called 'BAStat' has been released by the Bavarian Archive for Speech Signals at Ludwig Maximilians Universitaet, Munich. In contrast to primary resources like speech and text corpora BAStat comprises statistical estimates based on a number of primary spoken language resources: first and second order occurrence probability of phones, syllables and words, duration statistics, probabilities of pronunciation variants of words and probabilities of context information. Unlike other statistical speech resources BAStat is based solely on recordings of conversational German and therefore models spoken language not text. The resource consists of a bundle of 7-bit ASCII tables and matrices to maximize inter-operability between different operation systems and can be downloaded for free from the BAS web-site. This contribution gives a detailed description about the empirical basis, the contained data types, the format of the resulting statistical data, some interesting interpretations of grand figures and a brief comparison to the text-based statistical resource CELEX.</abstract>
      <bibkey>schiel-2010-bastat</bibkey>
    </paper>
    <paper id="192">
      <author><first>Kais</first><last>Dukes</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Abdul-Baquee M.</first><last>Sharaf</last></author>
      <title>Syntactic Annotation Guidelines for the <fixed-case>Q</fixed-case>uranic <fixed-case>A</fixed-case>rabic Dependency Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/278_Paper.pdf</url>
      <abstract>The Quranic Arabic Dependency Treebank (QADT) is part of the Quranic Arabic Corpus (http://corpus.quran.com), an online linguistic resource organized by the University of Leeds, and developed through online collaborative annotation. The website has become a popular study resource for Arabic and the Quran, and is now used by over 1,500 researchers and students daily. This paper presents the treebank, explains the choice of syntactic representation, and highlights key parts of the annotation guidelines. The text being analyzed is the Quran, the central religious book of Islam, written in classical Quranic Arabic (c. 600 CE). To date, all 77,430 words of the Quran have a manually verified morphological analysis, and syntactic analysis is in progress. 11,000 words of Quranic Arabic have been syntactically annotated as part of a gold standard treebank. Annotation guidelines are especially important to promote consistency for a corpus which is being developed through online collaboration, since often many people will participate from different backgrounds and with different levels of linguistic expertise. The treebank is available online for collaborative correction to improve accuracy, with suggestions reviewed by expert Arabic linguists, and compared against existing published books of Quranic Syntax.</abstract>
      <bibkey>dukes-etal-2010-syntactic</bibkey>
    </paper>
    <paper id="193">
      <author><first>Tommi</first><last>Vatanen</last></author>
      <author><first>Jaakko J.</first><last>Väyrynen</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <title>Language Identification of Short Text Segments with N-gram Models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/279_Paper.pdf</url>
      <abstract>There are many accurate methods for language identification of long text samples, but identification of very short strings still presents a challenge. This paper studies a language identification task, in which the test samples have only 5-21 characters. We compare two distinct methods that are well suited for this task: a naive Bayes classifier based on character n-gram models, and the ranking method by Cavnar and Trenkle (1994). For the n-gram models, we test several standard smoothing techniques, including the current state-of-the-art, the modified Kneser-Ney interpolation. Experiments are conducted with 281 languages using the Universal Declaration of Human Rights. Advanced language model smoothing techniques improve the identification accuracy and the respective classifiers outperform the ranking method. The higher accuracy is obtained at the cost of larger models and slower classification speed. However, there are several methods to reduce the size of an n-gram model, and our experiments with model pruning show that it provides an easy way to balance the size and the identification accuracy. We also compare the results to the language identifier in Google AJAX Language API, using a subset of 50 languages.</abstract>
      <bibkey>vatanen-etal-2010-language</bibkey>
    </paper>
    <paper id="194">
      <author><first>Joseph</first><last>Polifroni</last></author>
      <author><first>Imre</first><last>Kiss</last></author>
      <author><first>Mark</first><last>Adler</last></author>
      <title>Bootstrapping Named Entity Extraction for the Creation of Mobile Services</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/280_Paper.pdf</url>
      <abstract>As users become more accustomed to using their mobile devices to organize and schedule their lives, there is more of a demand for applications that can make that process easier. Automatic speech recognition technology has already been developed to enable essentially unlimited vocabulary in a mobile setting. Understanding the words that are spoken is the next challenge. In this paper, we describe efforts to develop a dataset and classifier to recognize named entities in speech. Using sets of both real and simulated data, in conjunction with a very large set of real named entities, we created a challenging corpus of training and test data. We use these data to develop a classifier to identify names and locations on a word-by-word basis. In this paper, we describe the process of creating the data and determining a set of features to use for named entity recognition. We report on our classification performance on these data, as well as point to future work in improving all aspects of the system.</abstract>
      <bibkey>polifroni-etal-2010-bootstrapping</bibkey>
    </paper>
    <paper id="195">
      <author><first>Bert</first><last>Réveil</last></author>
      <author><first>Jean-Pierre</first><last>Martens</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <title>Improving Proper Name Recognition by Adding Automatically Learned Pronunciation Variants to the Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/281_Paper.pdf</url>
      <abstract>This paper deals with the task of large vocabulary proper name recognition. In order to accomodate a wide diversity of possible name pronunciations (due to non-native name origins or speaker tongues) a multilingual acoustic model is combined with a lexicon comprising 3 grapheme-to-phoneme (G2P) transcriptions from G2P transcribers for 3 different languages) and up to 4 so-called phoneme-to-phoneme (P2P) transcriptions. The latter are generated with (speaker tongue, name source) specific P2P converters that try to transform a set of baseline name transcriptions into a pool of transcription variants that lie closer to the `true name pronunciations. The experimental results show that the generated P2P variants can be employed to improve name recognition, and that the obtained accuracy is comparable to what is achieved with typical (TY) transcriptions (made by a human expert). Furthermore, it is demonstrated that the P2P conversion can best be instantiated from a baseline transcription in the name source language, and that knowledge of the speaker tongue is an important input as well for the P2P transcription process.</abstract>
      <bibkey>reveil-etal-2010-improving</bibkey>
    </paper>
    <paper id="196">
      <author><first>Majdi</first><last>Sawalha</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <title>Fine-Grain Morphological Analyzer and Part-of-Speech Tagger for <fixed-case>A</fixed-case>rabic Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/282_Paper.pdf</url>
      <abstract>Morphological analyzers and part-of-speech taggers are key technologies for most text analysis applications. Our aim is to develop a part-of-speech tagger for annotating a wide range of Arabic text formats, domains and genres including both vowelized and non-vowelized text. Enriching the text with linguistic analysis will maximize the potential for corpus re-use in a wide range of applications. We foresee the advantage of enriching the text with part-of-speech tags of very fine-grained grammatical distinctions, which reflect expert interest in syntax and morphology, but not specific needs of end-users, because end-user applications are not known in advance. In this paper we review existing Arabic Part-of-Speech Taggers and tag-sets, and illustrate four different Arabic PoS tag-sets for a sample of Arabic text from the Quran. We describe the detailed fine-grained morphological feature tag set of Arabic, and the fine-grained Arabic morphological analyzer algorithm. We faced practical challenges in applying the morphological analyzer to the 100-million-word Web Arabic Corpus: we had to port the software to the National Grid Service, adapt the analyser to cope with spelling variations and errors, and utilise a Broad-Coverage Lexical Resource combining 23 traditional Arabic lexicons. Finally we outline the construction of a Gold Standard for comparative evaluation.</abstract>
      <bibkey>sawalha-atwell-2010-fine</bibkey>
    </paper>
    <paper id="197">
      <author><first>Carlos</first><last>Periñán-Pascual</last></author>
      <author><first>Francisco</first><last>Arcas-Túnez</last></author>
      <title>The Architecture of <fixed-case>F</fixed-case>un<fixed-case>G</fixed-case>ram<fixed-case>KB</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/284_Paper.pdf</url>
      <abstract>Natural language understanding systems require a knowledge base provided with conceptual representations reflecting the structure of human beings cognitive system. Although surface semantics can be sufficient in some other systems, the construction of a robust knowledge base guarantees its use in most natural language processing applications, consolidating thus the concept of resource reuse. In this scenario, FunGramKB is presented as a multipurpose knowledge base whose model has been particularly designed for natural language understanding tasks. The theoretical basement of this knowledge engineering project lies in the construction of two complementary types of interlingua: the conceptual logical structure, i.e. a lexically-driven interlingua which can predict linguistic phenomena according to the Role and Reference Grammar syntax-semantics interface, and the COREL scheme, i.e. a concept-oriented interlingua on which our rule-based reasoning engine is able to make inferences effectively. The objective of the paper is to describe the different conceptual, lexical and grammatical modules which make up the architecture of FunGramKB, together with an exploratory outline on how to exploit such a knowledge base within an NLP system.</abstract>
      <bibkey>perinan-pascual-arcas-tunez-2010-architecture</bibkey>
    </paper>
    <paper id="198">
      <author><first>Patrick</first><last>Bauer</last></author>
      <author><first>David</first><last>Scheler</last></author>
      <author><first>Tim</first><last>Fingscheidt</last></author>
      <title><fixed-case>WTIMIT</fixed-case>: The <fixed-case>TIMIT</fixed-case> Speech Corpus Transmitted Over The 3<fixed-case>G</fixed-case> <fixed-case>AMR</fixed-case> Wideband Mobile Network</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/285_Paper.pdf</url>
      <abstract>In anticipation of upcoming mobile telephony services with higher speech quality, a wideband (50 Hz to 7 kHz) mobile telephony derivative of TIMIT has been recorded called WTIMIT. It opens up various scientific investigations; e.g., on speech quality and intelligibility, as well as on wideband upgrades of network-side interactive voice response (IVR) systems with retrained or bandwidth-extended acoustic models for automatic speech recognition (ASR). Wideband telephony could enable network-side speech recognition applications such as remote dictation or spelling without the need of distributed speech recognition techniques. The WTIMIT corpus was transmitted via two prepared Nokia 6220 mobile phones over T-Mobile's 3G wideband mobile network in The Hague, The Netherlands, employing the Adaptive Multirate Wideband (AMR-WB) speech codec. The paper presents observations of transmission effects and phoneme recognition experiments. It turns out that in the case of wideband telephony, server-side ASR should not be carried out by simply decimating received signals to 8 kHz and applying existent narrowband acoustic models. Nor do we recommend just simulating the AMR-WB codec for training of wideband acoustic models. Instead, real-world wideband telephony channel data (such as WTIMIT) provides the best training material for wideband IVR systems.</abstract>
      <bibkey>bauer-etal-2010-wtimit</bibkey>
    </paper>
    <paper id="199">
      <author><first>Philip</first><last>van Oosten</last></author>
      <author><first>Dries</first><last>Tanghe</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Towards an Improved Methodology for Automated Readability Prediction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/286_Paper.pdf</url>
      <abstract>Since the first half of the 20th century, readability formulas have been widely employed to automatically predict the readability of an unseen text. In this article, the formulas and the text characteristics they are composed of are evaluated in the context of large Dutch and English corpora. We describe the behaviour of the formulas and the text characteristics by means of correlation matrices and a principal component analysis, and test the methodological validity of the formulas by means of collinearity tests. Both the correlation matrices and the principal component analysis show that the formulas described in this paper strongly correspond, regardless of the language for which they were designed. Furthermore, the collinearity test reveals shortcomings in the methodology that was used to create some of the existing readability formulas. All of this leads us to conclude that a new readability prediction method is needed. We finally make suggestions to come to a cleaner methodology and present web applications that will help us collect data to compile a new gold standard for readability prediction.</abstract>
      <bibkey>van-oosten-etal-2010-towards</bibkey>
    </paper>
    <paper id="200">
      <author><first>Majdi</first><last>Sawalha</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <title>Constructing and Using Broad-coverage Lexical Resource for Enhancing Morphological Analysis of <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/287_Paper.pdf</url>
      <abstract>Broad-coverage language resources which provide prior linguistic knowledge must improve the accuracy and the performance of NLP applications. We are constructing a broad-coverage lexical resource to improve the accuracy of morphological analyzers and part-of-speech taggers of Arabic text. Over the past 1200 years, many different kinds of Arabic language lexicons were constructed; these lexicons are different in ordering, size and aim or goal of construction. We collected 23 machine-readable lexicons, which are freely available on the web. We combined lexical resources into one large broad-coverage lexical resource by extracting information from disparate formats and merging traditional Arabic lexicons. To evaluate the broad-coverage lexical resource we computed coverage over the Quran, the Corpus of Contemporary Arabic, and a sample from the Arabic Web Corpus, using two methods. Counting exact word matches between test corpora and lexicon scored about 65-68%; Arabic has a rich morphology with many combinations of roots, affixes and clitics, so about a third of words in the corpora did not have an exact match in the lexicon. The second approach is to compute coverage in terms of use in a lemmatizer program, which strips clitics to look for a match for the underlying lexeme; this scored about 82-85%.</abstract>
      <bibkey>sawalha-atwell-2010-constructing</bibkey>
    </paper>
    <paper id="201">
      <author><first>Jérôme</first><last>Urbain</last></author>
      <author><first>Elisabetta</first><last>Bevacqua</last></author>
      <author><first>Thierry</first><last>Dutoit</last></author>
      <author><first>Alexis</first><last>Moinet</last></author>
      <author><first>Radoslaw</first><last>Niewiadomski</last></author>
      <author><first>Catherine</first><last>Pelachaud</last></author>
      <author><first>Benjamin</first><last>Picart</last></author>
      <author><first>Joëlle</first><last>Tilmanne</last></author>
      <author><first>Johannes</first><last>Wagner</last></author>
      <title>The <fixed-case>AVL</fixed-case>aughter<fixed-case>C</fixed-case>ycle Database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/289_Paper.pdf</url>
      <abstract>This paper presents the large audiovisual laughter database recorded as part of the AVLaughterCycle project held during the eNTERFACE09 Workshop in Genova. 24 subjects participated. The freely available database includes audio signal and video recordings as well as facial motion tracking, thanks to markers placed on the subjects face. Annotations of the recordings, focusing on laughter description, are also provided and exhibited in this paper. In total, the corpus contains more than 1000 spontaneous laughs and 27 acted laughs. The laughter utterances are highly variable: the laughter duration ranges from 250ms to 82s and the sounds cover voiced vowels, breath-like expirations, hum-, hiccup- or grunt-like sounds, etc. However, as the subjects had no one to interact with, the database contains very few speech-laughs. Acted laughs tend to be longer than spontaneous ones and are more often composed of voiced vowels. The database can be useful for automatic laughter processing or cognitive science works. For the AVLaughterCycle project, it has served to animate a laughing virtual agent with an output laugh linked to the conversational partners input laugh.</abstract>
      <bibkey>urbain-etal-2010-avlaughtercycle</bibkey>
    </paper>
    <paper id="202">
      <author><first>Gerlof</first><last>Bouma</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>Towards a Large Parallel Corpus of Cleft Constructions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/291_Paper.pdf</url>
      <abstract>We present our efforts to create a large-scale, semi-automatically annotated parallel corpus of cleft constructions. The corpus is intended to reduce or make more effective the manual task of finding examples of clefts in a corpus. The corpus is being developed in the context of the Collaborative Research Centre SFB 632, which is a large, interdisciplinary research initiative to study information structure, at the University of Potsdam and the Humboldt University in Berlin. The corpus is based on the Europarl corpus (version 3). We show how state-of-the-art NLP tools, like POS taggers and statistical dependency parsers, may facilitate powerful and precise searches. We argue that identifying clefts using automatically added syntactic structure annotation is ultimately to be preferred over using lower level, though more robust, extraction methods like regular expression matching. An evaluation of the extraction method for one of the languages also offers some support for this method. We end the paper by discussing the resulting corpus itself. We present some examples of interesting clefts and translational counterparts from the corpus and suggest ways of exploiting our newly created resource in the cross-linguistic study of clefts.</abstract>
      <bibkey>bouma-etal-2010-towards</bibkey>
    </paper>
    <paper id="203">
      <author><first>Ziqi</first><last>Zhang</last></author>
      <author><first>Anna Lisa</first><last>Gentile</last></author>
      <author><first>Lei</first><last>Xia</last></author>
      <author><first>José</first><last>Iria</last></author>
      <author><first>Sam</first><last>Chapman</last></author>
      <title>A Random Graph Walk based Approach to Computing Semantic Relatedness Using Knowledge from <fixed-case>W</fixed-case>ikipedia</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/292_Paper.pdf</url>
      <abstract>Determining semantic relatedness between words or concepts is a fundamental process to many Natural Language Processing applications. Approaches for this task typically make use of knowledge resources such as WordNet and Wikipedia. However, these approaches only make use of limited number of features extracted from these resources, without investigating the usefulness of combining various different features and their importance in the task of semantic relatedness. In this paper, we propose a random walk model based approach to measuring semantic relatedness between words or concepts, which seamlessly integrates various features extracted from Wikipedia to compute semantic relatedness. We empirically study the usefulness of these features in the task, and prove that by combining multiple features that are weighed according to their importance, our system obtains competitive results, and outperforms other systems on some datasets.</abstract>
      <bibkey>zhang-etal-2010-random</bibkey>
    </paper>
    <paper id="204">
      <author><first>Adrien</first><last>Lardilleux</last></author>
      <author><first>Julien</first><last>Gosme</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <title>Bilingual Lexicon Induction: Effortless Evaluation of Word Alignment Tools and Production of Resources for Improbable Language Pairs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/293_Paper.pdf</url>
      <abstract>In this paper, we present a simple protocol to evaluate word aligners on bilingual lexicon induction tasks from parallel corpora. Rather than resorting to gold standards, it relies on a comparison of the outputs of word aligners against a reference bilingual lexicon. The quality of this reference bilingual lexicon does not need to be particularly high, because evaluation quality is ensured by systematically filtering this reference lexicon with the parallel corpus the word aligners are trained on. We perform a comparison of three freely available word aligners on numerous language pairs from the Bible parallel corpus (Resnik et al., 1999): MGIZA++ (Gao and Vogel, 2008), BerkeleyAligner (Liang et al., 2006), and Anymalign (Lardilleux and Lepage, 2009). We then select the most appropriate one to produce bilingual lexicons for all language pairs of this corpus. These involve Cebuano, Chinese, Danish, English, Finnish, French, Greek, Indonesian, Latin, Spanish, Swedish, and Vietnamese. The 66 resulting lexicons are made freely available.</abstract>
      <bibkey>lardilleux-etal-2010-bilingual</bibkey>
    </paper>
    <paper id="205">
      <author><first>Marijn</first><last>Schraagen</last></author>
      <author><first>Gerrit</first><last>Bloothooft</last></author>
      <title>Evaluating Repetitions, or how to Improve your Multilingual <fixed-case>ASR</fixed-case> System by doing Nothing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/295_Paper.pdf</url>
      <abstract>Repetition is a common concept in human communication. This paper investigates possible benefits of repetition for automatic speech recognition under controlled conditions. Testing is performed on the newly created Autonomata TOO speech corpus, consisting of multilingual names for Points-Of-Interest as spoken by both native and non-native speakers. During corpus recording, ASR was being performed under baseline conditions using a Nuance Vocon 3200 system. On failed recognition, additional attempts for the same utterances were added to the corpus. Substantial improvements in recognition results are shown for all categories of speakers and utterances, even if speakers did not noticeably alter their previously misrecognized pronunciation. A categorization is proposed for various types of differences between utterance realisations. The number of attempts, the pronunciation of an utterance over multiple attempts compared to both previous attempts and reference pronunciation is analyzed for difference type and frequency. Variables such as the native language of the speaker and the languages in the lexicon are taken into account. Possible implications for ASR research are discussed.</abstract>
      <bibkey>schraagen-bloothooft-2010-evaluating</bibkey>
    </paper>
    <paper id="206">
      <author><first>Akira</first><last>Utsumi</last></author>
      <title>Exploring the Relationship between Semantic Spaces and Semantic Relations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/296_Paper.pdf</url>
      <abstract>This study examines the relationship between two kinds of semantic spaces ― i.e., spaces based on term frequency (tf) and word cooccurrence frequency (co) ― and four semantic relations ― i.e., synonymy, coordination, superordination, and collocation ― by comparing, for each semantic relation, the performance of two semantic spaces in predicting word association. The simulation experiment demonstrates that the tf-based spaces perform better in predicting word association based on the syntagmatic relation (i.e., superordination and collocation), while the co-based semantic spaces are suited for predicting word association based on the paradigmatic relation (i.e., synonymy and coordination). In addition, the co-based space with a larger context size yields better performance for the syntagmatic relation, while the co-based space with a smaller context size tends to show better performance for the paradigmatic relation. These results indicate that different semantic spaces can be used depending on what kind of semantic relatedness should be computed.</abstract>
      <bibkey>utsumi-2010-exploring</bibkey>
    </paper>
    <paper id="207">
      <author><first>Christina</first><last>Leitner</last></author>
      <author><first>Martin</first><last>Schickbichler</last></author>
      <author><first>Stefan</first><last>Petrik</last></author>
      <title>Example-Based Automatic Phonetic Transcription</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/299_Paper.pdf</url>
      <abstract>Current state-of-the-art systems for automatic phonetic transcription (APT) are mostly phone recognizers based on Hidden Markov models (HMMs). We present a different approach for APT especially designed for transcription with a large inventory of phonetic symbols. In contrast to most systems which are model-based, our approach is non-parametric using techniques derived from concatenative speech synthesis and template-based speech recognition. This example-based approach not only produces draft transcriptions that just need to be corrected instead of created from scratch but also provides a validation mechanism for ensuring consistency within the corpus. Implementations of this transcription framework are available as standalone Java software and extension to the ELAN linguistic annotation software. The transcription system was tested with audio files and reference transcriptions from the Austrian Pronunciation Database (ADABA) and compared to an HMM-based system trained on the same data set. The example-based and the HMM-based system achieve comparable phone recognition rates. A combination of rule-based and example-based APT in a constrained phone recognition scenario returned the best results.</abstract>
      <bibkey>leitner-etal-2010-example</bibkey>
    </paper>
    <paper id="208">
      <author><first>Anne</first><last>Garcia-Fernandez</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <title><fixed-case>MACAQ</fixed-case> : A Multi Annotated Corpus to Study how we Adapt Answers to Various Questions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/301_Paper.pdf</url>
      <abstract>This paper presents a corpus of human answers in natural language collected in order to build a base of examples useful when generating natural language answers. We present the corpus and the way we acquired it. Answers correspond to questions with fixed linguistic form, focus, and topic. Answers to a given question exist for two modalities of interaction: oral and written. The whole corpus of answers was annotated manually and automatically on different levels including words from the questions being reused in the answer, the precise element answering the question (or information-answer), and completions. A detailed description of the annotations is presented. Two examples of corpus analyses are described. The first analysis shows some differences between oral and written modality especially in terms of length of the answers. The second analysis concerns the reuse of the question focus in the answers.</abstract>
      <bibkey>garcia-fernandez-etal-2010-macaq</bibkey>
    </paper>
    <paper id="209">
      <author><first>Carlos-D.</first><last>Martínez-Hinarejos</last></author>
      <author><first>Vicent</first><last>Tamarit</last></author>
      <author><first>José-M.</first><last>Benedí</last></author>
      <title>Evaluation of <fixed-case>HMM</fixed-case>-based Models for the Annotation of Unsegmented Dialogue Turns</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/303_Paper.pdf</url>
      <abstract>Corpus-based dialogue systems rely on statistical models, whose parameters are inferred from annotated dialogues. The dialogues are usually annotated in terms of Dialogue Acts (DA), and the manual annotation is difficult (as annotation rule are hard to define), error-prone and time-consuming. Therefore, several semi-automatic annotation processes have been proposed to speed-up the process and consequently obtain a dialogue system in less total time. These processes are usually based on statistical models. The standard statistical annotation model is based on Hidden Markov Models (HMM). In this work, we explore the impact of different types of HMM, with different number of states, on annotation accuracy. We performed experiments using these models on two dialogue corpora (Dihana and SwitchBoard) of dissimilar features. The results show that some types of models improve standard HMM in a human-computer task-oriented dialogue corpus (Dihana corpus), but their impact is lower in a human-human non-task-oriented dialogue corpus (SwitchBoard corpus).</abstract>
      <bibkey>martinez-hinarejos-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="210">
      <author><first>Raheel</first><last>Nawaz</last></author>
      <author><first>Paul</first><last>Thompson</last></author>
      <author><first>John</first><last>McNaught</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <title>Meta-Knowledge Annotation of Bio-Events</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/306_Paper.pdf</url>
      <abstract>Biomedical corpora annotated with event-level information provide an important resource for the training of domain-specific information extraction (IE) systems. These corpora concentrate primarily on creating classified, structured representations of important facts and findings contained within the text. However, bio-event annotations often do not take into account additional information (meta-knowledge) that is expressed within the textual context of the bio-event, e.g., the pragmatic/rhetorical intent and the level of certainty ascribed to a particular bio-event by the authors. Such additional information is indispensible for correct interpretation of bio-events. Therefore, an IE system that simply presents a list of bare bio-events, without information concerning their interpretation, is of little practical use. We have addressed this sparseness of meta-knowledge available in existing bio-event corpora by developing a multi-dimensional annotation scheme tailored to bio-events. The scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the meta-knowledge expressed about different bio-events. To our knowledge, our scheme is unique within the field with regards to the diversity of meta-knowledge aspects annotated for each event.</abstract>
      <bibkey>nawaz-etal-2010-meta</bibkey>
    </paper>
    <paper id="211">
      <author><first>Yoshinobu</first><last>Kano</last></author>
      <author><first>Ruben</first><last>Dorado</last></author>
      <author><first>Luke</first><last>McCrohon</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <author><first>Jun’ichi</first><last>Tsujii</last></author>
      <title><fixed-case>U</fixed-case>-Compare: An Integrated Language Resource Evaluation Platform Including a Comprehensive <fixed-case>UIMA</fixed-case> Resource Library</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/307_Paper.pdf</url>
      <abstract>Language resources, including corpus and tools, are normally required to be combined in order to achieve a users specific task. However, resources tend to be developed independently in different, incompatible formats. In this paper we describe about U-Compare, which consists of the U-Compare component repository and the U-Compare platform. We have been building a highly interoperable resource library, providing the world largest ready-to-use UIMA component repository including wide variety of corpus readers and state-of-the-art language tools. These resources can be deployed as local services or web services, even possible to be hosted in clustered machines to increase the performance, while users do not need to be aware of such differences. In addition to the resource library, an integrated language processing platform is provided, allowing workflow creation, comparison, evaluation and visualization, using the resources in the library or any UIMA component, without any programming via graphical user interfaces, while a command line launcher is also available without GUIs. The evaluation itself is processed in a UIMA component, users can create and plug their own evaluation metrics in addition to the predefined metrics. U-Compare has been successfully used in many projects including BioCreative, Conll and the BioNLP shared task.</abstract>
      <bibkey>kano-etal-2010-u</bibkey>
    </paper>
    <paper id="212">
      <author><first>Janne Bondi</first><last>Johannessen</last></author>
      <author><first>Kristin</first><last>Hagen</last></author>
      <author><first>Anders</first><last>Nøklestad</last></author>
      <author><first>Joel</first><last>Priestley</last></author>
      <title>Enhancing Language Resources with Maps</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/308_Paper.pdf</url>
      <abstract>We will look at how maps can be integrated in research resources, such as language databases and language corpora. By using maps, search results can be illustrated in a way that immediately gives the user information that words or numbers on their own would not give. We will illustrate with two different resources, into which we have now added a Google Maps application: The Nordic Dialect Corpus (Johannessen et al. 2009) and The Nordic Syntactic Judgments Database (Lindstad et al. 2009). We have integrated Google Maps into these applications. The database contains some hundred syntactic test sentences that have been evaluated by four speakers in more than hundred locations in Norway and Sweden. Searching for the evaluations of a particular sentence gives a list of several hundred judgments, which are difficult for a human researcher to assess. With the map option, isoglosses are immediately visible. We show in the paper that both with the maps depicting corpus hits and with the maps depicting database results, the map visualizations actually show clear geographical differences that would be very difficult to spot just by reading concordance lines or database tables.</abstract>
      <bibkey>johannessen-etal-2010-enhancing</bibkey>
    </paper>
    <paper id="213">
      <author><first>Jana Z.</first><last>Sukkarieh</last></author>
      <author><first>Eleanor</first><last>Bolge</last></author>
      <title>Building a Textual Entailment Suite for the Evaluation of Automatic Content Scoring Technologies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/310_Paper.pdf</url>
      <abstract>Automatic content scoring for free-text responses has started to emerge as an application of Natural Language Processing in its own right, much like question answering or machine translation. The task, in general, is reduced to comparing a students answer to a model answer. Although a considerable amount of work has been done, common benchmarks and evaluation measures for this application do not currently exist. It is yet impossible to perform a comparative evaluation or progress tracking of this application across systems ― an application that we view as a textual entailment task. This paper concentrates on introducing an Educational Testing Service-built test suite that makes a step towards establishing such a benchmark. The suite can be used as regression and performance evaluations both intra-c-raterÂ® or inter automatic content scoring technologies. It is important to note that existing textual entailment test suites like PASCAL RTE or FraCas, though beneficial, are not suitable for our purposes since we deal with atypical naturally-occurring student responses that need to be categorized in order to serve as regression test cases.</abstract>
      <bibkey>sukkarieh-bolge-2010-building</bibkey>
    </paper>
    <paper id="214">
      <author><first>Haïfa</first><last>Zargayouna</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <title>Evaluation of Textual Knowledge Acquisition Tools: a Challenging Task</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/311_Paper.pdf</url>
      <abstract>A large effort has been devoted to the development of textual knowledge acquisition (KA) tools, but it is still difficult to assess the progress that has been made. The results produced by these tools are difficult to compare, due to the heterogeneity of the proposed methods and of their goals. Various experiments have been made to evaluate terminological and ontological tools. They show that in terminology as well as in ontology acquisition, it remains difficult to compare existing tools and to analyse their advantages and drawbacks. From our own experiments in evaluating terminology and ontology acquisition tools, it appeared that the difficulties and solutions are similar for both tasks. We propose a unified approach for the evaluation of textual KA tools that can be instantiated in different ways for various tasks. The main originality of this approach lies in the way it takes into account the subjectivity of evaluation and the relativity of gold standards. In this paper, we highlight the major difficulties of KA evaluation, we then present a unified proposal for the evaluation of terminologies and ontologies acquisition tools and the associated experiments. The proposed protocols take into consideration the specificity of this type of evaluation.</abstract>
      <bibkey>zargayouna-nazarenko-2010-evaluation</bibkey>
    </paper>
    <paper id="215">
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <title>Providing Multilingual, Multimodal Answers to Lexical Database Queries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/312_Paper.pdf</url>
      <abstract>Language users are increasingly turning to electronic resources to address their lexical information needs, due to their convenience and their ability to simultaneously capture different facets of lexical knowledge in a single interface. In this paper, we discuss techniques to respond to a user's lexical queries by providing multilingual and multimodal information, and facilitating navigating along different types of links. To this end, structured information from sources like WordNet, Wikipedia, Wiktionary, as well as Web services is linked and integrated to provide a multi-faceted yet consistent response to user queries. The meanings of words in many different languages are characterized by mapping them to appropriate WordNet sense identifiers and adding multilingual gloss descriptions as well as example sentences. Relationships are derived from WordNet and Wiktionary to allow users to discover semantically related words, etymologically related words, alternative spellings, as well as misspellings. Last but not least, images, audio recordings, and geographical maps extracted from Wikipedia and Wiktionary allow for a multimodal experience.</abstract>
      <bibkey>de-melo-weikum-2010-providing</bibkey>
    </paper>
    <paper id="216">
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Martina</first><last>Johnson</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <title>Building an <fixed-case>I</fixed-case>talian <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et through Semi-automatic Corpus Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/313_Paper.pdf</url>
      <abstract>n this paper, we outline the methodology we adopted to develop a FrameNet for Italian. The main element of novelty with respect to the original FrameNet is represented by the fact that the creation and annotation of Lexical Units is strictly grounded in distributional information (statistical distribution of verbal subcategorization frames, lexical and semantic preferences of each frame) automatically acquired from a large, dependency-parsed corpus. We claim that this approach allows us to overcome some of the shortcomings of the classical lexicographic method used to create FrameNet, by complementing the accuracy of manual annotation with the robustness of data on the global distributional patterns of a verb. In the paper, we describe our method for extracting distributional data from the corpus and the way we used it for the encoding and annotation of LUs. The long-term goal of our project is to create an electronic lexicon for Italian similar to the original English FrameNet. For the moment, we have developed a database of syntactic valences that will be made freely accessible via a web interface. This represents an autonomous resource besides the FrameNet lexicon, of which we have a beginning nucleus consisting of 791 annotated sentences.</abstract>
      <bibkey>lenci-etal-2010-building</bibkey>
    </paper>
    <paper id="217">
      <author><first>Rein Ove</first><last>Sikveland</last></author>
      <author><first>Anton</first><last>Öttl</last></author>
      <author><first>Ingunn</first><last>Amdal</last></author>
      <author><first>Mirjam</first><last>Ernestus</last></author>
      <author><first>Torbjørn</first><last>Svendsen</last></author>
      <author><first>Jens</first><last>Edlund</last></author>
      <title>Spontal-N: A Corpus of Interactional Spoken <fixed-case>N</fixed-case>orwegian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/314_Paper.pdf</url>
      <abstract>Spontal-N is a corpus of spontaneous, interactional Norwegian. To our knowledge, it is the first corpus of Norwegian in which the majority of speakers have spent significant parts of their lives in Sweden, and in which the recorded speech displays varying degrees of interference from Swedish. The corpus consists of studio quality audio- and video-recordings of four 30-minute free conversations between acquaintances, and a manual orthographic transcription of the entire material. On basis of the orthographic transcriptions, we automatically annotated approximately 50 percent of the material on the phoneme level, by means of a forced alignment between the acoustic signal and pronunciations listed in a dictionary. Approximately seven percent of the automatic transcription was manually corrected. Taking the manual correction as a gold standard, we evaluated several sources of pronunciation variants for the automatic transcription. Spontal-N is intended as a general purpose speech resource that is also suitable for investigating phonetic detail.</abstract>
      <bibkey>sikveland-etal-2010-spontal</bibkey>
    </paper>
    <paper id="218">
      <author><first>Svetla</first><last>Koeva</last></author>
      <author><first>Diana</first><last>Blagoeva</last></author>
      <author><first>Siya</first><last>Kolkovska</last></author>
      <title><fixed-case>B</fixed-case>ulgarian National Corpus Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/316_Paper.pdf</url>
      <abstract>The paper presents Bulgarian National Corpus project (BulNC) - a large-scale, representative, online available corpus of Bulgarian. The BulNC is also a monolingual general corpus, fully morpho-syntactically (and partially semantically) annotated, and manually provided with detailed meta-data descriptions. Presently the Bulgarian National corpus consists of about 320 000 000 graphical words and includes more than 10 000 samples. Briefly the corpus structure and the accepted criteria for representativeness and well-balancing are presented. The query language for advance search of collocations and concordances is demonstrated with some examples - it allows to retrieve word combinations, ordered queries, inflexionally and semantically related words, part-of-speech tags, utilising Boolean operations and grouping as well. The BulNC already plays a significant role in natural language processing of Bulgarian contributing to scientific advances in spelling and grammar checking, word sense disambiguation, speech recognition, text categorisation, topic extraction and machine translation. The BulNC can also be used in different investigations going beyond the linguistics: library studies, social sciences research, teaching methods studies, etc.</abstract>
      <bibkey>koeva-etal-2010-bulgarian</bibkey>
    </paper>
    <paper id="219">
      <author><first>Donghui</first><last>Lin</last></author>
      <author><first>Yoshiaki</first><last>Murakami</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <author><first>Yohei</first><last>Murakami</last></author>
      <author><first>Masahiro</first><last>Tanaka</last></author>
      <title>Composing Human and Machine Translation Services: Language Grid for Improving Localization Processes</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/317_Paper.pdf</url>
      <abstract>With the development of the Internet environments, more and more language services become accessible for common people. However, the gap between human translators and machine translators remains huge especially for the domain of localization processes that requires high translation quality. Although efforts of combining human and machine translators for supporting multilingual communication have been reported in previous research, how to apply such approaches for improving localization processes are rarely discussed. In this paper, we aim at improving localization processes by composing human and machine translation services based on the Language Grid, which is a language service platform that we have developed. Further, we conduct experiments to compare the translation quality and translation cost using several translation processes, including absolute machine translation processes, absolute human translation processes and translation processes by human and machine translation services. The experiment results show that composing monolingual roles and dictionary services improves the translation quality of machine translators, and that collaboration of human and machine translators is possible to reduce the cost comparing with the absolute bilingual human translation. We also discuss the generality of the experimental results and further challenging issues of the proposed localization processes.</abstract>
      <bibkey>lin-etal-2010-composing</bibkey>
    </paper>
    <paper id="220">
      <author><first>Boris</first><last>Haselbach</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <title>The Development of a Morphosyntactic Tagset for <fixed-case>A</fixed-case>frikaans and its Use with Statistical Tagging</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/318_Paper.pdf</url>
      <abstract>In this paper, we present a morphosyntactic tagset for Afrikaans based on the guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES). We compare our slim yet expressive tagset, MAATS (Morphosyntactic AfrikAans TagSet), with an existing one which primarily focuses on a detailed morphosyntactic and semantic description of word forms. MAATS will primarily be used for the extraction of lexical data from large pos-tagged corpora. We not only focus on morphosyntactic properties but also on the processability with statistical tagging. We discuss the tagset design and motivate our classification of Afrikaans word forms, in particular we focus on the categorization of verbs and conjunctions. The complete tagset in presented and we briefly discuss each word class. In a case study with an Afrikaans newspaper corpus, we evaluate our tagset with four different statistical taggers. Despite a relatively small amount of training data, however with a large tagger lexicon, TnT-Tagger scores 97.05 % accuracy. Additionally, we present some error sources and discuss future work.</abstract>
      <bibkey>haselbach-heid-2010-development</bibkey>
    </paper>
    <paper id="221">
      <author><first>Sophia Yat Mei</first><last>Lee</last></author>
      <author><first>Ying</first><last>Chen</last></author>
      <author><first>Shoushan</first><last>Li</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <title>Emotion Cause Events: Corpus Construction and Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/322_Paper.pdf</url>
      <abstract>Emotion processing has always been a great challenge. Given the fact that an emotion is triggered by cause events and that cause events are an integral part of emotion, this paper constructs a Chinese emotion cause corpus as a first step towards automatic inference of cause-emotion correlation. The corpus focuses on five primary emotions, namely happiness, sadness, fear, anger, and surprise. It is annotated with emotion cause events based on our proposed annotation scheme. Corpus data shows that most emotions are expressed with causes, and that causes mostly occur before the corresponding emotion verbs. We also examine the correlations between emotions and cause events in terms of linguistic cues: causative verbs, perception verbs, epistemic markers, conjunctions, prepositions, and others. Results show that each group of linguistic cues serves as an indicator marking the cause events in different structures of emotional constructions. We believe that the emotion cause corpus will be the useful resource for automatic emotion cause detection as well as emotion detection and classification.</abstract>
      <bibkey>lee-etal-2010-emotion</bibkey>
    </paper>
    <paper id="222">
      <author><first>Costanza</first><last>Navarretta</last></author>
      <title>The <fixed-case>DAD</fixed-case> Parallel Corpora and their Uses</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/325_Paper.pdf</url>
      <abstract>This paper deals with the uses of the annotations of third person singular neuter pronouns in the DAD parallel and comparable corpora of Danish and Italian texts and spoken data. The annotations contain information about the functions of these pronouns and their uses as abstract anaphora. Abstract anaphora have constructions such as verbal phrases, clauses and discourse segments as antecedents and refer to abstract objects comprising events, situations and propositions. The analysis of the annotated data shows the language specific characteristics of abstract anaphora in the two languages compared with the uses of abstract anaphora in English. Finally, the paper presents machine learning experiments run on the annotated data in order to identify the functions of third person singular neuter personal pronouns and neuter demonstrative pronouns. The results of these experiments vary from corpus to corpus. However, they are all comparable with the results obtained in similar tasks in other languages. This is very promising because the experiments have been run on both written and spoken data using a classification of the pronominal functions which is much more fine-grained than the classifications used in other studies.</abstract>
      <bibkey>navarretta-2010-dad</bibkey>
    </paper>
    <paper id="223">
      <author><first>Andrew</first><last>Thwaites</last></author>
      <author><first>Jeroen</first><last>Geertzen</last></author>
      <author><first>William D.</first><last>Marslen-Wilson</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <title><fixed-case>LIPS</fixed-case>: A Tool for Predicting the Lexical Isolation Point of a Word</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/326_Paper.pdf</url>
      <abstract>We present LIPS (Lexical Isolation Point Software), a tool for accurate lexical isolation point (IP) prediction in recordings of speech. The IP is the point in time in which a word is correctly recognised given the acoustic evidence available to the hearer. The ability to accurately determine lexical IPs is of importance to work in the field of cognitive processing, since it enables the evaluation of competing models of word recognition. IPs are also of importance in the field of neurolinguistics, where the analyses of high-temporal-resolution neuroimaging data require a precise time alignment of the observed brain activity with the linguistic input. LIPS provides an attractive alternative to costly multi-participant perception experiments by automatically computing IPs for arbitrary words. On a test set of words, the LIPS system predicts IPs with a mean difference from the actual IP of within 1ms. The difference from the predicted and actual IP approximate to a normal distribution with a standard deviation of around 80ms (depending on the model used).</abstract>
      <bibkey>thwaites-etal-2010-lips</bibkey>
    </paper>
    <paper id="224">
      <author><first>Caroline</first><last>Williams</last></author>
      <author><first>Andrew</first><last>Thwaites</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <author><first>Jeroen</first><last>Geertzen</last></author>
      <author><first>Billi</first><last>Randall</last></author>
      <author><first>Meredith</first><last>Shafto</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <author><first>Lorraine</first><last>Tyler</last></author>
      <title>The <fixed-case>C</fixed-case>ambridge Cookie-Theft Corpus: A Corpus of Directed and Spontaneous Speech of Brain-Damaged Patients and Healthy Individuals</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/327_Paper.pdf</url>
      <abstract>Investigating differences in linguistic usage between individuals who have suffered brain injury (hereafter patients) and those who havent can yield a number of benefits. It provides a better understanding about the precise way in which impairments affect patients language, improves theories of how the brain processes language, and offers heuristics for diagnosing certain types of brain damage based on patients speech. One method for investigating usage differences involves the analysis of spontaneous speech. In the work described here we construct a text corpus consisting of transcripts of individuals speech produced during two tasks: the Boston-cookie-theft picture description task (Goodglass and Kaplan, 1983) and a spontaneous speech task, which elicits a semi-prompted monologue, and/or free speech. Interviews with patients from 19yrs to 89yrs were transcribed, as were interviews with a comparable number of healthy individuals (20yrs to 89yrs). Structural brain images are available for approximately 30% of participants. This unique data source provides a rich resource for future research in many areas of language impairment and has been constructed to facilitate analysis with natural language processing and corpus linguistics techniques.</abstract>
      <bibkey>williams-etal-2010-cambridge</bibkey>
    </paper>
    <paper id="225">
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>René</first><last>van Horik</last></author>
      <author><first>Stef</first><last>Scagliola</last></author>
      <author><first>Eric</first><last>Sanders</last></author>
      <author><first>Paula</first><last>Witkamp</last></author>
      <title>The <fixed-case>V</fixed-case>eteran<fixed-case>T</fixed-case>apes: Research Corpus, Fragment Processing Tool, and Enhanced Publications for the e-Humanities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/328_Paper.pdf</url>
      <abstract>Enhanced Publications are a new way to publish scientific and other results in an electronic article. The advantage of EPs is that the relation between the article and the underlying data facilitate the peer review process and other quality assessment activities. Due to the link between de publication and the research data the publication can be much richer than a paper edition permits. We present an example of EPs in which links are made to interview fragments that include transcripts, audio segments, annotations and metadata. EPs call for a new paradigm of research methodology in which digital persistent access to research data are a central issue. In this contribution we highlight 1. The research data as it is archived and curated, 2. the concept ""enhanced publication"" and its scientific value, 3. the ""fragment fitter tool"", a language processing tool to facilitate the creation of EPs, 4. IPR issues related to the re-use of the interview data.</abstract>
      <bibkey>van-den-heuvel-etal-2010-veterantapes</bibkey>
    </paper>
    <paper id="226">
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <author><first>Manuel</first><last>Kirschner</last></author>
      <author><first>Zorana</first><last>Ratkovic</last></author>
      <title>Context Fusion: The Role of Discourse Structure and Centering Theory</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/330_Paper.pdf</url>
      <abstract>Questions are not asked in isolation. Their context, viz. the preceding interactions, might be of help to understand them and retrieve the correct answer. Previous research in Interactive Question Answering showed that context fusion has a big potential to improve the performance of answer retrieval. In this paper, we study how much context, and what elements of it, should be considered to answer Follow-Up Questions (FU Qs). Following previous research, we exploit Logistic Regression Models to learn aspects of dialogue structure relevant to answering FU Qs. We enrich existing models based on shallow features with deep features, relying on the theory of discourse structure of (Chai and Jin, 2004), and on Centering Theory, respectively. Using models trained on realistic IQA data, we show which of the various theoretically motivated features hold up against empirical evidence. We also show that, while these deep features do not outperform the shallow ones on their own, an IQA system's answer correctness increases if the shallow and deep features are combined.</abstract>
      <bibkey>bernardi-etal-2010-context</bibkey>
    </paper>
    <paper id="227">
      <author><first>Jun</first><last>Okamoto</last></author>
      <author><first>Shun</first><last>Ishizaki</last></author>
      <title>Homographic Ideogram Understanding Using Contextual Dynamic Network</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/331_Paper.pdf</url>
      <abstract>Conventional methods for disambiguation problems have been using statistical methods with co-occurrence of words in their contexts. It seems that human-beings assign appropriate word senses to the given ambiguous word in the sentence depending on the words which followed the ambiguous word when they could not disambiguate by using the previous contextual information. In this research, Contextual Dynamic Network Model is developed using the Associative Concept Dictionary which includes semantic relations among concepts/words and the relations can be represented with quantitative distances among them. In this model, an interactive activation method is used to identify a words meaning on the Contextual Semantic Network where the activation values on the network are calculated using the distances. The proposed method constructs dynamically the Contextual Semantic Network according to the input words sequentially that appear in the sentence including an ambiguous word. Therefore, in this research, after the model calculates the activation values, if there is little difference between the activation values, it reconstructs the network depending on the next words in input sentence. The evaluation of proposed method showed that the accuracy rates are high when Contextual Semantic Network has high density whose node are extended using around the ambiguous word.</abstract>
      <bibkey>okamoto-ishizaki-2010-homographic</bibkey>
    </paper>
    <paper id="228">
      <author><first>Cheikh M. Bamba</first><last>Dione</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <title>Design and Development of Part-of-Speech-Tagging Resources for <fixed-case>W</fixed-case>olof (<fixed-case>N</fixed-case>iger-<fixed-case>C</fixed-case>ongo, spoken in <fixed-case>S</fixed-case>enegal)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/333_Paper.pdf</url>
      <abstract>In this paper, we report on the design of a part-of-speech-tagset for Wolof and on the creation of a semi-automatically annotated gold standard. In order to achieve high-quality annotation relatively fast, we first generated an accurate lexicon that draws on existing word and name lists and takes into account inflectional and derivational morphology. The main motivation for the tagged corpus is to obtain data for training automatic taggers with machine learning approaches. Hence, we took machine learning considerations into account during tagset design and we present training experiments as part of this paper. The best automatic tagger achieves an accuracy of 95.2% in cross-validation experiments. We also wanted to create a basis for experimenting with annotation projection techniques, which exploit parallel corpora. For this reason, it was useful to use a part of the Bible as the gold standard corpus, for which sentence-aligned parallel versions in many languages are easy to obtain. We also report on preliminary experiments exploiting a statistical word alignment of the parallel text.</abstract>
      <bibkey>dione-etal-2010-design</bibkey>
    </paper>
    <paper id="229">
      <author><first>Roser</first><last>Morante</last></author>
      <title>Descriptive Analysis of Negation Cues in Biomedical Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/335_Paper.pdf</url>
      <abstract>In this paper we present a description of negation cues and their scope in biomedical texts, based on the cues that occur in the BioScope corpus. We provide information about the morphological type of the cue, the characteristics of the scope in relation to the morpho-syntactic features of the cue and of the clause, and the ambiguity level of the cue by describing in which cases certain negation cues do not express negation. Additionally, we provide positive and negative examples per cue from the BioScope corpus. We show that the scope depends mostly on the part-of-speech of the cue and on the syntactic features of the clause. Although several studies have focused on processing negation in biomedical texts, we are not aware of publicly available resources that describe the scope of negation cues in detail. This paper aims at providing information for producing guidelines to annotate corpora with a negation layer, and for building resources that find the scope of negation cues automatically.</abstract>
      <bibkey>morante-2010-descriptive</bibkey>
    </paper>
    <paper id="230">
      <author><first>Xuchen</first><last>Yao</last></author>
      <author><first>Irina</first><last>Borisova</last></author>
      <author><first>Mehwish</first><last>Alam</last></author>
      <title><fixed-case>PDTB</fixed-case> <fixed-case>XML</fixed-case>: the <fixed-case>XML</fixed-case>ization of the <fixed-case>P</fixed-case>enn <fixed-case>D</fixed-case>iscourse <fixed-case>T</fixed-case>ree<fixed-case>B</fixed-case>ank 2.0</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/336_Paper.pdf</url>
      <abstract>The current study presents a conversion and unification of the Penn Discourse TreeBank 2.0 (PDTB) and the Penn TreeBank (PTB) under XML format. The main goal of the PDTB XML is to create a tool for efficient and broad querying of the syntax and discourse information simultaneously. The key stages of the project are developing proper cross-references between different data types and their representation in the modified TIGER-XML format, and then writing the required declarative languages (XML Schema). PTB XML is compatible with TIGER-XML format. The PDTB XML is developed as a unified format for the convenience of XQuery users; it integrates discourse relations and XML structures into one unified hierarchy and builds the cross references between the syntactic trees and the discourse relations. The syntactic and discourse elements are assigned with unique IDs in order to build cross-references between them. The converted corpus allows for a simultaneous search for syntactically specified discourse information based on the XQuery standard, which is illustrated with a simple example in the article.</abstract>
      <bibkey>yao-etal-2010-pdtb</bibkey>
    </paper>
    <paper id="231">
      <author><first>Agnieszka</first><last>Mykowiecka</last></author>
      <author><first>Katarzyna</first><last>Głowińska</last></author>
      <author><first>Joanna</first><last>Rabiega-Wiśniewska</last></author>
      <title>Domain-related Annotation of <fixed-case>P</fixed-case>olish Spoken Dialogue Corpus <fixed-case>LUNA</fixed-case>.<fixed-case>PL</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/337_Paper.pdf</url>
      <abstract>The paper presents a corpus of Polish spoken dialogues annotated on several levels, from transcription of dialogues and their morphosyntactic analysis, to semantic annotation. The LUNA.PL corpus is the first semantically annotated corpus of Polish spontaneous speech. It contains 500 dialogues recorded at the Warsaw Transport Authority call centre. For each dialogue, the corpus contains recorded audio signal, its transcription and five XML files with annotations on subsequent levels. Speech transcription was done manually. Text annotation was constructed using a combination of rule based programmes and computer-aided manual work. For morphological annotation we used the already existing analyzer and manually disambiguated the results. Morphologically annotated texts of dialogues were automatically segmented into elementary syntactic chunks. Semantic annotation was done by a set of specially designed rules and then manually corrected. The paper describes details of the domain related semantic annotation which consists of two levels - concept level at which around 200 attributes and their values are annotated, and predicate level at which 47 frame types are recognized. We describe the domain model accepted, and the statistics over the entire annotated set of dialogues.</abstract>
      <bibkey>mykowiecka-etal-2010-domain</bibkey>
    </paper>
    <paper id="232">
      <author><first>Lubomir</first><last>Otrusina</last></author>
      <author><first>Pavel</first><last>Smrz</last></author>
      <title>A New Approach to Pseudoword Generation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/339_Paper.pdf</url>
      <abstract>Sense-tagged corpora are used to evaluate word sense disambiguation (WSD) systems. Manual creation of such resources is often prohibitively expensive. That is why the concept of pseudowords - conflations of two or more unambiguous words - has been integrated into WSD evaluation experiments. This paper presents a new method of pseudoword generation which takes into account semantic-relatedness of the candidate words forming parts of the pseudowords to the particular senses of the word to be disambiguated. We compare the new approach to its alternatives and show that the results on pseudowords, that are more similar to real ambiguous words, better correspond to the actual results. Two techniques assessing the similarity are studied - the first one takes advantage of manually created dictionaries (wordnets), the second one builds on the automatically computed statistical data obtained from large corpora. Pros and cons of the two techniques are discussed and the results on a standard task are demonstrated.</abstract>
      <bibkey>otrusina-smrz-2010-new</bibkey>
    </paper>
    <paper id="233">
      <author><first>Horacio</first><last>Saggion</last></author>
      <author><first>Elena</first><last>Stein-Sparvieri</last></author>
      <author><first>David</first><last>Maldavsky</last></author>
      <author><first>Sandra</first><last>Szasz</last></author>
      <title><fixed-case>NLP</fixed-case> Resources for the Analysis of Patient/Therapist Interviews</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/341_Paper.pdf</url>
      <abstract>We present a set of tools and resources for the analysis of interviews during psychotherapy sessions. One of the main components of the work is a dictionary-based text interpretation tool for the Spanish language. The tool is designed to identify a subset of Freudian drives in patient and therapist discourse.</abstract>
      <bibkey>saggion-etal-2010-nlp</bibkey>
    </paper>
    <paper id="234">
      <author><first>Dafydd</first><last>Gibbon</last></author>
      <author><first>Moses</first><last>Ekpenyong</last></author>
      <author><first>Eno-Abasi</first><last>Urua</last></author>
      <title><fixed-case>M</fixed-case>edefaidrin: Resources Documenting the Birth and Death Language Life-cycle</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/342_Paper.pdf</url>
      <abstract>Language resources are typically defined and created for application in speech technology contexts, but the documentation of languages which are unlikely ever to be provided with enabling technologies nevertheless plays an important role in defining the heritage of a speech community and in the provision of basic insights into the language oriented components of human cognition. This is particularly true of endangered languages. The present case study concerns the documentation both of the birth and of the endangerment within a rather short space of time of a spirit language, Medefaidrin, created and used as a vehicular language by a religious community in South-Eastern Nigeria. The documentation shows phonological, orthographic, morphological, syntactic and textual typological features of Medefaidrin which indicate that typological properties of English were a model for the creation of the language, rather than typological properties of the enclaving language, Ibibio. The documentation is designed as part of the West African Language Archive (WALA), following OLAC metadata standards.</abstract>
      <bibkey>gibbon-etal-2010-medefaidrin</bibkey>
    </paper>
    <paper id="235">
      <author><first>Satoshi</first><last>Sato</last></author>
      <author><first>Sayoko</first><last>Kaide</last></author>
      <title>A Person-Name Filter for Automatic Compilation of Bilingual Person-Name Lexicons</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/343_Paper.pdf</url>
      <abstract>This paper proposes a simple and fast person-name filter, which plays an important role in automatic compilation of a large bilingual person-name lexicon. This filter is based on pn_score, which is the sum of two component scores, the score of the first name and that of the last name. Each score is calculated from two term sets: one is a dense set in which most of the members are person names; another is a baseline set that contains less person names. The pn_score takes one of five values, {+2, +1, 0, -1, -2}, which correspond to strong positive, positive, undecidable, negative, and strong negative, respectively. This pn_score can be easily extended to bilingual pn_score that takes one of nine values, by summing scores of two languages. Experimental results show that our method works well for monolingual person names in English and Japanese; the F-score of each language is 0.929 and 0.939, respectively. The performance of the bilingual person-name filter is better; the F-score is 0.955.</abstract>
      <bibkey>sato-kaide-2010-person</bibkey>
    </paper>
    <paper id="236">
      <author><first>Rania</first><last>Al-Sabbagh</last></author>
      <author><first>Roxana</first><last>Girju</last></author>
      <title>Mining the Web for the Induction of a Dialectical <fixed-case>A</fixed-case>rabic Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/344_Paper.pdf</url>
      <abstract>This paper describes the first phase of building a lexicon of Egyptian Cairene Arabic (ECA) ― one of the most widely understood dialects in the Arab World ― and Modern Standard Arabic (MSA). Each ECA entry is mapped to its MSA synonym, Part-of-Speech (POS) tag and top-ranked contexts based on Web queries; and thus each entry is provided with basic syntactic and semantic information for a generic lexicon compatible with multiple NLP applications. Moreover, through their MSA synonyms, ECA entries acquire access to MSA available NLP tools and resources which are considerably available. Using an associationist approach based on the correlations between word co-occurrence patterns in both dialects, we change the direction of the acquisition process from parallel to circular to overcome a bottleneck of current research on Arabic dialects, namely the lack of parallel corpora, and to alleviate accuracy rates for using unrelated Web documents which are more frequently available. Manually evaluated for 1,000 word entries by two native speakers of the ECA-MSA varieties, the proposed approach achieves a promising F-measured performance rate of 70.9%. In discussion to the proposed algorithm, different semantic issues are highlighted for upcoming phases of the induction of a more comprehensive ECA-MSA lexicon.</abstract>
      <bibkey>al-sabbagh-girju-2010-mining</bibkey>
    </paper>
    <paper id="237">
      <author><first>Tobias</first><last>Heinroth</last></author>
      <author><first>Dan</first><last>Denich</last></author>
      <author><first>Alexander</first><last>Schmitt</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <title>Efficient Spoken Dialogue Domain Representation and Interpretation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/345_Paper.pdf</url>
      <abstract>We provide a detailed look on the functioning of the OwlSpeak Spoken Dialogue Manager, which is part of the EU-funded project ATRACO. OwlSpeak interprets Spoken Dialogue Ontologies and on this basis generates VoiceXML dialogue snippets. The dialogue snippets can be interpreted by all speech servers that provide VoiceXML support and therefore make the dialogue management independent from the hosting systems providing speech recognition and synthesis. Ontologies are used within the framework of our prototype to represent specific spoken dialogue domains that can dynamically be broadened or tightened during an ongoing dialogue. We provide an exemplary dialogue encoded as OWL model and explain how this model is interpreted by the dialogue manager. The combination of a unified model for dialogue domains and the strict model-view-controller architecture that underlies the dialogue manager lead to an efficient system that allows for a new way of spoken dialogue system development and can be used for further research on adaptive spoken dialogue strategies.</abstract>
      <bibkey>heinroth-etal-2010-efficient</bibkey>
    </paper>
    <paper id="238">
      <author><first>Philippe</first><last>Dreuw</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <author><first>Gregorio</first><last>Martinez</last></author>
      <author><first>Onno</first><last>Crasborn</last></author>
      <author><first>Justus</first><last>Piater</last></author>
      <author><first>Jose Miguel</first><last>Moya</last></author>
      <author><first>Mark</first><last>Wheatley</last></author>
      <title>The <fixed-case>S</fixed-case>ign<fixed-case>S</fixed-case>peak Project - Bridging the Gap Between Signers and Speakers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/346_Paper.pdf</url>
      <abstract>The SignSpeak project will be the first step to approach sign language recognition and translation at a scientific level already reached in similar research fields such as automatic speech recognition or statistical machine translation of spoken languages. Deaf communities revolve around sign languages as they are their natural means of communication. Although deaf, hard of hearing and hearing signers can communicate without problems amongst themselves, there is a serious challenge for the deaf community in trying to integrate into educational, social and work environments. The overall goal of SignSpeak is to develop a new vision-based technology for recognizing and translating continuous sign language to text. New knowledge about the nature of sign language structure from the perspective of machine recognition of continuous sign language will allow a subsequent breakthrough in the development of a new vision-based technology for continuous sign language recognition and translation. Existing and new publicly available corpora will be used to evaluate the research progress throughout the whole project.</abstract>
      <bibkey>dreuw-etal-2010-signspeak</bibkey>
    </paper>
    <paper id="239">
      <author><first>Junko</first><last>Kubo</last></author>
      <author><first>Keita</first><last>Tsuji</last></author>
      <author><first>Shigeo</first><last>Sugimoto</last></author>
      <title>Automatic Term Recognition Based on the Statistical Differences of Relative Frequencies in Different Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/347_Paper.pdf</url>
      <abstract>In this paper, we propose a method for automatic term recognition (ATR) which uses the statistical differences of relative frequencies of terms in target domain corpus and elsewhere. Generally, the target terms appear more frequently in target domain corpus than in other domain corpora. Utilizing such characteristics will lead to the improvement of extraction performance. Most of the ATR methods proposed so far only use the target domain corpus and do not take such characteristics into account. For the extraction experiment, we used the abstracts of a women's studies journal as a target domain corpus and those of academic journals of 39 domains as other domain corpora. The women's studies terms which were used for extraction evaluation were manually identified terms in the abstracts. The extraction performance was analyzed and we found that our method outperformed earlier methods. The previous methods were based on C-value, FLR and methods which were also used with other domain corpora.</abstract>
      <bibkey>kubo-etal-2010-automatic</bibkey>
    </paper>
    <paper id="240">
      <author><first>Violeta</first><last>Seretan</last></author>
      <author><first>Eric</first><last>Wehrli</last></author>
      <author><first>Luka</first><last>Nerima</last></author>
      <author><first>Gabriela</first><last>Soare</last></author>
      <title><fixed-case>F</fixed-case>ips<fixed-case>R</fixed-case>omanian: Towards a <fixed-case>R</fixed-case>omanian Version of the Fips Syntactic Parser</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/350_Paper.pdf</url>
      <abstract>We describe work in progress on the development of a full syntactic parser for Romanian. This work is part of a larger project of multilingual extension of the Fips parser (Wehrli, 2007), already available for French, English, German, Spanish, Italian, and Greek, to four new languages (Romanian, Romansh, Russian and Japanese). The Romanian version was built by starting with the Fips generic parsing architecture for the Romance languages and customising the grammatical component, in close relation to the development of the lexical component. We describe this process and report on preliminary results obtained for journalistic texts.</abstract>
      <bibkey>seretan-etal-2010-fipsromanian</bibkey>
    </paper>
    <paper id="241">
      <author><first>Jens</first><last>Edlund</last></author>
      <author><first>Jonas</first><last>Beskow</last></author>
      <author><first>Kjell</first><last>Elenius</last></author>
      <author><first>Kahl</first><last>Hellmer</last></author>
      <author><first>Sofia</first><last>Strönbergsson</last></author>
      <author><first>David</first><last>House</last></author>
      <title><fixed-case>S</fixed-case>pontal: A <fixed-case>S</fixed-case>wedish Spontaneous Dialogue Corpus of Audio, Video and Motion Capture</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/352_Paper.pdf</url>
      <abstract>We present the Spontal database of spontaneous Swedish dialogues. 120 dialogues of at least 30 minutes each have been captured in high-quality audio, high-resolution video and with a motion capture system. The corpus is currently being processed and annotated, and will be made available for research at the end of the project.</abstract>
      <bibkey>edlund-etal-2010-spontal</bibkey>
    </paper>
    <paper id="242">
      <author><first>Walid</first><last>Magdy</last></author>
      <author><first>Jinming</first><last>Min</last></author>
      <author><first>Johannes</first><last>Leveling</last></author>
      <author><first>Gareth J. F.</first><last>Jones</last></author>
      <title>Building a Domain-specific Document Collection for Evaluating Metadata Effects on Information Retrieval</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/353_Paper.pdf</url>
      <abstract>This paper describes the development of a structured document collection containing user-generated text and numerical metadata for exploring the exploitation of metadata in information retrieval (IR). The collection consists of more than 61,000 documents extracted from YouTube video pages on basketball in general and NBA (National Basketball Association) in particular, together with a set of 40 topics and their relevance judgements. In addition, a collection of nearly 250,000 user profiles related to the NBA collection is available. Several baseline IR experiments report the effect of using video-associated metadata on retrieval effectiveness. The results surprisingly show that searching the videos titles only performs significantly better than searching additional metadata text fields of the videos such as the tags or the description.</abstract>
      <bibkey>magdy-etal-2010-building</bibkey>
    </paper>
    <paper id="243">
      <author><first>Horacio</first><last>Saggion</last></author>
      <author><first>Adam</first><last>Funk</last></author>
      <title>Interpreting <fixed-case>S</fixed-case>enti<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et for Opinion Classification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/354_Paper.pdf</url>
      <abstract>We describe a set of tools, resources, and experiments for opinion classification in business-related datasources in two languages. In particular we concentrate on SentiWordNet text interpretation to produce word, sentence, and text-based sentiment features for opinion classification. We achieve good results in experiments using supervised learning machine over syntactic and sentiment-based features. We also show preliminary experiments where the use of summaries before opinion classification provides competitive advantage over the use of full documents.</abstract>
      <bibkey>saggion-funk-2010-interpreting</bibkey>
    </paper>
    <paper id="244">
      <author><first>Doris</first><last>Baum</last></author>
      <author><first>Daniel</first><last>Schneider</last></author>
      <author><first>Rolf</first><last>Bardeli</last></author>
      <author><first>Jochen</first><last>Schwenninger</last></author>
      <author><first>Barbara</first><last>Samlowski</last></author>
      <author><first>Thomas</first><last>Winkler</last></author>
      <author><first>Joachim</first><last>Köhler</last></author>
      <title><fixed-case>D</fixed-case>i<fixed-case>SC</fixed-case>o - A <fixed-case>G</fixed-case>erman Evaluation Corpus for Challenging Problems in the Broadcast Domain</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/355_Paper.pdf</url>
      <abstract>Typical broadcast material contains not only studio-recorded texts read by trained speakers, but also spontaneous and dialect speech, debates with cross-talk, voice-overs, and on-site reports with difficult acoustic environments. Standard approaches to speech and speaker recognition usually deteriorate under such conditions. This paper reports on the design, construction, and experimental analysis of DiSCo, a German corpus for the evaluation of speech and speaker recognition on challenging material from the broadcast domain. One of the key requirements for the design of this corpus was a good coverage of different types of serious programmes beyond clean speech and planned speech broadcast news. Corpus annotation encompasses manual segmentation, an orthographic transcription, and labelling with speech mode, dialect, and noise type. We indicate typical use cases for the corpus by reporting results from ASR, speech search, and speaker recognition on the new corpus, thereby obtaining insights into the difficulty of audio recognition on the various classes.</abstract>
      <bibkey>baum-etal-2010-disco</bibkey>
    </paper>
    <paper id="245">
      <author><first>Antonio</first><last>Balvet</last></author>
      <author><first>Cyril</first><last>Courtin</last></author>
      <author><first>Dominique</first><last>Boutet</last></author>
      <author><first>Christian</first><last>Cuxac</last></author>
      <author><first>Ivani</first><last>Fusellier-Souza</last></author>
      <author><first>Brigitte</first><last>Garcia</last></author>
      <author><first>Marie-Thérèse</first><last>L’Huillier</last></author>
      <author><first>Marie-Anne</first><last>Sallandre</last></author>
      <title>The Creagest Project: a Digitized and Annotated Corpus for <fixed-case>F</fixed-case>rench <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage (<fixed-case>LSF</fixed-case>) and Natural Gestural Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/356_Paper.pdf</url>
      <abstract>In this paper, we discuss the theoretical, sociolinguistic, methodological and technical objectives and issues of the French Creagest Project (2007-2012) in setting up, documenting and annotating a large corpus of adult and child French Sign Language (LSF) and of natural gestural language. The main objective of this ANR-funded research project is to set up a collaborative web-based platform for the study of semiogenesis in LSF (French Sign Language), i.e. the study of emerging structures and signs, be they used by Deaf adult signers, Deaf children, or even by Deaf and hearing subjects in interaction. In section 2, we address theoretical and practical issues, emphasizing the outstanding features of the Creagest Project. In section 3, we deal with methodological issues for data collection. Finally, in section 4, we examine technical aspects of LSF video data editing and corpus annotation, in the perspective of setting up a corpus-based formalized description of LSF.</abstract>
      <bibkey>balvet-etal-2010-creagest</bibkey>
    </paper>
    <paper id="246">
      <author><first>Jesús</first><last>Tomás</last></author>
      <author><first>Alejandro</first><last>Canovas</last></author>
      <author><first>Jaime</first><last>Lloret</last></author>
      <author><first>Miguel García</first><last>Pineda</last></author>
      <author><first>Jose L.</first><last>Abad</last></author>
      <title>Speech Translation in Pedagogical Environment Using Additional Sources of Knowledge</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/357_Paper.pdf</url>
      <abstract>A key aspect in the development of statistical translators is the synergic combination of different sources of knowledge. This work describes the effect and implications that would have adding additional other-than-voice information in a voice translation system. In the model discussed the additional information serves as the bases for the log-linear combination of several statistical models. A prototype that implements a real-time speech translation system from Spanish to English that is adapted to specific teaching-related environments is presented. In the scenario of analysis a teacher as speaker giving an educational class could use a real time translation system with foreign students. The teacher could add slides or class notes as additional reference to the voice translation system. Should notes be already translated into the destination language the system could have even more accuracy. We present the theoretical framework of the problem, summarize the overall architecture of the system, show how the system is enhanced with capabilities related to capturing the additional information; and finally present the initial performance results.</abstract>
      <bibkey>tomas-etal-2010-speech</bibkey>
    </paper>
    <paper id="247">
      <author><first>Elena</first><last>Grishina</last></author>
      <author><first>Svetlana</first><last>Savchuk</last></author>
      <author><first>Alexej</first><last>Poljakov</last></author>
      <title>Design and Data Collection for the Accentological Corpus of the <fixed-case>R</fixed-case>ussian Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/358_Paper.pdf</url>
      <abstract>Accentological corpus provides a researcher an opportunity to study word stress and stress variation, which are very important for the Russian language. Moreover, Accentological corpus allows studying the history of the Russian language stress development. The research presents the main characteristics of Accentological corpus available at ruscorpora.ru. Corpora size, type and sources of text material, the way it is represented in the corpora, types of linguistic annotation, corpora composition and ways of their effective use according to their purposes are described. There are two zones in the Accentological corpus. 1) The zone of prose includes oral texts and films transcripts, in which stressed syllables are marked according to the real pronunciation. 2) The zone of poetry contains texts with marked accented syllables, so it is possible to define the exact word stress using special rules. The Accentological corpus has four types of annotations (metatextual, morphological, semantic and sociological) and also has its own accentological mark-up. Due to accentological annotation each word is supplied with stress marks, so a user can make queries and retrieve the stressed or unstressed word forms in combination with grammatical and semantic features.</abstract>
      <bibkey>grishina-etal-2010-design</bibkey>
    </paper>
    <paper id="248">
      <author><first>Paul</first><last>Felt</last></author>
      <author><first>Owen</first><last>Merkling</last></author>
      <author><first>Marc</first><last>Carmen</last></author>
      <author><first>Eric</first><last>Ringger</last></author>
      <author><first>Warren</first><last>Lemmon</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <author><first>Robbie</first><last>Haertel</last></author>
      <title><fixed-case>CCASH</fixed-case>: A Web Application Framework for Efficient, Distributed Language Resource Development</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/360_Paper.pdf</url>
      <abstract>We introduce CCASH (Cost-Conscious Annotation Supervised by Humans), an extensible web application framework for cost-efficient annotation. CCASH provides a framework in which cost-efficient annotation methods such as Active Learning can be explored via user studies and afterwards applied to large annotation projects. CCASHs architecture is described as well as the technologies that it is built on. CCASH allows custom annotation tasks to be built from a growing set of useful annotation widgets. It also allows annotation methods (such as AL) to be implemented in any language. Being a web application framework, CCASH offers secure centralized data and annotation storage and facilitates collaboration among multiple annotations. By default it records timing information about each annotation and provides facilities for recording custom statistics. The CCASH framework has been used to evaluate a novel annotation strategy presented in a concurrently published paper, and will be used in the future to annotate a large Syriac corpus.</abstract>
      <bibkey>felt-etal-2010-ccash</bibkey>
    </paper>
    <paper id="249">
      <author><first>Michael</first><last>Pucher</last></author>
      <author><first>Friedrich</first><last>Neubarth</last></author>
      <author><first>Volker</first><last>Strom</last></author>
      <author><first>Sylvia</first><last>Moosmüller</last></author>
      <author><first>Gregor</first><last>Hofer</last></author>
      <author><first>Christian</first><last>Kranzler</last></author>
      <author><first>Gudrun</first><last>Schuchmann</last></author>
      <author><first>Dietmar</first><last>Schabus</last></author>
      <title>Resources for Speech Synthesis of Viennese Varieties</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/361_Paper.pdf</url>
      <abstract>This paper describes our work on developing corpora of three varieties of Viennese for unit selection speech synthesis. The synthetic voices for Viennese varieties, implemented with the open domain unit selection speech synthesis engine Multisyn of Festival will also be released within Festival. The paper especially focuses on two questions: how we selected the appropriate speakers and how we obtained the text sources needed for the recording of these non-standard varieties. Regarding the first one, it turned out that working with a prototypical professional speaker was much more preferable than striving for authenticity. In addition, we give a brief outline about the differences between the Austrian standard and its dialectal varieties and how we solved certain technical problems that are related to these differences. In particular, the specific set of phones applicable to each variety had to be determined by applying various constraints. Since such a set does not serve any descriptive purposes but rather is influencing the quality of speech synthesis, a careful design of such a set was an important task.</abstract>
      <bibkey>pucher-etal-2010-resources</bibkey>
    </paper>
    <paper id="250">
      <author><first>Michael</first><last>Wiegand</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <title>Predictive Features for Detecting Indefinite Polar Sentences</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/362_Paper.pdf</url>
      <abstract>In recent years, text classification in sentiment analysis has mostly focused on two types of classification, the distinction between objective and subjective text, i.e. subjectivity detection, and the distinction between positive and negative subjective text, i.e. polarity classification. So far, there has been little work examining the distinction between definite polar subjectivity and indefinite polar subjectivity. While the former are utterances which can be categorized as either positive or negative, the latter cannot be categorized as either of these two categories. This paper presents a small set of domain independent features to detect indefinite polar sentences. The features reflect the linguistic structure underlying these types of utterances. We give evidence for the effectiveness of these features by incorporating them into an unsupervised rule-based classifier for sentence-level analysis and compare its performance with supervised machine learning classifiers, i.e. Support Vector Machines (SVMs) and Nearest Neighbor Classifier (kNN). The data used for the experiments are web-reviews collected from three different domains.</abstract>
      <bibkey>wiegand-klakow-2010-predictive</bibkey>
    </paper>
    <paper id="251">
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Fabienne</first><last>Fritzinger</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <author><first>Marie</first><last>Hinrichs</last></author>
      <author><first>Thomas</first><last>Zastrow</last></author>
      <title>Term and Collocation Extraction by Means of Complex Linguistic Web Services</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/363_Paper.pdf</url>
      <abstract>We present a web service-based environment for the use of linguistic resources and tools to address issues of terminology and language varieties. We discuss the architecture, corpus representation formats, components and a chainer supporting the combination of tools into task-specific services. Integrated into this environment, single web services also become part of complex scenarios for web service use. Our web services take for example corpora of several million words as an input on which they perform preprocessing, such as tokenisation, tagging, lemmatisation and parsing, and corpus exploration, such as collocation extraction and corpus comparison. Here we present an example on extraction of single and multiword items typical of a specific domain or typical of a regional variety of German. We also give a critical review on needs and available functions from a user's point of view. The work presented here is part of ongoing experimentation in the D-SPIN project, the German national counterpart of CLARIN.</abstract>
      <bibkey>heid-etal-2010-term</bibkey>
    </paper>
    <paper id="252">
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <title>Preparing the field for an Open Resource Infrastructure: the role of the <fixed-case>FL</fixed-case>a<fixed-case>R</fixed-case>e<fixed-case>N</fixed-case>et Network of Excellence</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/369_Paper.pdf</url>
      <abstract>In order to overcome the fragmentation that affects the field of Language Resources and Technologies, an Open and Distributed Resource Infrastructure is the necessary step for building on each other achievements, integrating resources and technologies and avoiding dispersed or conflicting efforts. Since this endeavour represents a true cultural turnpoint in the LRs field, it needs a careful preparation, both in terms of acceptance by the community and thoughtful investigation of the various technical, organisational and practical aspects implied. To achieve this, we need to act as a community able to join forces on a set of shared priorities and we need to act at a worldwide level. FLaReNet ― Fostering Language Resources Network ― is a Thematic Network funded under the EU eContent program that aims at developing the needed common vision and fostering a European and International strategy for consolidating the sector, thus enhancing competitiveness at EU level and worldwide. In this paper we present the activities undertaken by FLaReNet in order to prepare and support the establishment of such an Infrastructure, which is becoming now a reality within the new MetaNet initiative.</abstract>
      <bibkey>calzolari-soria-2010-preparing</bibkey>
    </paper>
    <paper id="253">
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <author><first>Riccardo</first><last>Del Gratta</last></author>
      <author><first>Sara</first><last>Goggi</last></author>
      <author><first>Valeria</first><last>Quochi</last></author>
      <author><first>Irene</first><last>Russo</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Joseph</first><last>Mariani</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <title>The <fixed-case>LREC</fixed-case> Map of Language Resources and Technologies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/370_Paper.pdf</url>
      <abstract>In this paper we present the LREC Map of Language Resources and Tools, an innovative feature introduced with this LREC. The purpose of the Map is to shed light on the vast amount of resources and tools that represent the background of the research presented at LREC, in the attempt to fill in a gap in the community knowledge about the resources and tools that are used or created worldwide. It also aims at a change of culture in the field, actively engaging each researcher in the documentation task about resources. The Map has been developed on the basis of the information provided by LREC authors during the submission of papers to the LREC 2010 conference and the LREC workshops, and contains information about almost 2000 resources. The paper illustrates the motivation behind this initiative, its main characteristics, its relevance and future impact in the field, the metadata used to describe the resources, and finally presents some of the most relevant findings.</abstract>
      <bibkey>calzolari-etal-2010-lrec</bibkey>
    </paper>
    <paper id="254">
      <author><first>Nicolas</first><last>Moreau</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <author><first>Djamel</first><last>Mostefa</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>Jordi</first><last>Turmo</last></author>
      <author><first>Pere R.</first><last>Comas</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Davide</first><last>Buscaldi</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <title>Evaluation Protocol and Tools for Question-Answering on Speech Transcripts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/372_Paper.pdf</url>
      <abstract>Question Answering (QA) technology aims at providing relevant answers to natural language questions. Most Question Answering research has focused on mining document collections containing written texts to answer written questions. In addition to written sources, a large (and growing) amount of potentially interesting information appears in spoken documents, such as broadcast news, speeches, seminars, meetings or telephone conversations. The QAST track (Question-Answering on Speech Transcripts) was introduced in CLEF to investigate the problem of question answering in such audio documents. This paper describes in detail the evaluation protocol and tools designed and developed for the CLEF-QAST evaluation campaigns that have taken place between 2007 and 2009. We first remind the data, question sets, and submission procedures that were produced or set up during these three campaigns. As for the evaluation procedure, the interface that was developed to ease the assessors work is described. In addition, this paper introduces a methodology for a semi-automatic evaluation of QAST systems based on time slot comparisons. Finally, the QAST Evaluation Package 2007-2009 resulting from these evaluation campaigns is also introduced.</abstract>
      <bibkey>moreau-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="255">
      <author><first>Roser</first><last>Sanromà</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <title>The Database of <fixed-case>C</fixed-case>atalan Adjectives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/373_Paper.pdf</url>
      <abstract>We present the Database of Catalan Adjectives (DCA), a database with 2,296 adjective lemmata enriched with morphological, syntactic and semantic information. This set of adjectives has been collected from a fragment of the Corpus Textual Informatitzat de la Llengua Catalana of the Institut dEstudis Catalans and constitutes a representative sample of the adjective class in Catalan as a whole. The database includes both manually coded and automatically extracted information regarding the most prominent properties used in the literature regarding the semantics of adjectives, such as morphological origin, suffix (if any), predicativity, gradability, adjective position with respect to the head noun, adjective modifiers, or semantic class. The DCA can be useful for NLP applications using adjectives (from POS-taggers to Opinion Mining applications) and for linguistic analysis regarding the morphological, syntactic, and semantic properties of adjectives. We now make it available to the research community under a Creative Commons Attribution Share Alike 3.0 Spain license.</abstract>
      <bibkey>sanroma-boleda-2010-database</bibkey>
    </paper>
    <paper id="256">
      <author><first>Amal</first><last>Zouaq</last></author>
      <author><first>Michel</first><last>Gagnon</last></author>
      <author><first>Benoit</first><last>Ozell</last></author>
      <title>Can Syntactic and Logical Graphs help Word Sense Disambiguation?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/374_Paper.pdf</url>
      <abstract>This paper presents a word sense disambiguation (WSD) approach based on syntactic and logical representations. The objective here is to run a number of experiments to compare standard contexts (word windows, sentence windows) with contexts provided by a dependency parser (syntactic context) and a logical analyzer (logico-semantic context). The approach presented here relies on a dependency grammar for the syntactic representations. We also use a pattern knowledge base over the syntactic dependencies to extract flat predicative logical representations. These representations (syntactic and logical) are then used to build context vectors that are exploited in the WSD process. Various state-of-the-art algorithms including Simplified Lesk, Banerjee and Pedersen and frequency of co-occurrences are tested with these syntactic and logical contexts. Preliminary results show that defining context vectors based on these features may improve WSD by comparison with classical word and sentence context windows. However, future experiments are needed to provide more evidence over these issues.</abstract>
      <bibkey>zouaq-etal-2010-syntactic</bibkey>
    </paper>
    <paper id="257">
      <author><first>Meng</first><last>Wang</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Shiwen</first><last>Yu</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <title>Automatic Acquisition of <fixed-case>C</fixed-case>hinese Novel Noun Compounds</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/377_Paper.pdf</url>
      <abstract>Automatic acquisition of novel compounds is notoriously difficult because most novel compounds have relatively low frequency in a corpus. The current study proposes a new method to deal with the novel compound acquisition challenge. We model this task as a two-class classification problem in which a candidate compound is either classified as a compound or a non-compound. A machine learning method using SVM, incorporating two types of linguistically motivated features: semantic features and character features, is applied to identify rare but valid noun compounds. We explore two kinds of training data: one is virtual training data which is obtained by three statistical scores, i.e. co-occurrence frequency, mutual information and dependent ratio, from the frequent compounds; the other is real training data which is randomly selected from the infrequent compounds. We conduct comparative experiments, and the experimental results show that even with limited direct evidence in the corpus for the novel compounds, we can make full use of the typical frequent compounds to help in the discovery of the novel compounds.</abstract>
      <bibkey>wang-etal-2010-automatic</bibkey>
    </paper>
    <paper id="258">
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Suzan</first><last>Verberne</last></author>
      <author><first>Cornelis</first><last>Koster</last></author>
      <title>Constructing a Broad-coverage Lexicon for Text Mining in the Patent Domain</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/378_Paper.pdf</url>
      <abstract>For mining intellectual property texts (patents), a broad-coverage lexicon that covers general English words together with terminology from the patent domain is indispensable. The patent domain is very diffuse as it comprises a variety of technical domains (e.g. Human Necessities, Chemistry &amp; Metallurgy and Physics in the International Patent Classification). As a result, collecting a lexicon that covers the language used in patent texts is not a straightforward task. In this paper we describe the approach that we have developed for the semi-automatic construction of a broad-coverage lexicon for classification and information retrieval in the patent domain and which combines information from multiple sources. Our contribution is twofold. First, we provide insight into the difficulties of developing lexical resources for information retrieval and text mining in the patent domain, a research and development field that is expanding quickly. Second, we create a broad coverage lexicon annotated with rich lexical information and containing both general English word forms and domain terminology for various technical domains.</abstract>
      <bibkey>oostdijk-etal-2010-constructing</bibkey>
    </paper>
    <paper id="259">
      <author><first>Paul</first><last>Bedaride</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <title>Syntactic Testsuites and Textual Entailment Recognition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/379_Paper.pdf</url>
      <abstract>We focus on textual entailments mediated by syntax and propose a new methodology to evaluate textual entailment recognition systems on such data. The main idea is to generate a syntactically annotated corpus of pairs of (non-)entailments and to use error mining methodology from the parsing field to identify the most likely sources of errors. To generate the evaluation corpus we use a template based generation approach where sentences, semantic representations and syntactic annotations are all created at the same time. Furthermore, we adapt the error mining methodology initially proposed for parsing to the field of textual entailment. To illustrate the approach, we apply the proposed methodology to the Afazio RTE system (an hybrid system focusing on syntactic entailment) and show how it permits identifying the most likely sources of errors made by this system on a testsuite of 10 000 (non-)entailment pairs which is balanced in term of (non-)entailment and in term of syntactic annotations.</abstract>
      <bibkey>bedaride-gardent-2010-syntactic</bibkey>
    </paper>
    <paper id="260">
      <author><first>Jan</first><last>Štěpánek</last></author>
      <author><first>Petr</first><last>Pajas</last></author>
      <title>Querying Diverse Treebanks in a Uniform Way</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/381_Paper.pdf</url>
      <abstract>This paper presents a system for querying treebanks in a uniform way. The system is able to work with both dependency and constituency based treebanks in any language. We demonstrate its abilities on 11 different treebanks. The query language used by the system provides many features not available in other existing systems while still keeping the performance efficient. The paper also describes the conversion of ten treebanks into a common XML-based format used by the system, touching the question of standards and formats. The paper then shows several examples of linguistically interesting questions that the system is able to answer, for example browsing verbal clauses without subjects or extraposed relative clauses, generating the underlying grammar in a constituency treebank, searching for non-projective edges in a dependency treebank, or word-order typology of a language based on the treebank. The performance of several implementations of the system is also discussed by measuring the time requirements of some of the queries.</abstract>
      <bibkey>stepanek-pajas-2010-querying</bibkey>
    </paper>
    <paper id="261">
      <author><first>Rodolfo</first><last>Delmonte</last></author>
      <author><first>Antonella</first><last>Bristot</last></author>
      <author><first>Vincenzo</first><last>Pallotta</last></author>
      <title>Deep Linguistic Processing with <fixed-case>GETARUNS</fixed-case> for Spoken Dialogue Understanding</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/383_Paper.pdf</url>
      <abstract>In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. The current goal is that of extracting automatically argumentative information in order to build argumentative structure. The long term goal is using argumentative structure to produce automatic summarization of spoken dialogues. Very much like other deep linguistic processing systems, our system is a generic text/dialogue understanding system that can be used in connection with an ontology ― WordNet - and other similar repositories of commonsense knowledge. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkeley project. In a final section we present preliminary evaluation of the system on two tasks: the task of automatic argumentative labeling and another frequently addressed task: referential vs. non-referential pronominal detection. Results obtained fair much higher than those reported in similar experiments with machine learning approaches.</abstract>
      <bibkey>delmonte-etal-2010-deep</bibkey>
    </paper>
    <paper id="262">
      <author><first>Emad</first><last>Mohamed</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <title><fixed-case>A</fixed-case>rabic Part of Speech Tagging</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/384_Paper.pdf</url>
      <abstract>Arabic is a morphologically rich language, which presents a challenge for part of speech tagging. In this paper, we compare two novel methods for POS tagging of Arabic without the use of gold standard word segmentation but with the full POS tagset of the Penn Arabic Treebank. The first approach uses complex tags that describe full words and does not require any word segmentation. The second approach is segmentation-based, using a machine learning segmenter. In this approach, the words are first segmented, then the segments are annotated with POS tags. Because of the word-based approach, we evaluate full word accuracy rather than segment accuracy. Word-based POS tagging yields better results than segment-based tagging (93.93% vs. 93.41%). Word based tagging also gives the best results on known words, the segmentation-based approach gives better results on unknown words. Combining both methods results in a word accuracy of 94.37%, which is very close to the result obtained by using gold standard segmentation (94.91%).</abstract>
      <bibkey>mohamed-kubler-2010-arabic</bibkey>
    </paper>
    <paper id="263">
      <author><first>Alexander</first><last>Pak</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <title><fixed-case>T</fixed-case>witter as a Corpus for Sentiment Analysis and Opinion Mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/385_Paper.pdf</url>
      <abstract>Microblogging today has become a very popular communication tool among Internet users. Millions of users share opinions on different aspects of life everyday. Therefore microblogging web-sites are rich sources of data for opinion mining and sentiment analysis. Because microblogging has appeared relatively recently, there are a few research works that were devoted to this topic. In our paper, we focus on using Twitter, the most popular microblogging platform, for the task of sentiment analysis. We show how to automatically collect a corpus for sentiment analysis and opinion mining purposes. We perform linguistic analysis of the collected corpus and explain discovered phenomena. Using the corpus, we build a sentiment classifier, that is able to determine positive, negative and neutral sentiments for a document. Experimental evaluations show that our proposed techniques are efficient and performs better than previously proposed methods. In our research, we worked with English, however, the proposed technique can be used with any other language.</abstract>
      <bibkey>pak-paroubek-2010-twitter</bibkey>
    </paper>
    <paper id="264">
      <author><first>Rena</first><last>Nemoto</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <author><first>Jacques</first><last>Durand</last></author>
      <title>Word Boundaries in <fixed-case>F</fixed-case>rench: Evidence from Large Speech Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/386_Paper.pdf</url>
      <abstract>The goal of this paper is to investigate French word segmentation strategies using phonemic and lexical transcriptions as well as prosodic and part-of-speech annotations. Average fundamental frequency (f0) profiles and phoneme duration profiles are measured using 13 hours of broadcast news speech to study prosodic regularities of French words. Some influential factors are taken into consideration for f0 and duration measurements: word syllable length, word-final schwa, part-of-speech. Results from average f0 profiles confirm word final syllable accentuation and from average duration profiles, we can observe long word final syllable length. Both are common tendencies in French. From noun phrase studies, results of average f0 profiles illustrate higher noun first syllable after determiner. Inter-vocalic duration profile results show long inter-vocalic duration between determiner vowel and preceding word vowel. These results reveal measurable cues contributing to word boundary location. Further studies will include more detailed within syllable f0 patterns, other speaking styles and languages.</abstract>
      <bibkey>nemoto-etal-2010-word</bibkey>
    </paper>
    <paper id="265">
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Rosa</first><last>Stern</last></author>
      <title>A Lexicon of <fixed-case>F</fixed-case>rench Quotation Verbs for Automatic Quotation Extraction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/387_Paper.pdf</url>
      <abstract>Quotation extraction is an important information extraction task, especially when dealing with news wires. Quotations can be found in various configurations. In this paper, we focus on direct quotations introduced by a parenthetical clause, headed by a ""quotation verb"". Our study is based on a large French news wire corpus from the Agence France-Presse. We introduce and motivate an analysis at the discursive level of such quotations, which differs from the syntactic analyses generally proposed. We show how we enriched the Lefff syntactic lexicon so that it provides an account for quotation verbs heading a quotation parenthetical, especially those extracted from a news wire corpus. We also sketch how these lexical entries can be extended to the discursive level in order to model quotations introduced in a parenthetical clause in a complete way.</abstract>
      <bibkey>sagot-etal-2010-lexicon</bibkey>
    </paper>
    <paper id="266">
      <author><first>Marie</first><last>Mikulová</last></author>
      <author><first>Jan</first><last>Štěpánek</last></author>
      <title>Ways of Evaluation of the Annotators in Building the <fixed-case>P</fixed-case>rague <fixed-case>C</fixed-case>zech-<fixed-case>E</fixed-case>nglish <fixed-case>D</fixed-case>ependency <fixed-case>T</fixed-case>reebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/388_Paper.pdf</url>
      <abstract>In this paper, we present several ways to measure and evaluate the annotation and annotators, proposed and used during the building of the Czech part of the Prague Czech-English Dependency Treebank. At first, the basic principles of the treebank annotation project are introduced (division to three layers: morphological, analytical and tectogrammatical). The main part of the paper describes in detail one of the important phases of the annotation process: three ways of evaluation of the annotators - inter-annotator agreement, error rate and performance. The measuring of the inter-annotator agreement is complicated by the fact that the data contain added and deleted nodes, making the alignment between annotations non-trivial. The error rate is measured by a set of automatic checking procedures that guard the validity of some invariants in the data. The performance of the annotators is measured by a booking web application. All three measures are later compared and related to each other.</abstract>
      <bibkey>mikulova-stepanek-2010-ways</bibkey>
    </paper>
    <paper id="267">
      <author><first>Klaar</first><last>Vanopstal</last></author>
      <author><first>Robert</first><last>Vander Stichele</last></author>
      <author><first>Godelieve</first><last>Laureys</last></author>
      <author><first>Joost</first><last>Buysschaert</last></author>
      <title>Assessing the Impact of <fixed-case>E</fixed-case>nglish Language Skills and Education Level on <fixed-case>P</fixed-case>ub<fixed-case>M</fixed-case>ed Searches by <fixed-case>D</fixed-case>utch-speaking Users</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/389_Paper.pdf</url>
      <abstract>The aim of this study was to assess the retrieval effectiveness of nursing students in the Dutch-speaking part of Belgium. We tested two groups: students from the master of Nursing and Midwifery training, and students from the bachelor of Nursing program. The test consisted of five parts: first, the students completed an enquiry about their computer skills, experiences with PubMed and how they assessed their own language skills. Secondly, an introduction into the use of MeSH in PubMed was given, followed by a PubMed search. After the literature search, a second enquiry was completed in which the students were asked to give their opinion about the test. To conclude, an official language test was completed. The results of the PubMed search, i.e. a list of articles the students deemed relevant for a particular question, were compared to a gold standard. Precision, recall and F-score were calculated in order to evaluate the efficiency of the PubMed search. We used information from the search process, such as search term formulation and MeSH term selection to evaluate the search process and examined their relationship with the results of the language test and the level of education.</abstract>
      <bibkey>vanopstal-etal-2010-assessing</bibkey>
    </paper>
    <paper id="268">
      <author><first>Yasuharu</first><last>Den</last></author>
      <author><first>Hanae</first><last>Koiso</last></author>
      <author><first>Takehiko</first><last>Maruyama</last></author>
      <author><first>Kikuo</first><last>Maekawa</last></author>
      <author><first>Katsuya</first><last>Takanashi</last></author>
      <author><first>Mika</first><last>Enomoto</last></author>
      <author><first>Nao</first><last>Yoshida</last></author>
      <title>Two-level Annotation of Utterance-units in <fixed-case>J</fixed-case>apanese Dialogs: An Empirically Emerged Scheme</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/391_Paper.pdf</url>
      <abstract>In this paper, we propose a scheme for annotating utterance-level units in Japanese dialogs, which emerged from an analysis of the interrelationship among four schemes, i) inter-pausal units, ii) intonation units, iii) clause units, and iv) pragmatic units. The associations among the labels of these four units were illustrated by multiple correspondence analysis and hierarchical cluster analysis. Based on these results, we prescribe utterance-unit identification rules, which identify two sorts of utterance-units with different granularities: short and long utterance-units. Short utterance-units are identified by acoustic and prosodic disjuncture, and they are considered to constitute units of speaker's planning and hearer's understanding. Long utterance-units, on the other hand, are recognized by syntactic and pragmatic disjuncture, and they are regarded as units of interaction. We explore some characteristics of these utterance-units, focusing particularly on unit duration and syntactic property, other participants' responses, and mismatch between the two-levels. We also discuss how our two-level utterance-units are useful in analyzing cognitive and communicative aspects of spoken dialogs.</abstract>
      <bibkey>den-etal-2010-two</bibkey>
    </paper>
    <paper id="269">
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <title>Statistical <fixed-case>F</fixed-case>rench Dependency Parsing: Treebank Conversion and First Results</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/392_Paper.pdf</url>
      <abstract>We first describe the automatic conversion of the French Treebank (Abeillé and Barrier, 2004), a constituency treebank, into typed projective dependency trees. In order to evaluate the overall quality of the resulting dependency treebank, and to quantify the cases where the projectivity constraint leads to wrong dependencies, we compare a subset of the converted treebank to manually validated dependency trees. We then compare the performance of two treebank-trained parsers that output typed dependency parses. The first parser is the MST parser (Mcdonald et al., 2006), which we directly train on dependency trees. The second parser is a combination of the Berkeley parser (Petrov et al., 2006) and a functional role labeler: trained on the original constituency treebank, the Berkeley parser first outputs constituency trees, which are then labeled with functional roles, and then converted into dependency trees. We found that used in combination with a high-accuracy French POS tagger, the MST parser performs a little better for unlabeled dependencies (UAS=90.3% versus 89.6%), and better for labeled dependencies (LAS=87.6% versus 85.6%).</abstract>
      <bibkey>candito-etal-2010-statistical</bibkey>
    </paper>
    <paper id="270">
      <author><first>Yue</first><last>Ma</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <author><first>Laurent</first><last>Audibert</last></author>
      <title>Formal Description of Resources for Ontology-based Semantic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/393_Paper.pdf</url>
      <abstract>Ontology-based semantic annotation aims at putting fragments of a text in correspondence with proper elements of an ontology such that the formal semantics encoded by the ontology can be exploited to represent text interpretation. In this paper, we formalize a resource for this goal. The main difficulty in achieving good semantic annotations consists in identifying fragments to be annotated and labels to be associated with them. To this end, our approach takes advantage of standard web ontology languages as well as rich linguistic annotation platforms. This in turn is concerned with how to formalize the combination of the ontological and linguistical information, which is a topical issue that has got an increasing discussion recently. Different from existing formalizations, our purpose is to extend ontologies by semantic annotation rules whose complexity increases along two dimensions: the linguistic complexity and the rule syntactic complexity. This solution allows reusing best NLP tools for the production of various levels of linguistic annotations. It also has the merit to distinguish clearly the process of linguistic analysis and the ontological interpretation.</abstract>
      <bibkey>ma-etal-2010-formal</bibkey>
    </paper>
    <paper id="271">
      <author><first>Luis Javier</first><last>Rodríguez-Fuentes</last></author>
      <author><first>Mikel</first><last>Penagarikano</last></author>
      <author><first>Germán</first><last>Bordel</last></author>
      <author><first>Amparo</first><last>Varona</last></author>
      <author><first>Mireia</first><last>Díez</last></author>
      <title><fixed-case>KALAKA</fixed-case>: A <fixed-case>TV</fixed-case> Broadcast Speech Database for the Evaluation of Language Recognition Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/394_Paper.pdf</url>
      <abstract>A speech database, named KALAKA, was created to support the Albayzin 2008 Evaluation of Language Recognition Systems, organized by the Spanish Network on Speech Technologies from May to November 2008. This evaluation, designed according to the criteria and methodology applied in the NIST Language Recognition Evaluations, involved four target languages: Basque, Catalan, Galician and Spanish (official languages in Spain), and included speech signals in other (unknown) languages to allow open-set verification trials. In this paper, the process of designing, collecting data and building the train, development and evaluation datasets of KALAKA is described. Results attained in the Albayzin 2008 LRE are presented as a means of evaluating the database. The performance of a state-of-the-art language recognition system on a closed-set evaluation task is also presented for reference. Future work includes extending KALAKA by adding Portuguese and English as target languages and renewing the set of unknown languages needed to carry out open-set evaluations.</abstract>
      <bibkey>rodriguez-fuentes-etal-2010-kalaka</bibkey>
    </paper>
    <paper id="272">
      <author><first>Jarmila</first><last>Panevová</last></author>
      <author><first>Magda</first><last>Ševčíková</last></author>
      <title>Annotation of Morphological Meanings of Verbs Revisited</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/395_Paper.pdf</url>
      <abstract>Meanings of morphological categories are an indispensable component of representation of sentence semantics. In the Prague Dependency Treebank 2.0, sentence semantics is represented as a dependency tree consisting of labeled nodes and edges. Meanings of morphological categories are captured as attributes of tree nodes; these attributes are called grammatemes. The present paper focuses on morphological meanings of verbs, i.e. on meanings of the morphological category of tense, mood, aspect etc. After several introductory remarks, seven verbal grammatemes used in the PDT 2.0 annotation scenario are briefly introduced. After that, each of the grammatemes is examined. Three verbal grammatemes of the original set were included in the new set without changes, one of the grammatemes was extended, and three of them were substituted for three new ones. The revised grammateme set is to be included in the forthcoming version of PDT (tentatively called PDT 3.0). Rules for automatic and manual assignment of the revised grammatemes are further discussed in the paper.</abstract>
      <bibkey>panevova-sevcikova-2010-annotation</bibkey>
    </paper>
    <paper id="273">
      <author><first>Andrew</first><last>Hickl</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <title>Unsupervised Discovery of Collective Action Frames for Socio-Cultural Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/396_Paper.pdf</url>
      <bibkey>hickl-harabagiu-2010-unsupervised</bibkey>
    </paper>
    <paper id="274">
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <title>Predicting Morphological Types of <fixed-case>C</fixed-case>hinese Bi-Character Words by Machine Learning Approaches</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/397_Paper.pdf</url>
      <abstract>This paper presented an overview of Chinese bi-character words morphological types, and proposed a set of features for machine learning approaches to predict these types based on composite characters information. First, eight morphological types were defined, and 6,500 Chinese bi-character words were annotated with these types. After pre-processing, 6,178 words were selected to construct a corpus named Reduced Set. We analyzed Reduced Set and conducted the inter-annotator agreement test. The average kappa value of 0.67 indicates a substantial agreement. Second, Bi-character words morphological types are considered strongly related with the composite characters parts of speech in this paper, so we proposed a set of features which can simply be extracted from dictionaries to indicate the characters tendency of parts of speech. Finally, we used these features and adopted three machine learning algorithms, SVM, CRF, and Naïve Bayes, to predict the morphological types. On the average, the best algorithm CRF achieved 75% of the annotators performance.</abstract>
      <bibkey>huang-etal-2010-predicting</bibkey>
    </paper>
    <paper id="275">
      <author><first>Nicole</first><last>Novielli</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <title>Studying the Lexicon of Dialogue Acts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/399_Paper.pdf</url>
      <abstract>Dialogue Acts have been well studied in linguistics and attracted computational linguistics research for a long time: they constitute the basis of everyday conversations and can be identified with the communicative goal of a given utterance (e.g. asking for information, stating facts, expressing opinions, agreeing or disagreeing). Even if not constituting any deep understanding of the dialogue, automatic dialogue act labeling is a task that can be relevant for a wide range of applications in both human-computer and human-human interaction. We present a qualitative analysis of the lexicon of Dialogue Acts: we explore the relationship between the communicative goal of an utterance and its affective content as well as the salience of specific word classes for each speech act. The experiments described in this paper fit in the scope of a research study whose long-term goal is to build an unsupervised classifier that simply exploits the lexical semantics of utterances for automatically annotate dialogues with the proper speech acts.</abstract>
      <bibkey>novielli-strapparava-2010-studying</bibkey>
    </paper>
    <paper id="276">
      <author><first>Jorge</first><last>Vivaldi</last></author>
      <author><first>Iria</first><last>da Cunha</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <author><first>Patricia</first><last>Velázquez-Morales</last></author>
      <title>Automatic Summarization Using Terminological and Semantic Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/400_Paper.pdf</url>
      <abstract>This paper presents a new algorithm for automatic summarization of specialized texts combining terminological and semantic resources: a term extractor and an ontology. The term extractor provides the list of the terms that are present in the text together their corresponding termhood. The ontology is used to calculate the semantic similarity among the terms found in the main body and those present in the document title. The general idea is to obtain a relevance score for each sentence taking into account both the termhood of the terms found in such sentence and the similarity among such terms and those terms present in the title of the document. The phrases with the highest score are chosen to take part of the final summary. We evaluate the algorithm with Rouge, comparing the resulting summaries with the summaries of other summarizers. The sentence selection algorithm was also tested as part of a standalone summarizer. In both cases it obtains quite good results although the perception is that there is a space for improvement.</abstract>
      <bibkey>vivaldi-etal-2010-automatic</bibkey>
    </paper>
    <paper id="277">
      <author><first>Olivier</first><last>Hamon</last></author>
      <title>Is my Judge a good One?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/402_Paper.pdf</url>
      <abstract>This paper aims at measuring the reliability of judges in MT evaluation. The scope is two evaluation campaigns from the CESTA project, during which human evaluations were carried out on fluency and adequacy criteria for English-to-French documents. Our objectives were threefold: observe both inter- and intra-judge agreements, and then study the influence of the evaluation design especially implemented for the need of the campaigns. Indeed, a web interface was especially developed to help with the human judgments and store the results, but some design changes were made between the first and the second campaign. Considering the low agreements observed, the judges' behaviour has been analysed in that specific context. We also asked several judges to repeat their own evaluations a few times after the first judgments done during the official evaluation campaigns. Even if judges did not seem to agree fully at first sight, a less strict comparison led to a strong agreement. Furthermore, the evolution of the design during the project seemed to have been a source for the difficulties that judges encountered to keep the same interpretation of quality.</abstract>
      <bibkey>hamon-2010-judge</bibkey>
    </paper>
    <paper id="278">
      <author><first>Mátyás</first><last>Brendel</last></author>
      <author><first>Riccardo</first><last>Zaccarelli</last></author>
      <author><first>Laurence</first><last>Devillers</last></author>
      <title>Building a System for Emotions Detection from Speech to Control an Affective Avatar</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/403_Paper.pdf</url>
      <abstract>In this paper we describe a corpus set together from two sub-corpora. The CINEMO corpus contains acted emotional expression obtained by playing dubbing exercises. This new protocol is a way to collect mood-induced data in large amount which show several complex and shaded emotions. JEMO is a corpus collected with an emotion-detection game and contains more prototypical emotions than CINEMO. We show how the two sub-corpora balance and enrich each other and result in a better performance. We built male and female emotion models and use Sequential Fast Forward Feature Selection to improve detection performances. After feature-selection we obtain good results even with our strict speaker independent testing method. The global corpus contains 88 speakers (38 females, 50 males). This study has been done within the scope of the ANR (National Research Agency) Affective Avatar project which deals with building a system of emotions detection for monitoring an Artificial Agent by voice.</abstract>
      <bibkey>brendel-etal-2010-building</bibkey>
    </paper>
    <paper id="279">
      <author><first>Roxane</first><last>Segers</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Facilitating Non-expert Users of the <fixed-case>KYOTO</fixed-case> Platform: the <fixed-case>TMEKO</fixed-case> Editing Protocol for Synset to Ontology Mappings</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/406_Paper.pdf</url>
      <abstract>This paper presents the general architecture of the TMEKO protocol (Tutoring Methodology for Enriching the Kyoto Ontology) that guides non-expert users through the process of creating mappings from domain wordnet synsets to a shared ontology by answering natural language questions. TMEKO will be part of a Wiki-like community platform currently developed in the Kyoto project (http://www.kyoto-project.eu). The platform provides the architecture for ontology based fact mining to enable knowledge sharing across languages and cultures. A central part of the platform is the Wikyoto editing environment in which users can create their own domain wordnet for seven different languages and define relations to the central and shared ontology based on DOLCE. A substantial part of the mappings will involve important processes and qualities associated with the concept. Therefore, the TMEKO protocol provides specific interviews for creating complex mappings that go beyond subclass and equivalence relations. The Kyoto platform and the TMEKO protocol are developed and applied to the environment domain for seven different languages (English, Dutch, Italian, Spanish, Basque, Japanese and Chinese), but can easily be extended and adapted to other languages and domains.</abstract>
      <bibkey>segers-vossen-2010-facilitating</bibkey>
    </paper>
    <paper id="280">
      <author><first>Ekaterina</first><last>Buyko</last></author>
      <author><first>Elena</first><last>Beisswanger</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <title>The <fixed-case>G</fixed-case>ene<fixed-case>R</fixed-case>eg Corpus for Gene Expression Regulation Events — An Overview of the Corpus and its In-Domain and Out-of-Domain Interoperability</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/407_Paper.pdf</url>
      <abstract>Despite the large variety of corpora in the biomedical domain their annotations differ in many respects, e.g., the coverage of different, highly specialized knowledge domains, varying degrees of granularity of targeted relations, the specificity of linguistic anchoring of relations and named entities in documents, etc. We here present GeneReg (Gene Regulation Corpus), the result of an annotation campaign led by the Jena University Language &amp; Information Engineering (JULIE) Lab. The GeneReg corpus consists of 314 abstracts dealing with the regulation of gene expression in the model organism E. coli. Our emphasis in this paper is on the compatibility of the GeneReg corpus with the alternative Genia event corpus and with several in-domain and out-of-domain lexical resources, e.g., the Specialist Lexicon, FrameNet, and WordNet. The links we established from the GeneReg corpus to these external resources will help improve the performance of the automatic relation extraction engine JREx trained and evaluated on GeneReg.</abstract>
      <bibkey>buyko-etal-2010-genereg</bibkey>
    </paper>
    <paper id="281">
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Shinsuke</first><last>Mori</last></author>
      <title>Word-based Partial Annotation for Efficient Corpus Construction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/408_Paper.pdf</url>
      <abstract>In order to utilize the corpus-based techniques that have proven effective in natural language processing in recent years, costly and time-consuming manual creation of linguistic resources is often necessary. Traditionally these resources are created on the document or sentence-level. In this paper, we examine the benefit of annotating only particular words with high information content, as opposed to the entire sentence or document. Using the task of Japanese pronunciation estimation as an example, we devise a machine learning method that can be trained on data annotated word-by-word. This is done by dividing the estimation process into two steps (word segmentation and word-based pronunciation estimation), and introducing a point-wise estimator that is able to make each decision independent of the other decisions made for a particular sentence. In an evaluation, the proposed strategy is shown to provide greater increases in accuracy using a smaller number of annotated words than traditional sentence-based annotation techniques.</abstract>
      <bibkey>neubig-mori-2010-word</bibkey>
    </paper>
    <paper id="282">
      <author><first>Gertrud</first><last>Faaß</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <title>Design and Application of a Gold Standard for Morphological Analysis: <fixed-case>SMOR</fixed-case> as an Example of Morphological Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/409_Paper.pdf</url>
      <abstract>This paper describes general requirements for evaluating and documenting NLP tools with a focus on morphological analysers and the design of a Gold Standard. It is argued that any evaluation must be measurable and documentation thereof must be made accessible for any user of the tool. The documentation must be of a kind that it enables the user to compare different tools offering the same service, hence the descriptions must contain measurable values. A Gold Standard presents a vital part of any measurable evaluation process, therefore, the corpus-based design of a Gold Standard, its creation and problems that occur are reported upon here. Our project concentrates on SMOR, a morphological analyser for German that is to be offered as a web-service. We not only utilize this analyser for designing the Gold Standard, but also evaluate the tool itself at the same time. Note that the project is ongoing, therefore, we cannot present final results.</abstract>
      <bibkey>faass-etal-2010-design</bibkey>
    </paper>
    <paper id="283">
      <author><first>Richard</first><last>Schwarz</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Fabienne</first><last>Martin</last></author>
      <author><first>Achim</first><last>Stein</last></author>
      <title>Identification of Rare &amp; Novel Senses Using Translations in a Parallel Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/411_Paper.pdf</url>
      <abstract>The identification of rare and novel senses is a challenge in lexicography. In this paper, we present a new method for finding such senses using a word aligned multilingual parallel corpus. We use the Europarl corpus and therein concentrate on French verbs. We represent each occurrence of a French verb as a high dimensional term vector. The dimensions of such a vector are the possible translations of the verb according to the underlying word alignment. The dimensions are weighted by a weighting scheme to adjust to the significance of any particular translation. After collecting these vectors we apply forms of the K-means algorithm on the resulting vector space to produce clusters of distinct senses, so that standard uses produce large homogeneous clusters while rare and novel uses appear in small or heterogeneous clusters. We show in a qualitative and quantitative evaluation that the method can successfully find rare and novel senses.</abstract>
      <bibkey>schwarz-etal-2010-identification</bibkey>
    </paper>
    <paper id="284">
      <author><first>Cláudia</first><last>Freitas</last></author>
      <author><first>Cristina</first><last>Mota</last></author>
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Hugo Gonçalo</first><last>Oliveira</last></author>
      <author><first>Paula</first><last>Carvalho</last></author>
      <title>Second <fixed-case>HAREM</fixed-case>: Advancing the State of the Art of Named Entity Recognition in <fixed-case>P</fixed-case>ortuguese</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/412_Paper.pdf</url>
      <abstract>In this paper, we present Second HAREM, the second edition of an evaluation campaign for Portuguese, addressing named entity recognition (NER). This second edition also included two new tracks: the recognition and normalization of temporal entities (proposed by a group of participants, and hence not covered on this paper) and ReRelEM, the detection of semantic relations between named entities. We summarize the setup of Second HAREM by showing the preserved distinctive features and discussing the changes compared to the first edition. Furthermore, we present the main results achieved and describe the available resources and tools developed under this evaluation, namely,(i) the golden collections, i.e. a set of documents whose named entities and semantic relations between those entities were manually annotated, (ii) the Second HAREM collection (which contains the unannotated version of the golden collection), as well as the participating systems results on it, (iii) the scoring tools, and (iv) SAHARA, a Web application that allows interactive evaluation. We end the paper by offering some remarks about what was learned.</abstract>
      <bibkey>freitas-etal-2010-second</bibkey>
    </paper>
    <paper id="285">
      <author><first>Marc</first><last>Kupietz</last></author>
      <author><first>Cyril</first><last>Belica</last></author>
      <author><first>Holger</first><last>Keibel</last></author>
      <author><first>Andreas</first><last>Witt</last></author>
      <title>The <fixed-case>G</fixed-case>erman Reference Corpus <fixed-case>D</fixed-case>e<fixed-case>R</fixed-case>e<fixed-case>K</fixed-case>o: A Primordial Sample for Linguistic Research</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/414_Paper.pdf</url>
      <abstract>This paper describes DeReKo (Deutsches Referenzkorpus), the Archive of General Reference Corpora of Contemporary Written German at the Institut für Deutsche Sprache (IDS) in Mannheim, and the rationale behind its development. We discuss its design, its legal background, how to access it, available metadata, linguistic annotation layers, underlying standards, ongoing developments, and aspects of using the archive for empirical linguistic research. The focus of the paper is on the advantages of DeReKo's design as a primordial sample from which virtual corpora can be drawn for the specific purposes of individual studies. Both concepts, primordial sample and virtual corpus are explained and illustrated in detail. Furthermore, we describe in more detail how DeReKo deals with the fact that all its texts are subject to third parties' intellectual property rights, and how it deals with the issue of replicability, which is particularly challenging given DeReKo's dynamic growth and the possibility to construct from it an open number of virtual corpora.</abstract>
      <bibkey>kupietz-etal-2010-german</bibkey>
    </paper>
    <paper id="286">
      <author><first>Eva</first><last>Sassolini</last></author>
      <author><first>Alessandra</first><last>Cinini</last></author>
      <title>Cultural Heritage: Knowledge Extraction from Web Documents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/415_Paper.pdf</url>
      <abstract>This article presents the use of NLP techniques (text mining, text analysis) to develop specific tools that allow to create linguistic resources related to the cultural heritage domain. The aim of our approach is to create tools for the building of an online knowledge network, automatically extracted from text materials concerning this domain. A particular methodology was experimented by dividing the automatic acquisition of texts, and consequently, the creation of reference corpus in two phases. In the first phase, on-line documents have been extracted from lists of links provided by human experts. All documents extracted from the web by means of automatic spider have been stored in a repository of text materials. On the basis of these documents, automatic parsers create the reference corpus for the cultural heritage domain. Relevant information and semantic concepts are then extracted from this corpus. In a second phase, all these semantically relevant elements (such as proper names, names of institutions, names of places, and other relevant terms) have been used as basis for a new search strategy of text materials from heterogeneous sources. In this case also specialized crawlers (TP-crawler) have been used to work on a bulk of text materials available on line.</abstract>
      <bibkey>sassolini-cinini-2010-cultural</bibkey>
    </paper>
    <paper id="287">
      <author><first>Marta</first><last>Villegas</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Santiago</first><last>Bel</last></author>
      <author><first>Víctor</first><last>Rodríguez</last></author>
      <title>A Case Study on Interoperability for Language Resources and Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/418_Paper.pdf</url>
      <abstract>This paper reports our experience when integrating differ resources and services into a grid environment. The use case we address implies the deployment of several NLP applications as web services. The ultimate objective of this task was to create a scenario where researchers have access to a variety of services they can operate. These services should be easy to invoke and able to interoperate between one another. We essentially describe the interoperability problems we faced, which involve metadata interoperability, data interoperability and service interoperability. We devote special attention to service interoperability and explore the possibility to define common interfaces and semantic description of services. While the web services paradigm suits the integration of different services very well, this requires mutual understanding and the accommodation to common interfaces that not only provide technical solution but also ease the userâs work. Defining common interfaces benefits interoperability but requires the agreement about operations and the set of inputs/outputs. Semantic annotation allows defining some sort of taxonomy that organizes and collects the set of admissible operations and types input/output parameters.</abstract>
      <bibkey>villegas-etal-2010-case</bibkey>
    </paper>
    <paper id="288">
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <title>Semi-Automated Extension of a Specialized Medical Lexicon for <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/420_Paper.pdf</url>
      <abstract>This paper describes the development of a specialized lexical resource for a specialized domain, namely medicine. First, in order to assess the linguistic phenomena that need to be adressed, we based our observation on a large collection of more than 300'000 terms, organised around conceptual identifiers. Based on these observations, we highlight the specificities that such a lexicon should take into account, namely in terms of inflectional and derivational knowledge. In a first experiment, we show that general resources lack a large part of the words needed to process specialized language. Secondly, we describe an experiment to feed semi-automatically a medical lexicon and populate it with inflectional information. This experiment is based on a semi-automatic methods that tries to acquire inflectional knowledge from frequent endings of words recorded in existing lexicon. Thanks to this, we increased the coverage of the target vocabulary from 14.1% to 25.7%.</abstract>
      <bibkey>cartoni-zweigenbaum-2010-semi</bibkey>
    </paper>
    <paper id="289">
      <author><first>Kyle</first><last>Duarte</last></author>
      <author><first>Sylvie</first><last>Gibet</last></author>
      <title>Heterogeneous Data Sources for Signed Language Analysis and Synthesis: The <fixed-case>S</fixed-case>ign<fixed-case>C</fixed-case>om Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/421_Paper.pdf</url>
      <abstract>This paper describes how heterogeneous data sources captured in the SignCom project may be used for the analysis and synthesis of French Sign Language (LSF) utterances. The captured data combine video data and multimodal motion capture (mocap) data, including body and hand movements as well as facial expressions. These data are pre-processed, synchronized, and enriched by text annotations of signed language elicitation sessions. The addition of mocap data to traditional data structures provides additional phonetic data to linguists who desire to better understand the various parts of signs (handshape, movement, orientation, etc.) to very exacting levels, as well as their interactions and relative timings. We show how the phonologies of hand configurations and articulator movements may be studied using signal processing and statistical analysis tools to highlight regularities or temporal schemata between the different modalities. Finally, mocap data allows us to replay signs using a computer animation engine, specifically editing and rearranging movements and configurations in order to create novel utterances.</abstract>
      <bibkey>duarte-gibet-2010-heterogeneous</bibkey>
    </paper>
    <paper id="290">
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Alexander</first><last>Pak</last></author>
      <author><first>Djamel</first><last>Mostefa</last></author>
      <title>Annotations for Opinion Mining Evaluation in the Industrial Context of the <fixed-case>DOXA</fixed-case> project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/422_Paper.pdf</url>
      <abstract>After presenting opinion and sentiment analysis state of the art and the DOXA project, we review the few evaluation campaigns that have dealt in the past with opinion mining. Then we present the two level opinion and sentiment model that we will use for evaluation in the DOXA project and the annotation interface we use for hand annotating a reference corpus. We then present the corpus which will be used on DOXA and report on the hand-annotation task on a corpus of comments on video games and the solution adopted to obtain a sufficient level of inter-annotator agreement.</abstract>
      <bibkey>paroubek-etal-2010-annotations</bibkey>
    </paper>
    <paper id="291">
      <author><first>Milen</first><last>Kouylekov</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <title>Mining <fixed-case>W</fixed-case>ikipedia for Large-scale Repositories of Context-Sensitive Entailment Rules</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/425_Paper.pdf</url>
      <abstract>This paper focuses on the central role played by lexical information in the task of Recognizing Textual Entailment. In particular, the usefulness of lexical knowledge extracted from several widely used static resources, represented in the form of entailment rules, is compared with a method to extract lexical information from Wikipedia as a dynamic knowledge resource. The proposed acquisition method aims at maximizing two key features of the resulting entailment rules: coverage (i.e. the proportion of rules successfully applied over a dataset of TE pairs), and context sensitivity (i.e. the proportion of rules applied in appropriate contexts). Evaluation results show that Wikipedia can be effectively used as a source of lexical entailment rules, featuring both higher coverage and context sensitivity with respect to other resources.</abstract>
      <bibkey>kouylekov-etal-2010-mining</bibkey>
    </paper>
    <paper id="292">
      <author><first>Sara</first><last>Stymne</last></author>
      <author><first>Lars</first><last>Ahrenberg</last></author>
      <title>Using a Grammar Checker for Evaluation and Postprocessing of Statistical Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/426_Paper.pdf</url>
      <abstract>One problem in statistical machine translation (SMT) is that the output often is ungrammatical. To address this issue, we have investigated the use of a grammar checker for two purposes in connection with SMT: as an evaluation tool and as a postprocessing tool. To assess the feasibility of the grammar checker on SMT output, we performed an error analysis, which showed that the precision of error identification in general was higher on SMT output than in previous studies on human texts. Using the grammar checker as an evaluation tool gives a complementary picture to standard metrics such as Bleu, which do not account well for grammaticality. We use the grammar checker as a postprocessing tool by automatically applying the error correction suggestions it gives. There are only small overall improvements of the postprocessing on automatic metrics, but the sentences that are affected by the changes are improved, as shown both by automatic metrics and by a human error analysis. These results indicate that grammar checker techniques are a useful complement to SMT.</abstract>
      <bibkey>stymne-ahrenberg-2010-using</bibkey>
    </paper>
    <paper id="293">
      <author><first>Marion</first><last>Weller</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <title>Extraction of <fixed-case>G</fixed-case>erman Multiword Expressions from Parsed Corpora Using Context Features</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/428_Paper.pdf</url>
      <abstract>We report about tools for the extraction of German multiword expressions (MWEs) from text corpora; we extract word pairs, but also longer MWEs of different patterns, e.g. verb-noun structures with an additional prepositional phrase or adjective. Next to standard association-based extraction, we focus on morpho-syntactic, syntactic and lexical-choice features of the MWE candidates. A broad range of such properties (e.g. number and definiteness of nouns, adjacency of the MWEs components and their position in the sentence, preferred lexical modifiers, etc.) along with relevant example sentences, are extracted from dependency-parsed text and stored in a data base. A sample precision evaluation and an analysis of extraction errors are provided along with the discussion of our extraction architecture. We furthermore measure the contribution of the features to the precision of the extraction: by using both morpho-syntactic and syntactic features, we achieve a higher precision in the identification of idiomatic MWEs, than by using only properties of one type.</abstract>
      <bibkey>weller-heid-2010-extraction</bibkey>
    </paper>
    <paper id="294">
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <author><first>Eric</first><last>de La Clergerie</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <title>The Second Evaluation Campaign of <fixed-case>PASSAGE</fixed-case> on Parsing of <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/430_Paper.pdf</url>
      <bibkey>paroubek-etal-2010-second</bibkey>
    </paper>
    <paper id="295">
      <author><first>Kepa Joseba</first><last>Rodríguez</last></author>
      <author><first>Francesca</first><last>Delogu</last></author>
      <author><first>Yannick</first><last>Versley</last></author>
      <author><first>Egon W.</first><last>Stemle</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <title>Anaphoric Annotation of <fixed-case>W</fixed-case>ikipedia and Blogs in the Live Memories Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/431_Paper.pdf</url>
      <abstract>The Live Memories corpus is an Italian corpus annotated for anaphoric relations. This annotation effort aims to contribute to two significant issues for the CL research: the lack of annotated anaphoric resources for Italian and the increasing interest for the social Web. The Live Memories Corpus contains texts from the Italian Wikipedia about the region Trentino/Süd Tirol and from blog sites with users' comments. It is planned to add a set of articles of local news papers. The corpus includes manual annotated information about morphosyntactic agreement, anaphoricity, and semantic class of the NPs. The anaphoric annotation includes discourse deixis, bridging relations and markes cases of ambiguity with the annotation of alternative interpretations. For the annotation of the anaphoric links the corpus takes into account specific phenomena of the Italian language like incorporated clitics and phonetically non realized pronouns. Reliability studies for the annotation of the mentioned phenomena and for annotation of anaphoric links in general offer satisfactory results. The Wikipedia and blogs dataset will be distributed under Creative Commons Attributions licence.</abstract>
      <bibkey>rodriguez-etal-2010-anaphoric</bibkey>
    </paper>
    <paper id="296">
      <author><first>Dan</first><last>Flickinger</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Gisle</first><last>Ytrestøl</last></author>
      <title><fixed-case>W</fixed-case>iki<fixed-case>W</fixed-case>oods: Syntacto-Semantic Annotation for <fixed-case>E</fixed-case>nglish <fixed-case>W</fixed-case>ikipedia</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/432_Paper.pdf</url>
      <abstract>WikiWoods is an ongoing initiative to provide rich syntacto-semantic annotations for English Wikipedia. We sketch an automated processing pipeline to extract relevant textual content from Wikipedia sources, segment documents into sentence-like units, parse and disambiguate using a broad-coverage precision grammar, and support the export of syntactic and semantic information in various formats. The full parsed corpus is accompanied by a subset of Wikipedia articles for which gold-standard annotations in the same format were produced manually. This subset was selected to represent a coherent domain, Wikipedia entries on the broad topic of Natural Language Processing.</abstract>
      <bibkey>flickinger-etal-2010-wikiwoods</bibkey>
    </paper>
    <paper id="297">
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Caroline</first><last>Sporleder</last></author>
      <author><first>Fabian</first><last>Shirokov</last></author>
      <title>Speaker Attribution in Cabinet Protocols</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/434_Paper.pdf</url>
      <abstract>Historical cabinet protocols are a useful resource which enable historians to identify the opinions expressed by politicians on different subjects and at different points of time. While cabinet protocols are often available in digitized form, so far the only method to access their information content is by keyword-based search, which often returns sub-optimal results. We present a method for enriching German cabinet protocols with information about the originators of statements. This requires automatic speaker attribution. Unlike many other approaches, our method can also deal with cases in which the speaker is not explicitly identified in the sentence itself. Such cases are very common in our domain. To avoid costly manual annotation of training data, we design a rule-based system which exploits morpho-syntactic cues. We show that such a system obtains good results, especially with respect to recall which is particularly important for information access.</abstract>
      <bibkey>ruppenhofer-etal-2010-speaker</bibkey>
    </paper>
    <paper id="298">
      <author><first>Nick</first><last>Webb</last></author>
      <author><first>David</first><last>Benyon</last></author>
      <author><first>Jay</first><last>Bradley</last></author>
      <author><first>Preben</first><last>Hansen</last></author>
      <author><first>Oil</first><last>Mival</last></author>
      <title><fixed-case>W</fixed-case>izard of <fixed-case>O</fixed-case>z Experiments for a Companion Dialogue System: Eliciting Companionable Conversation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/435_Paper.pdf</url>
      <abstract>Within the EU-funded COMPANIONS project, we are working to evaluate new collaborative conversational models of dialogue. Such an evaluation requires us to benchmark approaches to companionable dialogue. In order to determine the impact of system strategies on our evaluation paradigm, we need to generate a range of companionable conversations, using dialogue strategies such as `empathy' and `positivity'. By companionable dialogue, we mean interactions that take user input of some scenario, and respond in a manner appropriate to the emotional content of the user utterance. In this paper, we describe our working Wizard of Oz (WoZ) system for systematically creating dialogues that fulfil these potential strategies, and enables us to deploy a range of potential techniques for selecting which parts of user input to address is which order, to inform the wizard response to the user based on a manual, on-the-fly assessment of the polarity of the user input.</abstract>
      <bibkey>webb-etal-2010-wizard</bibkey>
    </paper>
    <paper id="299">
      <author><first>Carlos Gómez</first><last>Gallo</last></author>
      <author><first>T. Florian</first><last>Jaeger</last></author>
      <author><first>Katrina</first><last>Furth</last></author>
      <title>A Database for the Exploration of <fixed-case>S</fixed-case>panish Planning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/436_Paper.pdf</url>
      <abstract>We describe a new task-based corpus in the Spanish language. The corpus consists of videos, transcripts, and annotations of the inter- action between a naive speaker and a confederate listener. The speaker instructs the listener to MOVE, ROTATE, or PAINT objects on a computer screen. This resource can be used to study how participants produce instructions in a collaborative goal-oriented scenario, in Spanish. The data set is ideally suited for investigating incremental processes of the production and interpretation of language. We demonstrate here how to use this corpus to explore language-specific differences in utterance planning, for English and Spanish speakers.</abstract>
      <bibkey>gallo-etal-2010-database</bibkey>
    </paper>
    <paper id="300">
      <author><first>Irina</first><last>Temnikova</last></author>
      <title>Cognitive Evaluation Approach for a Controlled Language Post-Editing Experiment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/437_Paper.pdf</url>
      <abstract>In emergency situations it is crucial that instructions are straightforward to understand. For this reason a controlled language for crisis management (CLCM), based on psycholinguistic studies of human comprehension under stress, was developed. In order to test the impact of CLCM machine translatability of this particular kind of sub-language text, a previous experiment involving machine translation and human post-editing has been conducted. Employing two automatic evaluation metrics, a previous evaluation of the experiment has proved that instructions written according to this CL can improve machine translation (MT) performance. This paper presents a new cognitive evaluation approach for MT post-editing, which is tested on the previous controlled and uncontrolled textual data. The presented evaluation approach allows a deeper look into the post-editing process and specifically how much effort post-editors put into correcting the different kinds of MT errors. The method is based on existing MT error classification, which is enriched with a new error ranking motivated by the cognitive effort involved in the detection and correction of these MT errors. The preliminary results of applying this approach to a subset of the original data confirmed once again the positive impact of CLCM on emergency instructions' machine translatability and thus the validity of the approach.</abstract>
      <bibkey>temnikova-2010-cognitive</bibkey>
    </paper>
    <paper id="301">
      <author><first>Jens</first><last>Allwood</last></author>
      <author><first>Harald</first><last>Hammarström</last></author>
      <author><first>Andries</first><last>Hendrikse</last></author>
      <author><first>Mtholeni N.</first><last>Ngcobo</last></author>
      <author><first>Nozibele</first><last>Nomdebevana</last></author>
      <author><first>Laurette</first><last>Pretorius</last></author>
      <author><first>Mac</first><last>van der Merwe</last></author>
      <title>Work on Spoken (Multimodal) Language Corpora in <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>frica</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/438_Paper.pdf</url>
      <abstract>This paper describes past, ongoing and planned work on the collection and transcription of spoken language samples for all the South African official languages and as part of this the training of researchers in corpus linguistic research skills. More specifically the work has involved (and still involves) establishing an international corpus linguistic network linked to a network hub at a UNISA website and the development of research tools, a corpus research guide and workbook for multimodal communication and spoken language corpus research. As an example of the work we are doing and hope to do more of in the future, we present a small pilot study of the influence of English and Afrikaans on the 100 most frequent words in spoken Xhosa as this is evidenced in the corpus of spoken interaction we have gathered so far. Other planned work, besides work on spoken language phenomena, involves comparison of spoken and written language and work on communicative body movements (gestures) and their relation to speech.</abstract>
      <bibkey>allwood-etal-2010-work</bibkey>
    </paper>
    <paper id="302">
      <author><first>Nils</first><last>Reiter</last></author>
      <author><first>Oliver</first><last>Hellwig</last></author>
      <author><first>Anand</first><last>Mishra</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Jens</first><last>Burkhardt</last></author>
      <title>Using <fixed-case>NLP</fixed-case> Methods for the Analysis of Rituals</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/439_Paper.pdf</url>
      <abstract>This paper gives an overview of an interdisciplinary research project that is concerned with the application of computational linguistics methods to the analysis of the structure and variance of rituals, as investigated in ritual science. We present motivation and prospects of a computational approach to ritual research, and explain the choice of specific analysis techniques. We discuss design decisions for data collection and processing and present the general NLP architecture. For the analysis of ritual descriptions, we apply the frame semantics paradigm with newly invented frames where appropriate. Using scientific ritual research literature, we experimented with several techniques of automatic extraction of domain terms for the domain of rituals. As ritual research is a highly interdisciplinary endeavour, a vocabulary common to all sub-areas of ritual research can is hard to specify and highly controversial. The domain terms extracted from ritual research literature are used as a basis for a common vocabulary and thus help the creation of ritual specific frames. We applied the tf*idf, 2 and PageRank algorithm to our ritual research literature corpus and two non-domain corpora: The British National Corpus and the British Academic Written English corpus. All corpora have been part of speech tagged and lemmatized. The domain terms have been evaluated by two ritual experts independently. Interestingly, the results of the algorithms were different for different parts of speech. This finding is in line with the fact that the inter-annotator agreement also differs between parts of speech.</abstract>
      <bibkey>reiter-etal-2010-using</bibkey>
    </paper>
    <paper id="303">
      <author><first>C. Anton</first><last>Rytting</last></author>
      <author><first>Paul</first><last>Rodrigues</last></author>
      <author><first>Tim</first><last>Buckwalter</last></author>
      <author><first>David</first><last>Zajic</last></author>
      <author><first>Bridget</first><last>Hirsch</last></author>
      <author><first>Jeff</first><last>Carnes</last></author>
      <author><first>Nathanael</first><last>Lynn</last></author>
      <author><first>Sarah</first><last>Wayland</last></author>
      <author><first>Chris</first><last>Taylor</last></author>
      <author><first>Jason</first><last>White</last></author>
      <author><first>Charles</first><last>Blake III</last></author>
      <author><first>Evelyn</first><last>Browne</last></author>
      <author><first>Corey</first><last>Miller</last></author>
      <author><first>Tristan</first><last>Purvis</last></author>
      <title>Error Correction for <fixed-case>A</fixed-case>rabic Dictionary Lookup</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/440_Paper.pdf</url>
      <abstract>We describe a new Arabic spelling correction system which is intended for use with electronic dictionary search by learners of Arabic. Unlike other spelling correction systems, this system does not depend on a corpus of attested student errors but on student- and teacher-generated ratings of confusable pairs of phonemes or letters. Separate error modules for keyboard mistypings, phonetic confusions, and dialectal confusions are combined to create a weighted finite-state transducer that calculates the likelihood that an input string could correspond to each citation form in a dictionary of Iraqi Arabic. Results are ranked by the estimated likelihood that a citation form could be misheard, mistyped, or mistranscribed for the input given by the user. To evaluate the system, we developed a noisy-channel model trained on students speech errors and use it to perturb citation forms from a dictionary. We compare our system to a baseline based on Levenshtein distance and find that, when evaluated on single-error queries, our system performs 28% better than the baseline (overall MRR) and is twice as good at returning the correct dictionary form as the top-ranked result. We believe this to be the first spelling correction system designed for a spoken, colloquial dialect of Arabic.</abstract>
      <bibkey>rytting-etal-2010-error</bibkey>
    </paper>
    <paper id="304">
      <author><first>Christopher R</first><last>Walker</last></author>
      <author><first>Hannah</first><last>Copperman</last></author>
      <title>Evaluating Complex Semantic Artifacts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/441_Paper.pdf</url>
      <abstract>Evaluating complex Natural Language Processing (NLP) systems can prove extremely difficult. In many cases, the best one can do is to evaluate these systems indirectly, by looking at the impact they have on the performance of the downstream use case. For complex end-to-end systems, these metrics are not always enlightening, especially from the perspective of NLP failure analysis, as component interaction can obscure issues specific to the NLP technology. We present an evaluation program for complex NLP systems designed to produce meaningful aggregate accuracy metrics with sufficient granularity to support active development by NLP specialists. Our goals were threefold: to produce reliable metrics, to produce useful metrics and to produce actionable data. Our use case is a graph-based Wikipedia search index. Since the evaluation of a complex graph structure is beyond the conceptual grasp of a single human judge, the problem needs to be broken down. Slices of complex data reflective of coherent Decision Points provide a good framework for evaluation using human judges (Medero et al., 2006). For NL semantics, there really is no substitute. Leveraging Decision Points allows complex semantic artifacts to be tracked with judge-driven evaluations that are accurate, timely and actionable.</abstract>
      <bibkey>walker-copperman-2010-evaluating</bibkey>
    </paper>
    <paper id="305">
      <author><first>Mohamed</first><last>Altantawy</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Ibrahim</first><last>Saleh</last></author>
      <title>Morphological Analysis and Generation of <fixed-case>A</fixed-case>rabic Nouns: A Morphemic Functional Approach</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/442_Paper.pdf</url>
      <abstract>MAGEAD is a morphological analyzer and generator for Modern Standard Arabic (MSA) and its dialects. We introduced MAGEAD in previous work with an implementation of MSA and Levantine Arabic verbs. In this paper, we port that system to MSA nominals (nouns and adjectives), which are far more complex to model than verbs. Our system is a functional morphological analyzer and generator, i.e., it analyzes to and generates from a representation consisting of a lexeme and linguistic feature-value pairs, where the features are syntactically (and perhaps semantically) meaningful, rather than just morphologically. A detailed evaluation of the current implementation comparing it to a commonly used morphological analyzer shows that it has good morphological coverage with precision and recall scores in the 90s. An error analysis reveals that the majority of recall and precision errors are problems in the gold standard or a result of the discrepancy between different models of form-based/functional morphology.</abstract>
      <bibkey>altantawy-etal-2010-morphological</bibkey>
    </paper>
    <paper id="306">
      <author><first>Hiroyuki</first><last>Kaji</last></author>
      <author><first>Takashi</first><last>Tsunakawa</last></author>
      <author><first>Daisuke</first><last>Okada</last></author>
      <title>Using Comparable Corpora to Adapt a Translation Model to Domains</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/443_Paper.pdf</url>
      <abstract>Statistical machine translation (SMT) requires a large parallel corpus, which is available only for restricted language pairs and domains. To expand the language pairs and domains to which SMT is applicable, we created a method for estimating translation pseudo-probabilities from bilingual comparable corpora. The essence of our method is to calculate pairwise correlations between the words associated with a source-language word, presently restricted to a noun, and its translations; word translation pseudo-probabilities are calculated based on the assumption that the more associated words a translation is correlated with, the higher its translation probability. We also describe a method we created for calculating noun-sequence translation pseudo-probabilities based on occurrence frequencies of noun sequences and constituent-word translation pseudo-probabilities. Then, we present a framework for merging the translation pseudo-probabilities estimated from in-domain comparable corpora with a translation model learned from an out-of-domain parallel corpus. Experiments using Japanese and English comparable corpora of scientific paper abstracts and a Japanese-English parallel corpus of patent abstracts showed promising results; the BLEU score was improved to some degree by incorporating the pseudo-probabilities estimated from the in-domain comparable corpora. Future work includes an optimization of the parameters and an extension to estimate translation pseudo-probabilities for verbs.</abstract>
      <bibkey>kaji-etal-2010-using</bibkey>
    </paper>
    <paper id="307">
      <author><first>Hannah</first><last>Copperman</last></author>
      <author><first>Christopher R.</first><last>Walker</last></author>
      <title>Fred’s Reusable Evaluation Device: Providing Support for Quick and Reliable Linguistic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/444_Paper.pdf</url>
      <abstract>This paper describes an interface that was developed for processing large amounts of human judgments of linguistically annotated data. Freds Reusable Evaluation Device (Fred) provides administrators with a tool to submit linguistic evaluation tasks to judges. Each evaluation task is then presented to exactly two judges, who can submit their judgments at their own leisure. Fred then provides several metrics to administrators. The most important metric is precision, which is provided for each evaluation task and each annotator. Administrators can look at precision for a given data set over time, as well as by evaluation type, data set, or annotator. Inter-annotator agreement is also reported, and that can be tracked over time as well. The interface was developed to provide a tool for evaluating semantically marked up text. The types of evaluations Fred has been used for so far include things like correctness of subject-relation identification, and correctness of temporal relations. However, Freds full versatility has not yet been fully exploited.</abstract>
      <bibkey>copperman-walker-2010-freds</bibkey>
    </paper>
    <paper id="308">
      <author><first>Alexis</first><last>Baird</last></author>
      <author><first>Christopher R.</first><last>Walker</last></author>
      <title>The Creation of a Large-Scale <fixed-case>LFG</fixed-case>-Based Gold Parsebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/445_Paper.pdf</url>
      <abstract>Systems for syntactically parsing sentences have long been recognized as a priority in Natural Language Processing. Statistics-based systems require large amounts of high quality syntactically parsed data. Using the XLE toolkit developed at PARC and the LFG Parsebanker interface developed at Bergen, the Parsebank Project at Powerset has generated a rapidly increasing volume of syntactically parsed data. By using these tools, we are able to leverage the LFG framework to provide richer analyses via both constituent (c-) and functional (f-) structures. Additionally, the Parsebanking Project uses source data from Wikipedia rather than source data limited to a specific genre, such as the Wall Street Journal. This paper outlines the process we used in creating a large-scale LFG-Based Parsebank to address many of the shortcomings of previously-created parse banks such as the Penn Treebank. While the Parsebank corpus is still in progress, preliminary results using the data in a variety of contexts already show promise.</abstract>
      <bibkey>baird-walker-2010-creation</bibkey>
    </paper>
    <paper id="309">
      <author><first>Kathryn</first><last>Baker</last></author>
      <author><first>Michael</first><last>Bloodgood</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <author><first>Nathaniel W.</first><last>Filardo</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Christine</first><last>Piatko</last></author>
      <title>A Modality Lexicon and its use in Automatic Tagging</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/446_Paper.pdf</url>
      <abstract>This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one tagger―a structure-based tagger―results in precision around 86% (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data.</abstract>
      <bibkey>baker-etal-2010-modality</bibkey>
    </paper>
    <paper id="310">
      <author><first>Michael</first><last>Tanenblatt</last></author>
      <author><first>Anni</first><last>Coden</last></author>
      <author><first>Igor</first><last>Sominsky</last></author>
      <title>The <fixed-case>C</fixed-case>oncept<fixed-case>M</fixed-case>apper Approach to Named Entity Recognition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/448_Paper.pdf</url>
      <abstract>ConceptMapper is an open source tool we created for classifying mentions in an unstructured text document based on concept terminologies (dictionaries) and yielding named entities as output. It is implemented as a UIMA (Unstructured Information Management Architecture) annotator and is highly configurable: concepts can come from standardised or proprietary terminologies; arbitrary attributes can be associated with dictionary entries, and those attributes can then be associated with the named entities in the output; numerous search strategies and search options can be specified; any tokenizer packaged as a UIMA annotator can be used to tokenize the dictionary, so the same tokenization can be guaranteed for the input and dictionary, minimising tokenization mismatch errors; and the types and features of UIMA annotations used as input and generated as output can also be controlled. We describe ConceptMapper and its configuration parameters and their trade-offs, then describe the results of an experiment wherein some of these parameters are varied and precision and recall are subsequently measured in the task of in identifying concepts in a collection English-language clinical reports (colon cancer pathology). ConceptMapper is available from the Apache UIMA Sandbox, covered by the Apache Open Source license.</abstract>
      <bibkey>tanenblatt-etal-2010-conceptmapper</bibkey>
    </paper>
    <paper id="311">
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Chiharu</first><last>Narawa</last></author>
      <title><fixed-case>LAF</fixed-case>/<fixed-case>G</fixed-case>r<fixed-case>AF</fixed-case>-grounded Representation of Dependency Structures</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/449_Paper.pdf</url>
      <abstract>This paper shows that a LAF/GrAF-based annotation schema can be used for the adequate representation of syntactic dependency structures possibly in many languages. We first argue that there are at least two types of textual units that can be annotated with dependency information: words/tokens and chunks/phrases. We especially focus on importance of the latter dependency unit: it is particularly useful for representing Japanese dependency structures, known as Kakari-Uke structure. Based on this consideration, we then discuss a sub-typing of GrAF to represent the corresponding dependency structures. We derive three node types, two edge types, and the associated constraints for properly representing both the token-based and the chunk-based dependency structures. We finally propose a wrapper program that, as a proof of concept, converts output data from different dependency parsers in proprietary XML formats to the GrAF-compliant XML representation. It partially proves the value of an international standard like LAF/GrAF in the Web service context: an existing dependency parser can be, in a sense, standardized, once wrapped by a data format conversion process.</abstract>
      <bibkey>hayashi-etal-2010-laf</bibkey>
    </paper>
    <paper id="312">
      <author><first>Marc</first><last>Carmen</last></author>
      <author><first>Paul</first><last>Felt</last></author>
      <author><first>Robbie</first><last>Haertel</last></author>
      <author><first>Deryle</first><last>Lonsdale</last></author>
      <author><first>Peter</first><last>McClanahan</last></author>
      <author><first>Owen</first><last>Merkling</last></author>
      <author><first>Eric</first><last>Ringger</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <title>Tag Dictionaries Accelerate Manual Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/451_Paper.pdf</url>
      <abstract>Expert human input can contribute in various ways to facilitate automatic annotation of natural language text. For example, a part-of-speech tagger can be trained on labeled input provided offline by experts. In addition, expert input can be solicited by way of active learning to make the most of annotator expertise. However, hiring individuals to perform manual annotation is costly both in terms of money and time. This paper reports on a user study that was performed to determine the degree of effect that a part-of-speech dictionary has on a group of subjects performing the annotation task. The user study was conducted using a modular, web-based interface created specifically for text annotation tasks. The user study found that for both native and non-native English speakers a dictionary with greater than 60% coverage was effective at reducing annotation time and increasing annotator accuracy. On the basis of this study, we predict that using a part-of-speech tag dictionary with coverage greater than 60% can reduce the cost of annotation in terms of both time and money.</abstract>
      <bibkey>carmen-etal-2010-tag</bibkey>
    </paper>
    <paper id="313">
      <author><first>Stasinos</first><last>Konstantopoulos</last></author>
      <title>Learning Language Identification Models: A Comparative Analysis of the Distinctive Features of Names and Common Words</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/452_Paper.pdf</url>
      <abstract>The intuition and basic hypothesis that this paper explores is that names are more characteristic of their language than common words are, and that a single name can have enough clues to confidently identify its language where random text of the same length wouldn't. To test this hypothesis, n-gramm modelling is used to learn language models which identify the language of isolated names and equally short document fragments. As the empirical results corroborate the prior intuition, an explanation is sought for the higher accuracy at which the language of names can be identified. The results of the application of these models, as well as the models themselves, are quantitatively and qualitatively analysed and a hypothesis is formed about the explanation of this difference. The conclusions derived are both technologically useful in information extraction or text-to-speech tasks, and theoretically interesting as a tool for improving our understanding of the morphology and phonology of the languages involved in the experiments.</abstract>
      <bibkey>konstantopoulos-2010-learning</bibkey>
    </paper>
    <paper id="314">
      <author><first>Chris Irwin</first><last>Davis</last></author>
      <author><first>Dan</first><last>Moldovan</last></author>
      <title>Feasibility of Automatically Bootstrapping a <fixed-case>P</fixed-case>ersian <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/453_Paper.pdf</url>
      <abstract>In this paper we describe a proof-of-concept for the bootstrapping of a Persian WordNet. This effort was motivated by previous work done at Stanford University on bootstrapping an Arabic WordNet using a parallel corpus and an English WordNet. The principle of that work is based on the premise that paradigmatic relations are by nature deeply semantic, and as such, are likely to remain intact between languages. We performed our task on a Persian-English bilingual corpus of George Orwells Nineteen Eighty-Four. The corpus was neither aligned nor sense tagged, so it was necessary that these were undertaken first. A combination of manual and semiautomated methods were used to tag and sentence align the corpus. Actual mapping of English word senses onto Persian was done using automated techniques. Although Persian is written in Arabic script, it is an Indo-European language, while Arabic is a Central Semitic language. Despite their linguistic differences, we endeavor to test the applicability of the Stanford strategy to our task.</abstract>
      <bibkey>davis-moldovan-2010-feasibility</bibkey>
    </paper>
    <paper id="315">
      <author><first>Aditi Sharma</first><last>Grover</last></author>
      <author><first>Gerhard B.</first><last>van Huyssteen</last></author>
      <author><first>Marthinus W.</first><last>Pretorius</last></author>
      <title>The <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>frican Human Language Technologies Audit</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/454_Paper.pdf</url>
      <abstract>Human language technologies (HLT) can play a vital role in bridging the digital divide and thus the HLT field has been recognised as a priority area by the South African government. We present our work on conducting a technology audit on the South African HLT landscape across the countrys eleven official languages. The process and the instruments employed in conducting the audit are described and an overview of the various complementary approaches used in the results analysis is provided. We find that a number of HLT language resources (LRs) are available in SA but they are of a very basic and exploratory nature. Lessons learnt in conducting a technology audit in a young and multilingual context are also discussed.</abstract>
      <bibkey>grover-etal-2010-south</bibkey>
    </paper>
    <paper id="316">
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <author><first>Oswald</first><last>Lanz</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Alexandros</first><last>Potamianos</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <author><first>Luca</first><last>Surian</last></author>
      <title><fixed-case>B</fixed-case>aby<fixed-case>E</fixed-case>xp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/455_Paper.pdf</url>
      <abstract>There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions.</abstract>
      <bibkey>poesio-etal-2010-babyexp</bibkey>
    </paper>
    <paper id="317">
      <author><first>Iñaki</first><last>Sainz</last></author>
      <author><first>Eva</first><last>Navas</last></author>
      <author><first>Inma</first><last>Hernáez</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <author><first>Francisco</first><last>Campillo</last></author>
      <title><fixed-case>TTS</fixed-case> Evaluation Campaign with a Common <fixed-case>S</fixed-case>panish Database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/456_Paper.pdf</url>
      <abstract>This paper describes the first TTS evaluation campaign designed for Spanish. Seven research institutions took part in the evaluation campaign and developed a voice from a common speech database provided by the organisation. Each participating team had a period of seven weeks to generate a voice. Next, a set of sentences were released and each team had to synthesise them within a week period. Finally, some of the synthesised test audio files were subjectively evaluated via an online test according to the following criteria: similarity to the original voice, naturalness and intelligibility. Box-plots, Wilcoxon tests and WER have been generated in order to analyse the results. Two main conclusions can be drawn: On the one hand, there is considerable margin for improvement to reach the quality level of the natural voice. On the other hand, two systems get significantly better results than the rest: one is based on statistical parametric synthesis and the other one is a concatenative system that makes use of a sinusoidal model to modify both prosody and smooth spectral joints. Therefore, it seems that some kind of spectral control is needed when building voices with a medium size database for unrestricted domains.</abstract>
      <bibkey>sainz-etal-2010-tts</bibkey>
    </paper>
    <paper id="318">
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Cristina</first><last>Mota</last></author>
      <title>Experiments in Human-computer Cooperation for the Semantic Annotation of <fixed-case>P</fixed-case>ortuguese Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/457_Paper.pdf</url>
      <abstract>In this paper, we present a system to aid human annotation of semantic information in the scope of the project AC/DC, called corte-e-costura. This system leverages on the human annotation effort, by providing the annotator with a simple system that applies rules incrementally. Our goal was twofold: first, to develop an easy-to-use system that required a minimum of learning from the part of the linguist; second, one that provided a straightforward way of checking the results obtained, in order to immediately evaluate the results of the rules devised. After explaining the motivation for its development from scratch, we present the current status of the AC/DC project and provide a quantitative description of its material in what concerns semantic annotation. We then present the corte-e-costura system in detail, providing the result of our first experiments with the semantic fields of colour and clothing. We end the paper with some discussion of future work as well as of the experience gained.</abstract>
      <bibkey>santos-mota-2010-experiments</bibkey>
    </paper>
    <paper id="319">
      <author><first>Koichiro</first><last>Honda</last></author>
      <author><first>Tomoyosi</first><last>Akiba</last></author>
      <title>Language Modeling Approach for Retrieving Passages in Lecture Audio Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/462_Paper.pdf</url>
      <abstract>Spoken Document Retrieval (SDR) is a promising technology for enhancing the utility of spoken materials. After the spoken documents have been transcribed by using a Large Vocabulary Continuous Speech Recognition (LVCSR) decoder, a text-based ad hoc retrieval method can be applied directly to the transcribed documents. However, recognition errors will significantly degrade the retrieval performance. To address this problem, we have previously proposed a method that aimed to fill the gap between automatically transcribed text and correctly transcribed text by using a statistical translation technique. In this paper, we extend the method by (1) using neighboring context to index the target passage, and (2) applying a language modeling approach for document retrieval. Our experimental evaluation shows that context information can improve retrieval performance, and that the language modeling approach is effective in incorporating context information into the proposed SDR method, which uses a translation model.</abstract>
      <bibkey>honda-akiba-2010-language</bibkey>
    </paper>
    <paper id="320">
      <author><first>Pamela</first><last>Forner</last></author>
      <author><first>Danilo</first><last>Giampiccolo</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <author><first>Anselmo</first><last>Peñas</last></author>
      <author><first>Álvaro</first><last>Rodrigo</last></author>
      <author><first>Richard</first><last>Sutcliffe</last></author>
      <title>Evaluating Multilingual Question Answering Systems at <fixed-case>CLEF</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/464_Paper.pdf</url>
      <abstract>The paper offers an overview of the key issues raised during the seven years activity of the Multilingual Question Answering Track at the Cross Language Evaluation Forum (CLEF). The general aim of the Multilingual Question Answering Track has been to test both monolingual and cross-language Question Answering (QA) systems that process queries and documents in several European languages, also drawing attention to a number of challenging issues for research in multilingual QA. The paper gives a brief description of how the task has evolved over the years and of the way in which the data sets have been created, presenting also a brief summary of the different types of questions developed. The document collections adopted in the competitions are sketched as well, and some data about the participation are provided. Moreover, the main evaluation measures used to evaluate system performances are explained and an overall analysis of the results achieved is presented.</abstract>
      <bibkey>forner-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="321">
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>Dóra</first><last>Szauter</last></author>
      <author><first>Attila</first><last>Almási</last></author>
      <author><first>György</first><last>Móra</last></author>
      <author><first>Zoltán</first><last>Alexin</last></author>
      <author><first>János</first><last>Csirik</last></author>
      <title><fixed-case>H</fixed-case>ungarian Dependency Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/465_Paper.pdf</url>
      <abstract>Herein, we present the process of developing the first Hungarian Dependency TreeBank. First, short references are made to dependency grammars we considered important in the development of our Treebank. Second, mention is made of existing dependency corpora for other languages. Third, we present the steps of converting the Szeged Treebank into dependency-tree format: from the originally phrase-structured treebank, we produced dependency trees by automatic conversion, checked and corrected them thereby creating the first manually annotated dependency corpus for Hungarian. We also go into detail about the two major sets of problems, i.e. coordination and predicative nouns and adjectives. Fourth, we give statistics on the treebank: by now, we have completed the annotation of business news, newspaper articles, legal texts and texts in informatics, at the same time, we are planning to convert the entire corpus into dependency tree format. Finally, we give some hints on the applicability of the system: the present database may be utilized ― among others ― in information extraction and machine translation as well.</abstract>
      <bibkey>vincze-etal-2010-hungarian</bibkey>
    </paper>
    <paper id="322">
      <author><first>Francesca</first><last>Fallucchi</last></author>
      <author><first>Maria Teresa</first><last>Pazienza</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <title>Generic Ontology Learners on Application Domains</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/466_Paper.pdf</url>
      <abstract>In ontology learning from texts, we have ontology-rich domains where we have large structured domain knowledge repositories or we have large general corpora with large general structured knowledge repositories such as WordNet (Miller, 1995). Ontology learning methods are more useful in ontology-poor domains. Yet, in these conditions, these methods have not a particularly high performance as training material is not sufficient. In this paper we present an LSP ontology learning method that can exploit models learned from a generic domain to extract new information in a specific domain. In our model, we firstly learn a model from training data and then we use the learned model to discover knowledge in a specific domain. We tested our model adaptation strategy using a background domain that is applied to learn the isa networks in the Earth Observation Domain as a specific domain. We will demonstrate that our method captures domain knowledge better than other generic models: our model better captures what is expected by domain experts than a baseline method based only on WordNet. This latter is better correlated with non-domain annotators asked to produce the ontology for the specific domain.</abstract>
      <bibkey>fallucchi-etal-2010-generic</bibkey>
    </paper>
    <paper id="323">
      <author><first>Alessandro</first><last>Oltramari</last></author>
      <author><first>Guido</first><last>Vetere</last></author>
      <author><first>Maurizio</first><last>Lenzerini</last></author>
      <author><first>Aldo</first><last>Gangemi</last></author>
      <author><first>Nicola</first><last>Guarino</last></author>
      <title>Senso Comune</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/468_Paper.pdf</url>
      <abstract>This paper introduces the general features of Senso Comune, an open knowledge base for the Italian language, focusing on the interplay of lexical and ontological knowledge, and outlining our approach to conceptual knowledge elicitation. Senso Comune consists of a machine-readable lexicon constrained by an ontological infrastructure. The idea at the basis of Senso Comune is that natural languages exist in use, and they belong to their users. In the line of Saussure's linguistics, natural languages are seen as a social product and their main strength relies on the users consensus. At the same time, language has specific goals: i.e. referring to entities that belong to the users world (be it physical or not) and that are made up in social environments where expressions are produced and understood. This usage leverages the creativity of those who produce words and try to understand them. This is the reason why ontology, i.e. a shared conceptualization of the world, can be regarded to as the soil on which the speakers' consensus may be rooted. Some final remarks concerning future work and applications are also given.</abstract>
      <bibkey>oltramari-etal-2010-senso</bibkey>
    </paper>
    <paper id="324">
      <author><first>Rogelio</first><last>Nazar</last></author>
      <author><first>Maarten</first><last>Janssen</last></author>
      <title>Combining Resources: Taxonomy Extraction from Multiple Dictionaries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/469_Paper.pdf</url>
      <abstract>The idea that dictionaries are a good source for (computational) information has been around for a long while, and the extraction of taxonomic information from them is something that has been attempted several times. However, such information extraction was typically based on the systematic analysis of the text of a single dictionary. In this paper, we demonstrate how it is possible to extract taxonomic information without any analysis of the specific text, by comparing the same lexical entry in a number of different dictionaries. Counting word frequencies in the dictionary entry for the same word in different dictionaries leads to a surprisingly good recovery of taxonomic information, without the need for any syntactic analysis of the entries in question nor any kind of language-specific treatment. As a case in point, we will show in this paper an experiment extracting hyperonymy relations from several Spanish dictionaries, measuring the effect that the different number of dictionaries have on the results.</abstract>
      <bibkey>nazar-janssen-2010-combining</bibkey>
    </paper>
    <paper id="325">
      <author><first>Yan</first><last>Zhao</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <title><fixed-case>POS</fixed-case> Multi-tagging Based on Combined Models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/470_Paper.pdf</url>
      <abstract>In the POS tagging task, there are two kinds of statistical models: one is generative model, such as the HMM, the others are discriminative models, such as the Maximum Entropy Model (MEM). POS multi-tagging decoding method includes the N-best paths method and forward-backward method. In this paper, we use the forward-backward decoding method based on a combined model of HMM and MEM. If P(t) is the forward-backward probability of each possible tag t, we first calculate P(t) according HMM and MEM separately. For all tags options in a certain position in a sentence, we normalize P(t) in HMM and MEM separately. Probability of the combined model is the sum of normalized forward-backward probabilities P norm(t) in HMM and MEM. For each word w, we select the best tag in which the probability of combined model is the highest. In the experiments, we use combined model and get higher accuracy than any single model on POS tagging tasks of three languages, which are Chinese, English and Dutch. The result indicates that our combined model is effective.</abstract>
      <bibkey>zhao-van-noord-2010-pos</bibkey>
    </paper>
    <paper id="326">
      <author><first>Ibon</first><last>Saratxaga</last></author>
      <author><first>Inmaculada</first><last>Hernáez</last></author>
      <author><first>Eva</first><last>Navas</last></author>
      <author><first>Iñaki</first><last>Sainz</last></author>
      <author><first>Iker</first><last>Luengo</last></author>
      <author><first>Jon</first><last>Sánchez</last></author>
      <author><first>Igor</first><last>Odriozola</last></author>
      <author><first>Daniel</first><last>Erro</last></author>
      <title><fixed-case>A</fixed-case>ho<fixed-case>T</fixed-case>ransf: A Tool for Multiband Excitation Based Speech Analysis and Modification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/471_Paper.pdf</url>
      <abstract>In this paper we present AhoTransf, a tool that enables analysis, visualization, modification and synthesis of speech. AhoTransf integrates a speech signal analysis model with a graphical user interface to allow visualization and modification of the parameters of the model. The synthesis capability allows hearing the modified signal thus providing a quick way to understand the perceptual effect of the changes in the parameters of the model. The speech analysis/synthesis algorithm is based in the Multiband Excitation technique, but uses a novel phase information representation the Relative Phase Shift (RPSs). With this representation, not only the amplitudes but also the phases of the harmonic components of the speech signal reveal their structured patterns in the visualization tool. AhoTransf is modularly conceived so that it can be used with different harmonic speech models.</abstract>
      <bibkey>saratxaga-etal-2010-ahotransf</bibkey>
    </paper>
    <paper id="327">
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <title>Identifying Paraphrases between Technical and Lay Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/472_Paper.pdf</url>
      <abstract>In previous work, we presented a preliminary study to identify paraphrases between technical and lay discourse types from medical corpora dedicated to the French language. In this paper, we test the hypothesis that the same kinds of paraphrases as for French can be detected between English technical and lay discourse types and report the adaptation of our method from French to English. Starting from the constitution of monolingual comparable corpora, we extract two kinds of paraphrases: paraphrases between nominalizations and verbal constructions and paraphrases between neo-classical compounds and modern-language phrases. We do this relying on morphological resources and a set of extraction rules we adapt from the original approach for French. Results show that paraphrases could be identified with a rather good precision, and that these types of paraphrase are relevant in the context of the opposition between technical and lay discourse types. These observations are consistent with the results obtained for French, which demonstrates the portability of the approach as well as the similarity of the two languages as regards the use of those kinds of expressions in technical and lay discourse types.</abstract>
      <bibkey>deleger-zweigenbaum-2010-identifying</bibkey>
    </paper>
    <paper id="328">
      <author><first>Stavros</first><last>Ntalampiras</last></author>
      <author><first>Todor</first><last>Ganchev</last></author>
      <author><first>Ilyas</first><last>Potamitis</last></author>
      <author><first>Nikos</first><last>Fakotakis</last></author>
      <title>Heterogeneous Sensor Database in Support of Human Behaviour Analysis in Unrestricted Environments: The Audio Part</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/474_Paper.pdf</url>
      <abstract>In the present paper we report on a recent effort that resulted in the establishment of a unique multimodal database, referred to as the PROMETHEUS database. This database was created in support of research and development activities, performed within the European Commission FP7 PROMETHEUS project, aiming at the creation of a framework for monitoring and interpretation of human behaviours in unrestricted indoors and outdoors environments. In the present paper we discuss the design and the implementation of the audio part of the database and offer statistical information about the audio content. Specifically, it contains single-person and multi-person scenarios, but also covers scenarios with interactions between groups of people. The database design was conceived with extended support of research and development activities devoted to detection of typical and atypical events, emergency and crisis situations, which assist for achieving situational awareness and more reliable interpretation of the context in which humans behave. The PROMETHEUS database allows for embracing a wide range of real-world applications, including smart-home and human-robot interaction interfaces, indoors/outdoors public areas surveillance, airport terminals or city park supervision, etc. A major portion of the PROMETHEUS database will be made publically available by the end of year 2010.</abstract>
      <bibkey>ntalampiras-etal-2010-heterogeneous</bibkey>
    </paper>
    <paper id="329">
      <author><first>Khalil</first><last>Dahab</last></author>
      <author><first>Anja</first><last>Belz</last></author>
      <title>A Game-based Approach to Transcribing Images of Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/476_Paper.pdf</url>
      <abstract>Creating language resources is expensive and time-consuming, and this forms a bottleneck in the development of language technology, for less-studied non-European languages in particular. The recent internet phenomenon of crowd-sourcing offers a cost-effective and potentially fast way of overcoming such language resource acquisition bottlenecks. We present a methodology that takes as its input scanned documents of typed or hand-written text, and produces transcriptions of the text as its output. Instead of using Optical Character Recognition (OCR) technology, the methodology is game-based and produces such transcriptions as a by-product. The approach is intended particularly for languages for which language technology and resources are scarce and reliable OCR technology may not exist. It can be used in place of OCR for transcribing individual documents, or to create corpora of paired images and transcriptions required to train OCR tools. We present Minefield, a prototype implementation of the approach which is currently collecting Arabic transcriptions.</abstract>
      <bibkey>dahab-belz-2010-game</bibkey>
    </paper>
    <paper id="330">
      <author><first>Nicolas</first><last>Serrano</last></author>
      <author><first>Francisco</first><last>Castro</last></author>
      <author><first>Alfons</first><last>Juan</last></author>
      <title>The <fixed-case>RODRIGO</fixed-case> Database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/477_Paper.pdf</url>
      <abstract>Annotation of digitized pages from historical document collections is very important to research on automatic extraction of text blocks, lines, and handwriting recognition. We have recently introduced a new handwritten text database, GERMANA, which is based on a Spanish manuscript from 1891. To our knowledge, GERMANA is the first publicly available database mostly written in Spanish and comparable in size to standard databases. In this paper, we present another handwritten text database, RODRIGO, completely written in Spanish and comparable in size to GERMANA. However, RODRIGO comes from a much older manuscript, from 1545, where the typical difficult characteristics of historical documents are more evident. In particular, the writing style, which has clear Gothic influences, is significantly more complex than that of GERMANA. We also provide baseline results of handwriting recognition for reference in future studies, using standard techniques and tools for preprocessing, feature extraction, HMM-based image modelling, and language modelling.</abstract>
      <bibkey>serrano-etal-2010-rodrigo</bibkey>
    </paper>
    <paper id="331">
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Danilo</first><last>Giampiccolo</last></author>
      <author><first>Medea Lo</first><last>Leggio</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <title>Building Textual Entailment Specialized Data Sets: a Methodology for Isolating Linguistic Phenomena Relevant to Inference</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/478_Paper.pdf</url>
      <abstract>This paper proposes a methodology for the creation of specialized data sets for Textual Entailment, made of monothematic Text-Hypothesis pairs (i.e. pairs in which only one linguistic phenomenon relevant to the entailment relation is highlighted and isolated). The expected benefits derive from the intuition that investigating the linguistic phenomena separately, i.e. decomposing the complexity of the TE problem, would yield an improvement in the development of specific strategies to cope with them. The annotation procedure assumes that humans have knowledge about the linguistic phenomena relevant to inference, and a classification of such phenomena both into fine grained and macro categories is suggested. We experimented with the proposed methodology over a sample of pairs taken from the RTE-5 data set, and investigated critical issues arising when entailment, contradiction or unknown pairs are considered. The result is a new resource, which can be profitably used both to advance the comprehension of the linguistic phenomena relevant to entailment judgments and to make a first step towards the creation of large-scale specialized data sets.</abstract>
      <bibkey>bentivogli-etal-2010-building</bibkey>
    </paper>
    <paper id="332">
      <author><first>Amal</first><last>Al-Saif</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <title>The <fixed-case>L</fixed-case>eeds <fixed-case>A</fixed-case>rabic Discourse Treebank: Annotating Discourse Connectives for <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/479_Paper.pdf</url>
      <abstract>We present the first effort towards producing an Arabic Discourse Treebank,a news corpus where all discourse connectives are identified and annotated with the discourse relations they convey as well as with the two arguments they relate.We discuss our collection of Arabic discourse connectives as well as principles for identifying and annotating them in context, taking into account properties specific to Arabic. In particular, we deal with the fact that Arabic has a rich morphology: we therefore include clitics as connectives as well as a wide range of nominalizations as potential arguments. We present a dedicated discourse annotation tool for Arabic and a large-scale annotation study. We show that both the human identification of discourse connectives and the determination of the discourse relations they convey is reliable. Our current annotated corpus encompasses a final 5651 annotated discourse connectives in 537 news texts. In future, we will release the annotated corpus to other researchers and use it for training and testing automated methods for discourse connective and relation recognition.</abstract>
      <bibkey>al-saif-markert-2010-leeds</bibkey>
    </paper>
    <paper id="333">
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <title>On the Role of Discourse Markers in Interactive Spoken Question Answering Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/481_Paper.pdf</url>
      <abstract>This paper presents a preliminary analysis of the role of some discourse markers and the vocalic hesitation ""euh"" in a corpus of spoken human utterances collected with the Ritel system, an open domain and spoken dialog system. The frequency and contextual combinatory of classical discourse markers and of the vocalic hesitation have been studied. This analysis pointed out some specificity in terms of combinatory of the analyzed items. The classical discourse markers seem to help initiating larger discursive blocks both at initial and medial positions of the on-going turns. The vocalic hesitation stand also for marking the user's embarrassments and wish to close the dialog.</abstract>
      <bibkey>vasilescu-etal-2010-role</bibkey>
    </paper>
    <paper id="334">
      <author><first>Björn</first><last>Schuller</last></author>
      <author><first>Riccardo</first><last>Zaccarelli</last></author>
      <author><first>Nicolas</first><last>Rollet</last></author>
      <author><first>Laurence</first><last>Devillers</last></author>
      <title><fixed-case>CINEMO</fixed-case> — A <fixed-case>F</fixed-case>rench Spoken Language Resource for Complex Emotions: Facts and Baselines</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/483_Paper.pdf</url>
      <abstract>The CINEMO corpus of French emotional speech provides a richly annotated resource to help overcome the apparent lack of learning and testing speech material for complex, i.e. blended or mixed emotions. The protocol for its collection was dubbing selected emotional scenes from French movies. 51 speakers are contained and the total speech time amounts to 2 hours and 13 minutes and 4k speech chunks after segmentation. Extensive labelling was carried out in 16 categories for major and minor emotions and in 6 continuous dimensions. In this contribution we give insight into the corpus statistics focusing in particular on the topic of complex emotions, and provide benchmark recognition results obtained in exemplary large feature space evaluations. In the result the labelling oft he collected speech clearly demonstrates that a complex handling of emotion seems needed. Further, the automatic recognition experiments provide evidence that the automatic recognition of blended emotions appears to be feasible.</abstract>
      <bibkey>schuller-etal-2010-cinemo</bibkey>
    </paper>
    <paper id="335">
      <author><first>Sara</first><last>Romano</last></author>
      <author><first>Francesco</first><last>Cutugno</last></author>
      <title>New Features in Spoken Language Search Hawk (<fixed-case>S</fixed-case>p<fixed-case>L</fixed-case>a<fixed-case>SH</fixed-case>): Query Language and Query Sequence</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/484_Paper.pdf</url>
      <abstract>In this work we present further development of the SpLaSH (Spoken Language Search Hawk) project. SpLaSH implements a data model for annotated speech corpora integrated with textual markup (i.e. POS tagging, syntax, pragmatics) including a toolkit used to perform complex queries across speech and text labels. The integration of time aligned annotations (TMA), represented making use of Annotation Graphs, with text aligned ones (TXA), stored in generic XML files, are provided by a data structure, the Connector Frame, acting as table-look-up linking temporal data to words in the text. SpLaSH imposes a very limited number of constraints to the data model design, allowing the integration of annotations developed separately within the same dataset and without any relative dependency. It also provides a GUI allowing three types of queries: simple query on TXA or TMA structures, sequence query on TMA structure and cross query on both TXA and TMA integrated structures. In this work new SpLaSH features will be presented: SpLaSH Query Language (SpLaSHQL) and Query Sequence.</abstract>
      <bibkey>romano-cutugno-2010-new</bibkey>
    </paper>
    <paper id="336">
      <author><first>Claire</first><last>Mouton</last></author>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <author><first>Benoit</first><last>Richert</last></author>
      <title><fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et Translation Using Bilingual Dictionaries with Evaluation on the <fixed-case>E</fixed-case>nglish-<fixed-case>F</fixed-case>rench Pair</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/485_Paper.pdf</url>
      <abstract>Semantic Role Labeling cannot be performed without an associated linguistic resource. A key resource for such a task is the FrameNet resource based on Fillmores theory of frame semantics. Like many linguistic resources, FrameNet has been built by English native speakers for the English language. To overcome the lack of such resources in other languages, we propose a new approach to FrameNet translation by using bilingual dictionaries and filtering the wrong translations. We define six scores to filter, based on translation redundancy and FrameNet structure. We also present our work on the enrichment of the obtained resource with nouns. This enrichment uses semantic spaces built on syntactical dependencies and a multi-represented k-NN classifier. We evaluate both the tasks on the French language over a subset of ten frames and show improved results compared to the existing French FrameNet. Our final resource contains 15,132 associations lexical units-frames for an estimated precision of 86%.</abstract>
      <bibkey>mouton-etal-2010-framenet</bibkey>
    </paper>
    <paper id="337">
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Petr</first><last>Pajas</last></author>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <title>Annotation Tool for Extended Textual Coreference and Bridging Anaphora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/487_Paper.pdf</url>
      <abstract>We present an annotation tool for the extended textual coreference and the bridging anaphora in the Prague Dependency TreebankÂ 2.0 (PDT 2.0). After we very briefly describe the annotation scheme, we focus on details of the annotation process from the technical point of view. We present the way of helping the annotators by several useful features implemented in the annotation tool, such as a possibility to combine surface and deep syntactic representation of sentences during the annotation, an automatic maintaining of the coreferential chain, underlining candidates for antecedents, etc. For studying differences among parallel annotations, the tool offers a simultaneous depicting of several annotations of the same data. The annotation tool can be used for other corpora too, as long as they have been transformed to the PML format. We present modifications of the tool for working with the coreference relations on other layers of language description, namely on the analytical layer and the morphological layer of PDT.</abstract>
      <bibkey>mirovsky-etal-2010-annotation</bibkey>
    </paper>
    <paper id="338">
      <author><first>Azad</first><last>Abad</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Danilo</first><last>Giampiccolo</last></author>
      <author><first>Shachar</first><last>Mirkin</last></author>
      <author><first>Emanuele</first><last>Pianta</last></author>
      <author><first>Asher</first><last>Stern</last></author>
      <title>A Resource for Investigating the Impact of Anaphora and Coreference on Inference.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/488_Paper.pdf</url>
      <abstract>Discourse phenomena play a major role in text processing tasks. However, so far relatively little study has been devoted to the relevance of discourse phenomena for inference. Therefore, an experimental study was carried out to assess the relevance of anaphora and coreference for Textual Entailment (TE), a prominent inference framework. First, the annotation of anaphoric and coreferential links in the RTE-5 Search data set was performed according to a specifically designed annotation scheme. As a result, a new data set was created where all anaphora and coreference instances in the entailing sentences which are relevant to the entailment judgment are solved and annotated.. A by-product of the annotation is a new augmented data set, where all the referring expressions which need to be resolved in the entailing sentences are replaced by explicit expressions. Starting from the final output of the annotation, the actual impact of discourse phenomena on inference engines was investigated, identifying the kind of operations that the systems need to apply to address discourse phenomena and trying to find direct mappings between these operation and annotation types.</abstract>
      <bibkey>abad-etal-2010-resource</bibkey>
    </paper>
    <paper id="339">
      <author><first>Robert</first><last>Remus</last></author>
      <author><first>Uwe</first><last>Quasthoff</last></author>
      <author><first>Gerhard</first><last>Heyer</last></author>
      <title><fixed-case>S</fixed-case>enti<fixed-case>WS</fixed-case> - A Publicly Available <fixed-case>G</fixed-case>erman-language Resource for Sentiment Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/490_Paper.pdf</url>
      <abstract>SentimentWortschatz, or SentiWS for short, is a publicly available German-language resource for sentiment analysis, opinion mining etc. It lists positive and negative sentiment bearing words weighted within the interval of [-1; 1] plus their part of speech tag, and if applicable, their inflections. The current version of SentiWS (v1.8b) contains 1,650 negative and 1,818 positive words, which sum up to 16,406 positive and 16,328 negative word forms, respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one. The present work describes the resources structure, the three sources utilised to assemble it and the semi-supervised method incorporated to weight the strength of its entries. Furthermore the resources contents are extensively evaluated using a German-language evaluation set we constructed. The evaluation set is verified being reliable and its shown that SentiWS provides a beneficial lexical resource for German-language sentiment analysis related tasks to build on.</abstract>
      <bibkey>remus-etal-2010-sentiws</bibkey>
    </paper>
    <paper id="340">
      <author><first>Polina</first><last>Panicheva</last></author>
      <author><first>John</first><last>Cardiff</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <title>Personal Sense and Idiolect: Combining Authorship Attribution and Opinion Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/491_Paper.pdf</url>
      <abstract>Subjectivity analysis and authorship attribution are very popular areas of research. However, work in these two areas has been done separately. We believe that by combining information about subjectivity in texts and authorship, the performance of both tasks can be improved. In the paper a personalized approach to opinion mining is presented, in which the notions of personal sense and idiolect are introduced; the approach is applied to the polarity classification task. It is assumed that different authors express their private states in text individually, and opinion mining results could be improved by analyzing texts by different authors separately. The hypothesis is tested on a corpus of movie reviews by ten authors. The results of applying the personalized approach to opinion mining are presented, confirming that the approach increases the performance of the opinion mining task. Automatic authorship attribution is further applied to model the personalized approach, classifying documents by their assumed authorship. Although the automatic authorship classification imposes a number of limitations on the dataset for further experiments, after overcoming these issues the authorship attribution technique modeling the personalized approach confirms the increase over the baseline with no authorship information used.</abstract>
      <bibkey>panicheva-etal-2010-personal</bibkey>
    </paper>
    <paper id="341">
      <author><first>Dirk</first><last>Goldhahn</last></author>
      <author><first>Uwe</first><last>Quasthoff</last></author>
      <title>Automatic Annotation of Co-Occurrence Relations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/492_Paper.pdf</url>
      <abstract>We introduce a method for automatically labelling edges of word co-occurrence graphs with semantic relations. Therefore we only make use of training data already contained within the graph. Starting point of this work is a graph based on word co-occurrence of the German language, which is created by applying iterated co-occurrence analysis. The edges of the graph have been partially annotated by hand with semantic relationships. In our approach we make use of the commonly appearing network motif of three words forming a triangular pattern. We assume that the fully annotated occurrences of these structures contain information useful for our purpose. Based on these patterns rules for reasoning are learned. The obtained rules are then combined using Dempster-Shafer theory to infer new semantic relations between words. Iteration of the annotation process is possible to increase the number of obtained relations. By applying the described process the graph can be enriched with semantic information at a high precision.</abstract>
      <bibkey>goldhahn-quasthoff-2010-automatic</bibkey>
    </paper>
    <paper id="342">
      <author><first>Max</first><last>Jakob</last></author>
      <author><first>Markéta</first><last>Lopatková</last></author>
      <author><first>Valia</first><last>Kordoni</last></author>
      <title>Mapping between Dependency Structures and Compositional Semantic Representations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/493_Paper.pdf</url>
      <abstract>This paper investigates the mapping between two semantic formalisms, namely the tectogrammatical layer of the Prague Dependency Treebank 2.0 (PDT) and (Robust) Minimal Recursion Semantics ((R)MRS). It is a first attempt to relate the dependency-based annotation scheme of PDT to a compositional semantics approach like (R)MRS. A mapping algorithm that converts PDT trees to (R)MRS structures is developed, associating (R)MRSs to each node on the dependency tree. Furthermore, composition rules are formulated and the relation between dependency in PDT and semantic heads in (R)MRS is analyzed. It turns out that structure and dependencies, morphological categories and some coreferences can be preserved in the target structures. Moreover, valency and free modifications are distinguished using the valency dictionary of PDT as an additional resource. The validation results show that systematically correct underspecified target representations can be obtained by a rule-based mapping approach, which is an indicator that (R)MRS is indeed robust in relation to the formal representation of Czech data. This finding is novel, for Czech, with its free word order and rich morphology, is typologically different than languages analyzed with (R)MRS to date.</abstract>
      <bibkey>jakob-etal-2010-mapping</bibkey>
    </paper>
    <paper id="343">
      <author><first>Danielle</first><last>Ben-Gera</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Valia</first><last>Kordoni</last></author>
      <title>Semantic Feature Engineering for Enhancing Disambiguation Performance in Deep Linguistic Processing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/494_Paper.pdf</url>
      <abstract>The task of parse disambiguation has gained in importance over the last decade as the complexity of grammars used in deep linguistic processing has been increasing. In this paper we propose to employ the fine-grained HPSG formalism in order to investigate the contribution of deeper linguistic knowledge to the task of ranking the different trees the parser outputs. In particular, we focus on the incorporation of semantic features in the disambiguation component and the stability of our model cross domains. Our work is carried out within DELPH-IN (http://www.delph-in.net), using the LinGo Redwoods and the WeScience corpora, parsed with the English Resource Grammar and the PET parser.</abstract>
      <bibkey>ben-gera-etal-2010-semantic</bibkey>
    </paper>
    <paper id="344">
      <author><first>Nuria</first><last>Gala</last></author>
      <author><first>Véronique</first><last>Rey</last></author>
      <author><first>Michael</first><last>Zock</last></author>
      <title>A Tool for Linking Stems and Conceptual Fragments to Enhance word Access</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/497_Paper.pdf</url>
      <abstract>Electronic dictionaries offer many possibilities unavailable in paper dictionaries to view, display or access information. However, even these resources fall short when it comes to access words sharing semantic features and certain aspects of form: few applications offer the possibility to access a word via a morphologically or semantically related word. In this paper, we present such an application, Polymots, a lexical database for contemporary French containing 20.000 words grouped in 2.000 families. The purpose of this resource is to group words into families on the basis of shared morpho-phonological and semantic information. Words with a common stem form a family; words in a family also share a set of common conceptual fragments (in some families there is a continuity of meaning, in others meaning is distributed). With this approach, we capitalize on the bidirectional link between semantics and morpho-phonology : the user can thus access words not only on the basis of ideas, but also on the basis of formal characteristics of the word, i.e. its morphological features. The resulting lexical database should help people learn French vocabulary and assist them to find words they are looking for, going thus beyond other existing lexical resources.</abstract>
      <bibkey>gala-etal-2010-tool</bibkey>
    </paper>
    <paper id="345">
      <author><first>Valia</first><last>Kordoni</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <title>Disambiguating Compound Nouns for a Dynamic <fixed-case>HPSG</fixed-case> Treebank of <fixed-case>W</fixed-case>all <fixed-case>S</fixed-case>treet <fixed-case>J</fixed-case>ournal Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/498_Paper.pdf</url>
      <abstract>The aim of this paper is twofold. We focus, on the one hand, on the task of dynamically annotating English compound nouns, and on the other hand we propose disambiguation methods and techniques which facilitate the annotation task. Both the aforementioned are part of a larger on-going effort which aims to create HPSG annotation for the texts from theWall Street Journal (henceforward WSJ) sections of the Penn Treebank (henceforward PTB) with the help of a hand-written large-scale and wide-coverage grammar of English, the English Resource Grammar (henceforward ERG; Flickinger (2002)). As we show in this paper, such annotations are very rich linguistically, since apart from syntax they also incorporate semantics, which does not only ensure that the treebank is guaranteed to be a truly sharable, re-usable and multi-functional linguistic resource, but also calls for the necessity of a better disambiguation of the internal (syntactic) structure of larger units of words, such as compound nouns, since this has an impact on the representation of their meaning, which is of utmost interest if the linguistic annotation of a given corpus is to be further understood as the practice of adding interpretative linguistic information of the highest quality in order to give added value to the corpus.</abstract>
      <bibkey>kordoni-zhang-2010-disambiguating</bibkey>
    </paper>
    <paper id="346">
      <author><first>Lukas</first><last>Michelbacher</last></author>
      <author><first>Florian</first><last>Laws</last></author>
      <author><first>Beate</first><last>Dorow</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <title>Building a Cross-lingual Relatedness Thesaurus using a Graph Similarity Measure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/499_Paper.pdf</url>
      <abstract>The Internet is an ever growing source of information stored in documents of different languages. Hence, cross-lingual resources are needed for more and more NLP applications. This paper presents (i) a graph-based method for creating one such resource and (ii) a resource created using the method, a cross-lingual relatedness thesaurus. Given a word in one language, the thesaurus suggests words in a second language that are semantically related. The method requires two monolingual corpora and a basic dictionary. Our general approach is to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. A bilingual dictionary containing basic vocabulary provides seed translations relating nodes from both graphs. We then use an inter-graph node-similarity algorithm to discover related words. Evaluation with three human judges revealed that 49% of the English and 57% of the German words discovered by our method are semantically related to the target words. We publish two resources in conjunction with this paper. First, noun coordinations extracted from the German and English Wikipedias. Second, the cross-lingual relatedness thesaurus which can be used in experiments involving interactive cross-lingual query expansion.</abstract>
      <bibkey>michelbacher-etal-2010-building</bibkey>
    </paper>
    <paper id="347">
      <author><first>Samuel</first><last>Broscheit</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Yannick</first><last>Versley</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <title>Extending <fixed-case>BART</fixed-case> to Provide a Coreference Resolution System for <fixed-case>G</fixed-case>erman</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/500_Paper.pdf</url>
      <abstract>We present a flexible toolkit-based approach to automatic coreference resolution on German text. We start with our previous work aimed at reimplementing the system from Soon et al. (2001) for English, and extend it to duplicate a version of the state-of-the-art proposal from Klenner and Ailloud (2009). Evaluation performed on a benchmarking dataset, namely the TueBa-D/Z corpus (Hinrichs et al., 2005b), shows that machine learning based coreference resolution can be robustly performed in a language other than English.</abstract>
      <bibkey>broscheit-etal-2010-extending</bibkey>
    </paper>
    <paper id="348">
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Helmut</first><last>Schmid</last></author>
      <author><first>Kerstin</first><last>Eckart</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <title>A Corpus Representation Format for Linguistic Web Services: The <fixed-case>D</fixed-case>-<fixed-case>SPIN</fixed-case> Text Corpus Format and its Relationship with <fixed-case>ISO</fixed-case> Standards</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/503_Paper.pdf</url>
      <abstract>In the framework of the preparation of linguistic web services for corpus processing, the need for a representation format was felt, which supports interoperability between different web services in a corpus processing pipeline, but also provides a well-defined interface to both, legacy tools and their data formats and upcoming international standards. We present the D-SPIN text corpus format, TCF, which was designed for this purpose. It is a stand-off XML format, inspired by the philosophy of the emerging standards LAF (Linguistic Annotation Framework) and its ``instances'' MAF for morpho-syntactic annotation and SynAF for syntactic annotation. Tools for the exchange with existing (best practice) formats are available, and a converter from MAF to TCF is being tested in spring 2010. We describe the usage scenario where TCF is embedded and the properties and architecture of TCF. We also give examples of TCF encoded data and describe the aspects of syntactic and semantic interoperability already addressed.</abstract>
      <bibkey>heid-etal-2010-corpus</bibkey>
    </paper>
    <paper id="349">
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Nicola</first><last>Cancedda</last></author>
      <author><first>Marc</first><last>Dymetman</last></author>
      <title>A Dataset for Assessing Machine Translation Evaluation Metrics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/504_Paper.pdf</url>
      <abstract>We describe a dataset containing 16,000 translations produced by four machine translation systems and manually annotated for quality by professional translators. This dataset can be used in a range of tasks assessing machine translation evaluation metrics, from basic correlation analysis to training and test of machine learning-based metrics. By providing a standard dataset for such tasks, we hope to encourage the development of better MT evaluation metrics.</abstract>
      <bibkey>specia-etal-2010-dataset</bibkey>
    </paper>
    <paper id="350">
      <author><first>Jakob</first><last>Halskov</last></author>
      <author><first>Dorte Haltrup</first><last>Hansen</last></author>
      <author><first>Anna</first><last>Braasch</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <title>Quality Indicators of <fixed-case>LSP</fixed-case> Texts — Selection and Measurements Measuring the Terminological Usefulness of Documents for an <fixed-case>LSP</fixed-case> Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/505_Paper.pdf</url>
      <abstract>This paper describes and evaluates a prototype quality assurance system for LSP corpora. The system will be employed in compiling a corpus of 11 M tokens for various linguistic and terminological purposes. The system utilizes a number of linguistic features as quality indicators. These represent two dimensions of quality, namely readability/formality (e.g. word length and passive constructions) and density of specialized knowledge (e.g. out-of-vocabulary items). Threshold values for each indicator are induced from a reference corpus of general (fiction, magazines and newspapers) and specialized language (the domains of Health/Medicine and Environment/Climate). In order to test the efficiency of the indicators, a number of terminologically relevant, irrelevant and possibly relevant texts are manually selected from target web sites as candidate texts. By applying the indicators to these candidate texts, the system is able to filter out non-LSP and poor LSP texts with a precision of 100% and a recall of 55%. Thus, the experiment described in this paper constitutes fundamental work towards a formulation of best practice for implementing quality assurance when selecting appropriate texts for an LSP corpus. The domain independence of the quality indicators still remains to be thoroughly tested on more than just two domains.</abstract>
      <bibkey>halskov-etal-2010-quality</bibkey>
    </paper>
    <paper id="351">
      <author><first>Gregor</first><last>Bertrand</last></author>
      <author><first>Florian</first><last>Nothdurft</last></author>
      <author><first>Steffen</first><last>Walter</last></author>
      <author><first>Andreas</first><last>Scheck</last></author>
      <author><first>Henrik</first><last>Kessler</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <title>Towards Investigating Effective Affective Dialogue Strategies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/506_Paper.pdf</url>
      <abstract>We describe an experimentalWizard-of-Oz-setup for the integration of emotional strategies into spoken dialogue management. With this setup we seek to evaluate different approaches to emotional dialogue strategies in human computer interaction with a spoken dialogue system. The study aims to analyse what kinds of emotional strategies work best in spoken dialogue management especially facing the problem that users may not be honest about their emotions. Therefore as well direct (user is asked about his state) as indirect (measurements of psychophysiological features) evidence is considered for the evaluation of our strategies.</abstract>
      <bibkey>bertrand-etal-2010-towards</bibkey>
    </paper>
    <paper id="352">
      <author><first>Rüdiger</first><last>Gleim</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <title>Computational Linguistics for Mere Mortals - Powerful but Easy-to-use Linguistic Processing for Scientists in the Humanities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/507_Paper.pdf</url>
      <abstract>Delivering linguistic resources and easy-to-use methods to a broad public in the humanities is a challenging task. On the one hand users rightly demand easy to use interfaces but on the other hand want to have access to the full flexibility and power of the functions being offered. Even though a growing number of excellent systems exist which offer convenient means to use linguistic resources and methods, they usually focus on a specific domain, as for example corpus exploration or text categorization. Architectures which address a broad scope of applications are still rare. This article introduces the eHumanities Desktop, an online system for corpus management, processing and analysis which aims at bridging the gap between powerful command line tools and intuitive user interfaces.</abstract>
      <bibkey>gleim-mehler-2010-computational</bibkey>
    </paper>
    <paper id="353">
      <author><first>Maria</first><last>Holmqvist</last></author>
      <title>Heuristic Word Alignment with Parallel Phrases</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/508_Paper.pdf</url>
      <abstract>We present a heuristic method for word alignment, which is the task of identifying corresponding words in parallel text. The heuristic method is based on parallel phrases extracted from manually word aligned sentence pairs. Word alignment is performed by matching parallel phrases to new sentence pairs, and adding word links from the parallel phrase to words in the matching sentence segment. Experiments on an English--Swedish parallel corpus showed that the heuristic phrase-based method produced word alignments with high precision but low recall. In order to improve alignment recall, phrases were generalized by replacing words with part-of-speech categories. The generalization improved recall but at the expense of precision. Two filtering strategies were investigated to prune the large set of generalized phrases. Finally, the phrase-based method was compared to statistical word alignment with Giza++ and we found that although statistical alignments based on large datasets will outperform phrase-based word alignment, a combination of phrase-based and statistical word alignment outperformed pure statistical alignment in terms of Alignment Error Rate (AER).</abstract>
      <bibkey>holmqvist-2010-heuristic</bibkey>
    </paper>
    <paper id="354">
      <author><first>Martijn</first><last>Goudbeek</last></author>
      <author><first>Mirjam</first><last>Broersma</last></author>
      <title>The Demo / Kemo Corpus: A Principled Approach to the Study of Cross-cultural Differences in the Vocal Expression and Perception of Emotion</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/511_Paper.pdf</url>
      <abstract>This paper presents the Demo / Kemo corpus of Dutch and Korean emotional speech. The corpus has been specifically developed for the purpose of cross-linguistic comparison, and is more balanced than any similar corpus available so far: a) it contains expressions by both Dutch and Korean actors as well as judgments by both Dutch and Korean listeners; b) the same elicitation technique and recording procedure was used for recordings of both languages; c) the same nonsense sentence, which was constructed to be permissible in both languages, was used for recordings of both languages; and d) the emotions present in the corpus are balanced in terms of valence, arousal, and dominance. The corpus contains a comparatively large number of emotions (eight) uttered by a large number of speakers (eight Dutch and eight Korean). The counterbalanced nature of the corpus will enable a stricter investigation of language-specific versus universal aspects of emotional expression than was possible so far. Furthermore, given the carefully controlled phonetic content of the expressions, it allows for analysis of the role of specific phonetic features in emotional expression in Dutch and Korean.</abstract>
      <bibkey>goudbeek-broersma-2010-demo</bibkey>
    </paper>
    <paper id="355">
      <author><first>Armando</first><last>Stellato</last></author>
      <author><first>Heiko</first><last>Stoermer</last></author>
      <author><first>Stefano</first><last>Bortoli</last></author>
      <author><first>Noemi</first><last>Scarpato</last></author>
      <author><first>Andrea</first><last>Turbati</last></author>
      <author><first>Paolo</first><last>Bouquet</last></author>
      <author><first>Maria Teresa</first><last>Pazienza</last></author>
      <title><fixed-case>M</fixed-case>askkot — An Entity-centric Annotation Platform</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/515_Paper.pdf</url>
      <abstract>The Semantic Web is facing the important challenge to maintain its promise of a real world-wide graph of interconnected resources. Unfortunately, while URIs almost guarantee a direct reference to entities, the relation between the two is not bijective. Many different URI references to same concepts and entities can arise when -- in such a heterogeneous setting as the WWW -- people independently build new ontologies, or populate shared ones with new arbitrarily identified individuals. The proliferation of URIs is an unwanted, though natural effect strictly bound to the same principles which characterize the Semantic Web; reducing this phenomenon will improve the recall of Semantic Search engines, which could rely on explicit links between heterogeneous information sources. To address this problem, in this paper we present an integrated environment combining the semantic annotation and ontology building features available in the Semantic Turkey web browser extension, with globally unique identifiers for entities provided by the okkam Entity Name System, thus realizing a valuable resource for preventing diffusion of multiple URIs on the (Semantic) Web.</abstract>
      <bibkey>stellato-etal-2010-maskkot</bibkey>
    </paper>
    <paper id="356">
      <author><first>Petr</first><last>Pollák</last></author>
      <author><first>Josef</first><last>Rajnoha</last></author>
      <title>Multi-Channel Database of Spontaneous <fixed-case>C</fixed-case>zech with Synchronization of Channels Recorded by Independent Devices</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/516_Paper.pdf</url>
      <abstract>This paper describes Czech spontaneous speech database of lectures on digital signal processing topic collected at Czech Technical University in Prague, commonly with the procedure of its recording and annotation. The database contains 21.7 hours of speech material from 22 speakers recorded in 4 channels with 3 principally different microphones. The annotation of the database is composed from basic time segmentation, orthographic transcription including marks for speaker and environmental non-speech events, pronunciation lexicon in SAMPA alphabet, session and speaker information describing recording conditions, and the documentation. The orthographic transcription with time segmentation is saved in XML format supported by frequently used annotation tool Transcriber. In this article, special attention is also paid to the description of time synchronization of signals recorded by two independent devices: computer based recording platform using two external sound cards and commercial audio recorder Edirol R09. This synchronization is based on cross-correlation analysis with simple automated selection of suitable short signal subparts. The collection and annotation of this database is now complete and its availability via ELRA is currently under preparation.</abstract>
      <bibkey>pollak-rajnoha-2010-multi</bibkey>
    </paper>
    <paper id="357">
      <author><first>Guillaume</first><last>Bernard</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <author><first>Olivier</first><last>Galibert</last></author>
      <title>A Question-answer Distance Measure to Investigate <fixed-case>QA</fixed-case> System Progress</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/518_Paper.pdf</url>
      <abstract>The performance of question answering system is evaluated through successive evaluations campaigns. A set of questions are given to the participating systems which are to find the correct answer in a collection of documents. The creation process of the questions may change from one evaluation to the next. This may entail an uncontroled question difficulty shift. For the QAst 2009 evaluation campaign, a new procedure was adopted to build the questions. Comparing results of QAst 2008 and QAst 2009 evaluations, a strong performance loss could be measured in 2009 for French and English, while the Spanish systems globally made progress. The measured loss might be related to this new way of elaborating questions. The general purpose of this paper is to propose a measure to calibrate the difficulty of a question set. In particular, a reasonable measure should output higher values for 2009 than for 2008. The proposed measure relies on a distance measure between the critical elements of a question and those of the associated correct answer. An increase of the proposed distance measure for French and English 2009 evaluations as compared to 2008 could be established. This increase correlates with the previously observed degraded performances. We conclude on the potential of this evaluation criterion: the importance of such a measure for the elaboration of new question corpora for questions answering systems and a tool to control the level of difficulty for successive evaluation campaigns.</abstract>
      <bibkey>bernard-etal-2010-question</bibkey>
    </paper>
    <paper id="358">
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <title>Fine-Grained Geographical Relation Extraction from <fixed-case>W</fixed-case>ikipedia</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/519_Paper.pdf</url>
      <abstract>In this paper, we present work on enhancing the basic data resource of a context-aware system. Electronic text offers a wealth of information about geospatial data and can be used to improve the completeness and accuracy of geospatial resources (e.g., gazetteers). First, we introduce a supervised approach to extracting geographical relations on a fine-grained level. Second, we present a novel way of using Wikipedia as a corpus based on self-annotation. A self-annotation is an automatically created high-quality annotation that can be used for training and evaluation. Wikipedia contains two types of different context: (i) unstructured text and (ii) structured data: templates (e.g., infoboxes about cities), lists and tables. We use the structured data to annotate the unstructured text. Finally, the extracted fine-grained relations are used to complete gazetteer data. The precision and recall scores of more than 97 percent confirm that a statistical IE pipeline can be used to improve the data quality of community-based resources.</abstract>
      <bibkey>blessing-schutze-2010-fine</bibkey>
    </paper>
    <paper id="359">
      <author><first>Sarra El</first><last>Ayari</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <title>Fine-grained Linguistic Evaluation of Question Answering Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/520_Paper.pdf</url>
      <abstract>Question answering systems are complex systems using natural language processing. Some evaluation campaigns are organized to evaluate such systems in order to propose a classification of systems based on final results (number of correct answers). Nevertheless, teams need to evaluate more precisely the results obtained by their systems if they want to do a diagnostic evaluation. There are no tools or methods to do these evaluations systematically. We present REVISE, a tool for glass box evaluation based on diagnostic of question answering system results.</abstract>
      <bibkey>ayari-etal-2010-fine</bibkey>
    </paper>
    <paper id="360">
      <author><first>Nao</first><last>Tatsumi</last></author>
      <author><first>Jun</first><last>Okamoto</last></author>
      <author><first>Shun</first><last>Ishizaki</last></author>
      <title>Evaluating Semantic Relations and Distances in the Associative Concept Dictionary using <fixed-case>NIRS</fixed-case>-imaging</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/521_Paper.pdf</url>
      <abstract>In this study, we extracted brain activities related to semantic relations and distances to improve the precision of distance calculation among concepts in the Associated Concept Dictionary (ACD). For the experiments, we used a multi-channel Near-infrared Spectroscopy (NIRS) device to measure the response properties of the changes in hemoglobin concentration during word-concept association tasks. The experiments stimuli were selected from pairs of stimulus words and associated words in the ACD and presented in the form of a visual stimulation to the subjects. In our experiments, we obtained subject response data and brain activation data in Broca's area ―a human brain region that is active in linguistic/word-concept decision tasks― and these data imply relations with the length of associative distance. This study showed that it was possible to connect brain activities to the semantic relation among concepts, and that it would improve the method for concept distance calculation in order to build a more human-like ontology model.</abstract>
      <bibkey>tatsumi-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="361">
      <author><first>Danica</first><last>Damljanovic</last></author>
      <author><first>Milan</first><last>Agatonovic</last></author>
      <author><first>Hamish</first><last>Cunningham</last></author>
      <title>Identification of the Question Focus: Combining Syntactic Analysis and Ontology-based Lookup through the User Interaction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/524_Paper.pdf</url>
      <abstract>Most question-answering systems contain a classifier module which determines a question category, based on which each question is assigned an answer type. However, setting up syntactic patterns for this classification is a big challenge. In addition, in the case of ontology-based systems, the answer type should be aligned to the queried knowledge structure. In this paper, we present an approach for determining the answer type semi-automatically. We first identify the question focus using syntactic parsing, and then try to identify the answer type by combining the head of the focus with the ontology-based lookup. When this combination is not enough to make conclusions automatically, the user is engaged into a dialog in order to resolve the answer type. User selections are saved and used for training the system in order to improve its performance over time. Further on, the answer type is used to show the feedback and the concise answer to the user. Our approach is evaluated using 250 questions from the Mooney Geoquery dataset.</abstract>
      <bibkey>damljanovic-etal-2010-identification</bibkey>
    </paper>
    <paper id="362">
      <author><first>Arnaud</first><last>Grappy</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Vincent</first><last>Barbier</last></author>
      <title>A Corpus for Studying Full Answer Justification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/529_Paper.pdf</url>
      <abstract>Question answering (QA) systems aim at retrieving precise information from a large collection of documents. To be considered as reliable by users, a QA system must provide elements to evaluate the answer. This notion of answer justification can also be useful when developping a QA system in order to give criteria for selecting correct answers. An answer justification can be found in a sentence, a passage made of several consecutive sentences or several passages of a document or several documents. Thus, we are interesting in pinpointing the set of information that allows to verify the correctness of the answer in a candidate passage and the question elements that are missing in this passage. Moreover, the relevant information is often given in texts in a different form from the question form: anaphora, paraphrases, synonyms. In order to have a better idea of the importance of all the phenomena we underlined, and to provide enough examples at the QA developer's disposal to study them, we decided to build an annotated corpus.</abstract>
      <bibkey>grappy-etal-2010-corpus</bibkey>
    </paper>
    <paper id="363">
      <author><first>Silvana Marianela Bernaola</first><last>Biggio</last></author>
      <author><first>Manuela</first><last>Speranza</last></author>
      <author><first>Roberto</first><last>Zanoli</last></author>
      <title>Entity Mention Detection using a Combination of Redundancy-Driven Classifiers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/530_Paper.pdf</url>
      <abstract>We present an experimental framework for Entity Mention Detection in which two different classifiers are combined to exploit Data Redundancy attained through the annotation of a large text corpus, as well as a number of Patterns extracted automatically from the same corpus. In order to recognize proper name, nominal, and pronominal mentions we not only exploit the information given by mentions recognized within the corpus being annotated, but also given by mentions occurring in an external and unannotated corpus. The system was first evaluated in the Evalita 2009 evaluation campaign obtaining good results. The current version is being used in a number of applications: on the one hand, it is used in the LiveMemories project, which aims at scaling up content extraction techniques towards very large scale extraction from multimedia sources. On the other hand, it is used to annotate corpora, such as Italian Wikipedia, thus providing easy access to syntactic and semantic annotation for both the Natural Language Processing and Information Retrieval communities. Moreover a web service version of the system is available and the system is going to be integrated into the TextPro suite of NLP tools.</abstract>
      <bibkey>biggio-etal-2010-entity</bibkey>
    </paper>
    <paper id="364">
      <author><first>Gábor</first><last>Recski</last></author>
      <author><first>András</first><last>Rung</last></author>
      <author><first>Attila</first><last>Zséder</last></author>
      <author><first>András</first><last>Kornai</last></author>
      <title><fixed-case>NP</fixed-case> Alignment in Bilingual Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/531_Paper.pdf</url>
      <abstract>Aligning the NPs of parallel corpora is logically halfway between the sentence- and word-alignment tasks that occupy much of the MT literature, but has received far less attention. NP alignment is a challenging problem, capable of rapidly exposing flaws both in the word-alignment and in the NP chunking algorithms one may bring to bear. It is also a very rewarding problem in that NPs are semantically natural translation units, which means that (i) word alignments will cross NP boundaries only exceptionally, and (ii) within sentences already aligned, the proportion of 1-1 alignments will be higher for NPs than words. We created a simple gold standard for English-Hungarian, Orwells 1984, (since this already exists in manually verified POS-tagged format in many languages thanks to the Multex and MultexEast project) by manually verifying the automaticaly generated NP chunking (we used the yamcha, mallet and hunchunk taggers) and manually aligning the maximal NPs and PPs. The maximum NP chunking problem is much harder than base NP chunking, with F-measure in the .7 range (as opposed to over .94 for base NPs). Since the results are highly impacted by the quality of the NP chunking, we tested our alignment algorithms both with real world (machine obtained) chunkings, where results are in the .35 range for the baseline algorithm which propagates GIZA++ word alignments to the NP level, and on idealized (manually obtained) chunkings, where the baseline reaches .4 and our current system reaches .64.</abstract>
      <bibkey>recski-etal-2010-np</bibkey>
    </paper>
    <paper id="365">
      <author><first>Andrew</first><last>Gargett</last></author>
      <author><first>Konstantina</first><last>Garoufi</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <author><first>Kristina</first><last>Striegnitz</last></author>
      <title>The <fixed-case>GIVE</fixed-case>-2 Corpus of Giving Instructions in Virtual Environments</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/532_Paper.pdf</url>
      <abstract>We present the GIVE-2 Corpus, a new corpus of human instruction giving. The corpus was collected by asking one person in each pair of subjects to guide the other person towards completing a task in a virtual 3D environment with typed instructions. This is the same setting as that of the recent GIVE Challenge, and thus the corpus can serve as a source of data and as a point of comparison for NLG systems that participate in the GIVE Challenge. The instruction-giving data we collect is multilingual (45 German and 63 English dialogues), and can easily be extended to further languages by using our software, which we have made available. We analyze the corpus to study the effects of learning by repeated participation in the task and the effects of the participants' spatial navigation abilities. Finally, we present a novel annotation scheme for situated referring expressions and compare the referring expressions in the German and English data.</abstract>
      <bibkey>gargett-etal-2010-give</bibkey>
    </paper>
    <paper id="366">
      <author><first>Alexander</first><last>Vorwerk</last></author>
      <author><first>Xiaohui</first><last>Wang</last></author>
      <author><first>Dorothea</first><last>Kolossa</last></author>
      <author><first>Steffen</first><last>Zeiler</last></author>
      <author><first>Reinhold</first><last>Orglmeister</last></author>
      <title><fixed-case>WAPUSK</fixed-case>20 - A Database for Robust Audiovisual Speech Recognition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/533_Paper.pdf</url>
      <abstract>Audiovisual speech recognition (AVSR) systems have been proven superior over audio-only speech recognizers in noisy environments by incorporating features of the visual modality. In order to develop reliable AVSR systems, appropriate simultaneously recorded speech and video data is needed. In this paper, we will introduce a corpus (WAPUSK20) that consists of audiovisual data of 20 speakers uttering 100 sentences each with four channels of audio and a stereoscopic video. The latter is intended to support more accurate lip tracking and the development of stereo data based normalization techniques for greater robustness of the recognition results. The sentence design has been adopted from the GRID corpus that has been widely used for AVSR experiments. Recordings have been made under acoustically realistic conditions in a usual office room. Affordable hardware equipment has been used, such as a pre-calibrated stereo camera and standard PC components. The software written to create this corpus was designed in MATLAB with help of hardware specific software provided by the hardware manufacturers and freely available open source software.</abstract>
      <bibkey>vorwerk-etal-2010-wapusk20</bibkey>
    </paper>
    <paper id="367">
      <author><first>Eneko</first><last>Agirre</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <title>Exploring Knowledge Bases for Similarity</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/534_Paper.pdf</url>
      <abstract>Graph-based similarity over WordNet has been previously shown to perform very well on word similarity. This paper presents a study of the performance of such a graph-based algorithm when using different relations and versions of Wordnet. The graph algorithm is based on Personalized PageRank, a random-walk based algorithm which computes the probability of a random-walk initiated in the target word to reach any synset following the relations in WordNet (Haveliwala, 2002). Similarity is computed as the cosine of the probability distributions for each word over WordNet. The best combination of relations includes all relations in WordNet 3.0, included disambiguated glosses, and automatically disambiguated topic signatures called KnowNets. All relations are part of the official release of WordNet, except KnowNets, which have been derived automatically. The results over the WordSim 353 dataset show that using the adequate relations the performance improves over previously published WordNet-based results on the WordSim353 dataset (Finkelstein et al., 2002). The similarity software and some graphs used in this paper are publicly available at http://ixa2.si.ehu.es/ukb.</abstract>
      <bibkey>agirre-etal-2010-exploring</bibkey>
    </paper>
    <paper id="368">
      <author><first>Cristina</first><last>Sánchez-Marco</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <author><first>Josep Maria</first><last>Fontana</last></author>
      <author><first>Judith</first><last>Domingo</last></author>
      <title>Annotation and Representation of a Diachronic Corpus of <fixed-case>S</fixed-case>panish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/535_Paper.pdf</url>
      <abstract>In this article we describe two different strategies for the automatic tagging of a Spanish diachronic corpus involving the adaptation of existing NLP tools developed for modern Spanish. In the initial approach we follow a state-of-the-art strategy, which consists on standardizing the spelling and the lexicon. This approach boosts POS-tagging accuracy to 90, which represents a raw improvement of over 20% with respect to the results obtained without any pre-processing. In order to enable non-expert users in NLP to use this new resource, the corpus has been integrated into IAC (Corpora Interface Access). We discuss the shortcomings of the initial approach and propose a new one, which does not consist in adapting the source texts to the tagger, but rather in modifying the tagger for the direct treatment of the old variants.This second strategy addresses some important shortcomings in the previous approach and is likely to be useful not only in the creation of diachronic linguistic resources but also for the treatment of dialectal or non-standard variants of synchronic languages as well.</abstract>
      <bibkey>sanchez-marco-etal-2010-annotation</bibkey>
    </paper>
    <paper id="369">
      <author><first>Ghulam</first><last>Raza</last></author>
      <title>Inferring Subcat Frames of Verbs in <fixed-case>U</fixed-case>rdu</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/536_Paper.pdf</url>
      <abstract>This paper describes an approach for inferring syntactic frames of verbs in Urdu from an untagged corpus. Urdu, like many other South Asian languages, is a free word order and case-rich language. Separable lexical units mark different constituents for case in phrases and clauses and are called case clitics. There is not always a one to one correspondence between case clitic form and case, and case and grammatical function in Urdu. Case clitics, therefore, can not serve as direct clues for extracting the syntactic frames of verbs. So a two-step approach has been implemented. In a first step, all case clitic combinations for a verb are extracted and the unreliable ones are filtered out by applying the inferential statistics. In a second step, the information of occurrences of case clitic forms in different combinations as a whole and on individual level is processed to infer all possible syntactic frames of the verb.</abstract>
      <bibkey>raza-2010-inferring</bibkey>
    </paper>
    <paper id="370">
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Faiza</first><last>Gara</last></author>
      <author><first>Olivier</first><last>Mesnard</last></author>
      <author><first>Meriama</first><last>Laïb</last></author>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <title><fixed-case>LIMA</fixed-case> : A Multilingual Framework for Linguistic Analysis and Linguistic Resources Development and Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/537_Paper.pdf</url>
      <abstract>The increasing amount of available textual information makes necessary the use of Natural Language Processing (NLP) tools. These tools have to be used on large collections of documents in different languages. But NLP is a complex task that relies on many processes and resources. As a consequence, NLP tools must be both configurable and efficient: specific software architectures must be designed for this purpose. We present in this paper the LIMA multilingual analysis platform, developed at CEA LIST. This configurable platform has been designed to develop NLP based industrial applications while keeping enough flexibility to integrate various processes and resources. This design makes LIMA a linguistic analyzer that can handle languages as different as French, English, German, Arabic or Chinese. Beyond its architecture principles and its capabilities as a linguistic analyzer, LIMA also offers a set of tools dedicated to the test and the evaluation of linguistic modules and to the production and the management of new linguistic resources.</abstract>
      <bibkey>besancon-etal-2010-lima</bibkey>
    </paper>
    <paper id="371">
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <title>A Named Entity Labeler for <fixed-case>G</fixed-case>erman: Exploiting <fixed-case>W</fixed-case>ikipedia and Distributional Clusters</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/538_Paper.pdf</url>
      <abstract>Named Entity Recognition is a relatively well-understood NLP task, with many publicly available training resources and software for processing English data. Other languages tend to be underserved in this area. For German, CoNLL-2003 Shared Task provided training data, but there are no publicly available, ready-to-use tools. We fill this gap and develop a German NER system with state-of-the-art performance. In addition to CoNLL 2003 labeled training data, we use two additional resources: (i) 32 million words of unlabeled news article text and (ii) infobox labels from German Wikipedia articles. From the unlabeled text we derive distributional word clusters. Then we use cluster membership features and Wikipedia infobox label features to train a supervised model on the labeled training data. This approach allows us to deal better with word-types unseen in the training data and achieve good performance on German with little engineering effort.</abstract>
      <bibkey>chrupala-klakow-2010-named</bibkey>
    </paper>
    <paper id="372">
      <author><first>Magnus</first><last>Rosell</last></author>
      <title>Text Cluster Trimming for Better Descriptions and Improved Quality</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/540_Paper.pdf</url>
      <abstract>Text clustering is potentially very useful for exploration of text sets that are too large to study manually. The success of such a tool depends on whether the results can be explained to the user. An automatically extracted cluster description usually consists of a few words that are deemed representative for the cluster. It is preferably short in order to be easily grasped. However, text cluster content is often diverse. We introduce a trimming method that removes texts that do not contain any, or a few of the words in the cluster description. The result is clusters that match their descriptions better. In experiments on two quite different text sets we obtain significant improvements in both internal and external clustering quality for the trimmed clustering compared to the original. The trimming thus has two positive effects: it forces the clusters to agree with their descriptions (resulting in better descriptions) and improves the quality of the trimmed clusters.</abstract>
      <bibkey>rosell-2010-text</bibkey>
    </paper>
    <paper id="373">
      <author><first>Jesús</first><last>González-Rubio</last></author>
      <author><first>Jorge</first><last>Civera</last></author>
      <author><first>Alfons</first><last>Juan</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <title><fixed-case>S</fixed-case>aturnalia: A <fixed-case>L</fixed-case>atin-<fixed-case>C</fixed-case>atalan Parallel Corpus for Statistical <fixed-case>MT</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/541_Paper.pdf</url>
      <abstract>Currently, a great effort is being carried out in the digitalisation of large historical document collections for preservation purposes. The documents in these collections are usually written in ancient languages, such as Latin or Greek, which limits the access of the general public to their content due to the language barrier. Therefore, digital libraries aim not only at storing raw images of digitalised documents, but also to annotate them with their corresponding text transcriptions and translations into modern languages. Unfortunately, ancient languages have at their disposal scarce electronic resources to be exploited by natural language processing techniques. This paper describes the compilation process of a novel Latin-Catalan parallel corpus as a new task for statistical machine translation (SMT). Preliminary experimental results are also reported using a state-of-the-art phrase-based SMT system. The results presented in this work reveal the complexity of the task and its challenging, but interesting nature for future development.</abstract>
      <bibkey>gonzalez-rubio-etal-2010-saturnalia</bibkey>
    </paper>
    <paper id="374">
      <author><first>Emilia</first><last>Apostolova</last></author>
      <author><first>Sean</first><last>Neilan</last></author>
      <author><first>Gary</first><last>An</last></author>
      <author><first>Noriko</first><last>Tomuro</last></author>
      <author><first>Steven</first><last>Lytinen</last></author>
      <title><fixed-case>D</fixed-case>jangology: A Light-weight Web-based Tool for Distributed Collaborative Text Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/543_Paper.pdf</url>
      <abstract>Manual text annotation is a resource-consuming endeavor necessary for NLP systems when they target new tasks or domains for which there are no existing annotated corpora. Distributing the annotation work across multiple contributors is a natural solution to reduce and manage the effort required. Although there are a few publicly available tools which support distributed collaborative text annotation, most of them have complex user interfaces and require a significant amount of involvement from the annotators/contributors as well as the project developers and administrators. We present a light-weight web application for highly distributed annotation projects - Djangology. The application takes advantage of the recent advances in web framework architecture that allow rapid development and deployment of web applications thus minimizing development time for customization. The application's web-based interface gives project administrators the ability to easily upload data, define project schemas, assign annotators, monitor progress, and review inter-annotator agreement statistics. The intuitive web-based user interface encourages annotator participation as contributors are not burdened by tool manuals, local installation, or configuration. The system has achieved a user response rate of 70% in two annotation projects involving more than 250 medical experts from various geographic locations.</abstract>
      <bibkey>apostolova-etal-2010-djangology</bibkey>
    </paper>
    <paper id="375">
      <author><first>Leon</first><last>Derczynski</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Analysing Temporally Annotated Corpora with <fixed-case>CAV</fixed-case>a<fixed-case>T</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/546_Paper.pdf</url>
      <abstract>We present CAVaT, a tool that performs Corpus Analysis and Validation for TimeML. CAVaT is an open source, modular checking utility for statistical analysis of features specific to temporally-annotated natural language corpora. It provides reporting, highlights salient links between a variety of general and time-specific linguistic features, and also validates a temporal annotation to ensure that it is logically consistent and sufficiently annotated. Uniquely, CAVaT provides analysis specific to TimeML-annotated temporal information. TimeML is a standard for annotating temporal information in natural language text. In this paper, we present the reporting part of CAVaT, and then its error-checking ability, including the workings of several novel TimeML document verification methods. This is followed by the execution of some example tasks using the tool to show relations between times, events, signals and links. We also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been detected with CAVaT.</abstract>
      <bibkey>derczynski-gaizauskas-2010-analysing</bibkey>
    </paper>
    <paper id="376">
      <author><first>Martin</first><last>Reynaert</last></author>
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Orphée</first><last>De Clercq</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Franciska</first><last>de Jong</last></author>
      <title>Balancing <fixed-case>S</fixed-case>o<fixed-case>N</fixed-case>a<fixed-case>R</fixed-case>: <fixed-case>IPR</fixed-case> versus Processing Issues in a 500-Million-Word Written <fixed-case>D</fixed-case>utch Reference Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/549_Paper.pdf</url>
      <abstract>In The Low Countries, a major reference corpus for written Dutch is being built. We discuss the interplay between data acquisition and data processing during the creation of the SoNaR Corpus. Based on developments in traditional corpus compiling and new web harvesting approaches, SoNaR is designed to contain 500 million words, balanced over 36 text types including both traditional and new media texts. Beside its balanced design, every text sample included in SoNaR will have its IPR issues settled to the largest extent possible. This data collection task presents many challenges because every decision taken on the level of text acquisition has ramifications for the level of processing and the general usability of the corpus. As far as the traditional text types are concerned, each text brings its own processing requirements and issues. For new media texts - SMS, chat - the problem is even more complex, issues such as anonimity, recognizability and citation right, all present problems that have to be tackled. The solutions actually lead to the creation of two corpora: a gigaword SoNaR, IPR-cleared for research purposes, and the smaller - of commissioned size - more privacy compliant SoNaR, IPR-cleared for commercial purposes as well.</abstract>
      <bibkey>reynaert-etal-2010-balancing</bibkey>
    </paper>
    <paper id="377">
      <author><first>Samuel</first><last>Cruz-Lara</last></author>
      <author><first>Gil</first><last>Francopoulo</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <title><fixed-case>MLIF</fixed-case> : A Metamodel to Represent and Exchange Multilingual Textual Information</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/550_Paper.pdf</url>
      <abstract>The fast evolution of language technology has produced pressing needs in standardization. The multiplicity of language resources representation levels and the specialization of these representations make difficult the interaction between linguistic resources and components manipulating these resources. In this paper, we describe the MultiLingual Information Framework (MLIF ― ISO CD 24616). MLIF is a metamodel which allows the representation and the exchange of multilingual textual information. This generic metamodel is designed to provide a common platform for all the tools developed around the existing multilingual data exchange formats. This platform provides, on the one hand, a set of generic data categories for various application domains, and on the other hand, strategies for the interoperability with existing standards. The objective is to reach a better convergence between heterogeneous standardisation activities that are taking place in the domain of data modeling (XML; W3C), text management (TEI; TEIC), multilingual information (TMX-LISA; XLIFF-OASIS) and multimedia (SMILText; W3C). This is a work in progress within ISO-TC37 in order to define a new ISO standard.</abstract>
      <bibkey>cruz-lara-etal-2010-mlif</bibkey>
    </paper>
    <paper id="378">
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Jonas</first><last>Sunde</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <title>Generating <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>ets of Various Granularities: The <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et Transformer</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/552_Paper.pdf</url>
      <abstract>We present a method and a software tool, the FrameNet Transformer, for deriving customized versions of the FrameNet database based on frame and frame element relations. The FrameNet Transformer allows users to iteratively coarsen the FrameNet sense inventory in two ways. First, the tool can merge entire frames that are related by user-specified relations. Second, it can merge word senses that belong to frames related by specified relations. Both methods can be interleaved. The Transformer automatically outputs format-compliant FrameNet versions, including modified corpus annotation files that can be used for automatic processing. The customized FrameNet versions can be used to determine which granularity is suitable for particular applications. In our evaluation of the tool, we show that our method increases accuracy of statistical semantic parsers by reducing the number of word-senses (frames) per lemma, and increasing the number of annotated sentences per lexical unit and frame. We further show in an experiment on the FATE corpus that by coarsening FrameNet we do not incur a significant loss of information that is relevant to the Recognizing Textual Entailment task.</abstract>
      <bibkey>ruppenhofer-etal-2010-generating</bibkey>
    </paper>
    <paper id="379">
      <author><first>Francesca</first><last>Bonin</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <author><first>Giulia</first><last>Venturi</last></author>
      <title>A Contrastive Approach to Multi-word Extraction from Domain-specific Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/553_Paper.pdf</url>
      <abstract>In this paper, we present a novel approach to multi-word terminology extraction combining a well-known automatic term recognition approach, the C--NC value method, with a contrastive ranking technique, aimed at refining obtained results either by filtering noise due to common words or by discerning between semantically different types of terms within heterogeneous terminologies. Differently from other contrastive methods proposed in the literature that focus on single terms to overcome the multi-word terms' sparsity problem, the proposed contrastive function is able to handle variation in low frequency events by directly operating on pre-selected multi-word terms. This methodology has been tested in two case studies carried out in the History of Art and Legal domains. Evaluation of achieved results showed that the proposed two--stage approach improves significantly multi--word term extraction results. In particular, for what concerns the legal domain it provides an answer to a well-known problem in the semi--automatic construction of legal ontologies, namely that of singling out law terms from terms of the specific domain being regulated.</abstract>
      <bibkey>bonin-etal-2010-contrastive</bibkey>
    </paper>
    <paper id="380">
      <author><first>Olivier</first><last>Blanc</last></author>
      <author><first>Matthieu</first><last>Constant</last></author>
      <author><first>Anne</first><last>Dister</last></author>
      <author><first>Patrick</first><last>Watrin</last></author>
      <title>Partial Parsing of Spontaneous Spoken <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/554_Paper.pdf</url>
      <abstract>This paper describes the process and the resources used to automatically annotate a French corpus of spontaneous speech transcriptions in super-chunks. Super-chunks are enhanced chunks that can contain lexical multiword units. This partial parsing is based on a preprocessing stage of the spoken data that consists in reformatting and tagging utterances that break the syntactic structure of the text, such as disfluencies. Spoken specificities were formalized thanks to a systematic linguistic study of a 40-hour-long speech transcription corpus. The chunker uses large-coverage and fine-grained language resources for general written language that have been augmented with resources specific to spoken French. It consists in iteratively applying finite-state lexical and syntactic resources and outputing a finite automaton representing all possible chunk analyses. The best path is then selected thanks to a hybrid disambiguation stage. We show that our system reaches scores that are comparable with state-of-the-art results in the field.</abstract>
      <bibkey>blanc-etal-2010-partial</bibkey>
    </paper>
    <paper id="381">
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Laurence</first><last>Bolot</last></author>
      <author><first>Emilie</first><last>Chételat-Pelé</last></author>
      <author><first>Annick</first><last>Choisier</last></author>
      <author><first>Maxime</first><last>Delorme</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <author><first>Jérémie</first><last>Segouat</last></author>
      <author><first>Cyril</first><last>Verrecchia</last></author>
      <author><first>Flora</first><last>Badin</last></author>
      <author><first>Nadège</first><last>Devos</last></author>
      <title>Sign Language Corpora for Analysis, Processing and Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/555_Paper.pdf</url>
      <abstract>Sign Languages (SLs) are the visuo-gestural languages practised by the deaf communities. Research on SLs requires to build, to analyse and to use corpora. The aim of this paper is to present various kinds of new uses of SL corpora. The way data are used take advantage of the new capabilities of annotation software for visualisation, numerical annotation, and processing. The nature of the data can be video-based or motion capture-based. The aims of the studies include language analysis, animation processing, and evaluation. We describe here some LIMSIs studies, and some studies from other laboratories as examples.</abstract>
      <bibkey>braffort-etal-2010-sign</bibkey>
    </paper>
    <paper id="382">
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Emanuele</first><last>Pianta</last></author>
      <author><first>Rodolfo</first><last>Delmonte</last></author>
      <author><first>Michele</first><last>Brunelli</last></author>
      <title><fixed-case>V</fixed-case>en<fixed-case>P</fixed-case>ro: A Morphological Analyzer for Venetan</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/556_Paper.pdf</url>
      <abstract>This document reports the process of extending MorphoPro for Venetan, a lesser-used language spoken in the Nort-Eastern part of Italy. MorphoPro is the morphological component of TextPro, a suite of tools oriented towards a number of NLP tasks. In order to extend this component to Venetan, we developed a declarative representation of the morphological knowledge necessary to analyze and synthesize Venetan words. This task was challenging for several reasons, which are common to a number of lesser-used languages: although Venetan is widely used as an oral language in everyday life, its written usage is very limited; efforts for defining a standard orthography and grammar are very recent and not well established; despite recent attempts to propose a unified orthography, no Venetan standard is widely used. Besides, there are different geographical varieties and it is strongly influenced by Italian.</abstract>
      <bibkey>tonelli-etal-2010-venpro</bibkey>
    </paper>
    <paper id="383">
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Dave</first><last>Graff</last></author>
      <author><first>Mike</first><last>Ciul</last></author>
      <title>From Speech to Trees: Applying Treebank Annotation to <fixed-case>A</fixed-case>rabic Broadcast News</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/558_Paper.pdf</url>
      <abstract>The Arabic Treebank (ATB) Project at the Linguistic Data Consortium (LDC) has embarked on a large corpus of Broadcast News (BN) transcriptions, and this has led to a number of new challenges for the data processing and annotation procedures that were originally developed for Arabic newswire text (ATB1, ATB2 and ATB3). The corpus requirements currently posed by the DARPA GALE Program, including English translation of Arabic BN transcripts, word-level alignment of Arabic and English data, and creation of a corresponding English Treebank, place significant new constraints on ATB corpus creation, and require careful coordination among a wide assortment of concurrent activities and participants. Nonetheless, in spite of the new challenges posed by BN data, the ATBs newly improved pipeline and revised annotation guidelines for newswire have proven to be robust enough that very few changes were necessary to account for the new genre of data. This paper presents the points where some adaptation has been necessary, and the overall pipeline as used in the production of BN ATB data.</abstract>
      <bibkey>maamouri-etal-2010-speech</bibkey>
    </paper>
    <paper id="384">
      <author><first>Enikő</first><last>Héja</last></author>
      <title>The Role of Parallel Corpora in Bilingual Lexicography</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/559_Paper.pdf</url>
      <abstract>This paper describes an approach based on word alignment on parallel corpora, which aims at facilitating the lexicographic work of dictionary building. Although this method has been widely used in the MT community for at least 16 years, as far as we know, it has not been applied to facilitate the creation of bilingual dictionaries for human use. The proposed corpus-driven technique, in particular the exploitation of parallel corpora, proved to be helpful in the creation of such dictionaries for several reasons. Most importantly, a parallel corpus of appropriate size guarantees that the most relevant translations are included in the dictionary. Moreover, based on the translational probabilities it is possible to rank translation candidates, which ensures that the most frequently used translation variants go first within an entry. A further advantage is that all the relevant example sentences from the parallel corpora are easily accessible, thus facilitating the selection of the most appropriate translations from possible translation candidates. Due to these properties the method is particularly apt to enable the production of active or encoding dictionaries.</abstract>
      <bibkey>heja-2010-role</bibkey>
    </paper>
    <paper id="385">
      <author><first>Harry</first><last>Bunt</last></author>
      <author><first>Jan</first><last>Alexandersson</last></author>
      <author><first>Jean</first><last>Carletta</last></author>
      <author><first>Jae-Woong</first><last>Choe</last></author>
      <author><first>Alex Chengyu</first><last>Fang</last></author>
      <author><first>Koiti</first><last>Hasida</last></author>
      <author><first>Kiyong</first><last>Lee</last></author>
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <author><first>David</first><last>Traum</last></author>
      <title>Towards an <fixed-case>ISO</fixed-case> Standard for Dialogue Act Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/560_Paper.pdf</url>
      <abstract>This paper describes an ISO project which aims at developing a standard for annotating spoken and multimodal dialogue with semantic information concerning the communicative functions of utterances, the kind of semantic content they address, and their relations with what was said and done earlier in the dialogue. The project, ISO 24617-2 ""Semantic annotation framework, Part 2: Dialogue acts"", is currently at DIS stage. The proposed annotation schema distinguishes 9 orthogonal dimensions, allowing each functional segment in dialogue to have a function in each of these dimensions, thus accounting for the multifunctionality that utterances in dialogue often have. A number of core communicative functions is defined in the form of ISO data categories, available at http://semantic-annotation.uvt.nl/dialogue-acts/iso-datcats.pdf; they are divided into ""dimension-specific"" functions, which can be used only in a particular dimension, such as Turn Accept in the Turn Management dimension, and ""general-purpose"" functions, which can be used in any dimension, such as Inform and Request. An XML-based annotation language, ""DiAML"" is defined, with an abstract syntax, a semantics, and a concrete syntax.</abstract>
      <bibkey>bunt-etal-2010-towards</bibkey>
    </paper>
    <paper id="386">
      <author><first>Archna</first><last>Bhatia</last></author>
      <author><first>Rajesh</first><last>Bhatt</last></author>
      <author><first>Bhuvana</first><last>Narasimhan</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <author><first>Michael</first><last>Tepper</last></author>
      <author><first>Ashwini</first><last>Vaidya</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <title>Empty Categories in a <fixed-case>H</fixed-case>indi Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/561_Paper.pdf</url>
      <abstract>We are in the process of creating a multi-representational and multi-layered treebank for Hindi/Urdu (Palmer et al., 2009), which has three main layers: dependency structure, predicate-argument structure (PropBank), and phrase structure. This paper discusses an important issue in treebank design which is often neglected: the use of empty categories (ECs). All three levels of representation make use of ECs. We make a high-level distinction between two types of ECs, trace and silent, on the basis of whether they are postulated to mark displacement or not. Each type is further refined into several subtypes based on the underlying linguistic phenomena which the ECs are introduced to handle. This paper discusses the stages at which we add ECs to the Hindi/Urdu treebank and why. We investigate methodically the different types of ECs and their role in our syntactic and semantic representations. We also examine our decisions whether or not to coindex each type of ECs with other elements in the representation.</abstract>
      <bibkey>bhatia-etal-2010-empty</bibkey>
    </paper>
    <paper id="387">
      <author><first>Marina</first><last>Lloberes</last></author>
      <author><first>Irene</first><last>Castellón</last></author>
      <author><first>Lluís</first><last>Padró</last></author>
      <title><fixed-case>S</fixed-case>panish <fixed-case>F</fixed-case>ree<fixed-case>L</fixed-case>ing Dependency Grammar</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/562_Paper.pdf</url>
      <abstract>This paper presents the development of an open-source Spanish Dependency Grammar implemented in FreeLing environment. This grammar was designed as a resource for NLP applications that require a step further in natural language automatic analysis, as is the case of Spanish-to-Basque translation. The development of wide-coverage rule-based grammars using linguistic knowledge contributes to extend the existing Spanish deep parsers collection, which sometimes is limited. Spanish FreeLing Dependency Grammar, named EsTxala, provides deep and robust parse trees, solving attachments for any structure and assigning syntactic functions to dependencies. These steps are dealt with hand-written rules based on linguistic knowledge. As a result, FreeLing Dependency Parser gives a unique analysis as a dependency tree for each sentence analyzed. Since it is a resource open to the scientific community, exhaustive grammar evaluation is being done to determine its accuracy as well as strategies for its manteinance and improvement. In this paper, we show the results of an experimental evaluation carried out over EsTxala in order to test our evaluation methodology.</abstract>
      <bibkey>lloberes-etal-2010-spanish</bibkey>
    </paper>
    <paper id="388">
      <author><first>Magali Sanches</first><last>Duran</last></author>
      <author><first>Marcelo Adriano</first><last>Amâncio</last></author>
      <author><first>Sandra Maria</first><last>Aluísio</last></author>
      <title>Assigning Wh-Questions to Verbal Arguments: Annotation Tools Evaluation and Corpus Building</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/564_Paper.pdf</url>
      <abstract>This work reports the evaluation and selection of annotation tools to assign wh-question labels to verbal arguments in a sentence. Wh-question assignment discussed herein is a kind of semantic annotation which involves two tasks: making delimitation of verbs and arguments, and linking verbs to its arguments by question labels. As it is a new type of semantic annotation, there is no report about requirements an annotation tool should have to face it. For this reason, we decided to select the most appropriated tool in two phases. In the first phase, we executed the task with an annotation tool we have used before in another task. Such phase helped us to test the task and enabled us to know which features were or not desirable in an annotation tool for our purpose. In the second phase, guided by such requirements, we evaluated several tools and selected a tool for the real task. After corpus annotation conclusion, we report some of the annotation results and some comments on the improvements there should be made in an annotation tool to better support such kind of annotation task.</abstract>
      <bibkey>duran-etal-2010-assigning</bibkey>
    </paper>
    <paper id="389">
      <author><first>Ralph</first><last>Grishman</last></author>
      <title>The Impact of Task and Corpus on Event Extraction Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/565_Paper.pdf</url>
      <abstract>The term event extraction covers a wide range of information extraction tasks, and methods developed and evaluated for one task may prove quite unsuitable for another. Understanding these task differences is essential to making broad progress in event extraction. We look back at the MUC and ACE tasks in terms of one characteristic, the breadth of the scenario ― how wide a range of information is subsumed in a single extraction task. We examine how this affects strategies for collecting information and methods for semi-supervised training of new extractors. We also consider the heterogeneity of corpora ― how varied the topics of documents in a corpus are. Extraction systems may be intended in principle for general news but are typically evaluated on topic-focused corpora, and this evaluation context may affect system design. As one case study, we examine the task of identifying physical attack events in news corpora, observing the effect on system performance of shifting from an attack-event-rich corpus to a more varied corpus and considering how the impact of this shift may be mitigated.</abstract>
      <bibkey>grishman-2010-impact</bibkey>
    </paper>
    <paper id="390">
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <title>Consistent and Flexible Integration of Morphological Annotation in the <fixed-case>A</fixed-case>rabic Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/566_Paper.pdf</url>
      <abstract>Complications arise for standoff annotation when the annotation is not on the source text itself, but on a more abstract representation. This is particularly the case in a language such as Arabic with morphological and orthographic challenges, and we discuss various aspects of these issues in the context of the Arabic Treebank. The Standard Arabic Morphological Analyzer (SAMA) is closely integrated into the annotation workflow, as the basis for the abstraction between the explicit source text and the more abstract token representation. However, this integration with SAMA gives rise to various problems for the annotation workflow and for maintaining the link between the Treebank and SAMA. In this paper we discuss how we have overcome these problems with consistent and more precise categorization of all of the tokens for their relationship with SAMA. We also discuss how we have improved the creation of several distinct alternative forms of the tokens used in the syntactic trees. As a result, the Treebank provides a resource relating the different forms of the same underlying token with varying degrees of vocalization, in terms of how they relate (1) to each other, (2) to the syntactic structure, and (3) to the morphological analyzer.</abstract>
      <bibkey>kulick-etal-2010-consistent</bibkey>
    </paper>
    <paper id="391">
      <author><first>Andrea</first><last>Zaninello</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <title>Creation of Lexical Resources for a Characterisation of Multiword Expressions in <fixed-case>I</fixed-case>talian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/567_Paper.pdf</url>
      <abstract>The theoretical characterisation of multiword expressions (MWEs) is tightly connected to their actual occurrences in data and to their representation in lexical resources. We present three lexical resources for Italian MWEs, namely an electronic lexicon, a series of example corpora and a database of MWEs represented around morphosyntactic patterns. These resources are matched against, and created from, a very large web-derived corpus for Italian that spans across registers and domains. We can thus test expressions coded by lexicographers in a dictionary, thereby discarding unattested expressions, revisiting lexicographers's choices on the basis of frequency information, and at the same time creating an example sub-corpus for each entry. We organise MWEs on the basis of the morphosyntactic information obtained from the data in an electronic, flexible knowledge-base containing structured annotation exploitable for multiple purposes. We also suggest further work directions towards characterising MWEs by analysing the data organised in our database through lexico-semantic information available in WordNet or MultiWordNet-like resources, also in the perspective of expanding their set through the extraction of other similar compact expressions.</abstract>
      <bibkey>zaninello-nissim-2010-creation</bibkey>
    </paper>
    <paper id="392">
      <author><first>Jana</first><last>Šindlerová</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <title>Building a Bilingual <fixed-case>V</fixed-case>al<fixed-case>L</fixed-case>ex Using Treebank Token Alignment: First Observations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/568_Paper.pdf</url>
      <abstract>We explore the potential and limitations of a concept of building a bilingual valency lexicon based on the alignment of nodes in a parallel treebank. Our aim is to build an electronic Czech-&gt;English Valency Lexicon by collecting equivalences from bilingual treebank data and storing them in two already existing electronic valency lexicons, PDT-VALLEX and Engvallex. For this task a special annotation interface has been built upon the TrEd editor, allowing quick and easy collecting of frame equivalences in either of the source lexicons. The issues encountered so far include limitations of technical character, theory-dependent limitations and limitations concerning the achievable degree of quality of human annotation. The issues of special interest for both linguists and MT specialists involved in the project include linguistically motivated non-balance between the frame equivalents, either in number or in type of valency participants. The first phases of annotation so far attest the assumption that there is a unique correspondence between the functors of the translation-equivalent frames. Also, hardly any linguistically significant non-balance between the frames has been found, which is partly promising considering the linguistic theory used and partly caused by little stylistic variety of the annotated corpus texts.</abstract>
      <bibkey>sindlerova-bojar-2010-building</bibkey>
    </paper>
    <paper id="393">
      <author><first>Alberto</first><last>Díaz</last></author>
      <author><first>Pablo</first><last>Gervás</last></author>
      <author><first>Antonio</first><last>García</last></author>
      <author><first>Laura</first><last>Plaza</last></author>
      <title>Development and Use of an Evaluation Collection for Personalisation of Digital Newspapers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/569_Paper.pdf</url>
      <abstract>This paper presents the process of development and the characteristics of an evaluation collection for a personalisation system for digital newspapers. This system selects, adapts and presents contents according to a user model that define information needs. The collection presented here contains data that are cross-related over four different axes: a set of news items from an electronic newspaper, collected into subsets corresponding to a particular sequence of days, packaged together and cross-indexed with a set of user profiles that represent the particular evolution of interests of a set of real users over the given days, expressed in each case according to four different representation frameworks: newspaper sections, Yahoo categories, keywords, and relevance feedback over the set of news items for the previous day. This information provides a minimum starting material over which one can evaluate for a given system how it addresses the first two observations - adapting to different users and adapting to particular users over time - providing that the particular system implements the representation of information needs according to the four frameworks employed in the collection. This collection has been successfully used to perform some different experiments to determine the effectiveness of the personalization system presented.</abstract>
      <bibkey>diaz-etal-2010-development</bibkey>
    </paper>
    <paper id="394">
      <author><first>Jonathan H.</first><last>Clark</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <title><fixed-case>L</fixed-case>oony<fixed-case>B</fixed-case>in: Keeping Language Technologists Sane through Automated Management of Experimental (Hyper)Workflows</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/570_Paper.pdf</url>
      <abstract>Many contemporary language technology systems are characterized by long pipelines of tools with complex dependencies. Too often, these workflows are implemented by ad hoc scripts; or, worse, tools are run manually, making experiments difficult to reproduce. These practices are difficult to maintain in the face of rapidly evolving workflows while they also fail to expose and record important details about intermediate data. Further complicating these systems are hyperparameters, which often cannot be directly optimized by conventional methods, requiring users to determine which combination of values is best via trial and error. We describe LoonyBin, an open-source tool that addresses these issues by providing: 1) a visual interface for the user to create and modify workflows; 2) a well-defined mechanism for tracking metadata and provenance; 3) a script generator that compiles visual workflows into shell scripts; and 4) a new workflow representation we call a HyperWorkflow, which intuitively and succinctly encodes small experimental variations within a larger workflow.</abstract>
      <bibkey>clark-lavie-2010-loonybin</bibkey>
    </paper>
    <paper id="395">
      <author><first>Keith J.</first><last>Miller</last></author>
      <author><first>Sarah</first><last>McLeod</last></author>
      <author><first>Elizabeth</first><last>Schroeder</last></author>
      <author><first>Mark</first><last>Arehart</last></author>
      <author><first>Kenneth</first><last>Samuel</last></author>
      <author><first>James</first><last>Finley</last></author>
      <author><first>Vanesa</first><last>Jurica</last></author>
      <author><first>John</first><last>Polk</last></author>
      <title>Improving Personal Name Search in the <fixed-case>TIGR</fixed-case> System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/571_Paper.pdf</url>
      <abstract>This paper describes the development and evaluation of enhancements to the specialized information retrieval capabilities of a multimodal reporting system. The system enables collection and dissemination of information through a distributed data architecture by allowing users to input free text documents, which are indexed for subsequent search and retrieval by other users. This unstructured data entry method is essential for users of this system, but it requires an intelligent support system for processing queries against the data. The system, known as TIGR (Tactical Ground Reporting), allows keyword searching and geospatial filtering of results, but lacked the ability to efficiently index and search person names and perform approximate name matching. To improve TIGRs ability to provide accurate, comprehensive results for queries on person names we iteratively updated existing entity extraction and name matching technologies to better align with the TIGR use case. We evaluated each version of the entity extraction and name matching components to find the optimal configuration for the TIGR context, and combined those pieces into a named entity extraction, indexing, and search module that integrates with the current TIGR system. By comparing system-level evaluations of the original and updated TIGR search processes, we show that our enhancements to personal name search significantly improved the performance of the overall information retrieval capabilities of the TIGR system.</abstract>
      <bibkey>miller-etal-2010-improving</bibkey>
    </paper>
    <paper id="396">
      <author><first>Kornel</first><last>Laskowski</last></author>
      <author><first>Jens</first><last>Edlund</last></author>
      <title>A Snack Implementation and Tcl/Tk Interface to the Fundamental Frequency Variation Spectrum Algorithm</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/576_Paper.pdf</url>
      <abstract>Intonation is an important aspect of vocal production, used for a variety of communicative needs. Its modeling is therefore crucial in many speech understanding systems, particularly those requiring inference of speaker intent in real-time. However, the estimation of pitch, traditionally the first step in intonation modeling, is computationally inconvenient in such scenarios. This is because it is often, and most optimally, achieved only after speech segmentation and recognition. A consequence is that earlier speech processing components, in todays state-of-the-art systems, lack intonation awareness by fiat; it is not known to what extent this circumscribes their performance. In the current work, we present a freely available implementation of an alternative to pitch estimation, namely the computation of the fundamental frequency variation (FFV) spectrum, which can be easily employed at any level within a speech processing system. It is our hope that the implementation we describe aid in the understanding of this novel acoustic feature space, and that it facilitate its inclusion, as desired, in the front-end routines of speech recognition, dialog act recognition, and speaker recognition systems.</abstract>
      <bibkey>laskowski-edlund-2010-snack</bibkey>
    </paper>
    <paper id="397">
      <author><first>Jette</first><last>Viethen</last></author>
      <author><first>Simon</first><last>Zwarts</last></author>
      <author><first>Robert</first><last>Dale</last></author>
      <author><first>Markus</first><last>Guhe</last></author>
      <title>Dialogue Reference in a Visual Domain</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/578_Paper.pdf</url>
      <abstract>A central purpose of referring expressions is to distinguish intended referents from other entities that are in the context; but how is this context determined? This paper draws a distinction between discourse context ―other entities that have been mentioned in the dialogue― and visual context ―visually available objects near the intended referent. It explores how these two different aspects of context have an impact on subsequent reference in a dialogic situation where the speakers share both discourse and visual context. In addition we take into account the impact of the reference history ―forms of reference used previously in the discourse― on forming what have been called conceptual pacts. By comparing the output of different parameter settings in our model to a data set of human-produced referring expressions, we determine that an approach to subsequent reference based on conceptual pacts provides a better explanation of our data than previously proposed algorithmic approaches which compute a new distinguishing description for the intended referent every time it is mentioned.</abstract>
      <bibkey>viethen-etal-2010-dialogue</bibkey>
    </paper>
    <paper id="398">
      <author><first>Sunao</first><last>Hara</last></author>
      <author><first>Norihide</first><last>Kitaoka</last></author>
      <author><first>Kazuya</first><last>Takeda</last></author>
      <title>Estimation Method of User Satisfaction Using N-gram-based Dialog History Model for Spoken Dialog System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/579_Paper.pdf</url>
      <abstract>In this paper, we propose an estimation method of user satisfaction for a spoken dialog system using an N-gram-based dialog history model. We have collected a large amount of spoken dialog data accompanied by usability evaluation scores by users in real environments. The database is made by a field-test in which naive users used a client-server music retrieval system with a spoken dialog interface on their own PCs. An N-gram model is trained from the sequences that consist of users' dialog acts and/or the system's dialog acts for each one of six user satisfaction levels: from 1 to 5 and φ (task not completed). Then, the satisfaction level is estimated based on the N-gram likelihood. Experiments were conducted on the large real data and the results show that our proposed method achieved good classification performance; the classification accuracy was 94.7% in the experiment on a classification into dialogs with task completion and those without task completion. Even if the classifier detected all of the task incomplete dialog correctly, our proposed method achieved the false detection rate of only 6%.</abstract>
      <bibkey>hara-etal-2010-estimation</bibkey>
    </paper>
    <paper id="399">
      <author><first>Peng-Wen</first><last>Chen</last></author>
      <author><first>Snehal Kumar</first><last>Chennuru</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <title>A Language Approach to Modeling Human Behaviors</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/580_Paper.pdf</url>
      <abstract>The modeling of human behavior becomes more and more important due to the increasing popularity of context-aware computing and people-centric mobile applications. Inspired by the principle of action-as-language, we propose that human ambulatory behavior shares similar properties as natural languages. In addition, by exploiting this similarity, we will be able to index, recognize, cluster, retrieve, and infer high-level semantic meanings of human behaviors via the use of natural language processing techniques. In this paper, we developed a Life Logger system to help build the behavior language corpus which supports our ""Behavior as Language"" research. The constructed behavior corpus shows Zipf's distribution over the frequency of vocabularies which is aligned with our ""Behavior as Language"" assumption. Our preliminary results of using smoothed n-gram language model for activity recognition achieved an average accuracy rate of 94% in distinguishing among human ambulatory behaviors including walking, running, and cycling. This behavior-as-language corpus will enable researchers to study higher level human behavior based on the syntactic and semantic analysis of the corpus data.</abstract>
      <bibkey>chen-etal-2010-language</bibkey>
    </paper>
    <paper id="400">
      <author><first>Masaki</first><last>Murata</last></author>
      <author><first>Tomohiro</first><last>Ohno</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <author><first>Yasuyoshi</first><last>Inagaki</last></author>
      <title>Construction of Chunk-Aligned Bilingual Lecture Corpus for Simultaneous Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/581_Paper.pdf</url>
      <abstract>With the development of speech and language processing, speech translation systems have been developed. These studies target spoken dialogues, and employ consecutive interpretation, which uses a sentence as the translation unit. On the other hand, there exist a few researches about simultaneous interpreting, and recently, the language resources for promoting simultaneous interpreting research, such as the publication of an analytical large-scale corpus, has been prepared. For the future, it is necessary to make the corpora more practical toward realization of a simultaneous interpreting system. In this paper, we describe the construction of a bilingual corpus which can be used for simultaneous lecture interpreting research. Simultaneous lecture interpreting systems are required to recognize translation units in the middle of a sentence, and generate its translation at the proper timing. We constructed the bilingual lecture corpus by the following steps. First, we segmented sentences in the lecture data into semantically meaningful units for the simultaneous interpreting. And then, we assigned the translations to these units from the viewpoint of the simultaneous interpreting. In addition, we investigated the possibility of automatically detecting the simultaneous interpreting timing from our corpus.</abstract>
      <bibkey>murata-etal-2010-construction</bibkey>
    </paper>
    <paper id="401">
      <author><first>Stergos</first><last>Afantenos</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Laurence</first><last>Danlos</last></author>
      <title>Learning Recursive Segments for Discourse Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/582_Paper.pdf</url>
      <abstract>Automatically detecting discourse segments is an important preliminary step towards full discourse parsing. Previous research on discourse segmentation have relied on the assumption that elementary discourse units (EDUs) in a document always form a linear sequence (i.e., they can never be nested). Unfortunately, this assumption turns out to be too strong, for some theories of discourse, like the ""Segmented Discourse Representation Theory"" or SDRT, allow for nested discourse units. In this paper, we present a simple approach to discourse segmentation that is able to produce nested EDUs. Our approach builds on standard multi-class classification techniques making use of a regularized maximum entropy model, combined with a simple repairing heuristic that enforces global coherence. Our system was developed and evaluated on the first round of annotations provided by the French Annodis project (an ongoing effort to create a discourse bank for French). Cross-validated on only 47 documents (1,445 EDUs), our system achieves encouraging performance results with an F-score of 73% for finding EDUs.</abstract>
      <bibkey>afantenos-etal-2010-learning</bibkey>
    </paper>
    <paper id="402">
      <author><first>Shu</first><last>Zhang</last></author>
      <author><first>Wenjie</first><last>Jia</last></author>
      <author><first>Yingju</first><last>Xia</last></author>
      <author><first>Yao</first><last>Meng</last></author>
      <author><first>Hao</first><last>Yu</last></author>
      <title>Extracting Product Features and Sentiments from <fixed-case>C</fixed-case>hinese Customer Reviews</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/583_Paper.pdf</url>
      <abstract>With the growing interest in opinion mining from web data, more works are focused on mining in English and Chinese reviews. Probing into the problem of product opinion mining, this paper describes the details of our language resources, and imports them into the task of extracting product feature and sentiment task. Different from the traditional unsupervised methods, a supervised method is utilized to identify product features, combining the domain knowledge and lexical information. Nearest vicinity match and syntactic tree based methods are proposed to identify the opinions regarding the product features. Multi-level analysis module is proposed to determine the sentiment orientation of the opinions. With the experiments on the electronic reviews of COAE 2008, the validities of the product features identified by CRFs and the two opinion words identified methods are testified and compared. The results show the resource is well utilized in this task and our proposed method is valid.</abstract>
      <bibkey>zhang-etal-2010-extracting</bibkey>
    </paper>
    <paper id="403">
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <title>Open Soucre Graph Transducer Interpreter and Grammar Development Environment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/585_Paper.pdf</url>
      <abstract>Graph and tree transducers have been applied in many NLP areas―among them, machine translation, summarization, parsing, and text generation. In particular, the successful use of tree rewriting transducers for the introduction of syntactic structures in statistical machine translation contributed to their popularity. However, the potential of such transducers is limited because they do not handle graphs and because they consume the source structure in that they rewrite it instead of leaving it intact for intermediate consultations. In this paper, we describe an open source tree and graph transducer interpreter, which combines the advantages of graph transducers and two-tape Finite State Transducers and surpasses the limitations of state-of-the-art tree rewriting transducers. Along with the transducer, we present a graph grammar development environment that supports the compilation and maintenance of graph transducer grammatical and lexical resources. Such an environment is indispensable for any effort to create consistent large coverage NLP-resources by human experts.</abstract>
      <bibkey>bohnet-wanner-2010-open</bibkey>
    </paper>
    <paper id="404">
      <author><first>Min-Jae</first><last>Kwon</last></author>
      <author><first>Hae-Yun</first><last>Lee</last></author>
      <author><first>Hee-Rahk</first><last>Chae</last></author>
      <title>Linking <fixed-case>K</fixed-case>orean Words with an Ontology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/586_Paper.pdf</url>
      <abstract>The need for ontologies has increased in computer science or information science recently. Especially, NLP systems such as information retrieval, machine translation, etc. require ontologies whose concepts are connected to natural language words. There are a few Korean wordnets such as U-WIN, KorLex, CoreNet, etc. Most of them, however, stand alone without any link to an ontology. Hence, we need a Korean wordnet which is linked to a language-neutral ontology such as SUMO, OpenCyc, DOLCE, etc. In this paper, we will present a method of linking Korean word senses with the concepts of an ontology, which is part of an ongoing project. We use a Korean-English bilingual dictionary, Princeton WordNet (Fellbaum 1998), and the ontology SmartSUMO (Oberle et al. 2007). The current version of WordNet is mapped into SUMO, which constitutes a major part of SmartSUMO. We focus on mapping Korean word senses with corresponding English word senses by way of Princeton WordNet which is mapped into SUMO. This paper will show that we need to apply different algorithms of linking, depending on the information types that a bilingual dictionary contains.</abstract>
      <bibkey>kwon-etal-2010-linking</bibkey>
    </paper>
    <paper id="405">
      <author><first>Sabine</first><last>Ploux</last></author>
      <author><first>Armelle</first><last>Boussidan</last></author>
      <author><first>Hyungsuk</first><last>Ji</last></author>
      <title>The Semantic Atlas: an Interactive Model of Lexical Representation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/592_Paper.pdf</url>
      <abstract>In this paper we describe two geometrical models of meaning representation, the Semantic Atlas (SA) and the Automatic Contexonym Organizing Model (ACOM). The SA provides maps of meaning generated through correspondence factor analysis. The models can handle different types of word relations: synonymy in the SA and co-occurrence in ACOM. Their originality relies on an artifact called 'cliques' - a fine grained infra linguistic sub-unit of meaning. The SA is composed of several dictionaries and thesauri enhanced with a process of symmetrisation. It is currently available for French and English in monolingual versions as well as in a bilingual translation version. Other languages are under development and testing. ACOM deals with unannotated corpora. The models are used by research teams worldwide that investigate synonymy, translation processes, genre comparison, psycholinguistics and polysemy modeling. Both models can be consulted online via a flexible interface allowing for interactive navigation on http://dico.isc.cnrs.fr. This site is the most consulted address of the French National Center for Scientific Researchs domain (CNRS), one of the major research bodies in France. The international interest it has triggered led us to initiate the process of going open source. In the meantime, all our databases are freely available on request.</abstract>
      <bibkey>ploux-etal-2010-semantic</bibkey>
    </paper>
    <paper id="406">
      <author><first>Roberto P. A.</first><last>Araujo</last></author>
      <author><first>Rafael L.</first><last>de Oliveira</last></author>
      <author><first>Eder M.</first><last>de Novais</last></author>
      <author><first>Thiago D.</first><last>Tadeu</last></author>
      <author><first>Daniel B.</first><last>Pereira</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <title><fixed-case>SIN</fixed-case>otas: the Evaluation of a <fixed-case>NLG</fixed-case> Application</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/593_Paper.pdf</url>
      <abstract>SINotas is a data-to-text NLG application intended to produce short textual reports on students academic performance from a database conveying their grades, weekly attendance rates and related academic information. Although developed primarily as a testbed for Portuguese Natural Language Generation, SINotas generates reports of interest to both students keen to learn how their professors would describe their efforts, and to the professors themselves, who may benefit from an at-a-glance view of the students performance. In a traditional machine learning approach, SINotas uses a data-text aligned corpus as training data for decision-tree induction. The current system comprises a series of classifiers that implement major Document Planning subtasks (namely, data interpretation, content selection, within- and between-sentence structuring), and a small surface realisation grammar of Brazilian Portuguese. In this paper we focus on the evaluation work of the system, applying a number of intrinsic and user-based evaluation metrics to a collection of text reports generated from real application data.</abstract>
      <bibkey>araujo-etal-2010-sinotas</bibkey>
    </paper>
    <paper id="407">
      <author><first>Isabella</first><last>Poggi</last></author>
      <author><first>Francesca</first><last>D’Errico</last></author>
      <author><first>Laura</first><last>Vincze</last></author>
      <title>Types of Nods. The Polysemy of a Social Signal</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/596_Paper.pdf</url>
      <abstract>The work analyses the head nod, a down-up movement of the head, as a polysemic social signal, that is, a signal with a number of different meanings which all share some common semantic element. Based on the analysis of 100 nods drawn from the SSPNet corpus of TV political debates, a typology of nods is presented that distinguishes Speakers, Interlocutors and Third Listeners nods, with their subtypes (confirmation, agreement, approval, submission and permission, greeting and thanks, backchannel giving and backchannel request, emphasis, ironic agreement, literal and rhetoric question, and others). For each nod the analysis specifies: 1. characteristic features of how it is produced, among which main direction, amplitude, velocity and number of repetitions; 2. cues in other modalities, like direction and duration of gaze; 3. conversational context in which the nod typically occurs. For the Interlocutors or Third Listeners nod, the preceding speech act is relevant: yes/no answer or information for a nod of confirmation, expression of opinion for one of agreement, prosocial action for greetings and thanks; for the Speakers nods, instead, their meanings are mainly distinguished by accompanying signals.</abstract>
      <bibkey>poggi-etal-2010-types</bibkey>
    </paper>
    <paper id="408">
      <author><first>Toomas</first><last>Altosaar</last></author>
      <author><first>Louis</first><last>ten Bosch</last></author>
      <author><first>Guillaume</first><last>Aimetti</last></author>
      <author><first>Christos</first><last>Koniaris</last></author>
      <author><first>Kris</first><last>Demuynck</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <title>A Speech Corpus for Modeling Language Acquisition: <fixed-case>CAREGIVER</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/597_Paper.pdf</url>
      <abstract>A multi-lingual speech corpus used for modeling language acquisition called CAREGIVER has been designed and recorded within the framework of the EU funded Acquisition of Communication and Recognition Skills (ACORNS) project. The paper describes the motivation behind the corpus and its design by relying on current knowledge regarding infant language acquisition. Instead of recording infants and children, the voices of their primary and secondary caregivers were captured in both infant-directed and adult-directed speech modes over four languages in a read speech manner. The challenges and methods applied to obtain similar prompts in terms of complexity and semantics across different languages, as well as the normalized recording procedures employed at different locations, is covered. The corpus contains nearly 66000 utterance based audio files spoken over a two-year period by 17 male and 17 female native speakers of Dutch, English, Finnish, and Swedish. An orthographical transcription is available for every utterance. Also, time-aligned word and phone annotations for many of the sub-corpora also exist. The CAREGIVER corpus will be published via ELRA.</abstract>
      <bibkey>altosaar-etal-2010-speech</bibkey>
    </paper>
    <paper id="409">
      <author><first>Sanja</first><last>Seljan</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Jan</first><last>Šnajder</last></author>
      <author><first>Bojana Dalbelo</first><last>Bašić</last></author>
      <author><first>Vjekoslav</first><last>Osmann</last></author>
      <title>Corpus Aligner (<fixed-case>C</fixed-case>or<fixed-case>A</fixed-case>l) Evaluation on <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>roatian Parallel Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/599_Paper.pdf</url>
      <abstract>An increasing demand for new language resources of recent EU members and accessing countries has in turn initiated the development of different language tools and resources, such as alignment tools and corresponding translation memories for new languages pairs. The primary goal of this paper is to provide a description of a free sentence alignment tool CorAl (Corpus Aligner), developed at the Faculty of Electrical Engineering and Computing, University of Zagreb. The tool performs paragraph alignment at the first step of the alignment process, which is followed by sentence alignment. Description of the tool is followed by its evaluation. The paper describes an experiment with applying the CorAl aligner to a English-Croatian parallel corpus of legislative domain using metrics of precision, recall and F1-measure. Results are discussed and the concluding sections discuss future directions of CorAl development.</abstract>
      <bibkey>seljan-etal-2010-corpus</bibkey>
    </paper>
    <paper id="410">
      <author><first>Siim</first><last>Orasmaa</last></author>
      <author><first>Reina</first><last>Käärik</last></author>
      <author><first>Jaak</first><last>Vilo</last></author>
      <author><first>Tiit</first><last>Hennoste</last></author>
      <title>Information Retrieval of Word Form Variants in Spoken Language Corpora Using Generalized Edit Distance</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/600_Paper.pdf</url>
      <abstract>An important feature of spoken language corpora is existence of different spelling variants of words in transcription. So there is an important problem for linguist who works with large spoken corpora: how to find all variants of the word without annotating them manually? Our work describes a search engine that enables finding different spelling variants (true positives) from corpus of spoken language, and reduces efficiently the amount of false positives returned during the search. Our search engine uses a generalized variant of the edit distance algorithm that allows defining text-specific string to string transformations in addition to the default edit operations defined in edit distance. We have extended our algorithm with capability to block transformations in specific substrings of search words. User can mark certain regions (blocked regions) of the search word where edit operations are not allowed. Our material comes from the Corpus of Spoken Estonian of the University of Tartu which consists of about 2000 dialogues and texts, about 1.4 million running text units in total.</abstract>
      <bibkey>orasmaa-etal-2010-information</bibkey>
    </paper>
    <paper id="411">
      <author><first>Montserrat</first><last>Marimon</last></author>
      <title>The <fixed-case>S</fixed-case>panish Resource Grammar</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/602_Paper.pdf</url>
      <abstract>This paper describes the Spanish Resource Grammar, an open-source multi-purpose broad-coverage precise grammar for Spanish. The grammar is implemented on the Linguistic Knowledge Builder (LKB) system, it is grounded in the theoretical framework of Head-driven Phrase Structure Grammar (HPSG), and it uses Minimal Recursion Semantics (MRS) for the semantic representation. We have developed a hybrid architecture which integrates shallow processing functionalities -- morphological analysis, and Named Entity recognition and classification -- into the parsing process. The SRG has a full coverage lexicon of closed word classes and it contains 50,852 lexical entries for open word classes. The grammar also has 64 lexical rules to perform valence changing operations on lexical items, and 191 phrase structure rules that combine words and phrases into larger constituents and compositionally build up their semantic representation. The annotation of each parsed sentence in an LKB grammar simultaneously represents a traditional phrase structure tree, and a MRS semantic representation. We provide evaluation results on sentences from newspaper texts and discuss future work.</abstract>
      <bibkey>marimon-2010-spanish</bibkey>
    </paper>
    <paper id="412">
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Eric</first><last>Villemonte de la Clergerie</last></author>
      <author><first>Gil</first><last>Francopoulo</last></author>
      <author><first>Marie-Laure</first><last>Guénot</last></author>
      <title><fixed-case>PASSAGE</fixed-case> Syntactic Representation: a Minimal Common Ground for Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/603_Paper.pdf</url>
      <abstract>The current PASSAGE syntactic representation is the result of 9 years of constant evolution with the aim of providing a common ground for evaluating parsers of French whatever their type and supporting theory. In this paper we present the latest developments concerning the formalism and show first through a review of basic linguistic phenomena that it is a plausible minimal common ground for representing French syntax in the context of generic black box quantitative objective evaluation. For the phenomena reviewed, which include: the notion of syntactic head, apposition, control and coordination, we explain how PASSAGE representation relates to other syntactic representation schemes for French and English, slightly extending the annotation to address English when needed. Second, we describe the XML format chosen for PASSAGE and show that it is compliant with the latest propositions in terms of linguistic annotation standard. We conclude discussing the influence that corpus-based evaluation has on the characteristics of syntactic representation when willing to assess the performance of any kind of parser.</abstract>
      <bibkey>vilnat-etal-2010-passage</bibkey>
    </paper>
    <paper id="413">
      <author><first>Mark</first><last>Fishel</last></author>
      <author><first>Harri</first><last>Kirik</last></author>
      <title>Linguistically Motivated Unsupervised Segmentation for Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/604_Paper.pdf</url>
      <abstract>In this paper we use statistical machine translation and morphology information from two different morphological analyzers to try to improve translation quality by linguistically motivated segmentation. The morphological analyzers we use are the unsupervised Morfessor morpheme segmentation and analyzer toolkit and the rule-based morphological analyzer T3. Our translations are done using the Moses statistical machine translation toolkit with training on the JRC-Acquis corpora and translating on Estonian to English and English to Estonian language directions. In our work we model such linguistic phenomena as word lemmas and endings and splitting compound words into simpler parts. Also lemma information was used to introduce new factors to the corpora and to use this information for better word alignment or for alternative path back-off translation. From the results we find that even though these methods have shown previously and keep showing promise of improved translation, their success still largely depends on the corpora and language pairs used.</abstract>
      <bibkey>fishel-kirik-2010-linguistically</bibkey>
    </paper>
    <paper id="414">
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Oliviero</first><last>Stock</last></author>
      <title>Predicting Persuasiveness in Political Discourses</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/607_Paper.pdf</url>
      <abstract>In political speeches, the audience tends to react or resonate to signals of persuasive communication, including an expected theme, a name or an expression. Automatically predicting the impact of such discourses is a challenging task. In fact nowadays, with the huge amount of textual material that flows on the Web (news, discourses, blogs, etc.), it can be useful to have a measure for testing the persuasiveness of what we retrieve or possibly of what we want to publish on Web. In this paper we exploit a corpus of political discourses collected from various Web sources, tagged with audience reactions, such as applause, as indicators of persuasive expressions. In particular, we use this data set in a machine learning framework to explore the possibility of classifying the transcript of political discourses, according to their persuasive power, predicting the sentences that possibly trigger applause. We also explore differences between Democratic and Republican speeches, experiment the resulting classifiers in grading some of the discourses in the Obama-McCain presidential campaign available on the Web.</abstract>
      <bibkey>strapparava-etal-2010-predicting</bibkey>
    </paper>
    <paper id="415">
      <author><first>Didier</first><last>Cadic</last></author>
      <author><first>Cédric</first><last>Boidin</last></author>
      <author><first>Christophe</first><last>d’Alessandro</last></author>
      <title>Towards Optimal <fixed-case>TTS</fixed-case> Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/608_Paper.pdf</url>
      <abstract>Unit selection text-to-speech systems currently produce very natural synthesized phrases by concatenating speech segments from a large database. Recently, increasing demand for designing high quality voices with less data has created need for further optimization of the textual corpus recorded by the speaker. This corpus is traditionally the result of a condensation process: sentences are selected from a reference corpus, using an optimization algorithm (generally greedy) guided by the coverage rate of classic units (diphones, triphones, wordsâ¦). Such an approach is, however, strongly constrained by the finite content of the reference corpus, providing limited language possibilities. To gain flexibility in the optimization process, in this paper, we introduce a new corpus building procedure based on sentence construction rather than sentence selection. Sentences are generated using Finite State Transducers, assisted by a human operator and guided by a new frequency-weighted coverage criterion based on Vocalic Sandwiches. This semi-automatic process requires time-consuming human intervention but seems to give access to much denser corpora, with a density increase of 30 to 40% for a given coverage rate.</abstract>
      <bibkey>cadic-etal-2010-towards</bibkey>
    </paper>
    <paper id="416">
      <author><first>Claudia</first><last>Borg</last></author>
      <author><first>Mike</first><last>Rosner</last></author>
      <author><first>Gordon J.</first><last>Pace</last></author>
      <title>Automatic Grammar Rule Extraction and Ranking for Definitions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/609_Paper.pdf</url>
      <abstract>Plain text corpora contain much information which can only be accessed through human annotation and semantic analysis, which is typically very time consuming to perform. Analysis of such texts at a syntactic or grammatical structure level can however extract some of this information in an automated manner, even if identifying effective rules can be extremely difficult. One such type of implicit information present in texts is that of definitional phrases and sentences. In this paper, we investigate the use of evolutionary algorithms to learn classifiers to discriminate between definitional and non-definitional sentences in non-technical texts, and show how effective grammar-based definition discriminators can be automatically learnt with minor human intervention.</abstract>
      <bibkey>borg-etal-2010-automatic</bibkey>
    </paper>
    <paper id="417">
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Nikos</first><last>Tsourakis</last></author>
      <author><first>Johanna</first><last>Gerlach</last></author>
      <author><first>Maria</first><last>Georgescul</last></author>
      <author><first>Yukie</first><last>Nakao</last></author>
      <author><first>Claudia</first><last>Baur</last></author>
      <title>A Multilingual <fixed-case>CALL</fixed-case> Game Based on Speech Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/610_Paper.pdf</url>
      <abstract>We describe a multilingual Open Source CALL game, CALL-SLT, which reuses speech translation technology developed using the Regulus platform to create an automatic conversation partner that allows intermediate-level language students to improve their fluency. We contrast CALL-SLT with Wang's and Seneff's ``translation game'' system, in particular focussing on three issues. First, we argue that the grammar-based recognition architecture offered by Regulus is more suitable for this type of application; second, that it is preferable to prompt the student in a language-neutral form, rather than in the L1; and third, that we can profitably record successful interactions by native speakers and store them to be reused as online help for students. The current system, which will be demoed at the conference, supports four L2s (English, French, Japanese and Swedish) and two L1s (English and French). We conclude by describing an evaluation exercise, where a version of CALL-SLT configured for English L2 and French L1 was used by several hundred high school students. About half of the subjects reported positive impressions of the system.</abstract>
      <bibkey>rayner-etal-2010-multilingual</bibkey>
    </paper>
    <paper id="418">
      <author><first>Peter</first><last>Adolphs</last></author>
      <author><first>Xiwen</first><last>Cheng</last></author>
      <author><first>Tina</first><last>Klüwer</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <title>Question Answering Biographic Information and Social Network Powered by the Semantic Web</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/611_Paper.pdf</url>
      <abstract>After several years of development, the vision of the Semantic Web is gradually becoming reality. Large data repositories have been created and offer semantic information in a machine-processable form for various domains. Semantic Web data can be published on the Web, gathered automatically, and reasoned about. All these developments open interesting perspectives for building a new class of domain-specific, broad-coverage information systems that overcome a long-standing bottleneck of AI systems, the notoriously incomplete knowledge base. We present a system that shows how the wealth of information in the Semantic Web can be interfaced with humans once again, using natural language for querying and answering rather than technical formalisms. Whereas current Question Answering systems typically select snippets from Web documents retrieved by a search engine, we utilize Semantic Web data, which allows us to provide natural-language answers that are tailored to the current dialog context. Furthermore, we show how to use natural language processing technologies to acquire new data and enrich existing data in a Semantic Web framework. Our system has acquired a rich biographic data resource by combining existing Semantic Web resources, which are discovered from semi-structured textual data in Web pages, with information extracted from free natural language texts.</abstract>
      <bibkey>adolphs-etal-2010-question</bibkey>
    </paper>
    <paper id="419">
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <title>Metaphor Corpus Annotated for Source - Target Domain Mappings</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/612_Paper.pdf</url>
      <abstract>Besides making our thoughts more vivid and filling our communication with richer imagery, metaphor also plays an important structural role in our cognition. Although there is a consensus in the linguistics and NLP research communities that the phenomenon of metaphor is not restricted to similarity-based extensions of meanings of isolated words, but rather involves reconceptualization of a whole area of experience (target domain) in terms of another (source domain), there still has been no proposal for a comprehensive procedure for annotation of cross-domain mappings. However, a corpus annotated for conceptual mappings could provide a new starting point for both linguistic and cognitive experiments. The annotation scheme we present in this paper is a step towards filling this gap. We test our procedure in an experimental setting involving multiple annotators and estimate their agreement on the task. The associated corpus annotated for source ― target domain mappings will be publicly available.</abstract>
      <bibkey>shutova-teufel-2010-metaphor</bibkey>
    </paper>
    <paper id="420">
      <author><first>Federico</first><last>Sangati</last></author>
      <author><first>Willem</first><last>Zuidema</last></author>
      <author><first>Rens</first><last>Bod</last></author>
      <title>Efficiently Extract Rrecurring Tree Fragments from Large Treebanks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/613_Paper.pdf</url>
      <abstract>In this paper we describe FragmentSeeker, a tool which is capable to identify all those tree constructions which are recurring multiple times in a large Phrase Structure treebank. The tool is based on an efficient kernel-based dynamic algorithm, which compares every pair of trees of a given treebank and computes the list of fragments which they both share. We describe two different notions of fragments we will use, i.e. standard and partial fragments, and provide the implementation details on how to extract them from a syntactically annotated corpus. We have tested our system on the Penn Wall Street Journal treebank for which we present quantitative and qualitative analysis on the obtained recurring structures, as well as provide empirical time performance. Finally we propose possible ways our tool could contribute to different research fields related to corpus analysis and processing, such as parsing, corpus statistics, annotation guidance, and automatic detection of argument structure.</abstract>
      <bibkey>sangati-etal-2010-efficiently</bibkey>
    </paper>
    <paper id="421">
      <author><first>Alexandros</first><last>Lazaridis</last></author>
      <author><first>Theodoros</first><last>Kostoulas</last></author>
      <author><first>Todor</first><last>Ganchev</last></author>
      <author><first>Iosif</first><last>Mporas</last></author>
      <author><first>Nikos</first><last>Fakotakis</last></author>
      <title><fixed-case>V</fixed-case>ergina: A <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek Speech Database for Speech Synthesis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/614_Paper.pdf</url>
      <abstract>The present paper outlines the Vergina speech database, which was developed in support of research and development of corpus-based unit selection and statistical parametric speech synthesis systems for Modern Greek language. In the following, we describe the design, development and implementation of the recording campaign, as well as the annotation of the database. Specifically, a text corpus of approximately 5 million words, collected from newspaper articles, periodicals, and paragraphs of literature, was processed in order to select the utterances-sentences needed for producing the speech database and to achieve a reasonable phonetic coverage. The broad coverage and contents of the selected utterances-sentences of the database ― text corpus collected from different domains and writing styles ― makes this database appropriate for various application domains. The database, recorded in audio studio, consists of approximately 3,000 phonetically balanced Modern Greek utterances corresponding to approximately four hours of speech. Annotation of the Vergina speech database was performed using task-specific tools, which are based on a hidden Markov model (HMM) segmentation method, and then manual inspection and corrections were performed.</abstract>
      <bibkey>lazaridis-etal-2010-vergina</bibkey>
    </paper>
    <paper id="422">
      <author><first>Vivi</first><last>Nastase</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <author><first>Benjamin</first><last>Boerschinger</last></author>
      <author><first>Caecilia</first><last>Zirn</last></author>
      <author><first>Anas</first><last>Elghafari</last></author>
      <title><fixed-case>W</fixed-case>iki<fixed-case>N</fixed-case>et: A Very Large Scale Multi-Lingual Concept Network</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/615_Paper.pdf</url>
      <abstract>This paper describes a multi-lingual large-scale concept network obtained automatically by mining for concepts and relations and exploiting a variety of sources of knowledge from Wikipedia. Concepts and their lexicalizations are extracted from Wikipedia pages, in particular from article titles, hyperlinks, disambiguation pages and cross-language links. Relations are extracted from the category and page network, from the category names, from infoboxes and the body of the articles. The resulting network has two main components: (i) a central, language independent index of concepts, which serves to keep track of the concepts' lexicalizations both within a language and across languages, and to separate linguistic expressions of concepts from the relations in which they are involved (concepts themselves are represented as numeric IDs); (ii) a large network built on the basis of the relations extracted, represented as relations between concepts (more specifically, the numeric IDs). The various stages of obtaining the network were separately evaluated, and the results show a qualitative resource.</abstract>
      <bibkey>nastase-etal-2010-wikinet</bibkey>
    </paper>
    <paper id="423">
      <author><first>Niraj</first><last>Aswani</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Developing Morphological Analysers for <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian Languages: Experimenting with the <fixed-case>H</fixed-case>indi and <fixed-case>G</fixed-case>ujarati Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/616_Paper.pdf</url>
      <abstract>A considerable amount of work has been put into development of stemmers and morphological analysers. The majority of these approaches use hand-crafted suffix-replacement rules but a few try to discover such rules from corpora. While most of the approaches remove or replace suffixes, there are examples of derivational stemmers which are based on prefixes as well. In this paper we present a rule-based morphological analyser. We propose an approach that takes both prefixes as well as suffixes into account. Given a corpus and a dictionary, our method can be used to obtain a set of suffix-replacement rules for deriving an inflected words root form. We developed an approach for the Hindi language but show that the approach is portable, at least to related languages, by adapting it to the Gujarati language. Given that the entire process of developing such a ruleset is simple and fast, our approach can be used for rapid development of morphological analysers and yet it can obtain competitive results with analysers built relying on human authored rules.</abstract>
      <bibkey>aswani-gaizauskas-2010-developing</bibkey>
    </paper>
    <paper id="424">
      <author><first>Hiroki</first><last>Hanaoka</last></author>
      <author><first>Hideki</first><last>Mima</last></author>
      <author><first>Jun’ichi</first><last>Tsujii</last></author>
      <title>A <fixed-case>J</fixed-case>apanese Particle Corpus Built by Example-Based Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/617_Paper.pdf</url>
      <abstract>This paper is a report on an on-going project of creating a new corpus focusing on Japanese particles. The corpus will provide deeper syntactic/semantic information than the existing resources. The initial target particle is ``to'' which occurs 22,006 times in 38,400 sentences of the existing corpus: the Kyoto Text Corpus. In this annotation task, an ``example-based'' methodology is adopted for the corpus annotation, which is different from the traditional annotation style. This approach provides the annotators with an example sentence rather than a linguistic category label. By avoiding linguistic technical terms, it is expected that any native speakers, with no special knowledge on linguistic analysis, can be an annotator without long training, and hence it can reduce the annotation cost. So far, 10,475 occurrences have been already annotated, with an inter-annotator agreement of 0.66 calculated by Cohen's kappa. The initial disagreement analyses and future directions are discussed in the paper.</abstract>
      <bibkey>hanaoka-etal-2010-japanese</bibkey>
    </paper>
    <paper id="425">
      <author><first>Caroline</first><last>Sporleder</last></author>
      <author><first>Linlin</first><last>Li</last></author>
      <author><first>Philip</first><last>Gorinski</last></author>
      <author><first>Xaver</first><last>Koch</last></author>
      <title>Idioms in Context: The <fixed-case>IDIX</fixed-case> Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/618_Paper.pdf</url>
      <abstract>Idioms and other figuratively used expressions pose considerable problems to natural language processing applications because they are very frequent and often behave idiosyncratically. Consequently, there has been much research on the automatic detection and extraction of idiomatic expressions. Most studies focus on type-based idiom detection, i.e., distinguishing whether a given expression can (potentially) be used idiomatically. However, many expressions such as ""break the ice"" can have both literal and non-literal readings and need to be disambiguated in a given context (token-based detection). So far relatively few approaches have attempted context-based idiom detection. One reason for this may be that few annotated resources are available that disambiguate expressions in context. With the IDIX corpus, we aim to address this. IDIX is available as an add-on to the BNC and disambiguates different usages of a subset of idioms. We believe that this resource will be useful both for linguistic and computational linguistic studies.</abstract>
      <bibkey>sporleder-etal-2010-idioms</bibkey>
    </paper>
    <paper id="426">
      <author><first>Theodoros</first><last>Kostoulas</last></author>
      <author><first>Otilia</first><last>Kocsis</last></author>
      <author><first>Todor</first><last>Ganchev</last></author>
      <author><first>Fernando</first><last>Fernández-Aranda</last></author>
      <author><first>Juan J.</first><last>Santamaría</last></author>
      <author><first>Susana</first><last>Jiménez-Murcia</last></author>
      <author><first>Maher Ben</first><last>Moussa</last></author>
      <author><first>Nadia</first><last>Magnenat-Thalmann</last></author>
      <author><first>Nikos</first><last>Fakotakis</last></author>
      <title>The <fixed-case>P</fixed-case>lay<fixed-case>M</fixed-case>ancer Database: A Multimodal Affect Database in Support of Research and Development Activities in Serious Game Environment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/619_Paper.pdf</url>
      <abstract>The present paper reports on a recent effort that resulted in the establishment of a unique multimodal affect database, referred to as the PlayMancer database. This database was created in support of the research and development activities, taking place within the PlayMancer project, which aim at the development of a serious game environment in support of treatment of patients with behavioural and addictive disorders, such as eating disorders and gambling addictions. Specifically, for the purpose of data collection, we designed and implemented a pilot trial with healthy test subjects. Speech, video and bio-signals (pulse-rate, SpO2) were captured synchronously, during the interaction of healthy people with a number of video games. The collected data were annotated by the test subjects (self-annotation), targeting proper interpretation of the underlying affective states. The broad-shouldered design of the PlayMancer database allows its use for the needs of research on multimodal affect-emotion recognition and multimodal human-computer interaction in serious games environment.</abstract>
      <bibkey>kostoulas-etal-2010-playmancer</bibkey>
    </paper>
    <paper id="427">
      <author><first>Swaran</first><last>Lata</last></author>
      <author><first>Somnath Chandra Vijay</first><last>Kumar</last></author>
      <title>Development of Linguistic Resources and Tools for Providing Multilingual Solutions in <fixed-case>I</fixed-case>ndian Languages — A Report on National Initiative</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/620_Paper.pdf</url>
      <abstract>The multilingual diversity of India is one of the most unique in world. Currently there are 22 constitutionally recognized languages with 12 scripts. Apart from these, there are at least 35 different languages and 2000 dialects in 4 major language families. It is thus evident that, development and proliferation of software solutions in the Indic multilingual environment requires continuous and sustained effort to edge out challenges in all core areas namely storage and encoding, input mechanism, browser support and data exchange. Linguistic Resources and Tools are the key building blocks to develop multilingual solutions. In this paper, we shall present an overview of the major national initiative in India for the development and standardization of Linguistic Resources and Tools for developing and deployment of multilingual ICT solutions in India.</abstract>
      <bibkey>lata-kumar-2010-development</bibkey>
    </paper>
    <paper id="428">
      <author><first>Cristina</first><last>Nicolae</last></author>
      <author><first>Gabriel</first><last>Nicolae</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <title><fixed-case>C</fixed-case>-3: Coherence and Coreference Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/622_Paper.pdf</url>
      <abstract>The phenomenon of coreference, covering entities, their mentions and their properties, is intricately linked to the phenomenon of coherence, covering the structure of rhetorical relations in a discourse. A text corpus that has both phenomena annotated can be used to test hypotheses about their interrelation or to detect other phenomena. We present the process by which C-3, a new corpus, was obtained by annotating the Discourse GraphBank coherence corpus with entity and mention information. The annotation followed a set of ACE guidelines adapted to favor coreference and to include entities of unknown types in the annotation. Together with the corpus we offer a new annotation tool specifically designed to annotate entity and mention information within a simple and functional graphical interface that combines the best of all worlds from available annotation tools. The potential usefulness of C-3 is discussed, as well as an application in which the corpus proved to be a valuable resource.</abstract>
      <bibkey>nicolae-etal-2010-c</bibkey>
    </paper>
    <paper id="429">
      <author><first>Stephen A.</first><last>Boxwell</last></author>
      <author><first>Chris</first><last>Brew</last></author>
      <title>A Pilot <fixed-case>A</fixed-case>rabic <fixed-case>CCG</fixed-case>bank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/623_Paper.pdf</url>
      <abstract>We describe a process for converting the Penn Arabic Treebank into the CCG formalism. Previous efforts have yielded CCGbanks in English, German, and Turkish, thus opening these languages to the sophisticated computational tools developed for CCG and enabling further cross-linguistic development. Conversion from a context free grammar treebank to a CCGbank is a four stage process: head finding, argument classification, binarization, and category conversion. In the process of implementing a basic CCGbank conversion algorithm, we reveal properties of Arabic grammar that interfere with conversion, such as subject topicalization, genitive constructions, relative clauses, and optional pronominal subjects. All of these problematic phenomena can be resolved in a variety of ways - we discuss advantages and disadvantages of each in their respective sections. We detail these and describe our categorial analysis of each of these Arabic grammatical phenomena in depth, as well as technical details on their integration into the conversion algorithm.</abstract>
      <bibkey>boxwell-brew-2010-pilot</bibkey>
    </paper>
    <paper id="430">
      <author><first>Cécile</first><last>Fougeron</last></author>
      <author><first>Lise</first><last>Crevier-Buchman</last></author>
      <author><first>Corinne</first><last>Fredouille</last></author>
      <author><first>Alain</first><last>Ghio</last></author>
      <author><first>Christine</first><last>Meunier</last></author>
      <author><first>Claude</first><last>Chevrie-Muller</last></author>
      <author><first>Jean-Francois</first><last>Bonastre</last></author>
      <author><first>Antonia</first><last>Colazo Simon</last></author>
      <author><first>Céline</first><last>Delooze</last></author>
      <author><first>Danielle</first><last>Duez</last></author>
      <author><first>Cédric</first><last>Gendrot</last></author>
      <author><first>Thierry</first><last>Legou</last></author>
      <author><first>Nathalie</first><last>Levèque</last></author>
      <author><first>Claire</first><last>Pillot-Loiseau</last></author>
      <author><first>Serge</first><last>Pinto</last></author>
      <author><first>Gilles</first><last>Pouchoulin</last></author>
      <author><first>Danièle</first><last>Robert</last></author>
      <author><first>Jacqueline</first><last>Vaissiere</last></author>
      <author><first>François</first><last>Viallet</last></author>
      <author><first>Coralie</first><last>Vincent</last></author>
      <title>The <fixed-case>D</fixed-case>es<fixed-case>P</fixed-case>ho-<fixed-case>AP</fixed-case>a<fixed-case>D</fixed-case>y Project: Developing an Acoustic-phonetic Characterization of Dysarthric Speech in <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/626_Paper.pdf</url>
      <abstract>This paper presents the rationale, objectives and advances of an on-going project (the DesPho-APaDy project funded by the French National Agency of Research) which aims to provide a systematic and quantified description of French dysarthric speech, over a large population of patients and three dysarthria types (related to the parkinson's disease, the Amyotrophic Lateral Sclerosis disease, and a pure cerebellar alteration). The two French corpora of dysarthric patients, from which the speech data have been selected for analysis purposes, are firstly described. Secondly, this paper discusses and outlines the requirement of a structured and organized computerized platform in order to store, organize and make accessible (for selected and protected usage) dysarthric speech corpora and associated patients clinical information (mostly disseminated in different locations: labs, hospitals, â¦). The design of both a computer database and a multi-field query interface is proposed for the clinical context. Finally, advances of the project related to the selection of the population used for the dysarthria analysis, the preprocessing of the speech files, their orthographic transcription and their automatic alignment are also presented.</abstract>
      <bibkey>fougeron-etal-2010-despho</bibkey>
    </paper>
    <paper id="431">
      <author><first>Mithun</first><last>Balakrishna</last></author>
      <author><first>Dan</first><last>Moldovan</last></author>
      <author><first>Marta</first><last>Tatu</last></author>
      <author><first>Marian</first><last>Olteanu</last></author>
      <title>Semi-Automatic Domain Ontology Creation from Text Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/627_Paper.pdf</url>
      <abstract>Analysts in various domains, especially intelligence and financial, have to constantly extract useful knowledge from large amounts of unstructured or semi-structured data. Keyword-based search, faceted search, question-answering, etc. are some of the automated methodologies that have been used to help analysts in their tasks. General-purpose and domain-specific ontologies have been proposed to help these automated methods in organizing data and providing access to useful information. However, problems in ontology creation and maintenance have resulted in expensive procedures for expanding/maintaining the ontology library available to support the growing and evolving needs of analysts. In this paper, we present a generalized and improved procedure to automatically extract deep semantic information from text resources and rapidly create semantically-rich domain ontologies while keeping the manual intervention to a minimum. We also present evaluation results for the intelligence and financial ontology libraries, semi-automatically created by our proposed methodologies using freely-available textual resources from the Web.</abstract>
      <bibkey>balakrishna-etal-2010-semi</bibkey>
    </paper>
    <paper id="432">
      <author><first>Maite</first><last>Melero</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Lluís</first><last>Padró</last></author>
      <author><first>Martí</first><last>Quixal</last></author>
      <author><first>Carlos</first><last>Rodríguez</last></author>
      <author><first>Roser</first><last>Saurí</last></author>
      <title>Language Technology Challenges of a ‘Small’ Language (<fixed-case>C</fixed-case>atalan)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/628_Paper.pdf</url>
      <abstract>In this paper, we present a brief snapshot of the state of affairs in computational processing of Catalan and the initiatives that are starting to take place in an effort to bring the field a step forward, by making a better and more efficient use of the already existing resources and tools, by bridging the gap between research and market, and by establishing periodical meeting points for the community. In particular, we present the results of the First Workshop on the Computational Processing of Catalan, which succeeded in putting together a fair representation of the research in the area, and received attention from both the industry and the administration. Aside from facilitating communication among researchers and between developers and users, the Workshop provided the organizers with valuable information about existing resources, tools, developers and providers. This information has allowed us to go a step further by setting up a harvesting procedure which will hopefully build the seed of a portal-catalogue-observatory of language resources and technologies in Catalan.</abstract>
      <bibkey>melero-etal-2010-language</bibkey>
    </paper>
    <paper id="433">
      <author><first>John</first><last>Lee</last></author>
      <author><first>Dag</first><last>Haug</last></author>
      <title>Porting an <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek and <fixed-case>L</fixed-case>atin Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/631_Paper.pdf</url>
      <abstract>We have recently converted a dependency treebank, consisting of ancient Greek and Latin texts, from one annotation scheme to another that was independently designed. This paper makes two observations about this conversion process. First, we show that, despite significant surface differences between the two treebanks, a number of straightforward transformation rules yield a substantial level of compatibility between them, giving evidence for their sound design and high quality of annotation. Second, we analyze some linguistic annotations that require further disambiguation, proposing some simple yet effective machine learning methods.</abstract>
      <bibkey>lee-haug-2010-porting</bibkey>
    </paper>
    <paper id="434">
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <title>Comparing Computational Models of Selectional Preferences - Second-order Co-Occurrence vs. Latent Semantic Clusters</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/632_Paper.pdf</url>
      <abstract>This paper presents a comparison of three computational approaches to selectional preferences: (i) an intuitive distributional approach that uses second-order co-occurrence of predicates and complement properties; (ii) an EM-based clustering approach that models the strengths of predicate--noun relationships by latent semantic clusters (Rooth et al., 1999); and (iii) an extension of the latent semantic clusters by incorporating the MDL principle into the EM training, thus explicitly modelling the predicate--noun selectional preferences by WordNet classes (Schulte im Walde et al., 2008). Concerning the distributional approach, we were interested not only in how well the model describes selectional preferences, but moreover which second-order properties are most salient. For example, a typical direct object of the verb 'drink' is usually fluid, might be hot or cold, can be bought, might be bottled, etc. The general question we ask is: what characterises the predicate's restrictions to the semantic realisation of its complements? Our second interest lies in the actual comparison of the models: How does a very simple distributional model compare to much more complex approaches, and which representation of selectional preferences is more appropriate, using (i) second-order properties, (ii) an implicit generalisation of nouns (by clusters), or (iii) an explicit generalisation of nouns by WordNet classes within clusters? We describe various experiments on German data and two evaluations, and demonstrate that the simple distributional model outperforms the more complex cluster-based models in most cases, but does itself not always beat the powerful frequency baseline.</abstract>
      <bibkey>schulte-im-walde-2010-comparing</bibkey>
    </paper>
    <paper id="435">
      <author><first>Paul</first><last>McNamee</last></author>
      <author><first>Hoa Trang</first><last>Dang</last></author>
      <author><first>Heather</first><last>Simpson</last></author>
      <author><first>Patrick</first><last>Schone</last></author>
      <author><first>Stephanie M.</first><last>Strassel</last></author>
      <title>An Evaluation of Technologies for Knowledge Base Population</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/634_Paper.pdf</url>
      <abstract>Previous content extraction evaluations have neglected to address problems which complicate the incorporation of extracted information into an existing knowledge base. Previous question answering evaluations have likewise avoided tasks such as explicit disambiguation of target entities and handling a fixed set of questions about entities without previous determination of possible answers. In 2009 NIST conducted a Knowledge Base Population track at its Text Analysis Conference to unite the content extraction and question answering communities and jointly explore some of these issues. This exciting new evaluation attracted 13 teams from 6 countries that submitted results in two tasks, Entity Linking and Slot Filling. This paper explains the motivation and design of the tasks, describes the language resources that were developed for this evaluation, offers comparisons to previous community evaluations, and briefly summarizes the performance obtained by systems. We also identify relevant issues pertaining to target selection, challenging queries, and performance measures.</abstract>
      <bibkey>mcnamee-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="436">
      <author><first>Óscar</first><last>Ferrández</last></author>
      <author><first>Michael</first><last>Ellsworth</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Collin F.</first><last>Baker</last></author>
      <title>Aligning <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et and <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et based on Semantic Neighborhoods</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/636_Paper.pdf</url>
      <abstract>This paper presents an algorithm for aligning FrameNet lexical units to WordNet synsets. Both, FrameNet and WordNet, are well-known as well as widely-used resources by the entire research community. They help systems in the comprehension of the semantics of texts, and therefore, finding strategies to link FrameNet and WordNet involves challenges related to a better understanding of the human language. Such deep analysis is exploited by researchers to improve the performance of their applications. The alignment is achieved by exploiting the particular characteristics of each lexical-semantic resource, with special emphasis on the explicit, formal semantic relations in each. Semantic neighborhoods are computed for each alignment of lemmas, and the algorithm calculates correlation scores by comparing such neighborhoods. The results suggest that the proposed algorithm is appropriate for aligning the FrameNet and WordNet hierarchies. Furthermore, the algorithm can aid research on increasing the coverage of FrameNet, building FrameNets in other languages, and creating a system for querying a joint FrameNet-WordNet hierarchy.</abstract>
      <bibkey>ferrandez-etal-2010-aligning</bibkey>
    </paper>
    <paper id="437">
      <author><first>Hassina</first><last>Aliane</last></author>
      <author><first>Zaia</first><last>Alimazighi</last></author>
      <author><first>Ahmed Cherif</first><last>Mazari</last></author>
      <title>Al —<fixed-case>K</fixed-case>halil : The <fixed-case>A</fixed-case>rabic Linguistic Ontology Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/638_Paper.pdf</url>
      <abstract>Despite Arabic is the language of hundred millions of people over the world, little has been done in terms of computerized linguistic resources, tools or applications. In this paper we describe a project which aim is to contribute filling this gap. The project consists in building an ontology centered infrastructure for Arabic Language resources and applications. The core of this infrastructure is a linguistic ontology that is founded on Arabic Traditional Grammar. The methodology we have chosen consists in reusing an existing ontology, namely the Gold linguistic ontology. GOLD is the first ontology being designed for linguistic description on the semantic web. We first construct our ontology manually by relating our concepts from Arabic Linguistics to the upper concepts of GOLD, furthermore an information extraction algorithm is implemented to automatically enrich the ontology. We discuss the development of the ontology and present our vision for the whole project which aims at using this ontology for creating tools and resources for both linguists and NLP Researchers. Indeed, the ontology is seen , not only as a domain ontology but also as a resource for different linguistic and NLP applications.</abstract>
      <bibkey>aliane-etal-2010-al</bibkey>
    </paper>
    <paper id="438">
      <author><first>Marc</first><last>Kemps-Snijders</last></author>
      <author><first>Thomas</first><last>Koller</last></author>
      <author><first>Han</first><last>Sloetjes</last></author>
      <author><first>Huib</first><last>Verwey</last></author>
      <title><fixed-case>LAT</fixed-case> Bridge: Bridging Tools for Annotation and Exploration of Rich Linguistic Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/639_Paper.pdf</url>
      <abstract>We present a software module, the LAT Bridge, which enables bidirectional communication between the annotation and exploration tools developed at the Max Planck Institute for Psycholinguistics as part of our Language Archiving Technology (LAT) tool suite. These existing annotation and exploration tools enable the annotation, enrichment, exploration and archive management of linguistic resources. The user community has expressed the desire to use different combinations of LAT tools in conjunction with each other. The LAT Bridge is designed to cater for a number of basic data interaction scenarios between the LAT annotation and exploration tools. These interaction scenarios (e.g. bootstrapping a wordlist, searching for annotation examples or lexical entries) have been identified in collaboration with researchers at our institute. We had to take into account that the LAT tools for annotation and exploration represent a heterogeneous application scenario with desktop-installed and web-based tools. Additionally, the LAT Bridge has to work in situations where the Internet is not available or only in an unreliable manner (i.e. with a slow connection or with frequent interruptions). As a result, the LAT Bridges architecture supports both online and offline communication between the LAT annotation and exploration tools.</abstract>
      <bibkey>kemps-snijders-etal-2010-lat</bibkey>
    </paper>
    <paper id="439">
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Adam</first><last>Liška</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <title>Evaluating Utility of Data Sources in a Large Parallel <fixed-case>C</fixed-case>zech-<fixed-case>E</fixed-case>nglish Corpus <fixed-case>C</fixed-case>z<fixed-case>E</fixed-case>ng 0.9</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/642_Paper.pdf</url>
      <abstract>CzEng 0.9 is the third release of a large parallel corpus of Czech and English. For the current release, CzEng was extended by significant amount of texts from various types of sources, including parallel web pages, electronically available books and subtitles. This paper describes and evaluates filtering techniques employed in the process in order to avoid misaligned or otherwise damaged parallel sentences in the collection. We estimate the precision and recall of two sets of filters. The first set was used to process the data before their inclusion into CzEng. The filters from the second set were newly created to improve the filtering process for future releases of CzEng. Given the overall amount and variance of sources of the data, our experiments illustrate the utility of parallel data sources with respect to extractable parallel segments. As a similar behaviour can be expected for other language pairs, our results can be interpreted as guidelines indicating which sources should other researchers exploit first.</abstract>
      <bibkey>bojar-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="440">
      <author><first>Maria</first><last>Liakata</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <author><first>Advaith</first><last>Siddharthan</last></author>
      <author><first>Colin</first><last>Batchelor</last></author>
      <title>Corpora for the Conceptualisation and Zoning of Scientific Papers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/644_Paper.pdf</url>
      <abstract>We present two complementary annotation schemes for sentence based annotation of full scientific papers, CoreSC and AZ-II, applied to primary research articles in chemistry. AZ-II is the extension of AZ for chemistry papers. AZ has been shown to have been reliably annotated by independent human coders and useful for various information access tasks. Like AZ, AZ-II follows the rhetorical structure of a scientific paper and the knowledge claims made by the authors. The CoreSC scheme takes a different view of scientific papers, treating them as the humanly readable representations of scientific investigations. It seeks to retrieve the structure of the investigation from the paper as generic high-level Core Scientific Concepts (CoreSC). CoreSCs have been annotated by 16 chemistry experts over a total of 265 full papers in physical chemistry and biochemistry. We describe the differences and similarities between the two schemes in detail and present the two corpora produced using each scheme. There are 36 shared papers in the corpora, which allows us to quantitatively compare aspects of the annotation schemes. We show the correlation between the two schemes, their strengths and weeknesses and discuss the benefits of combining a rhetorical based analysis of the papers with a content-based one.</abstract>
      <bibkey>liakata-etal-2010-corpora</bibkey>
    </paper>
    <paper id="441">
      <author><first>Alberto</first><last>Tretti</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <title>Analysis and Presentation of Results for Mobile Local Search</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/647_Paper.pdf</url>
      <abstract>Aggregation of long lists of concepts is important to avoid overwhelming a small display. Focusing on the domain of mobile local search, this paper presents the development of an application to perform filtering and aggregation of results obtained through the Yahoo! Local web service. First, we performed an analysis of the data available through Yahoo! Local by crawling its database with over 170 thousand local listings located in Chicago. Then, we compiled resources and developed algorithms to filter and aggregate local search results. The methods developed exploit Yahoo!s listings categorization to reduce the result space and pinpoint the category containing the most relevant results. Finally, we evaluated a prototype through a user study, which pitted our system against Yahoo! Local and against a plain list of search results. The results obtained from the study show that our aggregation methods are quite effective, cutting down the number of entries returned to the user by 43% on average, but leaving search efficiency and user satisfaction unaffected.</abstract>
      <bibkey>tretti-di-eugenio-2010-analysis</bibkey>
    </paper>
    <paper id="442">
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Thierry</first><last>Bazillon</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Jérôme</first><last>Farinas</last></author>
      <title>The <fixed-case>EPAC</fixed-case> Corpus: Manual and Automatic Annotations of Conversational Speech in <fixed-case>F</fixed-case>rench Broadcast News</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/650_Paper.pdf</url>
      <abstract>This paper presents the EPAC corpus which is composed by a set of 100 hours of conversational speech manually transcribed and by the outputs of automatic tools (automatic segmentation, transcription, POS tagging, etc.) applied on the entire French ESTER 1 audio corpus: this concerns about 1700 hours of audio recordings from radiophonic shows. This corpus was built during the EPAC project funded by the French Research Agency (ANR) from 2007 to 2010. This corpus increases significantly the amount of French manually transcribed audio recordings easily available and it is now included as a part of the ESTER 1 corpus in the ELRA catalog without additional cost. By providing a large set of automatic outputs of speech processing tools, the EPAC corpus should be useful to researchers who want to work on such data without having to develop and deal with such tools. These automatic annotations are various: segmentation and speaker diarization, one-best hypotheses from the LIUM automatic speech recognition system with confidence measures, but also word-lattices and confusion networks, named entities, part-of-speech tags, chunks, etc. The 100 hours of speech manually transcribed were split into three data sets in order to get an official training corpus, an official development corpus and an official test corpus. These data sets were used to develop and to evaluate some automatic tools which have been used to process the 1700 hours of audio recording. For example, on the EPAC test data set our ASR system yields a word error rate equals to 17.25%.</abstract>
      <bibkey>esteve-etal-2010-epac</bibkey>
    </paper>
    <paper id="443">
      <author><first>Katrin</first><last>Tomanek</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <title>Annotation Time Stamps — Temporal Metadata from the Linguistic Annotation Process</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/652_Paper.pdf</url>
      <abstract>We describe the re-annotation of selected types of named entities (persons, organizations, locations) from the Muc7 corpus. The focus of this annotation initiative is on recording the time needed for the linguistic process of named entity annotation. Annotation times are measured on two basic annotation units -- sentences vs. complex noun phrases. We gathered evidence that decision times are non-uniformly distributed over the annotation units, while they do not substantially deviate among annotators. This data seems to support the hypothesis that annotation times very much depend on the inherent ""hardness"" of each single annotation decision. We further show how such time-stamped information can be used for empirically grounded studies of selective sampling techniques, such as Active Learning. We directly compare Active Learning costs on the basis of token-based vs. time-based measurements. The data reveals that Active Learning keeps its competitive advantage over random sampling in both scenarios though the difference is less marked for the time metric than for the token metric.</abstract>
      <bibkey>tomanek-hahn-2010-annotation</bibkey>
    </paper>
    <paper id="444">
      <author><first>Stuart</first><last>Moore</last></author>
      <author><first>Sabine</first><last>Buchholz</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <title>Annotating the <fixed-case>E</fixed-case>nron Email Corpus with Number Senses</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/653_Paper.pdf</url>
      <abstract>The Enron Email Corpus provides ``Real World'' text in the business email domain, which is a target domain for many speech and language applications. We present a section of this corpus annotated with number senses - labelling each number as a date, time, year, telephone number etc. We show that sense categories and their frequencies are very different in this domain than in newswire text. The annotated corpus can provide valuable material for the development of number sense disambiguation techniques. We have released the annotations into the public domain, to allow other researchers to perform comparisons.</abstract>
      <bibkey>moore-etal-2010-annotating</bibkey>
    </paper>
    <paper id="445">
      <author><first>Piroska</first><last>Lendvai</last></author>
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Sándor</first><last>Darányi</last></author>
      <author><first>Pablo</first><last>Gervás</last></author>
      <author><first>Raquel</first><last>Hervás</last></author>
      <author><first>Scott</first><last>Malec</last></author>
      <author><first>Federico</first><last>Peinado</last></author>
      <title>Integration of Linguistic Markup into Semantic Models of Folk Narratives: The Fairy Tale Use Case</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/654_Paper.pdf</url>
      <abstract>Propp's influential structural analysis of fairy tales created a powerful schema for representing storylines in terms of character functions, which is directly exploitable for computational semantic analysis, and procedural generation of stories of this genre. We tackle two resources that draw on the Proppian model - one formalizes it as a semantic markup scheme and the other as an ontology -, both lacking linguistic phenomena explicitly represented in them. The need for integrating linguistic information into structured semantic resources is motivated by the emergence of suitable standards that facilitate this, as well as the benefits such joint representation would create for transdisciplinary research across Digital Humanities, Computational Linguistics, and Artificial Intelligence.</abstract>
      <bibkey>lendvai-etal-2010-integration</bibkey>
    </paper>
    <paper id="446">
      <author><first>Bora</first><last>Savas</last></author>
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <title>An <fixed-case>LMF</fixed-case>-based Web Service for Accessing <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et-type Semantic Lexicons</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/655_Paper.pdf</url>
      <abstract>This paper describes a Web service for accessing WordNet-type semantic lexicons. The central idea behind the service design is: given a query, the primary functionality of lexicon access is to present a partial lexicon by extracting the relevant part of the target lexicon. Based on this idea, we implemented the system as a RESTful Web service whose input query is specified by the access URI and whose output is presented in a standardized XML data format. LMF, an ISO standard for modeling lexicons, plays the most prominent role: the access URI pattern basically reflects the lexicon structure as defined by LMF; the access results are rendered based on Wordnet-LMF, which is a version of LMF XML-serialization. The Web service currently provides accesses to Princeton WordNet, Japanese WordNet, as well as the EDR Electronic Dictionary as a trial. To accommodate the EDR dictionary within the same framework, we modeled it also as a WordNet-type semantic lexicon. This paper thus argues possible alternatives to model innately bilingual/multilingual lexicons like EDR with LMF, and proposes possible revisions to Wordnet-LMF.</abstract>
      <bibkey>savas-etal-2010-lmf</bibkey>
    </paper>
    <paper id="447">
      <author><first>Jordi</first><last>Atserias</last></author>
      <author><first>Giuseppe</first><last>Attardi</last></author>
      <author><first>Maria</first><last>Simi</last></author>
      <author><first>Hugo</first><last>Zaragoza</last></author>
      <title>Active Learning for Building a Corpus of Questions for Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/656_Paper.pdf</url>
      <abstract>This paper describes how we built a dependency Treebank for questions. The questions for the Treebank were drawn from questions from the TREC 10 QA task and from Yahoo! Answers. Among the uses for the corpus is to train a dependency parser achieving good accuracy on parsing questions without hurting its overall accuracy. We also explore active learning techniques to determine the suitable size for a corpus of questions in order to achieve adequate accuracy while minimizing the annotation efforts.</abstract>
      <bibkey>atserias-etal-2010-active</bibkey>
    </paper>
    <paper id="448">
      <author><first>Paul</first><last>Cook</last></author>
      <author><first>Suzanne</first><last>Stevenson</last></author>
      <title>Automatically Identifying Changes in the Semantic Orientation of Words</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/657_Paper.pdf</url>
      <abstract>The meanings of words are not fixed but in fact undergo change, with new word senses arising and established senses taking on new aspects of meaning or falling out of usage. Two types of semantic change are amelioration and pejoration; in these processes a word sense changes to become more positive or negative, respectively. In this first computational study of amelioration and pejoration we adapt a web-based method for determining semantic orientation to the task of identifying ameliorations and pejorations in corpora from differing time periods. We evaluate our proposed method on a small dataset of known historical ameliorations and pejorations, and find it to perform better than a random baseline. Since this test dataset is small, we conduct a further evaluation on artificial examples of amelioration and pejoration, and again find evidence that our proposed method is able to identify changes in semantic orientation. Finally, we conduct a preliminary evaluation in which we apply our methods to the task of finding words which have recently undergone amelioration or pejoration.</abstract>
      <bibkey>cook-stevenson-2010-automatically</bibkey>
    </paper>
    <paper id="449">
      <author><first>Anton</first><last>Leuski</last></author>
      <author><first>David</first><last>Traum</last></author>
      <title><fixed-case>NPCE</fixed-case>ditor: A Tool for Building Question-Answering Characters</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/660_Paper.pdf</url>
      <abstract>NPCEditor is a system for building and deploying virtual characters capable of engaging a user in spoken dialog on a limited domain. The dialogue may take any form as long as the character responses can be specified a priori. For example, NPCEditor has been used for constructing question answering characters where a user asks questions and the character responds, but other scenarios are possible. At the core of the system is a state of the art statistical language classification technology for mapping from user's text input to system responses. NPCEditor combines the classifier with a database that stores the character information and relevant language data, a server that allows the character designer to deploy the completed characters, and a user-friendly editor that helps the designer to accomplish both character design and deployment tasks. In the paper we define the overall system architecture, describe individual NPCEditor components, and guide the reader through the steps of building a virtual character.</abstract>
      <bibkey>leuski-traum-2010-npceditor</bibkey>
    </paper>
    <paper id="450">
      <author><first>Changqin</first><last>Quan</last></author>
      <author><first>Fuji</first><last>Ren</last></author>
      <title>Automatic Annotation of Word Emotion in Sentences Based on <fixed-case>R</fixed-case>en-<fixed-case>CEC</fixed-case>ps</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/662_Paper.pdf</url>
      <abstract>Textual information is an important communication medium contained rich expression of emotion, and emotion recognition on text has wide applications. Word emotion analysis is fundamental in the problem of textual emotion recognition. Through an analysis of the characteristics of word emotion expression, we use word emotion vector to describe the combined basic emotions in a word, which can be used to distinguish direct and indirect emotion words, express emotion ambiguity in words, and express multiple emotions in words. Based on Ren-CECps (a Chinese emotion corpus), we do an experiment to explore the role of emotion word for sentence emotion recognition and we find that the emotions of a simple sentence (sentence without negative words, conjunctions, or question mark) can be approximated by an addition of the word emotions. Then MaxEnt modeling is used to find which context features are effective for recognizing word emotion in sentences. The features of word, N-words, POS, Pre-N-words emotion, Pre-is-degree-word, Pre-is-negativeword, Pre-is-conjunction and their combination have been experimented. After that, we use the two metrics: Kappa coefficient of agreement and Voting agreement to measure the word annotation agreement of Ren-CECps. The experiments on above context features showed promising results compared with word emotion agreement on people's judgments.</abstract>
      <bibkey>quan-ren-2010-automatic</bibkey>
    </paper>
    <paper id="451">
      <author><first>Kseniya</first><last>Zablotskaya</last></author>
      <author><first>Steffen</first><last>Walter</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <title>Speech Data Corpus for Verbal Intelligence Estimation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/663_Paper.pdf</url>
      <abstract>The goal of our research is the development of algorithms for automatic estimation of a person's verbal intelligence based on the analysis of transcribed spoken utterances. In this paper we present the corpus of German native speakers' monologues and dialogues about the same topics collected at the University of Ulm, Germany. The monologues were descriptions of two short films; the dialogues were discussions about problems of German education. The data corpus contains the verbal intelligence quotients of each speaker, which were measured with the Hamburg Wechsler Intelligence Test for Adults. In this paper we describe our corpus, why we decided to create it, and how it was collected. We also describe some approaches which can be applied to the transcribed spoken utterances for extraction of different features which could have a correlation with a person's verbal intelligence. The data corpus consists of 71 monologues and 30 dialogues (about 10 hours of audio data).</abstract>
      <bibkey>zablotskaya-etal-2010-speech</bibkey>
    </paper>
    <paper id="452">
      <author><first>Yi</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Yongsheng</first><last>Yang</last></author>
      <author><first>Denise</first><last>DiPersio</last></author>
      <author><first>Meghan</first><last>Glenn</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <title>A Very Large Scale <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese Broadcast Corpus for <fixed-case>GALE</fixed-case> Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/664_Paper.pdf</url>
      <abstract>In this paper, we present the design, collection, transcription and analysis of a Mandarin Chinese Broadcast Collection of over 3000 hours. The data was collected by Hong Kong University of Science and Technology (HKUST) in China on a cable TV and satellite transmission platform established in support of the DARPA Global Autonomous Language Exploitation (GALE) program. The collection includes broadcast news (BN) and broadcast conversation (BC) including talk shows, roundtable discussions, call-in shows, editorials and other conversational programs that focus on news and current events. HKUST also collects detailed information about all recorded programs. A subset of BC and BN recordings are manually transcribed with standard Chinese characters in UTF-8 encoding, using specific mark-ups for a small set of spontaneous and conversational speech phenomena. The collection is among the largest and first of its kind for Mandarin Chinese Broadcast speech, providing abundant and diverse samples for Mandarin speech recognition and other application-dependent tasks, such as spontaneous speech processing and recognition, topic detection, information retrieval, and speaker recognition. HKUSTâs acoustic analysis of 500 hours of the speech and transcripts demonstrates the positive impact this data could have on system performance.</abstract>
      <bibkey>liu-etal-2010-large</bibkey>
    </paper>
    <paper id="453">
      <author><first>Anca</first><last>Dinu</last></author>
      <title>Building a <fixed-case>G</fixed-case>enerative <fixed-case>L</fixed-case>exicon for <fixed-case>R</fixed-case>omanian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/665_Paper.pdf</url>
      <abstract>We present in this paper an on-going research: the construction and annotation of a Romanian Generative Lexicon (RoGL). Our system follows the specifications of CLIPS project for Italian language. It contains a corpus, a type ontology, a graphical interface and a database from which we generate data in XML format.</abstract>
      <bibkey>dinu-2010-building</bibkey>
    </paper>
    <paper id="454">
      <author><first>Jerid</first><last>Francom</last></author>
      <author><first>Amy</first><last>LaCross</last></author>
      <author><first>Adam</first><last>Ussishkin</last></author>
      <title>How Specialized are Specialized Corpora? Behavioral Evaluation of Corpus Representativeness for <fixed-case>M</fixed-case>altese.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/666_Paper.pdf</url>
      <abstract>In this paper we bring to light a novel intersection between corpus linguistics and behavioral data that can be employed as an evaluation metric for resources for low-density languages, drawing on well-established psycholinguistic factors. Using the low-density language Maltese as a test case, we highlight the challenges that face researchers developing resources for languages with sparsely available data and identify a key empirical link between corpus and psycholinguistic research as a tool to evaluate corpus resources. Specifically, we compare two robust variables identified in the psycholinguistic literature: word frequency (as measured in a corpus) and word familiarity (as measured in a rating task). We then apply statistical methods to evaluate the extent to which familiarity ratings predict corpus frequency for verbs in the Maltese corpus from three angles: 1) token frequency, 2) frequency distributions and 3) morpho-syntactic type (binyan). This research provides a multidisciplinary approach to corpus development and evaluation, in particular for less-resourced languages that lack a wide access to diverse language data.</abstract>
      <bibkey>francom-etal-2010-specialized</bibkey>
    </paper>
    <paper id="455">
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>Christopher</first><last>Caruso</last></author>
      <author><first>Denise</first><last>DiPersio</last></author>
      <title>Large Scale Multilingual Broadcast Data Collection to Support Machine Translation and Distillation Technology Development</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/667_Paper.pdf</url>
      <abstract>The development of technologies to address machine translation and distillation of multilingual broadcast data depends heavily on the collection of large volumes of material from modern data providers. To address the needs of GALE researchers, the Linguistic Data Consortium (LDC) developed a system for collecting broadcast news and conversation from a variety of Arabic, Chinese and English broadcasters. The system is highly automated, easily extensible and robust and is capable of collecting, processing and evaluating hundreds of hours of content from several dozen sources per day. In addition to this extensive system, LDC manages three remote collection sites to maximize the variety of available broadcast data and has designed a portable broadcast collection platform to facilitate remote collection. This paper will present a detailed a description of the design and implementation of LDCs collection system, the technical challenges and solutions to large scale broadcast data collection efforts and an overview of the systems operation. This paper will also discuss the challenges of managing remote collections, in particular, the strategies used to normalize data formats, naming conventions and delivery methods to achieve optimal integration of remotely-collected data into LDCs collection database and downstream tasking workflow.</abstract>
      <bibkey>walker-etal-2010-large</bibkey>
    </paper>
    <paper id="456">
      <author><first>Laura</first><last>Street</last></author>
      <author><first>Nathan</first><last>Michalov</last></author>
      <author><first>Rachel</first><last>Silverstein</last></author>
      <author><first>Michael</first><last>Reynolds</last></author>
      <author><first>Lurdes</first><last>Ruela</last></author>
      <author><first>Felicia</first><last>Flowers</last></author>
      <author><first>Angela</first><last>Talucci</last></author>
      <author><first>Priscilla</first><last>Pereira</last></author>
      <author><first>Gabriella</first><last>Morgon</last></author>
      <author><first>Samantha</first><last>Siegel</last></author>
      <author><first>Marci</first><last>Barousse</last></author>
      <author><first>Antequa</first><last>Anderson</last></author>
      <author><first>Tashom</first><last>Carroll</last></author>
      <author><first>Anna</first><last>Feldman</last></author>
      <title>Like Finding a Needle in a Haystack: Annotating the <fixed-case>A</fixed-case>merican National Corpus for Idiomatic Expressions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/668_Paper.pdf</url>
      <abstract>Our paper presents the details of a pilot study in which we tagged portions of the American National Corpus (ANC) for idioms composed of verb-noun constructions, prepositional phrases, and subordinate clauses. The three data sets we analyzed included 1,500-sentence samples from the spoken, the nonfiction, and the fiction portions of the ANC. Our paper provides the details of the tagset we developed, the motivation behind our choices, and the inter-annotator agreement measures we deemed appropriate for this task. In tagging the ANC for idiomatic expressions, our annotators achieved a high level of agreement (&gt; .80) on the tags but a low level of agreement (&lt; .00) on what constituted an idiom. These findings support the claim that identifying idiomatic and metaphorical expressions is a highly difficult and subjective task. In total, 135 idiom types and 154 idiom tokens were identified. Based on the total tokens found for each idiom class, we suggest that future research on idiom detection and idiom annotation include prepositional phrases as this class of idioms occurred frequently in the nonfiction and spoken samples of our corpus</abstract>
      <bibkey>street-etal-2010-like</bibkey>
    </paper>
    <paper id="457">
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Bruno</first><last>Pouliquen</last></author>
      <author><first>Mohamed</first><last>Ebrahim</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <title>Adapting a resource-light highly multilingual Named Entity Recognition system to <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/669_Paper.pdf</url>
      <abstract>We present a fully functional Arabic information extraction (IE) system that is used to analyze large volumes of news texts every day to extract the named entity (NE) types person, organization, location, date and number, as well as quotations (direct reported speech) by and about people. The Named Entity Recognition (NER) system was not developed for Arabic, but - instead - a highly multilingual, almost language-independent NER system was adapted to also cover Arabic. The Semitic language Arabic substantially differs from the Indo-European and Finno-Ugric languages currently covered. This paper thus describes what Arabic language-specific resources had to be developed and what changes needed to be made to the otherwise language-independent rule set in order to be applicable to the Arabic language. The achieved evaluation results are generally satisfactory, but could be improved for certain entity types. The results of the IE tools can be seen on the Arabic pages of the freely accessible Europe Media Monitor (EMM) application NewsExplorer, which can be found at http://press.jrc.it/overview.html.</abstract>
      <bibkey>zaghouani-etal-2010-adapting</bibkey>
    </paper>
    <paper id="458">
      <author><first>Xuansong</first><last>Li</last></author>
      <author><first>Niyu</first><last>Ge</last></author>
      <author><first>Stephen</first><last>Grimes</last></author>
      <author><first>Stephanie M.</first><last>Strassel</last></author>
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <title>Enriching Word Alignment with Linguistic Tags</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/670_Paper.pdf</url>
      <abstract>Incorporating linguistic knowledge into word alignment is becoming increasingly important for current approaches in statistical machine translation research. To improve automatic word alignment and ultimately machine translation quality, an annotation framework is jointly proposed by LDC (Linguistic Data Consortium) and IBM. The framework enriches word alignment corpora to capture contextual, syntactic and language-specific features by introducing linguistic tags to the alignment annotation. Two annotation schemes constitute the framework: alignment and tagging. The alignment scheme aims to identify minimum translation units and translation relations by using minimum-match and attachment annotation approaches. A set of word tags and alignment link tags are designed in the tagging scheme to describe these translation units and relations. The framework produces a solid ground-level alignment base upon which larger translation unit alignment can be automatically induced. To test the soundness of this work, evaluation is performed on a pilot annotation, resulting in inter- and intra- annotator agreement of above 90%. To date LDC has produced manual word alignment and tagging on 32,823 Chinese-English sentences following this framework.</abstract>
      <bibkey>li-etal-2010-enriching</bibkey>
    </paper>
    <paper id="459">
      <author><first>Kathleen</first><last>Eberhard</last></author>
      <author><first>Hannele</first><last>Nicholson</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <author><first>Susan</first><last>Gundersen</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <title>The <fixed-case>I</fixed-case>ndiana “Cooperative Remote Search Task” (<fixed-case>CR</fixed-case>e<fixed-case>ST</fixed-case>) Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/671_Paper.pdf</url>
      <abstract>This paper introduces a novel corpus of natural language dialogues obtained from humans performing a cooperative, remote, search task (CReST) as it occurs naturally in a variety of scenarios (e.g., search and rescue missions in disaster areas). This corpus is unique in that it involves remote collaborations between two interlocutors who each have to perform tasks that require the other's assistance. In addition, one interlocutor's tasks require physical movement through an indoor environment as well as interactions with physical objects within the environment. The multi-modal corpus contains the speech signals as well as transcriptions of the dialogues, which are additionally annotated for dialog structure, disfluencies, and for constituent and dependency syntax. On the dialogue level, the corpus was annotated for separate dialogue moves, based on the classification developed by Carletta et al. (1997) for coding task-oriented dialogues. Disfluencies were annotated using the scheme developed by Lickley (1998). The syntactic annotation comprises POS annotation, Penn Treebank style constituent annotations as well as dependency annotations based on the dependencies of pennconverter.</abstract>
      <bibkey>eberhard-etal-2010-indiana</bibkey>
    </paper>
    <paper id="460">
      <author><first>Carl</first><last>Christensen</last></author>
      <author><first>Ross</first><last>Hendrickson</last></author>
      <author><first>Deryle</first><last>Lonsdale</last></author>
      <title>Principled Construction of Elicited Imitation Tests</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/672_Paper.pdf</url>
      <abstract>In this paper we discuss the methodology behind the construction of elicited imitation (EI) test items. First we examine varying uses for EI tests in research and in testing overall oral proficiency. We also mention criticisms of previous test items. Then we identify the factors that contribute to the difficulty of an EI item as shown in previous studies. Based on this discussion, we describe a way of automating the creation of test items in order to better evaluate language learners' oral proficiency while improving item naturalness. We present a new item construction tool and the process that it implements in order to create test items from a corpus, identifying relevant features needed to compile a database of EI test items. We examine results from administration of a new EI test engineered in this manner, illustrating the effect that standard language resources can have on creating an effective EI test item repository. We also sketch ongoing work on test item generation for other languages and an adaptive test that will use this collection of test items.</abstract>
      <bibkey>christensen-etal-2010-principled</bibkey>
    </paper>
    <paper id="461">
      <author><first>Bharat Ram</first><last>Ambati</last></author>
      <author><first>Mridul</first><last>Gupta</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <title>A High Recall Error Identification Tool for <fixed-case>H</fixed-case>indi Treebank Validation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/673_Paper.pdf</url>
      <abstract>This paper describes the development of a hybrid tool for a semi-automated process for validation of treebank annotation at various levels. The tool is developed for error detection at the part-of-speech, chunk and dependency levels of a Hindi treebank, currently under development. The tool aims to identify as many errors as possible at these levels to achieve consistency in the task of annotation. Consistency in treebank annotation is a must for making data as error-free as possible and for providing quality assurance. The tool is aimed at ensuring consistency and to make manual validation cost effective. We discuss a rule based and a hybrid approach (statistical methods combined with rule-based methods) by which a high-recall system can be developed and used to identify errors in the treebank. We report some results of using the tool on a sample of data extracted from the Hindi treebank. We also argue how the tool can prove useful in improving the annotation guidelines which would in turn, better the quality of annotation in subsequent iterations.</abstract>
      <bibkey>ambati-etal-2010-high</bibkey>
    </paper>
    <paper id="462">
      <author><first>Susan</first><last>Robinson</last></author>
      <author><first>Antonio</first><last>Roque</last></author>
      <author><first>David</first><last>Traum</last></author>
      <title>Dialogues in Context: An Objective User-Oriented Evaluation Approach for Virtual Human Dialogue</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/674_Paper.pdf</url>
      <abstract>As conversational agents are now being developed to encounter more complex dialogue situations it is increasingly difficult to find satisfactory methods for evaluating these agents. Task-based measures are insufficient where there is no clearly defined task. While user-based evaluation methods may give a general sense of the quality of an agent's performance, they shed little light on the relative quality or success of specific features of dialogue that are necessary for system improvement. This paper examines current dialogue agent evaluation practices and motivates the need for a more detailed approach for defining and measuring the quality of dialogues between agent and user. We present a framework for evaluating the dialogue competence of artificial agents involved in complex and underspecified tasks when conversing with people. A multi-part coding scheme is proposed that provides a qualitative analysis of human utterances, and rates the appropriateness of the agent's responses to these utterances. The scheme is outlined, and then used to evaluate Staff Duty Officer Moleno, a virtual guide in Second Life.</abstract>
      <bibkey>robinson-etal-2010-dialogues</bibkey>
    </paper>
    <paper id="463">
      <author><first>Xuchen</first><last>Yao</last></author>
      <author><first>Pravin</first><last>Bhutada</last></author>
      <author><first>Kallirroi</first><last>Georgila</last></author>
      <author><first>Kenji</first><last>Sagae</last></author>
      <author><first>Ron</first><last>Artstein</last></author>
      <author><first>David</first><last>Traum</last></author>
      <title>Practical Evaluation of Speech Recognizers for Virtual Human Dialogue Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/675_Paper.pdf</url>
      <abstract>We perform a large-scale evaluation of multiple off-the-shelf speech recognizers across diverse domains for virtual human dialogue systems. Our evaluation is aimed at speech recognition consumers and potential consumers with limited experience with readily available recognizers. We focus on practical factors to determine what levels of performance can be expected from different available recognizers in various projects featuring different types of conversational utterances. Our results show that there is no single recognizer that outperforms all other recognizers in all domains. The performance of each recognizer may vary significantly depending on the domain, the size and perplexity of the corpus, the out-of-vocabulary rate, and whether acoustic and language model adaptation has been used or not. We expect that our evaluation will prove useful to other speech recognition consumers, especially in the dialogue community, and will shed some light on the key problem in spoken dialogue systems of selecting the most suitable available speech recognition system for a particular application, and what impact training will have.</abstract>
      <bibkey>yao-etal-2010-practical</bibkey>
    </paper>
    <paper id="464">
      <author><first>Kiyonori</first><last>Ohtake</last></author>
      <author><first>Teruhisa</first><last>Misu</last></author>
      <author><first>Chiori</first><last>Hori</last></author>
      <author><first>Hideki</first><last>Kashioka</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <title>Dialogue Acts Annotation for <fixed-case>NICT</fixed-case> <fixed-case>K</fixed-case>yoto Tour Dialogue Corpus to Construct Statistical Dialogue Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/676_Paper.pdf</url>
      <abstract>This paper introduces a new corpus of consulting dialogues designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus. We have collected more than 150 hours of consulting dialogues in the tourist guidance domain. We are developing the corpus that consists of speech, transcripts, speech act (SA) tags, morphological analysis results, dependency analysis results, and semantic content tags. This paper outlines our taxonomy of dialogue act (DA) annotation that can describe two aspects of an utterance: the communicative function (SA), and the semantic content of the utterance. We provide an overview of the Kyoto tour dialogue corpus and a preliminary analysis using the DA tags. We also show a result of a preliminary experiment for SA tagging via Support Vector Machines (SVMs). We introduce the current states of the corpus development In addition, we mention the usage of our corpus for the spoken dialogue system that is being developed.</abstract>
      <bibkey>ohtake-etal-2010-dialogue</bibkey>
    </paper>
    <paper id="465">
      <author><first>Bal Krishna</first><last>Bal</last></author>
      <author><first>Patrick</first><last>Saint Dizier</last></author>
      <title>Towards Building Annotated Resources for Analyzing Opinions and Argumentation in News Editorials</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/677_Paper.pdf</url>
      <abstract>This paper describes an annotation scheme for argumentation in opinionated texts such as newspaper editorials, developed from a corpus of approximately 500 English texts from Nepali and international newspaper sources. We present the results of analysis and evaluation of the corpus annotation ― currently, the inter-annotator agreement kappa value being 0.80 which indicates substantial agreement between the annotators. We also discuss some of linguistic resources (key factors for distinguishing facts from opinions, opinion lexicon, intensifier lexicon, pre-modifier lexicon, modal verb lexicon, reporting verb lexicon, general opinion patterns from the corpus etc.) developed as a result of our corpus analysis, which can be used to identify an opinion or a controversial issue, arguments supporting an opinion, orientation of the supporting arguments and their strength (intrinsic, relative and in terms of persuasion). These resources form the backbone of our work especially for performing the opinion analysis in the lower levels, i.e., in the lexical and sentence levels. Finally, we shed light on the perspectives of the given work clearly outlining the challenges.</abstract>
      <bibkey>bal-saint-dizier-2010-towards</bibkey>
    </paper>
    <paper id="466">
      <author><first>Iris</first><last>Eshkol</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <author><first>Nathalie</first><last>Friburger</last></author>
      <title><fixed-case>E</fixed-case>slo: From Transcription to Speakers’ Personal Information Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/678_Paper.pdf</url>
      <abstract>This paper presents the preliminary works to put online a French oral corpus and its transcription. This corpus is the Socio-Linguistic Survey in Orleans, realized in 1968. First, we numerized the corpus, then we handwritten transcribed it with the Transcriber software adding different tags about speakers, time, noise, etc. Each document (audio file and XML file of the transcription) was described by a set of metadata stored in an XML format to allow an easy consultation. Second, we added different levels of annotations, recognition of named entities and annotation of personal information about speakers. This two annotation tasks used the CasSys system of transducer cascades. We used and modified a first cascade to recognize named entities. Then we built a second cascade to annote the designating entities, i.e. information about the speaker. These second cascade parsed the named entity annotated corpus. The objective is to locate information about the speaker and, also, what kind of information can designate him/her. These two cascades was evaluated with precision and recall measures.</abstract>
      <bibkey>eshkol-etal-2010-eslo</bibkey>
    </paper>
    <paper id="467">
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Nuria</first><last>Bel</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Gerhard</first><last>Budin</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Eva</first><last>Hajicova</last></author>
      <author><first>Kimmo</first><last>Koskenniemi</last></author>
      <author><first>Lothar</first><last>Lemnitzer</last></author>
      <author><first>Bente</first><last>Maegaard</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <author><first>Jean-Marie</first><last>Pierrel</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Inguna</first><last>Skadina</last></author>
      <author><first>Dan</first><last>Tufis</last></author>
      <author><first>Remco</first><last>van Veenendaal</last></author>
      <author><first>Tamas</first><last>Váradi</last></author>
      <author><first>Martin</first><last>Wynne</last></author>
      <title>Resource and Service Centres as the Backbone for a Sustainable Service Infrastructure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/679_Paper.pdf</url>
      <abstract>Currently, research infrastructures are being designed and established in many disciplines since they all suffer from an enormous fragmentation of their resources and tools. In the domain of language resources and tools the CLARIN initiative has been funded since 2008 to overcome many of the integration and interoperability hurdles. CLARIN can build on knowledge and work from many projects that were carried out during the last years and wants to build stable and robust services that can be used by researchers. Here service centres will play an important role that have the potential of being persistent and that adhere to criteria as they have been established by CLARIN. In the last year of the so-called preparatory phase these centres are currently developing four use cases that can demonstrate how the various pillars CLARIN has been working on can be integrated. All four use cases fulfil the criteria of being cross-national.</abstract>
      <bibkey>wittenburg-etal-2010-resource</bibkey>
    </paper>
    <paper id="468">
      <author><first>Stefania</first><last>Spina</last></author>
      <title>The Dictionary of <fixed-case>I</fixed-case>talian Collocations: Design and Integration in an Online Learning Environment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/681_Paper.pdf</url>
      <abstract>In this paper, I introduce the DICI, an electronic dictionary of Italian collocations designed to support the acquisition of the collocational competence in learners of Italian as a second or foreign language. I briefly describe the composition of the reference Italian corpus from which the collocations are extracted, and the methodology of extraction and filtering of candidate collocations. It is an experimental methodology, based on POS filtering, frequency and statistical measures, and tested on a 12-million-word sample from the reference corpus. Furthermore, I explain the main criteria for the composition of the dictionary, in addition to its integration with a Virtual Learning Environment (VLE), aimed at supporting learning activities on collocations. I briefly describe some of the main features of this integration with the VLE, such as the automatic recognition of collocations in written Italian texts, the possibility for students to obtain further linguistic information on selected collocations, and the automatic generation of tests for collocational competence assessment of language learners. While the main goal of the DICI is pedagogical, it is also intended to contribute to research in the field of collocations.</abstract>
      <bibkey>spina-2010-dictionary</bibkey>
    </paper>
    <paper id="469">
      <author><first>Suguru</first><last>Matsuyoshi</last></author>
      <author><first>Megumi</first><last>Eguchi</last></author>
      <author><first>Chitose</first><last>Sao</last></author>
      <author><first>Koji</first><last>Murakami</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <title>Annotating Event Mentions in Text with Modality, Focus, and Source Information</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/682_Paper.pdf</url>
      <abstract>Many natural language processing tasks, including information extraction, question answering and recognizing textual entailment, require analysis of the polarity, focus of polarity, tense, aspect, mood and source of the event mentions in a text in addition to its predicate-argument structure analysis. We refer to modality, polarity and other associated information as extended modality. In this paper, we propose a new annotation scheme for representing the extended modality of event mentions in a sentence. Our extended modality consists of the following seven components: Source, Time, Conditional, Primary modality type, Actuality, Evaluation and Focus. We reviewed the literature about extended modality in Linguistics and Natural Language Processing (NLP) and defined appropriate labels of each component. In the proposed annotation scheme, information of extended modality of an event mention is summarized at the core predicate of the event mention for immediate use in NLP applications. We also report on the current progress of our manual annotation of a Japanese corpus of about 50,000 event mentions, showing a reasonably high ratio of inter-annotator agreement.</abstract>
      <bibkey>matsuyoshi-etal-2010-annotating</bibkey>
    </paper>
    <paper id="470">
      <author><first>Sisay</first><last>Adugna</last></author>
      <author><first>Andreas</first><last>Eisele</last></author>
      <title><fixed-case>E</fixed-case>nglish — <fixed-case>O</fixed-case>romo Machine Translation: An Experiment Using a Statistical Approach</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/683_Paper.pdf</url>
      <abstract>This paper deals with translation of English documents to Oromo using statistical methods. Whereas English is the lingua franca of online information, Oromo, despite its relative wide distribution within Ethiopia and neighbouring countries like Kenya and Somalia, is one of the most resource scarce languages. The paper has two main goals: one is to test how far we can go with the available limited parallel corpus for the English ― Oromo language pair and the applicability of existing Statistical Machine Translation (SMT) systems on this language pair. The second goal is to analyze the output of the system with the objective of identifying the challenges that need to be tackled. Since the language is resource scarce as mentioned above, we cannot get as many parallel documents as we want for the experiment. However, using a limited corpus of 20,000 bilingual sentences and 163,000 monolingual sentences, translation accuracy in terms of BLEU Score of 17.74% was achieved.</abstract>
      <bibkey>adugna-eisele-2010-english</bibkey>
    </paper>
    <paper id="471">
      <author><first>Atsushi</first><last>Fujii</last></author>
      <title>Modeling <fixed-case>W</fixed-case>ikipedia Articles to Enhance Encyclopedic Search</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/684_Paper.pdf</url>
      <abstract>Reflecting the rapid growth of science, technology, and culture, it has become common practice to consult tools on the World Wide Web for various terms. Existing search engines provide an enormous volume of information, but retrieved information is not organized. Hand-compiled encyclopedias provide organized information, but the quantity of information is limited. To integrate the advantages of both tools, we have been proposing methods for encyclopedic search targeting information on the Web and patent information. In this paper, we propose a method to categorize multiple expository texts for a single term based on viewpoints. Because viewpoints required for explanation are different depending on the type of a term, such as animals and diseases, it is difficult to manually produce a large scale system. We use Wikipedia to extract a prototype of a viewpoint structure for each term type. We also use articles in Wikipedia for a machine learning method, which categorizes a given text into an appropriate viewpoint. We evaluate the effectiveness of our method experimentally.</abstract>
      <bibkey>fujii-2010-modeling</bibkey>
    </paper>
    <paper id="472">
      <author><first>Matthias</first><last>Hartung</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <title>A Semi-supervised Type-based Classification of Adjectives: Distinguishing Properties and Relations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/685_Paper.pdf</url>
      <abstract>We present a semi-supervised machine-learning approach for the classification of adjectives into property- vs. relation-denoting adjectives, a distinction that is highly relevant for ontology learning. The feasibility of this classification task is evaluated in a human annotation experiment. We observe that token-level annotation of these classes is expensive and difficult. Yet, a careful corpus analysis reveals that adjective classes tend to be stable, with few occurrences of class shifts observed at the token level. As a consequence, we opt for a type-based semi-supervised classification approach. The class labels obtained from manual annotation are projected to large amounts of unannotated token samples. Training on heuristically labeled data yields high classification performance on our own data and on a data set compiled from WordNet. Our results suggest that it is feasible to automatically distinguish adjectives denoting properties and relations, using small amounts of annotated data.</abstract>
      <bibkey>hartung-frank-2010-semi</bibkey>
    </paper>
    <paper id="473">
      <author><first>Andreas</first><last>Eisele</last></author>
      <author><first>Yu</first><last>Chen</last></author>
      <title><fixed-case>M</fixed-case>ulti<fixed-case>UN</fixed-case>: A Multilingual Corpus from United Nation Documents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf</url>
      <abstract>This paper describes the acquisition, preparation and properties of a corpus extracted from the official documents of the United Nations (UN). This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language. We describe the methods we used for crawling, document formatting, and sentence alignment. This corpus also includes a common test set for machine translation. We present the results of a French-Chinese machine translation experiment performed on this corpus.</abstract>
      <bibkey>eisele-chen-2010-multiun</bibkey>
    </paper>
    <paper id="474">
      <author><first>Myriam</first><last>Rakho</last></author>
      <author><first>Matthieu</first><last>Constant</last></author>
      <title>Evaluating the Impact of Some Linguistic Information on the Performances of a Similarity-based and Translation-oriented Word-Sense Disambiguation Method</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/687_Paper.pdf</url>
      <abstract>In this article, we present an experiment of linguistic parameter tuning in the representation of the semantic space of polysemous words. We evaluate quantitatively the influence of some basic linguistic knowledge (lemmas, multi-word expressions, grammatical tags and syntactic relations) on the performances of a similarity-based Word-Sense disambiguation method. The question we try to answer, by this experiment, is which kinds of linguistic knowledge are most useful for the semantic disambiguation of polysemous words, in a multilingual framework. The experiment is about 20 French polysemous words (16 nouns and 4 verbs) and we make use of the French-English part of the sentence-aligned EuroParl Corpus for training and testing. Our results show a strong correlation between the system accuracy and the degree of precision of the linguistic features used, particularly the syntactic dependency relations. Furthermore, the lemma-based approach absolutely outperforms the word form-based approach. The best accuracy achieved by our system amounts to 90%.</abstract>
      <bibkey>rakho-constant-2010-evaluating</bibkey>
    </paper>
    <paper id="475">
      <author><first>Eckhard</first><last>Bick</last></author>
      <title><fixed-case>F</fixed-case>r<fixed-case>AG</fixed-case>, a Hybrid Constraint Grammar Parser for <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/688_Paper.pdf</url>
      <abstract>This paper describes a hybrid system (FrAG) for tagging / parsing French text, and presents results from ongoing development work, corpus annotation and evaluation. The core of the system is a sentence scope Constraint Grammar (CG), with linguist-written rules. However, unlike traditional CG, the system uses hybrid techniques on both its morphological input side and its syntactic output side. Thus, FrAG draws on a pre-existing probabilistic Decision Tree Tagger (DTT) before and in parallel with its own lexical stage, and feeds its output into a Phrase Structure Grammar (PSG) that uses CG syntactic function tags rather than ordinary terminals in its rewriting rules. As an alternative architecture, dependency tree structures are also supported. In the newest version, dependencies are assigned within the CG-framework itself, and can interact with other rules. To provide semantic context, a semantic prototype ontology for nouns is used, covering a large part of the lexicon. In a recent test run on Parliamentary debate transcripts, FrAG achieved F-scores of 98.7 % for part of speech (PoS) and between 93.1 % and 96.2 % for syntactic function tags. Dependency links were correct in 95.9 %.</abstract>
      <bibkey>bick-2010-frag</bibkey>
    </paper>
    <paper id="476">
      <author><first>Julia Maria</first><last>Schulz</last></author>
      <author><first>Christa</first><last>Womser-Hacker</last></author>
      <author><first>Thomas</first><last>Mandl</last></author>
      <title>Multilingual Corpus Development for Opinion Mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/689_Paper.pdf</url>
      <abstract>Opinion Mining is a discipline that has attracted some attention lately. Most of the research in this field has been done for English or Asian languages, due to the lack of resources in other languages. In this paper we describe an approach of building a manually annotated multilingual corpus for the domain of product reviews, which can be used as a basis for fine-grained opinion analysis also considering direct and indirect opinion targets. For each sentence in a review, the mentioned product features with their respective opinion polarity and strength on a scale from 0 to 3 are labelled manually by two annotators. The languages represented in the corpus are English, German and Spanish and the corpus consists of about 500 product reviews per language. After a short introduction and a description of related work, we illustrate the annotation process, including a description of the annotation methodology and the developed tool for the annotation process. Then first results on the inter-annotator agreement for opinions and product features are presented. We conclude the paper with an outlook on future work.</abstract>
      <bibkey>schulz-etal-2010-multilingual</bibkey>
    </paper>
    <paper id="477">
      <author><first>Bartosz</first><last>Broda</last></author>
      <author><first>Michał</first><last>Marcińczuk</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <title>Building a Node of the Accessible Language Technology Infrastructure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/690_Paper.pdf</url>
      <abstract>A limited prototype of the CLARIN Language Technology Infrastructure (LTI) node is presented. The node prototype provides several types of web services for Polish. The functionality encompasses morpho-syntactic processing, shallow semantic processing of corpus on the basis of the SuperMatrix system and plWordNet browsing. We take the prototype as the starting point for the discussion on requirements that must be fulfilled by the LTI. Some possible solutions are proposed for less frequently discussed problems, e.g. streaming processing of language data on the remote processing node. We experimentally investigate how to tackle with several requirements from many discussed. Such aspects as processing large volumes of data, asynchronous mode of processing and scalability of the architecture to large number of users got especial attention in the constructed prototype of the Web Service for morpho-syntactic processing of Polish called TaKIPI-WS (http://plwordnet.pwr.wroc.pl/clarin/ws/takipi/). TaKIPI-WS is a distributed system with a three-layer architecture, an asynchronous model of request handling and multi-agent-based processing. TaKIPI-WS consists of three layers: WS Interface, Database and Daemons. The role of the Database is to store and exchange data between the Interface and the Daemons. The Daemons (i.e. taggers) are responsible for executing the requests queued in the database. Results of the performance tests are presented in the paper, too.</abstract>
      <bibkey>broda-etal-2010-building</bibkey>
    </paper>
    <paper id="478">
      <author><first>Cássia</first><last>Trojahn</last></author>
      <author><first>Paulo</first><last>Quaresma</last></author>
      <author><first>Renata</first><last>Vieira</last></author>
      <title>An <fixed-case>API</fixed-case> for Multi-lingual Ontology Matching</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/691_Paper.pdf</url>
      <abstract>Ontology matching consists of generating a set of correspondences between the entities of two ontologies. This process is seen as a solution to data heterogeneity in ontology-based applications, enabling the interoperability between them. However, existing matching systems are designed by assuming that the entities of both source and target ontologies are written in the same languages ( English, for instance). Multi-lingual ontology matching is an open research issue. This paper describes an API for multi-lingual matching that implements two strategies, direct translation-based and indirect. The first strategy considers direct matching between two ontologies (i.e., without intermediary ontologies), with the help of external resources, i.e., translations. The indirect alignment strategy, proposed by (Jung et al., 2009), is based on composition of alignments. We evaluate these strategies using simple string similarity based matchers and three ontologies written in English, French, and Portuguese, an extension of the OAEI benchmark test 206.</abstract>
      <bibkey>trojahn-etal-2010-api</bibkey>
    </paper>
    <paper id="479">
      <author><first>Volker</first><last>Fritzsch</last></author>
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Friedhelm</first><last>Schwenker</last></author>
      <title>An Open Source Process Engine Framework for Realtime Pattern Recognition and Information Fusion Tasks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/693_Paper.pdf</url>
      <abstract>The process engine for pattern recognition and information fusion tasks, the \emph{pepr framework}, aims to empower the researcher to develop novel solutions in the field of pattern recognition and information fusion tasks in a timely manner, by supporting reuse and combination of well tested and established components in an environment, that eases the wiring of distinct algorithms and description of the control flow through graphical tooling. The framework, not only consisting of the runtime environment, comes with several highly useful components that can be leveraged as a starting point in creating new solutions, as well as a graphical process builder that allows for easy development of pattern recognition processes in a graphical, modeled manner. Additionally, numerous work has been invested in order to keep the entry barrier with regards to extending the framework as low as possible, enabling developers to add additional functionality to the framework in as less time as possible.</abstract>
      <bibkey>fritzsch-etal-2010-open</bibkey>
    </paper>
    <paper id="480">
      <author><first>Niraj</first><last>Aswani</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Transliteration using Multiple Similarity Metrics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/694_Paper.pdf</url>
      <abstract>In this paper, we present an approach to measure the transliteration similarity of English-Hindi word pairs. Our approach has two components. First we propose a bi-directional mapping between one or more characters in the Devanagari script and one or more characters in the Roman script (pronounced as in English). This allows a given Hindi word written in Devanagari to be transliterated into the Roman script and vice-versa. Second, we present an algorithm for computing a similarity measure that is a variant of Dices coefficient measure and the LCSR measure and which also takes into account the constraints needed to match English-Hindi transliterated words. Finally, by evaluating various similarity metrics individually and together under a multiple measure agreement scenario, we show that it is possible to achieve a 0.92 f-measure in identifying English-Hindi word pairs that are transliterations. In order to assess the portability of our approach to other similar languages we adapt our system to the Gujarati language.</abstract>
      <bibkey>aswani-gaizauskas-2010-english</bibkey>
    </paper>
    <paper id="481">
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>Ana</first><last>García-Serrano</last></author>
      <title><fixed-case>Q</fixed-case>-<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et: Extracting Polarity from <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Senses</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/695_Paper.pdf</url>
      <abstract>This paper presents Q-WordNet, a lexical resource consisting of WordNet senses automatically annotated by positive and negative polarity. Polarity classification amounts to decide whether a text (sense, sentence, etc.) may be associated to positive or negative connotations. Polarity classification is becoming important within the fields of Opinion Mining and Sentiment Analysis for determining opinions about commercial products, on companies reputation management, brand monitoring, or to track attitudes by mining online forums, blogs, etc. Inspired by work on classification of word senses by polarity (e.g., SentiWordNet), and taking WordNet as a starting point, we build Q-WordNet. Instead of applying external tools such as supervised classifiers to annotated WordNet synsets by polarity, we try to effectively maximize the linguistic information contained in WordNet, thereby taking advantage of the human effort put by lexicographers and annotators. The resulting resource is a subset of WordNet senses classified as positive or negative. In this approach, neutral polarity is seen as the absence of positive or negative polarity. The evaluation of Q-WordNet shows an improvement with respect to previous approaches. We believe that Q-WordNet can be used as a starting point for data-driven approaches in sentiment analysis.</abstract>
      <bibkey>agerri-garcia-serrano-2010-q</bibkey>
    </paper>
    <paper id="482">
      <author><first>Taiji</first><last>Nagasaka</last></author>
      <author><first>Ran</first><last>Shimanouchi</last></author>
      <author><first>Akiko</first><last>Sakamoto</last></author>
      <author><first>Takafumi</first><last>Suzuki</last></author>
      <author><first>Yohei</first><last>Morishita</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Suguru</first><last>Matsuyoshi</last></author>
      <title>Utilizing Semantic Equivalence Classes of <fixed-case>J</fixed-case>apanese Functional Expressions in Translation Rule Acquisition from Parallel Patent Sentences</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/696_Paper.pdf</url>
      <abstract>In the ``Sandglass'' MT architecture, we identify the class of monosemous Japanese functional expressions and utilize it in the task of translating Japanese functional expressions into English. We employ the semantic equivalence classes of a recently compiled large scale hierarchical lexicon of Japanese functional expressions. We then study whether functional expressions within a class can be translated into a single canonical English expression. Based on the results of identifying monosemous semantic equivalence classes, this paper studies how to extract rules for translating functional expressions in Japanese patent documents into English. In this study, we use about 1.8M Japanese-English parallel sentences automatically extracted from Japanese-English patent families, which are distributed through the Patent Translation Task at the NTCIR-7 Workshop. Then, as a toolkit of a phrase-based SMT (Statistical Machine Translation) model, Moses is applied and Japanese-English translation pairs are obtained in the form of a phrase translation table. Finally, we extract translation pairs of Japanese functional expressions from the phrase translation table. Through this study, we found that most of the semantic equivalence classes judged as monosemous based on manual translation into English have only one translation rules even in the patent domain.</abstract>
      <bibkey>nagasaka-etal-2010-utilizing</bibkey>
    </paper>
    <paper id="483">
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <title>Syntactic Dependencies for Multilingual and Multilevel Corpus Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/697_Paper.pdf</url>
      <abstract>The relevance of syntactic dependency annotated corpora is nowadays unquestioned. However, a broad debate on the optimal set of dependency relation tags did not take place yet. As a result, largely varying tag sets of a largely varying size are used in different annotation initiatives. We propose a hierarchical dependency structure annotation schema that is more detailed and more flexible than the known annotation schemata. The schema allows us to choose the level of the desired detail of annotation, which facilitates the use of the schema for corpus annotation for different languages and for different NLP applications. Thanks to the inclusion of semantico-syntactic tags into the schema, we can annotate a corpus not only with syntactic dependency structures, but also with valency patterns as they are usually found in separate treebanks such as PropBank and NomBank. Semantico-syntactic tags and the level of detail of the schema furthermore facilitate the derivation of deep-syntactic and semantic annotations, leading to truly multilevel annotated dependency corpora. Such multilevel annotations can be readily used for the task of ML-based acquisition of grammar resources that map between the different levels of linguistic representation ― something which forms part of, for instance, any natural language text generator.</abstract>
      <bibkey>mille-wanner-2010-syntactic</bibkey>
    </paper>
    <paper id="484">
      <author><first>Hiroaki</first><last>Sato</last></author>
      <title>How <fixed-case>F</fixed-case>rame<fixed-case>SQL</fixed-case> Shows the <fixed-case>J</fixed-case>apanese <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/698_Paper.pdf</url>
      <abstract>FrameSQL is a web-based application which the author (Sato, 2003; Sato 2008) created originally for searching the Berkeley FrameNet lexical database. FrameSQL now can handle the Japanese lexical database built by the Japanese FrameNet project (JFN) of Keio University in Japan. FrameSQL can search and view the JFN data released in March of 2009 on a standard web browser. Users do not need to install any additional software tools to use FrameSQL, nor do they even need to download the JFN data to their local computer, because FrameSQL accesses the database of the server computer, and executes searches. FrameSQL not only shows a clear view of the headwords grammar and combinatorial properties of the database, but also relates a Japanese word with its counterparts in English. FrameSQL puts together the Japanese and English lexical databases, and the user can access them seamlessly, as if they were a unified database. Mutual hyperlinks among these databases and the bilingual search mode make it easy to compare semantic structures of corresponding lexical units between these languages, and it could be useful for building multilingual lexical resources.</abstract>
      <bibkey>sato-2010-framesql</bibkey>
    </paper>
    <paper id="485">
      <author><first>Aya</first><last>Nishikawa</last></author>
      <author><first>Ryo</first><last>Nishimura</last></author>
      <author><first>Yasuhiko</first><last>Watanabe</last></author>
      <author><first>Yoshihiro</first><last>Okada</last></author>
      <title>A Context Sensitive Variant Dictionary for Supporting Variant Selection</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/699_Paper.pdf</url>
      <abstract>In Japanese, there are a large number of notational variants of words. This is because Japanese words are written in three kinds of characters: kanji (Chinese) characters, hiragara letters, and katakana letters. Japanese students study basic rules of Japanese writing in school for many years. However, it is difficult to learn which variant is suitable for a certain context in official, business, and technical documents because the rules have many exceptions. Previous Japanese writing support systems were not concerned with them sufficiently. This is because their main purposes were misspelling detection. Students often use variants which are not misspelling but unsuitable for the contexts in official, business, and technical documents. To solve this problem, we developed a context sensitive variant dictionary. A writing support system based on the context sensitive variant dictionary detects unsuitable variants for the contexts in students' reports and shows suitable ones to the students. In this study, we first show how to develop a context sensitive variant dictionary by which our system determines which variant is suitable for a context in official, business, and technical documents. Finally, we conducted a control experiment and show the effectiveness of our dictionary.</abstract>
      <bibkey>nishikawa-etal-2010-context</bibkey>
    </paper>
    <paper id="486">
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Géraldine</first><last>Walther</last></author>
      <title>A Morphological Lexicon for the <fixed-case>P</fixed-case>ersian Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/700_Paper.pdf</url>
      <abstract>We introduce PerLex, a large-coverage and freely-available morphological lexicon for the Persian language. We describe the main features of the Persian morphology, and the way we have represented it within the Alexina formalism, on which PerLex is based. We focus on the methodology we used for constructing lexical entries from various sources, as well as the problems related to typographic normalisation. The resulting lexicon shows a satisfying coverage on a reference corpus and should therefore be a good starting point for developing a syntactic lexicon for the Persian language.</abstract>
      <bibkey>sagot-walther-2010-morphological</bibkey>
    </paper>
    <paper id="487">
      <author><first>Benoît</first><last>Sagot</last></author>
      <title>The Lefff, a Freely Available and Large-coverage Morphological and Syntactic Lexicon for <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/701_Paper.pdf</url>
      <abstract>In this paper, we introduce the Lefff, a freely available, accurate and large-coverage morphological and syntactic lexicon for French, used in many NLP tools such as large-coverage parsers. We first describe Alexina, the lexical framework in which the Lefff is developed as well as the linguistic notions and formalisms it is based on. Next, we describe the various sources of lexical data we used for building the Lefff, in particular semi-automatic lexical development techniques and conversion and merging of existing resources. Finally, we illustrate the coverage and precision of the resource by comparing it with other resources and by assessing its impact in various NLP tools.</abstract>
      <bibkey>sagot-2010-lefff</bibkey>
    </paper>
    <paper id="488">
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Wauter</first><last>Bosma</last></author>
      <title>Integrating a Large Domain Ontology of Species into <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/703_Paper.pdf</url>
      <abstract>With the proliferation of applications sharing information represented in multiple ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. Connecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineering. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet in the framework of the KYOTO project. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts represented in Species 2000.</abstract>
      <bibkey>cuadros-etal-2010-integrating</bibkey>
    </paper>
    <paper id="489">
      <author><first>Jean-Luc</first><last>Rouas</last></author>
      <author><first>Mayumi</first><last>Beppu</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <title>Comparison of Spectral Properties of Read, Prepared and Casual Speech in <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/704_Paper.pdf</url>
      <abstract>In this paper, we investigate the acoustic properties of phonemes in three speaking styles: read speech, prepared speech and spontaneous speech. Our aim is to better understand why speech recognition systems still fails to achieve good performances on spontaneous speech. This work follows the work of Nakamura et al. on Japanese speaking styles, with the difference that we here focus on French. Using Nakamura's method, we use classical speech recognition features, MFCC, and try to represent the effects of the speaking styles on the spectral space. Two measurements are defined in order to represent the spectral space reduction and the spectral variance extension. Experiments are then carried on to investigate if indeed we find some differences between the three speaking styles using these measurements. We finally compare our results to those obtained by Nakamura on Japanese to see if the same phenomenon appears. We happen to find some cues, and it also seems that phone duration also plays an important role regarding spectral reduction, especially for spontaneous speech.</abstract>
      <bibkey>rouas-etal-2010-comparison</bibkey>
    </paper>
    <paper id="490">
      <author><first>Svetla</first><last>Koeva</last></author>
      <title>Lexicon and Grammar in <fixed-case>B</fixed-case>ulgarian <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/705_Paper.pdf</url>
      <abstract>In this paper, we report on our attempt at assigning semantic information from the English FrameNet to lexical units in the Bulgarian valency lexicon. The paper briefly presents the model underlying the Bulgarian FrameNet (BulFrameNet): each lexical entry consists of a lexical unit; a semantic frame from the English FrameNet, expressing abstract semantic structure; a grammatical class, defining the inflexional paradigm; a valency frame describing (some of) the syntactic and lexical-semantic combinatory properties (an optional component); and (semantically and syntactically) annotated examples. The target is a corpus-based lexicon giving an exhaustive account of the semantic and syntactic combinatory properties of an extensive number of Bulgarian lexical units. The Bulgarian FrameNet database so far contains unique descriptions of over 3 000 Bulgarian lexical units, approx. one tenth of them aligned with appropriate semantic frames, supports XML import and export and will be accessible, i.e., displayed and queried via the web.</abstract>
      <bibkey>koeva-2010-lexicon</bibkey>
    </paper>
    <paper id="491">
      <author><first>Peter</first><last>Spyns</last></author>
      <author><first>Elisabeth</first><last>D’Halleweyn</last></author>
      <title><fixed-case>F</fixed-case>lemish-<fixed-case>D</fixed-case>utch <fixed-case>HLT</fixed-case> Policy: Evolving to New Forms of Collaboration</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/707_Paper.pdf</url>
      <abstract>In the last decade, the Dutch Language Union has taken a serious interest in digital language resources and human language technologies (HLT), because they are crucial for a language to be able to survive in the information society. In this paper we report on the current state of the joint Flemish-Dutch efforts in the field of HLT for Dutch (HLTD) and how follow-up activities are being prepared. We explain the overall mechanism of evaluating an R&amp;D programme and the role of evaluation in the policy cycle to establish new R&amp;D funding activities. This is applied to the joint Flemish-Dutch STEVIN programme. Outcomes of the STEVIN scientific midterm review are shortly discussed as the overall final evaluation is currently still on-going. As part of preparing for future policy plans, an HLTD forecast is presented. Also new opportunities are outlined, in particular in the context of the European CLARIN infrastructure project that can lead to new avenues for joint Flemish-Dutch cooperation on HLTD.</abstract>
      <bibkey>spyns-dhalleweyn-2010-flemish</bibkey>
    </paper>
    <paper id="492">
      <author><first>Elisabetta</first><last>Jezek</last></author>
      <author><first>Valeria</first><last>Quochi</last></author>
      <title>Capturing Coercions in Texts: a First Annotation Exercise</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/713_Paper.pdf</url>
      <abstract>In this paper we report the first results of an annotation exercise of argument coercion phenomena performed on Italian texts. Our corpus consists of ca 4000 sentences from the PAROLE sottoinsieme corpus (Bindi et al. 2000) annotated with Selection and Coercion relations among verb-noun pairs formatted in XML according to the Generative Lexicon Mark-up Language (GLML) format (Pustejovsky et al., 2008). For the purposes of coercion annotation, we selected 26 Italian verbs that impose semantic typing on their arguments in either Subject, Direct Object or Complement position. Every sentence of the corpus is annotated with the source type for the noun arguments by two annotators plus a judge. An overall agreement of 0.87 kappa indicates that the annotation methodology is reliable. A qualitative analysis of the results allows us to outline some suggestions for improvement of the task: 1) a different account of complex types for nouns has to be devised and 2) a more comprehensive account of coercion mechanisms requires annotation of the deeper meaning dimensions that are targeted in coercion operations, such as those captured by Qualia relations.</abstract>
      <bibkey>jezek-quochi-2010-capturing</bibkey>
    </paper>
    <paper id="493">
      <author><first>Brigitte</first><last>Jörg</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Alastair</first><last>Burt</last></author>
      <title><fixed-case>LT</fixed-case> World: Ontology and Reference Information Portal</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/714_Paper.pdf</url>
      <abstract>LT World (www.lt-world.org) is an ontology-driven web portal aimed at serving the global language technology community. Ontology-driven means, that the system is driven by an ontological schema to manage the research information and knowledge life-cycles: identify relevant concepts of information, structure and formalize them, assign relationships, functions and views, add states and rules, modify them. For modelling such a complex structure, we employ (i) concepts from the research domain, such as person, organisation, project, tool, data, patent, news, event (ii) concepts from the LT domain, such as technology and resource (iii) concepts from closely related domains, such as language, linguistics, and mathematics. Whereas the research entities represent the general context, that is, a research environment as such, the LT entities define the information and knowledge space of the field, enhanced by entities from closely related areas. By managing information holistically ― that is, within a research context ― its inherent semantics becomes much more transparent. This paper introduces LT World as a reference information portal through ontological eyes: its content, its system, its method for maintaining knowledge-rich items, its ontology as an asset.</abstract>
      <bibkey>jorg-etal-2010-lt</bibkey>
    </paper>
    <paper id="494">
      <author><first>Thiago D.</first><last>Tadeu</last></author>
      <author><first>Eder M.</first><last>de Novais</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <title>Extracting Surface Realisation Templates from Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/715_Paper.pdf</url>
      <abstract>In Natural Language Generation (NLG), template-based surface realisation is an effective solution to the problem of producing surface strings from a given semantic representation, but many applications may not be able to provide the input knowledge in the required level of detail, which in turn may limit the use of the available NLG resources. However, if we know in advance what the most likely output sentences are (e.g., because a corpus on the relevant application domain happens to be available), then corpus knowledge may be used to quickly deploy a surface realisation engine for small-scale applications, for which it may be sufficient to select a sentence (in natural language) that resembles the desired output, and then modify some or all of its constituents accordingly. In other words, the application may simply 'point to' an existing sentence in the corpus and specify only the changes that need to take place to obtain the desired surface string. In this paper we describe one such approach to surface realisation, in which we extract syntactically-structured templates from a target corpus, and use these templates to produce existing and modified versions of the target sentences by a combination of canned text and basic dependency-tree operations.</abstract>
      <bibkey>tadeu-etal-2010-extracting</bibkey>
    </paper>
    <paper id="495">
      <author><first>Arif</first><last>Bramantoro</last></author>
      <author><first>Ulrich</first><last>Schäfer</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <title>Towards an Integrated Architecture for Composite Language Services and Multiple Linguistic Processing Components</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/717_Paper.pdf</url>
      <abstract>Web services are increasingly being used in the natural language processing community as a way to increase the interoperability amongst language resources. This paper extends our previous work on integrating two different platforms, i.e. Heart of Gold and Language Grid. The Language Grid is an infrastructure built on top of the Internet to provide distributed language services. Heart of Gold is known as middleware architecture for integrating deep and shallow natural language processing components. The new feature of the integrated architecture is the combination of composite language services in the Language Grid and the multiple linguistic processing components in Heart of Gold to provide a better quality of language resources available on the Web. Thus, language resources with different characteristics can be combined based on the concept of service oriented computing with different treatment for each combination. Having Heart of Gold fully integrated in the Language Grid environment would contribute to the heterogeneity of language services.</abstract>
      <bibkey>bramantoro-etal-2010-towards</bibkey>
    </paper>
    <paper id="496">
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <title>Maximum Entropy Classifier Ensembling using Genetic Algorithm for <fixed-case>NER</fixed-case> in <fixed-case>B</fixed-case>engali</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/718_Paper.pdf</url>
      <abstract>In this paper, we propose classifier ensemble selection for Named Entity Recognition (NER) as a single objective optimization problem. Thereafter, we develop a method based on genetic algorithm (GA) to solve this problem. Our underlying assumption is that rather than searching for the best feature set for a particular classifier, ensembling of several classifiers which are trained using different feature representations could be a more fruitful approach. Maximum Entropy (ME) framework is used to generate a number of classifiers by considering the various combinations of the available features. In the proposed approach, classifiers are encoded in the chromosomes. A single measure of classification quality, namely F-measure is used as the objective function. Evaluation results on a resource constrained language like Bengali yield the recall, precision and F-measure values of 71.14%, 84.07% and 77.11%, respectively. Experiments also show that the classifier ensemble identified by the proposed GA based approach attains higher performance than all the individual classifiers and two different conventional baseline ensembles.</abstract>
      <bibkey>ekbal-saha-2010-maximum</bibkey>
    </paper>
    <paper id="497">
      <author><first>Mohamed</first><last>Belgacem</last></author>
      <author><first>Georges</first><last>Antoniadis</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <title>Automatic Identification of <fixed-case>A</fixed-case>rabic Dialects</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/719_Paper.pdf</url>
      <abstract>In this work, automatic recognition of Arabic dialects is proposed. An acoustic survey of the proportion of vocalic intervals and the standard deviation of consonantal intervals in nine dialects (Tunisia, Morocco, Algeria, Egypt, Syria, Lebanon, Yemen, Golfs Countries and Iraq) is performed using the platform Alize and Gaussian Mixture Models (GMM). The results show the complexity of the automatic identification of Arabic dialects since. No clear border can be found between the dialects, but a gradual transition between them. They can even vary slightly from one city to another. The existence of this gradual change is easy to understand: it corresponds to a human and social reality, to the contact, friendships forged and affinity in the environment more or less immediate of the individual. This document also raises questions about the classes or macro classes of Arabic dialects noticed from the confusion matrix and the design of the hierarchical tree obtained.</abstract>
      <bibkey>belgacem-etal-2010-automatic</bibkey>
    </paper>
    <paper id="498">
      <author><first>Sathish</first><last>Pammi</last></author>
      <author><first>Marcela</first><last>Charfuelan</last></author>
      <author><first>Marc</first><last>Schröder</last></author>
      <title>Multilingual Voice Creation Toolkit for the <fixed-case>MARY</fixed-case> <fixed-case>TTS</fixed-case> Platform</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/720_Paper.pdf</url>
      <abstract>This paper describes an open source voice creation toolkit that supports the creation of unit selection and HMM-based voices, for the MARY (Modular Architecture for Research on speech Synthesis) TTS platform. We aim to provide the tools and generic reusable run-time system modules so that people interested in supporting a new language and creating new voices for MARY TTS can do so. The toolkit has been successfully applied to the creation of British English, Turkish, Telugu and Mandarin Chinese language components and voices. These languages are now supported by MARY TTS as well as German and US English. The toolkit can be easily employed to create voices in the languages already supported by MARY TTS. The voice creation toolkit is mainly intended to be used by research groups on speech technology throughout the world, notably those who do not have their own pre-existing technology yet. We try to provide them with a reusable technology that lowers the entrance barrier for them, making it easier to get started. The toolkit is developed in Java and includes intuitive Graphical User Interface (GUI) for most of the common tasks in the creation of a synthetic voice.</abstract>
      <bibkey>pammi-etal-2010-multilingual</bibkey>
    </paper>
    <paper id="499">
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Laska</first><last>Laskova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <title>Exploring Co-Reference Chains for Concept Annotation of Domain Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/721_Paper.pdf</url>
      <abstract>The paper explores the co-reference chains as a way for improving the density of concept annotation over domain texts. The idea extends authors previous work on relating the ontology to the text terms in two domains ― IT and textile. Here IT domain is used. The challenge is to enhance relations among concepts instead of text entities, the latter pursued in most works. Our ultimate goal is to exploit these additional chains for concept disambiguation as well as sparseness resolution at concept level. First, a gold standard was prepared with manually connected links among concepts, anaphoric pronouns and contextual equivalents. This step was necessary not only for test purposes, but also for better orientation in the co-referent types and distribution. Then, two automatic systems were tested on the gold standard. Note that these systems were not designed specially for concept chaining. The conclusion is that the state-of-the-art co-reference resolution systems might address the concept sparseness problem, but not so much the concept disambiguation task. For the latter, word-sense disambiguation systems have to be integrated.</abstract>
      <bibkey>osenova-etal-2010-exploring</bibkey>
    </paper>
    <paper id="500">
      <author><first>Kathrin</first><last>Spreyer</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>Training Parsers on Partial Trees: A Cross-language Comparison</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/722_Paper.pdf</url>
      <abstract>We present a study that compares data-driven dependency parsers obtained by means of annotation projection between language pairs of varying structural similarity. We show how the partial dependency trees projected from English to Dutch, Italian and German can be exploited to train parsers for the target languages. We evaluate the parsers against manual gold standard annotations and find that the projected parsers substantially outperform our heuristic baselines by 9―25% UAS, which corresponds to a 21―43% reduction in error rate. A comparative error analysis focuses on how the projected target language parsers handle subjects, which is especially interesting for Italian as an instance of a pro-drop language. For Dutch, we further present experiments with German as an alternative source language. In both source languages, we contrast standard baseline parsers with parsers that are enhanced with the predictions from large-scale LFG grammars through a technique of parser stacking, and show that improvements of the source language parser can directly lead to similar improvements of the projected target language parser.</abstract>
      <bibkey>spreyer-etal-2010-training</bibkey>
    </paper>
    <paper id="501">
      <author><first>Peter</first><last>Menke</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <title>The Ariadne System: A Flexible and Extensible Framework for the Modeling and Storage of Experimental Data in the Humanities.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/723_Paper.pdf</url>
      <abstract>During the last decades, interdisciplinarity has become a central keyword in research. As a consequence, many concepts, theories and scientific methods get in contact with each other, resulting in many different strategies and variants of acquiring, structuring, and sharing data sets. To handle these kind of data sets, his paper introduces the Ariadne Corpus Management System that allows researchers to manage and create multimodal corpora from multiple heteogeneous data sources. After an introductory demarcation from other annotation and corpus management tools, the underlying data model is presented which enables users to represent and process heterogeneous data sets within a single, consistent framework. Secondly, a set of automatized procedures is described that offers assistance to researchers in various data-related use cases. Thirdly, an approach to easy yet powerful data retrieval is introduced in form of a specialised querying language for multimodal data. Finally, the web-based graphical user interface and its advantages are illustrated.</abstract>
      <bibkey>menke-mehler-2010-ariadne</bibkey>
    </paper>
    <paper id="502">
      <author><first>Alessandra</first><last>Giordani</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <title>Corpora for Automatically Learning to Map Natural Language Questions into <fixed-case>SQL</fixed-case> Queries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/724_Paper.pdf</url>
      <abstract>Automatically translating natural language into machine-readable instructions is one of major interesting and challenging tasks in Natural Language (NL) Processing. This problem can be addressed by using machine learning algorithms to generate a function that find mappings between natural language and programming language semantics. For this purpose suitable annotated and structured data are required. In this paper, we describe our method to construct and semi-automatically annotate these kinds of data, consisting of pairs of NL questions and SQL queries. Additionally, we describe two different datasets obtained by applying our annotation method to two well-known corpora, GeoQueries and RestQueries. Since we believe that syntactic levels are important, we also generate and make available relational pairs represented by means of their syntactic trees whose lexical content has been generalized. We validate the quality of our corpora by experimenting with them and our machine learning models to derive automatic NL/SQL translators. Our promising results suggest that our corpora can be effectively used to carry out research in the field of natural language interface to database.</abstract>
      <bibkey>giordani-moschitti-2010-corpora</bibkey>
    </paper>
    <paper id="503">
      <author><first>Naoki</first><last>Ishikawa</last></author>
      <author><first>Ryo</first><last>Nishimura</last></author>
      <author><first>Yasuhiko</first><last>Watanabe</last></author>
      <author><first>Yoshihiro</first><last>Okada</last></author>
      <author><first>Masaki</first><last>Murata</last></author>
      <title>Detection of submitters suspected of pretending to be someone else in a community site</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/727_Paper.pdf</url>
      <abstract>One of the essential factors in community sites is anonymous submission. This is because anonymity gives users chances to submit messages (questions, problems, answers, opinions, etc.) without regard to shame and reputation. However, some users abuse the anonymity and disrupt communications in a community site. These users and their submissions discourage other users, keep them from retrieving good communication records, and decrease the credibility of the communication site. To solve this problem, we conducted an experimental study to detect submitters suspected of pretending to be someone else to manipulate communications in a community site by using machine learning techniques. In this study, we used messages in the data of Yahoo! chiebukuro for data training and examination.</abstract>
      <bibkey>ishikawa-etal-2010-detection</bibkey>
    </paper>
    <paper id="504">
      <author><first>Fabienne</first><last>Fritzinger</last></author>
      <author><first>Marion</first><last>Weller</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <title>A Survey of Idiomatic Preposition-Noun-Verb Triples on Token Level</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/728_Paper.pdf</url>
      <abstract>Most of the research on the extraction of idiomatic multiword expressions (MWEs) focused on the acquisition of MWE types. In the present work we investigate whether a text instance of a potentially idiomatic MWE is actually used idiomatically in a given context or not. Inspired by the dataset provided by (Cook et al., 2008), we manually analysed 9,700 instances of potentially idiomatic prepositionnoun- verb triples (a frequent pattern among German MWEs) to identify, on token level, idiomatic vs. literal uses. In our dataset, all sentences are provided along with their morpho-syntactic properties. We describe our data extraction and annotation steps, and we discuss quantitative results from both EUROPARL and a German newspaper corpus. We discuss the relationship between idiomaticity and morpho-syntactic fixedness, and we address issues of ambiguity between literal and idiomatic use of MWEs. Our data show that EUROPARL is particularly well suited for MWE extraction, as most MWEs in this corpus are indeed used only idiomatically.</abstract>
      <bibkey>fritzinger-etal-2010-survey</bibkey>
    </paper>
    <paper id="505">
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <author><first>Chris</first><last>Manning</last></author>
      <title>Parsing to <fixed-case>S</fixed-case>tanford Dependencies: Trade-offs between Speed and Accuracy</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/730_Paper.pdf</url>
      <abstract>We investigate a number of approaches to generating Stanford Dependencies, a widely used semantically-oriented dependency representation. We examine algorithms specifically designed for dependency parsing (Nivre, Nivre Eager, Covington, Eisner, and RelEx) as well as dependencies extracted from constituent parse trees created by phrase structure parsers (Charniak, Charniak-Johnson, Bikel, Berkeley and Stanford). We found that constituent parsers systematically outperform algorithms designed specifically for dependency parsing. The most accurate method for generating dependencies is the Charniak-Johnson reranking parser, with 89% (labeled) attachment F1 score. The fastest methods are Nivre, Nivre Eager, and Covington, used with a linear classifier to make local parsing decisions, which can parse the entire Penn Treebank development set (section 22) in less than 10 seconds on an Intel Xeon E5520. However, this speed comes with a substantial drop in F1 score (about 76% for labeled attachment) compared to competing methods. By tuning how much of the search space is explored by the Charniak-Johnson parser, we are able to arrive at a balanced configuration that is both fast and nearly as good as the most accurate approaches.</abstract>
      <bibkey>cer-etal-2010-parsing</bibkey>
    </paper>
    <paper id="506">
      <author><first>Antonio</first><last>Reyes</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <title>Evaluating Humour Features on Web Comments</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/731_Paper.pdf</url>
      <abstract>Research on automatic humor recognition has developed several features which discriminate funny text from ordinary text. The features have been demonstrated to work well when classifying the funniness of single sentences up to entire blogs. In this paper we focus on evaluating a set of the best humor features reported in the literature over a corpus retrieved from the Slashdot Web site. The corpus is categorized in a community-driven process according to the following tags: funny, informative, insightful, offtopic, flamebait, interesting and troll. These kinds of comments can be found on almost every large Web site; therefore, they impose a new challenge to humor retrieval since they come along with unique characteristics compared to other text types. If funny comments were retrieved accurately, they would be of a great entertainment value for the visitors of a given Web page. Our objective, thus, is to distinguish between an implicit funny comment from a not funny one. Our experiments are preliminary but nonetheless large-scale: 600,000 Web comments. We evaluate the classification accuracy of naive Bayes classifiers, decision trees, and support vector machines. The results suggested interesting findings.</abstract>
      <bibkey>reyes-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="507">
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>Acquiring Reliable Predicate-argument Structures from Raw Corpora for Case Frame Compilation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/733_Paper.pdf</url>
      <abstract>We present a method for acquiring reliable predicate-argument structures from raw corpora for automatic compilation of case frames. Such lexicon compilation requires highly reliable predicate-argument structures to practically contribute to Natural Language Processing (NLP) applications, such as paraphrasing, text entailment, and machine translation. However, to precisely identify predicate-argument structures, case frames are required. This issue is similar to the question ""what came first: the chicken or the egg?"" In this paper, we propose the first step in the extraction of reliable predicate-argument structures without using case frames. We first apply chunking to raw corpora and then extract reliable chunks to ensure that high-quality predicate-argument structures are obtained from the chunks. We conducted experiments to confirm the effectiveness of our approach. We successfully extracted reliable chunks of an accuracy of 98% and high-quality predicate-argument structures of an accuracy of 97%. Our experiments confirmed that we succeeded in acquiring highly reliable predicate-argument structures that can be used to compile case frames.</abstract>
      <bibkey>kawahara-kurohashi-2010-acquiring</bibkey>
    </paper>
    <paper id="508">
      <author><first>Emiliano</first><last>Giovannetti</last></author>
      <title>An Unsupervised Approach for Semantic Relation Interpretation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/734_Paper.pdf</url>
      <abstract>In this work we propose a hybrid unsupervised approach for semantic relation extraction from Italian and English texts. The system takes as input pairs of ""distributionally similar"" terms, possibly involved in a semantic relation. To validate and label the anonymous relations holding between the terms in input, the candidate pairs of terms are looked for on the Web in the context of reliable lexico-syntactic patterns. This paper focuses on the definition of the patterns, on the measures used to assess the reliability of the suggested specific semantic relation and on the evaluation of the implemented system. So far, the system is able to extract the following types of semantic relations: hyponymy, meronymy, and co-hyponymy. The approach can however be easily extended to manage other relations by defining the appropriate battery of reliable lexico-syntactic patterns. Accuracy of the system was measured with scores of 83.3% for hyponymy, 75% for meronymy and 72.2% for co-hyponymy extraction.</abstract>
      <bibkey>giovannetti-2010-unsupervised</bibkey>
    </paper>
    <paper id="509">
      <author><first>Oi Yee</first><last>Kwong</last></author>
      <title>Constructing an Annotated Story Corpus: Some Observations and Issues</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/736_Paper.pdf</url>
      <abstract>This paper discusses our ongoing work on constructing an annotated corpus of childrens stories for further studies on the linguistic, computational, and cognitive aspects of story structure and understanding. Given its semantic nature and the need for extensive common sense and world knowledge, story understanding has been a notoriously difficult topic in natural language processing. In particular, the notion of story structure for maintaining coherence has received much attention, while its strong version in the form of story grammar has triggered much debate. The relation between discourse coherence and the interestingness, or the point, of a story has not been satisfactorily settled. Introspective analysis on story comprehension has led to some important observations, based on which we propose a preliminary annotation scheme covering the structural, functional, and emotional aspects connecting discourse segments in stories. The annotation process will shed light on how story structure interacts with story point via various linguistic devices, and the annotated corpus is expected to be a useful resource for computational discourse processing, especially for studying various issues regarding the interface between coherence and interestingness of stories.</abstract>
      <bibkey>kwong-2010-constructing</bibkey>
    </paper>
    <paper id="510">
      <author><first>Klaar</first><last>Vanopstal</last></author>
      <author><first>Bart</first><last>Desmet</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Towards a Learning Approach for Abbreviation Detection and Resolution.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/737_Paper.pdf</url>
      <abstract>The explosion of biomedical literature and with it the -uncontrolled- creation of abbreviations presents some special challenges for both human readers and computer applications. We developed an annotated corpus of Dutch medical text, and experimented with two approaches to abbreviation detection and resolution. Our corpus is composed of abstracts from two medical journals from the Low Countries in which approximately 65 percent (NTvG) and 48 percent (TvG) of the abbreviations have a corresponding full form in the abstract. Our first approach, a pattern-based system, consists of two steps: abbreviation detection and definition matching. This system has an average F-score of 0.82 for the detection of both defined and undefined abbreviations and an average F-score of 0.77 was obtained for the definitions. For our second approach, an SVM-based classifier was used on the preprocessed data sets, leading to an average F-score of 0.93 for the abbreviations; for the definitions an average F-score of 0.82 was obtained.</abstract>
      <bibkey>vanopstal-etal-2010-towards</bibkey>
    </paper>
    <paper id="511">
      <author><first>Catarina</first><last>Magro</last></author>
      <title>When <fixed-case>CORDIAL</fixed-case> Becomes Friendly: Endowing the <fixed-case>CORDIAL</fixed-case> Corpus with a Syntactic Annotation Layer</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/738_Paper.pdf</url>
      <abstract>This paper reports on the syntactic annotation of a previously compiled and tagged corpus of European Portuguese (EP) dialects ― The Syntax-oriented Corpus of Portuguese Dialects (CORDIAL-SIN). The parsed version of CORDIAL-SIN is intended to be a more efficient resource for the purpose of studying dialect syntax by allowing automated searches for various syntactic constructions of interest. To achieve this goal we adopted a rich annotation system (the UPenn corpora annotation system) which codifies syntactic information of high relevance. The annotation produces tree representations, in form of labelled parenthesis, that are integrally searchable with CorpusSearch, a search engine for parsed corpora (Randall, 2005-2007). The present paper focuses on CORDIAL-SIN annotation issues, namely it presents the general principles and guidelines of the adopted annotation system and describes the methodology for constructing the parsed version of the corpus and for searching it (tools and procedures). Last section addresses the question of how an annotation system originally designed for Middle English can be adapted to meet the particular needs of a Portuguese corpus of dialectal speech.</abstract>
      <bibkey>magro-2010-cordial</bibkey>
    </paper>
    <paper id="512">
      <author><first>Mridul</first><last>Gupta</last></author>
      <author><first>Vineet</first><last>Yadav</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <title>Partial Parsing as a Method to Expedite Dependency Annotation of a <fixed-case>H</fixed-case>indi Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/739_Paper.pdf</url>
      <abstract>The paper describes an approach to expedite the process of manual annotation of a Hindi dependency treebank which is currently under development. We propose a way by which consistency among a set of manual annotators could be improved. Furthermore, we show that our setup can also prove useful for evaluating when an inexperienced annotator is ready to start participating in the production of the treebank. We test our approach on sample sets of data obtained from an ongoing work on creation of this treebank. The results asserting our proposal are reported in this paper. We report results from a semi-automated approach of dependency annotation experiment. We find out the rate of agreement between annotators using Cohens Kappa. We also compare results with respect to the total time taken to annotate sample data-sets using a completely manual approach as opposed to a semi-automated approach. It is observed from the results that this semi-automated approach when carried out with experienced and trained human annotators improves the overall quality of treebank annotation and also speeds up the process.</abstract>
      <bibkey>gupta-etal-2010-partial</bibkey>
    </paper>
    <paper id="513">
      <author><first>Marc</first><last>Verhagen</last></author>
      <title>The <fixed-case>B</fixed-case>randeis Annotation Tool</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/740_Paper.pdf</url>
      <abstract>The Brandeis Annotation Tool (BAT) is a web-based text annotation tool that is centered around the notions of layered annotation and task decomposition. It allows annotations to refer to other annotations and to take a complicated task and split it into easier subtasks. The central organizing concept of BAT is the annotation layer. A corpus administrator can create annotation layers that involve annotation of extents, attributes or relations. The layer definition includes the labels used, the attributes that are available and restrictions on the values for those attributes. For each annotation layer, files can be assigned to one or more annotators and one judge. When annotators log in, the assigned layers and files therein are presented. When selecting a file to annotate, the interface uses the layer definition to display the annotation interface. The web-interface connects administrators and annotators to a central repository for all data and simplifies many of the housekeeping tasks while keeping requirements at a minimum (that is, users only need an internet connection and a well-behaved browser). BAT has been used mainly for temporal annotation, but can be considered a more general tool for several kinds of textual annotation.</abstract>
      <bibkey>verhagen-2010-brandeis</bibkey>
    </paper>
    <paper id="514">
      <author><first>Iker</first><last>Luengo</last></author>
      <author><first>Eva</first><last>Navas</last></author>
      <author><first>Igor</first><last>Odriozola</last></author>
      <author><first>Ibon</first><last>Saratxaga</last></author>
      <author><first>Inmaculada</first><last>Hernaez</last></author>
      <author><first>Iñaki</first><last>Sainz</last></author>
      <author><first>Daniel</first><last>Erro</last></author>
      <title>Modified <fixed-case>LTSE</fixed-case>-<fixed-case>VAD</fixed-case> Algorithm for Applications Requiring Reduced Silence Frame Misclassification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/741_Paper.pdf</url>
      <abstract>The LTSE-VAD is one of the best known algorithms for voice activity detection. In this paper we present a modified version of this algorithm, that makes the VAD decision not taking into account account the estimated background noise level, but the signal to noise ratio (SNR). This makes the algorithm robust not only to noise level changes, but also to signal level changes. We compare the modified algorithm with the original one, and with three other standard VAD systems. The results show that the modified version gets the lowest silence misclassification rate, while maintaining a reasonably low speech misclassification rate. As a result, this algorithm is more suitable for identification tasks, such as speaker or emotion recognition, where silence misclassification can be very harmful. A series of automatic emotion identification experiments are also carried out, proving that the modified version of the algorithm helps increasing the correct emotion classification rate.</abstract>
      <bibkey>luengo-etal-2010-modified</bibkey>
    </paper>
    <paper id="515">
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>Keith</first><last>Suderman</last></author>
      <author><first>Brian</first><last>Simms</last></author>
      <title><fixed-case>ANC</fixed-case>2<fixed-case>G</fixed-case>o: A Web Application for Customized Corpus Creation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/745_Paper.pdf</url>
      <abstract>We describe a web application called ANC2Go that enables the user to select data from the Open American National Corpus (OANC) and the Manually Annotated Sub-corpus (MASC) together with some or all of the annotations available. The user also may select from among a variety of options for output format, or may receive the selected portions of the corpus and annotations in their original GrAF XML standoff format.. The request is processed by merging the annotations selected and rendering them in the desired output format, then bundling the results and making it available for download. Thus, users can create a customized corpus with data and annotations of their choosing, delivered in the format that is most convenient for their use. ANC2Go will be released as a web service in the near future. Both the OANC and MASC are freely available for any use from the American National Corpus website and may be accessed through the ANC2Go application, or they may downloaded in their entirety.</abstract>
      <bibkey>ide-etal-2010-anc2go</bibkey>
    </paper>
    <paper id="516">
      <author><first>Shunsuke</first><last>Kozawa</last></author>
      <author><first>Hitomi</first><last>Tohyama</last></author>
      <author><first>Kiyotaka</first><last>Uchimoto</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <title>Collection of Usage Information for Language Resources from Academic Articles</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/746_Paper.pdf</url>
      <abstract>Recently, language resources (LRs) are becoming indispensable for linguistic researches. However, existing LRs are often not fully utilized because their variety of usage is not well known, indicating that their intrinsic value is not recognized very well either. Regarding this issue, lists of usage information might improve LR searches and lead to their efficient use. In this research, therefore, we collect a list of usage information for each LR from academic articles to promote the efficient utilization of LRs. This paper proposes to construct a text corpus annotated with usage information (UI corpus). In particular, we automatically extract sentences containing LR names from academic articles. Then, the extracted sentences are annotated with usage information by two annotators in a cascaded manner. We show that the UI corpus contributes to efficient LR searches by combining the UI corpus with a metadata database of LRs and comparing the number of LRs retrieved with and without the UI corpus.</abstract>
      <bibkey>kozawa-etal-2010-collection</bibkey>
    </paper>
    <paper id="517">
      <author><first>Nick</first><last>Rizzolo</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <title>Learning Based <fixed-case>J</fixed-case>ava for Rapid Development of <fixed-case>NLP</fixed-case> Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/747_Paper.pdf</url>
      <abstract>Today's natural language processing systems are growing more complex with the need to incorporate a wider range of language resources and more sophisticated statistical methods. In many cases, it is necessary to learn a component with input that includes the predictions of other learned components or to assign simultaneously the values that would be assigned by multiple components with an expressive, data dependent structure among them. As a result, the design of systems with multiple learning components is inevitably quite technically complex, and implementations of conceptually simple NLP systems can be time consuming and prone to error. Our new modeling language, Learning Based Java (LBJ), facilitates the rapid development of systems that learn and perform inference. LBJ has already been used to build state of the art NLP systems. In this paper, we first demonstrate that there exists a theoretical model that describes most NLP approaches adeptly. Second, we show how our improvements to the LBJ language enable the programmer to describe the theoretical model succinctly. Finally, we introduce the concept of data driven compilation, a translation process in which the efficiency of the generated code benefits from the data given as input to the learning algorithms.</abstract>
      <bibkey>rizzolo-roth-2010-learning</bibkey>
    </paper>
    <paper id="518">
      <author><first>Jorge</first><last>Vivaldi</last></author>
      <author><first>Horacio</first><last>Rodríguez</last></author>
      <title>Finding Domain Terms using <fixed-case>W</fixed-case>ikipedia</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/748_Paper.pdf</url>
      <abstract>In this paper we present a new approach for obtaining the terminology of a given domain using the category and page structures of the Wikipedia in a language independent way. Our approach consists basically, for each domain, on navigating the Category graph of the Wikipedia starting from the root nodes associated to the domain. A heavy filtering mechanism is carried out for preventing as much as possible the inclusion of spurious categories. For each selected category all the pages belonging to it are then recovered and filtered. This procedure is iterate several times until achieving convergence. Both category names and page names are considered candidates to belong to the terminology of the domain. This approach has been applied to three broad coverage domains: astronomy, chemistry and medicine, and two languages, English and Spanish, showing a promising performance.</abstract>
      <bibkey>vivaldi-rodriguez-2010-finding</bibkey>
    </paper>
    <paper id="519">
      <author><first>Claire</first><last>Brierley</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <title><fixed-case>P</fixed-case>ro<fixed-case>POSEC</fixed-case>: A Prosody and <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> Annotated Spoken <fixed-case>E</fixed-case>nglish Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/749_Paper.pdf</url>
      <abstract>We have previously reported on ProPOSEL, a purpose-built Prosody and PoS English Lexicon compatible with the Python Natural Language ToolKit. ProPOSEC is a new corpus research resource built using this lexicon, intended for distribution with the Aix-MARSEC dataset. ProPOSEC comprises multi-level parallel annotations, juxtaposing prosodic and syntactic information from different versions of the Spoken English Corpus, with canonical dictionary forms, in a query format optimized for Perl, Python, and text processing programs. The order and content of fields in the text file is as follows: (1) Aix-MARSEC file number; (2) word; (3) LOB PoS-tag; (4) C5 PoS-tag; (5) Aix SAM-PA phonetic transcription; (6) SAM-PA phonetic transcription from ProPOSEL; (7) syllable count; (8) lexical stress pattern; (9) default content or function word tag; (10) DISC stressed and syllabified phonetic transcription; (11) alternative DISC representation, incorporating lexical stress pattern; (12) nested arrays of phonemes and tonic stress marks from Aix. As an experimental dataset, ProPOSEC can be used to study correlations between these annotation tiers, where significant findings are then expressed as additional features for phrasing models integral to Text-to-Speech and Speech Recognition. As a training set, ProPOSEC can be used for machine learning tasks in Information Retrieval and Speech Understanding systems.</abstract>
      <bibkey>brierley-atwell-2010-proposec</bibkey>
    </paper>
    <paper id="520">
      <author><first>Margarita Alonso</first><last>Ramos</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <author><first>Orsolya</first><last>Vincze</last></author>
      <author><first>Gerard Casamayor</first><last>del Bosque</last></author>
      <author><first>Nancy Vázquez</first><last>Veiga</last></author>
      <author><first>Estela Mosqueira</first><last>Suárez</last></author>
      <author><first>Sabela Prieto</first><last>González</last></author>
      <title>Towards a Motivated Annotation Schema of Collocation Errors in Learner Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/751_Paper.pdf</url>
      <abstract>Collocations play a significant role in second language acquisition. In order to be able to offer efficient support to learners, an NLP-based CALL environment for learning collocations should be based on a representative collocation error annotated learner corpus. However, so far, no theoretically-motivated collocation error tag set is available. Existing learner corpora tag collocation errors simply as lexical errors ― which is clearly insufficient given the wide range of different collocation errors that the learners make. In this paper, we present a fine-grained three-dimensional typology of collocation errors that has been derived in an empirical study from the learner corpus CEDEL2 compiled by a team at the Autonomous University of Madrid. The first dimension captures whether the error concerns the collocation as a whole or one of its elements; the second dimension captures the language-oriented error analysis, while the third exemplifies the interpretative error analysis. To facilitate a smooth annotation along this typology, we adapted Knowtator, a flexible off-the-shelf annotation tool implemented as a Protégé plugin.</abstract>
      <bibkey>ramos-etal-2010-towards</bibkey>
    </paper>
    <paper id="521">
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Dekai</first><last>Wu</last></author>
      <title>Evaluating Machine Translation Utility via Semantic Role Labels</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/752_Paper.pdf</url>
      <abstract>We present the methodology that underlies mew metrics for semantic machine translation evaluation we are developing. Unlike widely-used lexical and n-gram based MT evaluation metrics, the aim of semantic MT evaluation is to measure the utility of translations. We discuss the design of empirical studies to evaluate the utility of machine translation output by assessing the accuracy for key semantic roles. These roles are from the English 5W templates (who, what, when, where, why) used in recent GALE distillation evaluations. Recent work by Wu and Fung (2009) introduced semantic role labeling into statistical machine translation to enhance the quality of MT output. However, this approach has so far only been evaluated using lexical and n-gram based SMT evaluation metrics like BLEU which are not aimed at evaluating the utility of MT output. Direct data analysis are still needed to understand how semantic models can be leveraged to evaluate the utility of MT output. In this paper, we discuss a new methodology for evaluating the utility of the machine translation output, by assessing the accuracy with which human readers are able to complete the English 5W templates.</abstract>
      <bibkey>lo-wu-2010-evaluating</bibkey>
    </paper>
    <paper id="522">
      <author><first>Yu</first><last>Chen</last></author>
      <author><first>Andreas</first><last>Eisele</last></author>
      <title>Integrating a Rule-based with a Hierarchical Translation System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/754_Paper.pdf</url>
      <abstract>Recent developments on hybrid systems that combine rule-based machine translation (RBMT) systems with statistical machine translation (SMT) generally neglect the fact that RBMT systems tend to produce more syntactically well-formed translations than data-driven systems. This paper proposes a method that alleviates this issue by preserving more useful structures produced by RBMT systems and utilizing them in a SMT system that operates on hierarchical structures instead of flat phrases alone. For our experiments, we use Joshua as the decoder. It is the first attempt towards a tighter integration of MT systems from different paradigms that both support hierarchical analysis. Preliminary results show consistent improvements over the previous approach.</abstract>
      <bibkey>chen-eisele-2010-integrating</bibkey>
    </paper>
    <paper id="523">
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Olga</first><last>Uryupina</last></author>
      <author><first>Yannick</first><last>Versley</last></author>
      <title>Creating a Coreference Resolution System for <fixed-case>I</fixed-case>talian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/755_Paper.pdf</url>
      <abstract>This paper summarizes our work on creating a full-scale coreference resolution (CR) system for Italian, using BART ― an open-source modular CR toolkit initially developed for English corpora. We discuss our experiments on language-specific issues of the task. As our evaluation experiments show, a language-agnostic system (designed primarily for English) can achieve a performance level in high forties (MUC F-score) when re-trained and tested on a new language, at least on gold mention boundaries. Compared to this level, we can improve our F-score by around 10% introducing a small number of language-specific changes. This shows that, with a modular coreference resolution platform, such as BART, one can straightforwardly develop a family of robust and reliable systems for various languages. We hope that our experiments will encourage researchers working on coreference in other languages to create their own full-scale coreference resolution systems ― as we have mentioned above, at the moment such modules exist only for very few languages other than English.</abstract>
      <bibkey>poesio-etal-2010-creating</bibkey>
    </paper>
    <paper id="524">
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Pavel</first><last>Straňák</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <title>Data Issues in <fixed-case>E</fixed-case>nglish-to-<fixed-case>H</fixed-case>indi Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/756_Paper.pdf</url>
      <abstract>Statistical machine translation to morphologically richer languages is a challenging task and more so if the source and target languages differ in word order. Current state-of-the-art MT systems thus deliver mediocre results. Adding more parallel data often helps improve the results; if it doesn't, it may be caused by various problems such as different domains, bad alignment or noise in the new data. In this paper we evaluate the English-to-Hindi MT task from this data perspective. We discuss several available parallel data sources and provide cross-evaluation results on their combinations using two freely available statistical MT systems. We demonstrate various problems encountered in the data and describe automatic methods of data cleaning and normalization. We also show that the contents of two independently distributed data sets can unexpectedly overlap, which negatively affects translation quality. Together with the error analysis, we also present a new tool for viewing aligned corpora, which makes it easier to detect difficult parts in the data even for a developer not speaking the target language.</abstract>
      <bibkey>bojar-etal-2010-data</bibkey>
    </paper>
    <paper id="525">
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <title>Belgisch Staatsblad Corpus: Retrieving <fixed-case>F</fixed-case>rench-<fixed-case>D</fixed-case>utch Sentences from Official Documents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/758_Paper.pdf</url>
      <abstract>We describe the compilation of a large corpus of French-Dutch sentence pairs from official Belgian documents which are available in the online version of the publication Belgisch Staatsblad/Moniteur belge, and which have been published between 1997 and 2006. After downloading files in batch, we filtered out documents which have no translation in the other language, documents which contain several languages (by checking on discriminating words), and pairs of documents with a substantial difference in length. We segmented the documents into sentences and aligned the latter, which resulted in 5 million sentence pairs (only one-to-one links were included in the parallel corpus); there are 2.4 million unique pairs. Sample-based evaluation of the sentence alignment results indicates a near 100% accuracy, which can be explained by the text genre, the procedure filtering out weakly parallel articles and the restriction to one-to-one links. The corpus is larger than a number of well-known French-Dutch resources. It is made available to the community. Further investigation is needed in order to determine the original language in which documents were written.</abstract>
      <bibkey>vanallemeersch-2010-belgisch</bibkey>
    </paper>
    <paper id="526">
      <author><first>Šárka</first><last>Zikánová</last></author>
      <author><first>Lucie</first><last>Mladová</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Pavlína</first><last>Jínová</last></author>
      <title>Typical Cases of Annotators’ Disagreement in Discourse Annotations in <fixed-case>P</fixed-case>rague Dependency Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/762_Paper.pdf</url>
      <abstract>In this paper, we present the first results of the parallel Czech discourse annotation in the Prague Dependency Treebank 2.0. Having established an annotation scenario for capturing semantic relations crossing the sentence boundary in a discourse, and having annotated the first sections of the treebank according to these guidelines, we report now on the results of the first evaluation of these manual annotations. We give an overview of the process of the annotation itself, which we believe is to a large degree language-independent and therefore accessible to any discourse researcher. Next, we describe the inter-annotator agreement measurement, and, most importantly, we classify and analyze the most common types of annotators disagreement and propose solutions for the next phase of the annotation. The annotation is carried out on dependency trees (on the tectogrammatical layer), this approach is quite novel and it brings us some advantages when interpreting the syntactic structure of the discourse units.</abstract>
      <bibkey>zikanova-etal-2010-typical</bibkey>
    </paper>
    <paper id="527">
      <author><first>Yu</first><last>Fu</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <title>Determining the Origin and Structure of Person Names</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/763_Paper.pdf</url>
      <abstract>This paper presents a novel system HENNA (Hybrid Person Name Analyzer) for identifying language origin and analyzing linguistic structures of person names. We conduct ME-based classification methods for the language origin identification and achieve very promising performance. We will show that word-internal character sequences provide surprisingly strong evidence for predicting the language origin of person names. Our approach is context-, language- and domain-independent and can thus be easily adapted to person names in or from other languages. Furthermore, we provide a novel strategy to handle origin ambiguities or multiple origins in a name. HENNA also provides a person name parser for the analysis of linguistic and knowledge structures of person names. All the knowledge about a person name in HENNA is modelled in a person-name ontology, including relationships between language origins, linguistic features and grammars of person names of a specific language and interpretation of name elements. The approaches presented here are useful extensions of the named entity recognition task.</abstract>
      <bibkey>fu-etal-2010-determining</bibkey>
    </paper>
    <paper id="528">
      <author><first>Arndt</first><last>Riester</last></author>
      <author><first>David</first><last>Lorenz</last></author>
      <author><first>Nina</first><last>Seemann</last></author>
      <title>A Recursive Annotation Scheme for Referential Information Status</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/764_Paper.pdf</url>
      <abstract>We provide a robust and detailed annotation scheme for information status, which is easy to use, follows a semantic rather than cognitive motivation, and achieves reasonable inter-annotator scores. Our annotation scheme is based on two main assumptions: firstly, that information status strongly depends on (in)definiteness, and secondly, that it ought to be understood as a property of referents rather than words. Therefore, our scheme banks on overt (in)definiteness marking and provides different categories for each class. Definites are grouped according to the information source by which the referent is identified. A special aspect of the scheme is that non-anaphoric expressions (e.g.\ names) are classified as to whether their referents are likely to be known or unknown to an expected audience. The annotation scheme provides a solution for annotating complex nominal expressions which may recursively contain embedded expressions. In annotating a corpus of German radio news bulletins, a kappa score of .66 for the full scheme was achieved, a core scheme of six top-level categories yields kappa = .78.</abstract>
      <bibkey>riester-etal-2010-recursive</bibkey>
    </paper>
    <paper id="529">
      <author><first>Jaouad</first><last>Mousser</last></author>
      <title>A Large Coverage Verb Taxonomy for <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/766_Paper.pdf</url>
      <abstract>In this article I present a lexicon for Arabic verbs which exploits Levins verb-classes (Levin, 1993) and the basic development procedure used by (Schuler, 2005). The verb lexicon in its current state has 173 classes which contain 4392 verbs and 498 frames providing information about verb root, the deverbal form of the verb, the participle, thematic roles, subcategorisation frames and syntactic and semantic descriptions of each verb. The taxonomy is available in XML format. It can be ported to MYSQL, YAML or JSON and accessed either in Arabic characters or in the Buckwalter transliteration.</abstract>
      <bibkey>mousser-2010-large</bibkey>
    </paper>
    <paper id="530">
      <author><first>Helena</first><last>Spilková</last></author>
      <author><first>Daniel</first><last>Brenner</last></author>
      <author><first>Anton</first><last>Öttl</last></author>
      <author><first>Pavel</first><last>Vondřička</last></author>
      <author><first>Wim</first><last>van Dommelen</last></author>
      <author><first>Mirjam</first><last>Ernestus</last></author>
      <title>The Kachna <fixed-case>L</fixed-case>1/<fixed-case>L</fixed-case>2 Picture Replication Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/768_Paper.pdf</url>
      <abstract>This paper presents the Kachna corpus of spontaneous speech, in which ten Czech and ten Norwegian speakers were recorded both in their native language and in English. The dialogues are elicited using a picture replication task that requires active cooperation and interaction of speakers by asking them to produce a drawing as close to the original as possible. The corpus is appropriate for the study of interactional features and speech reduction phenomena across native and second languages. The combination of productions in non-native English and in speakers native language is advantageous for investigation of L2 issues while providing a L1 behaviour reference from all the speakers. The corpus consists of 20 dialogues comprising 12 hours 53 minutes of recording, and was collected in 2008. Preparation of the transcriptions, including a manual orthographic transcription and an automatically generated phonetic transcription, is currently in progress. The phonetic transcriptions are automatically generated by aligning acoustic models with the speech signal on the basis of the orthographic transcriptions and a dictionary of pronunciation variants compiled for the relevant language. Upon completion the corpus will be made available via the European Language Resources Association (ELRA).</abstract>
      <bibkey>spilkova-etal-2010-kachna</bibkey>
    </paper>
    <paper id="531">
      <author><first>Stefano</first><last>Baccianella</last></author>
      <author><first>Andrea</first><last>Esuli</last></author>
      <author><first>Fabrizio</first><last>Sebastiani</last></author>
      <title><fixed-case>S</fixed-case>enti<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/769_Paper.pdf</url>
      <abstract>In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET 2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20% with respect to SENTIWORDNET 1.0.</abstract>
      <bibkey>baccianella-etal-2010-sentiwordnet</bibkey>
    </paper>
    <paper id="532">
      <author><first>Pierre</first><last>Tirilly</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Patrick</first><last>Gros</last></author>
      <title>News Image Annotation on a Large Parallel Text-image Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/772_Paper.pdf</url>
      <abstract>In this paper, we present a multimodal parallel text-image corpus, and propose an image annotation method that exploits the textual information associated with images. Our corpus contains news articles composed of a text, images and image captions, and is significantly larger than the other news corpora proposed in image annotation papers (27,041 articles and 42,568 captionned images). In our experiments, we use the text of the articles as a textual information source to annotate images, and image captions as a groundtruth to evaluate our annotation algorithm. Our annotation method identifies relevant named entities in the texts, and associates them with high-level visual concepts detected in the images (in this paper, faces and logos). The named entities most suited to image annotation are selected using an unsupervised score based on their statistics, inspired from the weights used in information retrieval. Our experiments show that, although it is very simple, our annotation method achieves an acceptable accuracy on our real-world news corpus.</abstract>
      <bibkey>tirilly-etal-2010-news</bibkey>
    </paper>
    <paper id="533">
      <author><first>Diego</first><last>De Cao</last></author>
      <author><first>Danilo</first><last>Croce</last></author>
      <author><first>Roberto</first><last>Basili</last></author>
      <title>Extensive Evaluation of a <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et-<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et mapping resource</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/773_Paper.pdf</url>
      <abstract>Lexical resources are basic components of many text processing system devoted to information extraction, question answering or dialogue. In paste years many resources have been developed such as FrameNet and WordNet. FrameNet describes prototypical situations (i.e. Frames) while WordNet defines lexical meaning (senses) for the majority of English nouns, verbs, adjectives and adverbs. A major difference between FrameNet and WordNet refers to their coverage. Due of this lack of coverage, in recent years some approaches have been studied to make a bridge between this two resources, so a resource is used to extend the coverage of the other one. The nature of these approaches leave from supervised to supervised methods. The major problem is that there is not a standard in evaluation of the mapping. Each different work have tested own approach with a custom gold standard. This work give an extensive evaluation of the model proposed in (De Cao et al., 2008) using gold standard proposed in other works. Moreover this work give an empirical comparison between other available resources. As outcome of this work we also release the full mapping resource made according to the model proposed in (De Cao et al., 2008).</abstract>
      <bibkey>de-cao-etal-2010-extensive</bibkey>
    </paper>
    <paper id="534">
      <author><first>Djamé</first><last>Seddah</last></author>
      <title>Exploring the Spinal-<fixed-case>STIG</fixed-case> Model for Parsing <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/775_Paper.pdf</url>
      <abstract>We evaluate statistical parsing of French using two probabilistic models derived from the Tree Adjoining Grammar framework: a Stochastic Tree Insertion Grammars model (STIG) and a specific instance of this formalism, called Spinal Tree Insertion Grammar model which exhibits interesting properties with regard to data sparseness issues common to small treebanks such as the Paris 7 French Treebank. Using David Chiangs STIG parser (Chiang, 2003), we present results of various experiments we conducted to explore those models for French parsing. The grammar induction makes use of a head percolation table tailored for the French Treebank and which is provided in this paper. Using two evaluation metrics, we found that the parsing performance of a STIG model is tied to the size of the underlying Tree Insertion Grammar, with a more compact grammar, a spinal STIG, outperforming a genuine STIG. We finally note that a ""spinal"" framework seems to emerge in the literature. Indeed, the use of vertical grammars such as Spinal STIG instead of horizontal grammars such as PCFGs, afflicted with well known data sparseness issues, seems to be a promising path toward better parsing performance.</abstract>
      <bibkey>seddah-2010-exploring</bibkey>
    </paper>
    <paper id="535">
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Irina</first><last>Prodanof</last></author>
      <title>Annotating Event Anaphora: A Case Study</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/776_Paper.pdf</url>
      <abstract>In recent years we have resgitered a renewed interest in event detection and temporal processing of text/discourse. TimeML (Pustejovsky et al., 2003a) has shed new lights on the notion of event and developed a new methodology for its annotation. On a parallel, works on anaphora resolution have developed a reliable methodology for the annotation and pointed out the core role of this phenomenon for the improvement of NLP systems. This paper tries to put together these two lines of research by describing a case study for the creation of an annotation scheme on event anaphora. We claim that this work could have consequences for the annotation of eventualities as proposed in TimeML and on the use of the tag and on the study of anaphora and its annotation. The annotation scheme and its guidelines have been developed on the basis of a coarse grained bottom up approach. In order to do this, we have performed a small sampling annotation which has highlighted shortcomings and open issues which need to be resolved.</abstract>
      <bibkey>caselli-prodanof-2010-annotating</bibkey>
    </paper>
    <paper id="536">
      <author><first>Bente</first><last>Maegaard</last></author>
      <author><first>Mohamed</first><last>Attia</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <author><first>Steven</first><last>Krauwer</last></author>
      <author><first>Mustafa</first><last>Yaseen</last></author>
      <title>Cooperation for <fixed-case>A</fixed-case>rabic Language Resources and Tools — The <fixed-case>MEDAR</fixed-case> Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/777_Paper.pdf</url>
      <abstract>The paper describes some of the work carried out within the European funded project MEDAR. The project has three streams of activity: the technical stream, the cooperation stream and the dissemination stream. MEDAR has first updated the existing surveys and BLARK for Arabic, and then the technical stream focused on machine translation. The consortium identified a number of freely available MT systems and then customized two versions of the famous MOSES package. The Consortium addressed the needs to package MOSES for English to Arabic (while the main MT stream is on Arabic to English). For performance assessment purposes, the partners produced test data that allowed carrying out an evaluation campaign with 5 different systems (including from outside the consortium) and two online ones. Both the MT baselines and the collected data will be made available via ELRA catalogue. The cooperation stream focuses mostly on the cooperation roadmap for Human Language Technologies for Arabic. Cooperation Roadmap for the region directed towards the Arabic HLT in general. It is the purpose of the roadmap to outline areas and priorities for collaboration, in terms of collaboration between EU countries and Arabic speaking countries, as well as cooperation in general: between countries, between universities, and last but not least between universities and industry.</abstract>
      <bibkey>maegaard-etal-2010-cooperation</bibkey>
    </paper>
    <paper id="537">
      <author><first>Katerina</first><last>Pastra</last></author>
      <author><first>Christian</first><last>Wallraven</last></author>
      <author><first>Michael</first><last>Schultze</last></author>
      <author><first>Argyro</first><last>Vataki</last></author>
      <author><first>Kathrin</first><last>Kaulard</last></author>
      <title>The <fixed-case>POETICON</fixed-case> Corpus: Capturing Language Use and Sensorimotor Experience in Everyday Interaction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/778_Paper.pdf</url>
      <abstract>Natural language use, acquisition, and understanding takes place usually in multisensory and multimedia communication environments. Therefore, for one to model language in its interaction and integration with sensorimotor experiences, one needs a representative corpus of such interplay. In this paper, we will present the first corpus of language use and sensorimotor experience recordings in everyday human:human interaction, in which spontaneous language communication has been recorded along with corresponding multiview video recordings, recordings of 3D full body kinematics, and 3D tracking of objects in focus. It is a twelve-hour corpus which comprises of six everyday human:human interaction scenes, each one performed 3 times by 4 different English-speaking couples (interaction between a male and a female actor), each couple acting each scene in two settings: a fully naturalistic setting in which 5-camera multi-view video recordings take place, and a high-tech setting, with full body motion capture for both individuals, a 2-camera multiview video recording, and 3D tracking of focus objects. The corpus has been developed within an EU-funded cognitive systems research project, POETICON (http://www.poeticon.eu), and represents a new type of language resources for cognitive systems. Namely, a corpus that reveals the dynamic role of language in its interplay with sensorimotor experiences and which allows one to computationally model this interplay.</abstract>
      <bibkey>pastra-etal-2010-poeticon</bibkey>
    </paper>
    <paper id="538">
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Roxane</first><last>Bertrand</last></author>
      <author><first>Mathilde</first><last>Guardiola</last></author>
      <author><first>Marie-Laure</first><last>Guénot</last></author>
      <author><first>Christine</first><last>Meunier</last></author>
      <author><first>Irina</first><last>Nesterenko</last></author>
      <author><first>Berthille</first><last>Pallaud</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <author><first>Béatrice</first><last>Priego-Valverde</last></author>
      <author><first>Stéphane</first><last>Rauzy</last></author>
      <title>The <fixed-case>OTIM</fixed-case> Formal Annotation Model: A Preliminary Step before Annotation Scheme</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/781_Paper.pdf</url>
      <abstract>Large annotation projects, typically those addressing the question of multimodal annotation in which many different kinds of information have to be encoded, have to elaborate precise and high level annotation schemes. Doing this requires first to define the structure of the information: the different objects and their organization. This stage has to be as much independent as possible from the coding language constraints. This is the reason why we propose a preliminary formal annotation model, represented with typed feature structures. This representation requires a precise definition of the different objects, their properties (or features) and their relations, represented in terms of type hierarchies. This approach has been used to specify the annotation scheme of a large multimodal annotation project (OTIM) and experimented in the annotation of a multimodal corpus (CID, Corpus of Interactional Data). This project aims at collecting, annotating and exploiting a dialogue video corpus in a multimodal perspective (including speech and gesture modalities). The corpus itself, is made of 8 hours of dialogues, fully transcribed and richly annotated (phonetics, syntax, pragmatics, gestures, etc.).</abstract>
      <bibkey>blache-etal-2010-otim</bibkey>
    </paper>
    <paper id="539">
      <author><first>Sanaz</first><last>Jabbari</last></author>
      <author><first>Mark</first><last>Hepple</last></author>
      <author><first>Louise</first><last>Guthrie</last></author>
      <title>Evaluating Lexical Substitution: Analysis and New Measures</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/782_Paper.pdf</url>
      <abstract>Lexical substitution is the task of finding a replacement for a target word in a sentence so as to preserve, as closely as possible, the meaning of the original sentence. It has been proposed that lexical substitution be used as a basis for assessing the performance of word sense disambiguation systems, an idea realised in the English Lexical Substitution Task of SemEval-2007. In this paper, we examine the evaluation metrics used for the English Lexical Substitution Task and identify some problems that arise for them. We go on to propose some alternative measures for this purpose, that avoid these problems, and which in turn can be seen as redefining the key tasks that lexical substitution systems should be expected to perform. We hope that these new metrics will better serve to guide the development of lexical substitution systems in future work. One of the new metrics addresses how effective systems are in ranking substitution candidates, a key ability for lexical substitution systems, and we report some results concerning the assessment of systems produced by this measure as compared to the relevant measure from SemEval-2007.</abstract>
      <bibkey>jabbari-etal-2010-evaluating</bibkey>
    </paper>
    <paper id="540">
      <author><first>Mehrnoush</first><last>Shamsfard</last></author>
      <author><first>Hakimeh</first><last>Fadaei</last></author>
      <author><first>Elham</first><last>Fekri</last></author>
      <title>Extracting Lexico-conceptual Knowledge for Developing <fixed-case>P</fixed-case>ersian <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/784_Paper.pdf</url>
      <abstract>Semantic lexicons and lexical ontologies are some major resources in natural language processing. Developing such resources are time consuming tasks for which some automatic methods are proposed. This paper describes some methods used in semi-automatic development of FarsNet; a lexical ontology for the Persian language. FarsNet includes the Persian WordNet with more than 10000 synsets of nouns, verbs and adjectives. In this paper we discuss extraction of lexico-conceptual relations such as synonymy, antonymy, hyperonymy, hyponymy, meronymy, holonymy and other lexical or conceptual relations between words and concepts (synsets) from Persian resources. Relations are extracted from different resources like web, corpora, Wikipedia, Wiktionary, dictionaries and WordNet. In the system presented in this paper a variety of approaches are applied in the task of relation extraction to extract ladled or unlabeled relations. They exploit the texts, structures, hyperlinks and statistics of web documents as well as the relations of English WordNet and entries of mono and bi-lingual dictionaries.</abstract>
      <bibkey>shamsfard-etal-2010-extracting</bibkey>
    </paper>
    <paper id="541">
      <author><first>Paula Vaz</first><last>Lobo</last></author>
      <author><first>David Martins</first><last>de Matos</last></author>
      <title>Fairy Tale Corpus Organization Using Latent Semantic Mapping and an Item-to-item Top-n Recommendation Algorithm</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/786_Paper.pdf</url>
      <abstract>In this paper we present a fairy tale corpus that was semantically organized and tagged. The proposed method uses latent semantic mapping to represent the stories and a top-n item-to-item recommendation algorithm to define clusters of similar stories. Each story can be placed in more than one cluster and stories in the same cluster are related to the same concepts. The results were manually evaluated regarding the groupings as perceived by human judges. The evaluation resulted in a precision of 0.81, a recall of 0.69, and an f-measure of 0.75 when using tf*idf for word frequency. Our method is topic- and language-independent, and, contrary to traditional clustering methods, automatically defines the number of clusters based on the set of documents. This method can be used as a setup for traditional clustering or classification. The resulting corpus will be used for recommendation purposes, although it can also be used for emotion extraction, semantic role extraction, meaning extraction, text classification, among others.</abstract>
      <bibkey>lobo-de-matos-2010-fairy</bibkey>
    </paper>
    <paper id="542">
      <author><first>Alistair</first><last>Willis</last></author>
      <author><first>David</first><last>King</last></author>
      <author><first>David</first><last>Morse</last></author>
      <author><first>Anton</first><last>Dil</last></author>
      <author><first>Chris</first><last>Lyal</last></author>
      <author><first>Dave</first><last>Roberts</last></author>
      <title>From <fixed-case>XML</fixed-case> to <fixed-case>XML</fixed-case>: The Why and How of Making the Biodiversity Literature Accessible to Researchers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/787_Paper.pdf</url>
      <abstract>We present the ABLE document collection, which consists of a set of annotated volumes of the Bulletin of the British Museum (Natural History). These were developed during our ongoing work on automating the markup of scanned copies of the biodiversity literature. Such automation is required if historic literature is to be used to inform contemporary issues in biodiversity research. We consider an enhanced TEI XML markup language, which is used as an intermediate stage in translating from the initial XML obtained from Optical Character Recognition to taXMLit, the target annotation schema. The intermediate representation allows additional information from external sources such as a taxonomic thesaurus to be incorporated before the final translation into taXMLit. We give an overview of the project workflow in automating the markup process, and consider what extensions to existing markup schema will be required to best support working taxonomists. Finally, we discuss some of the particular issues which were encountered in converting between different XML formats.</abstract>
      <bibkey>willis-etal-2010-xml</bibkey>
    </paper>
    <paper id="543">
      <author><first>Linda</first><last>Brandschain</last></author>
      <author><first>David</first><last>Graff</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>Chris</first><last>Caruso</last></author>
      <author><first>Abby</first><last>Neely</last></author>
      <title>Greybeard Longitudinal Speech Study</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/789_Paper.pdf</url>
      <abstract>The Greybeard Project was designed so as to enable research in speaker recognition using data that have been collected over a long period of time. Since 1994, LDC has been collecting speech samples for use in research and evaluations. By mining our earlier collections we assembled a list of subjects who had participated in multiple studies. These participants were then contacted and asked to take part in the Greybeard Project. The only constraint was that the participants must have made numerous calls in prior studies and the calls had to be a minimum of two years old. The archived data was sorted by participant and subsequent calls were added to their files. This is the first longitudinal study of its kind. The resulting corpus contains multiple calls for each participant that span anywhere from two to 12 years in time. It is our hope that these data will enable speaker recognition researchers to explore the effects of aging on voice.</abstract>
      <bibkey>brandschain-etal-2010-greybeard</bibkey>
    </paper>
    <paper id="544">
      <author><first>Francisco</first><last>Campillo</last></author>
      <author><first>Daniela</first><last>Braga</last></author>
      <author><first>Ana Belén</first><last>Mourín</last></author>
      <author><first>Carmen</first><last>García-Mateo</last></author>
      <author><first>Pedro</first><last>Silva</last></author>
      <author><first>Miguel Sales</first><last>Dias</last></author>
      <author><first>Francisco</first><last>Méndez</last></author>
      <title>Building High Quality Databases for Minority Languages such as <fixed-case>G</fixed-case>alician</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/790_Paper.pdf</url>
      <abstract>This paper describes the result of a joint R&amp;D project between Microsoft Portugal and the Signal Theory Group of the University of Vigo (Spain), where a set of language resources was developed with application to Text―to―Speech synthesis. First, a large Corpus of 10000 Galician sentences was designed and recorded by a professional female speaker. Second, a lexicon with phonetic and grammatical information of over 90000 entries was collected and reviewed manually by a linguist expert. And finally, these resources were used for a MOS (Mean Opinion Score) perceptual test to compare two state―of―the―art speech synthesizers of both groups, the one from Microsoft based on HMM, and the one from the University of Vigo based on unit selection.</abstract>
      <bibkey>campillo-etal-2010-building</bibkey>
    </paper>
    <paper id="545">
      <author><first>William D.</first><last>Lewis</last></author>
      <author><first>Chris</first><last>Wendt</last></author>
      <author><first>David</first><last>Bullock</last></author>
      <title>Achieving Domain Specificity in <fixed-case>SMT</fixed-case> without Overt Siloing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/791_Paper.pdf</url>
      <abstract>We examine pooling data as a method for improving Statistical Machine Translation (SMT) quality for narrowly defined domains, such as data for a particular company or public entity. By pooling all available data, building large SMT engines, and using domain-specific target language models, we see boosts in quality, and can achieve the generalizability and resiliency of a larger SMT but with the precision of a domain-specific engine.</abstract>
      <bibkey>lewis-etal-2010-achieving</bibkey>
    </paper>
    <paper id="546">
      <author><first>Linda</first><last>Brandschain</last></author>
      <author><first>David</first><last>Graff</last></author>
      <author><first>Chris</first><last>Cieri</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>Chris</first><last>Caruso</last></author>
      <author><first>Abby</first><last>Neely</last></author>
      <title>Mixer 6</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/792_Paper.pdf</url>
      <abstract>Linguistic Data Consortiums Human Subjects Data Collection lab conducts multi-modal speech collections to develop corpora for use in speech, speaker and language research and evaluations. The Mixer collections have evolved over the years to best accommodate the ever changing needs of the research community and to hopefully keep one step ahead by providing increasingly challenging data. Over the years Mixer collections have grown to include socio-linguistic interviews, a wide variety of telephone conditions and multiple languages, recording conditions, channels and speech acts.. Mixer 6 was the most recent collection. This paper describes the Mixer 6 Phase 1 project. Mixer 6 Phase 1 was a study supporting linguistic research, technology development and education. The object of this study was to record speech in a variety of situations that vary formality and model multiple naturally occurring interactions as well as a variety of channel conditions</abstract>
      <bibkey>brandschain-etal-2010-mixer</bibkey>
    </paper>
    <paper id="547">
      <author><first>Markus</first><last>Egg</last></author>
      <author><first>Gisela</first><last>Redeker</last></author>
      <title>How Complex is Discourse Structure?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/796_Paper.pdf</url>
      <abstract>This paper contributes to the question of which degree of complexity is called for in representations of discourse structure. We review recent claims that tree structures do not suffice as a model for discourse structure, with a focus on the work done on the Discourse Graphbank (DGB) of Wolf and Gibson (2005, 2006). We will show that much of the additional complexity in the DGB is not inherent in the data, but due to specific design choices that underlie W&amp;Gs annotation. Three kinds of configuration are identified whose DGB analysis violates tree-structure constraints, but for which an analysis in terms of tree structures is possible, viz., crossed dependencies that are eventually based on lexical or referential overlap, multiple-parent structures that could be handled in terms of Marcus (1996) Nuclearity Principle, and potential list structures, in which whole lists of segments are related to a preceding segment in the same way. We also discuss the recent results which Lee et al. (2008) adduce as evidence for a complexity of discourse structure that cannot be handled in terms of tree structures.</abstract>
      <bibkey>egg-redeker-2010-complex</bibkey>
    </paper>
    <paper id="548">
      <author><first>Mohammed</first><last>Attia</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Lamia</first><last>Tounsi</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <title>An Automatically Built Named Entity Lexicon for <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/797_Paper.pdf</url>
      <abstract>We have adapted and extended the automatic Multilingual, Interoperable Named Entity Lexicon approach to Arabic, using Arabic WordNet (AWN) and Arabic Wikipedia (AWK). First, we extract AWNs instantiable nouns and identify the corresponding categories and hyponym subcategories in AWK. Then, we exploit Wikipedia inter-lingual links to locate correspondences between articles in ten different languages in order to identify Named Entities (NEs). We apply keyword search on AWK abstracts to provide for Arabic articles that do not have a correspondence in any of the other languages. In addition, we perform a post-processing step to fetch further NEs from AWK not reachable through AWN. Finally, we investigate diacritization using matching with geonames databases, MADA-TOKAN tools and different heuristics for restoring vowel marks of Arabic NEs. Using this methodology, we have extracted approximately 45,000 Arabic NEs and built, to the best of our knowledge, the largest, most mature and well-structured Arabic NE lexical resource to date. We have stored and organised this lexicon following the LMF ISO standard. We conduct a quantitative and qualitative evaluation against a manually annotated gold standard and achieve precision scores from 95.83% (with 66.13% recall) to 99.31% (with 61.45% recall) according to different values of a threshold.</abstract>
      <bibkey>attia-etal-2010-automatically</bibkey>
    </paper>
    <paper id="549">
      <author><first>Zhiyi</first><last>Song</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Gary</first><last>Krug</last></author>
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <title>Enhanced Infrastructure for Creation and Collection of Translation Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/798_Paper.pdf</url>
      <abstract>Statistical Machine Translation (MT) systems have achieved impressive results in recent years, due in large part to the increasing availability of parallel text for system training and development. This paper describes recent efforts at Linguistic Data Consortium to create linguistic resources for MT, including corpora, specifications and resource infrastructure. We review LDC's three-pronged ap-proach to parallel text corpus development (acquisition of existing parallel text from known repositories, harvesting and aligning of potential parallel documents from the web, and manual creation of parallel text by professional translators), and describe recent adap-tations that have enabled significant expansions in the scope, variety, quality, efficiency and cost-effectiveness of translation resource creation at LDC.</abstract>
      <bibkey>song-etal-2010-enhanced</bibkey>
    </paper>
    <paper id="550">
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <title>e<fixed-case>X</fixed-case>tended <fixed-case>W</fixed-case>ord<fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/799_Paper.pdf</url>
      <abstract>This paper presents a novel automatic approach to partially integrate FrameNet and WordNet. In that way we expect to extend FrameNet coverage, to enrich WordNet with frame semantic information and possibly to extend FrameNet to languages other than English. The method uses a knowledge-based Word Sense Disambiguation algorithm for matching the FrameNet lexical units to WordNet synsets. Specifically, we exploit a graph-based Word Sense Disambiguation algorithm that uses a large-scale knowledge-base derived from existing semantic resources. We have developed and tested additional versions of this algorithm showing substantial improvements over state-of-the-art results. Finally, we show some examples and figures of the resulting semantic resource.</abstract>
      <bibkey>laparra-rigau-2010-extended</bibkey>
    </paper>
    <paper id="551">
      <author><first>Barbara</first><last>Plank</last></author>
      <title>Improved Statistical Measures to Assess Natural Language Parser Performance across Domains</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/801_Paper.pdf</url>
      <abstract>We examine the performance of three dependency parsing systems, in particular, their performance variation across Wikipedia domains. We assess the performance variation of (i) Alpino, a deep grammar-based system coupled with a statistical disambiguation versus (ii) MST and Malt, two purely data-driven statistical dependency parsing systems. The question is how the performance of each parser correlates with simple statistical measures of the text (e.g. sentence length, unknown word rate, etc.). This would give us an idea of how sensitive the different systems are to domain shifts, i.e. which system is more in need for domain adaptation techniques. To this end, we extend the statistical measures used by Zhang and Wang (2009) for English and evaluate the systems on several Wikipedia domains by focusing on a freer word-order language, Dutch. The results confirm the general findings of Zhang and Wang (2009), i.e. different parsing systems have different sensitivity against various statistical measure of the text, where the highest correlation to parsing accuracy was found for the measure we added, sentence perplexity.</abstract>
      <bibkey>plank-2010-improved</bibkey>
    </paper>
    <paper id="552">
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Angelo</first><last>Lucia</last></author>
      <author><first>Jianting</first><last>Zhang</last></author>
      <title>Annotating Event Chains for Carbon Sequestration Literature</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/802_Paper.pdf</url>
      <abstract>In this paper we present a project of annotating event chains for an important scientific domain ― carbon sequestration. This domain aims to reduce carbon emissions and has been identified by the U.S. National Academy of Engineering (NAE) as a grand challenge problem for the 21st century. Given a collection of scientific literature, we identify a set of centroid experiments; and then link and order the observations and events centered around these experiments on temporal or causal chains. We describe the fundamental challenges on annotations and our general solutions to address them. We expect that our annotation efforts will produce significant advances in inter-operability through new information extraction techniques and permit scientists to build knowledge that will provide better understanding of important scientific challenges in this domain, share and re-use of diverse data sets and experimental results in a more efficient manner. In addition, the annotations of metadata and ontology for these literature will provide important support for data lifecycle activities.</abstract>
      <bibkey>ji-etal-2010-annotating</bibkey>
    </paper>
    <paper id="553">
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <author><first>Christian</first><last>Boitet</last></author>
      <title>mwetoolkit: a Framework for Multiword Expression Identification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/803_Paper.pdf</url>
      <abstract>This paper presents the Multiword Expression Toolkit (mwetoolkit), an environment for type and language-independent MWE identification from corpora. The mwetoolkit provides a targeted list of MWE candidates, extracted and filtered according to a number of user-defined criteria and a set of standard statistical association measures. For generating corpus counts, the toolkit provides both a corpus indexation facility and a tool for integration with web search engines, while for evaluation, it provides validation and annotation facilities. The mwetoolkit also allows easy integration with a machine learning tool for the creation and application of supervised MWE extraction models if annotated data is available. In our experiment, the mwetoolkit was tested and evaluated in the context of MWE extraction in the biomedical domain. Our preliminary results show that the toolkit performs better than other approaches, especially concerning recall. Moreover, this first version can also be extended in several ways in order to improve the quality of the results.</abstract>
      <bibkey>ramisch-etal-2010-mwetoolkit</bibkey>
    </paper>
    <paper id="554">
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <title>There’s no Data like More Data? Revisiting the Impact of Data Size on a Classification Task</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/806_Paper.pdf</url>
      <abstract>In the paper we investigate the impact of data size on a Word Sense Disambiguation task (WSD). We question the assumption that the knowledge acquisition bottleneck, which is known as one of the major challenges for WSD, can be solved by simply obtaining more and more training data. Our case study on 1,000 manually annotated instances of the German verb ""drohen"" (threaten) shows that the best performance is not obtained when training on the full data set, but by carefully selecting new training instances with regard to their informativeness for the learning process (Active Learning). We present a thorough evaluation of the impact of different sampling methods on the data sets and propose an improved method for uncertainty sampling which dynamically adapts the selection of new instances to the learning progress of the classifier, resulting in more robust results during the initial stages of learning. A qualitative error analysis identifies problems for automatic WSD and discusses the reasons for the great gap in performance between human annotators and our automatic WSD system.</abstract>
      <bibkey>rehbein-ruppenhofer-2010-theres</bibkey>
    </paper>
    <paper id="555">
      <author><first>Jirka</first><last>Hana</last></author>
      <author><first>Anna</first><last>Feldman</last></author>
      <title>A Positional Tagset for <fixed-case>R</fixed-case>ussian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/807_Paper.pdf</url>
      <abstract>Fusional languages have rich inflection. As a consequence, tagsets capturing their morphological features are necessarily large. A natural way to make a tagset manageable is to use a structured system. In this paper, we present a positional tagset for describing morphological properties of Russian. The tagset was inspired by the Czech positional system (Hajic, 2004). We have used preliminary versions of this tagset in our previous work (e.g., Hana et al. (2004, 2006); Feldman (2006); Feldman and Hana (2010)). Here, we both systematize and extend these preliminary versions (by adding information about animacy, aspect and reflexivity); give a more detailed description of the tagset and provide comparison with the Czech system. Each tag of the tagset consists of 16 positions, each encoding one morphological feature (part-of-speech, detailed part-of-speech, gender, animacy, number, case, possessor's gender and number, person, reflexivity, tense, aspect, degree of comparison, negation, voice, variant). The tagset contains approximately 2,000 tags.</abstract>
      <bibkey>hana-feldman-2010-positional</bibkey>
    </paper>
    <paper id="556">
      <author><first>Georgios</first><last>Petasis</last></author>
      <author><first>Dimitrios</first><last>Petasis</last></author>
      <title><fixed-case>B</fixed-case>log<fixed-case>B</fixed-case>uster: A Tool for Extracting Corpora from the Blogosphere</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/808_Paper.pdf</url>
      <abstract>This paper presents BlogBuster, a tool for extracting a corpus from the blogosphere. The topic of cleaning arbitrary web pages with the goal of extracting a corpus from web data, suitable for linguistic and language technology research and development, has attracted significant research interest recently. Several general purpose approaches for removing boilerplate have been presented in the literature; however the blogosphere poses additional requirements, such as a finer control over the extracted textual segments in order to accurately identify important elements, i.e. individual blog posts, titles, posting dates or comments. BlogBuster tries to provide such additional details along with boilerplate removal, following a rule-based approach. A small set of rules were manually constructed by observing a limited set of blogs from the Blogger and Wordpress hosting platforms. These rules operate on the DOM tree of an HTML page, as constructed by a popular browser, Mozilla Firefox. Evaluation results suggest that BlogBuster is very accurate when extracting corpora from blogs hosted in the Blogger and Wordpress, while exhibiting a reasonable precision when applied to blogs not hosted in these two popular blogging platforms.</abstract>
      <bibkey>petasis-petasis-2010-blogbuster</bibkey>
    </paper>
    <paper id="557">
      <author><first>Mehrnoush</first><last>Shamsfard</last></author>
      <author><first>Hoda Sadat</first><last>Jafari</last></author>
      <author><first>Mahdi</first><last>Ilbeygi</last></author>
      <title><fixed-case>ST</fixed-case>e<fixed-case>P</fixed-case>-1: A Set of Fundamental Tools for <fixed-case>P</fixed-case>ersian Text Processing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/809_Paper.pdf</url>
      <abstract>Many NLP applications need fundamental tools to convert the input text into appropriate form or format and extract the primary linguistic knowledge of words and sentences. These tools perform segmentation of text into sentences, words and phrases, checking and correcting the spellings, doing lexical and morphological analysis, POS tagging and so on. Persian is among languages with complex preprocessing tasks. Having different writing prescriptions, spacings between or within words, character codings and spellings are some of the difficulties and challenges in converting various texts into a standard one. The lack of fundamental text processing tools such as morphological analyser (especially for derivational morphology) and POS tagger is another problem in Persian text processing. This paper introduces a set of fundamental tools for Persian text processing in STeP-1 package. STeP-1 (Standard Text Preparation for Persian language) performs a combination of tokenization, spell checking, morphological analysis and POS tagging. It also turns all Persian texts with different prescribed forms of writing to a series of tokens in the standard style introduced by Academy of Persian Language and Literature (APLL). Experimental results show high performance.</abstract>
      <bibkey>shamsfard-etal-2010-step</bibkey>
    </paper>
    <paper id="558">
      <author><first>Drahomíra „johanka“</first><last>Spoustová</last></author>
      <author><first>Miroslav</first><last>Spousta</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <title>Building a Web Corpus of <fixed-case>C</fixed-case>zech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/810_Paper.pdf</url>
      <abstract>Large corpora are essential to modern methods of computational linguistics and natural language processing. In this paper, we describe an ongoing project whose aim is to build a largest corpus of Czech texts. We are building the corpus from Czech Internet web pages, using (and, if needed, developing) advanced downloading, cleaning and automatic linguistic processing tools. Our concern is to keep the whole process language independent and thus applicable also for building web corpora of other languages. In the paper, we briefly describe the crawling, cleaning, and part-of-speech tagging procedures. Using a prototype corpus, we provide a comparison with a current corpora (in particular, SYN2005, part of the Czech National Corpora). We analyse part-of-speech tag distribution, OOV word ratio, average sentence length and Spearman rank correlation coefficient of the distance of ranks of 500 most frequent words. Our results show that our prototype corpus is now quite homogenous. The challenging task is to find a way to decrease the homogeneity of the text while keeping the high quality of the data.</abstract>
      <bibkey>spoustova-etal-2010-building</bibkey>
    </paper>
    <paper id="559">
      <author><first>Cristina</first><last>Vertan</last></author>
      <title>Towards the Integration of Language Tools Within Historical Digital Libraries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/811_Paper.pdf</url>
      <abstract>During the last years the campaign of mass digitization made available catalogues and valuable rare manuscripts and old printed books vie the Internet. The Manuscriptorium digital library ingested hundreds of olumes and it is expected that the volume will grow up in the next years. Other European initiatives like Europeana and Monasterium have also as central activities the online presentation of cultural heritage. With the growing of the available on-line volumes, a special attention was paid to the management and retrieval of documents within digital libraries. Enabling semantic technologies and intelligent linking and search are a big step forward, but they still do not succeed in making the content of old rare books intelligible to the broad public or specialists in other domains or languages. In this paper we will argue that multilingual language technologies have the potential to fill this gap. We overview the existent language resources for historical documents, and present an architecture which aims at presenting such texts to the normal user, without altering the character of the texts.</abstract>
      <bibkey>vertan-2010-towards</bibkey>
    </paper>
    <paper id="560">
      <author><first>Adriane</first><last>Boyd</last></author>
      <title><fixed-case>EAGLE</fixed-case>: an Error-Annotated Corpus of Beginning Learner <fixed-case>G</fixed-case>erman</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/812_Paper.pdf</url>
      <abstract>This paper describes the Error-Annotated German Learner Corpus (EAGLE), a corpus of beginning learner German with grammatical error annotation. The corpus contains online workbook and and hand-written essay data from learners in introductory German courses at The Ohio State University. We introduce an error typology developed for beginning learners of German that focuses on linguistic properties of lexical items present in the learner data and present the detailed error typologies for selection, agreement, and word order errors. The corpus uses an error annotation format that extends the multi-layer standoff format proposed by Luedeling et al. (2005) to include incremental target hypotheses for each error. In this format, each annotated error includes information about the location of tokens affected by the error, the error type, and the proposed target correction. The multi-layer standoff format allows us to annotate ambiguous errors with more than one possible target correction and to annotate the multiple, overlapping errors common in beginning learner productions.</abstract>
      <bibkey>boyd-2010-eagle</bibkey>
    </paper>
    <paper id="561">
      <author><first>Olivier</first><last>Ferret</last></author>
      <title>Testing Semantic Similarity Measures for Extracting Synonyms from a Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/815_Paper.pdf</url>
      <abstract>The definition of lexical semantic similarity measures has been the subject of lots of works for many years. In this article, we focus more specifically on distributional semantic similarity measures. Although several evaluations of this kind of measures were already achieved for determining if they actually catch semantic relatedness, it is still difficult to determine if a measure that performs well in an evaluation framework can be applied more widely with the same success. In the work we present here, we first select a semantic similarity measure by testing a large set of such measures against the WordNet-based Synonymy Test, an extended TOEFL test proposed in (Freitag et al., 2005), and we show that its accuracy is comparable to the accuracy of the best state of the art measures while it has less demanding requirements. Then, we apply this measure for extracting automatically synonyms from a corpus and we evaluate the relevance of this process against two reference resources, WordNet and the Moby thesaurus. Finally, we compare our results in details to those of (Curran and Moens, 2002).</abstract>
      <bibkey>ferret-2010-testing</bibkey>
    </paper>
    <paper id="562">
      <author><first>Ernesto William</first><last>De Luca</last></author>
      <title>A Corpus for Evaluating Semantic Multilingual Web Retrieval Systems: The Sense Folder Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/816_Paper.pdf</url>
      <abstract>In this paper, we present the multilingual Sense Folder Corpus. After the analysis of different corpora, we describe the requirements that have to be satisfied for evaluating semantic multilingual retrieval approaches. Justified by the unfulfilled requirements explained, we start creating a small bilingual hand-tagged corpus of 502 documents retrieved from Web searches. The documents contained in this collection have been created using Google queries. A single ambiguous word has been searched and related documents (approx. the first 60 documents for every keyword) have been retrieved. The document collection has been extended at the query word level, using single ambiguous words for English (argument, bank, chair, network and rule) and for Italian (argomento, lingua, regola, rete and stampa). The search and annotation process has been done both in a monolingual way for the English and the Italian language. 252 English and 250 Italian documents have been retrieved from Google and saved in their original rank. The performance of semantic multilingual retrieval systems has been evaluated using such a corpus with three baselines (Random, First Sense and Most Frequent Sense) that are formally presented and discussed. The fine-grained evaluation of the Sense Folder approach is discussed in details.</abstract>
      <bibkey>de-luca-2010-corpus</bibkey>
    </paper>
    <paper id="563">
      <author><first>Roberta</first><last>Catizone</last></author>
      <author><first>Alexiei</first><last>Dingli</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Using Dialogue Corpora to Extend Information Extraction Patterns for Natural Language Understanding of Dialogue</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/818_Paper.pdf</url>
      <abstract>This paper examines how Natural Language Process (NLP) resources and online dialogue corpora can be used to extend coverage of Information Extraction (IE) templates in a Spoken Dialogue system. IE templates are used as part of a Natural Language Understanding module for identifying meaning in a user utterance. The use of NLP tools in Dialogue systems is a difficult task given 1) spoken dialogue is often not well-formed and 2) there is a serious lack of dialogue data. In spite of that, we have devised a method for extending IE patterns using standard NLP tools and available dialogue corpora found on the web. In this paper, we explain our method which includes using a set of NLP modules developed using GATE (a General Architecture for Text Engineering), as well as a general purpose editing tool that we built to facilitate the IE rule creation process. Lastly, we present directions for future work in this area.</abstract>
      <bibkey>catizone-etal-2010-using</bibkey>
    </paper>
    <paper id="564">
      <author><first>Lamia</first><last>Tounsi</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <title><fixed-case>A</fixed-case>rabic Parsing Using Grammar Transforms</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/819_Paper.pdf</url>
      <abstract>We investigate Arabic Context Free Grammar parsing with dependency annotation comparing lexicalised and unlexicalised parsers. We study how morphosyntactic as well as function tag information percolation in the form of grammar transforms (Johnson, 1998, Kulick et al., 2006) affects the performance of a parser and helps dependency assignment. We focus on the three most frequent functional tags in the Arabic Penn Treebank: subjects, direct objects and predicates . We merge these functional tags with their phrasal categories and (where appropriate) percolate case information to the non-terminal (POS) category to train the parsers. We then automatically enrich the output of these parsers with full dependency information in order to annotate trees with Lexical Functional Grammar (LFG) f-structure equations with produce f-structures, i.e. attribute-value matrices approximating to basic predicate-argument-adjunct structure representations. We present a series of experiments evaluating how well lexicalized, history-based, generative (Bikel) as well as latent variable PCFG (Berkeley) parsers cope with the enriched Arabic data. We measure quality and coverage of both the output trees and the generated LFG f-structures. We show that joint functional and morphological information percolation improves both the recovery of trees as well as dependency results in the form of LFG f-structures.</abstract>
      <bibkey>tounsi-van-genabith-2010-arabic</bibkey>
    </paper>
    <paper id="565">
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Caroline</first><last>Sporleder</last></author>
      <title>Constructing a Textual Semantic Relation Corpus Using a Discourse Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/820_Paper.pdf</url>
      <abstract>In this paper, we present our work on constructing a textual semantic relation corpus by making use of an existing treebank annotated with discourse relations. We extract adjacent text span pairs and group them into six categories according to the different discourse relations between them. After that, we present the details of our annotation scheme, which includes six textual semantic relations, 'backward entailment', 'forward entailment', 'equality', 'contradiction', 'overlapping', and 'independent'. We also discuss some ambiguous examples to show the difficulty of such annotation task, which cannot be easily done by an automatic mapping between discourse relations and semantic relations. We have two annotators and each of them performs the task twice. The basic statistics on the constructed corpus looks promising: we achieve 81.17% of agreement on the six semantic relation annotation with a .718 kappa score, and it increases to 91.21% if we collapse the last two labels with a .775 kappa score.</abstract>
      <bibkey>wang-sporleder-2010-constructing</bibkey>
    </paper>
    <paper id="566">
      <author><first>Na-Rae</first><last>Han</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Soo-Hwa</first><last>Lee</last></author>
      <author><first>Jin-Young</first><last>Ha</last></author>
      <title>Using an Error-Annotated Learner Corpus to Develop an <fixed-case>ESL</fixed-case>/<fixed-case>EFL</fixed-case> Error Correction System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/821_Paper.pdf</url>
      <abstract>This paper presents research on building a model of grammatical error correction, for preposition errors in particular, in English text produced by language learners. Unlike most previous work which trains a statistical classifier exclusively on well-formed text written by native speakers, we train a classifier on a large-scale, error-tagged corpus of English essays written by ESL learners, relying on contextual and grammatical features surrounding preposition usage. First, we show that such a model can achieve high performance values: 93.3% precision and 14.8% recall for error detection and 81.7% precision and 13.2% recall for error detection and correction when tested on preposition replacement errors. Second, we show that this model outperforms models trained on well-edited text produced by native speakers of English. We discuss the implications of our approach in the area of language error modeling and the issues stemming from working with a noisy data set whose error annotations are not exhaustive.</abstract>
      <bibkey>han-etal-2010-using</bibkey>
    </paper>
    <paper id="567">
      <author><first>Ian</first><last>McGraw</last></author>
      <author><first>Chia-ying</first><last>Lee</last></author>
      <author><first>Lee</first><last>Hetherington</last></author>
      <author><first>Stephanie</first><last>Seneff</last></author>
      <author><first>Jim</first><last>Glass</last></author>
      <title>Collecting Voices from the Cloud</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/822_Paper.pdf</url>
      <abstract>The collection and transcription of speech data is typically an expensive and time-consuming task. Voice over IP and cloud computing are poised to greatly reduce this impediment to research on spoken language interfaces in many domains. This paper documents our efforts to deploy speech-enabled web interfaces to large audiences over the Internet via Amazon Mechanical Turk, an online marketplace for work. Using the open source WAMI Toolkit, we collected corpora in two different domains which collectively constitute over 113 hours of speech. The first corpus contains 100,000 utterances of read speech, and was collected by asking workers to record street addresses in the United States. For the second task, we collected conversations with FlightBrowser, a multimodal spoken dialogue system. The FlightBrowser corpus obtained contains 10,651 utterances composing 1,113 individual dialogue sessions from 101 distinct users. The aggregate time spent collecting the data for both corpora was just under two weeks. At times, our servers were logging audio from workers at rates faster than real-time. We describe the process of collection and transcription of these corpora while providing an analysis of the advantages and limitations to this data collection method.</abstract>
      <bibkey>mcgraw-etal-2010-collecting</bibkey>
    </paper>
    <paper id="568">
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>Josep Maria</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <title>Contrastive Lexical Evaluation of Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/823_Paper.pdf</url>
      <abstract>This paper advocates a complementary measure of translation performance that focuses on the constrastive ability of two or more systems or system versions to adequately translate source words. This is motivated by three main reasons : 1) existing automatic metrics sometimes do not show significant differences that can be revealed by fine-grained focussed human evaluation, 2) these metrics are based on direct comparisons between system hypotheses with the corresponding reference translations, thus ignoring the input words that were actually translated, and 3) as these metrics do not take input hypotheses from several systems at once, fine-grained contrastive evaluation can only be done indirectly. This proposal is illustrated on a multi-source Machine Translation scenario where multiple translations of a source text are available. Significant gains (up to +1.3 BLEU point) are achieved on these experiments, and contrastive lexical evaluation is shown to provide new information that can help to better analyse a system's performance.</abstract>
      <bibkey>max-etal-2010-contrastive</bibkey>
    </paper>
    <paper id="569">
      <author><first>Elaine</first><last>Uí Dhonnchadha</last></author>
      <author><first>Josef</first><last>Van Genabith</last></author>
      <title>Partial Dependency Parsing for <fixed-case>I</fixed-case>rish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/824_Paper.pdf</url>
      <abstract>We present a partial dependency parser for Irish. Constraint Grammar (CG) based rules are used to annotate dependency relations and grammatical functions. Chunking is performed using a regular-expression grammar which operates on the dependency tagged sentences. As this is the first implementation of a parser for unrestricted Irish text (to our knowledge), there were no guidelines or precedents available. Therefore deciding what constitutes a syntactic unit, and how it should be annotated, accounts for a major part of the early development effort. Currently, all tokens in a sentence are tagged for grammatical function and local dependency. Long-distance dependencies, prepositional attachments or coordination are not handled, resulting in a partial dependency analysis. Evaluations show that the partial dependency analysis achieves an f-score of 93.60% on development data and 94.28% on unseen test data, while the chunker achieves an f-score of 97.20% on development data and 93.50% on unseen test data.</abstract>
      <bibkey>ui-dhonnchadha-van-genabith-2010-partial</bibkey>
    </paper>
    <paper id="570">
      <author><first>Paola</first><last>Monachesi</last></author>
      <author><first>Thomas</first><last>Markus</last></author>
      <title>Socially Driven Ontology Enrichment for e<fixed-case>L</fixed-case>earning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/826_Paper.pdf</url>
      <abstract>One of the objectives of the Language Technologies for Life-Long Learning (LTfLL) project, is to develop a knowledge sharing system that connects learners to resources and learners to other learners. To this end, we complement the formal knowledge represented by existing domain ontologies with the informal knowledge emerging from social tagging. More specifically, we crawl data from social media applications such as Delicious, Slideshare and YouTube. Similarity measures are employed to select possible lexicalizations of concepts that are related to the ones present in the given ontology and which are assumed to be socially relevant with respect to the input lexicalisation. In order to identify the appropriate relationships which exist between the extracted related terms and the existing domain ontology, we employ several heuristics that rely on the use of a large background knowledge base, such as DBpedia. An evaluation of the resulting ontology has been carried out. The methodology proposed allows for an appropriate enrichment process and produces a complementary vocabulary to that of a domain expert.</abstract>
      <bibkey>monachesi-markus-2010-socially</bibkey>
    </paper>
    <paper id="571">
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <title>Mining Naturally-occurring Corrections and Paraphrases from <fixed-case>W</fixed-case>ikipedia’s Revision History</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/827_Paper.pdf</url>
      <abstract>Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining Wikipedias revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr.</abstract>
      <bibkey>max-wisniewski-2010-mining</bibkey>
    </paper>
    <paper id="572">
      <author><first>Sara</first><last>Rosenthal</last></author>
      <author><first>William</first><last>Lipovsky</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Kapil</first><last>Thadani</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <title>Towards Semi-Automated Annotation for Prepositional Phrase Attachment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/828_Paper.pdf</url>
      <abstract>This paper investigates whether high-quality annotations for tasks involving semantic disambiguation can be obtained without a major investment in time or expense. We examine the use of untrained human volunteers from Amazons Mechanical Turk in disambiguating prepositional phrase (PP) attachment over sentences drawn from the Wall Street Journal corpus. Our goal is to compare the performance of these crowdsourced judgments to the annotations supplied by trained linguists for the Penn Treebank project in order to indicate the viability of this approach for annotation projects that involve contextual disambiguation. The results of our experiments on a sample of the Wall Street Journal corpus show that invoking majority agreement between multiple human workers can yield PP attachments with fairly high precision. This confirms that a crowdsourcing approach to syntactic annotation holds promise for the generation of training corpora in new domains and genres where high-quality annotations are not available and difficult to obtain.</abstract>
      <bibkey>rosenthal-etal-2010-towards</bibkey>
    </paper>
    <paper id="573">
      <author><first>Patrice</first><last>Lopez</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <title><fixed-case>GRISP</fixed-case>: A Massive Multilingual Terminological Database for Scientific and Technical Domains</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/829_Paper.pdf</url>
      <abstract>The development of a multilingual terminology is a very long and costly process. We present the creation of a multilingual terminological database called GRISP covering multiple technical and scientific fields from various open resources. A crucial aspect is the merging of the different resources which is based in our proposal on the definition of a sound conceptual model, different domain mapping and the use of structural constraints and machine learning techniques for controlling the fusion process. The result is a massive terminological database of several millions terms, concepts, semantic relations and definitions. The accuracy of the concept merging between several resources have been evaluated following several methods. This resource has allowed us to improve significantly the mean average precision of an information retrieval system applied to a large collection of multilingual and multidomain patent documents. New specialized terminologies, not specifically created for text processing applications, can be aggregated and merged to GRISP with minimal manual efforts.</abstract>
      <bibkey>lopez-romary-2010-grisp</bibkey>
    </paper>
    <paper id="574">
      <author><first>Rita</first><last>Marinelli</last></author>
      <title>Lexical Resources and Ontological Classifications for the Recognition of Proper Names Sense Extension</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/830_Paper.pdf</url>
      <abstract>Particular uses of PNs with sense extension are focussed on and inspected taking into account the presence of PNs in lexical semantic databases and electronic corpora. Methodology to select ad include PNs in semantic databases is described; the use of PNs in corpora of Italian Language is examined and evaluated, analyzing the behaviour of a set of PNs in different periods of time. Computational resources can facilitate our study in this field in an effective way by helping codify, translate and handle particular cases of polysemy, but also guiding in metaphorical and metonymic sense recognition, supported by the ontological classification of the lexical semantic entities. The relationship between the abstract and the concrete, which is at the basis of the Conceptual Metaphor perspective, can be considered strictly related to the variation of the ontological values found in our analysis of the PNs and their belonging classes which are codified in the ItalWordNet database.</abstract>
      <bibkey>marinelli-2010-lexical</bibkey>
    </paper>
    <paper id="575">
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Piroska</first><last>Lendvai</last></author>
      <title>Towards a Standardized Linguistic Annotation of the Textual Content of Labels in Knowledge Representation Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/832_Paper.pdf</url>
      <abstract>WWe propose applying standardized linguistic annotation to terms included in labels of knowledge representation schemes (taxonomies or ontologies), hypothesizing that this would help improving ontology-based semantic annotation of texts. We share the view that currently used methods for including lexical and terminological information in such hierarchical networks of concepts are not satisfactory, and thus put forward ― as a preliminary step to our annotation goal ― a model for modular representation of conceptual, terminological and linguistic information within knowledge representation systems. Our CTL model is based on two recent initiatives that describe the representation of terminologies and lexicons in ontologies: the Terminae method for building terminological and ontological models from text (Aussenac-Gilles et al., 2008), and the LexInfo metamodel for ontology lexica (Buitelaar et al., 2009). CTL goes beyond the mere fusion of the two models and introduces an additional level of representation for the linguistic objects, whereas those are no longer limited to lexical information but are covering the full range of linguistic phenomena, including constituency and dependency. We also show that the approach benefits linguistic and semantic analysis of external documents that are often to be linked to semantic resources for enrichment with concepts that are newly extracted or inferred.</abstract>
      <bibkey>declerck-lendvai-2010-towards</bibkey>
    </paper>
    <paper id="576">
      <author><first>Yohei</first><last>Murakami</last></author>
      <author><first>Donghui</first><last>Lin</last></author>
      <author><first>Masahiro</first><last>Tanaka</last></author>
      <author><first>Takao</first><last>Nakaguchi</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <title>Language Service Management with the Language Grid</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/833_Paper.pdf</url>
      <abstract>As the number of language resources accessible on the Internet increases, many efforts have been made for combining language resources and language processing tools to create new services. However, existing language resource coordination frameworks cannot manage issues of intellectual property associated with language resources, which make it difficult for most end-users to get supports for their intercultural collaborations because they always have to deal with the issues by themselves. In this paper, we aim at constructing a new language service management architecture on the Language Grid, which enables language resource providers to control access to their resources in accordance with their own policies. Furthermore, we apply the proposed architecture to the operating Language Grid in order to validate the effectiveness of the architecture. As a result, several service management models utilizing the monitoring and access constraints are occurring to satisfy various requirements from language resource providers. These models can handle paid-for language resources as well as free language resources. Finally, we discuss further challenging issues of combining language resources under each different policies.</abstract>
      <bibkey>murakami-etal-2010-language</bibkey>
    </paper>
    <paper id="577">
      <author><first>Kristina</first><last>Vučković</last></author>
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <title>Improving Chunking Accuracy on <fixed-case>C</fixed-case>roatian Texts by Morphosyntactic Tagging</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/834_Paper.pdf</url>
      <abstract>In this paper, we present the results of an experiment with utilizing a stochastic morphosyntactic tagger as a pre-processing module of a rule-based chunker and partial parser for Croatian in order to raise its overall chunking and partial parsing accuracy on Croatian texts. In order to conduct the experiment, we have manually chunked and partially parsed 459 sentences from the Croatia Weekly 100 kw newspaper sub-corpus taken from the Croatian National Corpus, that were previously also morphosyntactically disambiguated and lemmatized. Due to the lack of resources of this type, these sentences were designated as a temporary chunking and partial parsing gold standard for Croatian. We have then evaluated the chunker and partial parser in three different scenarios: (1) chunking previously morphosyntactically untagged text, (2) chunking text that was tagged using the stochastic morphosyntactic tagger for Croatian and (3) chunking manually tagged text. The obtained F1-scores for the three scenarios were, respectively, 0.874 (P: 0.825, R: 0.930), 0.891 (P: 0.856, R: 0.928) and 0.914 (P: 0.904, R: 0.925). The paper provides the description of language resources and tools used in the experiment, its setup and discussion of results and perspectives for future work.</abstract>
      <bibkey>vuckovic-etal-2010-improving</bibkey>
    </paper>
    <paper id="578">
      <author><first>David K.</first><last>Elson</last></author>
      <author><first>Kathleen R.</first><last>McKeown</last></author>
      <title>Building a Bank of Semantically Encoded Narratives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/835_Paper.pdf</url>
      <abstract>We propose a methodology for a novel type of discourse annotation whose model is tuned to the analysis of a text as narrative. This is intended to be the basis of a story bank resource that would facilitate the automatic analysis of narrative structure and content. The methodology calls for annotators to construct propositions that approximate a reference text, by selecting predicates and arguments from among controlled vocabularies drawn from resources such as WordNet and VerbNet. Annotators then integrate the propositions into a conceptual graph that maps out the entire discourse; the edges represent temporal, causal and other relationships at the level of story content. Because annotators must identify the recurring objects and themes that appear in the text, they also perform coreference resolution and word sense disambiguation as they encode propositions. We describe a collection experiment and a method for determining inter-annotator agreement when multiple annotators encode the same short story. Finally, we describe ongoing work toward extending the method to integrate the annotators interpretations of character agency (the goals, plans and beliefs that are relevant, yet not explictly stated in the text).</abstract>
      <bibkey>elson-mckeown-2010-building</bibkey>
    </paper>
    <paper id="579">
      <author><first>Billy Tak-Ming</first><last>Wong</last></author>
      <title>Semantic Evaluation of Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/837_Paper.pdf</url>
      <abstract>It is recognized that many evaluation metrics of machine translation in use that focus on surface word level suffer from their lack of tolerance of linguistic variance, and the incorporation of linguistic features can improve their performance. To this end, WordNet is therefore widely utilized by recent evaluation metrics as a thesaurus for identifying synonym pairs. On this basis, word pairs in similar meaning, however, are still neglected. We investigate the significance of this particular word group to the performance of evaluation metrics. In our experiments we integrate eight different measures of lexical semantic similarity into an evaluation metric based on standard measures of unigram precision, recall and F-measure. It is found that a knowledge-based measure proposed by Wu and Palmer and a corpus-based measure, namely Latent Semantic Analysis, lead to an observable gain in correlation with human judgments of translation quality, in an extent to which better than the use of WordNet for synonyms.</abstract>
      <bibkey>wong-2010-semantic</bibkey>
    </paper>
    <paper id="580">
      <author><first>Bento Carlos</first><last>Dias-da-Silva</last></author>
      <author><first>Ariani</first><last>Di Felippo</last></author>
      <title><fixed-case>REBECA</fixed-case>: Turning <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Databases into “Ontolexicons”</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/838_Paper.pdf</url>
      <abstract>In this paper we outline the design and present a sample of the REBECA bilingual lexical-conceptual database constructed by linking two monolingual lexical resources in which a set of lexicalized concepts of the North-American English database, the Princeton WordNet (WN.Pr) synsets, is aligned with its corresponding set of lexicalized concepts of the Brazilian Portuguese database, the Brazilian Portuguese WordNet synsets under construction, by means of the MultiNet-based interlingual schema, the concepts of which are the ones represented by the Princeton WordNet synsets. Implemented in the Protégé-OWL editor, the alignment of the two databases illustrates how wordnets can be turned into ontolexicons. At the current stage of development, the wheeled-vehicle conceptual domain was modeled to develop and to test REBECAs design and contents, respectively. The collection of 205 ontological concepts worked out, i.e. REBECA´s alignment indexes, is exemplified in the wheeled- vehicle conceptual domain, e.g. [CAR], [RAILCAR], etc., and it was selected in the WN.Pr database, version 2.0. Future work includes the population of the database with more lexical data and other conceptual domains so that the intricacies of adding more concepts and devising the spreading or pruning the relationships between them can be properly evaluated.</abstract>
      <bibkey>dias-da-silva-di-felippo-2010-rebeca</bibkey>
    </paper>
    <paper id="581">
      <author><first>Athanasios</first><last>Karasimos</last></author>
      <author><first>Evanthia</first><last>Petropoulou</last></author>
      <title>A Crash Test with Linguistica in <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek: The Case of Derivational Affixes and Bound Stems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/841_Paper.pdf</url>
      <abstract>This paper attempts to participate in the ongoing discussion in search of a suitable model for the computational treatment of Greek morphology. Focusing on the unsupervised morphology learning technique, and particularly on the model of Linguistica by Goldsmith (2001), we attempt a computational treatment of specific word formation phenomena in Modern Greek (MG), such as suffixation and compounding with bound stems, through the use of various corpora. The inability of the system to accept any morphological rule as input, hence the term 'unsupervised', interferes to a great extent with its efficiency in parsing, especially in languages with rich morphology, such as MG, among others. Specifically, neither the rich allomorphy, nor the complex combinability of morphemes in MG appear to be treated efficiently through this technique, resulting in low scores of proper word segmentation (22% in inflectional suffixes and 13% in derivational ones), as well as the recognition of false morphemes.</abstract>
      <bibkey>karasimos-petropoulou-2010-crash</bibkey>
    </paper>
    <paper id="582">
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Thierry</first><last>Declerck</last></author>
      <title>Extraction, Merging, and Monitoring of Company Data from Heterogeneous Sources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/842_Paper.pdf</url>
      <abstract>We describe the implementation of an enterprise monitoring system that builds on an ontology-based information extraction (OBIE) component applied to heterogeneous data sources. The OBIE component consists of several IE modules - each extracting on a regular temporal basis a specific fraction of company data from a given data source - and a merging tool, which is used to aggregate all the extracted information about a company. The full set of information about companies, which is to be extracted and merged by the OBIE component, is given in the schema of a domain ontology, which is guiding the information extraction process. The monitoring system, in case it detects changes in the extracted and merged information on a company with respect to the actual state of the knowledge base of the underlying ontology, ensures the update of the population of the ontology. As we are using an ontology extended with temporal information, the system is able to assign time intervals to any of the object instances. Additionally, detected changes can be communicated to end-users, who can validate and possibly correct the resulting updates in the knowledge base.</abstract>
      <bibkey>federmann-declerck-2010-extraction</bibkey>
    </paper>
    <paper id="583">
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <title>Hybrid Constituent and Dependency Parsing with <fixed-case>T</fixed-case>singhua <fixed-case>C</fixed-case>hinese Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/844_Paper.pdf</url>
      <abstract>In this paper, we describe our hybrid parsing model on the Mandarin Chinese processing. In particular, we work on the Tsinghua Chinese Treebank (TCT), whose annotation has both constitutes and the head information of each constitute. The model we design combines the mainstream constitute parsing and dependency parsing. We present in detail 1) how to (partially) encode the head information into the constitute parsing, 2) how to encode constitute information into the dependency parsing, and 3) how to restore the head information using the dependency structure. For each of them, we take different strategies to deal with different cases. In an open shared task evaluation, we achieve an f1-score of 85.23% for the constitute parsing, 82.35% with partial head information, and 74.27% with complete head information. The error analysis shows the challenge of restoring multiple-headed constitutes and also some potentials to use the dependency structure to guide the constitute parsing, which will be our future work to explore.</abstract>
      <bibkey>wang-zhang-2010-hybrid</bibkey>
    </paper>
    <paper id="584">
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <author><first>Martijn</first><last>Van Otterlo</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <title>Spatial Role Labeling: Task Definition and Annotation Scheme</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/846_Paper.pdf</url>
      <abstract>One of the essential functions of natural language is to talk about spatial relationships between objects. Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through spaces relative to some reference point. Learning how to map this information onto a formal representation from a text is a challenging problem. At present no well-defined framework for automatic spatial information extraction exists that can handle all of these issues. In this paper we introduce the task of spatial role labeling and propose an annotation scheme that is language-independent and facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics with the intent of covering all aspects of spatial concepts, including both static and dynamic spatial relations. We illustrate our annotation scheme with many examples throughout the paper, and in addition we highlight how to connect to spatial calculi such as region connection calculus and also how our approach fits into related work.</abstract>
      <bibkey>kordjamshidi-etal-2010-spatial</bibkey>
    </paper>
    <paper id="585">
      <author><first>Irene</first><last>Russo</last></author>
      <title>Discovering Polarity for Ambiguous and Objective Adjectives through Adverbial Modification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/847_Paper.pdf</url>
      <abstract>The field of opinion mining has emerged in recent years as an exciting challenge for computational linguistics: investigating how humans express subjective judgments through linguistic means paves the way for automatic recognition and summarization of opinionated texts, with the possibility of determining the polarities and strengths of opinions asserted. Sentiment lexicons are basic resources for investigating the orientation of a text that can be performed considering polarized words included in it but they encode the polarity of word types instead that the polarity of word tokens. The expression of an opinion through the choice of lexical items is context-sensitive and sentiment lexicons could be integrated with syntagmatic patterns that emerge as significant with statistical analyses. In this paper it will be proposed a corpus analysis of adverbially modified ambiguous (e.g. fast, rich) and objective adjectives (e.g. chemical, political) - that can be occasionally exploited to express a subjective judgments -. Comparing polarity encoded in sentiment lexicons and the results of a logistic regression analysis, the role of adverbial cues for polarity detection will be evaluated on the basis of a small sample of sentences manually annotated.</abstract>
      <bibkey>russo-2010-discovering</bibkey>
    </paper>
    <paper id="586">
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <title>Constructing of an Ontology-based Lexicon for <fixed-case>B</fixed-case>ulgarian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/848_Paper.pdf</url>
      <abstract>In this paper we report on the progress in the creation of an Ontology-based lexicon for Bulgarian. We have started with the concept set from an upper ontology (DOLCE). Then it was extended with concepts selected from the OntoWordNet, which correspond to Core WordNet and EuroWordNet Basic concepts. The underlying idea behind the ontology-based lexicon is its organization via two semantic relations - equivalence and subsumption. These relations reflect the distribution of lexical unit senses with respect to the concepts in the ontology. The lexical unit candidates for concept mapping have been selected from two large and well-developed lexical resources for Bulgarian - a machine readable explanatory dictionary and a morphological lexicon. In the initial step, the lexical units were handled that have equivalent senses to the concepts in the ontology (2500 at the moment). Then, in the second stage, we are proceeding with lexical units selected on their frequency distribution in a large Bulgarian corpus. This step is the more challenging one, since it might require also additions of concepts to the ontology. The main applications of the lexicon are envisaged to be the semantic annotation and semantic IR for Bulgarian.</abstract>
      <bibkey>simov-osenova-2010-constructing</bibkey>
    </paper>
    <paper id="587">
      <author><first>Meghan Lammie</first><last>Glenn</last></author>
      <author><first>Stephanie M.</first><last>Strassel</last></author>
      <author><first>Haejoong</first><last>Lee</last></author>
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <author><first>Ramez</first><last>Zakhary</last></author>
      <author><first>Xuansong</first><last>Li</last></author>
      <title>Transcription Methods for Consistency, Volume and Efficiency</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/849_Paper.pdf</url>
      <abstract>This paper describes recent efforts at Linguistic Data Consortium at the University of Pennsylvania to create manual transcripts as a shared resource for human language technology research and evaluation. Speech recognition and related technologies in particular call for substantial volumes of transcribed speech for use in system development, and for human gold standard references for evaluating performance over time. Over the past several years LDC has developed a number of transcription approaches to support the varied goals of speech technology evaluation programs in multiple languages and genres. We describe each transcription method in detail, and report on the results of a comparative analysis of transcriber consistency and efficiency, for two transcription methods in three languages and five genres. Our findings suggest that transcripts for planned speech are generally more consistent than those for spontaneous speech, and that careful transcription methods result in higher rates of agreement when compared to quick transcription methods. We conclude with a general discussion of factors contributing to transcription quality, efficiency and consistency.</abstract>
      <bibkey>glenn-etal-2010-transcription</bibkey>
    </paper>
    <paper id="588">
      <author><first>Claudiu</first><last>Mihăilă</last></author>
      <author><first>Iustina</first><last>Ilisei</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <title><fixed-case>R</fixed-case>omanian Zero Pronoun Distribution: A Comparative Study</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/851_Paper.pdf</url>
      <abstract>Anaphora resolution is still a challenging research field in natural language processing, lacking a algorithm that correctly resolves anaphoric pronouns. Anaphoric zero pronouns pose an even greater challenge, since this category is not lexically realised. Thus, their resolution is conditioned by their prior identification stage. This paper reports on the distribution of zero pronouns in Romanian in various genres: encyclopaedic, legal, literary, and news-wire texts. For this purpose, the RoZP corpus has been created, containing almost 50000 tokens and 800 zero pronouns which are manually annotated. The distribution patterns are compared across genres, and exceptional cases are presented in order to facilitate the methodological process of developing a future zero pronoun identification and resolution algorithm. The evaluation results emphasise that zero pronouns appear frequently in Romanian, and their distribution depends largely on the genre. Additionally, possible features are revealed for their identification, and a search scope for the antecedent has been determined, increasing the chances of correct resolution.</abstract>
      <bibkey>mihaila-etal-2010-romanian</bibkey>
    </paper>
    <paper id="589">
      <author><first>Renata</first><last>Savy</last></author>
      <title><fixed-case>P</fixed-case>r.<fixed-case>A</fixed-case>.<fixed-case>T</fixed-case>i.<fixed-case>D</fixed-case>: A Coding Scheme for Pragmatic Annotation of Dialogues.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/852_Paper.pdf</url>
      <abstract>Our purpose is to propose and discuss the latest version of an integrated method for dialogue analysis, annotation and evaluation., using a set of different pragmatic parameters. The annotation scheme Pr.A.Ti.D was built up on task-oriented dialogues. Dialogues are part of the CLIPS corpus of spoken Italian, which consists of spoken material stratified as regard as the diatopic variation. A description of the multilevel annotation scheme is provided, discussing some problems of its design and formalisation in a DTD for Xml mark-up. A further goal was to extend the use of Pr.A.Ti.D to other typologies of task-oriented texts and to verify the necessity and the amount of possible changes to the scheme, in order to make it more general and less oriented to specific purposes: a test on map task dialogues and consequent modifications of the scheme are presented. The application of the scheme allowed us to extract pragmatic indexes, typical of each kind of text types, and to perform both a qualitative and quantitative analysis of texts. Finally, in a linguistic perspective, a comparative analyses of conversational and communicative styles in dialogues performed by speakers belonging to different linguistic cultures and areas is proposed.</abstract>
      <bibkey>savy-2010-pr</bibkey>
    </paper>
    <paper id="590">
      <author><first>Prasanth</first><last>Kolachina</last></author>
      <author><first>Sudheer</first><last>Kolachina</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <author><first>Viswanath</first><last>Naidu</last></author>
      <author><first>Rajeev</first><last>Sangal</last></author>
      <author><first>Akshar</first><last>Bharati</last></author>
      <title>Grammar Extraction from Treebanks for <fixed-case>H</fixed-case>indi and <fixed-case>T</fixed-case>elugu</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/854_Paper.pdf</url>
      <abstract>Grammars play an important role in many Natural Language Processing (NLP) applications. The traditional approach to creating grammars manually, besides being labor-intensive, has several limitations. With the availability of large scale syntactically annotated treebanks, it is now possible to automatically extract an approximate grammar of a language in any of the existing formalisms from a corresponding treebank. In this paper, we present a basic approach to extract grammars from dependency treebanks of two Indian languages, Hindi and Telugu. The process of grammar extraction requires a generalization mechanism. Towards this end, we explore an approach which relies on generalization of argument structure over the verbs based on their syntactic similarity. Such a generalization counters the effect of data sparseness in the treebanks. A grammar extracted using this system can not only expand already existing knowledge bases for NLP tasks such as parsing, but also aid in the creation of grammars for languages where none exist. Further, we show that the grammar extraction process can help in identifying annotation errors and thus aid in the task of the treebank validation.</abstract>
      <bibkey>kolachina-etal-2010-grammar</bibkey>
    </paper>
    <paper id="591">
      <author><first>Andrejs</first><last>Vasiljevs</last></author>
      <author><first>Kaspars</first><last>Balodis</last></author>
      <title>Corpus Based Analysis for Multilingual Terminology Entry Compounding</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/855_Paper.pdf</url>
      <abstract>This paper proposes statistical analysis methods for improvement of terminology entry compounding. Terminology entry compounding is a mechanism that identifies matching entries across multiple multilingual terminology collections. Bilingual or trilingual term entries are unified in compounded multilingual entry. We suggest that corpus analysis can improve entry compounding results by analysing contextual terms of given term in the corpus data. Proposed algorithm is described. It is implemented in an experimental setup. Results of experiment on compounding of Latvian and Lithuanian terminology resources are provided. These results encourage further research for different language pairs and in different domains.</abstract>
      <bibkey>vasiljevs-balodis-2010-corpus</bibkey>
    </paper>
    <paper id="592">
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <author><first>Haejoong</first><last>Lee</last></author>
      <author><first>Stephen</first><last>Grimes</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <author><first>Robert</first><last>Parker</last></author>
      <author><first>David</first><last>Lee</last></author>
      <author><first>Andrea</first><last>Mazzucchi</last></author>
      <title>Technical Infrastructure at <fixed-case>L</fixed-case>inguistic <fixed-case>D</fixed-case>ata <fixed-case>C</fixed-case>onsortium: Software and Hardware Resources for Linguistic Data Creation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/857_Paper.pdf</url>
      <abstract>Linguistic Data Consortium (LDC) at the University of Pennsylvania has participated as a data provider in a variety of governmentsponsored programs that support development of Human Language Technologies. As the number of projects increases, the quantity and variety of the data LDC produces have increased dramatically in recent years. In this paper, we describe the technical infrastructure, both hardware and software, that LDC has built to support these complex, large-scale linguistic data creation efforts at LDC. As it would not be possible to cover all aspects of LDCs technical infrastructure in one paper, this paper focuses on recent development. We also report on our plans for making our custom-built software resources available to the community as open source software, and introduce an initiative to collaborate with software developers outside LDC. We hope that our approaches and software resources will be useful to the community members who take on similar challenges.</abstract>
      <bibkey>maeda-etal-2010-technical</bibkey>
    </paper>
    <paper id="593">
      <author><first>José M.</first><last>García-Miguel</last></author>
      <author><first>Gael</first><last>Vaamonde</last></author>
      <author><first>Fita González</first><last>Domínguez</last></author>
      <title><fixed-case>ADESSE</fixed-case>, a Database with Syntactic and Semantic Annotation of a Corpus of <fixed-case>S</fixed-case>panish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/859_Paper.pdf</url>
      <abstract>This is an overall description of ADESSE (""Base de datos de verbos, Alternancias de Diátesis y Esquemas Sintactico-Semánticos del Español""), an online database (http://adesse.uvigo.es/) with syntactic and semantic information for all clauses in a corpus of Spanish. The manually annotated corpus has 1.5 million words, 159,000 clauses and 3,450 different verb lemmas. ADESSE is an expanded version of BDS (""Base de datos sintácticos del español actual""), which contains the grammatical features of verbs and verb-arguments in the corpus. ADESSE has added semantic features such as verb sense, verb class and semantic role of arguments to make possible a detailed syntactic and semantic corpus-based characterization of verb valency. Each verb entry in the database is described in terms of valency potential and valency realizations (diatheses). The former includes a set of semantic roles of participants in a particular event type and a classification into a conceptual hierarchy of process types. Valency realizations are described in terms of correspondences of voice, syntactic functions and categories, and semantic roles. Verbs senses are discriminated at two levels: a more abstract level linked to a valency potential, and more specific verb senses taking into account particular lexical instantiations of arguments.</abstract>
      <bibkey>garcia-miguel-etal-2010-adesse</bibkey>
    </paper>
    <paper id="594">
      <author><first>David</first><last>Guthrie</last></author>
      <author><first>Mark</first><last>Hepple</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <title>Efficient Minimal Perfect Hash Language Models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/860_Paper.pdf</url>
      <abstract>The availability of large collections of text have made it possible to build language models that incorporate counts of billions of n-grams. This paper proposes two new methods of efficiently storing large language models that allow O(1) random access and use significantly less space than all known approaches. We introduce two novel data structures that take advantage of the distribution of n-grams in corpora and make use of various numbers of minimal perfect hashes to compactly store language models containing full frequency counts of billions of n-grams using 2.5 Bytes per n-gram and language models of quantized probabilities using 2.26 Bytes per n-gram. These methods allow language processing applications to take advantage of much larger language models than previously was possible using the same hardware and we additionally describe how they can be used in a distributed environment to store even larger models. We show that our approaches are simple to implement and can easily be combined with pruning and quantization to achieve additional reductions in the size of the language model.</abstract>
      <bibkey>guthrie-etal-2010-efficient</bibkey>
    </paper>
    <paper id="595">
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Dan</first><last>Adams</last></author>
      <author><first>Henry</first><last>Goldberg</last></author>
      <author><first>Jonathan</first><last>Herr</last></author>
      <author><first>Ron</first><last>Keesing</last></author>
      <author><first>Daniel</first><last>Oblinger</last></author>
      <author><first>Heather</first><last>Simpson</last></author>
      <author><first>Robert</first><last>Schrag</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <title>The <fixed-case>DARPA</fixed-case> Machine Reading Program - Encouraging Linguistic and Reasoning Research with a Series of Reading Tasks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/862_Paper.pdf</url>
      <abstract>The goal of DARPAs Machine Reading (MR) program is nothing less than making the worlds natural language corpora available for formal processing. Most text processing research has focused on locating mission-relevant text (information retrieval) and on techniques for enriching text by transforming it to other forms of text (translation, summarization) ― always for use by humans. In contrast, MR will make knowledge contained in text available in forms that machines can use for automated processing. This will be done with little human intervention. Machines will learn to read from a few examples and they will read to learn what they need in order to answer questions or perform some reasoning task. Three independent Reading Teams are building universal text engines which will capture knowledge from naturally occurring text and transform it into the formal representations used by Artificial Intelligence. An Evaluation Team is selecting and annotating text corpora with task domain concepts, creating model reasoning systems with which the reading systems will interact, and establishing question-answer sets and evaluation protocols to measure progress toward this goal. We describe development of the MR evaluation framework, including test protocols, linguistic resources and technical infrastructure.</abstract>
      <bibkey>strassel-etal-2010-darpa</bibkey>
    </paper>
    <paper id="596">
      <author><first>Heather</first><last>Simpson</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Robert</first><last>Parker</last></author>
      <author><first>Paul</first><last>McNamee</last></author>
      <title><fixed-case>W</fixed-case>ikipedia and the Web of Confusable Entities: Experience from Entity Linking Query Creation for <fixed-case>TAC</fixed-case> 2009 Knowledge Base Population</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/864_Paper.pdf</url>
      <abstract>The Text Analysis Conference (TAC) is a series of Natural Language Processing evaluation workshops organized by the National Institute of Standards and Technology. The Knowledge Base Population (KBP) track at TAC 2009, a hybrid descendant of the TREC Question Answering track and the Automated Content Extraction (ACE) evaluation program, is designed to support development of systems that are capable of automatically populating a knowledge base with information about entities mined from unstructured text. An important component of the KBP evaluation is the Entity Linking task, where systems must accurately associate text mentions of unknown Person (PER), Organization (ORG), and Geopolitical (GPE) names to entries in a knowledge base. Linguistic Data Consortium (LDC) at the University of Pennsylvania creates and distributes linguistic resources including data, annotations, system assessment, tools and specifications for the TAC KBP evaluations. This paper describes the 2009 resource creation efforts, with particular focus on the selection and development of named entity mentions for the Entity Linking task evaluation.</abstract>
      <bibkey>simpson-etal-2010-wikipedia</bibkey>
    </paper>
    <paper id="597">
      <author><first>Damien</first><last>Nouvel</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Nathalie</first><last>Friburger</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <title>An Analysis of the Performances of the <fixed-case>C</fixed-case>as<fixed-case>EN</fixed-case> Named Entities Recognition System in the Ester2 Evaluation Campaign</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/865_Paper.pdf</url>
      <abstract>In this paper, we present a detailed and critical analysis of the behaviour of the CasEN named entity recognition system during the French Ester2 evaluation campaign. In this project, CasEN has been confronted with the task of detecting and categorizing named entities in manual and automatic transcriptions of radio broadcastings. At first, we give a general presentation of the Ester2 campaign. Then, we describe our system, based on transducers. Next, we depict how systems were evaluated during this campaign and we report the main official results. Afterwards, we investigate in details the influence of some annotation biases which have significantly affected the estimation of the performances of systems. At last, we conduct an in-depth analysis of the effective errors of the CasEN system, providing us with some useful indications about phenomena that gave rise to errors (e.g. metonymy, encapsulation, detection of right boundaries) and are as many challenges for named entity recognition systems.</abstract>
      <bibkey>nouvel-etal-2010-analysis</bibkey>
    </paper>
    <paper id="598">
      <author><first>Jiří</first><last>Materna</last></author>
      <author><first>Karel</first><last>Pala</last></author>
      <title>Using Ontologies for Semi-automatic Linking <fixed-case>V</fixed-case>erba<fixed-case>L</fixed-case>ex with <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/867_Paper.pdf</url>
      <abstract>This work presents a method of linking verbs and their valency frames in VerbaLex database developed at the Centre for NLP at the Faculty of Informatics Masaryk University to the frames in Berkeley FrameNet. While completely manual work may take a long time, the proposed semi-automatic approach requires a smaller amount of human effort to reach sufficient results. The method of linking VerbaLex frames to FrameNet frames consists of two phases. The goal of the first one is to find an appropriate FrameNet frame for each frame in VerbaLex. The second phase includes assigning FrameNet frame elements to the deep semantic roles in VerbaLex. In this work main emphasis is put on the exploitation of ontologies behind VerbaLex and FrameNet. Especially, the method of linking FrameNet frame elements with VerbaLex semantic roles is built using the information provided by the ontology of semantic types in FrameNet. Based on the proposed technique, a semi-automatic linking tool has been developed. By linking FrameNet to VerbaLex, we are able to find a non-trivial subset of the interlingual FrameNet frames (including their frame-to-frame relations), which could be used as a core for building FrameNet in Czech.</abstract>
      <bibkey>materna-pala-2010-using</bibkey>
    </paper>
    <paper id="599">
      <author><first>Thepchai</first><last>Supnithi</last></author>
      <author><first>Taneth</first><last>Ruangrajitpakorn</last></author>
      <author><first>Kanokorn</first><last>Trakultaweekool</last></author>
      <author><first>Peerachet</first><last>Porkaew</last></author>
      <title><fixed-case>A</fixed-case>uto<fixed-case>T</fixed-case>ag<fixed-case>TCG</fixed-case> : A Framework for Automatic <fixed-case>T</fixed-case>hai <fixed-case>CG</fixed-case> Tagging</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/868_Paper.pdf</url>
      <abstract>This paper aims to develop a framework for automatic CG tagging. We investigated two main algorithms, CRF and Statistical alignment model based on information theory (SAM). We found that SAM gives the best results both in word level and sentence level. We got the accuracy 89.25% in word level and 82.49% in sentence level. Combining both methods can be suited for both known and unknown word.</abstract>
      <bibkey>supnithi-etal-2010-autotagtcg</bibkey>
    </paper>
    <paper id="600">
      <author><first>Helena</first><last>Blancafort</last></author>
      <title>Learning Morphology of <fixed-case>R</fixed-case>omance, <fixed-case>G</fixed-case>ermanic and <fixed-case>S</fixed-case>lavic Languages with the Tool Linguistica</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/872_Paper.pdf</url>
      <abstract>In this paper we present preliminary work conducted on semi-automatic induction of inflectional paradigms from non annotated corpora using the open-source tool Linguistica (Goldsmith 2001) that can be utilized without any prior knowledge of the language. The aim is to induce morphology information from corpora such as to compare languages and foresee the difficulty to develop morphosyntactic lexica. We report on a series of corpus-based experiments run with Linguistica in Romance languages (Catalan, French, Italian, Portuguese, and Spanish), Germanic languages (Dutch, English and German), and Slavic language Polish. For each language we obtained interesting clusters of stems sharing the same suffixes. They can be seen as mini inflectional paradigms that include productive derivative suffixes. We ranked results depending on the size of the paradigms (maximum number of suffixes per stem) per language. Results show that it is useful to get a first idea of the role and complexity of inflection and derivation in a language, to compare results with other languages, and that it could be useful to build lexicographic resources from scratch. Still, special post-processing is needed to face the two principal drawbacks of the tool: no clear distinction between inflection and derivation, and not taking allomorphy into account.</abstract>
      <bibkey>blancafort-2010-learning</bibkey>
    </paper>
    <paper id="601">
      <author><first>Noureddine</first><last>Loukil</last></author>
      <author><first>Kais</first><last>Haddar</last></author>
      <author><first>Abdelmajid</first><last>Benhamadou</last></author>
      <title>A Syntactic Lexicon for <fixed-case>A</fixed-case>rabic Verbs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/873_Paper.pdf</url>
      <abstract>In this paper, we present a modeling of a syntactic lexicon for Arabic verbs. The structure of the lexicon is based on the recently introduced ISO standard called the Lexical Markup Framework. This standard enables us to describe the lexical information in a versatile way using general guidelines and make possible to share the resources developed in compliance with it. We discuss the syntactic information associated to verbs and the model we propose to structure and represent the entries within the lexicon. To study the usability of the lexicon in a real application, we designed a rule-based system that translates a LMF syntactic resource into Type Description Language compliant resource. The rules are mapping information from LMF entries and types to TDL types. The generated lexicon is used as input for a previously written HPSG grammar for Arabic built within the Language Knowledge Builder platform. Finally, we discuss improvements in parsing results and possible perspectives of this work.</abstract>
      <bibkey>loukil-etal-2010-syntactic</bibkey>
    </paper>
    <paper id="602">
      <author><first>Girish Nath</first><last>Jha</last></author>
      <title>The <fixed-case>TDIL</fixed-case> Program and the <fixed-case>I</fixed-case>ndian Langauge Corpora Intitiative (<fixed-case>ILCI</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/874_Paper.pdf</url>
      <abstract>India is considered a linguistic ocean with 4 language families and 22 scheduled national languages, and 100 un-scheduled languages reported by the 2001 census. This puts tremendous pressures on the Indian government to not only have comprehensive language policies, but also to create resources for their maintenance and development. In the age of information technology, there is a greater need to have a fine balance between allocation of resources to each language keeping in view the political compulsions, electoral potential of a linguistic community and other issues. In this connection, the government of India through various ministries and a think tank consisting of eminent linguistics and policy makers has done a commendable job despite the obvious roadblocks. This paper describes the Indian governments policies towards language development and maintenance in the age of technology through the Ministry of HRD through its various agencies and the Ministry of Communications &amp; Information Technology (MCIT) through its dedicated program called TDIL (Technology Development for Indian Languages). The paper also describes some of the recent activities of the TDIL in general and in particular, an innovative corpora project called ILCI - Indian Languages Corpora Initiative.</abstract>
      <bibkey>jha-2010-tdil</bibkey>
    </paper>
    <paper id="603">
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <title>Towards Sentiment Analysis of Financial Texts in <fixed-case>C</fixed-case>roatian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/876_Paper.pdf</url>
      <abstract>The paper presents results of an experiment dealing with sentiment analysis of Croatian text from the domain of finance. The goal of the experiment was to design a system model for automatic detection of general sentiment and polarity phrases in these texts. We have assembled a document collection from web sources writing on the financial market in Croatia and manually annotated articles from a subset of that collection for general sentiment. Additionally, we have manually annotated a number of these articles for phrases encoding positive or negative sentiment within a text. In the paper, we provide an analysis of the compiled resources. We show a statistically significant correspondence (1) between the overall market trend on the Zagreb Stock Exchange and the number of positively and negatively accented articles within periods of trend and (2) between the general sentiment of articles and the number of polarity phrases within those articles. We use this analysis as an input for designing a rule-based local grammar system for automatic detection of polarity phrases and evaluate it on held out data. The system achieves F1-scores of 0.61 (P: 0.94, R: 0.45) and 0.63 (P: 0.97, R: 0.47) on positive and negative polarity phrases.</abstract>
      <bibkey>agic-etal-2010-towards</bibkey>
    </paper>
    <paper id="604">
      <author><first>Sylwia</first><last>Ozdowska</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <title>Inferring Syntactic Rules for Word Alignment through Inductive Logic Programming</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/878_Paper.pdf</url>
      <abstract>This paper presents and evaluates an original approach to automatically align bitexts at the word level. It relies on a syntactic dependency analysis of the source and target texts and is based on a machine-learning technique, namely inductive logic programming (ILP). We show that ILP is particularly well suited for this task in which the data can only be expressed by (translational and syntactic) relations. It allows us to infer easily rules called syntactic alignment rules. These rules make the most of the syntactic information to align words. A simple bootstrapping technique provides the examples needed by ILP, making this machine learning approach entirely automatic. Moreover, through different experiments, we show that this approach requires a very small amount of training data, and its performance rivals some of the best existing alignment systems. Furthermore, cases of syntactic isomorphisms or non-isomorphisms between the source language and the target language are easily identified through the inferred rules.</abstract>
      <bibkey>ozdowska-claveau-2010-inferring</bibkey>
    </paper>
    <paper id="605">
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Jakub</first><last>Waszczuk</last></author>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <title>Towards the Annotation of Named Entities in the <fixed-case>N</fixed-case>ational <fixed-case>C</fixed-case>orpus of <fixed-case>P</fixed-case>olish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/879_Paper.pdf</url>
      <abstract>We present the named entity annotation task within the on-going project of the National Corpus of Polish. To the best of our knowledge, this is the first attempt at a large-scale corpus annotation of Polish named entities. We describe the scope and the TEI-inspired hierarchy of named entities admitted for this task, as well as the TEI-conformant multi-level stand-off annotation format. We also discuss some methodological strategies including the annotation of embedded, coordinated and discontinuous names. Our annotation platform consists of two main tools interconnected by converting facilities. A rule-based natural language processing platform SProUT is used for the automatic pre-annotation of named entities, due to the previously created Polish extraction grammars adapted to the annotation task. A customizable graphical tree editor TrEd, extended to our needs, provides an ergonomic environment for manual correction of annotations. Despite some difficult cases encountered in the early annotation phase, about 2,600 named entities in 1,800 corpus sentences have presently been annotated, which allowed to validate the project methodology and tools.</abstract>
      <bibkey>savary-etal-2010-towards</bibkey>
    </paper>
    <paper id="606">
      <author><first>Avaré</first><last>Stewart</last></author>
      <author><first>Kerstin</first><last>Denecke</last></author>
      <author><first>Wolfgang</first><last>Nejdl</last></author>
      <title>Cross-Corpus Textual Entailment for Sublanguage Analysis in Epidemic Intelligence</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/881_Paper.pdf</url>
      <abstract>Textual entailment has been recognized as a generic task that captures major semantic inference needs across many natural language processing applications. However, to date, textual entailment has not been considered in a cross-corpus setting, nor for user generated content. Given the emergence of Medicine 2.0, medical blogs are becoming an increasingly accepted source of information. However, given the characteristics of blogs( which tend to be noisy and informal; or contain a interspersing of subjective and factual sentences) a potentially large amount of irrelevant information may be present. Given the potential noise, the overarching problem with respect to information extraction from social media is achieving the correct level of sentence filtering - as opposed to document or blog post level. Specifically for the task of medical intelligence gathering. In this paper, we propose an approach to textual entailment with uses the text from one source of user generated content (T text) for sentence-level filtering within a new and less amenable one (H text), when the underlying domain, tasks or semantic information is the same, or overlaps.</abstract>
      <bibkey>stewart-etal-2010-cross</bibkey>
    </paper>
    <paper id="607">
      <author><first>Javier</first><last>Couto</last></author>
      <author><first>Helena</first><last>Blancafort</last></author>
      <author><first>Somara</first><last>Seng</last></author>
      <author><first>Nicolas</first><last>Kuchmann-Beauger</last></author>
      <author><first>Anass</first><last>Talby</last></author>
      <author><first>Claude</first><last>de Loupy</last></author>
      <title><fixed-case>OAL</fixed-case>: A <fixed-case>NLP</fixed-case> Architecture to Improve the Development of Linguistic Resources for <fixed-case>NLP</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/882_Paper.pdf</url>
      <abstract>The performance of most NLP applications relies upon the quality of linguistic resources. The creation, maintenance and enrichment of those resources are a labour-intensive task, especially when no tools are available. In this paper we present the NLP architecture OAL, designed to assist computational linguists in the whole process of the development of resources in an industrial context: from corpora compilation to quality assurance. To add new words more easily to the morphosyntactic lexica, a guesser that lemmatizes and assigns morphosyntactic tags as well as inflection paradigms to a new word has been developed. Moreover, different control mechanisms are set up to check the coherence and consistency of the resources. Today OAL manages resources in five European languages: French, English, Spanish, Italian and Polish. Chinese and Portuguese are in process. The development of OAL has followed an incremental strategy. At present, semantic lexica, a named entities guesser and a named entities phonetizer are being developed.</abstract>
      <bibkey>couto-etal-2010-oal</bibkey>
    </paper>
    <paper id="608">
      <author><first>Karel</first><last>Pala</last></author>
      <author><first>Christiane</first><last>Fellbaum</last></author>
      <author><first>Sonja</first><last>Bosch</last></author>
      <title>Lexical Resources for Noun Compounds in <fixed-case>C</fixed-case>zech, <fixed-case>E</fixed-case>nglish and <fixed-case>Z</fixed-case>ulu</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/883_Paper.pdf</url>
      <abstract>In this paper we discuss noun compounding, a highly generative, productive process, in three distinct languages: Czech, English and Zulu. Derivational morphology presents a large grey area between regular, compositional and idiosyncratic, non-compositional word forms. The structural properties of compounds in each of the languages are reviewed and contrasted. Whereas English compounds are head-final and thus left-branching, Czech and Zulu compounds usually consist of a leftmost governing head and a rightmost dependent element. Semantic properties of compounds are discussed with special reference to semantic relations between compound members which cross-linguistically show universal patterns, but idiosyncratic, language specific compounds are also identified. The integration of compounds into lexical resources, and WordNets in particular, remains a challenge that needs to be considered in terms of the compounds syntactic idiosyncrasy and semantic compositionality. Experiments with processing compounds in Czech, English and Zulu are reported and partly evaluated. The obtained partial lists of the Czech, English and Zulu compounds are also described.</abstract>
      <bibkey>pala-etal-2010-lexical</bibkey>
    </paper>
    <paper id="609">
      <author><first>Dietrich</first><last>Rebholz-Schuhmann</last></author>
      <author><first>Antonio José</first><last>Jimeno Yepes</last></author>
      <author><first>Erik M.</first><last>van Mulligen</last></author>
      <author><first>Ning</first><last>Kang</last></author>
      <author><first>Jan</first><last>Kors</last></author>
      <author><first>David</first><last>Milward</last></author>
      <author><first>Peter</first><last>Corbett</last></author>
      <author><first>Ekaterina</first><last>Buyko</last></author>
      <author><first>Katrin</first><last>Tomanek</last></author>
      <author><first>Elena</first><last>Beisswanger</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <title>The <fixed-case>CALBC</fixed-case> Silver Standard Corpus for Biomedical Named Entities — A Study in Harmonizing the Contributions from Four Independent Named Entity Taggers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/888_Paper.pdf</url>
      <abstract>The production of gold standard corpora is time-consuming and costly. We propose an alternative: the âsilver standard corpus (SSC), a corpus that has been generated by the harmonisation of the annotations that have been delivered from a selection of annotation systems. The systems have to share the type system for the annotations and the harmonisation solution has use a suitable similarity measure for the pair-wise comparison of the annotations. The annotation systems have been evaluated against the harmonised set (630.324 sentences, 15,956,841 tokens). We can demonstrate that the annotation of proteins and genes shows higher diversity across all used annotation solutions leading to a lower agreement against the harmonised set in comparison to the annotations of diseases and species. An analysis of the most frequent annotations from all systems shows that a high agreement amongst systems leads to the selection of terms that are suitable to be kept in the harmonised set. This is the first large-scale approach to generate an annotated corpus from automated annotation systems. Further research is required to understand, how the annotations from different systems have to be combined to produce the best annotation result for a harmonised corpus.</abstract>
      <bibkey>rebholz-schuhmann-etal-2010-calbc</bibkey>
    </paper>
    <paper id="610">
      <author><first>Gabor</first><last>Melli</last></author>
      <title>Concept Mentions within <fixed-case>KDD</fixed-case>-2009 Abstracts (kdd09cma1) Linked to a <fixed-case>KDD</fixed-case> Ontology (kddo1)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/889_Paper.pdf</url>
      <abstract>We introduce the kddo1 ontology and semantically annotated kdd09cma1 corpus from the field of knowledge discovery in database (KDD) research. The corpus is based on the abstracts for the papers accepted into the KDD-2009 conference. Each abstract has its concept mentions identified and, where possible, linked to the appropriate concept in the ontology. The ontology is based on a human generated and readable semantic wiki focused on concepts and relationships for the domain along with other related topics, papers and researchers from information sciences. To our knowledge this is the first ontology and interlinked corpus for a subdiscipline within computing science. The dataset enables the evaluation of supervised approaches to semantic annotation of documents that contain a large number of high-level concepts relative the number of named entity mentions. We plan to continue to evolve the ontology based on the discovered relations within the corpus and to extend the corpus to cover other research paper abstracts from the domain. Both resources are publicly available at http://www.gabormelli.com/Projects/kdd/data/.</abstract>
      <bibkey>melli-2010-concept</bibkey>
    </paper>
    <paper id="611">
      <author><first>Petra-Maria</first><last>Strauß</last></author>
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Georg</first><last>Layher</last></author>
      <author><first>Holger</first><last>Hoffmann</last></author>
      <title>Evaluation of the <fixed-case>PIT</fixed-case> Corpus Or What a Difference a Face Makes?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/890_Paper.pdf</url>
      <abstract>This paper presents the evaluation of the PIT Corpus of multi-party dialogues recorded in a Wizard-of-Oz environment. An evaluation has been performed with two different foci: First, a usability evaluation was used to take a look at the overall ratings of the system. A shortened version of the SASSI questionnaire, namely the SASSISV, and the well established AttrakDiff questionnaire assessing the hedonistic and pragmatic dimension of computer systems have been analysed. In a second evaluation, the user's gaze direction was analysed in order to assess the difference in the user's (gazing) behaviour if interacting with the computer versus the other dialogue partner. Recordings have been performed in different setups of the system, e.g. with and without avatar. Thus, the presented evaluation further focuses on the difference in the interaction caused by deploying an avatar. The quantitative analysis of the gazing behaviour has resulted in several encouraging significant differences. As a possible interpretation it could be argued that users are more attentive towards systems with an avatar - the difference a face makes.</abstract>
      <bibkey>strauss-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="612">
      <author><first>Luka</first><last>Nerima</last></author>
      <author><first>Eric</first><last>Wehrli</last></author>
      <author><first>Violeta</first><last>Seretan</last></author>
      <title>A Recursive Treatment of Collocations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/891_Paper.pdf</url>
      <abstract>This article discusses the treatment of collocations in the context of a long-term project on the development of multilingual NLP tools. Besides classical two-word collocations, we will focus on the case of complex collocations (3 words or more) for which a recursive design is presented in the form of collocation of collocations. Although comparatively less numerous than two-word collocations, the complex collocations pose important challenges for NLP. The article discusses how these collocations are retrieved from corpora, inserted and stored in a lexical database, how the parser uses such knowledge and what are the advantages offered by a recursive approach to complex collocations.</abstract>
      <bibkey>nerima-etal-2010-recursive</bibkey>
    </paper>
    <paper id="613">
      <author><first>Bonaventura</first><last>Coppola</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <title>A General Purpose <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et-based Shallow Semantic Parser</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/893_Paper.pdf</url>
      <abstract>In this paper we present a new FrameNet-based Shallow Semantic Parser. Shallow Semantic Parsing has been a popular Natural Language Processing task since the 2004 and 2005 CoNLL Shared Task editions on Semantic Role Labeling, which were based on the PropBank lexical-semantic resource. Nonetheless, efforts in extending such task to the FrameNet setting have been constrained by practical software engineering issues. We hereby analyze these issues, identify desirable requirements for a practical parsing framework, and show the results of our software implementation. In particular, we attempt at meeting requirements arising from both a) the need of a flexible environment supporting current ongoing research, and b) the willingness of providing an effective platform supporting preliminary application prototypes in the field. After introducing the task of FrameNet-based Shallow Semantic Parsing, we sketch the system processing workflow and summarize a set of successful experimental results, directing the reader to previous published papers for extended experiment descriptions and wider discussion of the achieved results.</abstract>
      <bibkey>coppola-moschitti-2010-general</bibkey>
    </paper>
    <paper id="614">
      <author><first>Timo</first><last>Sowa</last></author>
      <author><first>Fiorenza</first><last>Arisio</last></author>
      <author><first>Luca</first><last>Cristoforetti</last></author>
      <title><fixed-case>DICIT</fixed-case>: Evaluation of a Distant-talking Speech Interface for Television</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/894_Paper.pdf</url>
      <abstract>The EC-funded project DICIT developed distant-talking interfaces for interactive TV. The final DICIT prototype system processes multimodal user input by speech and remote control. It was designed to understand both natural language and command-and-control-style speech input. We conducted an evaluation campaign to examine the usability and performance of the prototype. The task-oriented evaluation involved naive test persons and consisted of a subjective part with a usability questionnaire and an objective part. We used three groups of objective metrics to assess the system: one group related to speech component performance, one related to interface design and user awareness, and a final group related to task-based effectiveness and usability. These metrics were acquired with a dedicated transcription and annotation tool. The evaluation revealed a quite positive subjective assessments of the system and reasonable objective results. We report how the objective metrics helped us to determine problems in specific areas and to distinguish design-related issues from technical problems. The metrics computed over modality-specific groups also show that speech input gives a usability advantage over remote control for certain types of tasks.</abstract>
      <bibkey>sowa-etal-2010-dicit</bibkey>
    </paper>
    <paper id="615">
      <author><first>Arianne</first><last>Reimerink</last></author>
      <author><first>Pilar León</first><last>Araúz</last></author>
      <author><first>Pedro J. Magaña</first><last>Redondo</last></author>
      <title><fixed-case>E</fixed-case>co<fixed-case>L</fixed-case>exicon: An Environmental <fixed-case>TKB</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/895_Paper.pdf</url>
      <abstract>EcoLexicon, a multilingual knowledge resource on the environment, provides an internally coherent information system covering a wide range of specialized linguistic and conceptual needs. Data in our terminological knowledge base (TKB) are primarily hosted in a relational database which is now linked to an ontology in order to apply reasoning techniques and enhance user queries. The advantages of ontological reasoning can only be obtained if conceptual description is based on systematic criteria and a wide inventory of non-hierarchical relations, which confer dynamism to knowledge representation. Thus, our research has mainly focused on conceptual modelling and providing a user-friendly multimodal interface. The dynamic interface, which combines conceptual (networks and definitions), linguistic (contexts, concordances) and graphical information offers users the freedom to surf it according to their needs. Furthermore, dynamism is also present at the representational level. Contextual constraints have been applied to reconceptualise versatile concepts that cause a great deal of information overload.</abstract>
      <bibkey>reimerink-etal-2010-ecolexicon</bibkey>
    </paper>
    <paper id="616">
      <author><first>José João</first><last>Almeida</last></author>
      <author><first>André</first><last>Santos</last></author>
      <author><first>Alberto</first><last>Simões</last></author>
      <title>Bigorna – A Toolkit for Orthography Migration Challenges</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/898_Paper.pdf</url>
      <abstract>Languages are born, evolve and, eventually, die. During this evolution their spelling rules (and sometimes the syntactic and semantic ones) change, putting old documents out of use. In Portugal, a pair of political agreements with Brazil forced relevant changes on the way the Portuguese language is written. In this article we will detail these two Orthographic Agreements (one in the thirties and the other more recently, in the nineties), and the challenges present on the automatic migration of old documents spelling to their actual one. We will reveal Bigorna, a toolkit for the classification of language variants, their comparison and the conversion of texts in different language versions. These tools will be explained together with examples of migration issues. As Birgorna relies on a set of conversion rules we will also discuss how to infer conversion rules from a set of documents (texts with different ages). The document concludes with a brief evaluation on the conversion and classification tool results and their relevance in the current Portuguese language scenario.</abstract>
      <bibkey>almeida-etal-2010-bigorna</bibkey>
    </paper>
    <paper id="617">
      <author><first>Jan Jona</first><last>Javoršek</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <title>Experimental Deployment of a Grid Virtual Organization for Human Language Technologies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/899_Paper.pdf</url>
      <abstract>We propose to create a grid virtual organization for human language technologies, at first chiefly with the task of enabling linguistic researches to use existing distributed computing facilities of the European grid infrastructure for more efficient processing of large data sets. After a brief overview of modern grid computing, a number of common use-cases of natural language processing tasks running on the grid are presented, notably corpus annotation with morpho-syntactic tagging (600+ million-word corpus annotated in less than a day), $n$-gram statistics processing of a corpus and creation of grid-backed web-accessible services with annotation and term-extraction as examples. Implementation considerations and common problems of using grid for this type of tasks are laid out. We conclude with an outline of a simple action plan for evolving the infrastructure created for these experiments into a fully functional Human Language Technology grid Virtual Organization with the goal of making the power of European grid infrastructure available to the linguistic community.</abstract>
      <bibkey>javorsek-erjavec-2010-experimental</bibkey>
    </paper>
    <paper id="618">
      <author><first>Eric</first><last>Charton</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <title><fixed-case>NLG</fixed-case>b<fixed-case>A</fixed-case>se: A Free Linguistic Resource for Natural Language Processing Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/900_Paper.pdf</url>
      <abstract>Availability of labeled language resources, such as annotated corpora and domain dependent labeled language resources is crucial for experiments in the field of Natural Language Processing. Most often, due to lack of resources, manual verification and annotation of electronic text material is a prerequisite for the development of NLP tools. In the context of under-resourced language, the lack of copora becomes a crucial problem because most of the research efforts are supported by organizations with limited funds. Using free, multilingual and highly structured corpora like Wikipedia to produce automatically labeled language resources can be an answer to those needs. This paper introduces NLGbAse, a multilingual linguistic resource built from the Wikipedia encyclopedic content. This system produces structured metadata which make possible the automatic annotation of corpora with syntactical and semantical labels. A metadata contains semantical and statistical informations related to an encyclopedic document. To validate our approach, we built and evaluated a Named Entity Recognition tool, trained with Wikipedia corpora annotated by our system.</abstract>
      <bibkey>charton-torres-moreno-2010-nlgbase</bibkey>
    </paper>
    <paper id="619">
      <author><first>Wauter</first><last>Bosma</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Bootstrapping Language Neutral Term Extraction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/902_Paper.pdf</url>
      <abstract>A variety of methods exist for extracting terms and relations between terms from a corpus, each of them having strengths and weaknesses. Rather than just using the joint results, we apply different extraction methods in a way that the results of one method are input to another. This gives us the leverage to find terms and relations that otherwise would not be found. Our goal is to create a semantic model of a domain. To that end, we aim to find the complete terminology of the domain, consisting of terms and relations such as hyponymy and meronymy, and connected to generic wordnets and ontologies. Terms are ranked by domain-relevance only as a final step, after terminology extraction is completed. Because term relations are a large part of the semantics of a term, we estimate the relevance from its relation to other terms, in addition to occurrence and document frequencies. In the KYOTO project, we apply language-neutral terminology extraction from a parsed corpus for seven languages.</abstract>
      <bibkey>bosma-vossen-2010-bootstrapping</bibkey>
    </paper>
    <paper id="620">
      <author><first>Jinho D.</first><last>Choi</last></author>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title><fixed-case>P</fixed-case>ropbank Instance Annotation Guidelines Using a Dedicated Editor, Jubilee</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/903_Paper.pdf</url>
      <abstract>This paper gives guidelines of how to annotate Propbank instances using a dedicated editor, Jubilee. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Jubilee facilitates this annotation process by displaying several resources of syntactic and semantic information simultaneously: the syntactic structure of a sentence is displayed in the main frame, the available senses with their corresponding argument structures are displayed in another frame, all available Propbank arguments are displayed for the annotators choice, and example annotations of each sense of the predicate are available to the annotator for viewing. Easy access to each of these resources allows the annotator to quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation. Jubilee has been successfully adapted to many Propbank projects in several universities. The tool runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.</abstract>
      <bibkey>choi-etal-2010-propbank-instance</bibkey>
    </paper>
    <paper id="621">
      <author><first>Kumutha</first><last>Swampillai</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <title>Inter-sentential Relations in Information Extraction Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/905_Paper.pdf</url>
      <abstract>In natural language relationships between entities can asserted within a single sentence or over many sentences in a document. Many information extraction systems are constrained to extracting binary relations that are asserted within a single sentence (single-sentence relations) and this limits the proportion of relations they can extract since those expressed across multiple sentences (inter-sentential relations) are not considered. The analysis in this paper focuses on finding the distribution of inter-sentential and single-sentence relations in two corpora used for the evaluation of Information Extraction systems: the MUC6 corpus and the ACE corpus from 2003. In order to carry out this analysis we had to manually mark up all the management succession relations described in the MUC6 corpus. It was found that inter-sentential relations constitute 28.5% and 9.4% of the total number of relations in MUC6 and ACE03 respectively. This places upper bounds on the recall of information extraction systems that do not consider relations that are asserted across multiple sentences (71.5% and 90.6% respectively).</abstract>
      <bibkey>swampillai-stevenson-2010-inter</bibkey>
    </paper>
    <paper id="622">
      <author><first>Peter</first><last>Nabende</last></author>
      <title>Applying a Dynamic <fixed-case>B</fixed-case>ayesian Network Framework to Transliteration Identification</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/906_Paper.pdf</url>
      <abstract>Identification of transliterations is aimed at enriching multilingual lexicons and improving performance in various Natural Language Processing (NLP) applications including Cross Language Information Retrieval (CLIR) and Machine Translation (MT). This paper describes work aimed at using the widely applied graphical models approach of Dynamic Bayesian Networks (DBNs) to transliteration identification. The task of estimating transliteration similarity is not very different from specific identification tasks where DBNs have been successfully applied; it is also possible to adapt DBN models from the other identification domains to the transliteration identification domain. In particular, we investigate the applicability of a DBN framework initially proposed by Filali and Bilmes (2005) to learn edit distance estimation parameters for use in pronunciation classification. The DBN framework enables the specification of a variety of models representing different factors that can affect string similarity estimation. Three DBN models associated with two of the DBN classes originally specified by Filali and Bilmes (2005) have been tested on an experimental set up of Russian-English transliteration identification. Two of the DBN models result in high transliteration identification accuracy and combining the models leads to even much better transliteration identification accuracy.</abstract>
      <bibkey>nabende-2010-applying</bibkey>
    </paper>
    <paper id="623">
      <author><first>Alexandra</first><last>Balahur</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <author><first>Mijail</first><last>Kabadjov</last></author>
      <author><first>Vanni</first><last>Zavarella</last></author>
      <author><first>Erik</first><last>van der Goot</last></author>
      <author><first>Matina</first><last>Halkia</last></author>
      <author><first>Bruno</first><last>Pouliquen</last></author>
      <author><first>Jenya</first><last>Belyaeva</last></author>
      <title>Sentiment Analysis in the News</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/909_Paper.pdf</url>
      <abstract>Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles ― author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which we apply these concepts. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance.</abstract>
      <bibkey>balahur-etal-2010-sentiment</bibkey>
    </paper>
    <paper id="624">
      <author><first>Daniel</first><last>Sonntag</last></author>
      <author><first>Bogdan</first><last>Sacaleanu</last></author>
      <title>Speech Grammars for Textual Entailment Patterns in Multimodal Question Answering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/911_Paper.pdf</url>
      <abstract>Over the last several years, speech-based question answering (QA) has become very popular in contrast to pure search engine based approaches on a desktop. Open-domain QA systems are now much more powerful and precise, and they can be used in speech applications. Speech-based question answering systems often rely on predefined grammars for speech understanding. In order to improve the coverage of such complex AI systems, we reused speech patterns used to generate textual entailment patterns. These can make multimodal question understanding more robust. We exemplify this in the context of a domain-specific dialogue scenario. As a result, written text input components (e.g., in a textual input field) can deal with more flexible input according to the derived textual entailment patterns. A multimodal QA dialogue spanning over several domains of interest, i.e., personal address book entries, questions about the music domain and politicians and other celebrities, demonstrates how the textual input mode can be used in a multimodal dialogue shell.</abstract>
      <bibkey>sonntag-sacaleanu-2010-speech</bibkey>
    </paper>
    <paper id="625">
      <author><first>Jan</first><last>Strunk</last></author>
      <title>Enriching a Treebank to Investigate Relative Clause Extraposition in <fixed-case>G</fixed-case>erman</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/917_Paper.pdf</url>
      <abstract>I describe the construction of a corpus for research on relative clause extraposition in German based on the treebank TüBa-D/Z. I also define an annotation scheme for the relations between relative clauses and their antecedents which is added as a second annotation level to the syntactic trees. This additional annotation level allows for a direct representation of the relevant parts of the relative construction and also serves as a locus for the annotation of additional features which are partly automatically derived from the underlying treebank and partly added manually. Finally, I also report on the results of two pilot studies using this enriched treebank. The first study tests claims made in the theoretical literature on relative clause extraposition with regard to syntactic locality, definiteness, and restrictiveness. It shows that although the theoretical claims often go in the right direction, they go too far by positing categorical constraints that are not supported by the corpus data and thus underestimate the complexity of the data. The second pilot study goes one step in the direction of taking this complexity into account by demonstrating the potential of the enriched treebank for building a multivariate model of relative clause extraposition as a syntactic alternation.</abstract>
      <bibkey>strunk-2010-enriching</bibkey>
    </paper>
    <paper id="626">
      <author><first>Claude</first><last>de Loupy</last></author>
      <author><first>Marie</first><last>Guégan</last></author>
      <author><first>Christelle</first><last>Ayache</last></author>
      <author><first>Somara</first><last>Seng</last></author>
      <author><first>Juan-Manuel Torres</first><last>Moreno</last></author>
      <title>A <fixed-case>F</fixed-case>rench Human Reference Corpus for Multi-Document Summarization and Sentence Compression</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/919_Paper.pdf</url>
      <abstract>This paper presents two corpora produced within the RPM2 project: a multi-document summarization corpus and a sentence compression corpus. Both corpora are in French. The first one is the only one we know in this language. It contains 20 topics with 20 documents each. A first set of 10 documents per topic is summarized and then the second set is used to produce an update summarization (new information). 4 annotators were involved and produced a total of 160 abstracts. The second corpus contains all the sentences of the first one. 4 annotators were asked to compress the 8432 sentences. This is the biggest corpus of compressed sentences we know, whatever the language. The paper provides some figures in order to compare the different annotators: compression rates, number of tokens per sentence, percentage of tokens kept according to their POS, position of dropped tokens in the sentence compression phase, etc. These figures show important differences from an annotator to the other. Another point is the different strategies of compression used according to the length of the sentence.</abstract>
      <bibkey>de-loupy-etal-2010-french</bibkey>
    </paper>
    <paper id="627">
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Carrie</first><last>Lewis</last></author>
      <author><first>William D.</first><last>Lewis</last></author>
      <title>The Problems of Language Identification within Hugely Multilingual Data Sets</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/921_Paper.pdf</url>
      <abstract>As the data for more and more languages is finding its way into digital form, with an increasing amount of this data being posted to the Web, it has become possible to collect language data from the Web and create large multilingual resources, covering hundreds or even thousands of languages. ODIN, the Online Database of INterlinear text (Lewis, 2006), is such a resource. It currently consists of nearly 200,000 data points for over 1,000 languages, the data for which was harvested from linguistic documents on the Web. We identify a number of issues with language identification for such broad-coverage resources including the lack of training data, ambiguous language names, incomplete language code sets, and incorrect uses of language names and codes. After providing a short overview of existing language code sets maintained by the linguistic community, we discuss what linguists and the linguistic community can do to make the process of language identification easier.</abstract>
      <bibkey>xia-etal-2010-problems</bibkey>
    </paper>
    <paper id="628">
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <author><first>Ansaf</first><last>Salleb-Aoussi</last></author>
      <author><first>Vikas</first><last>Bhardwaj</last></author>
      <author><first>Nancy</first><last>Ide</last></author>
      <title>Word Sense Annotation of Polysemous Words by Multiple Annotators</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/922_Paper.pdf</url>
      <abstract>We describe results of a word sense annotation task using WordNet, involving half a dozen well-trained annotators on ten polysemous words for three parts of speech. One hundred sentences for each word were annotated. Annotators had the same level of training and experience, but interannotator agreement (IA) varied across words. There was some effect of part of speech, with higher agreement on nouns and adjectives, but within the words for each part of speech there was wide variation. This variation in IA does not correlate with number of senses in the inventory, or the number of senses actually selected by annotators. In fact, IA was sometimes quite high for words with many senses. We claim that the IA variation is due to the word meanings, contexts of use, and individual differences among annotators. We find some correlation of IA with sense confusability as measured by a sense confusion threshhold (CT). Data mining for association rules on a flattened data representation indicating each annotator's sense choices identifies outliers for some words, and systematic differences among pairs of annotators on others.</abstract>
      <bibkey>passonneau-etal-2010-word</bibkey>
    </paper>
    <paper id="629">
      <author><first>Michael</first><last>Gasser</last></author>
      <title>Expanding the Lexicon for a Resource-Poor Language Using a Morphological Analyzer and a Web Crawler</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/926_Paper.pdf</url>
      <abstract>Resource-poor languages may suffer from a lack of any of the basic resources that are fundamental to computational linguistics, including an adequate digital lexicon. Given the relatively small corpus of texts that exists for such languages, extending the lexicon presents a challenge. Languages with complex morphology present a special case, however, because individual words in these languages provide a great deal of information about the grammatical properties of the roots that they are based on. Given a morphological analyzer, it is even possible to extract novel roots from words. In this paper, we look at the case of Tigrinya, a Semitic language with limited lexical resources for which a morphological analyzer is available. It is shown that this analyzer applied to the list of more than 200,000 Tigrinya words that is extracted by a web crawler can extend the lexicon in two ways, by adding new roots and by inferring some of the derivational constraints that apply to known roots.</abstract>
      <bibkey>gasser-2010-expanding</bibkey>
    </paper>
    <paper id="630">
      <author><first>Susan Windisch</first><last>Brown</last></author>
      <author><first>Travis</first><last>Rood</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title>Number or Nuance: Which Factors Restrict Reliable Word Sense Annotation?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/927_Paper.pdf</url>
      <abstract>This study attempts to pinpoint the factors that restrict reliable word sense annotation, focusing on the influence of the number of senses annotators use and the semantic granularity of those senses. Both of these factors may be possible causes of low interannotator agreement (ITA) when tagging with fine-grained word senses, and, consequently, low WSD system performance (Ng et al., 1999; Snyder &amp; Palmer, 2004; Chklovski &amp; Mihalcea, 2002). If number of senses is the culprit, modifying the task to show fewer senses at a time could improve annotator reliability. However, if overly nuanced distinctions are the problem, then more general, coarse-grained distinctions may be necessary for annotator success and may be all that is needed to supply systems with the types of distinctions that people make. We describe three experiments that explore the role of these factors in annotation performance. Our results indicate that of these two factors, only the granularity of the senses restricts interannotator agreement, with broader senses resulting in higher annotation reliability.</abstract>
      <bibkey>brown-etal-2010-number</bibkey>
    </paper>
    <paper id="631">
      <author><first>Joshua B.</first><last>Gordon</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <title>An Evaluation Framework for Natural Language Understanding in Spoken Dialogue Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/928_Paper.pdf</url>
      <abstract>We present an evaluation framework to enable developers of information seeking, transaction based spoken dialogue systems to compare the robustness of natural language understanding (NLU) approaches across varying levels of word error rate and contrasting domains. We develop statistical and semantic parsing based approaches to dialogue act identification and concept retrieval. Voice search is used in each approach to ultimately query the database. Included in the framework is a method for developers to bootstrap a representative pseudo-corpus, which is used to estimate NLU performance in a new domain. We illustrate the relative merits of these NLU techniques by contrasting our statistical NLU approach with a semantic parsing method over two contrasting applications, our CheckItOut library system and the deployed Lets Go Public! system, across four levels of word error rate. We find that with respect to both dialogue act identification and concept retrieval, our statistical NLU approach is more likely to robustly accommodate the freer form, less constrained utterances of CheckItOut at higher word error rates than is possible with semantic parsing.</abstract>
      <bibkey>gordon-passonneau-2010-evaluation</bibkey>
    </paper>
    <paper id="632">
      <author><first>Andrew</first><last>Hickl</last></author>
      <author><first>Arnold</first><last>Jung</last></author>
      <author><first>Ying</first><last>Shi</last></author>
      <title>Multilingual Question Generation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/930_Paper.pdf</url>
      <bibkey>hickl-etal-2010-multilingual</bibkey>
    </paper>
    <paper id="633">
      <author><first>René</first><last>Witte</last></author>
      <author><first>Ninus</first><last>Khamis</last></author>
      <author><first>Juergen</first><last>Rilling</last></author>
      <title>Flexible Ontology Population from Text: The <fixed-case>O</fixed-case>wl<fixed-case>E</fixed-case>xporter</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/932_Paper.pdf</url>
      <abstract>Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modeling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domainand NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.</abstract>
      <bibkey>witte-etal-2010-flexible</bibkey>
    </paper>
    <paper id="634">
      <author><first>Rashmi</first><last>Prasad</last></author>
      <author><first>Aravind</first><last>Joshi</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <title>Exploiting Scope for Shallow Discourse Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/935_Paper.pdf</url>
      <abstract>We present an approach to automatically identifying the arguments of discourse connectives based on data from the Penn Discourse Treebank. Of the two arguments of connectives, called Arg1 and Arg2, we focus on Arg1, which has proven more challenging to identify. Our approach employs a sentence-based representation of arguments, and distinguishes ""intra-sentential connectives"", which take both their arguments in the same sentence, from ""inter-sentential connectives"", whose arguments are found in different sentences. The latter are further distinguished by paragraph position into ""ParaInit"" connectives, which appear in a paragraph-initial sentence, and ""ParaNonInit"" connectives, which appear elsewhere. The paper focusses on predicting Arg1 of Inter-sentential ParaNonInit connectives, presenting a set of scope-based filters that reduce the search space for Arg1 from all the previous sentences in the paragraph to a subset of them. For cases where these filters do not uniquely identify Arg1, coreference-based heuristics are employed. Our analysis shows an absolute 3% performance improvement over the high baseline of 83.3% for identifying Arg1 of Inter-sentential ParaNonInit connectives.</abstract>
      <bibkey>prasad-etal-2010-exploiting</bibkey>
    </paper>
    <paper id="635">
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <title><fixed-case>I</fixed-case>ndo<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/939_Paper.pdf</url>
      <abstract>India is a multilingual country where machine translation and cross lingual search are highly relevant problems. These problems require large resources- like wordnets and lexicons- of high quality and coverage. Wordnets are lexical structures composed of synsets and semantic relations. Synsets are sets of synonyms. They are linked by semantic relations like hypernymy (is-a), meronymy (part-of), troponymy (manner-of) etc. IndoWordnet is a linked structure of wordnets of major Indian languages from Indo-Aryan, Dravidian and Sino-Tibetan families. These wordnets have been created by following the expansion approach from Hindi wordnet which was made available free for research in 2006. Since then a number of Indian languages have been creating their wordnets. In this paper we discuss the methodology, coverage, important considerations and multifarious benefits of IndoWordnet. Case studies are provided for Marathi, Sanskrit, Bodo and Telugu, to bring out the basic methodology of and challenges involved in the expansion approach. The guidelines the lexicographers follow for wordnet construction are enumerated. The difference between IndoWordnet and EuroWordnet also is discussed.</abstract>
      <bibkey>bhattacharyya-2010-indowordnet</bibkey>
    </paper>
    <paper id="636">
      <author><first>Kirk</first><last>Roberts</last></author>
      <author><first>Srikanth</first><last>Gullapalli</last></author>
      <author><first>Cosmin Adrian</first><last>Bejan</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <title>A Linguistic Resource for Semantic Parsing of Motion Events</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/941_Paper.pdf</url>
      <abstract>This paper presents a corpus of annotated motion events and their event structure. We consider motion events triggered by a set of motion evoking words and contemplate both literal and figurative interpretations of them. Figurative motion events are extracted into the same event structure but are marked as figurative in the corpus. To represent the event structure of motion, we use the FrameNet annotation standard, which encodes motion in over 70 frames. In order to acquire a diverse set of texts that are different from FrameNet's, we crawled blog and news feeds for five different domains: sports, newswire, finance, military, and gossip. We then annotated these documents with an automatic FrameNet parser. Its output was manually corrected to account for missing and incorrect frames as well as missing and incorrect frame elements. The corpus, UTD-MotionEvent, may act as a resource for semantic parsing, detection of figurative language, spatial reasoning, and other tasks.</abstract>
      <bibkey>roberts-etal-2010-linguistic</bibkey>
    </paper>
    <paper id="637">
      <author><first>Jennifer</first><last>DeCamp</last></author>
      <title>Language Technology Resource Center</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/942_Paper.pdf</url>
      <abstract>This paper describes the Language Technology Resource Center (LTRC), a United States Government website for providing information and tools for users of languages (e.g., translators, analysts, systems administrators, researchers, developers, etc.) The LTRC provides information on a broad range of products and tools. It also provides a survey where product developers and researchers can provide the U.S. Government and the public with information about their work. A variety of reports are generated, including reports of all tools of a given type in a given language or dialect. Information is provided about standards, professional organizations, online resources, and other resources. The LTRC was developed and is run by MITRE, a Federally Funded Research and Development Center (FFRDC), on behalf of the U.S. Government. One of the major challenges for the future is to identify and provide information on the many new tools that are appearing. International collaboration is critical to cover this number of tools.</abstract>
      <bibkey>decamp-2010-language-technology</bibkey>
    </paper>
    <paper id="638">
      <author><first>Manuela</first><last>Sassi</last></author>
      <author><first>Gabriella</first><last>Pardelli</last></author>
      <author><first>Stefania</first><last>Biagioni</last></author>
      <author><first>Carlo</first><last>Carlesi</last></author>
      <author><first>Sara</first><last>Goggi</last></author>
      <title>A Digital Archive of Research Papers in Computer Science</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/945_Paper.pdf</url>
      <abstract>This paper presents the results of a terminological work conducted by the authors on a Digital Archives Net of the Italian National Research Council (CNR) in the field of Computer Science. In particular, the research tends to analyse the use of certain terms in Computer Science in order to verify their change over the time with the aim of retrieving from the net the very essence of documentation. Its main source is a reference corpus made up of 13,500 documents which collects the scientific productions of CNR. This study is divided in three sections: 1) an introductory one dedicated to the data extracted from the scientific documentation; 2) the second section is devoted to the description of the contents managed by the PUMA system; 3) the third part contains a statistical representation of terms extracted from archive: some comparison tables between the occurrences of the most used terms in the scientific documentation will be created and diagrams with percentages about the most frequently used terms will be displayed too. Indexes and concordances will allow to reflect on the use of certain terms in this field and give possible keys for having access to the extraction of knowledge.</abstract>
      <bibkey>sassi-etal-2010-digital</bibkey>
    </paper>
    <paper id="639">
      <author><first>Zygmunt</first><last>Vetulani</last></author>
      <author><first>Marek</first><last>Kubis</last></author>
      <author><first>Tomasz</first><last>Obrębski</last></author>
      <title><fixed-case>P</fixed-case>ol<fixed-case>N</fixed-case>et — <fixed-case>P</fixed-case>olish <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et: Data and Tools</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/947_Paper.pdf</url>
      <abstract>This paper presents the PolNet-Polish WordNet project which aims at building a linguistically oriented ontology for Polish compatible with other WordNet projects such as Princeton WordNet, EuroWordNet and other similarly organized ontologies. The main idea behind this kind of ontologies is to use words related by synonymy to construct formal representations of concepts. In the paper we sketch the PolNet project methodology and implementation. We present data obtained so far, as well as the WQuery tool for querying and maintaining PolNet. WQuery is a query language that make use of data types based on synsets, word senses and various semantic relations which occur in wordnet-like lexical databases. The tool is particularly useful to deal with complex querying tasks like searching for cycles in semantic relations, finding isolated synsets or computing overall statistics. Both data and tools presented in this paper have been applied within an advanced AI system POLINT-112-SMS with emulated natural language competence, where they are used in the understanding subsystem.</abstract>
      <bibkey>vetulani-etal-2010-polnet</bibkey>
    </paper>
    <paper id="640">
      <author><first>Victoria</first><last>Arranz</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <title><fixed-case>ELRA</fixed-case>’s Services 15 Years on...Sharing and Anticipating the Community</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/948_Paper.pdf</url>
      <abstract>15 years have gone by and ELRA continues embracing the needs of the HLT community to design its services and to implement them through its operational body, ELDA. The needs of the community have become much more ambitious...Larger language resources (LR), better quality ones (how do we reach a compromise between price ― maybe free ― and quality?), more annotations, at different levels and for different modalities...easy access to these LRs and solved IPR issues, appropriate and adaptable licensing schemas...large activity in HLT evaluation, both in terms of setting up the evaluation and in helping produce all necessary data, protocols, specifications as well as conducting the whole process...producing the LRs researchers and developers need, LRs for a wide variety of activities and technologies...for development, for training, for evaluation...Disseminating all knowledge in the field, whether generated at ELRA or elsewhere...keeping the community up to date with what goes on regularly (LREC conferences, LangTech, Newsletters, HLT Evaluation Portal, etc.). Needless to say, part of ELRAs evolution implies facing and anticipating the realities of the new Internet and data exchange era and remaining a LR backbone...looking into new models of LR data centres and platforms, LR access and exchange via web services, new models for infrastructures and repositories with even higher collaboration to make it happen. ELRA/ELDA participate in a number of international projects focused on this new production and sharing schema that will be detailed in the current paper.</abstract>
      <bibkey>arranz-choukri-2010-elras</bibkey>
    </paper>
    <paper id="641">
      <author><first>Youssef Aït</first><last>Ouguengay</last></author>
      <author><first>Aïcha</first><last>Bouhjar</last></author>
      <title>For Standardised <fixed-case>A</fixed-case>mazigh Linguistic Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/949_Paper.pdf</url>
      <abstract>Amazigh language and culture may well be viewed to have known an unprecedented booming in Morocco : more than a hundred- which are published by the Royal Institute of Amazigh Culture (IRCAM), an institution created in 2001 to preserve, promote and endorse Amazigh culture in all its dimensions. Crucially, publications in the Amazigh language would not have seen light without the valiant attempts to upgrade the language on the linguistic and technological levels. The central thrust of this contribution is to provide a vista about the whole range of actions carried out by IRCAM. Of prime utility to this presentation is what was accomplished to supply Amazigh with the necessary tools and corpora without which the Amazigh language would emphatically fail to have a place in the world of NITCs. After a brief description of the prime specificities that characterise the standardisation of Amazigh in Morocco, a retrospective on the basic computer tools now available for the processing of Amazigh will be set out. It is concluded that the homogenisation of a considerable number of corpora should, by right, be viewed as a strategic move and an incontrovertible prerequisite to the computerisation of Amazigh,</abstract>
      <bibkey>ouguengay-bouhjar-2010-standardised</bibkey>
    </paper>
    <paper id="642">
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>D. Terence</first><last>Langendoen</last></author>
      <author><first>Johannes</first><last>Leveling</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <title>A Road Map for Interoperable Language Resource Metadata</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/951_Paper.pdf</url>
      <abstract>LRs remain expensive to create and thus rare relative to demand across languages and technology types. The accidental re-creation of an LR that already exists is a nearly unforgivable waste of scarce resources that is unfortunately not so easy to avoid. The number of catalogs the HLT researcher must search, with their different formats, make it possible to overlook an existing resource. This paper sketches the sources of this problem and outlines a proposal to rectify along with a new vision of LR cataloging that will to facilitates the documentation and exploitation of a much wider range of LRs than previously considered.</abstract>
      <bibkey>cieri-etal-2010-road</bibkey>
    </paper>
    <paper id="643">
      <author><first>Quan</first><last>Nguyen</last></author>
      <author><first>Michael</first><last>Kipp</last></author>
      <title>Annotation of Human Gesture using 3<fixed-case>D</fixed-case> Skeleton Controls</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/952_Paper.pdf</url>
      <abstract>The manual transcription of human gesture behavior from video for linguistic analysis is a work-intensive process that results in a rather coarse description of the original motion. We present a novel approach for transcribing gestural movements: by overlaying an articulated 3D skeleton onto the video frame(s) the human coder can replicate original motions on a pose-by-pose basis by manipulating the skeleton. Our tool is integrated in the ANVIL tool so that both symbolic interval data and 3D pose data can be entered in a single tool. Our method allows a relatively quick annotation of human poses which has been validated in a user study. The resulting data are precise enough to create animations that match the original speaker's motion which can be validated with a realtime viewer. The tool can be applied for a variety of research topics in the areas of conversational analysis, gesture studies and intelligent virtual agents.</abstract>
      <bibkey>nguyen-kipp-2010-annotation</bibkey>
    </paper>
    <paper id="644">
      <author><first>Michal</first><last>Gishri</last></author>
      <author><first>Vered</first><last>Silber-Varod</last></author>
      <author><first>Ami</first><last>Moyal</last></author>
      <title>Lexicon Design for Transcription of Spontaneous Voice Messages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/953_Paper.pdf</url>
      <abstract>Building a comprehensive pronunciation lexicon is a crucial element in the success of any speech recognition engine. The first stage of lexicon design involves the compilation of a comprehensive word list that keeps the Out-Of-Vocabulary (OOV) word rate to a minimum. The second stage involves providing optimized phonemic representations for all lexical items on the list. The research presented here focuses on the first stage of lexicon design ― word list compilation, and describes the methodologies employed in the collection of a pronunciation lexicon designed for the purpose of American English voice message transcription using speech recognition. The lexicon design used is based on a topic domain structure with a target of 90% word coverage for each domain. This differs somewhat from standard approaches where probable words from textual corpora are extracted. This paper raises four issues involved in lexicon design for the transcription of spontaneous voice messages: the inclusion of interjections and other characteristics common to spontaneous speech; the identification of unique messaging terminology; the relative ratio of proper nouns to common words; and the overall size of the lexicon.</abstract>
      <bibkey>gishri-etal-2010-lexicon</bibkey>
    </paper>
    <paper id="645">
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <title>Adapting to Trends in Language Resource Development: A Progress Report on <fixed-case>LDC</fixed-case> Activities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2010/pdf/954_Paper.pdf</url>
      <abstract>This paper describes changing needs among the communities that exploit language resources and recent LDC activities and publications that support those needs by providing greater volumes of data and associated resources in a growing inventory of languages with ever more sophisticated annotation. Specifically, it covers the evolving role of data centers with specific emphasis on the LDC, the publications released by the LDC in the two years since our last report and the sponsored research programs that provide LRs initially to participants in those programs but eventually to the larger HLT research communities and beyond.</abstract>
      <bibkey>cieri-liberman-2010-adapting</bibkey>
    </paper>
  </volume>
</collection>
