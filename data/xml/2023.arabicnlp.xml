<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.arabicnlp">
  <volume id="1" ingest-date="2023-11-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of ArabicNLP 2023</booktitle>
      <editor><first>Hassan</first><last>Sawaf</last></editor>
      <editor><first>Samhaa</first><last>El-Beltagy</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Ahmed</first><last>Abdelali</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Ibrahim</first><last>Abu Farha</last></editor>
      <editor><first>Nizar</first><last>Habash</last></editor>
      <editor><first>Salam</first><last>Khalifa</last></editor>
      <editor><first>Amr</first><last>Keleg</last></editor>
      <editor><first>Hatem</first><last>Haddad</last></editor>
      <editor><first>Imed</first><last>Zitouni</last></editor>
      <editor><first>Khalil</first><last>Mrini</last></editor>
      <editor><first>Rawan</first><last>Almatham</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore (Hybrid)</address>
      <month>December</month>
      <year>2023</year>
      <url hash="563c2570">2023.arabicnlp-1</url>
      <venue>arabicnlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="137b6fce">2023.arabicnlp-1.0</url>
      <bibkey>arabicnlp-ws-2023-arabicnlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Violet: A Vision-Language Model for <fixed-case>A</fixed-case>rabic Image Captioning with Gemini Decoder</title>
      <author><first>Abdelrahman</first><last>Mohamed</last></author>
      <author><first>Fakhraddin</first><last>Alwajih</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Alcides</first><last>Inciarte</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>1-11</pages>
      <abstract>Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed Violet. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. Violet performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of 61.2 on our manually annotated dataset and achieves an improvement of 13 points on Flickr8k.</abstract>
      <url hash="b60aa624">2023.arabicnlp-1.1</url>
      <bibkey>mohamed-etal-2023-violet</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Nâbra: <fixed-case>S</fixed-case>yrian <fixed-case>A</fixed-case>rabic Dialects with Morphological Annotations</title>
      <author><first>Amal</first><last>Nayouf</last></author>
      <author><first>Tymaa</first><last>Hammouda</last></author>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Fadi</first><last>Zaraket</last></author>
      <author><first>Mohamad-Bassam</first><last>Kurdy</last></author>
      <pages>12-23</pages>
      <abstract>This paper presents Nâbra (نَبْرَة), a corpora of Syrian Arabic dialects with morphological annotations. A team of Syrian natives collected more than <tex-math>6K</tex-math> sentences containing about <tex-math>60K</tex-math> words from several sources including social media posts, scripts of movies and series, lyrics of songs and local proverbs to build Nâbra. Nâbra covers several local Syrian dialects including those of Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and Suwayda. A team of nine annotators annotated the <tex-math>60K</tex-math> tokens with full morphological annotations across sentence contexts. We trained the annotators to follow methodological annotation guidelines to ensure unique morpheme annotations, and normalized the annotations. F1 and <tex-math>\kappa</tex-math> agreement scores ranged between 74% and 98% across features, showing the excellent quality of Nâbra annotations. Our corpora are open-source and publicly available as part of the Currasat portal https://sina.birzeit.edu/currasat.</abstract>
      <url hash="6d461638">2023.arabicnlp-1.2</url>
      <bibkey>nayouf-etal-2023-nabra</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>HICMA</fixed-case>: The Handwriting Identification for Calligraphy and Manuscripts in <fixed-case>A</fixed-case>rabic Dataset</title>
      <author><first>Anis</first><last>Ismail</last></author>
      <author><first>Zena</first><last>Kamel</last></author>
      <author><first>Reem</first><last>Mahmoud</last></author>
      <pages>24-32</pages>
      <abstract>Arabic is one of the most globally spoken languages with more than 313 million speakers worldwide. Arabic handwriting is known for its cursive nature and the variety of writing styles used. Despite the increase in effort to digitize artistic and historical elements, no public dataset was released to deal with Arabic text recognition for realistic manuscripts and calligraphic text. We present the Handwriting Identification of Manuscripts and Calligraphy in Arabic (HICMA) dataset as the first publicly available dataset with real-world and diverse samples of Arabic handwritten text in manuscripts and calligraphy. With more than 5,000 images across five different styles, the HICMA dataset includes image-text pairs and style labels for all images. We further present a comparison of the current state-of-the-art optical character recognition models in Arabic and benchmark their performance on the HICMA dataset, which serves as a baseline for future works. Both the HICMA dataset and its benchmarking tool are made available to the public under the CC BY-NC 4.0 license in the hope that the presented work opens the door to further enhancements of complex Arabic text recognition.</abstract>
      <url hash="d3282e98">2023.arabicnlp-1.3</url>
      <bibkey>ismail-etal-2023-hicma</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.3</doi>
      <video href="2023.arabicnlp-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Automated De-Identification of <fixed-case>A</fixed-case>rabic Medical Records</title>
      <author><first>Veysel</first><last>Kocaman</last></author>
      <author><first>Youssef</first><last>Mellah</last></author>
      <author><first>Hasham</first><last>Haq</last></author>
      <author><first>David</first><last>Talby</last></author>
      <pages>33-40</pages>
      <abstract>As Electronic Health Records (EHR) become ubiquitous in healthcare systems worldwide, including in Arabic-speaking countries, the dual imperative of safeguarding patient privacy and leveraging data for research and quality improvement grows. This paper presents a first-of-its-kind automated de-identification pipeline for medical text specifically tailored for the Arabic language. This includes accurate medical Named Entity Recognition (NER) for identifying personal information; data obfuscation models to replace sensitive entities with fake entities; and an implementation that natively scales to large datasets on commodity clusters. This research makes two contributions. First, we adapt two existing NER architectures— BERT For Token Classification (BFTC) and BiLSTM-CNN-Char – to accommodate the unique syntactic and morphological characteristics of the Arabic language. Comparative analysis suggests that BFTC models outperform Bi-LSTM models, achieving higher F1 scores for both identifying and redacting personally identifiable information (PII) from Arabic medical texts. Second, we augment the deep learning models with a contextual parser engine to handle commonly missed entities. Experiments show that the combined pipeline demonstrates superior performance with micro F1 scores ranging from 0.94 to 0.98 on the test dataset, which is a translated version of the i2b2 2014 de-identification challenge, across 17 sensitive entities. This level of accuracy is in line with that achieved with manual de-identification by domain experts, suggesting that a fully automated and scalable process is now viable.</abstract>
      <url hash="edda6a28">2023.arabicnlp-1.4</url>
      <bibkey>kocaman-etal-2023-automated</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.4</doi>
      <video href="2023.arabicnlp-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title><fixed-case>A</fixed-case>r<fixed-case>TST</fixed-case>: <fixed-case>A</fixed-case>rabic Text and Speech Transformer</title>
      <author><first>Hawau</first><last>Toyin</last></author>
      <author><first>Amirbek</first><last>Djanibekov</last></author>
      <author><first>Ajinkya</first><last>Kulkarni</last></author>
      <author><first>Hanan</first><last>Aldarmaki</last></author>
      <pages>41-51</pages>
      <abstract>We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.</abstract>
      <url hash="eae2ffef">2023.arabicnlp-1.5</url>
      <bibkey>toyin-etal-2023-artst</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>TARJAMAT</fixed-case>: Evaluation of Bard and <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> on Machine Translation of Ten <fixed-case>A</fixed-case>rabic Varieties</title>
      <author><first>Karima</first><last>Kadaoui</last></author>
      <author><first>Samar</first><last>Magdy</last></author>
      <author><first>Abdul</first><last>Waheed</last></author>
      <author><first>Md Tawkat Islam</first><last>Khondaker</last></author>
      <author><first>Ahmed</first><last>El-Shangiti</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>52-75</pages>
      <abstract>Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level dialectal variants. Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems. On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate. Finally, we undertake a human-centric study to scrutinize the efficacy of the relatively recent model, Bard, in following human instructions during translation tasks. Our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.</abstract>
      <url hash="03c39121">2023.arabicnlp-1.6</url>
      <bibkey>kadaoui-etal-2023-tarjamat</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Leveraging Domain Adaptation and Data Augmentation to Improve Qur’anic <fixed-case>IR</fixed-case> in <fixed-case>E</fixed-case>nglish and <fixed-case>A</fixed-case>rabic</title>
      <author><first>Vera</first><last>Pavlova</last></author>
      <pages>76-88</pages>
      <abstract>In this work, we approach the problem of Qur’anic information retrieval (IR) in Arabic and English. Using the latest state-of-the-art methods in neural IR, we research what helps to tackle this task more efficiently. Training retrieval models requires a lot of data, which is difficult to obtain for training in-domain. Therefore, we commence with training on a large amount of general domain data and then continue training on in-domain data. To handle the lack of in-domain data, we employed a data augmentation technique, which considerably improved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in Qur’anic IR for both English and Arabic. The absence of an Islamic corpus and domain-specific model for IR task in English motivated us to address this lack of resources and take preliminary steps of the Islamic corpus compilation and domain-specific language model (LM) pre-training, which helped to improve the performance of the retrieval models that use the domain-specific LM as the shared backbone. We examined several language models (LMs) in Arabic to select one that efficiently deals with the Qur’anic IR task. Besides transferring successful experiments from English to Arabic, we conducted additional experiments with retrieval task in Arabic to amortize the scarcity of general domain datasets used to train the retrieval models. Handling Qur’anic IR task combining English and Arabic allowed us to enhance the comparison and share valuable insights across models and languages.</abstract>
      <url hash="68aeeb0f">2023.arabicnlp-1.7</url>
      <bibkey>pavlova-2023-leveraging</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>LANS</fixed-case>: Large-scale <fixed-case>A</fixed-case>rabic News Summarization Corpus</title>
      <author><first>Abdulaziz</first><last>Alhamadani</last></author>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Jianfeng</first><last>He</last></author>
      <author><first>Aadyant</first><last>Khatri</last></author>
      <author><first>Chang-Tien</first><last>Lu</last></author>
      <pages>89-100</pages>
      <abstract>Text summarization has been intensively studied in many languages, and some languages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is still in its developing stages. Existing ATS datasets are either small or lack diversity. We build, LANS, a large-scale and diverse dataset for Arabic Text Summarization task. LANS offers 8.4 million articles and their summaries extracted from newspapers websites’ metadata between 1999 and 2019. The high-quality and diverse summaries are written by journalists from 22 major Arab newspapers and include an eclectic mix of at least more than 7 topics from each source. We conduct an intrinsic evaluation on LANS by both automatic and human evaluations. Human evaluation of 1,000 random samples reports 95.4% accuracy for our collected summaries, and automatic evaluation quantifies the diversity and abstractness of the summaries.</abstract>
      <url hash="841721b1">2023.arabicnlp-1.8</url>
      <bibkey>alhamadani-etal-2023-lans</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Beyond <fixed-case>E</fixed-case>nglish: Evaluating <fixed-case>LLM</fixed-case>s for <fixed-case>A</fixed-case>rabic Grammatical Error Correction</title>
      <author><first>Sang</first><last>Kwon</last></author>
      <author><first>Gagan</first><last>Bhatia</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>101-119</pages>
      <abstract>Large language models (LLMs) finetuned to follow human instruction have recently exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC), especially on languages other than English, remains significantly unexplored. In this work, we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a complex task due to Arabic’s rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to 65.49 F1 score under expert prompting (approximately 5 points higher than our established baseline). Despite these positive results, we find that instruction finetuned models, regardless of their size, are still outperformed by fully finetuned ones, even if they are significantly smaller in size. This disparity highlights substantial room for improvements for LLMs. Inspired by methods used in low-resource machine translation, we also develop a method exploiting synthetic data that significantly outperforms previous models on two standard Arabic benchmarks. Our best model achieves a new SOTA on Arabic GEC, with 73.29 and 73.26 F1 on the 2014 and 2015 QALB datasets, respectively, compared to peer-reviewed published baselines.</abstract>
      <url hash="d0f75440">2023.arabicnlp-1.9</url>
      <bibkey>kwon-etal-2023-beyond</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Aswat: <fixed-case>A</fixed-case>rabic Audio Dataset for Automatic Speech Recognition Using Speech-Representation Learning</title>
      <author><first>Lamya</first><last>Alkanhal</last></author>
      <author><first>Abeer</first><last>Alessa</last></author>
      <author><first>Elaf</first><last>Almahmoud</last></author>
      <author><first>Rana</first><last>Alaqil</last></author>
      <pages>120-127</pages>
      <abstract>Recent advancements in self-supervised speech-representation learning for automatic speech recognition (ASR) approaches have significantly improved the results on many benchmarks with low-cost data labeling. In this paper, we train two self-supervised frameworks for ASR, namely wav2vec, and data2vec, in which we conduct multiple experiments and analyze their results. Furthermore, we introduce Aswat dataset, which covers multiple genres and features speakers with vocal variety. Aswat contains 732 hours of clean Arabic speech that can be used in the pretraining task for learning latent speech representations, which results in achieving a lower word error rate (WER) in Arabic ASR. We report the baseline results and achieve state-of-the-art WERs of 11.7% and 10.3% on Common Voice (CV) and the second round of Multi-Genre Broadcast (MGB-2) respectively, as a result of including our dataset Aswat.</abstract>
      <url hash="11b2cc8a">2023.arabicnlp-1.10</url>
      <bibkey>alkanhal-etal-2023-aswat</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.10</doi>
      <video href="2023.arabicnlp-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Analyzing Multilingual Competency of <fixed-case>LLM</fixed-case>s in Multi-Turn Instruction Following: A Case Study of <fixed-case>A</fixed-case>rabic</title>
      <author><first>Sabri</first><last>Boughorbel</last></author>
      <author><first>Majd</first><last>Hawasly</last></author>
      <pages>128-139</pages>
      <abstract>While significant progress has been made in benchmarking Large Language Models (LLMs) across various tasks, there is a lack of comprehensive evaluation of their abilities in responding to multi-turn instructions in less-commonly tested languages like Arabic. Our paper offers a detailed examination of the proficiency of open LLMs in such scenarios in Arabic. Utilizing a customized Arabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a uniform evaluator for both English and Arabic queries to assess and compare the performance of the LLMs on various open-ended tasks. Our findings reveal variations in model responses on different task categories, e.g., logic vs. literacy, when instructed in English or Arabic. We find that fine-tuned base models using multilingual and multi-turn datasets could be competitive to models trained from scratch on multilingual data. Finally, we hypothesize that an ensemble of small, open LLMs could perform competitively to proprietary LLMs on the benchmark.</abstract>
      <url hash="fac90b87">2023.arabicnlp-1.11</url>
      <bibkey>boughorbel-hawasly-2023-analyzing</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Cross-Dialectal Named Entity Recognition in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Niama</first><last>El Elkhbir</last></author>
      <author><first>Urchade</first><last>Zaratiana</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>140-149</pages>
      <abstract>In this paper, we study the transferability of Named Entity Recognition (NER) models between Arabic dialects. This question is important because the available manually-annotated resources are not distributed equally across dialects: Modern Standard Arabic (MSA) is much richer than other dialects for which little to no datasets exist. How well does a NER model, trained on MSA, perform on other dialects? To answer this question, we construct four datasets. The first is an MSA dataset extracted from the ACE 2005 corpus. The others are datasets for Egyptian, Morocan and Syrian which we manually annotate following the ACE guidelines. We train a span-based NER model on top of a pretrained language model (PLM) encoder on the MSA data and study its performance on the other datasets in zero-shot settings. We study the performance of multiple PLM encoders from the literature and show that they achieve acceptable performance with no annotation effort. Our annotations and models are publicly available (<url>https://github.com/niamaelkhbir/Arabic-Cross-Dialectal-NER</url>).</abstract>
      <url hash="66137563">2023.arabicnlp-1.12</url>
      <bibkey>elkhbir-etal-2023-cross</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Enhancing <fixed-case>A</fixed-case>rabic Machine Translation for <fixed-case>E</fixed-case>-commerce Product Information: Data Quality Challenges and Innovative Selection Approaches</title>
      <author><first>Bryan</first><last>Zhang</last></author>
      <author><first>Salah</first><last>Danial</last></author>
      <author><first>Stephan</first><last>Walter</last></author>
      <pages>150-157</pages>
      <abstract>Product information in e-commerce is usually localized using machine translation (MT) systems. Arabic language has rich morphology and dialectal variations, so Arabic MT in e-commerce training requires a larger volume of data from diverse data sources; Given the dynamic nature of e-commerce, such data needs to be acquired periodically to update the MT. Consequently, validating the quality of training data periodically within an industrial setting presents a notable challenge. Meanwhile, the performance of MT systems is significantly impacted by the quality and appropriateness of the training data. Hence, this study first examines the Arabic MT in e-commerce and investigates the data quality challenges for English-Arabic MT in e-commerce then proposes heuristics-based and topic-based data selection approaches to improve MT for product information. Both online and offline experiment results have shown our proposed approaches are effective, leading to improved shopping experiences for customers.</abstract>
      <url hash="78f5230a">2023.arabicnlp-1.13</url>
      <bibkey>zhang-etal-2023-enhancing-arabic</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>IDRISI</fixed-case>-<fixed-case>D</fixed-case>: <fixed-case>A</fixed-case>rabic and <fixed-case>E</fixed-case>nglish Datasets and Benchmarks for Location Mention Disambiguation over Disaster Microblogs</title>
      <author><first>Reem</first><last>Suwaileh</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <author><first>Muhammad</first><last>Imran</last></author>
      <pages>158-169</pages>
      <abstract>Extracting and disambiguating geolocation information from social media data enables effective disaster management, as it helps response authorities; for example, locating incidents for planning rescue activities and affected people for evacuation. Nevertheless, the dearth of resources and tools hinders the development and evaluation of Location Mention Disambiguation (LMD) models in the disaster management domain. Consequently, the LMD task is greatly understudied, especially for the low resource languages such as Arabic. To fill this gap, we introduce IDRISI-D, the largest to date English and the first Arabic public LMD datasets. Additionally, we introduce a modified hierarchical evaluation framework that offers a lenient and nuanced evaluation of LMD systems. We further benchmark IDRISI-D datasets using representative baselines and show the competitiveness of BERT-based models.</abstract>
      <url hash="d0c73ca6">2023.arabicnlp-1.14</url>
      <bibkey>suwaileh-etal-2023-idrisi-arabic</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>C</fixed-case>amel<fixed-case>P</fixed-case>arser2.0: A State-of-the-Art Dependency Parser for <fixed-case>A</fixed-case>rabic</title>
      <author><first>Ahmed</first><last>Elshabrawy</last></author>
      <author><first>Muhammed</first><last>AbuOdeh</last></author>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>170-180</pages>
      <abstract>We present CamelParser2.0, an open-source Python-based Arabic dependency parser targeting two popular Arabic dependency formalisms, the Columbia Arabic Treebank (CATiB), and Universal Dependencies (UD). The CamelParser2.0 pipeline handles the processing of raw text and produces tokenization, part-of-speech and rich morphological features. As part of developing CamelParser2.0, we explore many system design hyper-parameters, such as parsing model architecture and pretrained language model selection, achieving new state-of-the-art performance across diverse Arabic genres under gold and predicted tokenization settings.</abstract>
      <url hash="f6e5ed81">2023.arabicnlp-1.15</url>
      <bibkey>elshabrawy-etal-2023-camelparser2</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>GARI</fixed-case>: Graph Attention for Relative Isomorphism of <fixed-case>A</fixed-case>rabic Word Embeddings</title>
      <author><first>Muhammad</first><last>Ali</last></author>
      <author><first>Maha</first><last>Alshmrani</last></author>
      <author><first>Jianbin</first><last>Qin</last></author>
      <author><first>Yan</first><last>Hu</last></author>
      <author><first>Di</first><last>Wang</last></author>
      <pages>181-190</pages>
      <abstract>Bilingual Lexical Induction (BLI) is a core challenge in NLP, it relies on the relative isomorphism of individual embedding spaces. Existing attempts aimed at controlling the relative isomorphism of different embedding spaces fail to incorporate the impact of semantically related words in the model training objective. To address this, we propose GARI that combines the distributional training objectives with multiple isomorphism losses guided by the graph attention network. GARI considers the impact of semantical variations of words in order to define the relative isomorphism of the embedding spaces. Experimental evaluation using the Arabic language data set shows that GARI outperforms the existing research by improving the average P@1 by a relative score of up to 40.95% and 76.80% for in-domain and domain mismatch settings respectively.</abstract>
      <url hash="0bb956ab">2023.arabicnlp-1.16</url>
      <bibkey>ali-etal-2023-gari</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>A</fixed-case>r<fixed-case>T</fixed-case>rivia: Harvesting <fixed-case>A</fixed-case>rabic <fixed-case>W</fixed-case>ikipedia to Build A New <fixed-case>A</fixed-case>rabic Question Answering Dataset</title>
      <author><first>Sultan</first><last>Alrowili</last></author>
      <author><first>K</first><last>Vijay-Shanker</last></author>
      <pages>191-207</pages>
      <abstract>We present ArTrivia, a new Arabic question-answering dataset consisting of more than 10,000 question-answer pairs along with relevant passages, covering a wide range of 18 diverse topics in Arabic. We created our dataset using a newly proposed pipeline that leverages diverse structured data sources from Arabic Wikipedia. Moreover, we conducted a comprehensive statistical analysis of ArTrivia and assessed the performance of each component in our pipeline. Additionally, we compared the performance of ArTrivia against the existing TyDi QA dataset using various experimental setups. Our analysis highlights the significance of often overlooked aspects in dataset creation, such as answer normalization, in enhancing the quality of QA datasets. Our evaluation also shows that ArTrivia presents more challenging and out-of-distribution questions to TyDi, raising questions about the feasibility of using ArTrivia as a complementary dataset to TyDi.</abstract>
      <url hash="0bb8cd9b">2023.arabicnlp-1.17</url>
      <bibkey>alrowili-vijay-shanker-2023-artrivia</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.17</doi>
      <video href="2023.arabicnlp-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title><fixed-case>A</fixed-case>r<fixed-case>S</fixed-case>arcas<fixed-case>M</fixed-case>oji Dataset: The Emoji Sentiment Roles in <fixed-case>A</fixed-case>rabic Ironic Contexts</title>
      <author><first>Shatha Ali A.</first><last>Hakami</last></author>
      <author><first>Robert</first><last>Hendley</last></author>
      <author><first>Phillip</first><last>Smith</last></author>
      <pages>208-217</pages>
      <abstract>In digital communication, emoji are essential in decoding nuances such as irony, sarcasm, and humour. However, their incorporation in Arabic natural language processing (NLP) has been cautious because of the perceived complexities of the Arabic language. This paper introduces ArSarcasMoji, a dataset of 24,630 emoji-augmented texts, with 17. 5% that shows irony. Through our analysis, we highlight specific emoji patterns paired with sentiment roles that denote irony in Arabic texts. The research counters prevailing notions, emphasising the importance of emoji’s role in understanding Arabic textual irony, and addresses their potential for accurate irony detection in Arabic digital content.</abstract>
      <url hash="5f8ccd03">2023.arabicnlp-1.18</url>
      <bibkey>hakami-etal-2023-arsarcasmoji</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Performance Implications of Using Unrepresentative Corpora in <fixed-case>A</fixed-case>rabic Natural Language Processing</title>
      <author><first>Saied</first><last>Alshahrani</last></author>
      <author><first>Norah</first><last>Alshahrani</last></author>
      <author><first>Soumyabrata</first><last>Dey</last></author>
      <author><first>Jeanna</first><last>Matthews</last></author>
      <pages>218-231</pages>
      <abstract>Wikipedia articles are a widely used source of training data for Natural Language Processing (NLP) research, particularly as corpora for low-resource languages like Arabic. However, it is essential to understand the extent to which these corpora reflect the representative contributions of native speakers, especially when many entries in a given language are directly translated from other languages or automatically generated through automated mechanisms. In this paper, we study the performance implications of using inorganic corpora that are not representative of native speakers and are generated through automated techniques such as bot generation or automated template-based translation. The case of the Arabic Wikipedia editions gives a unique case study of this since the Moroccan Arabic Wikipedia edition (ARY) is small but representative, the Egyptian Arabic Wikipedia edition (ARZ) is large but unrepresentative, and the Modern Standard Arabic Wikipedia edition (AR) is both large and more representative. We intrinsically evaluate the performance of two main NLP upstream tasks, namely word representation and language modeling, using word analogy evaluations and fill-mask evaluations using our two newly created datasets: Arab States Analogy Dataset (ASAD) and Masked Arab States Dataset (MASD). We demonstrate that for good NLP performance, we need both large and organic corpora; neither alone is sufficient. We show that producing large corpora through automated means can be a counter-productive, producing models that both perform worse and lack cultural richness and meaningful representation of the Arabic language and its native speakers.</abstract>
      <url hash="fa9c0ab4">2023.arabicnlp-1.19</url>
      <bibkey>alshahrani-etal-2023-performance</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Octopus: A Multitask Model and Toolkit for <fixed-case>A</fixed-case>rabic Natural Language Generation</title>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>232-243</pages>
      <abstract>Understanding Arabic text and generating human-like responses is a challenging task. While many researchers have proposed models and solutions for individual problems, there is an acute shortage of a comprehensive Arabic natural language generation toolkit that is capable of handling a wide range of tasks. In this work, we present a robust Arabic text-to-text Transformer model, namely AraT5v2, methodically trained on extensive and diverse data, utilizing an extended sequence length of 2,048 tokens. We explore various pretraining strategies including unsupervised, supervised, and joint pertaining, under both single and multitask settings. Our models outperform competitive baselines with large margins. We take our work one step further by developing and publicly releasing OCTOPUS, a Python-based package and command-line toolkit tailored for eight Arabic generation tasks all exploiting a single model. We provide a link to the models and the toolkit through our public repository.</abstract>
      <url hash="afa812d6">2023.arabicnlp-1.20</url>
      <bibkey>elmadany-etal-2023-octopus</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>A</fixed-case>l<fixed-case>G</fixed-case>hafa Evaluation Benchmark for <fixed-case>A</fixed-case>rabic Language Models</title>
      <author><first>Ebtesam</first><last>Almazrouei</last></author>
      <author><first>Ruxandra</first><last>Cojocaru</last></author>
      <author><first>Michele</first><last>Baldo</last></author>
      <author><first>Quentin</first><last>Malartic</last></author>
      <author><first>Hamza</first><last>Alobeidli</last></author>
      <author><first>Daniele</first><last>Mazzotta</last></author>
      <author><first>Guilherme</first><last>Penedo</last></author>
      <author><first>Giulia</first><last>Campesan</last></author>
      <author><first>Mugariya</first><last>Farooq</last></author>
      <author><first>Maitha</first><last>Alhammadi</last></author>
      <author><first>Julien</first><last>Launay</last></author>
      <author><first>Badreddine</first><last>Noune</last></author>
      <pages>244-275</pages>
      <abstract>Recent advances in the space of Arabic large language models have opened up a wealth of potential practical applications. From optimal training strategies, large scale data acquisition and continuously increasing NLP resources, the Arabic LLM landscape has improved in a very short span of time, despite being plagued by training data scarcity and limited evaluation resources compared to English. In line with contributing towards this ever-growing field, we introduce AlGhafa, a new multiple-choice evaluation benchmark for Arabic LLMs. For showcasing purposes, we train a new suite of models, including a 14 billion parameter model, the largest monolingual Arabic decoder-only model to date. We use a collection of publicly available datasets, as well as a newly introduced HandMade dataset consisting of 8 billion tokens. Finally, we explore the quantitative and qualitative toxicity of several Arabic models, comparing our models to existing public Arabic LLMs.</abstract>
      <url hash="31cff25b">2023.arabicnlp-1.21</url>
      <bibkey>almazrouei-etal-2023-alghafa</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>A</fixed-case>r<fixed-case>B</fixed-case>anking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical <fixed-case>A</fixed-case>rabic</title>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Ahmet</first><last>Birim</last></author>
      <author><first>Mohammed</first><last>Khalilia</last></author>
      <author><first>Mustafa</first><last>Erden</last></author>
      <author><first>Sana</first><last>Ghanem</last></author>
      <pages>276-287</pages>
      <abstract>This paper presents the ArBanking77, a large Arabic dataset for intent detection in the banking domain. Our dataset was arabized and localized from the original English Banking77 dataset, which consists of 13,083 queries to ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA) and Palestinian dialect, with each query classified into one of the 77 classes (intents). Furthermore, we present a neural model, based on AraBERT, fine-tuned on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and Palestinian dialect, respectively. We performed extensive experimentation in which we simulated low-resource settings, where the model is trained on a subset of the data and augmented with noisy queries to simulate colloquial terms, mistakes and misspellings found in real NLP systems, especially live chat queries. The data and the models are publicly available at https://sina.birzeit.edu/arbanking77.</abstract>
      <url hash="50af7c08">2023.arabicnlp-1.22</url>
      <bibkey>jarrar-etal-2023-arbanking77</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>A</fixed-case>rab<fixed-case>I</fixed-case>cros: <fixed-case>AI</fixed-case>-Powered <fixed-case>A</fixed-case>rabic Crossword Puzzle Generation for Educational Applications</title>
      <author><first>Kamyar</first><last>Zeinalipour</last></author>
      <author><first>Mohamed</first><last>Saad</last></author>
      <author><first>Marco</first><last>Maggini</last></author>
      <author><first>Marco</first><last>Gori</last></author>
      <pages>288-301</pages>
      <abstract>This paper presents the first Arabic crossword puzzle generator driven by advanced AI technology. Leveraging cutting-edge large language models including GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system generates distinctive and challenging clues. Based on a dataset comprising over 50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot learning strategies, and rigorous quality-checking protocols to enforce the generation of high-quality clue-answer pairs. Importantly, educational crosswords contribute to enhancing memory, expanding vocabulary, and promoting problem-solving skills, thereby augmenting the learning experience through a fun and engaging approach, reshaping the landscape of traditional learning methods. The overall system can be exploited as a powerful educational tool that amalgamates AI and innovative learning techniques, heralding a transformative era for Arabic crossword puzzles and the intersection of technology and education.</abstract>
      <url hash="a7e66efc">2023.arabicnlp-1.23</url>
      <bibkey>zeinalipour-etal-2023-arabicros</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Machine Translation of <fixed-case>O</fixed-case>mani <fixed-case>A</fixed-case>rabic Dialect from Social Media</title>
      <author><first>Khoula</first><last>Al-Kharusi</last></author>
      <author><first>Abdurahman</first><last>AAlAbdulsalam</last></author>
      <pages>302-309</pages>
      <abstract>Research studies on Machine Translation (MT) between Modern Standard Arabic (MSA) and English are abundant. However, studies on MT between Omani Arabic (OA) dialects and English are very scarce. This research study focuses on the lack of availability of an Omani dialect parallel dataset, as well as MT of OA to English. The study uses social media data from X (formerly Twitter) to build an authentic parallel text of the Omani dialects. The research presents baseline results on this dataset using Google Translate, Microsoft Translation, and Marian NMT. A taxonomy of the most common linguistic errors is used to analyze the translations made by the NMT systems to provide insights on future improvements. Finally, transfer learning is used to adapt Marian NMT to the Omani dialect, which significantly improved by 9.88 points in the BLEU score.</abstract>
      <url hash="898484b8">2023.arabicnlp-1.24</url>
      <bibkey>al-kharusi-aalabdulsalam-2023-machine</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.24</doi>
      <video href="2023.arabicnlp-1.24.mp4"/>
    </paper>
    <paper id="25">
      <title><fixed-case>A</fixed-case>rabic Fine-Grained Entity Recognition</title>
      <author><first>Haneen</first><last>Liqreina</last></author>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Mohammed</first><last>Khalilia</last></author>
      <author><first>Ahmed</first><last>El-Shangiti</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>310-323</pages>
      <abstract>Traditional NER systems are typically trained to recognize coarse-grained categories of entities, and less attention is given to classifying entities into a hierarchy of fine-grained lower-level sub-types. This article aims to advance Arabic NER with fine-grained entities. We chose to extend Wojood (an open-source Nested Arabic Named Entity Corpus) with sub-types. In particular, four main entity types in Wojood (geopolitical entity (GPE), location (LOC), organization (ORG), and facility (FAC) are extended with 31 sub-types of entities. To do this, we first revised Wojood’s annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC’s ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC, ORG, and FAC (~ 44K) in Wojood are manually annotated with the LDC’s ACE subtypes. This extended version of Wojood is called WojoodFine. To evaluate our annotations, we measured the inter-annotator agreement (IAA) using both Cohen’s Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively. To compute the baselines of WojoodFine, we fine-tune three pre-trained Arabic BERT encoders in three settings: flat NER, nested NER and nested NER with sub-types and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our corpus and models are open source and available at https://sina.birzeit.edu/wojood/.</abstract>
      <url hash="7a165363">2023.arabicnlp-1.25</url>
      <bibkey>liqreina-etal-2023-arabic</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>Investigating Zero-shot Cross-lingual Language Understanding for <fixed-case>A</fixed-case>rabic</title>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Moataz</first><last>Ahmed</last></author>
      <pages>324-334</pages>
      <abstract>Numerous languages exhibit shared characteristics, especially in morphological features. For instance, Arabic and Russian both belong to the fusional language category. The question arises: Do such common traits influence language comprehension across diverse linguistic backgrounds? This study explores the possibility of transferring comprehension skills across languages to Arabic in a zero-shot scenario. Specifically, we demonstrate that training language models on other languages can enhance comprehension of Arabic, as evidenced by our evaluations in three key tasks: natural language inference, question answering, and named entity recognition. Our experiments reveal that certain morphologically rich languages (MRLs), such as Russian, display similarities to Arabic when assessed in a zero-shot context, particularly in tasks like question answering and natural language inference. However, this similarity is less pronounced in tasks like named entity recognition.</abstract>
      <url hash="14afdc91">2023.arabicnlp-1.26</url>
      <bibkey>alyafeai-ahmed-2023-investigating</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.26</doi>
      <video href="2023.arabicnlp-1.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Evaluating <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> and Bard <fixed-case>AI</fixed-case> on <fixed-case>A</fixed-case>rabic Sentiment Analysis</title>
      <author><first>Abdulmohsen</first><last>Al-Thubaity</last></author>
      <author><first>Sakhar</first><last>Alkhereyf</last></author>
      <author><first>Hanan</first><last>Murayshid</last></author>
      <author><first>Nouf</first><last>Alshalawi</last></author>
      <author><first>Maha</first><last>Omirah</last></author>
      <author><first>Raghad</first><last>Alateeq</last></author>
      <author><first>Rawabi</first><last>Almutairi</last></author>
      <author><first>Razan</first><last>Alsuwailem</last></author>
      <author><first>Manal</first><last>Alhassoun</last></author>
      <author><first>Imaan</first><last>Alkhanen</last></author>
      <pages>335-349</pages>
      <abstract>Large Language Models (LLMs) such as ChatGPT and Bard AI have gained much attention due to their outstanding performance on a range of NLP tasks. These models have demonstrated remarkable proficiency across various languages without the necessity for full supervision. Nevertheless, their performance in low-resource languages and dialects, like Arabic dialects in comparison to English, remains to be investigated. In this paper, we conduct a comprehensive evaluation of three LLMs for Dialectal Arabic Sentiment Analysis: namely, ChatGPT based on GPT-3.5 and GPT-4, and Bard AI. We use a Saudi dialect Twitter dataset to assess their capability in sentiment text classification and generation. For classification, we compare the performance of fully fine-tuned Arabic BERT-based models with the LLMs in few-shot settings. For data generation, we evaluate the quality of the generated new sentiment samples using human and automatic evaluation methods. The experiments reveal that GPT-4 outperforms GPT-3.5 and Bard AI in sentiment analysis classification, rivaling the top-performing fully supervised BERT-based language model. However, in terms of data generation, compared to manually annotated authentic data, these generative models often fall short in producing high-quality Dialectal Arabic text suitable for sentiment analysis.</abstract>
      <url hash="132c6156">2023.arabicnlp-1.27</url>
      <bibkey>al-thubaity-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>In-Context Meta-Learning vs. Semantic Score-Based Similarity: A Comparative Study in <fixed-case>A</fixed-case>rabic Short Answer Grading</title>
      <author><first>Menna</first><last>Fateen</last></author>
      <author><first>Tsunenori</first><last>Mine</last></author>
      <pages>350-358</pages>
      <abstract>Delegating short answer grading to automated systems enhances efficiency, giving teachers more time for vital human-centered aspects of education. Studies in automatic short answer grading (ASAG) approach the problem from instance-based or reference-based perspectives. Recent studies have favored instance-based methods, but they demand substantial data for training, which is often scarce in classroom settings. This study compares both approaches using an Arabic ASAG dataset. We employ in-context meta-learning for instance-based and semantic score-based similarity for reference-based grading. Results show both methods outperform a baseline and occasionally even surpass human raters when grading unseen answers. Notably, the semantic score-based similarity approach excels in zero-shot settings, outperforming in-context meta-learning. Our work contributes insights to Arabic ASAG and introduces a prompt category classification model, leveraging GPT3.5 to augment Arabic data for improved performance.</abstract>
      <url hash="5e70b1c8">2023.arabicnlp-1.28</url>
      <bibkey>fateen-mina-2023-context</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>SALMA</fixed-case>: <fixed-case>A</fixed-case>rabic Sense-Annotated Corpus and <fixed-case>WSD</fixed-case> Benchmarks</title>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Sanad</first><last>Malaysha</last></author>
      <author><first>Tymaa</first><last>Hammouda</last></author>
      <author><first>Mohammed</first><last>Khalilia</last></author>
      <pages>359-369</pages>
      <abstract>SALMA, the first Arabic sense-annotated corpus, consists of ~34K tokens, which are all sense-annotated. The corpus is annotated using two different sense inventories simultaneously (Modern and Ghani). SALMA novelty lies in how tokens and senses are associated. Instead of linking a token to only one intended sense, SALMA links a token to multiple senses and provides a score to each sense. A smart web-based annotation tool was developed to support scoring multiple senses against a given word. In addition to sense annotations, we also annotated the corpus using six types of named entities. The quality of our annotations was assessed using various metrics (Kappa, Linear Weighted Kappa, Quadratic Weighted Kappa, Mean Average Error, and Root Mean Square Error), which show very high inter-annotator agreement. To establish a Word Sense Disambiguation baseline using our SALMA corpus, we developed an end-to-end Word Sense Disambiguation system using Target Sense Verification. We used this system to evaluate three Target Sense Verification models available in the literature. Our best model achieved an accuracy with 84.2% using Modern and 78.7% using Ghani. The full corpus and the annotation tool are open-source and publicly available at https://sina.birzeit.edu/salma/.</abstract>
      <url hash="51c8783a">2023.arabicnlp-1.29</url>
      <bibkey>jarrar-etal-2023-salma</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>A</fixed-case>rabic dialect identification: An in-depth error analysis on the <fixed-case>MADAR</fixed-case> parallel corpus</title>
      <author><first>Helene</first><last>Olsen</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>370-384</pages>
      <abstract>This paper provides a systematic analysis and comparison of the performance of state-of-the-art models on the task of fine-grained Arabic dialect identification using the MADAR parallel corpus. We test approaches based on pre-trained transformer language models in addition to Naive Bayes models with a rich set of various features. Through a comprehensive data- and error analysis, we provide valuable insights into the strengths and weaknesses of both approaches. We discuss which dialects are more challenging to differentiate, and identify potential sources of errors. Our analysis reveals an important problem with identical sentences across dialect classes in the test set of the MADAR-26 corpus, which may confuse any classifier. We also show that none of the tested approaches captures the subtle distinctions between closely related dialects.</abstract>
      <url hash="da5a092a">2023.arabicnlp-1.30</url>
      <bibkey>olsen-etal-2023-arabic</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.30</doi>
      <video href="2023.arabicnlp-1.30.mp4"/>
    </paper>
    <paper id="31">
      <title><fixed-case>A</fixed-case>rabic Dialect Identification under Scrutiny: Limitations of Single-label Classification</title>
      <author><first>Amr</first><last>Keleg</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>385-398</pages>
      <abstract>Automatic Arabic Dialect Identification (ADI) of text has gained great popularity since it was introduced in the early 2010s. Multiple datasets were developed, and yearly shared tasks have been running since 2018. However, ADI systems are reported to fail in distinguishing between the micro-dialects of Arabic. We argue that the currently adopted framing of the ADI task as a single-label classification problem is one of the main reasons for that. We highlight the limitation of the incompleteness of the Dialect labels and demonstrate how it impacts the evaluation of ADI systems. A manual error analysis for the predictions of an ADI, performed by 7 native speakers of different Arabic dialects, revealed that <tex-math>\approx</tex-math> 67% of the validated errors are not true errors. Consequently, we propose framing ADI as a multi-label classification task and give recommendations for designing new ADI datasets.</abstract>
      <url hash="396853b2">2023.arabicnlp-1.31</url>
      <bibkey>keleg-magdy-2023-arabic</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>A</fixed-case>rabic Topic Classification in the Generative and <fixed-case>A</fixed-case>uto<fixed-case>ML</fixed-case> Era</title>
      <author><first>Doha</first><last>Albared</last></author>
      <author><first>Hadi</first><last>Hamoud</last></author>
      <author><first>Fadi</first><last>Zaraket</last></author>
      <pages>399-404</pages>
      <abstract>Most recent models for Arabic topic classification leveraged fine-tuning existing pre-trained transformer models and targeted a limited number of categories. More recently, advances in automated ML and generative models introduced novel potentials for the task. While these approaches work for English, it is a question of whether they perform well for low-resourced languages; Arabic in particular. This paper presents (i) ArBoNeClass; a novel Arabic dataset with an extended 14-topic class set covering modern books from social sciences and humanities along with newspaper articles, and (ii) a set of topic classifiers built from it. We finetuned an open LLM model to build ArGTClass. We compared its performance against the best models built with Vertex AI (Google), AutoML(H2O), and AutoTrain(HuggingFace). ArGTClass outperformed the VertexAi and AutoML models and was reasonably similar to the AutoTrain model.</abstract>
      <url hash="feef96d2">2023.arabicnlp-1.32</url>
      <bibkey>albared-etal-2023-arabic</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.32</doi>
    </paper>
    <paper id="33">
      <title>On Enhancing Fine-Tuning for Pre-trained Language Models</title>
      <author><first>Abir</first><last>Betka</last></author>
      <author><first>Zeyd</first><last>Ferhat</last></author>
      <author><first>Riyadh</first><last>Barka</last></author>
      <author><first>Selma</first><last>Boutiba</last></author>
      <author><first>Zineddine</first><last>Kahhoul</last></author>
      <author><first>Tiar</first><last>Lakhdar</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Habiba</first><last>Dahmani</last></author>
      <pages>405-410</pages>
      <abstract>The remarkable capabilities of Natural Language Models to grasp language subtleties has paved the way for their widespread adoption in diverse fields. However, adapting them for specific tasks requires the time-consuming process of fine-tuning, which consumes significant computational power and energy. Therefore, optimizing the fine-tuning time is advantageous. In this study, we propose an alternate approach that limits parameter manipulation to select layers. Our exploration led to identifying layers that offer the best trade-off between time optimization and performance preservation. We further validated this approach on multiple downstream tasks, and the results demonstrated its potential to reduce fine-tuning time by up to 50% while maintaining performance within a negligible deviation of less than 5%. This research showcases a promising technique for significantly improving fine-tuning efficiency without compromising task- or domain-specific learning capabilities.</abstract>
      <url hash="d3632000">2023.arabicnlp-1.33</url>
      <bibkey>betka-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.33</doi>
    </paper>
    <paper id="34">
      <title>Multi-Parallel Corpus of <fixed-case>N</fixed-case>orth <fixed-case>L</fixed-case>evantine <fixed-case>A</fixed-case>rabic</title>
      <author><first>Mateusz</first><last>Krubiński</last></author>
      <author><first>Hashem</first><last>Sellat</last></author>
      <author><first>Shadi</first><last>Saleh</last></author>
      <author><first>Adam</first><last>Pospíšil</last></author>
      <author><first>Petr</first><last>Zemánek</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>411-417</pages>
      <abstract>Low-resource Machine Translation (MT) is characterized by the scarce availability of training data and/or standardized evaluation benchmarks. In the context of Dialectal Arabic, recent works introduced several evaluation benchmarks covering both Modern Standard Arabic (MSA) and dialects, mapping, however, mostly to a single Indo-European language - English. In this work, we introduce a multi-lingual corpus consisting of 120,600 multi-parallel sentences in English, French, German, Greek, Spanish, and MSA selected from the OpenSubtitles corpus, which were manually translated into the North Levantine Arabic. By conducting a series of training and fine-tuning experiments, we explore how this novel resource can contribute to the research on Arabic MT.</abstract>
      <url hash="b18cf5da">2023.arabicnlp-1.34</url>
      <bibkey>krubinski-etal-2023-multi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.34</doi>
    </paper>
    <paper id="35">
      <title>Simplify: Automatic <fixed-case>A</fixed-case>rabic Sentence Simplification using Word Embeddings</title>
      <author><first>Yousef</first><last>SalahEldin</last></author>
      <author><first>Caroline</first><last>Sabty</last></author>
      <pages>418-422</pages>
      <abstract>Automatic Text Simplification (TS) involves simplifying language complexity while preserving the original meaning. The main objective of TS is to enhance the readability of complex texts, making them more accessible to a broader range of readers. This work focuses on developing a lexical text simplification system specifically for Arabic. We utilized FastText and Arabert pre-trained embedding models to create various simplification models. Our lexical approach involves a series of steps: identifying complex words, generating potential replacements, and selecting one replacement for the complex word within a sentence. We presented two main identification models: binary and multi-complexity models. We assessed the efficacy of these models by employing BERTScore to measure the similarity between the sentences generated by these models and the intended simple sentences. This comparative analysis evaluated the effectiveness of these models in accurately identifying and selecting complex words.</abstract>
      <url hash="028934b6">2023.arabicnlp-1.35</url>
      <bibkey>salaheldin-sabty-2023-simplify</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.35</doi>
      <video href="2023.arabicnlp-1.35.mp4"/>
    </paper>
    <paper id="36">
      <title>Offensive Language Detection in <fixed-case>A</fixed-case>rabizi</title>
      <author><first>Imene</first><last>Bensalem</last></author>
      <author><first>Meryem</first><last>Mout</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>423-434</pages>
      <abstract>Detecting offensive language in under-resourced languages presents a significant real-world challenge for social media platforms. This paper is the first work focused on the issue of offensive language detection in Arabizi, an under-explored topic in an under-resourced form of Arabic. For the first time, a comprehensive and critical overview of the existing work on the topic is presented. In addition, we carry out experiments using different BERT-like models and show the feasibility of detecting offensive language in Arabizi with high accuracy. Throughout a thorough analysis of results, we emphasize the complexities introduced by dialect variations and out-of-domain generalization. We use in our experiments a dataset that we have constructed by leveraging existing, albeit limited, resources. To facilitate further research, we make this dataset publicly accessible to the research community.</abstract>
      <url hash="20ac54aa">2023.arabicnlp-1.36</url>
      <bibkey>bensalem-etal-2023-offensive</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.36</doi>
      <video href="2023.arabicnlp-1.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Yet Another Model for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Ajinkya</first><last>Kulkarni</last></author>
      <author><first>Hanan</first><last>Aldarmaki</last></author>
      <pages>435-440</pages>
      <abstract>In this paper, we describe a spoken Arabic dialect identification (ADI) model for Arabic that consistently outperforms previously published results on two benchmark datasets: ADI-5 and ADI-17. We explore two architectural variations: ResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and features exratected from the pre-trained self-supervised model UniSpeech-SAT Large, as well as a fusion of all four variants. We find that individually, ECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features outperform models with MFCCs by a large margin. Furthermore, a fusion of all four variants consistently outperforms individual models. Our best models outperform previously reported results on both datasets, with accuracies of 84.7% and 96.9% on ADI-5 and ADI-17, respectively.</abstract>
      <url hash="f6845fbd">2023.arabicnlp-1.37</url>
      <bibkey>kulkarni-aldarmaki-2023-yet</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.37</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>V</fixed-case>ox<fixed-case>A</fixed-case>rabica: A Robust Dialect-Aware <fixed-case>A</fixed-case>rabic Speech Recognition System</title>
      <author><first>Abdul</first><last>Waheed</last></author>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Peter</first><last>Sullivan</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>441-449</pages>
      <abstract>Arabic is a complex language with many varieties and dialects spoken by ~ 450 millions all around the world. Due to the linguistic diversity and vari-ations, it is challenging to build a robust and gen-eralized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identi-fication (DID) as well as automatic speech recog-nition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the re-maining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selec-tion, and the option to raise flags for incorrect out-puts. Overall, we believe VoxArabica will be use-ful for a wide range of audiences concerned with Arabic research. Our system is currently running at https://cdce-206-12-100-168.ngrok.io/.</abstract>
      <url hash="49d0d8a2">2023.arabicnlp-1.38</url>
      <bibkey>waheed-etal-2023-voxarabica</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.38</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>KSAA</fixed-case>-<fixed-case>RD</fixed-case> Shared Task: <fixed-case>A</fixed-case>rabic Reverse Dictionary</title>
      <author><first>Rawan</first><last>Al-Matham</last></author>
      <author><first>Waad</first><last>Alshammari</last></author>
      <author><first>Abdulrahman</first><last>AlOsaimy</last></author>
      <author><first>Sarah</first><last>Alhumoud</last></author>
      <author><first>Asma</first><last>Wazrah</last></author>
      <author><first>Afrah</first><last>Altamimi</last></author>
      <author><first>Halah</first><last>Alharbi</last></author>
      <author><first>Abdullah</first><last>Alaifi</last></author>
      <pages>450-460</pages>
      <abstract>This paper outlines the KSAA-RD shared task, which aims to develop a Reverse Dictionary (RD) system for the Arabic language. RDs allow users to find words based on their meanings or definition. This shared task, KSAA-RD, includes two subtasks: Arabic RD and cross-lingual reverse dictionaries (CLRD). Given a definition (referred to as a “gloss”) in either Arabic or English, the teams compete to find the most similar word embeddings of their corresponding word. The winning team achieved 24.20 and 12.70 for RD and CLRD, respectively in terms of rank metric. In this paper, we describe the methods employed by the participating teams and offer an outlook for KSAA-RD.</abstract>
      <url hash="aa5a2886">2023.arabicnlp-1.39</url>
      <bibkey>al-matham-etal-2023-ksaa</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>UWB</fixed-case> at <fixed-case>A</fixed-case>rabic Reverse Dictionary shared task: Computing the meaning of a gloss</title>
      <author><first>Stephen</first><last>Taylor</last></author>
      <pages>461-466</pages>
      <abstract>To extract the ‘meaning’ of a gloss phrase, we build a list of sense-IDs for each word in the phrase which is in our vocabulary. We choose one sense-ID from each list so as to maximise similarity of all the IDs in the chosen subset. We take the meaning of the phrase in semantic space to be the weighted sum of the embedding vectors of the IDs.</abstract>
      <url hash="b1691d91">2023.arabicnlp-1.40</url>
      <bibkey>taylor-2023-uwb</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.40</doi>
      <video href="2023.arabicnlp-1.40.mp4"/>
    </paper>
    <paper id="41">
      <title>Qamosy at <fixed-case>A</fixed-case>rabic Reverse Dictionary shared task: Semi Decoder Architecture for Reverse Dictionary with <fixed-case>SBERT</fixed-case> Encoder</title>
      <author><first>Serry</first><last>Sibaee</last></author>
      <author><first>Samar</first><last>Ahmad</last></author>
      <author><first>Ibrahim</first><last>Khurfan</last></author>
      <author><first>Vian</first><last>Sabeeh</last></author>
      <author><first>Ahmed</first><last>Bahaaulddin</last></author>
      <author><first>Hanan</first><last>Belhaj</last></author>
      <author><first>Abdullah</first><last>Alharbi</last></author>
      <pages>467-471</pages>
      <abstract>A reverse dictionary takes a descriptive phrase of a particular concept and returns words with definitions that align with that phrase. While many reverse dictionaries cater to languages such as English and are readily available online or have been developed by researchers, there is a notable lack of similar resources for the Arabic language. This paper describes our participation in the Arabic Reverse Dictionary shared task. Our proposed method consists of two main steps: First, we convert word definitions into multidimensional vectors. Then, we train these encoded vectors using the Semi-Decoder model for our target task. Our system secured 2nd place based on the Rank metric for both embeddings (Electra and Sgns).</abstract>
      <url hash="43151356">2023.arabicnlp-1.41</url>
      <bibkey>sibaee-etal-2023-qamosy</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.41</doi>
      <video href="2023.arabicnlp-1.41.mp4"/>
    </paper>
    <paper id="42">
      <title>Abed at <fixed-case>KSAA</fixed-case>-<fixed-case>RD</fixed-case> Shared Task: Enhancing <fixed-case>A</fixed-case>rabic Word Embedding with Modified <fixed-case>BERT</fixed-case> Multilingual</title>
      <author><first>Abdelrahim</first><last>Qaddoumi</last></author>
      <pages>472-476</pages>
      <abstract>This paper presents a novel approach to the Arabic Reverse Dictionary Shared Task at WANLP 2023 by leveraging the BERT Multilingual model and introducing modifications augmentation and using a multi attention head. The proposed method aims to enhance the performance of the model in understanding and generating word embeddings for Arabic definitions, both in monolingual and cross-lingual contexts. It achieved good results compared to benchmark and other models in the shared task 1 and 2.</abstract>
      <url hash="a888e341">2023.arabicnlp-1.42</url>
      <bibkey>qaddoumi-2023-abed</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.42</doi>
    </paper>
    <paper id="43">
      <title>Rosetta Stone at <fixed-case>KSAA</fixed-case>-<fixed-case>RD</fixed-case> Shared Task: A Hop From Language Modeling To Word–Definition Alignment</title>
      <author><first>Ahmed</first><last>Elbakry</last></author>
      <author><first>Mohamed</first><last>Gabr</last></author>
      <author><first>Muhammad</first><last>ElNokrashy</last></author>
      <author><first>Badr</first><last>AlKhamissi</last></author>
      <pages>477-482</pages>
      <abstract>A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the “Tip-of-the-Tongue” (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each model within the ensemble. In contrast, the most effective solution for the second subtask involves translating the English test definitions into Arabic and applying them to the finetuned models originally trained for the first subtask. This straightforward method achieves the highest score across both subtasks.</abstract>
      <url hash="76093c51">2023.arabicnlp-1.43</url>
      <bibkey>elbakry-etal-2023-rosetta</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.43</doi>
      <video href="2023.arabicnlp-1.43.mp4"/>
    </paper>
    <paper id="44">
      <title><fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Persuasion Techniques and Disinformation Detection in <fixed-case>A</fixed-case>rabic Text</title>
      <author><first>Maram</first><last>Hasanain</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Samir</first><last>Abdaljalil</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Abed</first><last>Freihat</last></author>
      <pages>483-493</pages>
      <abstract>We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (1) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (2) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Task 1 and Task 2, respectively. Across both tasks, we observe that fine-tuning transformer models such as AraBERT is the core of majority of participating systems. We provide a description of the task setup, including description of datasets construction and the evaluation setup. We also provide a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. We hope this will enable further research on such important tasks within the Arabic NLP community.</abstract>
      <url hash="00be07a2">2023.arabicnlp-1.44</url>
      <bibkey>hasanain-etal-2023-araieval</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.44</doi>
    </paper>
    <paper id="45">
      <title><fixed-case>D</fixed-case>etective<fixed-case>R</fixed-case>edasers at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Leveraging Transformer Ensembles for <fixed-case>A</fixed-case>rabic Deception Detection</title>
      <author><first>Bryan</first><last>Tuck</last></author>
      <author><first>Fatima Zahra</first><last>Qachfar</last></author>
      <author><first>Dainis</first><last>Boumber</last></author>
      <author><first>Rakesh</first><last>Verma</last></author>
      <pages>494-501</pages>
      <abstract>This paper outlines a methodology aimed at combating disinformation in Arabic social media, a strategy that secured a first-place finish in tasks 2A and 2B at the ArAIEval shared task during the ArabicNLP 2023 conference. Our team, DetectiveRedasers, developed a hyperparameter-optimized pipeline centered around singular BERT-based models for the Arabic language, enhanced by a soft-voting ensemble strategy. Subsequent evaluation on the test dataset reveals that ensembles, although generally resilient, do not always outperform individual models. The primary contributions of this paper are its multifaceted strategy, which led to winning solutions for both binary (2A) and multiclass (2B) disinformation classification tasks.</abstract>
      <url hash="cdc8687b">2023.arabicnlp-1.45</url>
      <bibkey>tuck-etal-2023-detectiveredasers</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.45</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>HTE</fixed-case> at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Integrating Content Type Information in Binary Persuasive Technique Detection</title>
      <author><first>Khaldi</first><last>Hadjer</last></author>
      <author><first>Taqiy</first><last>Bouklouha</last></author>
      <pages>502-507</pages>
      <abstract>Propaganda frequently employs sophisticated persuasive strategies in order to influence public opinion and manipulate perceptions. As a result, automating the detection of persuasive techniques is critical in identifying and mitigating propaganda on social media and in mainstream media. This paper proposes a set of transformer-based models for detecting persuasive techniques in tweets and news that incorporate content type information as extra features or as an extra learning objective in a multitask learning setting. In addition to learning to detect the presence of persuasive techniques in text, our best model learns specific syntactic and lexical cues used to express them based on text genre (type) as an auxiliary task. To optimize the model and deal with data imbalance, a focal loss is used. As part of ArabicNLP2023-ArAIEval shared task, this model achieves the highest score in the shared task 1A out of 13 participants, according to the official results, with a micro-F1 of 76.34% and a macro-F1 of 73.21% on the test dataset.</abstract>
      <url hash="21b3445d">2023.arabicnlp-1.46</url>
      <bibkey>hadjer-bouklouha-2023-hte</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>USTHB</fixed-case> at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val’23 Shared Task: Disinformation Detection System based on Linguistic Feature Concatenation</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <author><first>Aicha</first><last>Zitouni</last></author>
      <author><first>Houda</first><last>Latrache</last></author>
      <author><first>Rachida</first><last>Djeradi</last></author>
      <pages>508-512</pages>
      <abstract>In this research paper, we undertake a comprehensive examination of several pivotal factors that impact the performance of Arabic Disinformation Detection in the ArAIEval’2023 shared task. Our exploration encompasses the influence of surface preprocessing, morphological preprocessing, the FastText vector model, and the weighted fusion of TF-IDF features. To carry out classification tasks, we employ the Linear Support Vector Classification (LSVC) model. In the evaluation phase, our system showcases significant results, achieving an F<tex-math>_1</tex-math> micro score of 76.70% and 50.46% for binary and multiple classification scenarios, respectively. These accomplishments closely correspond to the average F<tex-math>_1</tex-math> micro scores achieved by other systems submitted for the second subtask, standing at 77.96% and 64.85% for binary and multiple classification scenarios, respectively.</abstract>
      <url hash="2381f3f0">2023.arabicnlp-1.47</url>
      <bibkey>lichouri-etal-2023-usthb</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.47</doi>
    </paper>
    <paper id="48">
      <title>Mavericks at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Towards a Safer Digital Space - Transformer Ensemble Models Tackling Deception and Persuasion</title>
      <author><first>Sudeep</first><last>Mangalvedhekar</last></author>
      <author><first>Kshitij</first><last>Deshpande</last></author>
      <author><first>Yash</first><last>Patwardhan</last></author>
      <author><first>Vedant</first><last>Deshpande</last></author>
      <author><first>Ravindra</first><last>Murumkar</last></author>
      <pages>513-518</pages>
      <abstract>In this paper, we highlight our approach for the “Arabic AI Tasks Evaluation (ArAiEval) Shared Task 2023”. We present our approaches for task 1-A and task 2-A of the shared task which focus on persuasion technique detection and disinformation detection respectively. Detection of persuasion techniques and disinformation has become imperative to avoid distortion of authentic information. The tasks use multigenre snippets of tweets and news articles for the given binary classification problem. We experiment with several transformer-based models that were pre-trained on the Arabic language. We fine-tune these state-of-the-art models on the provided dataset. Ensembling is employed to enhance the performance of the systems. We achieved a micro F1-score of 0.742 on task 1-A (8th rank on the leaderboard) and 0.901 on task 2-A (7th rank on the leaderboard) respectively.</abstract>
      <url hash="889a71f1">2023.arabicnlp-1.48</url>
      <bibkey>mangalvedhekar-etal-2023-mavericks</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.48</doi>
      <video href="2023.arabicnlp-1.48.mp4"/>
    </paper>
    <paper id="49">
      <title><fixed-case>K</fixed-case>now<fixed-case>T</fixed-case>ell<fixed-case>C</fixed-case>onvince at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Disinformation and Persuasion Detection in <fixed-case>A</fixed-case>rabic using Similar and Contrastive Representation Alignment</title>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>519-524</pages>
      <abstract>In an era of widespread digital communication, the challenge of identifying and countering disinformation has become increasingly critical. However, compared to the solutions available in the English language, the resources and strategies for tackling this multifaceted problem in Arabic are relatively scarce. To address this issue, this paper presents our solutions to tasks in ArAIEval 2023. Task 1 focuses on detecting persuasion techniques, while Task 2 centers on disinformation detection within Arabic text. Leveraging a multi-head model architecture, fine-tuning techniques, sequential learning, and innovative activation functions, our contributions significantly enhance persuasion techniques and disinformation detection accuracy. Beyond improving performance, our work fills a critical research gap in content analysis for Arabic, empowering individuals, communities, and digital platforms to combat deceptive content effectively and preserve the credibility of information sources within the Arabic-speaking world.</abstract>
      <url hash="4c628a36">2023.arabicnlp-1.49</url>
      <bibkey>veeramani-etal-2023-knowtellconvince</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>PTUK</fixed-case>-<fixed-case>HULAT</fixed-case> at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task Fine-tuned Distilbert to Predict Disinformative Tweets</title>
      <author><first>Areej</first><last>Jaber</last></author>
      <author><first>Paloma</first><last>Martinez</last></author>
      <pages>525-529</pages>
      <abstract>Disinformation involves the dissemination of incomplete, inaccurate, or misleading information; it has the objective, goal, or purpose of deliberately or intentionally lying to others aboutthe truth. The spread of disinformative information on social media has serious implications, and it causes concern among internet users in different aspects. Automatic classification models are required to detect disinformative posts on social media, especially on Twitter. In this article, DistilBERT multilingual model was fine-tuned to classify tweets either as dis-informative or not dis-informative in Subtask 2A of the ArAIEval shared task. The system outperformed the baseline and achieved F1 micro 87% and F1 macro 80%. Our system ranked 11 compared with all participants.</abstract>
      <url hash="28df3cb8">2023.arabicnlp-1.50</url>
      <bibkey>jaber-martinez-2023-ptuk</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.50</doi>
      <video href="2023.arabicnlp-1.50.mp4"/>
    </paper>
    <paper id="51">
      <title><fixed-case>A</fixed-case>ra<fixed-case>D</fixed-case>etector at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: An Ensemble of <fixed-case>A</fixed-case>rabic-specific pre-trained <fixed-case>BERT</fixed-case> and <fixed-case>GPT</fixed-case>-4 for <fixed-case>A</fixed-case>rabic Disinformation Detection</title>
      <author><first>Ahmed</first><last>Bahaaulddin</last></author>
      <author><first>Vian</first><last>Sabeeh</last></author>
      <author><first>Hanan</first><last>Belhaj</last></author>
      <author><first>Serry</first><last>Sibaee</last></author>
      <author><first>Samar</first><last>Ahmad</last></author>
      <author><first>Ibrahim</first><last>Khurfan</last></author>
      <author><first>Abdullah</first><last>Alharbi</last></author>
      <pages>530-535</pages>
      <abstract>The rapid proliferation of disinformation through social media has become one of the most dangerous means to deceive and influence people’s thoughts, viewpoints, or behaviors due to social media’s facilities, such as rapid access, lower cost, and ease of use. Disinformation can spread through social media in different ways, such as fake news stories, doctored images or videos, deceptive data, and even conspiracy theories, thus making detecting disinformation challenging. This paper is a part of participation in the ArAIEval competition that relates to disinformation detection. This work evaluated four models: MARBERT, the proposed ensemble model, and two tests over GPT-4 (zero-shot and Few-shot). GPT-4 achieved micro-F1 79.01% while the ensemble method obtained 76.83%. Despite no improvement in the micro-F1 score on the dev dataset using the ensemble approach, we still used it for the test dataset predictions. We believed that merging different classifiers might enhance the system’s prediction accuracy.</abstract>
      <url hash="ba0eedc5">2023.arabicnlp-1.51</url>
      <bibkey>bahaaulddin-etal-2023-aradetector</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.51</doi>
      <video href="2023.arabicnlp-1.51.mp4"/>
    </paper>
    <paper id="52">
      <title>rematchka at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Prefix-Tuning &amp; Prompt-tuning for Improved Detection of Propaganda and Disinformation in <fixed-case>A</fixed-case>rabic Social Media Content</title>
      <author><first>Reem</first><last>Abdel-Salam</last></author>
      <pages>536-542</pages>
      <abstract>The rise of propaganda and disinformation in the digital age has necessitated the development of effective detection methods to combat the spread of deceptive information. In this paper we present our approach proposed for ArAIEval shared task : propaganda and disinformation detection in Arabic text. Our system utilised different pre-trained BERT based models, that makes use of prompt-learning based on knowledgeable expansion and prefix-tuning. The proposed approach secured third place in subtask-1A with 0.7555 F1-micro score, second place in subtask-1B with 0.5658 F1-micro score. However, for subtask-2A &amp; 2B, the proposed system achieved fourth place with an F1-micro score of 0.9040, 0.8219 respectively. Our findings suggest that prompt-tuning-based &amp; prefix-tuning based models performed better than conventional fine-tuning. Furthermore, using loss aware class imbalance, improved performance.</abstract>
      <url hash="4b476387">2023.arabicnlp-1.52</url>
      <bibkey>abdel-salam-2023-rematchka</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.52</doi>
    </paper>
    <paper id="53">
      <title>Itri Amigos at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Transformer vs. Compression-Based Models for Persuasion Techniques and Disinformation Detection</title>
      <author><first>Jehad</first><last>Oumer</last></author>
      <author><first>Nouman</first><last>Ahmed</last></author>
      <author><first>Natalia</first><last>Flechas Manrique</last></author>
      <pages>543-548</pages>
      <abstract>Social media has significantly amplified the dissemination of misinformation. Researchers have employed natural language processing and machine learning techniques to identify and categorize false information on these platforms. While there is a well-established body of research on detecting fake news in English and Latin languages, the study of Arabic fake news detection remains limited. This paper describes the methods used to tackle the challenges of the ArAIEval shared Task 2023. We conducted experiments with both monolingual Arabic and multi-lingual pre-trained Language Models (LM). We found that the monolingual Arabic models outperformed in all four subtasks. Additionally, we explored a novel lossless compression method, which, while not surpassing pretrained LM performance, presents an intriguing avenue for future experimentation to achieve comparable results in a more efficient and rapid manner.</abstract>
      <url hash="5a716bbf">2023.arabicnlp-1.53</url>
      <bibkey>oumer-etal-2023-itri</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.53</doi>
      <video href="2023.arabicnlp-1.53.mp4"/>
    </paper>
    <paper id="54">
      <title><fixed-case>R</fixed-case>e<fixed-case>DASP</fixed-case>ersuasion at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Multilingual and Monolingual Models For <fixed-case>A</fixed-case>rabic Persuasion Detection</title>
      <author><first>Fatima Zahra</first><last>Qachfar</last></author>
      <author><first>Rakesh</first><last>Verma</last></author>
      <pages>549-557</pages>
      <abstract>To enhance persuasion detection, we investigate the use of multilingual systems on Arabic data by conducting a total of 22 experiments using baselines, multilingual, and monolingual language transformers. Our aim is to provide a comprehensive evaluation of the various systems employed throughout this task, with the ultimate goal of comparing their performance and identifying the most effective approach. Our empirical analysis shows that *ReDASPersuasion* system performs best when combined with multilingual “XLM-RoBERTa” and monolingual pre-trained transformers on Arabic dialects like “CAMeLBERT-DA SA” depending on the NLP classification task.</abstract>
      <url hash="f5adbc18">2023.arabicnlp-1.54</url>
      <bibkey>qachfar-verma-2023-redaspersuasion-araieval</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>UL</fixed-case> &amp; <fixed-case>UM</fixed-case>6<fixed-case>P</fixed-case> at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Transformer-based model for Persuasion Techniques and Disinformation detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Salima</first><last>Lamsiyah</last></author>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Hamza</first><last>Alami</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <pages>558-564</pages>
      <abstract>In this paper, we introduce our participating system to the ArAIEval Shared Task, addressing both the detection of persuasion techniques and disinformation tasks. Our proposed system employs a pre-trained transformer-based language model for Arabic, alongside a classifier. We have assessed the performance of three Arabic Pre-trained Language Models (PLMs) for sentence encoding. Additionally, to enhance our model’s performance, we have explored various training objectives, including Cross-Entropy loss, regularized Mixup loss, asymmetric multi-label loss, and Focal Tversky loss. On the official test set, our system has achieved micro-F1 scores of 0.7515, 0.5666, 0.904, and 0.8333 for Sub-Task 1A, Sub-Task 1B, Sub-Task 2A, and Sub-Task 2B, respectively. Furthermore, our system has secured the 4th, 1st, 3rd, and 2nd positions, respectively, among all participating systems in sub-tasks 1A, 1B, 2A, and 2B of the ArAIEval shared task.</abstract>
      <url hash="547a9161">2023.arabicnlp-1.55</url>
      <bibkey>lamsiyah-etal-2023-ul-um6p</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.55</doi>
    </paper>
    <paper id="56">
      <title><fixed-case>AAST</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Tackling Persuasion technique and Disinformation Detection using Pre-Trained Language Models On Imbalanced Datasets</title>
      <author><first>Ahmed</first><last>El-Sayed</last></author>
      <author><first>Omar</first><last>Nasr</last></author>
      <author><first>Noureldin</first><last>Elmadany</last></author>
      <pages>565-569</pages>
      <abstract>This paper presents the pipeline developed by the AAST-NLP team to address both the persuasion technique detection and disinformation detection shared tasks. The proposed system for all the tasks’ sub-tasks consisted of preprocessing the data and finetuning AraBERT on the given datasets, in addition to several procedures performed for each subtask to adapt to the problems faced in it. The previously described system was used in addition to Dice loss as the loss function for sub-task 1A, which consisted of a binary classification problem. In that sub-task, the system came in eleventh place. We trained the AraBERT for task 1B, which was a multi-label problem with 24 distinct labels, using binary cross-entropy to train a classifier for each label. On that sub-task, the system came in third place. We utilised AraBERT with Dice loss on both subtasks 2A and 2B, ranking second and third among the proposed models for the respective subtasks.</abstract>
      <url hash="aebd8ece">2023.arabicnlp-1.56</url>
      <bibkey>el-sayed-etal-2023-aast</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.56</doi>
      <video href="2023.arabicnlp-1.56.mp4"/>
    </paper>
    <paper id="57">
      <title><fixed-case>PD</fixed-case>-<fixed-case>AR</fixed-case> at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: A <fixed-case>BERT</fixed-case>-Centric Approach to Tackle <fixed-case>A</fixed-case>rabic Disinformation</title>
      <author><first>Pritam</first><last>Deka</last></author>
      <author><first>Ashwathy</first><last>Revi</last></author>
      <pages>570-575</pages>
      <abstract>This work explores Arabic disinformation identification, a crucial task in natural language processing, using a state-of-the-art NLP model. We highlight the performance of our system model against baseline models, including multilingual and Arabic-specific ones, and showcase the effectiveness of domain-specific pre-trained models. This work advocates for the adoption of tailored pre-trained models in NLP, emphasizing their significance in understanding diverse languages. By merging advanced NLP techniques with domain-specific pre-training, it advances Arabic disinformation identification.</abstract>
      <url hash="8ef51d36">2023.arabicnlp-1.57</url>
      <bibkey>deka-revi-2023-pd</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.57</doi>
      <video href="2023.arabicnlp-1.57.mp4"/>
    </paper>
    <paper id="58">
      <title>Nexus at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Fine-Tuning <fixed-case>A</fixed-case>rabic Language Models for Propaganda and Disinformation Detection</title>
      <author><first>Yunze</first><last>Xiao</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <pages>576-582</pages>
      <abstract>The spread of disinformation and propagandistic content poses a threat to societal harmony, undermining informed decision-making and trust in reliable sources. Online platforms often serve as breeding grounds for such content, and malicious actors exploit the vulnerabilities of audiences to shape public opinion. Although there have been research efforts aimed at the automatic identification of disinformation and propaganda in social media content, there remain challenges in terms of performance. The ArAIEval shared task aims to further research on these particular issues within the context of the Arabic language. In this paper, we discuss our participation in these shared tasks. We competed in subtasks 1A and 2A, where our submitted system secured positions 9th and 10th, respectively. Our experiments consist of fine-tuning transformer models and using zero- and few-shot learning with GPT-4.</abstract>
      <url hash="8672aa16">2023.arabicnlp-1.58</url>
      <bibkey>xiao-alam-2023-nexus</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.58</doi>
      <video href="2023.arabicnlp-1.58.mp4"/>
    </paper>
    <paper id="59">
      <title>Frank at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: <fixed-case>A</fixed-case>rabic Persuasion and Disinformation: The Power of Pretrained Models</title>
      <author><first>Dilshod</first><last>Azizov</last></author>
      <author><first>Jiyong</first><last>Li</last></author>
      <author><first>Shangsong</first><last>Liang</last></author>
      <pages>583-588</pages>
      <abstract>In this work, we present our systems developed for “ArAIEval” shared task of ArabicNLP 2023 (CITATION). We used an mBERT transformer for Subtask 1A, which targets persuasion in Arabic tweets, and we used the MARBERT transformer for Subtask 2A to identify disinformation in Arabic tweets. Our persuasion detection system achieved micro-F1 of <b>0.745</b> by surpassing the baseline by 13.2%, and registered a macro-F1 of 0.717 based on leaderboard scores. Similarly, our disinformation system recorded a micro-F1 of <b>0.816</b>, besting the naïve majority by 6.7%, with a macro-F1 of 0.637. Furthermore, we present our preliminary results on a variety of pre-trained models. In terms of overall ranking, our systems placed <tex-math>7^\text{th}</tex-math> out of 16 and <tex-math>12^\text{th}</tex-math> out of 17 teams for Subtasks 1A and 2A, respectively.</abstract>
      <url hash="6ef492e3">2023.arabicnlp-1.59</url>
      <bibkey>azizov-etal-2023-frank</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.59</doi>
    </paper>
    <paper id="60">
      <title>Raphael at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Understanding Persuasive Language and Tone, an <fixed-case>LLM</fixed-case> Approach</title>
      <author><first>Utsav</first><last>Shukla</last></author>
      <author><first>Manan</first><last>Vyas</last></author>
      <author><first>Shailendra</first><last>Tiwari</last></author>
      <pages>589-593</pages>
      <abstract>The widespread dissemination of propaganda and disinformation on both social media and mainstream media platforms has become an urgent concern, attracting the interest of various stakeholders such as government bodies and social media companies. The challenge intensifies when dealing with understudied languages like Arabic. In this paper, we outline our approach for detecting persuasion techniques in Arabic tweets and news article paragraphs. We submitted our system to ArAIEval 2023 Shared Task 1, covering both subtasks. Our main contributions include utilizing GPT-3 to discern tone and potential persuasion techniques in text, exploring various base language models, and employing a multi-task learning approach for the specified subtasks.</abstract>
      <url hash="456bd5cc">2023.arabicnlp-1.60</url>
      <bibkey>shukla-etal-2023-raphael</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.60</doi>
    </paper>
    <paper id="61">
      <title>Legend at <fixed-case>A</fixed-case>r<fixed-case>AIE</fixed-case>val Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model</title>
      <author><first>Olumide</first><last>Ojo</last></author>
      <author><first>Olaronke</first><last>Adebanji</last></author>
      <author><first>Hiram</first><last>Calvo</last></author>
      <author><first>Damian</first><last>Dieke</last></author>
      <author><first>Olumuyiwa</first><last>Ojo</last></author>
      <author><first>Seye</first><last>Akinsanya</last></author>
      <author><first>Tolulope</first><last>Abiola</last></author>
      <author><first>Anna</first><last>Feldman</last></author>
      <pages>594-599</pages>
      <abstract>In this paper, we share our best performing submission to the Arabic AI Tasks Evaluation Challenge (ArAIEval) at ArabicNLP 2023. Our focus was on Task 1, which involves identifying persuasion techniques in excerpts from tweets and news articles. The persuasion technique in Arabic texts was detected using a training loop with XLM-RoBERTa, a language-agnostic text representation model. This approach proved to be potent, leveraging fine-tuning of a multilingual language model. In our evaluation of the test set, we achieved a micro F1 score of 0.64 for subtask A of the competition.</abstract>
      <url hash="bd9f3f2b">2023.arabicnlp-1.61</url>
      <bibkey>ojo-etal-2023-legend</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.61</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>NADI</fixed-case> 2023: The Fourth Nuanced <fixed-case>A</fixed-case>rabic Dialect Identification Shared Task</title>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Chiyu</first><last>Zhang</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>600-613</pages>
      <abstract>We describe the findings of the fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). The objective of NADI is to help advance state-of-the-art Arabic NLP by creating opportunities for teams of researchers to collaboratively compete under standardized conditions. It does so with a focus on Arabic dialects, offering novel datasets and defining subtasks that allow for meaningful comparisons between different approaches. NADI 2023 targeted both dialect identification (Subtask1) and dialect-to-MSA machine translation (Subtask 2 and Subtask 3). A total of 58 unique teams registered for the shared task, of whom 18 teams have participated (with 76 valid submissions during test phase). Among these, 16 teams participated in Subtask 1, 5 participated in Subtask 2, and 3 participated in Subtask 3. The winning teams achieved 87.27 F1 on Subtask 1, 14.76 Bleu in Subtask 2, and 21.10 Bleu in Subtask 3, respectively. Results show that all three subtasks remain challenging, thereby motivating future work in this area. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.</abstract>
      <url hash="abaddc57">2023.arabicnlp-1.62</url>
      <bibkey>abdul-mageed-etal-2023-nadi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>D</fixed-case>ialect<fixed-case>NLU</fixed-case> at <fixed-case>NADI</fixed-case> 2023 Shared Task: Transformer Based Multitask Approach Jointly Integrating Dialect and Machine Translation Tasks in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>614-619</pages>
      <abstract>With approximately 400 million speakers worldwide, Arabic ranks as the fifth most-spoken language globally, necessitating advancements in natural language processing. This paper addresses this need by presenting a system description of the approaches employed for the subtasks outlined in the Nuanced Arabic Dialect Identification (NADI) task at EMNLP 2023. For the first subtask, involving closed country-level dialect identification classification, we employ an ensemble of two Arabic language models. Similarly, for the second subtask, focused on closed dialect to Modern Standard Arabic (MSA) machine translation, our approach combines sequence-to-sequence models, all trained on an Arabic-specific dataset. Our team ranks 10th and 3rd on subtask 1 and subtask 2 respectively.</abstract>
      <url hash="003f8669">2023.arabicnlp-1.63</url>
      <bibkey>veeramani-etal-2023-dialectnlu</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>U</fixed-case>o<fixed-case>T</fixed-case> at <fixed-case>NADI</fixed-case> 2023 shared task: Automatic <fixed-case>A</fixed-case>rabic Dialect Identification is Made Possible</title>
      <author><first>Abduslam F A</first><last>Nwesri</last></author>
      <author><first>Nabila A S</first><last>Shinbir</last></author>
      <author><first>Hassan</first><last>Ebrahem</last></author>
      <pages>620-624</pages>
      <abstract>In this paper we present our approach towards Arabic Dialect identification which was part of the The Fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). We tested several techniques to identify Arabic dialects. We obtained the best result by fine-tuning the pre-trained MARBERTv2 model with a modified training dataset. The training set was expanded by sorting tweets based on dialects, concatenating every two adjacent tweets, and adding them to the original dataset as new tweets. We achieved 82.87 on F1 score and we were at the seventh position among 16 participants.</abstract>
      <url hash="08cef16d">2023.arabicnlp-1.64</url>
      <bibkey>nwesri-etal-2023-uot</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.64</doi>
      <video href="2023.arabicnlp-1.64.mp4"/>
    </paper>
    <paper id="65">
      <title><fixed-case>SANA</fixed-case> at <fixed-case>NADI</fixed-case> 2023 shared task: Ensemble of Layer-Wise <fixed-case>BERT</fixed-case>-based models for Dialectal <fixed-case>A</fixed-case>rabic Identification</title>
      <author><first>Nada</first><last>Almarwani</last></author>
      <author><first>Samah</first><last>Aloufi</last></author>
      <pages>625-630</pages>
      <abstract>Our system, submitted to the Nuanced Arabic Dialect Identification (NADI-23), tackles the first sub-task: Closed Country-level dialect identification. In this work, we propose a model that is based on an ensemble of layer-wise fine-tuned BERT-based models. The proposed model ranked fourth out of sixteen submissions, with an F1-macro score of 85.43.</abstract>
      <url hash="0562fd45">2023.arabicnlp-1.65</url>
      <bibkey>almarwani-aloufi-2023-sana</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.65</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>ISL</fixed-case>-<fixed-case>AAST</fixed-case> at <fixed-case>NADI</fixed-case> 2023 shared task: Enhancing <fixed-case>A</fixed-case>rabic Dialect Identification in the Era of Globalization and Technological Progress</title>
      <author><first>Shorouk</first><last>Adel</last></author>
      <author><first>Noureldin</first><last>Elmadany</last></author>
      <pages>631-636</pages>
      <abstract>Arabic dialects have extensive global usage owing to their significance and the vast number of Arabic speakers. However, technological progress and globalization are leading to significant transformations within Arabic dialects. They are acquiring new characteristics involving novel vocabulary and integrating of linguistic elements from diverse dialects. Consequently, sentiment analysis of these dialects is becoming more challenging. This study categorizes dialects among 18 countries, as introduced by the Nuanced Arabic Dialect Identification (NADI) shared task competition. Our approach incorporates the utilization of the MARABERT and MARABERT v2 models with a range of methodologies, including a feature extraction process. Our findings reveal that the most effective model is achieved by applying averaging and concatenation to the hidden layers of MARABERT v2, followed by feeding the resulting output into convolutional layers. Furthermore, employing the ensemble method on various methods enhances the model’s performance. Our system secures the 6th position among the top performers in the First subtask, achieving an F1 score of 83.73%.</abstract>
      <url hash="48c2a392">2023.arabicnlp-1.66</url>
      <bibkey>adel-elmadany-2023-isl</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.66</doi>
      <video href="2023.arabicnlp-1.66.mp4"/>
    </paper>
    <paper id="67">
      <title>Frank at <fixed-case>NADI</fixed-case> 2023 Shared Task: Trio-Based Ensemble Approach for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Dilshod</first><last>Azizov</last></author>
      <author><first>Jiyong</first><last>Li</last></author>
      <author><first>Shangsong</first><last>Liang</last></author>
      <pages>637-641</pages>
      <abstract>We present our system designed for Subtask 1 in the shared task NADI on Arabic Dialect Identification, which is part of ArabicNLP 2023. In our approach, we utilized models such as: MARBERT, MARBERTv2 (A) and MARBERTv2 (B). Subsequently, we created a majority voting ensemble of these models. We used MARBERTv2 with different hyperparameters, which significantly improved the overall performance of the ensemble model. In terms of performance, our systems achieved a competitive an F1 score of <b>84.76</b>. Overall, our system secured the <tex-math>5^\text{th}</tex-math> position out of 16 participating teams.</abstract>
      <url hash="07f967d7">2023.arabicnlp-1.67</url>
      <bibkey>azizov-etal-2023-frank-nadi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>NLP</fixed-case>eople at <fixed-case>NADI</fixed-case> 2023 Shared Task: <fixed-case>A</fixed-case>rabic Dialect Identification with Augmented Context and Multi-Stage Tuning</title>
      <author><first>Mohab</first><last>Elkaref</last></author>
      <author><first>Movina</first><last>Moses</last></author>
      <author><first>Shinnosuke</first><last>Tanaka</last></author>
      <author><first>James</first><last>Barry</last></author>
      <author><first>Geeth</first><last>Mel</last></author>
      <pages>642-646</pages>
      <abstract>This paper presents the approach of the NLPeople team to the Nuanced Arabic Dialect Identification (NADI) 2023 shared task. Subtask 1 involves identifying the dialect of a source text at the country level. Our approach to Subtask 1 makes use of language-specific language models, a clustering and retrieval method to provide additional context to a target sentence, a fine-tuning strategy which makes use of the provided data from the 2020 and 2021 shared tasks, and finally, ensembling over the predictions of multiple models. Our submission achieves a macro-averaged F1 score of 87.27, ranking 1st among the other participants in the task.</abstract>
      <url hash="971de8b5">2023.arabicnlp-1.68</url>
      <bibkey>elkaref-etal-2023-nlpeople-nadi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.68</doi>
    </paper>
    <paper id="69">
      <title><fixed-case>USTHB</fixed-case> at <fixed-case>NADI</fixed-case> 2023 shared task: Exploring Preprocessing and Feature Engineering Strategies for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <author><first>Aicha</first><last>Zitouni</last></author>
      <author><first>Houda</first><last>Latrache</last></author>
      <author><first>Rachida</first><last>Djeradi</last></author>
      <pages>647-651</pages>
      <abstract>In this paper, we conduct an in-depth analysis of several key factors influencing the performance of Arabic Dialect Identification NADI’2023, with a specific focus on the first subtask involving country-level dialect identification. Our investigation encompasses the effects of surface preprocessing, morphological preprocessing, FastText vector model, and the weighted concatenation of TF-IDF features. For classification purposes, we employ the Linear Support Vector Classification (LSVC) model. During the evaluation phase, our system demonstrates noteworthy results, achieving an F<tex-math>_1</tex-math> score of 62.51%. This achievement closely aligns with the average F<tex-math>_1</tex-math> scores attained by other systems submitted for the first subtask, which stands at 72.91%.</abstract>
      <url hash="f20d13b4">2023.arabicnlp-1.69</url>
      <bibkey>lichouri-etal-2023-usthb-nadi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.69</doi>
    </paper>
    <paper id="70">
      <title>rematchka at <fixed-case>NADI</fixed-case> 2023 shared task: Parameter Efficient tuning for Dialect Identification and Dialect Machine Translation</title>
      <author><first>Reem</first><last>Abdel-Salam</last></author>
      <pages>652-657</pages>
      <abstract>Dialect identification systems play a significant role in various fields and applications as in speech and language technologies, facilitating language education, supporting sociolinguistic research, preserving linguistic diversity, enhancing text-to-speech systems. In this paper, we provide our findings and results in NADI 2023 shared task for country-level dialect identification and machine translation (MT) from dialect to MSA. The proposed models achieved an F1-score of 86.18 at the dialect identification task, securing second place in first subtask. Whereas for the machine translation task, the submitted model achieved a BLEU score of 11.37 securing fourth and third place in second and third subtask. The proposed model utilizes parameter efficient training methods which achieves better performance when compared to conventional fine-tuning during the experimentation phase.</abstract>
      <url hash="c071bde0">2023.arabicnlp-1.70</url>
      <bibkey>abdel-salam-2023-rematchka-nadi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>anc at <fixed-case>NADI</fixed-case> 2023 Shared Task: A Comparison of Various T5-based Models for Translating <fixed-case>A</fixed-case>rabic Dialectical Text to <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic</title>
      <author><first>Abdullah</first><last>Khered</last></author>
      <author><first>Ingy</first><last>Abdelhalim</last></author>
      <author><first>Nadine</first><last>Abdelhalim</last></author>
      <author><first>Ahmed</first><last>Soliman</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>658-664</pages>
      <abstract>This paper presents the methods we developed for the Nuanced Arabic Dialect Identification (NADI) 2023 shared task, specifically targeting the two subtasks focussed on sentence-level machine translation (MT) of text written in any of four Arabic dialects (Egyptian, Emirati, Jordanian and Palestinian) to Modern Standard Arabic (MSA). Our team, UniManc, employed models based on T5: multilingual T5 (mT5), multi-task fine-tuned mT5 (mT0) and AraT5. These models were trained based on two configurations: joint model training for all regional dialects (J-R) and independent model training for every regional dialect (I-R). Based on the results of the official NADI 2023 evaluation, our I-R AraT5 model obtained an overall BLEU score of 14.76, ranking first in the Closed Dialect-to-MSA MT subtask. Moreover, in the Open Dialect-to-MSA MT subtask, our J-R AraT5 model also ranked first, obtaining an overall BLEU score of 21.10.</abstract>
      <url hash="14ee4c4a">2023.arabicnlp-1.71</url>
      <bibkey>khered-etal-2023-unimanc</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.71</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>IUNADI</fixed-case> at <fixed-case>NADI</fixed-case> 2023 shared task: Country-level <fixed-case>A</fixed-case>rabic Dialect Classification in Tweets for the Shared Task <fixed-case>NADI</fixed-case> 2023</title>
      <author><first>Yash</first><last>Hatekar</last></author>
      <author><first>Muhammad</first><last>Abdo</last></author>
      <pages>665-669</pages>
      <abstract>In this paper, we describe our participation in the NADI2023 shared task for the classification of Arabic dialects in tweets. For training, evaluation, and testing purposes, a primary dataset comprising tweets from 18 Arab countries is provided, along with three older datasets. The main objective is to develop a model capable of classifying tweets from these 18 countries. We outline our approach, which leverages various machine learning models. Our experiments demonstrate that large language models, particularly Arabertv2-Large, Arabertv2-Base, and CAMeLBERT-Mix DID MADAR, consistently outperform traditional methods such as SVM, XGBOOST, Multinomial Naive Bayes, AdaBoost, and Random Forests.</abstract>
      <url hash="77c7affc">2023.arabicnlp-1.72</url>
      <bibkey>hatekar-abdo-2023-iunadi</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.72</doi>
      <video href="2023.arabicnlp-1.72.mp4"/>
    </paper>
    <paper id="73">
      <title>The <fixed-case>H</fixed-case>elsinki-<fixed-case>NLP</fixed-case> Submissions at <fixed-case>NADI</fixed-case> 2023 Shared Task: Walking the Baseline</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Aleksandra</first><last>Miletić</last></author>
      <author><first>Olli</first><last>Kuparinen</last></author>
      <pages>670-677</pages>
      <abstract>The Helsinki-NLP team participated in the NADI 2023 shared tasks on Arabic dialect translation with seven submissions. We used statistical (SMT) and neural machine translation (NMT) methods and explored character- and subword-based data preprocessing. Our submissions placed second in both tracks. In the open track, our winning submission is a character-level SMT system with additional Modern Standard Arabic language models. In the closed track, our best BLEU scores were obtained with the leave-as-is baseline, a simple copy of the input, and narrowly followed by SMT systems. In both tracks, fine-tuning existing multilingual models such as AraT5 or ByT5 did not yield superior performance compared to SMT.</abstract>
      <url hash="770b88e1">2023.arabicnlp-1.73</url>
      <bibkey>scherrer-etal-2023-helsinki</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.73</doi>
    </paper>
    <paper id="74">
      <title>Mavericks at <fixed-case>NADI</fixed-case> 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach</title>
      <author><first>Vedant</first><last>Deshpande</last></author>
      <author><first>Yash</first><last>Patwardhan</last></author>
      <author><first>Kshitij</first><last>Deshpande</last></author>
      <author><first>Sudeep</first><last>Mangalvedhekar</last></author>
      <author><first>Ravindra</first><last>Murumkar</last></author>
      <pages>678-682</pages>
      <abstract>In this paper, we present our approach for the “Nuanced Arabic Dialect Identification (NADI) Shared Task 2023”. We highlight our methodology for subtask 1 which deals with country-level dialect identification. Recognizing dialects plays an instrumental role in enhancing the performance of various downstream NLP tasks such as speech recognition and translation. The task uses the Twitter dataset (TWT-2023) that encompasses 18 dialects for the multi-class classification problem. Numerous transformer-based models, pre-trained on Arabic language, are employed for identifying country-level dialects. We fine-tune these state-of-the-art models on the provided dataset. Ensembling method is leveraged to yield improved performance of the system. We achieved an F1-score of 76.65 (11th rank on leaderboard) on the test dataset.</abstract>
      <url hash="177c2810">2023.arabicnlp-1.74</url>
      <bibkey>deshpande-etal-2023-mavericks</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.74</doi>
      <video href="2023.arabicnlp-1.74.mp4"/>
    </paper>
    <paper id="75">
      <title><fixed-case>ANLP</fixed-case>-<fixed-case>RG</fixed-case> at <fixed-case>NADI</fixed-case> 2023 shared task: Machine Translation of <fixed-case>A</fixed-case>rabic Dialects: A Comparative Study of Transformer Models</title>
      <author><first>Wiem</first><last>Derouich</last></author>
      <author><first>Sameh</first><last>Kchaou</last></author>
      <author><first>Rahma</first><last>Boujelbane</last></author>
      <pages>683-689</pages>
      <abstract>In this paper, we present our findings within the context of the NADI-2023 Shared Task (Subtask 2). Our task involves developing a translation model from the Palestinian, Jordanian, Emirati, and Egyptian dialects to Modern Standard Arabic (MSA) using the MADAR parallel corpus, even though it lacks a parallel subset for the Emirati dialect. To address this challenge, we conducted a comparative analysis, evaluating the fine-tuning results of various transformer models using the MADAR corpus as a learning resource. Additionally, we assessed the effectiveness of existing translation tools in achieving our translation objectives. The best model achieved a BLEU score of 11.14% on the dev set and 10.02 on the test set.</abstract>
      <url hash="40168a82">2023.arabicnlp-1.75</url>
      <bibkey>derouich-etal-2023-anlp</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.75</doi>
      <video href="2023.arabicnlp-1.75.mp4"/>
    </paper>
    <paper id="76">
      <title>Qur’an <fixed-case>QA</fixed-case> 2023 Shared Task: Overview of Passage Retrieval and Reading Comprehension Tasks over the Holy Qur’an</title>
      <author><first>Rana</first><last>Malhas</last></author>
      <author><first>Watheq</first><last>Mansour</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>690-701</pages>
      <abstract>Motivated by the need for intelligent question answering (QA) systems on the Holy Qur’an and the success of the first Qur’an Question Answering shared task (Qur’an QA 2022 at OSACT 2022), we have organized the second version at ArabicNLP 2023. The Qur’an QA 2023 is composed of two sub-tasks: the passage retrieval (PR) task and the machine reading comprehension (MRC) task. The main aim of the shared task is to encourage state-of-the-art research on Arabic PR and MRC on the Holy Qur’an. Our shared task has attracted 9 teams to submit 22 runs for the PR task, and 6 teams to submit 17 runs for the MRC task. In this paper, we present an overview of the task and provide an outline of the approaches employed by the participating teams in both sub-tasks.</abstract>
      <url hash="84fd6604">2023.arabicnlp-1.76</url>
      <bibkey>malhas-etal-2023-quran</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.76</doi>
      <video href="2023.arabicnlp-1.76.mp4"/>
    </paper>
    <paper id="77">
      <title><fixed-case>AHJL</fixed-case> at Qur’an <fixed-case>QA</fixed-case> 2023 Shared Task: Enhancing Passage Retrieval using Sentence Transformer and Translation</title>
      <author><first>Hessa</first><last>Alawwad</last></author>
      <author><first>Lujain</first><last>Alawwad</last></author>
      <author><first>Jamilah</first><last>Alharbi</last></author>
      <author><first>Abdullah</first><last>Alharbi</last></author>
      <pages>702-707</pages>
      <abstract>The Holy Qur’an is central to Islam, influencing around two billion Muslims globally, and is known for its linguistic richness and complexity. This article discusses our involvement in the PR task (Task A) of the Qur’an QA 2023 Shared Task. We used two models: one employing the Sentence Transformer and the other using OpenAI’s embeddings for document retrieval. Both models, equipped with a translation feature, help interpret and understand Arabic language queries by translating them, executing the search, and then reverting the results to Arabic. Our results show that incorporating translation functionalities improves the performance in Arabic Question-Answering systems. The model with translation enhancement performed notably better in all metrics compared to the non-translation model.</abstract>
      <url hash="f87dab76">2023.arabicnlp-1.77</url>
      <bibkey>alawwad-etal-2023-ahjl</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.77</doi>
      <video href="2023.arabicnlp-1.77.mp4"/>
    </paper>
    <paper id="78">
      <title><fixed-case>L</fixed-case>ow<fixed-case>R</fixed-case>es<fixed-case>C</fixed-case>ontext<fixed-case>QA</fixed-case> at Qur’an <fixed-case>QA</fixed-case> 2023 Shared Task: Temporal and Sequential Representation Augmented Question Answering Span Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>708-713</pages>
      <abstract>The Qur’an holds immense theological and historical significance, and developing a technology-driven solution for answering questions from this sacred text is of paramount importance. This paper presents our approach to task B of Qur’an QA 2023, part of EMNLP 2023, addressing this challenge by proposing a robust method for extracting answers from Qur’anic passages. Leveraging the Qur’anic Reading Comprehension Dataset (QRCD) v1.2, we employ innovative techniques and advanced models to improve the precision and contextuality of answers derived from Qur’anic passages. Our methodology encompasses the utilization of start and end logits, Long Short-Term Memory (LSTM) networks, and fusion mechanisms, contributing to the ongoing dialogue at the intersection of technology and spirituality.</abstract>
      <url hash="a2ff194f">2023.arabicnlp-1.78</url>
      <bibkey>veeramani-etal-2023-lowrescontextqa</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.78</doi>
    </paper>
    <paper id="79">
      <title><fixed-case>GYM</fixed-case> at Qur’an <fixed-case>QA</fixed-case> 2023 Shared Task: Multi-Task Transfer Learning for <fixed-case>Q</fixed-case>uranic Passage Retrieval and Question Answering with Large Language Models</title>
      <author><first>Ghazaleh</first><last>Mahmoudi</last></author>
      <author><first>Yeganeh</first><last>Morshedzadeh</last></author>
      <author><first>Sauleh</first><last>Eetemadi</last></author>
      <pages>714-719</pages>
      <abstract>This work addresses the challenges of question answering for vintage texts like the Quran. It introduces two tasks: passage retrieval and reading comprehension. For passage retrieval, it employs unsupervised fine-tuning sentence encoders and supervised multi-task learning. In reading comprehension, it fine-tunes an Electra-based model, demonstrating significant improvements over baseline models. Our best AraElectra model achieves 46.1% partial Average Precision (pAP) on the unseen test set, outperforming the baseline by 23%.</abstract>
      <url hash="dde2223e">2023.arabicnlp-1.79</url>
      <bibkey>mahmoudi-etal-2023-gym</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.79</doi>
      <video href="2023.arabicnlp-1.79.mp4"/>
    </paper>
    <paper id="80">
      <title><fixed-case>LKAU</fixed-case>23 at Qur’an <fixed-case>QA</fixed-case> 2023: Using Transformer Models for Retrieving Passages and Finding Answers to Questions from the Qur’an</title>
      <author><first>Sarah</first><last>Alnefaie</last></author>
      <author><first>Abdullah</first><last>Alsaleh</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Mohammad</first><last>Alsalka</last></author>
      <author><first>Abdulrahman</first><last>Altahhan</last></author>
      <pages>720-727</pages>
      <abstract>The Qur’an QA 2023 shared task has two sub tasks: Passage Retrieval (PR) task and Machine Reading Comprehension (MRC) task. Our participation in the PR task was to further train several Arabic pre-trained models using a Sentence-Transformers architecture and to ensemble the best performing models. The results of the test set did not reflect the results of the development set. CL-AraBERT achieved the best results, with a 0.124 MAP. We also participate in the MRC task by further fine-tuning the base and large variants of AraBERT using Classical Arabic and Modern Standard Arabic datasets. Base AraBERT achieved the best result with the development set with a partial average precision (pAP) of 0.49, while it achieved 0.5 with the test set. In addition, we applied the ensemble approach of best performing models and post-processing steps to the final results. Our experiments with the development set showed that our proposed model achieved a 0.537 pAP. On the test set, our system obtained a pAP score of 0.49.</abstract>
      <url hash="c9b27de1">2023.arabicnlp-1.80</url>
      <bibkey>alnefaie-etal-2023-lkau23</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.80</doi>
      <video href="2023.arabicnlp-1.80.mp4"/>
    </paper>
    <paper id="81">
      <title><fixed-case>TCE</fixed-case> at Qur’an <fixed-case>QA</fixed-case> 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur’anic <fixed-case>QA</fixed-case></title>
      <author><first>Mohammed</first><last>Elkomy</last></author>
      <author><first>Amany</first><last>Sarhan</last></author>
      <pages>728-742</pages>
      <abstract>In this paper, we present our approach to tackle Qur’an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05% for task A and a partial Average Precision (pAP) of 57.11% for task B.</abstract>
      <url hash="280a4272">2023.arabicnlp-1.81</url>
      <bibkey>elkomy-sarhan-2023-tce</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.81</doi>
      <video href="2023.arabicnlp-1.81.mp4"/>
    </paper>
    <paper id="82">
      <title>Al-Jawaab at Qur’an <fixed-case>QA</fixed-case> 2023 Shared Task: Exploring Embeddings and <fixed-case>GPT</fixed-case> Models for Passage Retrieval and Reading Comprehension</title>
      <author><first>Abdulrezzak</first><last>Zekiye</last></author>
      <author><first>Fadi</first><last>Amroush</last></author>
      <pages>743-747</pages>
      <abstract>This paper introduces a comprehensive system designed to address two natural language processing tasks: Passage Retrieval (Task A) and Reading Comprehension (Task B), applied to datasets related to the Holy Qur’an. Task A was treated as a measurement of a textual similarity problem where the system leverages OpenAI’s “text-embedding-ada-002” embedding model to transform textual content into numerical representations, with cosine similarity serving as the proximity metric. Task B focuses on the extraction of answers from Qur’anic passages, employing the Generative Pre-trained Transformer-4 (GPT-4) language model. In Task A, the system is evaluated using the Mean Average Precision (MAP) metric, achieving MAP scores of 0.109438 and 0.06426543057 on the development and test datasets with an optimal similarity threshold set at 0.85. Task B evaluation employs partial Average Precision (pAP), where our system surpasses a baseline whole-passage retriever with pAP scores of 0.470 and 0.5393130538 on the development and test datasets, respectively.</abstract>
      <url hash="5a54cca8">2023.arabicnlp-1.82</url>
      <bibkey>zekiye-amroush-2023-al</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.82</doi>
      <video href="2023.arabicnlp-1.82.mp4"/>
    </paper>
    <paper id="83">
      <title><fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> 2023: The First <fixed-case>A</fixed-case>rabic Named Entity Recognition Shared Task</title>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Mohammed</first><last>Khalilia</last></author>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Nagham</first><last>Hamad</last></author>
      <author><first>Alaa’</first><last>Omar</last></author>
      <pages>748-758</pages>
      <abstract>We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER) Shared Task. The primary focus of WojoodNER 2023 is on Arabic NER, offering a novel NER datasets (i.e., Wojood) and the definition of subtasks designed to facilitate meaningful comparisons between different NER approaches. WojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45 unique teams registered for this shared task, with 11 of them actively participating in the test phase. Specifically, 11 teams participated in FlatNER, while 8 teams tackled NestedNER. The winning team achieved F1 score of 91.96 and 93.73 in FlatNER and NestedNER respectively.</abstract>
      <url hash="ede42fa5">2023.arabicnlp-1.83</url>
      <bibkey>jarrar-etal-2023-wojoodner</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.83</doi>
    </paper>
    <paper id="84">
      <title><fixed-case>ELYADATA</fixed-case> at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> Shared Task: Data and Model-centric Approaches for <fixed-case>A</fixed-case>rabic Flat and Nested <fixed-case>NER</fixed-case></title>
      <author><first>Imen</first><last>Laouirine</last></author>
      <author><first>Haroun</first><last>Elleuch</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <pages>759-764</pages>
      <abstract>This paper describes our submissions to the WojoodNER shared task organized during the first ArabicNLP conference. We participated in the two proposed sub-tasks of flat and nested Named Entity Recognition (NER). Our systems were ranked first over eight and third over eleven in the Nested NER and Flat NER, respectively. All our primary submissions are based on DiffusionNER models (Shen et al., 2023), where the NER task is formulated as a boundary-denoising diffusion process. Experiments on nested WojoodNER achieves the best results with a micro F1-score of 93.73%. For the flat sub-task, our primary system was the third-best system, with a micro F1-score of 91.92%.</abstract>
      <url hash="f7f73fa1">2023.arabicnlp-1.84</url>
      <bibkey>laouirine-etal-2023-elyadata</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.84</doi>
      <video href="2023.arabicnlp-1.84.mp4"/>
    </paper>
    <paper id="85">
      <title>Lotus at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> Shared Task: Multilingual Transformers: Unveiling Flat and Nested Entity Recognition</title>
      <author><first>Jiyong</first><last>Li</last></author>
      <author><first>Dilshod</first><last>Azizov</last></author>
      <author><first>Hilal</first><last>AlQuabeh</last></author>
      <author><first>Shangsong</first><last>Liang</last></author>
      <pages>765-770</pages>
      <abstract>We introduce our systems developed for two subtasks in the shared task “Wojood” on Arabic NER detection, part of ArabicNLP 2023. For Subtask 1, we employ the XLM-R model to predict Flat NER labels for given tokens using a single classifier capable of categorizing all labels. For Subtask 2, we use the XLM-R encoder by building 21 individual classifiers. Each classifier corresponds to a specific label and is designed to determine the presence of its respective label. In terms of performance, our systems achieved competitive <i>micro-F1</i> scores of <b>0.83</b> for Subtask 1 and <b>0.76</b> for Subtask 2, according to the leaderboard scores.</abstract>
      <url hash="6f41463c">2023.arabicnlp-1.85</url>
      <bibkey>li-etal-2023-lotus</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.85</doi>
    </paper>
    <paper id="86">
      <title><fixed-case>A</fixed-case>lex<fixed-case>U</fixed-case>-<fixed-case>AIC</fixed-case> at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> shared task: Sequence Labeling vs <fixed-case>MRC</fixed-case> and <fixed-case>SWA</fixed-case> for <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Shereen</first><last>Elkordi</last></author>
      <author><first>Noha</first><last>Adly</last></author>
      <author><first>Marwan</first><last>Torki</last></author>
      <pages>771-776</pages>
      <abstract>Named entity recognition (NER) is one of many challenging tasks in Arabic Natural Language Processing. It is also the base of many critical downstream tasks to help understand the source of major trends and public opinion. In this paper, we will describe our submission in the NER Shared Task of ArabicNLP 2023. We used a simple machine reading comprehension-based technique in the Flat NER Subtask ranking eighth on the leaderboard, while we fine-tuned a language model for the Nested NER Subtask ranking third on the leaderboard.</abstract>
      <url hash="e23a3f95">2023.arabicnlp-1.86</url>
      <bibkey>elkordi-etal-2023-alexu</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.86</doi>
      <video href="2023.arabicnlp-1.86.mp4"/>
    </paper>
    <paper id="87">
      <title><fixed-case>UM</fixed-case>6<fixed-case>P</fixed-case> &amp; <fixed-case>UL</fixed-case> at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> shared task: Improving Multi-Task Learning for Flat and Nested <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Salima</first><last>Lamsiyah</last></author>
      <author><first>Hamza</first><last>Alami</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <pages>777-782</pages>
      <abstract>In this paper, we present our submitted system for the WojoodNER Shared Task, addressing both flat and nested Arabic Named Entity Recognition (NER). Our system is based on a BERT-based multi-task learning model that leverages the existing Arabic Pretrained Language Models (PLMs) to encode the input sentences. To enhance the performance of our model, we have employed a multi-task loss variance penalty and combined several training objectives, including the Cross-Entropy loss, the Dice loss, the Tversky loss, and the Focal loss. Besides, we have studied the performance of three existing Arabic PLMs for sentence encoding. On the official test set, our system has obtained a micro-F1 score of 0.9113 and 0.9303 for Flat (Sub-Task 1) and Nested (Sub-Task 2) NER, respectively. It has been ranked in the 6th and the 2nd positions among all participating systems in Sub-Task 1 and Sub-Task 2, respectively.</abstract>
      <url hash="09352114">2023.arabicnlp-1.87</url>
      <bibkey>mahdaouy-etal-2023-um6p</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.87</doi>
    </paper>
    <paper id="88">
      <title><fixed-case>A</fixed-case>lpha<fixed-case>B</fixed-case>rains at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> shared task: <fixed-case>A</fixed-case>rabic Named Entity Recognition by Using Character-based Context-Sensitive Word Representations</title>
      <author><first>Toqeer</first><last>Ehsan</last></author>
      <author><first>Amjad</first><last>Ali</last></author>
      <author><first>Ala</first><last>Al-Fuqaha</last></author>
      <pages>783-788</pages>
      <abstract>This paper presents Arabic named entity recognition models by employing the single-task and the multi-task learning paradigms. The models have been developed using character-based contextualized Embeddings from Language Model (ELMo) in the input layers of the bidirectional long-short term memory networks. The ELMo embeddings are quite capable of learning the morphology and contextual information of the tokens in word sequences. The single-task learning models outperformed the multi-task learning models and achieved micro F1-scores of 0.8751 and 0.8884 for the flat and nested annotations, respectively.</abstract>
      <url hash="a696be9c">2023.arabicnlp-1.88</url>
      <bibkey>ehsan-etal-2023-alphabrains-wojoodner</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.88</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>LIPN</fixed-case> at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> shared task: A Span-Based Approach for Flat and Nested <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Niama</first><last>El Elkhbir</last></author>
      <author><first>Urchade</first><last>Zaratiana</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>789-796</pages>
      <abstract>The Wojood Named Entity Recognition (NER) shared task introduces a comprehensive Arabic NER dataset encompassing both flat and nested entity tasks, addressing the challenge of limited Arabic resources. In this paper, we present our team <b>LIPN</b> approach to addressing the two subtasks of WojoodNER SharedTask. We frame NER as a span classification problem. We employ a pretrained language model for token representations and neural network classifiers. We use global decoding for flat NER and a greedy strategy for nested NER. Our model secured the first position in flat NER and the fourth position in nested NER during the competition, with an F-score of 91.96 and 92.45 respectively. Our code is publicly available (<url>https://github.com/niamaelkhbir/LIPN-at-WojoodSharedTask</url>).</abstract>
      <url hash="767722f5">2023.arabicnlp-1.89</url>
      <bibkey>elkhbir-etal-2023-lipn</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.89</doi>
    </paper>
    <paper id="90">
      <title><fixed-case>A</fixed-case>lex-<fixed-case>U</fixed-case> 2023 <fixed-case>NLP</fixed-case> at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> shared task: <fixed-case>A</fixed-case>ra<fixed-case>BINDER</fixed-case> (Bi-encoder for <fixed-case>A</fixed-case>rabic Named Entity Recognition)</title>
      <author><first>Mariam</first><last>Hussein</last></author>
      <author><first>Sarah</first><last>Khaled</last></author>
      <author><first>Marwan</first><last>Torki</last></author>
      <author><first>Nagwa</first><last>El-Makky</last></author>
      <pages>797-802</pages>
      <abstract>Named Entity Recognition (NER) is a crucial task in natural language processing that facilitates the extraction of vital information from text. However, NER for Arabic presents a significant challenge due to the language’s unique characteristics. In this paper, we introduce AraBINDER, our submission to the Wojood NER Shared Task 2023 (ArabicNLP 2023). The shared task comprises two sub-tasks: sub-task 1 focuses on Flat NER, while sub-task 2 centers on Nested NER. We have participated in both sub-tasks. The Bi-Encoder has proven its efficiency for NER in English. We employ AraBINDER (Arabic Bi-Encoder for Named Entity Recognition), which uses the power of two transformer encoders and employs contrastive learning to map candidate text spans and entity types into the same vector representation space. This approach frames NER as a representation learning problem that maximizes the similarity between the vector representations of an entity mention and its type. Our experiments reveal that AraBINDER achieves a micro F-1 score of 0.918 for Flat NER and 0.9 for Nested NER on the Wojood dataset.</abstract>
      <url hash="3af7de80">2023.arabicnlp-1.90</url>
      <bibkey>hussein-etal-2023-alex</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.90</doi>
      <video href="2023.arabicnlp-1.90.mp4"/>
    </paper>
    <paper id="91">
      <title>El-Kawaref at <fixed-case>W</fixed-case>ojood<fixed-case>NER</fixed-case> shared task: <fixed-case>S</fixed-case>taged<fixed-case>NER</fixed-case> for <fixed-case>A</fixed-case>rabic Named Entity Recognition</title>
      <author><first>Nehal</first><last>Elkaref</last></author>
      <author><first>Mohab</first><last>Elkaref</last></author>
      <pages>803-808</pages>
      <abstract>Named Entity Recognition (NER) is the task of identifying word-units that correspond to mentions as location, organization, person, or currency. In this shared task we tackle flat-entity classification for Arabic, where for each word-unit a single entity should be identified. To resolve the classification problem we propose StagedNER a novel technique to fine-tuning NER downstream tasks that divides the learning process of a transformer-model into two phases, where a model is tasked to learn sequence tags and then entity tags rather than learn both together simultaneously for an input sequence. We create an ensemble of two base models using this method that yield a score of on the development set and an F1 performance of 90.03% on the validation set and 91.95% on the test set.</abstract>
      <url hash="70e13ab4">2023.arabicnlp-1.91</url>
      <bibkey>elkaref-elkaref-2023-el</bibkey>
      <doi>10.18653/v1/2023.arabicnlp-1.91</doi>
    </paper>
  </volume>
</collection>
