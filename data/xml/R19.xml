<?xml version='1.0' encoding='UTF-8'?>
<collection id="R19">
  <volume id="1" ingest-date="2020-01-15">
    <meta>
      <booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</booktitle>
      <url hash="ff24b8c4">R19-1</url>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Galia</first><last>Angelova</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="e25a755c">R19-1000</url>
      <bibkey>ranlp-2019-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Table Structure Recognition Based on Cell Relationship, a Bottom-Up Approach</title>
      <author><first>Darshan</first><last>Adiga</last></author>
      <author><first>Shabir Ahmad</first><last>Bhat</last></author>
      <author><first>Muzaffar Bashir</first><last>Shah</last></author>
      <author><first>Viveka</first><last>Vyeth</last></author>
      <pages>1–8</pages>
      <abstract>In this paper, we present a relationship extraction based methodology for table structure recognition in PDF documents. The proposed deep learning-based method takes a bottom-up approach to table recognition in PDF documents. We outline the shortcomings of conventional approaches based on heuristics and machine learning-based top-down approaches. In this work, we explain how the task of table structure recognition can be modeled as a cell relationship extraction task and the importance of the bottom-up approach in recognizing the table cells. We use Multilayer Feedforward Neural Network for table structure recognition and compare the results of three feature sets. To gauge the performance of the proposed method, we prepared a training dataset using 250 tables in PDF documents, carefully selecting the table structures that are most commonly found in the documents. Our model achieves an overall accuracy of 97.95% and an F1-Score of 92.62% on the test dataset.</abstract>
      <url hash="a58abc13">R19-1001</url>
      <doi>10.26615/978-954-452-056-4_001</doi>
      <bibkey>adiga-etal-2019-table</bibkey>
    </paper>
    <paper id="2">
      <title>Identification of Good and Bad News on <fixed-case>T</fixed-case>witter</title>
      <author><first>Piush</first><last>Aggarwal</last></author>
      <author><first>Ahmet</first><last>Aker</last></author>
      <pages>9–17</pages>
      <abstract>Social media plays a great role in news dissemination which includes good and bad news. However, studies show that news, in general, has a significant impact on our mental stature and that this influence is more in bad news. An ideal situation would be that we have a tool that can help to filter out the type of news we do not want to consume. In this paper, we provide the basis for such a tool. In our work, we focus on Twitter. We release a manually annotated dataset containing 6,853 tweets from 5 different topical categories. Each tweet is annotated with good and bad labels. We also investigate various machine learning systems and features and evaluate their performance on the newly generated dataset. We also perform a comparative analysis with sentiments showing that sentiment alone is not enough to distinguish between good and bad news.</abstract>
      <url hash="86b5e65f">R19-1002</url>
      <doi>10.26615/978-954-452-056-4_002</doi>
      <bibkey>aggarwal-aker-2019-identification</bibkey>
    </paper>
    <paper id="3">
      <title>Bilingual Low-Resource Neural Machine Translation with Round-Tripping: The Case of <fixed-case>P</fixed-case>ersian-<fixed-case>S</fixed-case>panish</title>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <pages>18–24</pages>
      <abstract>The quality of Neural Machine Translation (NMT), as a data-driven approach, massively depends on quantity, quality, and relevance of the training dataset. Such approaches have achieved promising results for bilingually high-resource scenarios but are inadequate for low-resource conditions. This paper describes a round-trip training approach to bilingual low-resource NMT that takes advantage of monolingual datasets to address training data scarcity, thus augmenting translation quality. We conduct detailed experiments on Persian-Spanish as a bilingually low-resource scenario. Experimental results demonstrate that this competitive approach outperforms the baselines.</abstract>
      <url hash="7f7b0b45">R19-1003</url>
      <doi>10.26615/978-954-452-056-4_003</doi>
      <bibkey>ahmadnia-dorr-2019-bilingual</bibkey>
    </paper>
    <paper id="4">
      <title>Enhancing Phrase-Based Statistical Machine Translation by Learning Phrase Representations Using Long Short-Term Memory Network</title>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <pages>25–32</pages>
      <abstract>Phrases play a key role in Machine Translation (MT). In this paper, we apply a Long Short-Term Memory (LSTM) model over conventional Phrase-Based Statistical MT (PBSMT). The core idea is to use an LSTM encoder-decoder to score the phrase table generated by the PBSMT decoder. Given a source sequence, the encoder and decoder are jointly trained in order to maximize the conditional probability of a target sequence. Analytically, the performance of a PBSMT system is enhanced by using the conditional probabilities of phrase pairs computed by an LSTM encoder-decoder as an additional feature in the existing log-linear model. We compare the performance of the phrase tables in the PBSMT to the performance of the proposed LSTM and observe its positive impact on translation quality. We construct a PBSMT model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set.</abstract>
      <url hash="6173e682">R19-1004</url>
      <doi>10.26615/978-954-452-056-4_004</doi>
      <bibkey>ahmadnia-dorr-2019-enhancing</bibkey>
    </paper>
    <paper id="5">
      <title>Automatic <fixed-case>P</fixed-case>ropbank Generation for <fixed-case>T</fixed-case>urkish</title>
      <author><first>Koray</first><last>AK</last></author>
      <author><first>Olcay Taner</first><last>Yıldız</last></author>
      <pages>33–41</pages>
      <abstract>Semantic role labeling (SRL) is an important task for understanding natural languages, where the objective is to analyse propositions expressed by the verb and to identify each word that bears a semantic role. It provides an extensive dataset to enhance NLP applications such as information retrieval, machine translation, information extraction, and question answering. However, creating SRL models are difficult. Even in some languages, it is infeasible to create SRL models that have predicate-argument structure due to lack of linguistic resources. In this paper, we present our method to create an automatic Turkish PropBank by exploiting parallel data from the translated sentences of English PropBank. Experiments show that our method gives promising results.</abstract>
      <url hash="1be63682">R19-1005</url>
      <doi>10.26615/978-954-452-056-4_005</doi>
      <bibkey>ak-yildiz-2019-automatic</bibkey>
    </paper>
    <paper id="6">
      <title>Multilingual sentence-level bias detection in <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Desislava</first><last>Aleksandrova</last></author>
      <author><first>François</first><last>Lareau</last></author>
      <author><first>Pierre André</first><last>Ménard</last></author>
      <pages>42–51</pages>
      <abstract>We propose a multilingual method for the extraction of biased sentences from Wikipedia, and use it to create corpora in Bulgarian, French and English. Sifting through the revision history of the articles that at some point had been considered biased and later corrected, we retrieve the last tagged and the first untagged revisions as the before/after snapshots of what was deemed a violation of Wikipedia’s neutral point of view policy. We extract the sentences that were removed or rewritten in that edit. The approach yields sufficient data even in the case of relatively small Wikipedias, such as the Bulgarian one, where 62k articles produced 5k biased sentences. We evaluate our method by manually annotating 520 sentences for Bulgarian and French, and 744 for English. We assess the level of noise and analyze its sources. Finally, we exploit the data with well-known classification methods to detect biased sentences. Code and datasets are hosted at https://github.com/crim-ca/wiki-bias.</abstract>
      <url hash="b69040b4">R19-1006</url>
      <doi>10.26615/978-954-452-056-4_006</doi>
      <bibkey>aleksandrova-etal-2019-multilingual</bibkey>
      <pwccode url="https://github.com/crim-ca/wiki-bias" additional="false">crim-ca/wiki-bias</pwccode>
    </paper>
    <paper id="7">
      <title>Supervised Morphological Segmentation Using Rich Annotated Lexicon</title>
      <author><first>Ebrahim</first><last>Ansari</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Mohammad</first><last>Mahmoudi</last></author>
      <author><first>Hamid</first><last>Haghdoost</last></author>
      <author><first>Jonáš</first><last>Vidra</last></author>
      <pages>52–61</pages>
      <abstract>Morphological segmentation of words is the process of dividing a word into smaller units called morphemes; it is tricky especially when a morphologically rich or polysynthetic language is under question. In this work, we designed and evaluated several Recurrent Neural Network (RNN) based models as well as various other machine learning based approaches for the morphological segmentation task. We trained our models using annotated segmentation lexicons. To evaluate the effect of the training data size on our models, we decided to create a large hand-annotated morphologically segmented corpus of Persian words, which is, to the best of our knowledge, the first and the only segmentation lexicon for the Persian language. In the experimental phase, using the hand-annotated Persian lexicon and two smaller similar lexicons for Czech and Finnish languages, we evaluated the effect of the training data size, different hyper-parameters settings as well as different RNN-based models.</abstract>
      <url hash="702726a7">R19-1007</url>
      <doi>10.26615/978-954-452-056-4_007</doi>
      <bibkey>ansari-etal-2019-supervised</bibkey>
    </paper>
    <paper id="8">
      <title>Combining Lexical Substitutes in Neural Word Sense Induction</title>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Boris</first><last>Sheludko</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>62–70</pages>
      <abstract>Word Sense Induction (WSI) is the task of grouping of occurrences of an ambiguous word according to their meaning. In this work, we improve the approach to WSI proposed by Amrami and Goldberg (2018) based on clustering of lexical substitutes for an ambiguous word in a particular context obtained from neural language models. Namely, we propose methods for combining information from left and right context and similarity to the ambiguous word, which result in generating more accurate substitutes than the original approach. Our simple yet efficient improvement establishes a new state-of-the-art on WSI datasets for two languages. Besides, we show improvements to the original approach on a lexical substitution dataset.</abstract>
      <url hash="39e16f18">R19-1008</url>
      <doi>10.26615/978-954-452-056-4_008</doi>
      <bibkey>arefyev-etal-2019-combining</bibkey>
    </paper>
    <paper id="9">
      <title>Detecting Clitics Related Orthographic Errors in <fixed-case>T</fixed-case>urkish</title>
      <author><first>Ugurcan</first><last>Arikan</last></author>
      <author><first>Onur</first><last>Gungor</last></author>
      <author><first>Suzan</first><last>Uskudarli</last></author>
      <pages>71–76</pages>
      <abstract>For the spell correction task, vocabulary based methods have been replaced with methods that take morphological and grammar rules into account. However, such tools are fairly immature, and, worse, non-existent for many low resource languages. Checking only if a word is well-formed with respect to the morphological rules of a language may produce false negatives due to the ambiguity resulting from the presence of numerous homophonic words. In this work, we propose an approach to detect and correct the “de/da” clitic errors in Turkish text. Our model is a neural sequence tagger trained with a synthetically constructed dataset consisting of positive and negative samples. The model’s performance with this dataset is presented according to different word embedding configurations. The model achieved an F1 score of 86.67% on a synthetically constructed dataset. We also compared the model’s performance on a manually curated dataset of challenging samples that proved superior to other spelling correctors with 71% accuracy compared to the second-best (Google Docs) with and accuracy of 34%.</abstract>
      <url hash="71d9b4b1">R19-1009</url>
      <doi>10.26615/978-954-452-056-4_009</doi>
      <bibkey>arikan-etal-2019-detecting</bibkey>
    </paper>
    <paper id="10">
      <title>Benchmark Dataset for Propaganda Detection in <fixed-case>C</fixed-case>zech Newspaper Texts</title>
      <author><first>Vít</first><last>Baisa</last></author>
      <author><first>Ondřej</first><last>Herman</last></author>
      <author><first>Ales</first><last>Horak</last></author>
      <pages>77–83</pages>
      <abstract>Propaganda of various pressure groups ranging from big economies to ideological blocks is often presented in a form of objective newspaper texts. However, the real objectivity is here shaded with the support of imbalanced views and distorted attitudes by means of various manipulative stylistic techniques. In the project of Manipulative Propaganda Techniques in the Age of Internet, a new resource for automatic analysis of stylistic mechanisms for influencing the readers’ opinion is developed. In its current version, the resource consists of 7,494 newspaper articles from four selected Czech digital news servers annotated for the presence of specific manipulative techniques. In this paper, we present the current state of the annotations and describe the structure of the dataset in detail. We also offer an evaluation of bag-of-words classification algorithms for the annotated manipulative techniques.</abstract>
      <url hash="3dde6542">R19-1010</url>
      <doi>10.26615/978-954-452-056-4_010</doi>
      <bibkey>baisa-etal-2019-benchmark</bibkey>
    </paper>
    <paper id="11">
      <title>Diachronic Analysis of Entities by Exploiting <fixed-case>W</fixed-case>ikipedia Page revisions</title>
      <author><first>Pierpaolo</first><last>Basile</last></author>
      <author><first>Annalina</first><last>Caputo</last></author>
      <author><first>Seamus</first><last>Lawless</last></author>
      <author><first>Giovanni</first><last>Semeraro</last></author>
      <pages>84–91</pages>
      <abstract>In the last few years, the increasing availability of large corpora spanning several time periods has opened new opportunities for the diachronic analysis of language. This type of analysis can bring to the light not only linguistic phenomena related to the shift of word meanings over time, but it can also be used to study the impact that societal and cultural trends have on this language change. This paper introduces a new resource for performing the diachronic analysis of named entities built upon Wikipedia page revisions. This resource enables the analysis over time of changes in the relations between entities (concepts), surface forms (words), and the contexts surrounding entities and surface forms, by analysing the whole history of Wikipedia internal links. We provide some useful use cases that prove the impact of this resource on diachronic studies and delineate some possible future usage.</abstract>
      <url hash="9a1416ed">R19-1011</url>
      <doi>10.26615/978-954-452-056-4_011</doi>
      <bibkey>basile-etal-2019-diachronic</bibkey>
    </paper>
    <paper id="12">
      <title>Using a Lexical Semantic Network for the Ontology Building</title>
      <author><first>Nadia</first><last>Bebeshina-Clairet</last></author>
      <author><first>Sylvie</first><last>Despres</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>92–101</pages>
      <abstract>Building multilingual ontologies is a hard task as ontologies are often data-rich resources. We introduce an approach which allows exploiting structured lexical semantic knowledge for the ontology building. Given a multilingual lexical semantic (non ontological) resource and an ontology model, it allows mining relevant semantic knowledge and make the ontology building and enhancement process faster.</abstract>
      <url hash="6666a491">R19-1012</url>
      <doi>10.26615/978-954-452-056-4_012</doi>
      <bibkey>bebeshina-clairet-etal-2019-using</bibkey>
    </paper>
    <paper id="13">
      <title>Naive Regularizers for Low-Resource Neural Machine Translation</title>
      <author><first>Meriem</first><last>Beloucif</last></author>
      <author><first>Ana Valeria</first><last>Gonzalez</last></author>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>102–111</pages>
      <abstract>Neural machine translation models have little inductive bias, which can be a disadvantage in low-resource scenarios. Neural models have to be trained on large amounts of data and have been shown to perform poorly when only limited data is available. We show that using naive regularization methods, based on sentence length, punctuation and word frequencies, to penalize translations that are very different from the input sentences, consistently improves the translation quality across multiple low-resource languages. We experiment with 12 language pairs, varying the training data size between 17k to 230k sentence pairs. Our best regularizer achieves an average increase of 1.5 BLEU score and 1.0 TER score across all the language pairs. For example, we achieve a BLEU score of 26.70 on the IWSLT15 English–Vietnamese translation task simply by using relative differences in punctuation as a regularizer.</abstract>
      <url hash="8b1d0353">R19-1013</url>
      <doi>10.26615/978-954-452-056-4_013</doi>
      <bibkey>beloucif-etal-2019-naive</bibkey>
    </paper>
    <paper id="14">
      <title>Exploring Graph-Algebraic <fixed-case>CCG</fixed-case> Combinators for Syntactic-Semantic <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Sebastian</first><last>Beschke</last></author>
      <pages>112–121</pages>
      <abstract>We describe a new approach to semantic parsing based on Combinatory Categorial Grammar (CCG). The grammar’s semantic construction operators are defined in terms of a graph algebra, which allows our system to induce a compact CCG lexicon. We introduce an expectation maximisation algorithm which we use to filter our lexicon down to 2500 lexical templates. Our system achieves a semantic triple (Smatch) precision that is competitive with other CCG-based AMR parsing approaches.</abstract>
      <url hash="fc7f97d8">R19-1014</url>
      <doi>10.26615/978-954-452-056-4_014</doi>
      <bibkey>beschke-2019-exploring</bibkey>
    </paper>
    <paper id="15">
      <title>Quasi Bidirectional Encoder Representations from Transformers for Word Sense Disambiguation</title>
      <author><first>Michele</first><last>Bevilacqua</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>122–131</pages>
      <abstract>While contextualized embeddings have produced performance breakthroughs in many Natural Language Processing (NLP) tasks, Word Sense Disambiguation (WSD) has not benefited from them yet. In this paper, we introduce QBERT, a Transformer-based architecture for contextualized embeddings which makes use of a co-attentive layer to produce more deeply bidirectional representations, better-fitting for the WSD task. As a result, we are able to train a WSD system that beats the state of the art on the concatenation of all evaluation datasets by over 3 points, also outperforming a comparable model using ELMo.</abstract>
      <url hash="01a19f02">R19-1015</url>
      <doi>10.26615/978-954-452-056-4_015</doi>
      <bibkey>bevilacqua-navigli-2019-quasi</bibkey>
    </paper>
    <paper id="16">
      <title>Evaluating the Consistency of Word Embeddings from Small Data</title>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Aurélie</first><last>Herbelot</last></author>
      <pages>132–141</pages>
      <abstract>In this work, we address the evaluation of distributional semantic models trained on smaller, domain-specific texts, specifically, philosophical text. Specifically, we inspect the behaviour of models using a pre-trained background space in learning. We propose a measure of consistency which can be used as an evaluation metric when no in-domain gold-standard data is available. This measure simply computes the ability of a model to learn similar embeddings from different parts of some homogeneous data. We show that in spite of being a simple evaluation, consistency actually depends on various combinations of factors, including the nature of the data itself, the model used to train the semantic space, and the frequency of the learnt terms, both in the background space and in the in-domain data of interest.</abstract>
      <url hash="a9cbe167">R19-1016</url>
      <doi>10.26615/978-954-452-056-4_016</doi>
      <bibkey>bloem-etal-2019-evaluating</bibkey>
    </paper>
    <paper id="17">
      <title>Cross-Domain Training for Goal-Oriented Conversational Agents</title>
      <author><first>Alexandra Maria</first><last>Bodîrlău</last></author>
      <author><first>Stefania</first><last>Budulan</last></author>
      <author><first>Traian</first><last>Rebedea</last></author>
      <pages>142–150</pages>
      <abstract>Goal-Oriented Chatbots in fields such as customer support, providing certain information or general help with bookings or reservations, suffer from low performance partly due to the difficulty of obtaining large domain-specific annotated datasets. Given that the problem is closely related to the domain of the conversational agent and the data belonging to a specific domain is difficult to annotate, there have been some attempts at surpassing these challenges such as unsupervised pre-training or transfer learning between different domains. A more thorough analysis of the transfer learning mechanism is justified by the significant improvement of the results demonstrated in the results section. We describe extensive experiments using transfer learning and warm-starting techniques with improvements of more than 5% in relative percentage of success rate in the majority of cases, and up to 10x faster convergence as opposed to training the system without them.</abstract>
      <url hash="0ce7d975">R19-1017</url>
      <doi>10.26615/978-954-452-056-4_017</doi>
      <bibkey>bodirlau-etal-2019-cross</bibkey>
    </paper>
    <paper id="18">
      <title>Learning Sentence Embeddings for Coherence Modelling and Beyond</title>
      <author><first>Tanner</first><last>Bohn</last></author>
      <author><first>Yining</first><last>Hu</last></author>
      <author><first>Jinhang</first><last>Zhang</last></author>
      <author><first>Charles</first><last>Ling</last></author>
      <pages>151–160</pages>
      <abstract>We present a novel and effective technique for performing text coherence tasks while facilitating deeper insights into the data. Despite obtaining ever-increasing task performance, modern deep-learning approaches to NLP tasks often only provide users with the final network decision and no additional understanding of the data. In this work, we show that a new type of sentence embedding learned through self-supervision can be applied effectively to text coherence tasks while serving as a window through which deeper understanding of the data can be obtained. To produce these sentence embeddings, we train a recurrent neural network to take individual sentences and predict their location in a document in the form of a distribution over locations. We demonstrate that these embeddings, combined with simple visual heuristics, can be used to achieve performance competitive with state-of-the-art on multiple text coherence tasks, outperforming more complex and specialized approaches. Additionally, we demonstrate that these embeddings can provide insights useful to writers for improving writing quality and informing document structuring, and assisting readers in summarizing and locating information.</abstract>
      <url hash="b9dba296">R19-1018</url>
      <doi>10.26615/978-954-452-056-4_018</doi>
      <bibkey>bohn-etal-2019-learning</bibkey>
    </paper>
    <paper id="19">
      <title>Risk Factors Extraction from Clinical Texts based on Linked Open Data</title>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <author><first>Galia</first><last>Angelova</last></author>
      <author><first>Zhivko</first><last>Angelov</last></author>
      <pages>161–167</pages>
      <abstract>This paper presents experiments in risk factors analysis based on clinical texts enhanced with Linked Open Data (LOD). The idea is to determine whether a patient has risk factors for a specific disease analyzing only his/her outpatient records. A semantic graph of “meta-knowledge” about a disease of interest is constructed, with integrated multilingual terms (labels) of symptoms, risk factors etc. coming from Wikidata, PubMed, Wikipedia and MESH, and linked to clinical records of individual patients via ICD–10 codes. Then a predictive model is trained to foretell whether patients are at risk to develop the disease of interest. The testing was done using outpatient records from a nation-wide repository available for the period 2011-2016. The results show improvement of the overall performance of all tested algorithms (kNN, Naive Bayes, Tree, Logistic regression, ANN), when the clinical texts are enriched with LOD resources.</abstract>
      <url hash="c7d7b6f5">R19-1019</url>
      <doi>10.26615/978-954-452-056-4_019</doi>
      <bibkey>boytcheva-etal-2019-risk</bibkey>
    </paper>
    <paper id="20">
      <title>Parallel Sentence Retrieval From Comparable Corpora for Biomedical Text Simplification</title>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>168–177</pages>
      <abstract>Parallel sentences provide semantically similar information which can vary on a given dimension, such as language or register. Parallel sentences with register variation (like expert and non-expert documents) can be exploited for the automatic text simplification. The aim of automatic text simplification is to better access and understand a given information. In the biomedical field, simplification may permit patients to understand medical and health texts. Yet, there is currently no such available resources. We propose to exploit comparable corpora which are distinguished by their registers (specialized and simplified versions) to detect and align parallel sentences. These corpora are in French and are related to the biomedical area. Manually created reference data show 0.76 inter-annotator agreement. Our purpose is to state whether a given pair of specialized and simplified sentences is parallel and can be aligned or not. We treat this task as binary classification (alignment/non-alignment). We perform experiments with a controlled ratio of imbalance and on the highly unbalanced real data. Our results show that the method we present here can be used to automatically generate a corpus of parallel sentences from our comparable corpus.</abstract>
      <url hash="d27f7b92">R19-1020</url>
      <doi>10.26615/978-954-452-056-4_020</doi>
      <bibkey>cardon-grabar-2019-parallel</bibkey>
    </paper>
    <paper id="21">
      <title>Classifying Author Intention for Writer Feedback in Related Work</title>
      <author><first>Arlene</first><last>Casey</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Dorota</first><last>Glowacka</last></author>
      <pages>178–187</pages>
      <abstract>The ability to produce high-quality publishable material is critical to academic success but many Post-Graduate students struggle to learn to do so. While recent years have seen an increase in tools designed to provide feedback on aspects of writing, one aspect that has so far been neglected is the Related Work section of academic research papers. To address this, we have trained a supervised classifier on a corpus of 94 Related Work sections and evaluated it against a manually annotated gold standard. The classifier uses novel features pertaining to citation types and co-reference, along with patterns found from studying Related Works. We show that these novel features contribute to classifier performance with performance being favourable compared to other similar works that classify author intentions and consider feedback for academic writing.</abstract>
      <url hash="ce3c6934">R19-1021</url>
      <doi>10.26615/978-954-452-056-4_021</doi>
      <bibkey>casey-etal-2019-classifying</bibkey>
    </paper>
    <paper id="22">
      <title>Sparse Victory – A Large Scale Systematic Comparison of count-based and prediction-based vectorizers for text classification</title>
      <author><first>Rupak</first><last>Chakraborty</last></author>
      <author><first>Ashima</first><last>Elhence</last></author>
      <author><first>Kapil</first><last>Arora</last></author>
      <pages>188–197</pages>
      <abstract>In this paper we study the performance of several text vectorization algorithms on a diverse collection of 73 publicly available datasets. Traditional sparse vectorizers like Tf-Idf and Feature Hashing have been systematically compared with the latest state of the art neural word embeddings like Word2Vec, GloVe, FastText and character embeddings like ELMo, Flair. We have carried out an extensive analysis of the performance of these vectorizers across different dimensions like classification metrics (.i.e. precision, recall, accuracy), dataset-size, and imbalanced data (in terms of the distribution of the number of class labels). Our experiments reveal that the sparse vectorizers beat the neural word and character embedding models on 61 of the 73 datasets by an average margin of 3-5% (in terms of macro f1 score) and this performance is consistent across the different dimensions of comparison.</abstract>
      <url hash="2e9efb5f">R19-1022</url>
      <doi>10.26615/978-954-452-056-4_022</doi>
      <bibkey>chakraborty-etal-2019-sparse</bibkey>
      <pwccode url="https://github.com/opennlp/Large-Scale-Text-Classification" additional="false">opennlp/Large-Scale-Text-Classification</pwccode>
    </paper>
    <paper id="23">
      <title>A Fine-Grained Annotated Multi-Dialectal <fixed-case>A</fixed-case>rabic Corpus</title>
      <author><first>Anis</first><last>Charfi</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Syed Hassan</first><last>Mehdi</last></author>
      <author><first>Esraa</first><last>Mohamed</last></author>
      <pages>198–204</pages>
      <abstract>We present ARAP-Tweet 2.0, a corpus of 5 million dialectal Arabic tweets and 50 million words of about 3000 Twitter users from 17 Arab countries. Compared to the first version, the new corpus has significant improvements in terms of the data volume and the annotation quality. It is fully balanced with respect to dialect, gender, and three age groups: under 25 years, between 25 and 34, and 35 years and above. This paper describes the process of creating the corpus starting from gathering the dialectal phrases to find the users, to annotating their accounts and retrieving their tweets. We also report on the evaluation of the annotation quality using the inter-annotator agreement measures which were applied to the whole corpus and not just a subset. The obtained results were substantial with average Cohen’s Kappa values of 0.99, 0.92, and 0.88 for the annotation of gender, dialect, and age respectively. We also discuss some challenges encountered when developing this corpus.s.</abstract>
      <url hash="2acbc8d0">R19-1023</url>
      <doi>10.26615/978-954-452-056-4_023</doi>
      <bibkey>charfi-etal-2019-fine</bibkey>
    </paper>
    <paper id="24">
      <title>Personality-dependent Neural Text Summarization</title>
      <author><first>Pablo</first><last>Costa</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <pages>205–212</pages>
      <abstract>In Natural Language Generation systems, personalization strategies - i.e, the use of information about a target author to generate text that (more) closely resembles human-produced language - have long been applied to improve results. The present work addresses one such strategy - namely, the use of Big Five personality information about the target author - applied to the case of abstractive text summarization using neural sequence-to-sequence models. Initial results suggest that having access to personality information does lead to more accurate (or human-like) text summaries, and paves the way for more robust systems of this kind.</abstract>
      <url hash="07a9836c">R19-1024</url>
      <doi>10.26615/978-954-452-056-4_024</doi>
      <bibkey>costa-paraboni-2019-personality</bibkey>
    </paper>
    <paper id="25">
      <title>Self-Adaptation for Unsupervised Domain Adaptation</title>
      <author><first>Xia</first><last>Cui</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>213–222</pages>
      <abstract>Lack of labelled data in the target domain for training is a common problem in domain adaptation. To overcome this problem, we propose a novel unsupervised domain adaptation method that combines projection and self-training based approaches. Using the labelled data from the source domain, we first learn a projection that maximises the distance among the nearest neighbours with opposite labels in the source domain. Next, we project the source domain labelled data using the learnt projection and train a classifier for the target class prediction. We then use the trained classifier to predict pseudo labels for the target domain unlabelled data. Finally, we learn a projection for the target domain as we did for the source domain using the pseudo-labelled target domain data, where we maximise the distance between nearest neighbours having opposite pseudo labels. Experiments on a standard benchmark dataset for domain adaptation show that the proposed method consistently outperforms numerous baselines and returns competitive results comparable to that of SOTA including self-training, tri-training, and neural adaptations.</abstract>
      <url hash="0b318cf9">R19-1025</url>
      <doi>10.26615/978-954-452-056-4_025</doi>
      <bibkey>cui-bollegala-2019-self</bibkey>
    </paper>
    <paper id="26">
      <title>Speculation and Negation detection in <fixed-case>F</fixed-case>rench biomedical corpora</title>
      <author><first>Clément</first><last>Dalloux</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>223–232</pages>
      <abstract>In this work, we propose to address the detection of negation and speculation, and of their scope, in French biomedical documents. It has been indeed observed that they play an important role and provide crucial clues for other NLP applications. Our methods are based on CRFs and BiLSTM. We reach up to 97.21 % and 91.30 % F-measure for the detection of negation and speculation cues, respectively, using CRFs. For the computing of scope, we reach up to 90.81 % and 86.73 % F-measure on negation and speculation, respectively, using BiLSTM-CRF fed with word embeddings.</abstract>
      <url hash="cd55ded7">R19-1026</url>
      <doi>10.26615/978-954-452-056-4_026</doi>
      <bibkey>dalloux-etal-2019-speculation</bibkey>
    </paper>
    <paper id="27">
      <title>Porting Multilingual Morphological Resources to <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-Lemon</title>
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Stefania</first><last>Racioppa</last></author>
      <pages>233–238</pages>
      <abstract>We describe work consisting in porting various morphological resources to the OntoLex-Lemon model. A main objective of this work is to offer a uniform representation of different morphological data sets in order to be able to compare and interlink multilingual resources and to cross-check and interlink or merge the content of morphological resources of one and the same language. The results of our work will be published on the Linguistic Linked Open Data cloud.</abstract>
      <url hash="375fde11">R19-1027</url>
      <doi>10.26615/978-954-452-056-4_027</doi>
      <bibkey>declerck-racioppa-2019-porting</bibkey>
    </paper>
    <paper id="28">
      <title>Dependency-Based Self-Attention for Transformer <fixed-case>NMT</fixed-case></title>
      <author><first>Hiroyuki</first><last>Deguchi</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>239–246</pages>
      <abstract>In this paper, we propose a new Transformer neural machine translation (NMT) model that incorporates dependency relations into self-attention on both source and target sides, dependency-based self-attention. The dependency-based self-attention is trained to attend to the modifiee for each token under constraints based on the dependency relations, inspired by Linguistically-Informed Self-Attention (LISA). While LISA is originally proposed for Transformer encoder for semantic role labeling, this paper extends LISA to Transformer NMT by masking future information on words in the decoder-side dependency-based self-attention. Additionally, our dependency-based self-attention operates at sub-word units created by byte pair encoding. The experiments show that our model improves 1.0 BLEU points over the baseline model on the WAT’18 Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.</abstract>
      <url hash="480cb5d2">R19-1028</url>
      <doi>10.26615/978-954-452-056-4_028</doi>
      <bibkey>deguchi-etal-2019-dependency</bibkey>
    </paper>
    <paper id="29">
      <title>Detecting Toxicity in News Articles: Application to <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>247–258</pages>
      <abstract>Online media aim for reaching ever bigger audience and for attracting ever longer attention span. This competition creates an environment that rewards sensational, fake, and toxic news. To help limit their spread and impact, we propose and develop a news toxicity detector that can recognize various types of toxic content. While previous research primarily focused on English, here we target Bulgarian. We created a new dataset by crawling a website that for five years has been collecting Bulgarian news articles that were manually categorized into eight toxicity groups. Then we trained a multi-class classifier with nine categories: eight toxic and one non-toxic. We experimented with different representations based on ElMo, BERT, and XLM, as well as with a variety of domain-specific features. Due to the small size of our dataset, we created a separate model for each feature type, and we ultimately combined these models into a meta-classifier. The evaluation results show an accuracy of 59.0% and a macro-F1 score of 39.7%, which represent sizable improvements over the majority-class baseline (Acc=30.3%, macro-F1=5.2%).</abstract>
      <url hash="a06b5392">R19-1029</url>
      <doi>10.26615/978-954-452-056-4_029</doi>
      <bibkey>dinkov-etal-2019-detecting</bibkey>
      <pwccode url="https://github.com/yoandinkov/ranlp-2019" additional="false">yoandinkov/ranlp-2019</pwccode>
    </paper>
    <paper id="30">
      <title>De-Identification of Emails: Pseudonymizing Privacy-Sensitive Data in a <fixed-case>G</fixed-case>erman Email Corpus</title>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <author><first>Ulrike</first><last>Krieg-Holz</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>259–269</pages>
      <abstract>We deal with the pseudonymization of those stretches of text in emails that might allow to identify real individual persons. This task is decomposed into two steps. First, named entities carrying privacy-sensitive information (e.g., names of persons, locations, phone numbers or dates) are identified, and, second, these privacy-bearing entities are replaced by synthetically generated surrogates (e.g., a person originally named ‘John Doe’ is renamed as ‘Bill Powers’). We describe a system architecture for surrogate generation and evaluate our approach on CodeAlltag, a German email corpus.</abstract>
      <url hash="43043d7d">R19-1030</url>
      <doi>10.26615/978-954-452-056-4_030</doi>
      <bibkey>eder-etal-2019-de</bibkey>
    </paper>
    <paper id="31">
      <title>Lexical Quantile-Based Text Complexity Measure</title>
      <author><first>Maksim</first><last>Eremeev</last></author>
      <author><first>Konstantin</first><last>Vorontsov</last></author>
      <pages>270–275</pages>
      <abstract>This paper introduces a new approach to estimating the text document complexity. Common readability indices are based on average length of sentences and words. In contrast to these methods, we propose to count the number of rare words occurring abnormally often in the document. We use the reference corpus of texts and the quantile approach in order to determine what words are rare, and what frequencies are abnormal. We construct a general text complexity model, which can be adjusted for the specific task, and introduce two special models. The experimental design is based on a set of thematically similar pairs of Wikipedia articles, labeled using crowdsourcing. The experiments demonstrate the competitiveness of the proposed approach.</abstract>
      <url hash="553b414f">R19-1031</url>
      <doi>10.26615/978-954-452-056-4_031</doi>
      <bibkey>eremeev-vorontsov-2019-lexical</bibkey>
    </paper>
    <paper id="32">
      <title>Demo Application for <fixed-case>LETO</fixed-case>: Learning Engine Through Ontologies</title>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <author><first>Yudivian</first><last>Almeida-Cruz</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <pages>276–284</pages>
      <abstract>The massive amount of multi-formatted information available on the Web necessitates the design of software systems that leverage this information to obtain knowledge that is valid and useful. The main challenge is to discover relevant information and continuously update, enrich and integrate knowledge from various sources of structured and unstructured data. This paper presents the Learning Engine Through Ontologies(LETO) framework, an architecture for the continuous and incremental discovery of knowledge from multiple sources of unstructured and structured data. We justify the main design decision behind LETO’s architecture and evaluate the framework’s feasibility using the Internet Movie Data Base(IMDB) and Twitter as a practical application.</abstract>
      <url hash="2aea80b4">R19-1032</url>
      <doi>10.26615/978-954-452-056-4_032</doi>
      <bibkey>estevez-velarde-etal-2019-demo</bibkey>
    </paper>
    <paper id="33">
      <title>Sentence Simplification for Semantic Role Labelling and Information Extraction</title>
      <author><first>Richard</first><last>Evans</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <pages>285–294</pages>
      <abstract>In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks: semantic role labelling (SRL) and information extraction (IE). The paper begins with our observation of challenges in the intrinsic evaluation of sentence simplification systems, which motivates the use of extrinsic evaluation of these systems with respect to other NLP tasks. We describe the two NLP systems and the test data used in the extrinsic evaluation, and present arguments and evidence motivating the integration of a sentence simplification step as a means of improving the accuracy of these systems. Our evaluation reveals that their performance is improved by the simplification step: the SRL system is better able to assign semantic roles to the majority of the arguments of verbs and the IE system is better able to identify fillers for all IE template slots.</abstract>
      <url hash="8704ae51">R19-1033</url>
      <doi>10.26615/978-954-452-056-4_033</doi>
      <bibkey>evans-orasan-2019-sentence</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>O</fixed-case>llo<fixed-case>B</fixed-case>ot - Towards A Text-Based <fixed-case>A</fixed-case>rabic Health Conversational Agent: Evaluation and Results</title>
      <author><first>Ahmed</first><last>Fadhil</last></author>
      <author><first>Ahmed</first><last>AbuRa’ed</last></author>
      <pages>295–303</pages>
      <abstract>We introduce OlloBot, an Arabic conversational agent that assists physicians and supports patients with the care process. It doesn’t replace the physicians, instead provides health tracking and support and assists physicians with the care delivery through a conversation medium. The current model comprises healthy diet, physical activity, mental health, in addition to food logging. Not only OlloBot tracks user daily food, it also offers useful tips for healthier living. We will discuss the design, development and testing of OlloBot, and highlight the findings and limitations arose from the testing.</abstract>
      <url hash="4f533371">R19-1034</url>
      <doi>10.26615/978-954-452-056-4_034</doi>
      <bibkey>fadhil-aburaed-2019-ollobot</bibkey>
    </paper>
    <paper id="35">
      <title>Developing the <fixed-case>O</fixed-case>ld <fixed-case>T</fixed-case>ibetan Treebank</title>
      <author><first>Christian</first><last>Faggionato</last></author>
      <author><first>Marieke</first><last>Meelen</last></author>
      <pages>304–312</pages>
      <abstract>This paper presents a full procedure for the development of a segmented, POS-tagged and chunkparsed corpus of Old Tibetan. As an extremely low-resource language, Old Tibetan poses non-trivial problems in every step towards the development of a searchable treebank. We demonstrate, however, that a carefully developed, semisupervised method of optimising and extending existing tools for Classical Tibetan, as well as creating specific ones for Old Tibetan can address these issues. We thus also present the first very Tibetan Treebank in a variety of formats to facilitate research in the fields of NLP, historical linguistics and Tibetan Studies.</abstract>
      <url hash="77f61ef5">R19-1035</url>
      <doi>10.26615/978-954-452-056-4_035</doi>
      <bibkey>faggionato-meelen-2019-developing</bibkey>
    </paper>
    <paper id="36">
      <title>Summarizing Legal Rulings: Comparative Experiments</title>
      <author><first>Diego</first><last>Feijo</last></author>
      <author><first>Viviane</first><last>Moreira</last></author>
      <pages>313–322</pages>
      <abstract>In the context of text summarization, texts in the legal domain have peculiarities related to their length and to their specialized vocabulary. Recent neural network-based approaches can achieve high-quality scores for text summarization. However, these approaches have been used mostly for generating very short abstracts for news articles. Thus, their applicability to the legal domain remains an open issue. In this work, we experimented with ten extractive and four abstractive models in a real dataset of legal rulings. These models were compared with an extractive baseline based on heuristics to select the most relevant parts of the text. Our results show that abstractive approaches significantly outperform extractive methods in terms of ROUGE scores.</abstract>
      <url hash="fe363e37">R19-1036</url>
      <doi>10.26615/978-954-452-056-4_036</doi>
      <bibkey>feijo-moreira-2019-summarizing</bibkey>
    </paper>
    <paper id="37">
      <title>Entropy as a Proxy for Gap Complexity in Open Cloze Tests</title>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>323–327</pages>
      <abstract>This paper presents a pilot study of entropy as a measure of gap complexity in open cloze tests aimed at learners of English. Entropy is used to quantify the information content in each gap, which can be used to estimate complexity. Our study shows that average gap entropy correlates positively with proficiency levels while individual gap entropy can capture contextual complexity. To the best of our knowledge, this is the first unsupervised information-theoretical approach to evaluating the quality of cloze tests.</abstract>
      <url hash="0d2992a4">R19-1037</url>
      <doi>10.26615/978-954-452-056-4_037</doi>
      <bibkey>felice-buttery-2019-entropy</bibkey>
    </paper>
    <paper id="38">
      <title>Song Lyrics Summarization Inspired by Audio Thumbnailing</title>
      <author><first>Michael</first><last>Fell</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Fabien</first><last>Gandon</last></author>
      <author><first>Alain</first><last>Giboin</last></author>
      <pages>328–337</pages>
      <abstract>Given the peculiar structure of songs, applying generic text summarization methods to lyrics can lead to the generation of highly redundant and incoherent text. In this paper, we propose to enhance state-of-the-art text summarization approaches with a method inspired by audio thumbnailing. Instead of searching for the thumbnail clues in the audio of the song, we identify equivalent clues in the lyrics. We then show how these summaries that take into account the audio nature of the lyrics outperform the generic methods according to both an automatic evaluation and human judgments.</abstract>
      <url hash="d165999f">R19-1038</url>
      <doi>10.26615/978-954-452-056-4_038</doi>
      <bibkey>fell-etal-2019-song</bibkey>
    </paper>
    <paper id="39">
      <title>Comparing Automated Methods to Detect Explicit Content in Song Lyrics</title>
      <author><first>Michael</first><last>Fell</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Michele</first><last>Corazza</last></author>
      <author><first>Fabien</first><last>Gandon</last></author>
      <pages>338–344</pages>
      <abstract>The Parental Advisory Label (PAL) is a warning label that is placed on audio recordings in recognition of profanity or inappropriate references, with the intention of alerting parents of material potentially unsuitable for children. Since 2015, digital providers – such as iTunes, Spotify, Amazon Music and Deezer – also follow PAL guidelines and tag such tracks as “explicit”. Nowadays, such labelling is carried out mainly manually on voluntary basis, with the drawbacks of being time consuming and therefore costly, error prone and partly a subjective task. In this paper, we compare automated methods ranging from dictionary-based lookup to state-of-the-art deep neural networks to automatically detect explicit contents in English lyrics. We show that more complex models perform only slightly better on this task, and relying on a qualitative analysis of the data, we discuss the inherent hardness and subjectivity of the task.</abstract>
      <url hash="5aff7ac8">R19-1039</url>
      <doi>10.26615/978-954-452-056-4_039</doi>
      <bibkey>fell-etal-2019-comparing</bibkey>
    </paper>
    <paper id="40">
      <title>Linguistic classification: dealing jointly with irrelevance and inconsistency</title>
      <author><first>Laura</first><last>Franzoi</last></author>
      <author><first>Andrea</first><last>Sgarro</last></author>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>345–352</pages>
      <abstract>In this paper, we present new methods for language classification which put to good use both syntax and fuzzy tools, and are capable of dealing with irrelevant linguistic features (i.e. features which should not contribute to the classification) and even inconsistent features (which do not make sense for specific languages). We introduce a metric distance, based on the generalized Steinhaus transform, which allows one to deal jointly with irrelevance and inconsistency. To evaluate our methods, we test them on a syntactic data set, due to the linguist G. Longobardi and his school. We obtain phylogenetic trees which sometimes outperform the ones obtained by Atkinson and Gray.</abstract>
      <url hash="5026a46f">R19-1040</url>
      <doi>10.26615/978-954-452-056-4_040</doi>
      <bibkey>franzoi-etal-2019-linguistic</bibkey>
    </paper>
    <paper id="41">
      <title>Corpus Lexicography in a Wider Context</title>
      <author><first>Chen</first><last>Gafni</last></author>
      <pages>353–359</pages>
      <abstract>This paper describes a set of tools that offers comprehensive solutions for corpus lexicography. The tools perform a range of tasks, including construction of corpus lexicon, integrating information from external dictionaries, internal analysis of the lexicon, and lexical analysis of the corpus. The set of tools is particularly useful for creating dictionaries for under-resourced languages. The tools are integrated in a general-purpose software that includes additional tools for various research tasks, such as linguistic development analysis. Equipped with a user-friendly interface, the described system can be easily incorporated in research in a variety of fields.</abstract>
      <url hash="feeed445">R19-1041</url>
      <doi>10.26615/978-954-452-056-4_041</doi>
      <bibkey>gafni-2019-corpus</bibkey>
    </paper>
    <paper id="42">
      <title>A Universal System for Automatic Text-to-Phonetics Conversion</title>
      <author><first>Chen</first><last>Gafni</last></author>
      <pages>360–366</pages>
      <abstract>This paper describes an automatic text-to-phonetics conversion system. The system was constructed to primarily serve as a research tool. It is implemented in a general-purpose linguistic software, which allows it to be incorporated in a multifaceted linguistic research in essentially any language. The system currently relies on two mechanisms to generate phonetic transcriptions from texts: (i) importing ready-made phonetic word forms from external dictionaries, and (ii) automatic generation of phonetic word forms based on a set of deterministic linguistic rules. The current paper describes the proposed system and its potential application to linguistic research.</abstract>
      <url hash="38c692a3">R19-1042</url>
      <doi>10.26615/978-954-452-056-4_042</doi>
      <bibkey>gafni-2019-universal</bibkey>
    </paper>
    <paper id="43">
      <title>Two Discourse Tree - Based Approaches to Indexing Answers</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>367–372</pages>
      <abstract>We explore anatomy of answers with respect to which text fragments from an answer are worth matching with a question and which should not be matched. We apply the Rhetorical Structure Theory to build a discourse tree of an answer and select elementary discourse units that are suitable for indexing. Manual rules for selection of these discourse units as well as automated classification based on web search engine mining are evaluated con-cerning improving search accuracy. We form two sets of question-answer pairs for FAQ and community QA search domains and use them for evaluation of the proposed indexing methodology, which delivers up to 16 percent improvement in search recall.</abstract>
      <url hash="c20ac7e9">R19-1043</url>
      <doi>10.26615/978-954-452-056-4_043</doi>
      <bibkey>galitsky-ilvovsky-2019-two</bibkey>
    </paper>
    <paper id="44">
      <title>Discourse-Based Approach to Involvement of Background Knowledge for Question Answering</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>373–381</pages>
      <abstract>We introduce a concept of a virtual discourse tree to improve question answering (Q/A) recall for complex, multi-sentence questions. Augmenting the discourse tree of an answer with tree fragments obtained from text corpora playing the role of ontology, we obtain on the fly a canonical discourse representation of this answer that is independent of the thought structure of a given author. This mechanism is critical for finding an answer that is not only relevant in terms of questions entities but also in terms of inter-relations between these entities in an answer and its style. We evaluate the Q/A system enabled with virtual discourse trees and observe a substantial increase of performance answering complex questions such as Yahoo! Answers and www.2carpros.com.</abstract>
      <url hash="bc2aefc1">R19-1044</url>
      <doi>10.26615/978-954-452-056-4_044</doi>
      <bibkey>galitsky-ilvovsky-2019-discourse</bibkey>
    </paper>
    <paper id="45">
      <title>On a Chatbot Providing Virtual Dialogues</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <pages>382–387</pages>
      <abstract>We present a chatbot that delivers content in the form of virtual dialogues automatically produced from the plain texts that are extracted and selected from the documents. This virtual dialogue content is provided in the form of answers derived from the found and selected documents split into fragments, and questions that are automatically generated for these answers based on the initial text.</abstract>
      <url hash="45b7af73">R19-1045</url>
      <doi>10.26615/978-954-452-056-4_045</doi>
      <bibkey>galitsky-etal-2019-chatbot</bibkey>
    </paper>
    <paper id="46">
      <title>Assessing socioeconomic status of <fixed-case>T</fixed-case>witter users: A survey</title>
      <author><first>Dhouha</first><last>Ghazouani</last></author>
      <author><first>Luigi</first><last>Lancieri</last></author>
      <author><first>Habib</first><last>Ounelli</last></author>
      <author><first>Chaker</first><last>Jebari</last></author>
      <pages>388–398</pages>
      <abstract>Every day, the emotion and opinion of different people across the world are reflected in the form of short messages using microblogging platforms. Despite the existence of enormous potential introduced by this data source, the Twitter community is still ambiguous and is not fully explored yet. While there are a huge number of studies examining the possibilities of inferring gender and age, there exist hardly researches on socioeconomic status (SES) inference of Twitter users. As socioeconomic status is essential to treating diverse questions linked to human behavior in several fields (sociology, demography, public health, etc.), we conducted a comprehensive literature review of SES studies, inference methods, and metrics. With reference to the research on literature’s results, we came to outline the most critical challenges for researchers. To the best of our knowledge, this paper is the first review that introduces the different aspects of SES inference. Indeed, this article provides the benefits for practitioners who aim to process and explore Twitter SES inference.</abstract>
      <url hash="607725ab">R19-1046</url>
      <doi>10.26615/978-954-452-056-4_046</doi>
      <bibkey>ghazouani-etal-2019-assessing</bibkey>
    </paper>
    <paper id="47">
      <title>Divide and Extract – Disentangling Clause Splitting and Proposition Extraction</title>
      <author><first>Darina</first><last>Gold</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>399–408</pages>
      <abstract>Proposition extraction from sentences is an important task for information extraction systems Evaluation of such systems usually conflates two aspects: splitting complex sentences into clauses and the extraction of propositions. It is thus difficult to independently determine the quality of the proposition extraction step. We create a manually annotated proposition dataset from sentences taken from restaurant reviews that distinguishes between clauses that need to be split and those that do not. The resulting proposition evaluation dataset allows us to independently compare the performance of proposition extraction systems on simple and complex clauses. Although performance drastically drops on more complex sentences, we show that the same systems perform best on both simple and complex clauses. Furthermore, we show that specific kinds of subordinate clauses pose difficulties to most systems.</abstract>
      <url hash="a6559902">R19-1047</url>
      <doi>10.26615/978-954-452-056-4_047</doi>
      <bibkey>gold-zesch-2019-divide</bibkey>
    </paper>
    <paper id="48">
      <title>Sparse Coding in Authorship Attribution for <fixed-case>P</fixed-case>olish Tweets</title>
      <author><first>Piotr</first><last>Grzybowski</last></author>
      <author><first>Ewa</first><last>Juralewicz</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <pages>409–417</pages>
      <abstract>The study explores application of a simple Convolutional Neural Network for the problem of authorship attribution of tweets written in Polish. In our solution we use two-step compression of tweets using Byte Pair Encoding algorithm and vectorisation as an input to the distributional model generated for the large corpus of Polish tweets by word2vec algorithm. Our method achieves results comparable to the state-of-the-art approaches for the similar task on English tweets and expresses a very good performance in the classification of Polish tweets. We tested the proposed method in relation to the number of authors and tweets per author. We also juxtaposed results for authors with different topic backgrounds against each other.</abstract>
      <url hash="9f339bb4">R19-1048</url>
      <doi>10.26615/978-954-452-056-4_048</doi>
      <bibkey>grzybowski-etal-2019-sparse</bibkey>
    </paper>
    <paper id="49">
      <title>Automatic Question Answering for Medical <fixed-case>MCQ</fixed-case>s: Can It go Further than Information Retrieval?</title>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <pages>418–422</pages>
      <abstract>We present a novel approach to automatic question answering that does not depend on the performance of an information retrieval (IR) system and does not require that the training data come from the same source as the questions. We evaluate the system performance on a challenging set of university-level medical science multiple-choice questions. Best performance is achieved when combining a neural approach with an IR approach, both of which work independently. Unlike previous approaches, the system achieves statistically significant improvement over the random guess baseline even for questions that are labeled as challenging based on the performance of baseline solvers.</abstract>
      <url hash="3f6f31c4">R19-1049</url>
      <doi>10.26615/978-954-452-056-4_049</doi>
      <bibkey>ha-yaneva-2019-automatic</bibkey>
    </paper>
    <paper id="50">
      <title>Self-Knowledge Distillation in Natural Language Processing</title>
      <author><first>Sangchul</first><last>Hahn</last></author>
      <author><first>Heeyoul</first><last>Choi</last></author>
      <pages>423–430</pages>
      <abstract>Since deep learning became a key player in natural language processing (NLP), many deep learning models have been showing remarkable performances in a variety of NLP tasks. Such high performance can be explained by efficient knowledge representation of deep learning models. Knowledge distillation from pretrained deep networks suggests that we can use more information from the soft target probability to train other neural networks. In this paper, we propose a self-knowledge distillation method, based on the soft target probabilities of the training model itself, where multimode information is distilled from the word embedding space right below the softmax layer. Due to the time complexity, our method approximates the soft target probabilities. In experiments, we applied the proposed method to two different and fundamental NLP tasks: language model and neural machine translation. The experiment results show that our proposed method improves performance on the tasks.</abstract>
      <url hash="c59dc67b">R19-1050</url>
      <doi>10.26615/978-954-452-056-4_050</doi>
      <bibkey>hahn-choi-2019-self</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="51">
      <title>From the Paft to the Fiiture: a Fully Automatic <fixed-case>NMT</fixed-case> and Word Embeddings Method for <fixed-case>OCR</fixed-case> Post-Correction</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Simon</first><last>Hengchen</last></author>
      <pages>431–436</pages>
      <abstract>A great deal of historical corpora suffer from errors introduced by the OCR (optical character recognition) methods used in the digitization process. Correcting these errors manually is a time-consuming process and a great part of the automatic approaches have been relying on rules or supervised machine learning. We present a fully automatic unsupervised way of extracting parallel data for training a character-based sequence-to-sequence NMT (neural machine translation) model to conduct OCR error correction.</abstract>
      <url hash="8cff198e">R19-1051</url>
      <doi>10.26615/978-954-452-056-4_051</doi>
      <bibkey>hamalainen-hengchen-2019-paft</bibkey>
      <pwccode url="https://github.com/mikahama/natas" additional="false">mikahama/natas</pwccode>
    </paper>
    <paper id="52">
      <title>Investigating Terminology Translation in Statistical and Neural Machine Translation: A Case Study on <fixed-case>E</fixed-case>nglish-to-<fixed-case>H</fixed-case>indi and <fixed-case>H</fixed-case>indi-to-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Md</first><last>Hasanuzzaman</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>437–446</pages>
      <abstract>Terminology translation plays a critical role in domain-specific machine translation (MT). In this paper, we conduct a comparative qualitative evaluation on terminology translation in phrase-based statistical MT (PB-SMT) and neural MT (NMT) in two translation directions: English-to-Hindi and Hindi-to-English. For this, we select a test set from a legal domain corpus and create a gold standard for evaluating terminology translation in MT. We also propose an error typology taking the terminology translation errors into consideration. We evaluate the MT systems’ performance on terminology translation, and demonstrate our findings, unraveling strengths, weaknesses, and similarities of PB-SMT and NMT in the area of term translation.</abstract>
      <url hash="22b4276a">R19-1052</url>
      <doi>10.26615/978-954-452-056-4_052</doi>
      <bibkey>haque-etal-2019-investigating</bibkey>
    </paper>
    <paper id="53">
      <title>Beyond <fixed-case>E</fixed-case>nglish-Only Reading Comprehension: Experiments in Zero-shot Multilingual Transfer for <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>447–459</pages>
      <abstract>Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects —history, biology, geography and philosophy—, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23%, which is well above the baseline of 24.89%.</abstract>
      <url hash="21539b59">R19-1053</url>
      <doi>10.26615/978-954-452-056-4_053</doi>
      <bibkey>hardalov-etal-2019-beyond</bibkey>
      <pwccode url="https://github.com/mhardalov/bg-reason-BERT" additional="false">mhardalov/bg-reason-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bulgarian-reading-comprehension-dataset">Bulgarian Reading Comprehension Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="54">
      <title>Tweaks and Tricks for Word Embedding Disruptions</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <pages>460–464</pages>
      <abstract>Word embeddings are established as very effective models used in several NLP applications. If they differ in their architecture and training process, they often exhibit similar properties and remain vector space models with continuously-valued dimensions describing the observed data. The complexity resides in the developed strategies for learning the values within each dimensional space. In this paper, we introduce the concept of disruption which we define as a side effect of the training process of embedding models. Disruptions are viewed as a set of embedding values that are more likely to be noise than effective descriptive features. We show that dealing with disruption phenomenon is of a great benefit to bottom-up sentence embedding representation. By contrasting several in-domain and pre-trained embedding models, we propose two simple but very effective tweaking techniques that yield strong empirical improvements on textual similarity task.</abstract>
      <url hash="879bd227">R19-1054</url>
      <doi>10.26615/978-954-452-056-4_054</doi>
      <bibkey>hazem-hernandez-2019-tweaks</bibkey>
    </paper>
    <paper id="55">
      <title>Meta-Embedding Sentence Representation for Textual Similarity</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <pages>465–473</pages>
      <abstract>Word embedding models are now widely used in most NLP applications. Despite their effectiveness, there is no clear evidence about the choice of the most appropriate model. It often depends on the nature of the task and on the quality and size of the used data sets. This remains true for bottom-up sentence embedding models. However, no straightforward investigation has been conducted so far. In this paper, we propose a systematic study of the impact of the main word embedding models on sentence representation. By contrasting in-domain and pre-trained embedding models, we show under which conditions they can be jointly used for bottom-up sentence embeddings. Finally, we propose the first bottom-up meta-embedding representation at the sentence level for textual similarity. Significant improvements are observed in several tasks including question-to-question similarity, paraphrasing and next utterance ranking.</abstract>
      <url hash="0ba5c91b">R19-1055</url>
      <doi>10.26615/978-954-452-056-4_055</doi>
      <bibkey>hazem-hernandez-2019-meta</bibkey>
    </paper>
    <paper id="56">
      <title>Emoji Powered Capsule Network to Detect Type and Target of Offensive Posts in Social Media</title>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>474–480</pages>
      <abstract>This paper describes a novel research approach to detect type and target of offensive posts in social media using a capsule network. The input to the network was character embeddings combined with emoji embeddings. The approach was evaluated on all three subtasks in Task 6 - SemEval 2019: OffensEval: Identifying and Categorizing Offensive Language in Social Media. The evaluation also showed that even though the capsule networks have not been used commonly in natural language processing tasks, they can outperform existing state of the art solutions for offensive language detection in social media.</abstract>
      <url hash="3f0f15b8">R19-1056</url>
      <doi>10.26615/978-954-452-056-4_056</doi>
      <bibkey>hettiarachchi-ranasinghe-2019-emoji</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>E</fixed-case>o<fixed-case>ANN</fixed-case>: Lexical Semantic Relation Classification Using an Ensemble of Artificial Neural Networks</title>
      <author><first>Rayehe</first><last>Hosseini Pour</last></author>
      <author><first>Mehrnoush</first><last>Shamsfard</last></author>
      <pages>481–486</pages>
      <abstract>Researchers use wordnets as a knowledge base in many natural language processing tasks and applications, such as question answering, textual entailment, discourse classification, and so forth. Lexico-semantic relations among words or concepts are important parts of knowledge encoded in wordnets. As the use of wordnets becomes extensively widespread, extending the existing ones gets more attention. Manually construction and extension of lexico-semantic relations for WordNets or knowledge graphs are very time-consuming. Using automatic relation extraction methods can speed up this process. In this study, we exploit an ensemble of lstm and convolutional neural networks in a supervised manner to capture lexico-semantic relations which can either be used directly in NLP applications or compose the edges of wordnets. The whole procedure of learning vector space representation of relations is language independent. We used Princeton WordNet 3.1, FarsNet 3.0 (the Persian wordnet), Root09 and EVALution as golden standards to evaluate the predictive performance of our model and the results are comparable on the two languages. Empirical results demonstrate that our model outperforms the state of the art models.</abstract>
      <url hash="0ea603e2">R19-1057</url>
      <doi>10.26615/978-954-452-056-4_057</doi>
      <bibkey>hosseini-pour-shamsfard-2019-eoann</bibkey>
    </paper>
    <paper id="58">
      <title>Opinions Summarization: Aspect Similarity Recognition Relaxes The Constraint of Predefined Aspects</title>
      <author><first>Nguyen</first><last>Huy Tien</last></author>
      <author><first>Le</first><last>Tung Thanh</last></author>
      <author><first>Nguyen</first><last>Minh Le</last></author>
      <pages>487–496</pages>
      <abstract>Recently research in opinions summarization focuses on rating expressions by aspects and/or sentiments they carry. To extract aspects of an expression, most studies require a predefined list of aspects or at least the number of aspects. Instead of extracting aspects, we rate expressions by aspect similarity recognition (ASR), which evaluates whether two expressions share at least one aspect. This subtask relaxes the limitation of predefining aspects and makes our opinions summarization applicable in domain adaptation. For the ASR subtask, we propose an attention-cell LSTM model, which integrates attention signals into the LSTM gates. According to the experimental results, the attention-cell LSTM works efficiently for learning latent aspects between two sentences in both settings of in-domain and cross-domain. In addition, the proposed extractive summarization method using ASR shows significant improvements over baselines on the Opinosis corpus.</abstract>
      <url hash="ed94089d">R19-1058</url>
      <doi>10.26615/978-954-452-056-4_058</doi>
      <bibkey>huy-tien-etal-2019-opinions</bibkey>
    </paper>
    <paper id="59">
      <title>Discourse-Aware Hierarchical Attention Network for Extractive Single-Document Summarization</title>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>497–506</pages>
      <abstract>Discourse relations between sentences are often represented as a tree, and the tree structure provides important information for summarizers to create a short and coherent summary. However, current neural network-based summarizers treat the source document as just a sequence of sentences and ignore the tree-like discourse structure inherent in the document. To incorporate the information of a discourse tree structure into the neural network-based summarizers, we propose a discourse-aware neural extractive summarizer which can explicitly take into account the discourse dependency tree structure of the source document. Our discourse-aware summarizer can jointly learn the discourse structure and the salience score of a sentence by using novel hierarchical attention modules, which can be trained on automatically parsed discourse dependency trees. Experimental results showed that our model achieved competitive or better performances against state-of-the-art models in terms of ROUGE scores on the DailyMail dataset. We further conducted manual evaluations. The results showed that our approach also gained the coherence of the output summaries.</abstract>
      <url hash="4a8868e8">R19-1059</url>
      <doi>10.26615/978-954-452-056-4_059</doi>
      <bibkey>ishigaki-etal-2019-discourse</bibkey>
    </paper>
    <paper id="60">
      <title>Semi-Supervised Induction of <fixed-case>POS</fixed-case>-Tag Lexicons with Tree Models</title>
      <author><first>Maciej</first><last>Janicki</last></author>
      <pages>507–515</pages>
      <abstract>We approach the problem of POS tagging of morphologically rich languages in a setting where only a small amount of labeled training data is available. We show that a bigram HMM tagger benefits from re-training on a larger untagged text using Baum-Welch estimation. Most importantly, this estimation can be significantly improved by pre-guessing tags for OOV words based on morphological criteria. We consider two models for this task: a character-based recurrent neural network, which guesses the tag from the string form of the word, and a recently proposed graph-based model of morphological transformations. In the latter, the unknown POS tags can be modeled as latent variables in a way very similar to Hidden Markov Tree models and an analogue of the Forward-Backward algorithm can be formulated, which enables us to compute expected values over unknown taggings. We evaluate both the quality of the induced tag lexicon and its impact on the HMM’s tagging accuracy. In both tasks, the graph-based morphology model performs significantly better than the RNN predictor. This confirms the intuition that morphologically related words provide useful information about an unknown word’s POS tag.</abstract>
      <url hash="478b9800">R19-1060</url>
      <doi>10.26615/978-954-452-056-4_060</doi>
      <bibkey>janicki-2019-semi</bibkey>
    </paper>
    <paper id="61">
      <title>Word Sense Disambiguation based on Constrained Random Walks in Linked Semantic Networks</title>
      <author><first>Arkadiusz</first><last>Janz</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <pages>516–525</pages>
      <abstract>Word Sense Disambiguation remains a challenging NLP task. Due to the lack of annotated training data, especially for rare senses, the supervised approaches are usually designed for specific subdomains limited to a narrow subset of identified senses. Recent advances in this area have shown that knowledge-based approaches are more scalable and obtain more promising results in all-words WSD scenarios. In this work we present a faster WSD algorithm based on the Monte Carlo approximation of sense probabilities given a context using constrained random walks over linked semantic networks. We show that the local semantic relatedness is mostly sufficient to successfully identify correct senses when an extensive knowledge base and a proper weighting scheme are used. The proposed methods are evaluated on English (SenseEval, SemEval) and Polish (Składnica, KPWr) datasets.</abstract>
      <url hash="3f9e5b75">R19-1061</url>
      <doi>10.26615/978-954-452-056-4_061</doi>
      <bibkey>janz-piasecki-2019-word</bibkey>
    </paper>
    <paper id="62">
      <title>Classification of Micro-Texts Using Sub-Word Embeddings</title>
      <author><first>Mihir</first><last>Joshi</last></author>
      <author><first>Nur</first><last>Zincir-Heywood</last></author>
      <pages>526–533</pages>
      <abstract>Extracting features and writing styles from short text messages is always a challenge. Short messages, like tweets, do not have enough data to perform statistical authorship attribution. Besides, the vocabulary used in these texts is sometimes improvised or misspelled. Therefore, in this paper, we propose combining four feature extraction techniques namely character n-grams, word n-grams, Flexible Patterns and a new sub-word embedding using the skip-gram model. Our system uses a Multi-Layer Perceptron to utilize these features from tweets to analyze short text messages. This proposed system achieves 85% accuracy, which is a considerable improvement over previous systems.</abstract>
      <url hash="274603f5">R19-1062</url>
      <doi>10.26615/978-954-452-056-4_062</doi>
      <bibkey>joshi-zincir-heywood-2019-classification</bibkey>
    </paper>
    <paper id="63">
      <title>Using Syntax to Resolve <fixed-case>NPE</fixed-case> in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <author><first>Allen</first><last>Antony</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>534–540</pages>
      <abstract>This paper describes a novel, syntax-based system for automatic detection and resolution of Noun Phrase Ellipsis (NPE) in English. The system takes in free input English text, detects the site of nominal elision, and if present, selects potential antecedent candidates. The rules are built using the syntactic information on ellipsis and its antecedent discussed in previous theoretical linguistics literature on NPE. Additionally, we prepare a curated dataset of 337 sentences from well-known, reliable sources, containing positive and negative samples of NPE. We split this dataset into two parts, and use one part to refine our rules and the other to test the performance of our final system. We get an F1-score of 76.47% for detection and 70.27% for NPE resolution on the testset. To the best of our knowledge, ours is the first system that detects and resolves NPE in English. The curated dataset used for this task, albeit small, covers a wide variety of NPE cases and will be made public for future work.</abstract>
      <url hash="8894d46e">R19-1063</url>
      <doi>10.26615/978-954-452-056-4_063</doi>
      <bibkey>khullar-etal-2019-using</bibkey>
    </paper>
    <paper id="64">
      <title>Is Similarity Visually Grounded? Computational Model of Similarity for the <fixed-case>E</fixed-case>stonian language</title>
      <author><first>Claudia</first><last>Kittask</last></author>
      <author><first>Eduard</first><last>Barbu</last></author>
      <pages>541–549</pages>
      <abstract>Researchers in Computational Linguistics build models of similarity and test them against human judgments. Although there are many empirical studies of the computational models of similarity for the English language, the similarity for other languages is less explored. In this study we are chiefly interested in two aspects. In the first place we want to know how much of the human similarity is grounded in the visual perception. To answer this question two neural computer vision models are used and their correlation with the human derived similarity scores is computed. In the second place we investigate if language influences the similarity computation. To this purpose diverse computational models trained on Estonian resources are evaluated against human judgments</abstract>
      <url hash="89cbffa5">R19-1064</url>
      <doi>10.26615/978-954-452-056-4_064</doi>
      <bibkey>kittask-barbu-2019-similarity</bibkey>
    </paper>
    <paper id="65">
      <title>Language-Agnostic <fixed-case>T</fixed-case>witter-Bot Detection</title>
      <author><first>Jürgen</first><last>Knauth</last></author>
      <pages>550–558</pages>
      <abstract>In this paper we address the problem of detecting Twitter bots. We analyze a dataset of 8385 Twitter accounts and their tweets consisting of both humans and different kinds of bots. We use this data to train machine learning classifiers that distinguish between real and bot accounts. We identify features that are easy to extract while still providing good results. We analyze different feature groups based on account specific, tweet specific and behavioral specific features and measure their performance compared to other state of the art bot detection methods. For easy future portability of our work we focus on language-agnostic features. With AdaBoost, the best performing classifier, we achieve an accuracy of 0.988 and an AUC of 0.995. As the creation of good training data in machine learning is often difficult - especially in the domain of Twitter bot detection - we additionally analyze to what extent smaller amounts of training data lead to useful results by reviewing cross-validated learning curves. Our results indicate that using few but expressive features already has a good practical benefit for bot detection, especially if only a small amount of training data is available.</abstract>
      <url hash="59f6fcbc">R19-1065</url>
      <doi>10.26615/978-954-452-056-4_065</doi>
      <bibkey>knauth-2019-language</bibkey>
    </paper>
    <paper id="66">
      <title>Multi-level analysis and recognition of the text sentiment on the example of consumer opinions</title>
      <author><first>Jan</first><last>Kocoń</last></author>
      <author><first>Monika</first><last>Zaśko-Zielińska</last></author>
      <author><first>Piotr</first><last>Miłkowski</last></author>
      <pages>559–567</pages>
      <abstract>In this article, we present a novel multi-domain dataset of Polish text reviews, annotated with sentiment on different levels: sentences and the whole documents. The annotation was made by linguists in a 2+1 scheme (with inter-annotator agreement analysis). We present a preliminary approach to the classification of labelled data using logistic regression, bidirectional long short-term memory recurrent neural networks (BiLSTM) and bidirectional encoder representations from transformers (BERT).</abstract>
      <url hash="f05f0742">R19-1066</url>
      <doi>10.26615/978-954-452-056-4_066</doi>
      <bibkey>kocon-etal-2019-multi-level</bibkey>
    </paper>
    <paper id="67">
      <title>A Qualitative Evaluation Framework for Paraphrase Identification</title>
      <author><first>Venelin</first><last>Kovatchev</last></author>
      <author><first>M. Antonia</first><last>Marti</last></author>
      <author><first>Maria</first><last>Salamo</last></author>
      <author><first>Javier</first><last>Beltran</last></author>
      <pages>568–577</pages>
      <abstract>In this paper, we present a new approach for the evaluation, error analysis, and interpretation of supervised and unsupervised Paraphrase Identification (PI) systems. Our evaluation framework makes use of a PI corpus annotated with linguistic phenomena to provide a better understanding and interpretation of the performance of various PI systems. Our approach allows for a qualitative evaluation and comparison of the PI models using human interpretable categories. It does not require modification of the training objective of the systems and does not place additional burden on the developers. We replicate several popular supervised and unsupervised PI systems. Using our evaluation framework we show that: 1) Each system performs differently with respect to a set of linguistic phenomena and makes qualitatively different kinds of errors; 2) Some linguistic phenomena are more challenging than others across all systems.</abstract>
      <url hash="704e5573">R19-1067</url>
      <doi>10.26615/978-954-452-056-4_067</doi>
      <bibkey>kovatchev-etal-2019-qualitative</bibkey>
    </paper>
    <paper id="68">
      <title>Study on Unsupervised Statistical Machine Translation for Backtranslation</title>
      <author><first>Anush</first><last>Kumar</last></author>
      <author><first>Nihal V.</first><last>Nayak</last></author>
      <author><first>Aditya</first><last>Chandra</last></author>
      <author><first>Mydhili K.</first><last>Nair</last></author>
      <pages>578–582</pages>
      <abstract>Machine Translation systems have drastically improved over the years for several language pairs. Monolingual data is often used to generate synthetic sentences to augment the training data which has shown to improve the performance of machine translation models. In our paper, we make use of an Unsupervised Statistical Machine Translation (USMT) to generate synthetic sentences. Our study compares the performance improvements in Neural Machine Translation model when using synthetic sentences from supervised and unsupervised Machine Translation models. Our approach of using USMT for backtranslation shows promise in low resource conditions and achieves an improvement of 3.2 BLEU score over the Neural Machine Translation model.</abstract>
      <url hash="fe816f1d">R19-1068</url>
      <doi>10.26615/978-954-452-056-4_068</doi>
      <bibkey>kumar-etal-2019-study</bibkey>
    </paper>
    <paper id="69">
      <title>Towards Functionally Similar Corpus Resources for Translation</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>583–592</pages>
      <abstract>The paper describes a computational approach to produce functionally comparable monolingual corpus resources for translation studies and contrastive analysis. We exploit a text-external approach, based on a set of Functional Text Dimensions to model text functions, so that each text can be represented as a vector in a multidimensional space of text functions. These vectors can be used to find reasonably homogeneous subsets of functionally similar texts across different corpora. Our models for predicting text functions are based on recurrent neural networks and traditional feature-based machine learning approaches. In addition to using the categories of the British National Corpus as our test case, we investigated the functional comparability of the English parts from the two parallel corpora: CroCo (English-German) and RusLTC (English-Russian) and applied our models to define functionally similar clusters in them. Our results show that the Functional Text Dimensions provide a useful description for text categories, while allowing a more flexible representation for texts with hybrid functions.</abstract>
      <url hash="c71335db">R19-1069</url>
      <doi>10.26615/978-954-452-056-4_069</doi>
      <bibkey>kunilovskaya-sharoff-2019-towards</bibkey>
    </paper>
    <paper id="70">
      <title>Question Similarity in Community Question Answering: A Systematic Exploration of Preprocessing Methods and Models</title>
      <author><first>Florian</first><last>Kunneman</last></author>
      <author><first>Thiago Castro</first><last>Ferreira</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <pages>593–601</pages>
      <abstract>Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers have developed methods to automatically detect question-similarity, which was one of the shared tasks in SemEval. The best performing systems for this task made use of Syntactic Tree Kernels or the SoftCosine metric. However, it remains unclear why these methods seem to work, whether their performance can be improved by better preprocessing methods and what kinds of errors they (and other methods) make. In this paper, we therefore systematically combine and compare these two approaches with the more traditional BM25 and translation-based models. Moreover, we analyze the impact of preprocessing steps (lowercasing, suppression of punctuation and stop words removal) and word meaning similarity based on different distributions (word translation probability, Word2Vec, fastText and ELMo) on the performance of the task. We conduct an error analysis to gain insight into the differences in performance between the system set-ups. The implementation is made publicly available from https://github.com/fkunneman/DiscoSumo/tree/master/ranlp.</abstract>
      <url hash="85b47cba">R19-1070</url>
      <doi>10.26615/978-954-452-056-4_070</doi>
      <bibkey>kunneman-etal-2019-question</bibkey>
      <pwccode url="https://github.com/fkunneman/DiscoSumo" additional="false">fkunneman/DiscoSumo</pwccode>
    </paper>
    <paper id="71">
      <title>A Classification-Based Approach to Cognate Detection Combining Orthographic and Semantic Similarity Information</title>
      <author><first>Sofie</first><last>Labat</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>602–610</pages>
      <abstract>This paper presents proof-of-concept experiments for combining orthographic and semantic information to distinguish cognates from non-cognates. To this end, a context-independent gold standard is developed by manually labelling English-Dutch pairs of cognates and false friends in bilingual term lists. These annotated cognate pairs are then used to train and evaluate a supervised binary classification system for the automatic detection of cognates. Two types of information sources are incorporated in the classifier: fifteen string similarity metrics capture form similarity between source and target words, while word embeddings model semantic similarity between the words. The experimental results show that even though the system already achieves good results by only incorporating orthographic information, the performance further improves by including semantic information in the form of embeddings.</abstract>
      <url hash="9f437e39">R19-1071</url>
      <doi>10.26615/978-954-452-056-4_071</doi>
      <bibkey>labat-lefever-2019-classification</bibkey>
    </paper>
    <paper id="72">
      <title>Resolving Pronouns for a Resource-Poor Language, <fixed-case>M</fixed-case>alayalam Using Resource-Rich Language, <fixed-case>T</fixed-case>amil.</title>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <pages>611–618</pages>
      <abstract>In this paper we give in detail how a resource rich language can be used for resolving pronouns for a less resource language. The source language, which is resource rich language in this study, is Tamil and the resource poor language is Malayalam, both belonging to the same language family, Dravidian. The Pronominal resolution developed for Tamil uses CRFs. Our approach is to leverage the Tamil language model to test Malayalam data and the processing required for Malayalam data is detailed. The similarity at the syntactic level between the languages is exploited in identifying the features for developing the Tamil language model. The word form or the lexical item is not considered as a feature for training the CRFs. Evaluation on Malayalam Wikipedia data shows that our approach is correct and the results, though not as good as Tamil, but comparable.</abstract>
      <url hash="a192769d">R19-1072</url>
      <doi>10.26615/978-954-452-056-4_072</doi>
      <bibkey>lalitha-devi-2019-resolving</bibkey>
    </paper>
    <paper id="73">
      <title>Semantic Role Labeling with Pretrained Language Models for Known and Unknown Predicates</title>
      <author><first>Daniil</first><last>Larionov</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Elena</first><last>Chistova</last></author>
      <author><first>Ivan</first><last>Smirnov</last></author>
      <pages>619–628</pages>
      <abstract>We build the first full pipeline for semantic role labelling of Russian texts. The pipeline implements predicate identification, argument extraction, argument classification (labeling), and global scoring via integer linear programming. We train supervised neural network models for argument classification using Russian semantically annotated corpus – FrameBank. However, we note that this resource provides annotations only to a very limited set of predicates. We combat the problem of annotation scarcity by introducing two models that rely on different sets of features: one for “known” predicates that are present in the training set and one for “unknown” predicates that are not. We show that the model for “unknown” predicates can alleviate the lack of annotation by using pretrained embeddings. We perform experiments with various types of embeddings including the ones generated by deep pretrained language models: word2vec, FastText, ELMo, BERT, and show that embeddings generated by deep pretrained language models are superior to classical shallow embeddings for argument classification of both “known” and “unknown” predicates.</abstract>
      <url hash="ce2927a0">R19-1073</url>
      <doi>10.26615/978-954-452-056-4_073</doi>
      <bibkey>larionov-etal-2019-semantic</bibkey>
    </paper>
    <paper id="74">
      <title>Structural Approach to Enhancing <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et with Conceptual Frame Semantics</title>
      <author><first>Svetlozara</first><last>Leseva</last></author>
      <author><first>Ivelina</first><last>Stoyanova</last></author>
      <pages>629–637</pages>
      <abstract>This paper outlines procedures for enhancing WordNet with conceptual information from FrameNet. The mapping of the two resources is non-trivial. We define a number of techniques for the validation of the consistency of the mapping and the extension of its coverage which make use of the structure of both resources and the systematic relations between synsets in WordNet and between frames in FrameNet, as well as between synsets and frames). We present a case study on causativity, a relation which provides enhancement complementary to the one using hierarchical relations, by means of linking in a systematic way large parts of the lexicon. We show how consistency checks and denser relations may be implemented on the basis of this relation. We, then, propose new frames based on causative-inchoative correspondences and in conclusion touch on the possibilities for defining new frames based on the types of specialisation that takes place from parent to child synset.</abstract>
      <url hash="67ff38ed">R19-1074</url>
      <doi>10.26615/978-954-452-056-4_074</doi>
      <bibkey>leseva-stoyanova-2019-structural</bibkey>
    </paper>
    <paper id="75">
      <title>Compositional Hyponymy with Positive Operators</title>
      <author><first>Martha</first><last>Lewis</last></author>
      <pages>638–647</pages>
      <abstract>Language is used to describe concepts, and many of these concepts are hierarchical. Moreover, this hierarchy should be compatible with forming phrases and sentences. We use linear-algebraic methods that allow us to encode words as collections of vectors. The representations we use have an ordering, related to subspace inclusion, which we interpret as modelling hierarchical information. The word representations built can be understood within a compositional distributional semantic framework, providing methods for composing words to form phrase and sentence level representations. We show that the resulting representations give competitive results on both word-level hyponymy and sentence-level entailment datasets.</abstract>
      <url hash="9836f1b1">R19-1075</url>
      <doi>10.26615/978-954-452-056-4_075</doi>
      <bibkey>lewis-2019-compositional</bibkey>
    </paper>
    <paper id="76">
      <title>The Impact of Semantic Linguistic Features in Relation Extraction: A Logical Relational Learning Approach</title>
      <author><first>Rinaldo</first><last>Lima</last></author>
      <author><first>Bernard</first><last>Espinasse</last></author>
      <author><first>Frederico</first><last>Freitas</last></author>
      <pages>648–654</pages>
      <abstract>Relation Extraction (RE) consists in detecting and classifying semantic relations between entities in a sentence. The vast majority of the state-of-the-art RE systems relies on morphosyntactic features and supervised machine learning algorithms. This paper tries to answer important questions concerning both the impact of semantic based features, and the integration of external linguistic knowledge resources on RE performance. For that, a RE system based on a logical and relational learning algorithm was used and evaluated on three reference datasets from two distinct domains. The yielded results confirm that the classifiers induced using the proposed richer feature set outperformed the classifiers built with morphosyntactic features in average 4% (F1-measure).</abstract>
      <url hash="962d0616">R19-1076</url>
      <doi>10.26615/978-954-452-056-4_076</doi>
      <bibkey>lima-etal-2019-impact</bibkey>
    </paper>
    <paper id="77">
      <title>Detecting Anorexia in <fixed-case>S</fixed-case>panish Tweets</title>
      <author><first>Pilar</first><last>López Úbeda</last></author>
      <author><first>Flor Miriam</first><last>Plaza del Arco</last></author>
      <author><first>Manuel Carlos</first><last>Díaz Galiano</last></author>
      <author><first>L. Alfonso</first><last>Urena Lopez</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <pages>655–663</pages>
      <abstract>Mental health is one of the main concerns of today’s society. Early detection of symptoms can greatly help people with mental disorders. People are using social networks more and more to express emotions, sentiments and mental states. Thus, the treatment of this information using NLP technologies can be applied to the automatic detection of mental problems such as eating disorders. However, the first step to solving the problem should be to provide a corpus in order to evaluate our systems. In this paper, we specifically focus on detecting anorexia messages on Twitter. Firstly, we have generated a new corpus of tweets extracted from different accounts including anorexia and non-anorexia messages in Spanish. The corpus is called SAD: Spanish Anorexia Detection corpus. In order to validate the effectiveness of the SAD corpus, we also propose several machine learning approaches for automatically detecting anorexia symptoms in the corpus. The good results obtained show that the application of textual classification methods is a promising option for developing this kind of system demonstrating that these tools could be used by professionals to help in the early detection of mental problems.</abstract>
      <url hash="48173a51">R19-1077</url>
      <doi>10.26615/978-954-452-056-4_077</doi>
      <bibkey>lopez-ubeda-etal-2019-detecting</bibkey>
    </paper>
    <paper id="78">
      <title>A type-theoretical reduction of morphological, syntactic and semantic compositionality to a single level of description</title>
      <author><first>Erkki</first><last>Luuk</last></author>
      <pages>664–673</pages>
      <abstract>The paper presents NLC, a new formalism for modeling natural language (NL) compositionality. NLC is a functional type system (i.e. one based on mathematical functions and their types). Its main features include a close correspondence with NL and an integrated modeling of morphological, syntactic and semantic compositionality. The integration is effected with a subclass of compound types (types which are syntactic compounds of multiple types or their terms), while the correspondence is sought with function types and polymorphism. The paper also presents an implementation of NLC in Coq. The implementation formalizes a diverse fragment of NL, with NLC expressions type checking and failing to type check in exactly the same ways that NL expressions pass and fail their acceptability tests. Among other things, this demonstrates the possibility of reducing morphological, syntactic and semantic compositionality to a single level of description. The level is tentatively identified with semantic compositionality — an interpretation which, besides being supported by results from language processing, has interesting implications on NL structure and modeling.</abstract>
      <url hash="afc6f102">R19-1078</url>
      <doi>10.26615/978-954-452-056-4_078</doi>
      <bibkey>luuk-2019-type</bibkey>
    </paper>
    <paper id="79">
      <title>v-trel: Vocabulary Trainer for Tracing Word Relations - An Implicit Crowdsourcing Approach</title>
      <author><first>Verena</first><last>Lyding</last></author>
      <author><first>Christos</first><last>Rodosthenous</last></author>
      <author><first>Federico</first><last>Sangati</last></author>
      <author><first>Umair</first><last>ul Hassan</last></author>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Alexander</first><last>König</last></author>
      <author><first>Jolita</first><last>Horbacauskiene</last></author>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <pages>674–683</pages>
      <abstract>In this paper, we present our work on developing a vocabulary trainer that uses exercises generated from language resources such as ConceptNet and crowdsources the responses of the learners to enrich the language resource. We performed an empirical evaluation of our approach with 60 non-native speakers over two days, which shows that new entries to expand Concept-Net can efficiently be gathered through vocabulary exercises on word relations. We also report on the feedback gathered from the users and an expert from language teaching, and discuss the potential of the vocabulary trainer application from the user and language learner perspective. The feedback suggests that v-trel has educational potential, while in its current state some shortcomings could be identified.</abstract>
      <url hash="e619259f">R19-1079</url>
      <doi>10.26615/978-954-452-056-4_079</doi>
      <bibkey>lyding-etal-2019-v</bibkey>
    </paper>
    <paper id="80">
      <title>Jointly Learning Author and Annotated Character N-gram Embeddings: A Case Study in Literary Text</title>
      <author><first>Suraj</first><last>Maharjan</last></author>
      <author><first>Deepthi</first><last>Mave</last></author>
      <author><first>Prasha</first><last>Shrestha</last></author>
      <author><first>Manuel</first><last>Montes</last></author>
      <author><first>Fabio A.</first><last>González</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>684–692</pages>
      <abstract>An author’s way of presenting a story through his/her writing style has a great impact on whether the story will be liked by readers or not. In this paper, we learn representations for authors of literary texts together with representations for character n-grams annotated with their functional roles. We train a neural character n-gram based language model using an external corpus of literary texts and transfer learned representations for use in downstream tasks. We show that augmenting the knowledge from external works of authors produces results competitive with other style-based methods for book likability prediction, genre classification, and authorship attribution.</abstract>
      <url hash="949d78fa">R19-1080</url>
      <doi>10.26615/978-954-452-056-4_080</doi>
      <bibkey>maharjan-etal-2019-jointly</bibkey>
    </paper>
    <paper id="81">
      <title>Generating Challenge Datasets for Task-Oriented Conversational Agents through Self-Play</title>
      <author><first>Sourabh</first><last>Majumdar</last></author>
      <author><first>Serra Sinem</first><last>Tekiroglu</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <pages>693–702</pages>
      <abstract>End-to-end neural approaches are becoming increasingly common in conversational scenarios due to their promising performances when provided with sufficient amount of data. In this paper, we present a novel methodology to address the interpretability of neural approaches in such scenarios by creating challenge datasets using dialogue self-play over multiple tasks/intents. Dialogue self-play allows generating large amount of synthetic data; by taking advantage of the complete control over the generation process, we show how neural approaches can be evaluated in terms of unseen dialogue patterns. We propose several out-of-pattern test cases each of which introduces a natural and unexpected user utterance phenomenon. As a proof of concept, we built a single and a multiple memory network, and show that these two architectures have diverse performances depending on the peculiar dialogue patterns.</abstract>
      <url hash="364417be">R19-1081</url>
      <doi>10.26615/978-954-452-056-4_081</doi>
      <bibkey>majumdar-etal-2019-generating</bibkey>
    </paper>
    <paper id="82">
      <title>Sentiment Polarity Detection in <fixed-case>A</fixed-case>zerbaijani Social News Articles</title>
      <author><first>Sevda</first><last>Mammadli</last></author>
      <author><first>Shamsaddin</first><last>Huseynov</last></author>
      <author><first>Huseyn</first><last>Alkaramov</last></author>
      <author><first>Ulviyya</first><last>Jafarli</last></author>
      <author><first>Umid</first><last>Suleymanov</last></author>
      <author><first>Samir</first><last>Rustamov</last></author>
      <pages>703–710</pages>
      <abstract>Text classification field of natural language processing has been experiencing remarkable growth in recent years. Especially, sentiment analysis has received a considerable attention from both industry and research community. However, only a few research examples exist for Azerbaijani language. The main objective of this research is to apply various machine learning algorithms for determining the sentiment of news articles in Azerbaijani language. Approximately, 30.000 social news articles have been collected from online news sites and labeled manually as negative or positive according to their sentiment categories. Initially, text preprocessing was implemented to data in order to eliminate the noise. Secondly, to convert text to a more machine-readable form, BOW (bag of words) model has been applied. More specifically, two methodologies of BOW model, which are tf-idf and frequency based model have been used as vectorization methods. Additionally, SVM, Random Forest, and Naive Bayes algorithms have been applied as the classification algorithms, and their combinations with two vectorization approaches have been tested and analyzed. Experimental results indicate that SVM outperforms other classification algorithms.</abstract>
      <url hash="916adab6">R19-1082</url>
      <doi>10.26615/978-954-452-056-4_082</doi>
      <bibkey>mammadli-etal-2019-sentiment</bibkey>
    </paper>
    <paper id="83">
      <title><fixed-case>I</fixed-case>nforex — a Collaborative Systemfor Text Corpora Annotation and Analysis Goes Open</title>
      <author><first>Michał</first><last>Marcińczuk</last></author>
      <author><first>Marcin</first><last>Oleksy</last></author>
      <pages>711–719</pages>
      <abstract>In the paper we present the latest changes introduce to Inforex — a web-based system for qualitative and collaborative text corpora annotation and analysis. One of the most important news is the release of source codes. Now the system is available on the GitHub repository (https://github.com/CLARIN-PL/Inforex) as an open source project. The system can be easily setup and run in a Docker container what simplifies the installation process. The major improvements include: semi-automatic text annotation, multilingual text preprocessing using CLARIN-PL web services, morphological tagging of XML documents, improved editor for annotation attribute, batch annotation attribute editor, morphological disambiguation, extended word sense annotation. This paper contains a brief description of the mentioned improvements. We also present two use cases in which various Inforex features were used and tested in real-life projects.</abstract>
      <url hash="44b874b1">R19-1083</url>
      <doi>10.26615/978-954-452-056-4_083</doi>
      <bibkey>marcinczuk-oleksy-2019-inforex</bibkey>
      <pwccode url="https://github.com/CLARIN-PL/Inforex" additional="false">CLARIN-PL/Inforex</pwccode>
    </paper>
    <paper id="84">
      <title>Semantic Language Model for <fixed-case>T</fixed-case>unisian Dialect</title>
      <author><first>Abir</first><last>Masmoudi</last></author>
      <author><first>Rim</first><last>Laatar</last></author>
      <author><first>Mariem</first><last>Ellouze</last></author>
      <author><first>Lamia</first><last>Hadrich Belguith</last></author>
      <pages>720–729</pages>
      <abstract>In this paper, we describe the process of creating a statistical Language Model (LM) for the Tunisian Dialect. Indeed, this work is part of the realization of Automatic Speech Recognition (ASR) system for the Tunisian Railway Transport Network. Since our eld of work has been limited, there are several words with similar behaviors (semantic for example) but they do not have the same appearance probability; their class groupings will therefore be possible. For these reasons, we propose to build an n-class LM that is based mainly on the integration of purely semantic data. Indeed, each class represents an abstraction of similar labels. In order to improve the sequence labeling task, we proposed to use a discriminative algorithm based on the Conditional Random Field (CRF) model. To better judge our choice of creating an n-class word model, we compared the created model with the 3-gram type model on the same test corpus of evaluation. Additionally, to assess the impact of using the CRF model to perform the semantic labelling task in order to construct semantic classes, we compared the n-class created model with using the CRF in the semantic labelling task and the n- class model without using the CRF in the semantic labelling task. The drawn comparison of the predictive power of the n-class model obtained by applying the CRF model in the semantic labelling is that it is better than the other two models presenting the highest value of its perplexity.</abstract>
      <url hash="57640f3f">R19-1084</url>
      <doi>10.26615/978-954-452-056-4_084</doi>
      <bibkey>masmoudi-etal-2019-semantic</bibkey>
    </paper>
    <paper id="85">
      <title>Automatic diacritization of <fixed-case>T</fixed-case>unisian dialect text using Recurrent Neural Network</title>
      <author><first>Abir</first><last>Masmoudi</last></author>
      <author><first>Mariem</first><last>Ellouze</last></author>
      <author><first>Lamia</first><last>Hadrich belguith</last></author>
      <pages>730–739</pages>
      <abstract>The absence of diacritical marks in the Arabic texts generally leads to morphological, syntactic and semantic ambiguities. This can be more blatant when one deals with under-resourced languages, such as the Tunisian dialect, which suffers from unavailability of basic tools and linguistic resources, like sufficient amount of corpora, multilingual dictionaries, morphological and syntactic analyzers. Thus, this language processing faces greater challenges due to the lack of these resources. The automatic diacritization of MSA text is one of the various complex problems that can be solved by deep neural networks today. Since the Tunisian dialect is an under-resourced language of MSA and as there are a lot of resemblance between both languages, we suggest to investigate a recurrent neural network (RNN) for this dialect diacritization problem. This model will be compared to our previous models models CRF and SMT (CITATION) based on the same dialect corpus. We can experimentally show that our model can achieve better outcomes (DER of 10.72%), as compared to the two models CRF (DER of 20.25%) and SMT (DER of 33.15%).</abstract>
      <url hash="46563a81">R19-1085</url>
      <doi>10.26615/978-954-452-056-4_085</doi>
      <bibkey>masmoudi-etal-2019-automatic</bibkey>
    </paper>
    <paper id="86">
      <title>Comparing <fixed-case>MT</fixed-case> Approaches for Text Normalization</title>
      <author><first>Claudia</first><last>Matos Veliz</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>740–749</pages>
      <abstract>One of the main characteristics of social media data is the use of non-standard language. Since NLP tools have been trained on traditional text material their performance drops when applied to social media data. One way to overcome this is to first perform text normalization. In this work, we apply text normalization to noisy English and Dutch text coming from different social media genres: text messages, message board posts and tweets. We consider the normalization task as a Machine Translation problem and test the two leading paradigms: statistical and neural machine translation. For SMT we explore the added value of varying background corpora for training the language model. For NMT we have a look at data augmentation since the parallel datasets we are working with are limited in size. Our results reveal that when relying on SMT to perform the normalization it is beneficial to use a background corpus that is close to the genre you are normalizing. Regarding NMT, we find that the translations - or normalizations - coming out of this model are far from perfect and that for a low-resource language like Dutch adding additional training data works better than artificially augmenting the data.</abstract>
      <url hash="8ba8d875">R19-1086</url>
      <doi>10.26615/978-954-452-056-4_086</doi>
      <bibkey>matos-veliz-etal-2019-comparing</bibkey>
    </paper>
    <paper id="87">
      <title>Sentiment and Emotion Based Representations for Fake Reviews Detection</title>
      <author><first>Alimuddin</first><last>Melleng</last></author>
      <author><first>Anna</first><last>Jurek-Loughrey</last></author>
      <author><first>Deepak</first><last>P</last></author>
      <pages>750–757</pages>
      <abstract>Fake reviews are increasingly prevalent across the Internet. They can be unethical as well as harmful. They can affect businesses and mislead individual customers. As the opinions on the Web are increasingly used the detection of fake reviews has become more and more critical. In this study, we explore the effectiveness of sentiment and emotions based representations for the task of building machine learning models for fake review detection. We perform empirical studies over three real world datasets and demonstrate that improved data representation can be achieved by combining sentiment and emotion extraction methods, as well as by performing sentiment and emotion analysis on a part-by-part basis by segmenting the reviews.</abstract>
      <url hash="c93eab66">R19-1087</url>
      <doi>10.26615/978-954-452-056-4_087</doi>
      <bibkey>melleng-etal-2019-sentiment</bibkey>
    </paper>
    <paper id="88">
      <title>Turning silver into gold: error-focused corpus reannotation with active learning</title>
      <author><first>Pierre André</first><last>Ménard</last></author>
      <author><first>Antoine</first><last>Mougeot</last></author>
      <pages>758–767</pages>
      <abstract>While high quality gold standard annotated corpora are crucial for most tasks in natural language processing, many annotated corpora published in recent years, created by annotators or tools, contains noisy annotations. These corpora can be viewed as more silver than gold standards, even if they are used in evaluation campaigns or to compare systems’ performances. As upgrading a silver corpus to gold level is still a challenge, we explore the application of active learning techniques to detect errors using four datasets designed for document classification and part-of-speech tagging. Our results show that the proposed method for the seeding step improves the chance of finding incorrect annotations by a factor of 2.73 when compared to random selection, a 14.71% increase from the baseline methods. Our query method provides an increase in the error detection precision on average by a factor of 1.78 against random selection, an increase of 61.82% compared to other query approaches.</abstract>
      <url hash="94de8053">R19-1088</url>
      <doi>10.26615/978-954-452-056-4_088</doi>
      <bibkey>menard-mougeot-2019-turning</bibkey>
    </paper>
    <paper id="89">
      <title>Community Perspective on Replicability in Natural Language Processing</title>
      <author><first>Margot</first><last>Mieskes</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Kevin</first><last>Cohen</last></author>
      <pages>768–775</pages>
      <abstract>With recent efforts in drawing attention to the task of replicating and/or reproducing results, for example in the context of COLING 2018 and various LREC workshops, the question arises how the NLP community views the topic of replicability in general. Using a survey, in which we involve members of the NLP community, we investigate how our community perceives this topic, its relevance and options for improvement. Based on over two hundred participants, the survey results confirm earlier observations, that successful reproducibility requires more than having access to code and data. Additionally, the results show that the topic has to be tackled from the authors’, reviewers’ and community’s side.</abstract>
      <url hash="d29cfb94">R19-1089</url>
      <doi>10.26615/978-954-452-056-4_089</doi>
      <bibkey>mieskes-etal-2019-community</bibkey>
    </paper>
    <paper id="90">
      <title>Unsupervised Data Augmentation for Less-Resourced Languages with no Standardized Spelling</title>
      <author><first>Alice</first><last>Millour</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <pages>776–784</pages>
      <abstract>Building representative linguistic resources and NLP tools for non-standardized languages is challenging: when spelling is not determined by a norm, multiple written forms can be encountered for a given word, inducing a large proportion of out-of-vocabulary words. To embrace this diversity, we propose a methodology based on crowdsourced alternative spellings we use to extract rules applied to match OOV words with one of their spelling variants. This virtuous process enables the unsupervised augmentation of multi-variant lexicons without expert rule definition. We apply this multilingual methodology on Alsatian, a French regional language and provide an intrinsic evaluation of the correctness of the variants pairs, and an extrinsic evaluation on a downstream task. We show that in a low-resource scenario, 145 inital pairs can lead to the generation of 876 additional variant pairs, and a diminution of OOV words improving the part-of-speech tagging performance by 1 to 4%.</abstract>
      <url hash="4873d2ab">R19-1090</url>
      <doi>10.26615/978-954-452-056-4_090</doi>
      <bibkey>millour-fort-2019-unsupervised</bibkey>
    </paper>
    <paper id="91">
      <title>Neural Feature Extraction for Contextual Emotion Detection</title>
      <author><first>Elham</first><last>Mohammadi</last></author>
      <author><first>Hessam</first><last>Amini</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>785–794</pages>
      <abstract>This paper describes a new approach for the task of contextual emotion detection. The approach is based on a neural feature extractor, composed of a recurrent neural network with an attention mechanism, followed by a classifier, that can be neural or SVM-based. We evaluated the model with the dataset of the task 3 of SemEval 2019 (EmoContext), which includes short 3-turn conversations, tagged with 4 emotion classes. The best performing setup was achieved using ELMo word embeddings and POS tags as input, bidirectional GRU as hidden units, and an SVM as the final classifier. This configuration reached 69.93% in terms of micro-average F1 score on the main 3 emotion classes, a score that outperformed the baseline system by 11.25%.</abstract>
      <url hash="7c5ca82a">R19-1091</url>
      <doi>10.26615/978-954-452-056-4_091</doi>
      <bibkey>mohammadi-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emocontext">EmoContext</pwcdataset>
    </paper>
    <paper id="92">
      <title>Empirical Study of Diachronic Word Embeddings for Scarce Data</title>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>795–803</pages>
      <abstract>Word meaning change can be inferred from drifts of time-varying word embeddings. However, temporal data may be too sparse to build robust word embeddings and to discriminate significant drifts from noise. In this paper, we compare three models to learn diachronic word embeddings on scarce data: incremental updating of a Skip-Gram from Kim et al. (2014), dynamic filtering from Bamler &amp; Mandt (2017), and dynamic Bernoulli embeddings from Rudolph &amp; Blei (2018). In particular, we study the performance of different initialisation schemes and emphasise what characteristics of each model are more suitable to data scarcity, relying on the distribution of detected drifts. Finally, we regularise the loss of these models to better adapt to scarce data.</abstract>
      <url hash="e7c565ec">R19-1092</url>
      <doi>10.26615/978-954-452-056-4_092</doi>
      <bibkey>montariol-allauzen-2019-empirical</bibkey>
    </paper>
    <paper id="93">
      <title>A Fast and Accurate Partially Deterministic Morphological Analysis</title>
      <author><first>Hajime</first><last>Morita</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>804–809</pages>
      <abstract>This paper proposes a partially deterministic morphological analysis method for improved processing speed. Maximum matching is a fast deterministic method for morphological analysis. However, the method tends to decrease performance due to lack of consideration of contextual information. In order to use maximum matching safely, we propose the use of Context Independent Strings (CISs), which are strings that do not have ambiguity in terms of morphological analysis. Our method first identifies CISs in a sentence using maximum matching without contextual information, then analyzes the unprocessed part of the sentence using a bi-gram-based morphological analysis model. We evaluate the method on a Japanese morphological analysis task. The experimental results show a 30% reduction of running time while maintaining improved accuracy.</abstract>
      <url hash="4c660793">R19-1093</url>
      <doi>10.26615/978-954-452-056-4_093</doi>
      <bibkey>morita-iwakura-2019-fast</bibkey>
    </paper>
    <paper id="94">
      <title>incom.py - A Toolbox for Calculating Linguistic Distances and Asymmetries between Related Languages</title>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>810–818</pages>
      <abstract>Languages may be differently distant from each other and their mutual intelligibility may be asymmetric. In this paper we introduce incom.py, a toolbox for calculating linguistic distances and asymmetries between related languages. incom.py allows linguist experts to quickly and easily perform statistical analyses and compare those with experimental results. We demonstrate the efficacy of incom.py in an incomprehension experiment on two Slavic languages: Bulgarian and Russian. Using incom.py we were able to validate three methods to measure linguistic distances and asymmetries: Levenshtein distance, word adaptation surprisal, and conditional entropy as predictors of success in a reading intercomprehension experiment.</abstract>
      <url hash="45a8521a">R19-1094</url>
      <doi>10.26615/978-954-452-056-4_094</doi>
      <bibkey>mosbach-etal-2019-incom</bibkey>
    </paper>
    <paper id="95">
      <title>A Holistic Natural Language Generation Framework for the Semantic Web</title>
      <author><first>Axel-Cyrille</first><last>Ngonga Ngomo</last></author>
      <author><first>Diego</first><last>Moussallem</last></author>
      <author><first>Lorenz</first><last>Bühmann</last></author>
      <pages>819–828</pages>
      <abstract>With the ever-growing generation of data for the Semantic Web comes an increasing demand for this data to be made available to non-semantic Web experts. One way of achieving this goal is to translate the languages of the Semantic Web into natural language. We present LD2NL, a framework that allows verbalizing the three key languages of the Semantic Web, i.e., RDF, OWL, and SPARQL. Our framework is based on a bottom-up approach to verbalization. We evaluated LD2NL in an open survey with 86 persons. Our results suggest that our framework can generate verbalizations that are close to natural languages and that can be easily understood by non-experts. Therewith, it enables non-domain experts to interpret Semantic Web data with more than 91% of the accuracy of domain experts.</abstract>
      <url hash="acb5bff8">R19-1095</url>
      <doi>10.26615/978-954-452-056-4_095</doi>
      <bibkey>ngonga-ngomo-etal-2019-holistic</bibkey>
    </paper>
    <paper id="96">
      <title>Building a Comprehensive <fixed-case>R</fixed-case>omanian Knowledge Base for Drug Administration</title>
      <author><first>Bogdan</first><last>Nicula</last></author>
      <author><first>Mihai</first><last>Dascalu</last></author>
      <author><first>Maria-Dorinela</first><last>Sîrbu</last></author>
      <author><first>Ștefan</first><last>Trăușan-Matu</last></author>
      <author><first>Alexandru</first><last>Nuță</last></author>
      <pages>829–836</pages>
      <abstract>Information on drug administration is obtained traditionally from doctors and pharmacists, as well as leaflets which provide in most cases cumbersome and hard-to-follow details. Thus, the need for medical knowledge bases emerges to provide access to concrete and well-structured information which can play an important role in informing patients. This paper introduces a Romanian medical knowledge base focused on drug-drug interactions, on representing relevant drug information, and on symptom-disease relations. The knowledge base was created by extracting and transforming information using Natural Language Processing techniques from both structured and unstructured sources, together with manual annotations. The resulting Romanian ontologies are aligned with larger English medical ontologies. Our knowledge base supports queries regarding drugs (e.g., active ingredients, concentration, expiration date), drug-drug interaction, symptom-disease relations, as well as drug-symptom relations.</abstract>
      <url hash="f25e0d58">R19-1096</url>
      <doi>10.26615/978-954-452-056-4_096</doi>
      <bibkey>nicula-etal-2019-building</bibkey>
    </paper>
    <paper id="97">
      <title>Summary Refinement through Denoising</title>
      <author><first>Nikola I.</first><last>Nikolov</last></author>
      <author><first>Alessandro</first><last>Calmanovici</last></author>
      <author><first>Richard</first><last>Hahnloser</last></author>
      <pages>837–843</pages>
      <abstract>We propose a simple method for post-processing the outputs of a text summarization system in order to refine its overall quality. Our approach is to train text-to-text rewriting models to correct information redundancy errors that may arise during summarization. We train on synthetically generated noisy summaries, testing three different types of noise that introduce out-of-context information within each summary. When applied on top of extractive and abstractive summarization baselines, our summary denoising models yield metric improvements while reducing redundancy.</abstract>
      <url hash="1bc32b6b">R19-1097</url>
      <doi>10.26615/978-954-452-056-4_097</doi>
      <bibkey>nikolov-etal-2019-summary</bibkey>
      <pwccode url="https://github.com/ninikolov/summary-denoising" additional="false">ninikolov/summary-denoising</pwccode>
    </paper>
    <paper id="98">
      <title>Large-Scale Hierarchical Alignment for Data-driven Text Rewriting</title>
      <author><first>Nikola I.</first><last>Nikolov</last></author>
      <author><first>Richard</first><last>Hahnloser</last></author>
      <pages>844–853</pages>
      <abstract>We propose a simple unsupervised method for extracting pseudo-parallel monolingual sentence pairs from comparable corpora representative of two different text styles, such as news articles and scientific papers. Our approach does not require a seed parallel corpus, but instead relies solely on hierarchical search over pre-trained embeddings of documents and sentences. We demonstrate the effectiveness of our method through automatic and extrinsic evaluation on text simplification from the normal to the Simple Wikipedia. We show that pseudo-parallel sentences extracted with our method not only supplement existing parallel data, but can even lead to competitive performance on their own.</abstract>
      <url hash="74dab92f">R19-1098</url>
      <doi>10.26615/978-954-452-056-4_098</doi>
      <bibkey>nikolov-hahnloser-2019-large</bibkey>
      <pwccode url="https://github.com/ninikolov/lha" additional="false">ninikolov/lha</pwccode>
    </paper>
    <paper id="99">
      <title>Dependency-Based Relative Positional Encoding for Transformer <fixed-case>NMT</fixed-case></title>
      <author><first>Yutaro</first><last>Omote</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>854–861</pages>
      <abstract>This paper proposes a new Transformer neural machine translation model that incorporates syntactic distances between two source words into the relative position representations of the self-attention mechanism. In particular, the proposed model encodes pair-wise relative depths on a source dependency tree, which are differences between the depths of the two source words, in the encoder’s self-attention. The experiments show that our proposed model achieves 0.5 point gain in BLEU on the Asian Scientific Paper Excerpt Corpus Japanese-to-English translation task.</abstract>
      <url hash="9365c240">R19-1099</url>
      <doi>10.26615/978-954-452-056-4_099</doi>
      <bibkey>omote-etal-2019-dependency</bibkey>
    </paper>
    <paper id="100">
      <title>From Image to Text in Sentiment Analysis via Regression and Deep Learning</title>
      <author><first>Daniela</first><last>Onita</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Adriana</first><last>Birlutiu</last></author>
      <pages>862–868</pages>
      <abstract>Images and text represent types of content which are used together for conveying user emotions in online social networks. These contents are usually associated with a sentiment category. In this paper, we investigate an approach for mapping images to text for three types of sentiment categories: positive, neutral and negative. The mapping from images to text is performed using a Kernel Ridge Regression model. We considered two types of image features: i) RGB pixel-values features, and ii) features extracted with a deep learning approach. The experimental evaluation was performed on a Twitter data set containing both text and images and the sentiment associated with these. The experimental results show a difference in performance for different sentiment categories, in particular the mapping that we propose performs better for the positive sentiment category in comparison with the neutral and negative ones. Furthermore, the experimental results show that the more complex deep learning features perform better than the RGB pixel-value features for all sentiment categories and for larger training sets.</abstract>
      <url hash="e8cefed3">R19-1100</url>
      <doi>10.26615/978-954-452-056-4_100</doi>
      <bibkey>onita-etal-2019-image</bibkey>
    </paper>
    <paper id="101">
      <title>Building a Morphological Analyser for <fixed-case>L</fixed-case>az</title>
      <author><first>Esra</first><last>Onal</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>869–877</pages>
      <abstract>This study is an attempt to contribute to documentation and revitalization efforts of endangered Laz language, a member of South Caucasian language family mainly spoken on northeastern coastline of Turkey. It constitutes the first steps to create a general computational model for word form recognition and production for Laz by building a rule-based morphological analyser using Helsinki Finite-State Toolkit (HFST). The evaluation results show that the analyser has a 64.9% coverage over a corpus collected for this study with 111,365 tokens. We have also performed an error analysis on randomly selected 100 tokens from the corpus which are not covered by the analyser, and these results show that the errors mostly result from Turkish words in the corpus and missing stems in our lexicon.</abstract>
      <url hash="b0663f1f">R19-1101</url>
      <doi>10.26615/978-954-452-056-4_101</doi>
      <bibkey>onal-tyers-2019-building</bibkey>
    </paper>
    <paper id="102">
      <title>Term Based Semantic Clusters for Very Short Text Classification</title>
      <author><first>Jasper</first><last>Paalman</last></author>
      <author><first>Shantanu</first><last>Mullick</last></author>
      <author><first>Kalliopi</first><last>Zervanou</last></author>
      <author><first>Yingqian</first><last>Zhang</last></author>
      <pages>878–887</pages>
      <abstract>Very short texts, such as tweets and invoices, present challenges in classification. Although term occurrences are strong indicators of content, in very short texts, the sparsity of these texts makes it difficult to capture important semantic relationships. A solution calls for a method that not only considers term occurrence, but also handles sparseness well. In this work, we introduce such an approach, the Term Based Semantic Clusters (TBSeC) that employs terms to create distinctive semantic concept clusters. These clusters are ranked using a semantic similarity function which in turn defines a semantic feature space that can be used for text classification. Our method is evaluated in an invoice classification task. Compared to well-known content representation methods the proposed method performs competitively.</abstract>
      <url hash="a8864031">R19-1102</url>
      <doi>10.26615/978-954-452-056-4_102</doi>
      <bibkey>paalman-etal-2019-term</bibkey>
    </paper>
    <paper id="103">
      <title>Quotation Detection and Classification with a Corpus-Agnostic Model</title>
      <author><first>Sean</first><last>Papay</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>888–894</pages>
      <abstract>The detection of quotations (i.e., reported speech, thought, and writing) has established itself as an NLP analysis task. However, state-of-the-art models have been developed on the basis of specific corpora and incorpo- rate a high degree of corpus-specific assumptions and knowledge, which leads to fragmentation. In the spirit of task-agnostic modeling, we present a corpus-agnostic neural model for quotation detection and evaluate it on three corpora that vary in language, text genre, and structural assumptions. The model (a) approaches the state-of-the-art on the corpora when using established feature sets and (b) shows reasonable performance even when us- ing solely word forms, which makes it applicable for non-standard (i.e., historical) corpora.</abstract>
      <url hash="208966a6">R19-1103</url>
      <doi>10.26615/978-954-452-056-4_103</doi>
      <bibkey>papay-pado-2019-quotation</bibkey>
    </paper>
    <paper id="104">
      <title>Validation of Facts Against Textual Sources</title>
      <author><first>Vamsi Krishna</first><last>Pendyala</last></author>
      <author><first>Simran</first><last>Sinha</last></author>
      <author><first>Satya</first><last>Prakash</last></author>
      <author><first>Shriya</first><last>Reddy</last></author>
      <author><first>Anupam</first><last>Jamatia</last></author>
      <pages>895–903</pages>
      <abstract>In today’s digital world of information, a fact verification system to disprove assertions made in speech, print media or online content is the need of the hour. We propose a system which would verify a claim against a source and classify the claim to be true, false, out-of-context or an inappropriate claim with respect to the textual source provided to the system. A true label is used if the claim is true, false if it is false, if the claim has no relation with the source then it is classified as out-of-context and if the claim cannot be verified at all then it is classified as inappropriate. This would help us to verify a claim or a fact as well as know about the source or our knowledge base against which we are trying to verify our facts. We used a two-step approach to achieve our goal. At first, we retrieved evidence related to the claims from the textual source using the Term Frequency-Inverse Document Frequency(TF-IDF) vectors. Later we classified the claim-evidence pairs as true, false, inappropriate and out of context using a modified version of textual entailment module. Textual entailment module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information using Bi-LSTM network to assess the veracity of the claim. The accuracy of the best performing system is 64.49%</abstract>
      <url hash="0245c7d7">R19-1104</url>
      <doi>10.26615/978-954-452-056-4_104</doi>
      <bibkey>pendyala-etal-2019-validation</bibkey>
    </paper>
    <paper id="105">
      <title>A Neural Network Component for Knowledge-Based Semantic Representations of Text</title>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Yudivian</first><last>Almeida-Cruz</last></author>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Andrés</first><last>Montoyo</last></author>
      <pages>904–911</pages>
      <abstract>This paper presents Semantic Neural Networks (SNNs), a knowledge-aware component based on deep learning. SNNs can be trained to encode explicit semantic knowledge from an arbitrary knowledge base, and can subsequently be combined with other deep learning architectures. At prediction time, SNNs provide a semantic encoding extracted from the input data, which can be exploited by other neural network components to build extended representation models that can face alternative problems. The SNN architecture is defined in terms of the concepts and relations present in a knowledge base. Based on this architecture, a training procedure is developed. Finally, an experimental setup is presented to illustrate the behaviour and performance of a SNN for a specific NLP problem, in this case, opinion mining for the classification of movie reviews.</abstract>
      <url hash="be4f8802">R19-1105</url>
      <doi>10.26615/978-954-452-056-4_105</doi>
      <bibkey>piad-morffis-etal-2019-neural</bibkey>
    </paper>
    <paper id="106">
      <title>Toponym Detection in the Bio-Medical Domain: A Hybrid Approach with Deep Learning</title>
      <author><first>Alistair</first><last>Plum</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <pages>912–921</pages>
      <abstract>This paper compares how different machine learning classifiers can be used together with simple string matching and named entity recognition to detect locations in texts. We compare five different state-of-the-art machine learning classifiers in order to predict whether a sentence contains a location or not. Following this classification task, we use a string matching algorithm with a gazetteer to identify the exact index of a toponym within the sentence. We evaluate different approaches in terms of machine learning classifiers, text pre-processing and location extraction on the SemEval-2019 Task 12 dataset, compiled for toponym resolution in the bio-medical domain. Finally, we compare the results with our system that was previously submitted to the SemEval-2019 task evaluation.</abstract>
      <url hash="5a059526">R19-1106</url>
      <doi>10.26615/978-954-452-056-4_106</doi>
      <bibkey>plum-etal-2019-toponym</bibkey>
    </paper>
    <paper id="107">
      <title>Combining <fixed-case>PBSMT</fixed-case> and <fixed-case>NMT</fixed-case> Back-translated Data for Efficient <fixed-case>NMT</fixed-case></title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Gideon</first><last>Maillette de Buy Wenniger</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>922–931</pages>
      <abstract>Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation, which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for back-translation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmented with back-translated data created by merging different MT approaches.</abstract>
      <url hash="e4c25fc2">R19-1107</url>
      <doi>10.26615/978-954-452-056-4_107</doi>
      <bibkey>poncelas-etal-2019-combining</bibkey>
    </paper>
    <paper id="108">
      <title>Unsupervised dialogue intent detection via hierarchical topic model</title>
      <author><first>Artem</first><last>Popov</last></author>
      <author><first>Victor</first><last>Bulatov</last></author>
      <author><first>Darya</first><last>Polyudova</last></author>
      <author><first>Eugenia</first><last>Veselova</last></author>
      <pages>932–938</pages>
      <abstract>One of the challenges during a task-oriented chatbot development is the scarce availability of the labeled training data. The best way of getting one is to ask the assessors to tag each dialogue according to its intent. Unfortunately, performing labeling without any provisional collection structure is difficult since the very notion of the intent is ill-defined. In this paper, we propose a hierarchical multimodal regularized topic model to obtain a first approximation of the intent set. Our rationale for hierarchical models usage is their ability to take into account several degrees of the dialogues relevancy. We attempt to build a model that can distinguish between subject-based (e.g. medicine and transport topics) and action-based (e.g. filing of an application and tracking application status) similarities. In order to achieve this, we divide set of all features into several groups according to part-of-speech analysis. Various feature groups are treated differently on different hierarchy levels.</abstract>
      <url hash="9022d139">R19-1108</url>
      <doi>10.26615/978-954-452-056-4_108</doi>
      <bibkey>popov-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="109">
      <title>Graph Embeddings for Frame Identification</title>
      <author><first>Alexander</first><last>Popov</last></author>
      <author><first>Jennifer</first><last>Sikos</last></author>
      <pages>939–948</pages>
      <abstract>Lexical resources such as WordNet (Miller, 1995) and FrameNet (Baker et al., 1998) are organized as graphs, where relationships between words are made explicit via the structure of the resource. This work explores how structural information from these lexical resources can lead to gains in a downstream task, namely frame identification. While much of the current work in frame identification uses various neural architectures to predict frames, those neural architectures only use representations of frames based on annotated corpus data. We demonstrate how incorporating knowledge directly from the FrameNet graph structure improves the performance of a neural network-based frame identification system. Specifically, we construct a bidirectional LSTM with a loss function that incorporates various graph- and corpus-based frame embeddings for learning and ultimately achieves strong performance gains with the graph-based embeddings over corpus-based embeddings alone.</abstract>
      <url hash="86d3c657">R19-1109</url>
      <doi>10.26615/978-954-452-056-4_109</doi>
      <bibkey>popov-sikos-2019-graph</bibkey>
    </paper>
    <paper id="110">
      <title>Know Your Graph. State-of-the-Art Knowledge-Based <fixed-case>WSD</fixed-case></title>
      <author><first>Alexander</first><last>Popov</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <pages>949–958</pages>
      <abstract>This paper introduces several improvements over the current state of the art in knowledge-based word sense disambiguation. Those innovations are the result of modifying and enriching a knowledge base created originally on the basis of WordNet. They reflect several separate but connected strategies: manipulating the shape and the content of the knowledge base, assigning weights over the relations in the knowledge base, and the addition of new relations to it. The main contribution of the paper is to demonstrate that the previously proposed knowledge bases organize linguistic and world knowledge suboptimally for the task of word sense disambiguation. In doing so, the paper also establishes a new state of the art for knowledge-based approaches. Its best models are competitive in the broader context of supervised systems as well.</abstract>
      <url hash="539a0715">R19-1110</url>
      <doi>10.26615/978-954-452-056-4_110</doi>
      <bibkey>popov-etal-2019-know</bibkey>
    </paper>
    <paper id="111">
      <title>Are ambiguous conjunctions problematic for machine translation?</title>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Sheila</first><last>Castilho</last></author>
      <pages>959–966</pages>
      <abstract>The translation of ambiguous words still poses challenges for machine translation. In this work, we carry out a systematic quantitative analysis regarding the ability of different machine translation systems to disambiguate the source language conjunctions “but” and “and”. We evaluate specialised test sets focused on the translation of these two conjunctions. The test sets contain source languages that do not distinguish different variants of the given conjunction, whereas the target languages do. In total, we evaluate the conjunction “but” on 20 translation outputs, and the conjunction “and” on 10. All machine translation systems almost perfectly recognise one variant of the target conjunction, especially for the source conjunction “but”. The other target variant, however, represents a challenge for machine translation systems, with accuracy varying from 50% to 95% for “but” and from 20% to 57% for “and”. The major error for all systems is replacing the correct target variant with the opposite one.</abstract>
      <url hash="441c540b">R19-1111</url>
      <doi>10.26615/978-954-452-056-4_111</doi>
      <bibkey>popovic-castilho-2019-ambiguous</bibkey>
    </paper>
    <paper id="112">
      <title><fixed-case>ULSA</fixed-case>na: Universal Language Semantic Analyzer</title>
      <author><first>Ondřej</first><last>Pražák</last></author>
      <author><first>Miloslav</first><last>Konopik</last></author>
      <pages>967–972</pages>
      <abstract>We present a live cross-lingual system capable of producing shallow semantic annotations of natural language sentences for 51 languages at this time. The domain of the input sentences is in principle unconstrained. The system uses single training data (in English) for all the languages. The resulting semantic annotations are therefore consistent across different languages. We use CoNLL Semantic Role Labeling training data and Universal dependencies as the basis for the system. The system is publicly available and supports processing data in batches; therefore, it can be easily used by the community for the following research tasks.</abstract>
      <url hash="52bca822">R19-1112</url>
      <doi>10.26615/978-954-452-056-4_112</doi>
      <bibkey>prazak-konopik-2019-ulsana</bibkey>
    </paper>
    <paper id="113">
      <title>Machine Learning Approach to Fact-Checking in <fixed-case>W</fixed-case>est <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Tomáš</first><last>Hercig</last></author>
      <author><first>Josef</first><last>Steinberger</last></author>
      <pages>973–979</pages>
      <abstract>Fake news detection and closely-related fact-checking have recently attracted a lot of attention. Automatization of these tasks has been already studied for English. For other languages, only a few studies can be found (e.g. (Baly et al., 2018)), and to the best of our knowledge, no research has been conducted for West Slavic languages. In this paper, we present datasets for Czech, Polish, and Slovak. We also ran initial experiments which set a baseline for further research into this area.</abstract>
      <url hash="16321080">R19-1113</url>
      <doi>10.26615/978-954-452-056-4_113</doi>
      <bibkey>priban-etal-2019-machine</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>NE</fixed-case>-Table: A Neural key-value table for Named Entities</title>
      <author><first>Janarthanan</first><last>Rajendran</last></author>
      <author><first>Jatin</first><last>Ganhotra</last></author>
      <author><first>Xiaoxiao</first><last>Guo</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Satinder</first><last>Singh</last></author>
      <author><first>Lazaros</first><last>Polymenakos</last></author>
      <pages>980–993</pages>
      <abstract>Many Natural Language Processing (NLP) tasks depend on using Named Entities (NEs) that are contained in texts and in external knowledge sources. While this is easy for humans, the present neural methods that rely on learned word embeddings may not perform well for these NLP tasks, especially in the presence of Out-Of-Vocabulary (OOV) or rare NEs. In this paper, we propose a solution for this problem, and present empirical evaluations on: a) a structured Question-Answering task, b) three related Goal-Oriented dialog tasks, and c) a Reading-Comprehension task, which show that the proposed method can be effective in dealing with both in-vocabulary and OOV NEs. We create extended versions of dialog bAbI tasks 1,2 and 4 and OOV versions of the CBT test set which are available at - https://github.com/IBM/ne-table-datasets/</abstract>
      <url hash="6442b059">R19-1114</url>
      <doi>10.26615/978-954-452-056-4_114</doi>
      <bibkey>rajendran-etal-2019-ne</bibkey>
      <pwccode url="https://github.com/IBM/ne-table-datasets" additional="false">IBM/ne-table-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
    </paper>
    <paper id="115">
      <title>Enhancing Unsupervised Sentence Similarity Methods with Deep Contextualised Word Representations</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>994–1003</pages>
      <abstract>Calculating Semantic Textual Similarity (STS) plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. All modern state of the art STS methods rely on word embeddings one way or another. The recently introduced contextualised word embeddings have proved more effective than standard word embeddings in many natural language processing tasks. This paper evaluates the impact of several contextualised word embeddings on unsupervised STS methods and compares it with the existing supervised/unsupervised STS methods for different datasets in different languages and different domains</abstract>
      <url hash="c073662c">R19-1115</url>
      <doi>10.26615/978-954-452-056-4_115</doi>
      <bibkey>ranasinghe-etal-2019-enhancing</bibkey>
    </paper>
    <paper id="116">
      <title>Semantic Textual Similarity with <fixed-case>S</fixed-case>iamese Neural Networks</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>1004–1011</pages>
      <abstract>Calculating the Semantic Textual Similarity (STS) is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. This paper evaluates Siamese recurrent architectures, a special type of neural networks, which are used here to measure STS. Several variants of the architecture are compared with existing methods</abstract>
      <url hash="40527a78">R19-1116</url>
      <doi>10.26615/978-954-452-056-4_116</doi>
      <bibkey>ranasinghe-etal-2019-semantic</bibkey>
    </paper>
    <paper id="117">
      <title>Analysing the Impact of Supervised Machine Learning on Automatic Term Extraction: <fixed-case>HAMLET</fixed-case> vs <fixed-case>T</fixed-case>ermo<fixed-case>S</fixed-case>tat</title>
      <author><first>Ayla</first><last>Rigouts Terryn</last></author>
      <author><first>Patrick</first><last>Drouin</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>1012–1021</pages>
      <abstract>Traditional approaches to automatic term extraction do not rely on machine learning (ML) and select the top n ranked candidate terms or candidate terms above a certain predefined cut-off point, based on a limited number of linguistic and statistical clues. However, supervised ML approaches are gaining interest. Relatively little is known about the impact of these supervised methodologies; evaluations are often limited to precision, and sometimes recall and f1-scores, without information about the nature of the extracted candidate terms. Therefore, the current paper presents a detailed and elaborate analysis and comparison of a traditional, state-of-the-art system (TermoStat) and a new, supervised ML approach (HAMLET), using the results obtained for the same, manually annotated, Dutch corpus about dressage.</abstract>
      <url hash="7a6c7251">R19-1117</url>
      <doi>10.26615/978-954-452-056-4_117</doi>
      <bibkey>rigouts-terryn-etal-2019-analysing</bibkey>
    </paper>
    <paper id="118">
      <title>Distant Supervision for Sentiment Attitude Extraction</title>
      <author><first>Nicolay</first><last>Rusnachenko</last></author>
      <author><first>Natalia</first><last>Loukachevitch</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>1022–1030</pages>
      <abstract>News articles often convey attitudes between the mentioned subjects, which is essential for understanding the described situation. In this paper, we describe a new approach to distant supervision for extracting sentiment attitudes between named entities mentioned in texts. Two factors (pair-based and frame-based) were used to automatically label an extensive news collection, dubbed as RuAttitudes. The latter became a basis for adaptation and training convolutional architectures, including piecewise max pooling and full use of information across different sentences. The results show that models, trained with RuAttitudes, outperform ones that were trained with only supervised learning approach and achieve 13.4% increase in F1-score on RuSentRel collection.</abstract>
      <url hash="f99ff711">R19-1118</url>
      <doi>10.26615/978-954-452-056-4_118</doi>
      <bibkey>rusnachenko-etal-2019-distant</bibkey>
    </paper>
    <paper id="119">
      <title>Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems</title>
      <author><first>Mansour</first><last>Saffar Mehrjardi</last></author>
      <author><first>Amine</first><last>Trabelsi</last></author>
      <author><first>Osmar R.</first><last>Zaiane</last></author>
      <pages>1031–1040</pages>
      <abstract>Self-attentional models are a new paradigm for sequence modelling tasks which differ from common sequence modelling methods, such as recurrence-based and convolution-based sequence learning, in the way that their architecture is only based on the attention mechanism. Self-attentional models have been used in the creation of the state-of-the-art models in many NLP task such as neural machine translation, but their usage has not been explored for the task of training end-to-end task-oriented dialogue generation systems yet. In this study, we apply these models on the DSTC2 dataset for training task-oriented chatbots. Our finding shows that self-attentional models can be exploited to create end-to-end task-oriented chatbots which not only achieve higher evaluation scores compared to recurrence-based models, but also do so more efficiently.</abstract>
      <url hash="fdfbdb79">R19-1119</url>
      <doi>10.26615/978-954-452-056-4_119</doi>
      <bibkey>saffar-mehrjardi-etal-2019-self</bibkey>
    </paper>
    <paper id="120">
      <title>Whom to Learn From? Graph- vs. Text-based Word Embeddings</title>
      <author><first>Małgorzata</first><last>Salawa</last></author>
      <author><first>António</first><last>Branco</last></author>
      <author><first>Ruben</first><last>Branco</last></author>
      <author><first>João</first><last>António Rodrigues</last></author>
      <author><first>Chakaveh</first><last>Saedi</last></author>
      <pages>1041–1051</pages>
      <abstract>Vectorial representations of meaning can be supported by empirical data from diverse sources and obtained with diverse embedding approaches. This paper aims at screening this experimental space and reports on an assessment of word embeddings supported (i) by data in raw texts vs. in lexical graphs, (ii) by lexical information encoded in association- vs. inference-based graphs, and obtained (iii) by edge reconstruction- vs. matrix factorisation vs. random walk-based graph embedding methods. The results observed with these experiments indicate that the best solutions with graph-based word embeddings are very competitive, consistently outperforming mainstream text-based ones.</abstract>
      <url hash="38f4e3f4">R19-1120</url>
      <doi>10.26615/978-954-452-056-4_120</doi>
      <bibkey>salawa-etal-2019-learn</bibkey>
    </paper>
    <paper id="121">
      <title>Persistence pays off: Paying Attention to What the <fixed-case>LSTM</fixed-case> Gating Mechanism Persists</title>
      <author><first>Giancarlo</first><last>Salton</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>1052–1059</pages>
      <abstract>Recurrent Neural Network Language Models composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results in Language Modeling. However, these models still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading. In this paper we demonstrate an effective mechanism for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information.</abstract>
      <url hash="44a99322">R19-1121</url>
      <doi>10.26615/978-954-452-056-4_121</doi>
      <bibkey>salton-kelleher-2019-persistence</bibkey>
    </paper>
    <paper id="122">
      <title>Development and Evaluation of Three Named Entity Recognition Systems for <fixed-case>S</fixed-case>erbian - The Case of Personal Names</title>
      <author><first>Branislava</first><last>Šandrih</last></author>
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Ranka</first><last>Stankovic</last></author>
      <pages>1060–1068</pages>
      <abstract>In this paper we present a rule- and lexicon-based system for the recognition of Named Entities (NE) in Serbian newspaper texts that was used to prepare a gold standard annotated with personal names. It was further used to prepare training sets for four different levels of annotation, which were further used to train two Named Entity Recognition (NER) systems: Stanford and spaCy. All obtained models, together with a rule- and lexicon-based system were evaluated on two sample texts: a part of the gold standard and an independent newspaper text of approximately the same size. The results show that rule- and lexicon-based system outperforms trained models in all four scenarios (measured by F1), while Stanford models has the highest precision. All systems obtain best results in recognizing full names, while the recognition of first names only is rather poor. The produced models are incorporated into a Web platform NER&amp;Beyond that provides various NE-related functions.</abstract>
      <url hash="87989f6a">R19-1122</url>
      <doi>10.26615/978-954-452-056-4_122</doi>
      <bibkey>sandrih-etal-2019-development</bibkey>
    </paper>
    <paper id="123">
      <title>Moral Stance Recognition and Polarity Classification from <fixed-case>T</fixed-case>witter and Elicited Text</title>
      <author><first>Wesley</first><last>Santos</last></author>
      <author><first>Ivandré</first><last>Paraboni</last></author>
      <pages>1069–1075</pages>
      <abstract>We introduce a labelled corpus of stances about moral issues for the Brazilian Portuguese language, and present reference results for both the stance recognition and polarity classification tasks. The corpus is built from Twitter and further expanded with data elicited through crowd sourcing and labelled by their own authors. Put together, the corpus and reference results are expected to be taken as a baseline for further studies in the field of stance recognition and polarity classification from text.</abstract>
      <url hash="7ed1ff6b">R19-1123</url>
      <doi>10.26615/978-954-452-056-4_123</doi>
      <bibkey>santos-paraboni-2019-moral</bibkey>
    </paper>
    <paper id="124">
      <title>The “Jump and Stay” Method to Discover Proper Verb Centered Constructions in Corpus Lattices</title>
      <author><first>Bálint</first><last>Sass</last></author>
      <pages>1076–1084</pages>
      <abstract>The research presented here is based on the theoretical model of corpus lattices. We implemented this as an effective data structure, and developed an algorithm based on this structure to discover essential verbal expressions from corpus data. The idea behind the algorithm is the “jump and stay” principle, which tells us that our target expressions will be found at such places in the lattice where the value of a suitable function (defined on the vertex set of the corpus lattice) significantly increases (jumps) and then remains the same (stays). We evaluated our method on Hungarian data. Evaluation shows that about 75% of the obtained expressions are correct, actual errors are rare. Thus, this paper is 1. a proof of concept concerning the corpus lattice model, opening the way to investigate this structure further through our implementation; and 2. a proof of concept of the “jump and stay” idea and the algorithm itself, opening the way to apply it further, e.g. for other languages.</abstract>
      <url hash="91a4ec22">R19-1124</url>
      <doi>10.26615/978-954-452-056-4_124</doi>
      <bibkey>sass-2019-jump</bibkey>
    </paper>
    <paper id="125">
      <title>Offence in Dialogues: A Corpus-Based Study</title>
      <author><first>Johannes</first><last>Schäfer</last></author>
      <author><first>Ben</first><last>Burtenshaw</last></author>
      <pages>1085–1093</pages>
      <abstract>In recent years an increasing number of analyses of offensive language has been published, however, dealing mainly with the automatic detection and classification of isolated instances. In this paper we aim to understand the impact of offensive messages in online conversations diachronically, and in particular the change in offensiveness of dialogue turns. In turn, we aim to measure the progression of offence level as well as its direction - For example, whether a conversation is escalating or declining in offence. We present our method of extracting linear dialogues from tree-structured conversations in social media data and make our code publicly available. Furthermore, we discuss methods to analyse this dataset through changes in discourse offensiveness. Our paper includes two main contributions; first, using a neural network to measure the level of offensiveness in conversations; and second, the analysis of conversations around offensive comments using decoupling functions.</abstract>
      <url hash="b9d26c8a">R19-1125</url>
      <doi>10.26615/978-954-452-056-4_125</doi>
      <bibkey>schafer-burtenshaw-2019-offence</bibkey>
    </paper>
    <paper id="126">
      <title><fixed-case>E</fixed-case>mo<fixed-case>T</fixed-case>ag – Towards an Emotion-Based Analysis of Emojis</title>
      <author><first>Abu Awal Md</first><last>Shoeb</last></author>
      <author><first>Shahab</first><last>Raji</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>1094–1103</pages>
      <abstract>Despite being a fairly recent phenomenon, emojis have quickly become ubiquitous. Besides their extensive use in social media, they are now also invoked in customer surveys and feedback forms. Hence, there is a need for techniques to understand their sentiment and emotion. In this work, we provide a method to quantify the emotional association of basic emotions such as anger, fear, joy, and sadness for a set of emojis. We collect and process a unique corpus of 20 million emoji-centric tweets, such that we can capture rich emoji semantics using a comparably small dataset. We evaluate the induced emotion profiles of emojis with regard to their ability to predict word affect intensities as well as sentiment scores.</abstract>
      <url hash="992a014e">R19-1126</url>
      <doi>10.26615/978-954-452-056-4_126</doi>
      <attachment type="presentation" hash="6c1399bd">R19-1126.Presentation.pdf</attachment>
      <bibkey>shoeb-etal-2019-emotag</bibkey>
    </paper>
    <paper id="127">
      <title>A Morpho-Syntactically Informed <fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model for Named Entity Recognition</title>
      <author><first>Lilia</first><last>Simeonova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1104–1113</pages>
      <abstract>We propose a morphologically informed model for named entity recognition, which is based on LSTM-CRF architecture and combines word embeddings, Bi-LSTM character embeddings, part-of-speech (POS) tags, and morphological information. While previous work has focused on learning from raw word input, using word and character embeddings only, we show that for morphologically rich languages, such as Bulgarian, access to POS information contributes more to the performance gains than the detailed morphological information. Thus, we show that named entity recognition needs only coarse-grained POS tags, but at the same time it can benefit from simultaneously using some POS information of different granularity. Our evaluation results over a standard dataset show sizeable improvements over the state-of-the-art for Bulgarian NER.</abstract>
      <url hash="74886e28">R19-1127</url>
      <doi>10.26615/978-954-452-056-4_127</doi>
      <bibkey>simeonova-etal-2019-morpho</bibkey>
    </paper>
    <paper id="128">
      <title>Named Entity Recognition in Information Security Domain for <fixed-case>R</fixed-case>ussian</title>
      <author><first>Anastasiia</first><last>Sirotina</last></author>
      <author><first>Natalia</first><last>Loukachevitch</last></author>
      <pages>1114–1120</pages>
      <abstract>In this paper we discuss the named entity recognition task for Russian texts related to cybersecurity. First of all, we describe the problems that arise in course of labeling unstructured texts from information security domain. We introduce guidelines for human annotators, according to which a corpus has been marked up. Then, a CRF-based system and different neural architectures have been implemented and applied to the corpus. The named entity recognition systems have been evaluated and compared to determine the most efficient one.</abstract>
      <url hash="64853053">R19-1128</url>
      <doi>10.26615/978-954-452-056-4_128</doi>
      <bibkey>sirotina-loukachevitch-2019-named</bibkey>
    </paper>
    <paper id="129">
      <title>Cross-Family Similarity Learning for Cognate Identification in Low-Resource Languages</title>
      <author><first>Eliel</first><last>Soisalon-Soininen</last></author>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <pages>1121–1130</pages>
      <abstract>We address the problem of cognate identification across vocabulary pairs of any set of languages. In particular, we focus on the case where the examined pair of languages are low-resource to the extent that no training data whatsoever in these languages, or even closely related ones, are available for the task. We investigate the extent to which training data from another, unrelated language family can be used instead. Our approach consists of learning a similarity metric from example cognates in Indo-European languages and applying it to low-resource Sami languages of the Uralic family. We apply two models following previous work: a Siamese convolutional neural network (S-CNN) and a support vector machine (SVM), and compare them with a Levenshtein-distance baseline. We test performance on three Sami languages and find that the S-CNN outperforms the other approaches, suggesting that it is better able to learn such general characteristics of cognateness that carry over across language families. We also experiment with fine-tuning the S-CNN model with data from within the language family in order to quantify how well this model can make use of a small amount of target-domain data to adapt.</abstract>
      <url hash="66454da6">R19-1129</url>
      <doi>10.26615/978-954-452-056-4_129</doi>
      <bibkey>soisalon-soininen-granroth-wilding-2019-cross</bibkey>
    </paper>
    <paper id="130">
      <title>Automatic Detection of Translation Direction</title>
      <author><first>Ilia</first><last>Sominsky</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <pages>1131–1140</pages>
      <abstract>Parallel corpora are crucial resources for NLP applications, most notably for machine translation. The direction of the (human) translation of parallel corpora has been shown to have significant implications for the quality of statistical machine translation systems that are trained with such corpora. We describe a method for determining the direction of the (manual) translation of parallel corpora at the sentence-pair level. Using several linguistically-motivated features, coupled with a neural network model, we obtain high accuracy on several language pairs. Furthermore, we demonstrate that the accuracy is correlated with the (typological) distance between the two languages.</abstract>
      <url hash="838ffdfd">R19-1130</url>
      <doi>10.26615/978-954-452-056-4_130</doi>
      <bibkey>sominsky-wintner-2019-automatic</bibkey>
    </paper>
    <paper id="131">
      <title>Automated Text Simplification as a Preprocessing Step for Machine Translation into an Under-resourced Language</title>
      <author><first>Sanja</first><last>Štajner</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <pages>1141–1150</pages>
      <abstract>In this work, we investigate the possibility of using fully automatic text simplification system on the English source in machine translation (MT) for improving its translation into an under-resourced language. We use the state-of-the-art automatic text simplification (ATS) system for lexically and syntactically simplifying source sentences, which are then translated with two state-of-the-art English-to-Serbian MT systems, the phrase-based MT (PBMT) and the neural MT (NMT). We explore three different scenarios for using the ATS in MT: (1) using the raw output of the ATS; (2) automatically filtering out the sentences with low grammaticality and meaning preservation scores; and (3) performing a minimal manual correction of the ATS output. Our results show improvement in fluency of the translation regardless of the chosen scenario, and difference in success of the three scenarios depending on the MT approach used (PBMT or NMT) with regards to improving translation fluency and post-editing effort.</abstract>
      <url hash="39f85c13">R19-1131</url>
      <doi>10.26615/978-954-452-056-4_131</doi>
      <bibkey>stajner-popovic-2019-automated</bibkey>
    </paper>
    <paper id="132">
      <title>Investigating Multilingual Abusive Language Detection: A Cautionary Tale</title>
      <author><first>Kenneth</first><last>Steimel</last></author>
      <author><first>Daniel</first><last>Dakota</last></author>
      <author><first>Yue</first><last>Chen</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>1151–1160</pages>
      <abstract>Abusive language detection has received much attention in the last years, and recent approaches perform the task in a number of different languages. We investigate which factors have an effect on multilingual settings, focusing on the compatibility of data and annotations. In the current paper, we focus on English and German. Our findings show large differences in performance between the two languages. We find that the best performance is achieved by different classification algorithms. Sampling to address class imbalance issues is detrimental for German and beneficial for English. The only similarity that we find is that neither data set shows clear topics when we compare the results of topic modeling to the gold standard. Based on our findings, we can conclude that a multilingual optimization of classifiers is not possible even in settings where comparable data sets are used.</abstract>
      <url hash="26796c6e">R19-1132</url>
      <doi>10.26615/978-954-452-056-4_132</doi>
      <bibkey>steimel-etal-2019-investigating</bibkey>
    </paper>
    <paper id="133">
      <title>Augmenting a <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Tagger with a Morphological Lexicon and a Lexical Category Identification Step</title>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <author><first>Örvar</first><last>Kárason</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>1161–1168</pages>
      <abstract>Previous work on using BiLSTM models for PoS tagging has primarily focused on small tagsets. We evaluate BiLSTM models for tagging Icelandic, a morphologically rich language, using a relatively large tagset. Our baseline BiLSTM model achieves higher accuracy than any other previously published tagger, when not taking advantage of a morphological lexicon. When we extend the model by incorporating such data, we outperform the earlier state-of-the-art results by a significant margin. We also report on work in progress that attempts to address the problem of data sparsity inherent to morphologically detailed, fine-grained tagsets. We experiment with training a separate model on only the lexical category and using the coarse-grained output tag as an input into to the main model. This method further increases the accuracy and reduces the tagging errors by 21.3% compared to previous state-of-the-art results. Finally, we train and test our tagger on a new gold standard for Icelandic.</abstract>
      <url hash="0514e139">R19-1133</url>
      <doi>10.26615/978-954-452-056-4_133</doi>
      <bibkey>steingrimsson-etal-2019-augmenting</bibkey>
      <pwccode url="https://github.com/steinst/ABLTagger" additional="false">steinst/ABLTagger</pwccode>
    </paper>
    <paper id="134">
      <title>Comparison of Machine Learning Approaches for Industry Classification Based on Textual Descriptions of Companies</title>
      <author><first>Andrey</first><last>Tagarev</last></author>
      <author><first>Nikola</first><last>Tulechki</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>1169–1175</pages>
      <abstract>This paper addresses the task of categorizing companies within industry classification schemes. The datasets consists of encyclopedic articles about companies and their economic activities. The target classification schema is build by mapping linked open data in a semi-supervised manner. Target classes are build bottom-up from DBpedia. We apply several state of the art text classification techniques, based both on deep-learning and classical vector-space models.</abstract>
      <url hash="0472315c">R19-1134</url>
      <doi>10.26615/978-954-452-056-4_134</doi>
      <bibkey>tagarev-etal-2019-comparison</bibkey>
    </paper>
    <paper id="135">
      <title>A Quantum-Like Approach to Word Sense Disambiguation</title>
      <author><first>Fabio</first><last>Tamburini</last></author>
      <pages>1176–1185</pages>
      <abstract>This paper presents a novel algorithm for Word Sense Disambiguation (WSD) based on Quantum Probability Theory. The Quantum WSD algorithm requires concepts representations as vectors in the complex domain and thus we have developed a technique for computing complex word and sentence embeddings based on the Paragraph Vectors algorithm. Despite the proposed method is quite simple and that it does not require long training phases, when it is evaluated on a standardized benchmark for this task it exhibits state-of-the-art (SOTA) performances.</abstract>
      <url hash="2a2f026d">R19-1135</url>
      <doi>10.26615/978-954-452-056-4_135</doi>
      <bibkey>tamburini-2019-quantum</bibkey>
    </paper>
    <paper id="136">
      <title>Understanding Neural Machine Translation by Simplification: The Case of Encoder-free Models</title>
      <author><first>Gongbo</first><last>Tang</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>1186–1193</pages>
      <abstract>In this paper, we try to understand neural machine translation (NMT) via simplifying NMT architectures and training encoder-free NMT models. In an encoder-free model, the sums of word embeddings and positional embeddings represent the source. The decoder is a standard Transformer or recurrent neural network that directly attends to embeddings via attention mechanisms. Experimental results show (1) that the attention mechanism in encoder-free models acts as a strong feature extractor, (2) that the word embeddings in encoder-free models are competitive to those in conventional models, (3) that non-contextualized source representations lead to a big performance drop, and (4) that encoder-free models have different effects on alignment quality for German-English and Chinese-English.</abstract>
      <url hash="c6424937">R19-1136</url>
      <doi>10.26615/978-954-452-056-4_136</doi>
      <bibkey>tang-etal-2019-understanding</bibkey>
    </paper>
    <paper id="137">
      <title>Text-Based Joint Prediction of Numeric and Categorical Attributes of Entities in Knowledge Bases</title>
      <author><first>V</first><last>Thejas</last></author>
      <author><first>Abhijeet</first><last>Gupta</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>1194–1202</pages>
      <abstract>Collaboratively constructed knowledge bases play an important role in information systems, but are essentially always incomplete. Thus, a large number of models has been developed for Knowledge Base Completion, the task of predicting new attributes of entities given partial descriptions of these entities. Virtually all of these models either concentrate on numeric attributes (&lt;Italy,GDP,2T$&gt;) or they concentrate on categorical attributes (&lt;Tim Cook,chairman,Apple&gt;). In this paper, we propose a simple feed-forward neural architecture to jointly predict numeric and categorical attributes based on embeddings learned from textual occurrences of the entities in question. Following insights from multi-task learning, our hypothesis is that due to the correlations among attributes of different kinds, joint prediction improves over separate prediction. Our experiments on seven FreeBase domains show that this hypothesis is true of the two attribute types: we find substantial improvements for numeric attributes in the joint model, while performance remains largely unchanged for categorical attributes. Our analysis indicates that this is the case because categorical attributes, many of which describe membership in various classes, provide useful ‘background knowledge’ for numeric prediction, while this is true to a lesser degree in the inverse direction.</abstract>
      <url hash="1e788182">R19-1137</url>
      <doi>10.26615/978-954-452-056-4_137</doi>
      <bibkey>thejas-etal-2019-text</bibkey>
    </paper>
    <paper id="138">
      <title><fixed-case>S</fixed-case>en<fixed-case>Z</fixed-case>i: A Sentiment Analysis Lexicon for the Latinised <fixed-case>A</fixed-case>rabic (<fixed-case>A</fixed-case>rabizi)</title>
      <author><first>Taha</first><last>Tobaili</last></author>
      <author><first>Miriam</first><last>Fernandez</last></author>
      <author><first>Harith</first><last>Alani</last></author>
      <author><first>Sanaa</first><last>Sharafeddine</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>1203–1211</pages>
      <abstract>Arabizi is an informal written form of dialectal Arabic transcribed in Latin alphanumeric characters. It has a proven popularity on chat platforms and social media, yet it suffers from a severe lack of natural language processing (NLP) resources. As such, texts written in Arabizi are often disregarded in sentiment analysis tasks for Arabic. In this paper we describe the creation of a sentiment lexicon for Arabizi that was enriched with word embeddings. The result is a new Arabizi lexicon consisting of 11.3K positive and 13.3K negative words. We evaluated this lexicon by classifying the sentiment of Arabizi tweets achieving an F1-score of 0.72. We provide a detailed error analysis to present the challenges that impact the sentiment analysis of Arabizi.</abstract>
      <url hash="0f6b016e">R19-1138</url>
      <doi>10.26615/978-954-452-056-4_138</doi>
      <bibkey>tobaili-etal-2019-senzi</bibkey>
    </paper>
    <paper id="139">
      <title>Mining the <fixed-case>UK</fixed-case> Web Archive for Semantic Change Detection</title>
      <author><first>Adam</first><last>Tsakalidis</last></author>
      <author><first>Marya</first><last>Bazzi</last></author>
      <author><first>Mihai</first><last>Cucuringu</last></author>
      <author><first>Pierpaolo</first><last>Basile</last></author>
      <author><first>Barbara</first><last>McGillivray</last></author>
      <pages>1212–1221</pages>
      <abstract>Semantic change detection (i.e., identifying words whose meaning has changed over time) started emerging as a growing area of research over the past decade, with important downstream applications in natural language processing, historical linguistics and computational social science. However, several obstacles make progress in the domain slow and difficult. These pertain primarily to the lack of well-established gold standard datasets, resources to study the problem at a fine-grained temporal resolution, and quantitative evaluation approaches. In this work, we aim to mitigate these issues by (a) releasing a new labelled dataset of more than 47K word vectors trained on the UK Web Archive over a short time-frame (2000-2013); (b) proposing a variant of Procrustes alignment to detect words that have undergone semantic shift; and (c) introducing a rank-based approach for evaluation purposes. Through extensive numerical experiments and validation, we illustrate the effectiveness of our approach against competitive baselines. Finally, we also make our resources publicly available to further enable research in the domain.</abstract>
      <url hash="40502b7d">R19-1139</url>
      <doi>10.26615/978-954-452-056-4_139</doi>
      <bibkey>tsakalidis-etal-2019-mining</bibkey>
    </paper>
    <paper id="140">
      <title>Cross-Lingual Word Embeddings for Morphologically Rich Languages</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>1222–1228</pages>
      <abstract>Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as Turkish and Finnish. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our model includes a simple extension which enables us to exploit morphemes for cross-lingual mapping. We applied our model for the Turkish-Finnish language pair on the bilingual word translation task. Results show that our model outperforms the baseline models by 2% in the nearest neighbour ranking.</abstract>
      <url hash="a6ad34bc">R19-1140</url>
      <doi>10.26615/978-954-452-056-4_140</doi>
      <bibkey>ustun-etal-2019-cross</bibkey>
    </paper>
    <paper id="141">
      <title>It Takes Nine to Smell a Rat: Neural Multi-Task Learning for Check-Worthiness Prediction</title>
      <author><first>Slavena</first><last>Vasileva</last></author>
      <author><first>Pepa</first><last>Atanasova</last></author>
      <author><first>Lluís</first><last>Màrquez</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1229–1239</pages>
      <abstract>We propose a multi-task deep-learning approach for estimating the check-worthiness of claims in political debates. Given a political debate, such as the 2016 US Presidential and Vice-Presidential ones, the task is to predict which statements in the debate should be prioritized for fact-checking. While different fact-checking organizations would naturally make different choices when analyzing the same debate, we show that it pays to learn from multiple sources simultaneously (PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and Washington Post) in a multi-task learning setup, even when a particular source is chosen as a target to imitate. Our evaluation shows state-of-the-art results on a standard dataset for the task of check-worthiness prediction.</abstract>
      <url hash="24bbadff">R19-1141</url>
      <doi>10.26615/978-954-452-056-4_141</doi>
      <bibkey>vasileva-etal-2019-takes</bibkey>
    </paper>
    <paper id="142">
      <title>Deep learning contextual models for prediction of sport event outcome from sportsman’s interviews</title>
      <author><first>Boris</first><last>Velichkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>1240–1246</pages>
      <abstract>This paper presents an approach for prediction of results for sport events. Usually the sport forecasting approaches are based on structured data. We test the hypothesis that the sports results can be predicted by using natural language processing and machine learning techniques applied over interviews with the players shortly before the sport events. The proposed method uses deep learning contextual models, applied over unstructured textual documents. Several experiments were performed for interviews with players in individual sports like boxing, martial arts, and tennis. The results from the conducted experiment confirmed our initial assumption that an interview from a sportsman before a match contains information that can be used for prediction the outcome from it. Furthermore, the results provide strong evidence in support of our research hypothesis, that is, we can predict the outcome from a sport match analyzing an interview, given before it.</abstract>
      <url hash="54dbd560">R19-1142</url>
      <doi>10.26615/978-954-452-056-4_142</doi>
      <bibkey>velichkov-etal-2019-deep</bibkey>
    </paper>
    <paper id="143">
      <title>Exploiting Frame-Semantics and Frame-Semantic Parsing for Automatic Extraction of Typological Information from Descriptive Grammars of Natural Languages</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Azam</first><last>Sheikh Muhammad</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Muhammad Irfan</first><last>Aslam</last></author>
      <author><first>Saania</first><last>Iqbal</last></author>
      <author><first>Nazia</first><last>Khurram</last></author>
      <pages>1247–1256</pages>
      <abstract>We describe a novel system for automatic extraction of typological linguistic information from descriptive grammars of natural languages, applying the theory of frame semantics in the form of frame-semantic parsing. The current proof-of-concept system covers a few selected linguistic features, but the methodology is general and can be extended not only to other typological features but also to descriptive grammars written in languages other than English. Such a system is expected to be a useful assistance for automatic curation of typological databases which otherwise are built manually, a very labor and time consuming as well as cognitively taxing enterprise.</abstract>
      <url hash="9fa91c1a">R19-1143</url>
      <doi>10.26615/978-954-452-056-4_143</doi>
      <bibkey>virk-etal-2019-exploiting</bibkey>
    </paper>
    <paper id="144">
      <title>Exploiting Open <fixed-case>IE</fixed-case> for Deriving Multiple Premises Entailment Corpus</title>
      <author><first>Martin</first><last>Víta</last></author>
      <author><first>Jakub</first><last>Klímek</last></author>
      <pages>1257–1264</pages>
      <abstract>Natural language inference (NLI) is a key part of natural language understanding. The NLI task is defined as a decision problem whether a given sentence – hypothesis – can be inferred from a given text. Typically, we deal with a text consisting of just a single premise/single sentence, which is called a single premise entailment (SPE) task. Recently, a derived task of NLI from multiple premises (MPE) was introduced together with the first annotated corpus and corresponding several strong baselines. Nevertheless, the further development in MPE field requires accessibility of huge amounts of annotated data. In this paper we introduce a novel method for rapid deriving of MPE corpora from an existing NLI (SPE) annotated data that does not require any additional annotation work. This proposed approach is based on using an open information extraction system. We demonstrate the application of the method on a well known SNLI corpus. Over the obtained corpus, we provide the first evaluations as well as we state a strong baseline.</abstract>
      <url hash="43509211">R19-1144</url>
      <doi>10.26615/978-954-452-056-4_144</doi>
      <bibkey>vita-klimek-2019-exploiting</bibkey>
    </paper>
    <paper id="145">
      <title>Towards Adaptive Text Summarization: How Does Compression Rate Affect Summary Readability of <fixed-case>L</fixed-case>2 Texts?</title>
      <author><first>Tatiana</first><last>Vodolazova</last></author>
      <author><first>Elena</first><last>Lloret</last></author>
      <pages>1265–1274</pages>
      <abstract>This paper addresses the problem of readability of automatically generated summaries in the context of second language learning. For this we experimented with a new corpus of level-annotated simplified English texts. The texts were summarized using a total of 7 extractive and abstractive summarization systems with compression rates of 20%, 40%, 60% and 80%. We analyzed the generated summaries in terms of lexical, syntactic and length-based features of readability, and concluded that summary complexity depends on the compression rate, summarization technique and the nature of the summarized corpus. Our experiments demonstrate the importance of choosing appropriate summarization techniques that align with user’s needs and language proficiency.</abstract>
      <url hash="9dd5d563">R19-1145</url>
      <doi>10.26615/978-954-452-056-4_145</doi>
      <bibkey>vodolazova-lloret-2019-towards</bibkey>
    </paper>
    <paper id="146">
      <title>The Impact of Rule-Based Text Generation on the Quality of Abstractive Summaries</title>
      <author><first>Tatiana</first><last>Vodolazova</last></author>
      <author><first>Elena</first><last>Lloret</last></author>
      <pages>1275–1284</pages>
      <abstract>In this paper we describe how an abstractive text summarization method improved the informativeness of automatic summaries by integrating syntactic text simplification, subject-verb-object concept frequency scoring and a set of rules that transform text into its semantic representation. We analyzed the impact of each component of our approach on the quality of generated summaries and tested it on DUC 2002 dataset. Our experiments showed that our approach outperformed other state-of-the-art abstractive methods while maintaining acceptable linguistic quality and redundancy rate.</abstract>
      <url hash="1d668807">R19-1146</url>
      <doi>10.26615/978-954-452-056-4_146</doi>
      <bibkey>vodolazova-lloret-2019-impact</bibkey>
    </paper>
    <paper id="147">
      <title><fixed-case>ETNLP</fixed-case>: A Visual-Aided Systematic Approach to Select Pre-Trained Embeddings for a Downstream Task</title>
      <author><first>Son</first><last>Vu Xuan</last></author>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Son</first><last>Tran</last></author>
      <author><first>Lili</first><last>Jiang</last></author>
      <pages>1285–1294</pages>
      <abstract>Given many recent advanced embedding models, selecting pre-trained word representation (i.e., word embedding) models best fit for a specific downstream NLP task is non-trivial. In this paper, we propose a systematic approach to extracting, evaluating, and visualizing multiple sets of pre-trained word embed- dings to determine which embeddings should be used in a downstream task. First, for extraction, we provide a method to extract a subset of the embeddings to be used in the downstream NLP tasks. Second, for evaluation, we analyse the quality of pre-trained embeddings using an input word analogy list. Finally, we visualize the embedding space to explore the embedded words interactively. We demonstrate the effectiveness of the proposed approach on our pre-trained word embedding models in Vietnamese to select which models are suitable for a named entity recogni- tion (NER) task. Specifically, we create a large Vietnamese word analogy list to evaluate and select the pre-trained embedding models for the task. We then utilize the selected embed- dings for the NER task and achieve the new state-of-the-art results on the task benchmark dataset. We also apply the approach to another downstream task of privacy-guaranteed embedding selection, and show that it helps users quickly select the most suitable embeddings. In addition, we create an open-source system using the proposed systematic approach to facilitate similar studies on other NLP tasks. The source code and data are available at https: //github.com/vietnlp/etnlp.</abstract>
      <url hash="9ed67059">R19-1147</url>
      <doi>10.26615/978-954-452-056-4_147</doi>
      <bibkey>vu-xuan-etal-2019-etnlp</bibkey>
      <pwccode url="https://github.com/vietnlp/etnlp" additional="true">vietnlp/etnlp</pwccode>
    </paper>
    <paper id="148">
      <title>Tagger for <fixed-case>P</fixed-case>olish Computer Mediated Communication Texts</title>
      <author><first>Wiktor</first><last>Walentynowicz</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <author><first>Marcin</first><last>Oleksy</last></author>
      <pages>1295–1303</pages>
      <abstract>In this paper we present a morpho-syntactic tagger dedicated to Computer-mediated Communication texts in Polish. Its construction is based on an expanded RNN-based neural network adapted to the work on noisy texts. Among several techniques, the tagger utilises fastText embedding vectors, sequential character embedding vectors, and Brown clustering for the coarse-grained representation of sentence structures. In addition a set of manually written rules was proposed for post-processing. The system was trained to disambiguate descriptions of words in relation to Parts of Speech tags together with the full morphological information in terms of values for the different grammatical categories. We present also evaluation of several model variants on the gold standard annotated CMC data, comparison to the state-of-the-art taggers for Polish and error analysis. The proposed tagger shows significantly better results in this domain and demonstrates the viability of adaptation.</abstract>
      <url hash="4da4688c">R19-1148</url>
      <doi>10.26615/978-954-452-056-4_148</doi>
      <bibkey>walentynowicz-etal-2019-tagger</bibkey>
    </paper>
    <paper id="149">
      <title>Evaluation of vector embedding models in clustering of text documents</title>
      <author><first>Tomasz</first><last>Walkowiak</last></author>
      <author><first>Mateusz</first><last>Gniewkowski</last></author>
      <pages>1304–1311</pages>
      <abstract>The paper presents an evaluation of word embedding models in clustering of texts in the Polish language. Authors verified six different embedding models, starting from widely used word2vec, across fastText with character n-grams embedding, to deep learning-based ELMo and BERT. Moreover, four standardisation methods, three distance measures and four clustering methods were evaluated. The analysis was performed on two corpora of texts in Polish classified into subjects. The Adjusted Mutual Information (AMI) metric was used to verify the quality of clustering results. The performed experiments show that Skipgram models with n-grams character embedding, built on KGR10 corpus and provided by Clarin-PL, outperforms other publicly available models for Polish. Moreover, presented results suggest that Yeo–Johnson transformation for document vectors standardisation and Agglomerative Clustering with a cosine distance should be used for grouping of text documents.</abstract>
      <url hash="d0c399d0">R19-1149</url>
      <doi>10.26615/978-954-452-056-4_149</doi>
      <bibkey>walkowiak-gniewkowski-2019-evaluation</bibkey>
    </paper>
    <paper id="150">
      <title>Bigger versus Similar: Selecting a Background Corpus for First Story Detection Based on Distributional Similarity</title>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Robert J.</first><last>Ross</last></author>
      <author><first>John D.</first><last>Kelleher</last></author>
      <pages>1312–1320</pages>
      <abstract>The current state of the art for First Story Detection (FSD) are nearest neighbour-based models with traditional term vector representations; however, one challenge faced by FSD models is that the document representation is usually defined by the vocabulary and term frequency from a background corpus. Consequently, the ideal background corpus should arguably be both large-scale to ensure adequate term coverage, and similar to the target domain in terms of the language distribution. However, given these two factors cannot always be mutually satisfied, in this paper we examine whether the distributional similarity of common terms is more important than the scale of common terms for FSD. As a basis for our analysis we propose a set of metrics to quantitatively measure the scale of common terms and the distributional similarity between corpora. Using these metrics we rank different background corpora relative to a target corpus. We also apply models based on different background corpora to the FSD task. Our results show that term distributional similarity is more predictive of good FSD performance than the scale of common terms; and, thus we demonstrate that a smaller recent domain-related corpus will be more suitable than a very large-scale general corpus for FSD.</abstract>
      <url hash="6dbbf3d3">R19-1150</url>
      <doi>10.26615/978-954-452-056-4_150</doi>
      <bibkey>wang-etal-2019-bigger</bibkey>
    </paper>
    <paper id="151">
      <title>Predicting Sentiment of <fixed-case>P</fixed-case>olish Language Short Texts</title>
      <author><first>Aleksander</first><last>Wawer</last></author>
      <author><first>Julita</first><last>Sobiczewska</last></author>
      <pages>1321–1327</pages>
      <abstract>The goal of this paper is to use all available Polish language data sets to seek the best possible performance in supervised sentiment analysis of short texts. We use text collections with labelled sentiment such as tweets, movie reviews and a sentiment treebank, in three comparison modes. In the first, we examine the performance of models trained and tested on the same text collection using standard cross-validation (in-domain). In the second we train models on all available data except the given test collection, which we use for testing (one vs rest cross-domain). In the third, we train a model on one data set and apply it to another one (one vs one cross-domain). We compare wide range of methods including machine learning on bag-of-words representation, bidirectional recurrent neural networks as well as the most recent pre-trained architectures ELMO and BERT. We formulate conclusions as to cross-domain and in-domain performance of each method. Unsurprisingly, BERT turned out to be a strong performer, especially in the cross-domain setting. What is surprising however, is solid performance of the relatively simple multinomial Naive Bayes classifier, which performed equally well as BERT on several data sets.</abstract>
      <url hash="809e9378">R19-1151</url>
      <doi>10.26615/978-954-452-056-4_151</doi>
      <bibkey>wawer-sobiczewska-2019-predicting</bibkey>
    </paper>
    <paper id="152">
      <title>Improving Named Entity Linking Corpora Quality</title>
      <author><first>Albert</first><last>Weichselbraun</last></author>
      <author><first>Adrian M.P.</first><last>Brasoveanu</last></author>
      <author><first>Philipp</first><last>Kuntschik</last></author>
      <author><first>Lyndon J.B.</first><last>Nixon</last></author>
      <pages>1328–1337</pages>
      <abstract>Gold standard corpora and competitive evaluations play a key role in benchmarking named entity linking (NEL) performance and driving the development of more sophisticated NEL systems. The quality of the used corpora and the used evaluation metrics are crucial in this process. We, therefore, assess the quality of three popular evaluation corpora, identifying four major issues which affect these gold standards: (i) the use of different annotation styles, (ii) incorrect and missing annotations, (iii) Knowledge Base evolution, (iv) and differences in annotating co-occurrences. This paper addresses these issues by formalizing NEL annotations and corpus versioning which allows standardizing corpus creation, supports corpus evolution, and paves the way for the use of lenses to automatically transform between different corpus configurations. In addition, the use of clearly defined scoring rules and evaluation metrics ensures a better comparability of evaluation results.</abstract>
      <url hash="85cd3078">R19-1152</url>
      <doi>10.26615/978-954-452-056-4_152</doi>
      <bibkey>weichselbraun-etal-2019-improving</bibkey>
    </paper>
    <paper id="153">
      <title>Sequential Graph Dependency Parser</title>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>1338–1345</pages>
      <abstract>We propose a method for non-projective dependency parsing by incrementally predicting a set of edges. Since the edges do not have a pre-specified order, we propose a set-based learning method. Our method blends graph, transition, and easy-first parsing, including a prior state of the parser as a special case. The proposed transition-based method successfully parses near the state of the art on both projective and non-projective languages, without assuming a certain parsing order.</abstract>
      <url hash="1fd59ed6">R19-1153</url>
      <doi>10.26615/978-954-452-056-4_153</doi>
      <bibkey>welleck-cho-2019-sequential</bibkey>
    </paper>
    <paper id="154">
      <title>Term-Based Extraction of Medical Information: Pre-Operative Patient Education Use Case</title>
      <author><first>Martin</first><last>Wolf</last></author>
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>1346–1355</pages>
      <abstract>The processing of medical information is not a trivial task for medical non-experts. The paper presents an artificial assistant designed to facilitate a reliable access to medical online contents. Interactions are modelled as doctor-patient Question Answering sessions within a pre-operative patient education scenario where the system addresses patient’s information needs explaining medical events and procedures. This implies an accurate medical information extraction from and reasoning with available medical knowledge and large amounts of unstructured multilingual online data. Bridging the gap between medical knowledge and data, we explore a language-agnostic approach to medical concepts mining from the standard terminologies, and the data-driven collection of the corresponding seed terms in a distant supervision setting for German. Experimenting with different terminologies, features and term matching strategies, we achieved a promising F-score of 0.91 on the medical term extraction task. The concepts and terms are used to search and retrieve definitions from the verified online free resources. The proof-of-concept definition retrieval system is designed and evaluated showing promising results, acceptable by humans in 92% of cases.</abstract>
      <url hash="1a9281e7">R19-1154</url>
      <doi>10.26615/978-954-452-056-4_154</doi>
      <bibkey>wolf-etal-2019-term</bibkey>
    </paper>
    <paper id="155">
      <title>A Survey of the Perceived Text Adaptation Needs of Adults with Autism</title>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Natalia</first><last>Ponomareva</last></author>
      <pages>1356–1363</pages>
      <abstract>NLP approaches to automatic text adaptation often rely on user-need guidelines which are generic and do not account for the differences between various types of target groups. One such group are adults with high-functioning autism, who are usually able to read long sentences and comprehend difficult words but whose comprehension may be impeded by other linguistic constructions. This is especially challenging for real-world user-generated texts such as product reviews, which cannot be controlled editorially and are thus a particularly good applcation for automatic text adaptation systems. In this paper we present a mixed-methods survey conducted with 24 adult web-users diagnosed with autism and an age-matched control group of 33 neurotypical participants. The aim of the survey was to identify whether the group with autism experienced any barriers when reading online reviews, what these potential barriers were, and what NLP methods would be best suited to improve the accessibility of online reviews for people with autism. The group with autism consistently reported significantly greater difficulties with understanding online product reviews compared to the control group and identified issues related to text length, poor topic organisation, and the use of irony and sarcasm.</abstract>
      <url hash="9a212fba">R19-1155</url>
      <doi>10.26615/978-954-452-056-4_155</doi>
      <bibkey>yaneva-etal-2019-survey</bibkey>
    </paper>
    <paper id="156">
      <title>An Open, Extendible, and Fast <fixed-case>T</fixed-case>urkish Morphological Analyzer</title>
      <author><first>Olcay Taner</first><last>Yıldız</last></author>
      <author><first>Begüm</first><last>Avar</last></author>
      <author><first>Gökhan</first><last>Ercan</last></author>
      <pages>1364–1372</pages>
      <abstract>In this paper, we present a two-level morphological analyzer for Turkish. The morphological analyzer consists of five main components: finite state transducer, rule engine for suffixation, lexicon, trie data structure, and LRU cache. We use Java language to implement finite state machine logic and rule engine, Xml language to describe the finite state transducer rules of the Turkish language, which makes the morphological analyzer both easily extendible and easily applicable to other languages. Empowered with the comprehensiveness of a lexicon of 54,000 bare-forms including 19,000 proper nouns, our morphological analyzer presents one of the most reliable analyzers produced so far. The analyzer is compared with Turkish morphological analyzers in the literature. By using LRU cache and a trie data structure, the system can analyze 100,000 words per second, which enables users to analyze huge corpora in a few hours.</abstract>
      <url hash="b85c84ca">R19-1156</url>
      <doi>10.26615/978-954-452-056-4_156</doi>
      <bibkey>yildiz-etal-2019-open</bibkey>
    </paper>
    <paper id="157">
      <title>Self-Attention Networks for Intent Detection</title>
      <author><first>Sevinj</first><last>Yolchuyeva</last></author>
      <author><first>Géza</first><last>Németh</last></author>
      <author><first>Bálint</first><last>Gyires-Tóth</last></author>
      <pages>1373–1379</pages>
      <abstract>Self-attention networks (SAN) have shown promising performance in various Natural Language Processing (NLP) scenarios, especially in machine translation. One of the main points of SANs is the strength of capturing long-range and multi-scale dependencies from the data. In this paper, we present a novel intent detection system which is based on a self-attention network and a Bi-LSTM. Our approach shows improvement by using a transformer model and deep averaging network-based universal sentence encoder compared to previous solutions. We evaluate the system on Snips, Smart Speaker, Smart Lights, and ATIS datasets by different evaluation metrics. The performance of the proposed model is compared with LSTM with the same datasets.</abstract>
      <url hash="1d371dc3">R19-1157</url>
      <doi>10.26615/978-954-452-056-4_157</doi>
      <bibkey>yolchuyeva-etal-2019-self</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="158">
      <title><fixed-case>T</fixed-case>urkish Tweet Classification with Transformer Encoder</title>
      <author><first>Atıf Emre</first><last>Yüksel</last></author>
      <author><first>Yaşar Alim</first><last>Türkmen</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <author><first>Berna</first><last>Altınel</last></author>
      <pages>1380–1387</pages>
      <abstract>Short-text classification is a challenging task, due to the sparsity and high dimensionality of the feature space. In this study, we aim to analyze and classify Turkish tweets based on their topics. Social media jargon and the agglutinative structure of the Turkish language makes this classification task even harder. As far as we know, this is the first study that uses a Transformer Encoder for short text classification in Turkish. The model is trained in a weakly supervised manner, where the training data set has been labeled automatically. Our results on the test set, which has been manually labeled, show that performing morphological analysis improves the classification performance of the traditional machine learning algorithms Random Forest, Naive Bayes, and Support Vector Machines. Still, the proposed approach achieves an F-score of 89.3 % outperforming those algorithms by at least 5 points.</abstract>
      <url hash="1a9a7840">R19-1158</url>
      <doi>10.26615/978-954-452-056-4_158</doi>
      <bibkey>yuksel-etal-2019-turkish</bibkey>
    </paper>
    <paper id="159">
      <title>Multilingual Dynamic Topic Model</title>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <pages>1388–1396</pages>
      <abstract>Dynamic topic models (DTMs) capture the evolution of topics and trends in time series data.Current DTMs are applicable only to monolingual datasets. In this paper we present the multilingual dynamic topic model (ML-DTM), a novel topic model that combines DTM with an existing multilingual topic modeling method to capture cross-lingual topics that evolve across time. We present results of this model on a parallel German-English corpus of news articles and a comparable corpus of Finnish and Swedish news articles. We demonstrate the capability of ML-DTM to track significant events related to a topic and show that it finds distinct topics and performs as well as existing multilingual topic models in aligning cross-lingual topics.</abstract>
      <url hash="cc8f6d5d">R19-1159</url>
      <doi>10.26615/978-954-452-056-4_159</doi>
      <bibkey>zosa-granroth-wilding-2019-multilingual</bibkey>
    </paper>
    <paper id="160">
      <title>A Wide-Coverage Context-Free Grammar for <fixed-case>I</fixed-case>celandic and an Accompanying Parsing System</title>
      <author><first>Vilhjálmur</first><last>Þorsteinsson</last></author>
      <author><first>Hulda</first><last>Óladóttir</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>1397–1404</pages>
      <abstract>We present an open-source, wide-coverage context-free grammar (CFG) for Icelandic, and an accompanying parsing system. The grammar has over 5,600 nonterminals, 4,600 terminals and 19,000 productions in fully expanded form, with feature agreement constraints for case, gender, number and person. The parsing system consists of an enhanced Earley-based parser and a mechanism to select best-scoring parse trees from shared packed parse forests. Our parsing system is able to parse about 90% of all sentences in articles published on the main Icelandic news websites. Preliminary evaluation with evalb shows an F-measure of 70.72% on parsed sentences. Our system demonstrates that parsing a morphologically rich language using a wide-coverage CFG can be practical.</abstract>
      <url hash="69ae74cc">R19-1160</url>
      <doi>10.26615/978-954-452-056-4_160</doi>
      <bibkey>thorsteinsson-etal-2019-wide</bibkey>
    </paper>
  </volume>
  <volume id="2" ingest-date="2020-01-16">
    <meta>
      <booktitle>Proceedings of the Student Research Workshop Associated with RANLP 2019</booktitle>
      <url hash="c2fdf0f9">R19-2</url>
      <editor><first>Venelin</first><last>Kovatchev</last></editor>
      <editor><first>Irina</first><last>Temnikova</last></editor>
      <editor><first>Branislava</first><last>Šandrih</last></editor>
      <editor><first>Ivelina</first><last>Nikolova</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="dbf59d4d">R19-2000</url>
      <bibkey>ranlp-2019-student</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Normalization of <fixed-case>K</fixed-case>azakh Texts</title>
      <author><first>Assina</first><last>Abdussaitova</last></author>
      <author><first>Alina</first><last>Amangeldiyeva</last></author>
      <pages>1–6</pages>
      <abstract>Kazakh language, like other agglutinative languages, has specific difficulties on both recognition of wrong words and generation the corrections for misspelt words. The main goal of this work is to develop a better algorithm for the normalization of Kazakh texts based on traditional and Machine Learning methods, as well as the new approach which is also considered in this paper. The procedure of election among methods of normalization has been conducted in a manner of comparative analysis. The results of the comparative analysis turned up successful and are shown in details.</abstract>
      <url hash="2565b0d1">R19-2001</url>
      <doi>10.26615/issn.2603-2821.2019_001</doi>
      <bibkey>abdussaitova-amangeldiyeva-2019-normalization</bibkey>
    </paper>
    <paper id="2">
      <title>Classification Approaches to Identify Informative Tweets</title>
      <author><first>Piush</first><last>Aggarwal</last></author>
      <pages>7–15</pages>
      <abstract>Social media platforms have become prime forums for reporting news, with users sharing what they saw, heard or read on social media. News from social media is potentially useful for various stakeholders including aid organizations, news agencies, and individuals. However, social media also contains a vast amount of non-news content. For users to be able to draw on benefits from news reported on social media it is necessary to reliably identify news content and differentiate it from non-news. In this paper, we tackle the challenge of classifying a social post as news or not. To this end, we provide a new manually annotated dataset containing 2,992 tweets from 5 different topical categories. Unlike earlier datasets, it includes postings posted by personal users who do not promote a business or a product and are not affiliated with any organization. We also investigate various baseline systems and evaluate their performance on the newly generated dataset. Our results show that the best classifiers are the SVM and BERT models.</abstract>
      <url hash="a93d448e">R19-2002</url>
      <doi>10.26615/issn.2603-2821.2019_002</doi>
      <bibkey>aggarwal-2019-classification</bibkey>
    </paper>
    <paper id="3">
      <title>Dialect-Specific Models for Automatic Speech Recognition of <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican <fixed-case>V</fixed-case>ernacular <fixed-case>E</fixed-case>nglish</title>
      <author><first>Rachel</first><last>Dorn</last></author>
      <pages>16–20</pages>
      <abstract>African American Vernacular English (AAVE) is a widely-spoken dialect of English, yet it is under-represented in major speech corpora. As a result, speakers of this dialect are often misunderstood by NLP applications. This study explores the effect on transcription accuracy of an automatic voice recognition system when AAVE data is used. Models trained on AAVE data and on Standard American English data were compared to a baseline model trained on a combination of the two dialects. The accuracy for both dialect-specific models was significantly higher than the baseline model, with the AAVE model showing over 18% improvement. By isolating the effect of having AAVE speakers in the training data, this study highlights the importance of increasing diversity in the field of natural language processing.</abstract>
      <url hash="ed2b49cc">R19-2003</url>
      <doi>10.26615/issn.2603-2821.2019_003</doi>
      <bibkey>dorn-2019-dialect</bibkey>
    </paper>
    <paper id="4">
      <title>Multilingual Language Models for Named Entity Recognition in <fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Antonia</first><last>Baumann</last></author>
      <pages>21–27</pages>
      <abstract>We assess the language specificity of recent language models by exploring the potential of a multilingual language model. In particular, we evaluate Google’s multilingual BERT (mBERT) model on Named Entity Recognition (NER) in German and English. We expand the work on language model fine-tuning by Howard and Ruder (2018), applying it to the BERT architecture. We successfully reproduce the NER results published by Devlin et al. (2019).Our results show that the multilingual language model generalises well for NER in the chosen languages, matching the native model in English and comparing well with recent approaches for German. However, it does not benefit from the added fine-tuning methods.</abstract>
      <url hash="2c98c8ea">R19-2004</url>
      <doi>10.26615/issn.2603-2821.2019_004</doi>
      <bibkey>baumann-2019-multilingual</bibkey>
    </paper>
    <paper id="5">
      <title>Parts of Speech Tagging for <fixed-case>K</fixed-case>annada</title>
      <author><first>Swaroop</first><last>L R</last></author>
      <author><first>Rakshith</first><last>Gowda G S</last></author>
      <author><first>Sourabh</first><last>U</last></author>
      <author><first>Shriram</first><last>Hegde</last></author>
      <pages>28–31</pages>
      <abstract>Parts of speech (POS) tagging is the process of assigning the part of speech tag to each and every word in a sentence. In this paper, we have presented POS tagger for Kannada, a low resource south Asian language, using Condition Random Fields. POS tagger developed in the work uses novel features native to Kannada language. The novel features include Sandhi splitting, where a compound word is broken down into two or more meaningful constituent words. The proposed model is trained and tested on the tagged dataset which contains 21 thousand sentences and achieves a highest accuracy of 94.56%.</abstract>
      <url hash="5457faeb">R19-2005</url>
      <doi>10.26615/issn.2603-2821.2019_005</doi>
      <bibkey>l-r-etal-2019-parts</bibkey>
    </paper>
    <paper id="6">
      <title>Cross-Lingual Coreference: The Case of <fixed-case>B</fixed-case>ulgarian and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Zara</first><last>Kancheva</last></author>
      <pages>32–38</pages>
      <abstract>The paper presents several common approaches towards cross- and multi-lingual coreference resolution in a search of the most effective practices to be applied within the work on Bulgarian-English manual coreference annotation of a short story. The work aims at outlining the typology of the differences in the annotated parallel texts. The results of the research prove to be comparable with the tendencies observed in similar works on other Slavic languages and show surprising differences between the types of markables and their frequency in Bulgarian and English.</abstract>
      <url hash="c2dcc62d">R19-2006</url>
      <doi>10.26615/issn.2603-2821.2019_006</doi>
      <bibkey>kancheva-2019-cross</bibkey>
    </paper>
    <paper id="7">
      <title>Towards Accurate Text Verbalization for <fixed-case>ASR</fixed-case> Based on Audio Alignment</title>
      <author><first>Diana</first><last>Geneva</last></author>
      <author><first>Georgi</first><last>Shopov</last></author>
      <pages>39–47</pages>
      <abstract>Verbalization of non-lexical linguistic units plays an important role in language modeling for automatic speech recognition systems. Most verbalization methods require valuable resources such as ground truth, large training corpus and expert knowledge which are often unavailable. On the other hand a considerable amount of audio data along with its transcribed text are freely available on the Internet and could be utilized for the task of verbalization. This paper presents a methodology for accurate verbalization of audio transcriptions based on phone-level alignment between the transcriptions and their corresponding audio recordings. Comparing this approach to a more general rule-based verbalization method shows a significant improvement in ASR recognition of non-lexical units. In the process of evaluating this approach we also expose the indirect influence of verbalization accuracy on the quality of acoustic models trained on automatically derived speech corpora.</abstract>
      <url hash="5977691d">R19-2007</url>
      <doi>10.26615/issn.2603-2821.2019_007</doi>
      <bibkey>geneva-shopov-2019-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Evaluation of Stacked Embeddings for <fixed-case>B</fixed-case>ulgarian on the Downstream Tasks <fixed-case>POS</fixed-case> and <fixed-case>NERC</fixed-case></title>
      <author><first>Iva</first><last>Marinova</last></author>
      <pages>48–54</pages>
      <abstract>This paper reports on experiments with different stacks of word embeddings and evaluation of their usefulness for Bulgarian downstream tasks such as Named Entity Recognition and Classification (NERC) and Part-of-speech (POS) Tagging. Word embeddings stay in the core of the development of NLP, with several key language models being created over the last two years like FastText (CITATION), ElMo (CITATION), BERT (CITATION) and Flair (CITATION). Stacking or combining different word embeddings is another technique used in this paper and still not reported for Bulgarian NERC. Well-established architecture is used for the sequence tagging task such as BI-LSTM-CRF, and different pre-trained language models are combined in the embedding layer to decide which combination of them scores better.</abstract>
      <url hash="9c3bb300">R19-2008</url>
      <doi>10.26615/issn.2603-2821.2019_008</doi>
      <bibkey>marinova-2019-evaluation</bibkey>
    </paper>
    <paper id="9">
      <title>Overview on <fixed-case>NLP</fixed-case> Techniques for Content-based Recommender Systems for Books</title>
      <author><first>Melania</first><last>Berbatova</last></author>
      <pages>55–61</pages>
      <abstract>Recommender systems are an essential part of today’s largest websites. Without them, it would be hard for users to find the right products and content. One of the most popular methods for recommendations is content-based filtering. It relies on analysing product metadata, a great part of which is textual data. Despite their frequent use, there is still no standard procedure for developing and evaluating content-based recommenders. In this paper, we will first examine current approaches for designing, training and evaluating recommender systems based on textual data for books recommendations for GoodReads’ website. We will give critiques on existing methods and suggest how natural language techniques can be employed for the improvement of content-based recommenders.</abstract>
      <url hash="515d0024">R19-2009</url>
      <doi>10.26615/issn.2603-2821.2019_009</doi>
      <bibkey>berbatova-2019-overview</bibkey>
    </paper>
    <paper id="10">
      <title>Corpora and Processing Tools for Non-standard Contemporary and Diachronic <fixed-case>B</fixed-case>alkan <fixed-case>S</fixed-case>lavic</title>
      <author><first>Teodora</first><last>Vukovic</last></author>
      <author><first>Nora</first><last>Muheim</last></author>
      <author><first>Olivier</first><last>Winistörfer</last></author>
      <author><first>Ivan</first><last>Šimko</last></author>
      <author><first>Anastasia</first><last>Makarova</last></author>
      <author><first>Sanja</first><last>Bradjan</last></author>
      <pages>62–68</pages>
      <abstract>The paper describes three corpora of different varieties of BS that are currently being developed with the goal of providing data for the analysis of the diatopic and diachronic variation in non-standard Balkan Slavic. The corpora includes spoken materials from Torlak, Macedonian dialects, as well as the manuscripts of pre-standardized Bulgarian. Apart from the texts, tools for PoS annotation and lemmatization for all varieties are being created, as well as syntactic parsing for Torlak and Bulgarian varieties. The corpora are built using a unified methodology, relying on the pest practices and state-of-the-art methods from the field. The uniform methodology allows the contrastive analysis of the data from different varieties. The corpora under construction can be considered a crucial contribution to the linguistic research on the languages in the Balkans as they provide the lacking data needed for the studies of linguistic variation in the Balkan Slavic, and enable the comparison of the said varieties with other neighbouring languages.</abstract>
      <url hash="3e90840e">R19-2010</url>
      <doi>10.26615/issn.2603-2821.2019_010</doi>
      <bibkey>vukovic-etal-2019-corpora</bibkey>
    </paper>
    <paper id="11">
      <title>Question Answering Systems Approaches and Challenges</title>
      <author><first>Reem</first><last>Alqifari</last></author>
      <pages>69–75</pages>
      <abstract>Question answering (QA) systems permit the user to ask a question using natural language, and the system provides a concise and correct answer. QA systems can be implemented for different types of datasets, structured or unstructured. In this paper, some of the recent studies will be reviewed and the limitations will be discussed. Consequently, the current issues are analyzed with the proposed solutions.</abstract>
      <url hash="99d15fea">R19-2011</url>
      <doi>10.26615/issn.2603-2821.2019_011</doi>
      <bibkey>alqifari-2019-question</bibkey>
    </paper>
    <paper id="12">
      <title>Adding Linguistic Knowledge to <fixed-case>NLP</fixed-case> Tasks for <fixed-case>B</fixed-case>ulgarian: The Verb Paradigm Patterns</title>
      <author><first>Ivaylo</first><last>Radev</last></author>
      <pages>76–82</pages>
      <abstract>This paper discusses some possible usages of one unexplored lexical language resource containing Bulgarian verb paradigms and their English translations. This type of data can be used for machine translation, generation of pseudo corpora/language exercises, and evaluation of parsers. Upon completion, the resource will be linked with other existing resources such as the morphological lexicon, valency lexicon, as well as BTB-WordNet.</abstract>
      <url hash="3a70b4ec">R19-2012</url>
      <doi>10.26615/issn.2603-2821.2019_012</doi>
      <bibkey>radev-2019-adding</bibkey>
    </paper>
    <paper id="13">
      <title>Multilingual Complex Word Identification: Convolutional Neural Networks with Morphological and Linguistic Features</title>
      <author><first>Kim Cheng</first><last>Sheang</last></author>
      <pages>83–89</pages>
      <abstract>The paper is about our experiments with Complex Word Identification system using deep learning approach with word embeddings and engineered features.</abstract>
      <url hash="b2792b26">R19-2013</url>
      <doi>10.26615/issn.2603-2821.2019_013</doi>
      <bibkey>sheang-2019-multilingual</bibkey>
    </paper>
    <paper id="14">
      <title>Neural Network-based Models with Commonsense Knowledge for Machine Reading Comprehension</title>
      <author><first>Denis</first><last>Smirnov</last></author>
      <pages>90–94</pages>
      <abstract>State-of-the-art machine reading comprehension models are capable of producing answers for factual questions about a given piece of text. However, some type of questions requires commonsense knowledge which cannot be inferred from the given text passage. Thus, external semantic information could enhance the performance of these models. This PhD research proposal provides a brief overview of some existing machine reading comprehension datasets and models and outlines possible ways of their improvement.</abstract>
      <url hash="2f74a343">R19-2014</url>
      <doi>10.26615/issn.2603-2821.2019_014</doi>
      <bibkey>smirnov-2019-neural</bibkey>
    </paper>
  </volume>
</collection>
