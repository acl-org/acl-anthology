<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.magmar">
  <volume id="1" ingest-date="2025-07-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Multimodal Augmented Generation via Multimodal Retrieval (MAGMaR 2025)</booktitle>
      <editor><first>Reno</first><last>Kriz</last></editor>
      <editor><first>Kenton</first><last>Murray</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="d07e4a40">2025.magmar-1</url>
      <venue>magmar</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-280-0</isbn>
    </meta>
    <frontmatter>
      <url hash="d59c688e">2025.magmar-1.0</url>
      <bibkey>magmar-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>R</fixed-case>eflect: Multimodal Self-Reflective <fixed-case>RAG</fixed-case>-based Automated Fact-Checking</title>
      <author><first>Uku</first><last>Kangur</last><affiliation>University of Tartu, Institute of Computer Science, Estonia</affiliation></author>
      <author><first>Krish</first><last>Agrawal</last><affiliation>Indian Institute of Technology Indore</affiliation></author>
      <author><first>Yashashvi</first><last>Singh</last><affiliation>Indian Institute of Information Technology Dharwad</affiliation></author>
      <author><first>Ahmed</first><last>Sabir</last><affiliation>University of Tartu, Institute of Computer Science, Estonia</affiliation></author>
      <author><first>Rajesh</first><last>Sharma</last><affiliation>University of Tartu, Institute of Computer Science, Estonia and Plaksha University, India</affiliation></author>
      <pages>1-17</pages>
      <abstract>In this work, we introduce MultiReflect, a novel multimodal self-reflective Retrieval Augmented Generation (RAG)-based automated fact-checking pipeline. MultiReflect is designed to address the challenges of rapidly outdated information, limitations in human query capabilities, and expert knowledge barriers in fact-checking. Our proposed pipeline leverages the latest advancements in Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) to enhance fact verification across text and images. Specifically, by integrating multimodal data processing with RAG-based evidence reflection, our system improves the accuracy of fact-checking by utilizing internet-sourced verification. We evaluate our results on the VERITE benchmarks and using several multimodal LLMs, outperforming baselines in binary classification.</abstract>
      <url hash="7b36dd58">2025.magmar-1.1</url>
      <bibkey>kangur-etal-2025-multireflect</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>C</fixed-case>oll<fixed-case>EX</fixed-case> – A Multimodal Agentic <fixed-case>RAG</fixed-case> System Enabling Interactive Exploration of Scientific Collections</title>
      <author><first>Florian</first><last>Schneider</last><affiliation>University of Hamburg, Germany</affiliation></author>
      <author><first>Narges</first><last>Baba Ahmadi</last><affiliation>University of Hamburg, Germany</affiliation></author>
      <author><first>Niloufar</first><last>Baba Ahmadi</last><affiliation>University of Hamburg, Germany</affiliation></author>
      <author><first>Iris</first><last>Vogel</last><affiliation>University of Hamburg, Germany</affiliation></author>
      <author><first>Martin</first><last>Semmann</last><affiliation>University of Hamburg, Germany</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>University of Hamburg, Germany</affiliation></author>
      <pages>18-39</pages>
      <abstract>In this paper, we introduce CollEx, an innovative multimodal agentic Retrieval-Augmented Generation (RAG) system designed to enhance interactive exploration of extensive scientific collections. Given the overwhelming volume and inherent complexity of scientific collections, conventional search systems often lack necessary intuitiveness and interactivity, presenting substantial barriers for learners, educators, and researchers. CollEx addresses these limitations by employing state-of-the-art Large Vision-Language Models (LVLMs) as multimodal agents accessible through an intuitive chat interface. By abstracting complex interactions via specialized agents equipped with advanced tools, CollEx facilitates curiosity-driven exploration, significantly simplifying access to diverse scientific collections and records therein. Our system integrates textual and visual modalities, supporting educational scenarios that are helpful for teachers, pupils, students, and researchers by fostering independent exploration as well as scientific excitement and curiosity. Furthermore, CollEx serves the research community by discovering interdisciplinary connections and complementing visual data. We illustrate the effectiveness of our system through a proof-of-concept application containing over 64,000 unique records across 32 collections from a local scientific collection from a public university.</abstract>
      <url hash="1b493ebe">2025.magmar-1.2</url>
      <bibkey>schneider-etal-2025-collex</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>V</fixed-case>ox<fixed-case>RAG</fixed-case>: A Step Toward Transcription-Free <fixed-case>RAG</fixed-case> Systems in Spoken Question Answering</title>
      <author><first>Zackary</first><last>Rackauckas</last><affiliation>Columbia University</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <pages>40-46</pages>
      <abstract>We introduce VoxRAG, a modular speech-to-speech retrieval-augmented generation system that bypasses transcription to retrieve semantically relevant audio segments directly from spoken queries. VoxRAG employs silence-aware segmentation, speaker diarization, CLAP audio embeddings, and FAISS retrieval using L2-normalized cosine similarity. We construct a 50-query test set recorded as spoken input by a native English speaker. Retrieval quality was evaluated using LLM-as-a-judge annotations. For very relevant segments, cosine similarity achieved a Recall@10 of 0.34. For somewhat relevant segments, Recall@10 rose to 0.60 and nDCG@10 to 0.27, highlighting strong topical alignment. Answer quality was judged on a 0–2 scale across relevance, accuracy, completeness, and precision, with mean scores of 0.84, 0.58, 0.56, and 0.46 respectively. While precision and retrieval quality remain key limitations, VoxRAG shows that transcription-free speech-to-speech retrieval is feasible in RAG systems.</abstract>
      <url hash="e975d524">2025.magmar-1.3</url>
      <bibkey>rackauckas-hirschberg-2025-voxrag</bibkey>
    </paper>
    <paper id="4">
      <title>Cross-modal Clustering-based Retrieval for Scalable and Robust Image Captioning</title>
      <author><first>Jingyi</first><last>You</last><affiliation>The Japan Research Institute</affiliation></author>
      <author><first>Hiroshi</first><last>Sasaki</last><affiliation>The Japan Research Institute</affiliation></author>
      <author><first>Kazuma</first><last>Kadowaki</last><affiliation>The Japan Research Institute</affiliation></author>
      <pages>47-58</pages>
      <abstract>Recent advances in retrieval-augmented generative image captioning (RAG-IC) have significantly improved caption quality by incorporating external knowledge and similar examples into language model-driven caption generators. However, these methods still encounter challenges when applied to real-world scenarios. First, many existing approaches rely on bimodal retrieval datastores that require large amounts of labeled data and substantial manual effort to construct, making them costly and time-consuming. Moreover, they simply retrieve the nearest samples to the input query from datastores, which leads to high redundancy in the retrieved content and subsequently degrades the quality of the generated captions. In this paper, we introduce a novel RAG-IC approach named <i>
          <b>C</b>r<b>o</b>ss-modal <b>Di</b>versity-promoting <b>Ret</b>rieval technique</i> (CoDiRet), which integrates a text-only unimodal retrieval module with our unique cluster-based retrieval mechanism. This proposal simultaneously enhances the scalability of the datastore, promotes diversity in retrieved content, and improves robustness against out-of-domain inputs, which eventually facilitates real-world applications. Experimental results demonstrate that our method, despite being exclusively trained on the COCO benchmark dataset, achieves competitive performance on the in-domain benchmark and generalizes robustly across different domains without additional training.</abstract>
      <url hash="393601e0">2025.magmar-1.4</url>
      <bibkey>you-etal-2025-cross</bibkey>
    </paper>
    <paper id="5">
      <title>Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities</title>
      <author><first>Nazarii</first><last>Drushchak</last><affiliation>SoftServe Inc. and Ukrainian Catholic University</affiliation></author>
      <author><first>Nataliya</first><last>Polyakovska</last><affiliation>SoftServe Inc.</affiliation></author>
      <author><first>Maryna</first><last>Bautina</last><affiliation>SoftServe Inc.</affiliation></author>
      <author><first>Taras</first><last>Semenchenko</last><affiliation>SoftServe Inc. and Taras Shevchenko National University of Kyiv</affiliation></author>
      <author><first>Jakub</first><last>Koscielecki</last><affiliation>SoftServe Inc.</affiliation></author>
      <author><first>Wojciech</first><last>Sykala</last><affiliation>SoftServe Inc.</affiliation></author>
      <author><first>Michal</first><last>Wegrzynowski</last><affiliation>SoftServe Inc.</affiliation></author>
      <pages>59-64</pages>
      <abstract>Retrieval-augmented generation (RAG) is a powerful paradigm for leveraging external data to enhance the capabilities of large language models (LLMs). However, most existing RAG solutions are tailored for single-modality or limited multimodal scenarios, restricting their applicability in real-world contexts where diverse data sources—including text, tables, images, and videos—must be integrated seamlessly. In this work proposes a unified <i>Multimodal Retrieval-augmented generation (mRAG)</i> system designed to unify information processing across all four modalities. Our pipeline ingests and indexes data from PDFs and videos using tools like Amazon Textract, Transcribe, Langfuse, and multimodal LLMs (e.g., Claude 3.5 Sonnet) for structured extraction and semantic enrichment. The dataset includes text queries, table lookups, image-based questions, and videos. Evaluation with the Deepeval framework shows improved retrieval accuracy and response quality, especially for structured text and tables. While performance on image and video queries is lower, the multimodal integration framework remains robust, underscoring the value of unified pipelines for diverse data.</abstract>
      <url hash="d0d49bdc">2025.magmar-1.5</url>
      <bibkey>drushchak-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="6">
      <title>Making <fixed-case>LVLM</fixed-case>s Look Twice: Contrastive Decoding with Contrast Images</title>
      <author><first>Avshalom</first><last>Manevich</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Bar Ilan University</affiliation></author>
      <pages>65-78</pages>
      <abstract>Large Vision-Language Models (LVLMs) are becoming increasingly popular for text-vision tasks requiring cross-modal reasoning, but often struggle with fine-grained visual discrimination. This limitation is evident in recent benchmarks like NaturalBench and D3, where closed models such as GPT-4o achieve only 39.6%, and open-source models perform below random chance (25%). We introduce Contrastive decoding with Contrast Images (CoCI), which adjusts LVLM outputs by contrasting them against outputs for similar images (Contrast Images - CIs). CoCI demonstrates strong performance across three distinct supervision regimes. First, when using naturally occurring CIs in benchmarks with curated image pairs, we achieve improvements of up to 98.9% on NaturalBench, 69.5% on D3, and 37.6% on MMVP. Second, for scenarios with modest training data (~5k samples), we show that a lightweight neural classifier can effectively select CIs from similar images at inference time, improving NaturalBench performance by up to 36.8%. Third, for scenarios with no training data, we develop a caption-matching technique that selects CIs by comparing LVLM-generated descriptions of candidate images. Notably, on VQAv2, our method improves VQA performance even in pointwise evaluation settings without explicit contrast images. Our approach demonstrates the potential for enhancing LVLMs at inference time through different CI selection approaches, each suited to different data availability scenarios.</abstract>
      <url hash="aec8971b">2025.magmar-1.6</url>
      <bibkey>manevich-tsarfaty-2025-making</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>MT</fixed-case>2<fixed-case>ST</fixed-case>: Adaptive Multi-Task to Single-Task Learning</title>
      <author><first>Dong</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Yanxuan</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>79-89</pages>
      <abstract>We propose <b>MT2ST</b>, a general and efficient framework for accelerating multi-task training by progressively transitioning to single-task optimization. Unlike conventional multi-task learning (MTL) or single-task fine-tuning (STL), MT2ST dynamically adjusts the training focus via two complementary strategies: <i>Diminish</i>, which gradually down-weights auxiliary losses, and <i>Switch</i>, which explicitly switches to the primary task at a scheduled point. We demonstrate the effectiveness of MT2ST across three key paradigms: representation learning, transformers, and diffusion models, covering both unimodal (text/image) and multimodal (vision-language) tasks. Extensive experiments show that MT2ST significantly improves training efficiency—achieving up to 56% FLOPs compression—while maintaining or surpassing task performance. These results suggest MT2ST as a general-purpose solution for scalable and adaptive multi-task training. Although this work is general-purpose, it is especially suitable for multimodal settings such as VQA or vision-language retrieval, where auxiliary pretraining (e.g., masked language modeling or contrastive learning) often diverges from final objectives. We include a VQA case study and outline its efficiency for multimodal retrieval.</abstract>
      <url hash="28cf5b09">2025.magmar-1.8</url>
      <bibkey>liu-yu-2025-mt2st</bibkey>
    </paper>
    <paper id="9">
      <title>Cross-Modal Augmentation for Low-Resource Language Understanding and Generation</title>
      <author><first>Zichao</first><last>Li</last><affiliation>Canoakbit Alliance</affiliation></author>
      <author><first>Zong</first><last>Ke</last><affiliation>National University of Singapore</affiliation></author>
      <pages>90-99</pages>
      <abstract>This paper introduces a multimodal retrieval-augmented generation (RAG) system designed to enhance language understanding and generation for low-resource languages. By integrating textual, visual, and geospatial data, the system leverages cross-lingual adaptation and multimodal augmentation to bridge the gap between high-resource and low-resource languages. Evaluated on the MM-COVID and LORELEI datasets, the system demonstrates superior performance in retrieval (precision: 85%, recall: 82%) and generation (BLEU: 28.4) tasks compared to baselines. Case studies in public health communication and disaster response highlight its practical utility. The results underscore the potential of multimodal AI to democratize access to technology and address global challenges in low-resource settings.</abstract>
      <url hash="d86c0ce6">2025.magmar-1.9</url>
      <bibkey>li-ke-2025-cross</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>FORTIFY</fixed-case>: Generative Model Fine-tuning with <fixed-case>ORPO</fixed-case> for <fixed-case>R</fixed-case>e<fixed-case>T</fixed-case>rieval Expansion of <fixed-case>I</fixed-case>n<fixed-case>F</fixed-case>ormal <fixed-case>N</fixed-case>ois<fixed-case>Y</fixed-case> Text</title>
      <author><first>Dan</first><last>DeGenaro</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Eugene</first><last>Yang</last><affiliation>Human Language Technology Center of Excellence, Johns Hopkins University</affiliation></author>
      <author><first>David</first><last>Etter</last><affiliation>Human Language Technology Center of Excellence</affiliation></author>
      <author><first>Cameron</first><last>Carpenter</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kate</first><last>Sanders</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Alexander</first><last>Martin</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Human Language Technology Center of Excellence, Johns Hopkins University</affiliation></author>
      <author><first>Reno</first><last>Kriz</last><affiliation>Human Language Technology Center of Excellence, Johns Hopkins University</affiliation></author>
      <pages>100-115</pages>
      <abstract>Despite recent advancements in neural retrieval, representing text fragments or phrases with proper contextualized embeddings is still challenging. Particularly in video retrieval, where documents are text extracted through OCR from the frames or ASR from audio tracks, the textual content is rarely complete sentences but only a bag of phrases. In this work, we propose FORTIFY, a generative model fine-tuning approach for noisy document rewriting and summarization, to improve the downstream retrieval effectiveness. By experimenting on MultiVENT 2.0, an informational video retrieval benchmark, we show Llama fine-tuned with FORTIFY provides an effective document expansion, leading to a 30% improvement over prompting an out-of-box Llama model on nDCG@10. Zero-shot transferring the model tailored for MultiVENT 2.0 to two out-of-distribution datasets still demonstrates competitive retrieval effectiveness to other document preprocessing alternatives.</abstract>
      <url hash="1824e1a0">2025.magmar-1.13</url>
      <bibkey>degenaro-etal-2025-fortify</bibkey>
    </paper>
  </volume>
</collection>
