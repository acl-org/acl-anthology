<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.sumeval">
  <volume id="2" ingest-date="2025-01-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Scaling Up Multilingual &amp; Multi-Cultural Evaluation</booktitle>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi</address>
      <month>January</month>
      <year>2025</year>
      <url hash="b8306ae5">2025.sumeval-2</url>
      <venue>sumeval</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="30b82a43">2025.sumeval-2.0</url>
      <bibkey>sumeval-ws-2025-2</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The First Multilingual Model For The Detection of Suicide Texts</title>
      <author><first>Rodolfo Joel</first><last>Zevallos</last></author>
      <author><first>Annika Marie</first><last>Schoene</last></author>
      <author><first>John E.</first><last>Ortega</last></author>
      <pages>1–11</pages>
      <abstract>Suicidal ideation is a serious health problem affecting millions of people worldwide. Social networks provide information about these mental health problems through users’ emotional expressions. We propose a multilingual model leveraging transformer architectures like mBERT, XML-R, and mT5 to detect suicidal text across posts in six languages - Spanish, English, German, Catalan, Portuguese and Italian. A Spanish suicide ideation tweet dataset was translated into five other languages using SeamlessM4T. Each model was fine-tuned on this multilingual data and evaluated across classification metrics. Results showed mT5 achieving the best performance overall with F1 scores above 85%, highlighting capabilities for cross-lingual transfer learning. The English and Spanish translations also displayed high quality based on perplexity. Our exploration underscores the importance of considering linguistic diversity in developing automated multilingual tools to identify suicidal risk. Limitations exist around semantic fidelity in translations and ethical implications which provide guidance for future human-in-the-loop evaluations.</abstract>
      <url hash="067e55db">2025.sumeval-2.1</url>
      <bibkey>zevallos-etal-2025-first</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>C</fixed-case>ross<fixed-case>I</fixed-case>n: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment</title>
      <author><first>Geyu</first><last>Lin</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <pages>12–23</pages>
      <abstract>Multilingual proficiency presents a significant challenge for large language models (LLMs). English-centric models are usually suboptimal in other languages, particularly those that are linguistically distant from English. This performance discrepancy mainly stems from the imbalanced distribution of training data across languages during pre-training and instruction tuning stages. To address this problem, we propose a novel approach called CrossIn, which utilizes a mixed composition of cross-lingual instruction tuning data. Our method leverages the compressed representation shared by various languages to efficiently enhance the model’s task-solving capabilities and multilingual proficiency within a single process. In addition, we introduce a multi-task and multi-faceted benchmark to evaluate the effectiveness of CrossIn. Experimental results demonstrate that our method substantially improves performance across tasks and languages, and we provide extensive insights into the impact of cross-lingual data volume and the integration of translation data on enhancing multilingual consistency and accuracy.</abstract>
      <url hash="6827375e">2025.sumeval-2.2</url>
      <bibkey>lin-etal-2025-crossin</bibkey>
    </paper>
    <paper id="3">
      <title>Evaluating Dialect Robustness of Language Models via Conversation Understanding</title>
      <author><first>Dipankar</first><last>Srirag</last></author>
      <author><first>Nihar Ranjan</first><last>Sahoo</last></author>
      <author><first>Aditya</first><last>Joshi</last></author>
      <pages>24–38</pages>
      <abstract>With an evergrowing number of LLMs reporting superlative performance for English, their ability to perform equitably for different dialects of English (i.e., dialect robustness) needs to be ascertained. Specifically, we use English language (US English or Indian English) conversations between humans who play the word-guessing game of ‘taboo‘. We formulate two evaluative tasks: target word prediction (TWP) (i.e., predict the masked target word in a conversation) and target word selection (TWS) (i.e., select the most likely masked target word in a conversation, from among a set of candidate words). Extending MD3, an existing dialectic dataset of taboo-playing conversations, we introduce M-MD3, a target-word-masked version of MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where en-US is transformed to include dialectal information) and en-TR (where dialectal information is removed from en-IN). We evaluate three multilingual LLMs–one open source (Llama3) and two closed-source (GPT-4/3.5). LLMs perform significantly better for US English than Indian English for both TWP and TWS tasks, for all settings, exhibiting marginalisation against the Indian dialect of English. While GPT-based models perform the best, the comparatively smaller models work more equitably after fine-tuning. Our evaluation methodology exhibits a novel and reproducible way to examine attributes of language models using pre-existing dialogue datasets with language varieties. Dialect being an artifact of one’s culture, this paper demonstrates the gap in the performance of multilingual LLMs for communities that do not use a mainstream dialect.</abstract>
      <url hash="b5a7c063">2025.sumeval-2.3</url>
      <bibkey>srirag-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="4">
      <title>Cross-Lingual Document Recommendations with Transformer-Based Representations: Evaluating Multilingual Models and Mapping Techniques</title>
      <author><first>Tsegaye Misikir</first><last>Tashu</last></author>
      <author><first>Eduard-Raul</first><last>Kontos</last></author>
      <author><first>Matthia</first><last>Sabatelli</last></author>
      <author><first>Matias</first><last>Valdenegro-Toro</last></author>
      <pages>39–47</pages>
      <abstract>Recommendation systems, for documents, have become tools for finding relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding beyond language connections, between two specific languages.</abstract>
      <url hash="998c098a">2025.sumeval-2.4</url>
      <bibkey>tashu-etal-2025-cross</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>VRCP</fixed-case>: Vocabulary Replacement Continued Pretraining for Efficient Multilingual Language Models</title>
      <author><first>Yuta</first><last>Nozaki</last></author>
      <author><first>Dai</first><last>Nakashima</last></author>
      <author><first>Ryo</first><last>Sato</last></author>
      <author><first>Naoki</first><last>Asaba</last></author>
      <author><first>Shintaro</first><last>Kawamura</last></author>
      <pages>48–59</pages>
      <abstract>Building large language models (LLMs) for non-English languages involves leveraging extensively trained English models through continued pre-training on the target language corpora. This approach harnesses the rich semantic knowledge embedded in English models, allowing superior performance compared to training from scratch. However, tokenizers not optimized for the target language may make inefficiencies in training. We propose Vocabulary Replacement Continued Pretraining (VRCP), a method that optimizes the tokenizer for the target language by replacing unique (solely available) vocabulary from the source tokenizer while maintaining the overall vocabulary size. This approach preserves the semantic knowledge of the source model while enhancing token efficiency and performance for the target language. We evaluated VRCP using the Llama-2 model on Japanese and Chinese corpora. The results show that VRCP matches the performance of vocabulary expansion methods on benchmarks and achieves superior performance in summarization tasks. Additionally, VRCP provides an optimized tokenizer that balances token efficiency, task performance, and GPU memory footprint, making it particularly suitable for resource-constrained environments.</abstract>
      <url hash="b04e5a20">2025.sumeval-2.5</url>
      <bibkey>nozaki-etal-2025-vrcp</bibkey>
    </paper>
  </volume>
</collection>
