<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlp4musa">
  <volume id="1" ingest-date="2025-01-10" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)</booktitle>
      <editor><first>Anna</first><last>Kruspe</last></editor>
      <editor><first>Sergio</first><last>Oramas</last></editor>
      <editor><first>Elena V.</first><last>Epure</last></editor>
      <editor><first>Mohamed</first><last>Sordo</last></editor>
      <editor><first>Benno</first><last>Weck</last></editor>
      <editor><first>SeungHeon</first><last>Doh</last></editor>
      <editor><first>Minz</first><last>Won</last></editor>
      <editor><first>Ilaria</first><last>Manco</last></editor>
      <editor><first>Gabriel</first><last>Meseguer-Brocal</last></editor>
      <publisher>Association for Computational Lingustics</publisher>
      <address>Oakland, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="aebc932a">2024.nlp4musa-1</url>
      <venue>nlp4musa</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="5450c908">2024.nlp4musa-1.0</url>
      <bibkey>nlp4musa-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Genre-Conformity in the Topics of Lyrics and Song Popularity</title>
      <author><first>Anna</first><last>Aljanaki</last></author>
      <pages>1–4</pages>
      <abstract>The genre of a song defines both musical (rhythmic, timbral, performative) aspects of a song, but also the themes of lyrics and the style of writing. The audience has certain expectations as to emotional and thematic content of the genre they listen to. In this paper we use Music4All database to investigate whether breaking these expectations influences song popularity. We use topic modeling to divide song lyrics into 36 clusters, and apply tag clustering to separate the songs into 15 musical genres. We observe that in some genres (metal, hip-hop) lyrics are mostly written in specific topics, whereas in other genres they are spread over most topics. In most genres, songs that have lyrics that are not representative of the genre, are more popular than songs with genre-conforming lyrics.</abstract>
      <url hash="297cacf1">2024.nlp4musa-1.1</url>
      <bibkey>aljanaki-2024-genre</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>PIAST</fixed-case>: A Multimodal Piano Dataset with Audio, Symbolic and Text</title>
      <author><first>Hayeon</first><last>Bang</last></author>
      <author><first>Eunjin</first><last>Choi</last></author>
      <author><first>Megan</first><last>Finch</last></author>
      <author><first>Seungheon</first><last>Doh</last></author>
      <author><first>Seolhee</first><last>Lee</last></author>
      <author><first>Gyeong-Hoon</first><last>Lee</last></author>
      <author><first>Juhan</first><last>Nam</last></author>
      <pages>5–10</pages>
      <abstract>While piano music has become a significant area of study in Music Information Retrieval (MIR), there is a notable lack of datasets for piano solo music with text labels. To address this gap, we present PIAST (PIano dataset with Audio, Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy of semantic tags, we collected 9,673 tracks from YouTube and added human annotations for 2,023 tracks by music experts, resulting in two subsets: PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and transcribed MIDI utilizing state-of-the-art piano transcription and beat tracking models. Among many possible tasks with the multimodal dataset, we conduct music tagging and retrieval using both audio and MIDI data and report baseline performances to demonstrate its potential as a valuable resource for MIR research.</abstract>
      <url hash="270c01a5">2024.nlp4musa-1.2</url>
      <bibkey>bang-etal-2024-piast</bibkey>
    </paper>
    <paper id="3">
      <title>Lyrics Transcription in Western Classical Music with Whisper: A Case Study on Schubert’s Winterreise</title>
      <author><first>Hans-Ulrich</first><last>Berendes</last></author>
      <author><first>Simon</first><last>Schwär</last></author>
      <author><first>Meinard</first><last>Müller</last></author>
      <pages>11–16</pages>
      <abstract>Automatic Lyrics Transcription (ALT) aims to transcribe sung words from music recordings and is closely related to Automatic Speech Recognition (ASR). Although not specifically designed for lyrics transcription, the state-of-the-art ASR model Whisper has recently proven effective for ALT and various related tasks in music information retrieval (MIR). This paper investigates Whisper’s performance on Western classical music, using the “Schubert Winterreise Dataset.” In particular, we found that the average Word Error Rate (WER) with the unmodified Whisper model is 0.56 for this dataset, while the performance varies greatly across songs and versions. In contrast, spoken versions of the song lyrics, which we recorded, are transcribed with a WER of 0.14. Further systematic experiments with source separation and time-scale modification techniques indicate that Whisper’s accuracy in lyrics transcription is less affected by the musical accompaniment and more by the singing style.</abstract>
      <url hash="7095777e">2024.nlp4musa-1.3</url>
      <bibkey>berendes-etal-2024-lyrics</bibkey>
    </paper>
    <paper id="4">
      <title>Harnessing High-Level Song Descriptors towards Natural Language-Based Music Recommendation</title>
      <author><first>Elena V.</first><last>Epure</last></author>
      <author><first>Gabriel Meseguer</first><last>Brocal</last></author>
      <author><first>Darius</first><last>Afchar</last></author>
      <author><first>Romain</first><last>Hennequin</last></author>
      <pages>17–24</pages>
      <abstract>Recommender systems relying on Language Models (LMs) have gained popularity in assisting users to navigate large catalogs. LMs often exploit item high-level descriptors, i.e. categories or consumption contexts, from training data or user preferences. This has been proven effective in domains like movies or products. In music though, understanding how effectively LMs utilize song descriptors for natural language-based music recommendation is relatively limited. In this paper, we assess LMs effectiveness in recommending songs based on user natural language requests and items with descriptors like genres, moods, and listening contexts. We formulate the recommendation as a dense retrieval problem and assess LMs as they become increasingly familiar with data pertinent to the task and domain. Our findings reveal improved performance as LMs are fine-tuned for general language similarity, information retrieval, and mapping longer descriptions to shorter, high-level descriptors in music.</abstract>
      <url hash="5b6a9fcc">2024.nlp4musa-1.4</url>
      <bibkey>epure-etal-2024-harnessing</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>NLP</fixed-case> Analysis of Environmental Themes in Phish Lyrics Across Concert Locations and Years</title>
      <author><first>Anna</first><last>Farzindar</last></author>
      <author><first>Jason</first><last>Jarvis</last></author>
      <pages>25–30</pages>
      <abstract>This study delves into the application of advanced AI and natural language processing techniques (NLP), to analyze the lyrics of Phish, a renowned American jam band. Focusing on environmental themes within their extensive repertoire, this paper aims to uncover latent topics pertaining to environmental discourse, by using the topic modeling and environmental classifier. Through meticulous preprocessing, modeling, and interpretation, our findings shed light on the multifaceted portrayal of environmental issues in Phish’s lyrics. In this study, our primary contribution lies in lyrical analysis, as well as visualization and interpretation of the topics their lyrics cover, over the forty plus years the band has existed. Our lyrical visualizations aim to facilitate an understanding of how Phish selects the timing and location for their live performances in relation to the themes present in their music.</abstract>
      <url hash="c1db641b">2024.nlp4musa-1.5</url>
      <bibkey>farzindar-jarvis-2024-nlp</bibkey>
    </paper>
    <paper id="6">
      <title>A Retrieval Augmented Approach for Text-to-Music Generation</title>
      <author><first>Robie</first><last>Gonzales</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>31–36</pages>
      <abstract>Generative text-to-music models such as MusicGen are capable of generating high fidelity music conditioned on a text prompt. However, expressing the essential features of music with text is a challenging task. In this paper, we present a retrieval-augmented approach for text-to-music generation. We first pre-compute a dataset of text-music embeddings obtained from a contrastive language-audio pretrained encoder (CLAP). Then, given an input text prompt, we retrieve the top <tex-math>k</tex-math> most similar musical aspects and augment the original prompt. This approach consistently generates music of higher audio quality as measured by the Frechét Audio Distance. We analyze the internal representations of MusicGen and find that augmented prompts lead to greater diversity in token distributions and display high text adherence. Our findings show the potential for increased control in text-to-music generation.</abstract>
      <url hash="381d1fde">2024.nlp4musa-1.6</url>
      <bibkey>gonzales-rudzicz-2024-retrieval</bibkey>
    </paper>
    <paper id="7">
      <title>Information Extraction of Music Entities in Conversational Music Queries</title>
      <author><first>Simon</first><last>Hachmeier</last></author>
      <author><first>Robert</first><last>Jäschke</last></author>
      <pages>37–42</pages>
      <abstract>The detection of music entities such as songs or performing artists in natural language queries is an important task when designing conversational music recommendation agents. Previous research has observed the applicability of named entity recognition approaches for this task based on pre-trained encoders like BERT. In recent years, large language models (LLMs) have surpassed these encoders in a variety of downstream tasks. In this paper, we validate the use of LLMs for information extraction of music entities in conversational queries by few-shot prompting. We test different numbers of examples and compare two sampling methods to obtain few-shot examples. Our results indicate that LLM performance can achieve state-of-the-art performance in the task.</abstract>
      <url hash="e896ade8">2024.nlp4musa-1.7</url>
      <bibkey>hachmeier-jaschke-2024-information</bibkey>
    </paper>
    <paper id="8">
      <title>Leveraging User-Generated Metadata of Online Videos for Cover Song Identification</title>
      <author><first>Simon</first><last>Hachmeier</last></author>
      <author><first>Robert</first><last>Jäschke</last></author>
      <pages>43–48</pages>
      <abstract>YouTube is a rich source of cover songs. Since the platform itself is organized in terms of videos rather than songs, the retrieval of covers is not trivial. The field of cover song identification addresses this problem and provides approaches that usually rely on audio content. However, including the user-generated video metadata available on YouTube promises improved identification results. In this paper, we propose a multi-modal approach for cover song identification on online video platforms. We combine the entity resolution models with audio-based approaches using a ranking model. Our findings implicate that leveraging user-generated metadata can stabilize cover song identification performance on YouTube.</abstract>
      <url hash="e22aa20c">2024.nlp4musa-1.8</url>
      <bibkey>hachmeier-jaschke-2024-leveraging</bibkey>
    </paper>
    <paper id="9">
      <title>Can Impressions of Music be Extracted from Thumbnail Images?</title>
      <author><first>Takashi</first><last>Harada</last></author>
      <author><first>Takehiro</first><last>Motomitsu</last></author>
      <author><first>Katsuhiko</first><last>Hayashi</last></author>
      <author><first>Yusuke</first><last>Sakai</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <pages>49–56</pages>
      <abstract>In recent years, there has been a notable increase in research on machine learning models for music retrieval and generation systems that are capable of taking natural language sentences as inputs. However, there is a scarcity of large-scale publicly available datasets, consisting of music data and their corresponding natural language descriptions known as music captions. In particular, non-musical information such as suitable situations for listening to a track and the emotions elicited upon listening is crucial for describing music. This type of information is underrepresented in existing music caption datasets due to the challenges associated with extracting it directly from music data. To address this issue, we propose a method for generating music caption data that incorporates non-musical aspects inferred from music thumbnail images, and validated the effectiveness of our approach through human evaluations.</abstract>
      <url hash="7c57770a">2024.nlp4musa-1.9</url>
      <bibkey>harada-etal-2024-impressions</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>R</fixed-case>aga Space Visualization: Analyzing Melodic Structures in Carnatic and <fixed-case>H</fixed-case>industani Music</title>
      <author><first>Soham</first><last>Korade</last></author>
      <author><first>Suswara</first><last>Pochampally</last></author>
      <author><first>Saroja</first><last>TK</last></author>
      <pages>57–61</pages>
      <abstract>The concept of raga in Indian classical music serves as a complex, multifaceted melodic entity that can be approached through various perspectives. Compositions within a raga act as foundational structures, serving as the bedrock for improvisations. Analyzing their textual notations is easier and more objective in comparison with analyzing audio samples. A significant amount of musical insights can be derived from the discrete swara sequences alone. This paper aims to construct an intuitive visualization of raga space, using swara sequences from raga compositions. Notations from public sources are normalized, and their TF-IDF features are projected into a low-dimensional space. This approach allows for qualitative analysis of both Carnatic and Hindustani ragas, mapping them to known raga theory.</abstract>
      <url hash="b8b3e271">2024.nlp4musa-1.10</url>
      <bibkey>korade-etal-2024-raga</bibkey>
    </paper>
    <paper id="11">
      <title>Musical Ethnocentrism in Large Language Models</title>
      <author><first>Anna</first><last>Kruspe</last></author>
      <pages>62–68</pages>
      <abstract>Large Language Models (LLMs) reflect the biases in their training data and, by extension, those of the people who created this training data. Detecting, analyzing, and mitigating such biases is becoming a focus of research. One type of bias that has been understudied so far are geocultural biases. Those can be caused by an imbalance in the representation of different geographic regions and cultures in the training data, but also by value judgments contained therein. In this paper, we make a first step towards analyzing musical biases in LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the first, we prompt LLMs to provide lists of the “Top 100” musical contributors of various categories and analyze their countries of origin. In the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries. Our results indicate a strong preference of the LLMs for Western music cultures in both experiments.</abstract>
      <url hash="a297ce9f">2024.nlp4musa-1.11</url>
      <bibkey>kruspe-2024-musical</bibkey>
      <revision id="1" href="2024.nlp4musa-1.11v1" hash="c6551d75"/>
      <revision id="2" href="2024.nlp4musa-1.11v2" hash="a297ce9f" date="2025-02-06">Corrected references.</revision>
    </paper>
    <paper id="12">
      <title>Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic Music: A Focus on Musical Phrase Segmentation</title>
      <author><first>Dinh-Viet-Toan</first><last>Le</last></author>
      <author><first>Louis</first><last>Bigo</last></author>
      <author><first>Mikaela</first><last>Keller</last></author>
      <pages>69–74</pages>
      <abstract>Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language Processing to build a vocabulary of subwords, which has been recently applied to symbolic music. Given that symbolic music can differ significantly from text, particularly with polyphony, we investigate how BPE behaves with different types of musical content. This study provides a qualitative analysis of BPE’s behavior across various instrumentations and evaluates its impact on a musical phrase segmentation task for both monophonic and polyphonic music. Our findings show that the BPE training process is highly dependent on the instrumentation and that BPE “supertokens” succeed in capturing abstract musical content. In a musical phrase segmentation task, BPE notably improves performance in a polyphonic setting, but enhances performance in monophonic tunes only within a specific range of BPE merges.</abstract>
      <url hash="82694e0c">2024.nlp4musa-1.12</url>
      <bibkey>le-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="13">
      <title>Lyrics for Success: Embedding Features for Song Popularity Prediction</title>
      <author><first>Giulio</first><last>Prevedello</last></author>
      <author><first>Ines</first><last>Blin</last></author>
      <author><first>Bernardo</first><last>Monechi</last></author>
      <author><first>Enrico</first><last>Ubaldi</last></author>
      <pages>75–80</pages>
      <abstract>Accurate song success prediction is vital for the music industry, guiding promotion and label decisions. Early, accurate predictions are thus crucial for informed business actions. We investigated the predictive power of lyrics embedding features, alone and in combination with other stylometric features and various Spotify metadata (audio, platform, playlists, reactions). We compiled a dataset of 12,428 Spotify tracks and targeted popularity 15 days post-release. For the embeddings, we used a Large Language Model and compared different configurations. We found that integrating embeddings with other lyrics and audio features improved early-phase predictions, underscoring the importance of a comprehensive approach to success prediction.</abstract>
      <url hash="cb649b0c">2024.nlp4musa-1.13</url>
      <bibkey>prevedello-etal-2024-lyrics</bibkey>
    </paper>
    <paper id="14">
      <title>The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?</title>
      <author><first>Pedro</first><last>Ramoneda</last></author>
      <author><first>Emila</first><last>Parada-Cabaleiro</last></author>
      <author><first>Benno</first><last>Weck</last></author>
      <author><first>Xavier</first><last>Serra</last></author>
      <pages>81–86</pages>
      <abstract>In this work, we explore the use and reliability of Large Language Models (LLMs) in musicology. From a discussion with experts and students, we assess the current acceptance and concerns regarding this, nowadays ubiquitous, technology. We aim to go one step further, proposing a semi-automatic method to create an initial benchmark using retrieval-augmented generation models and multiple-choice question generation, validated by human experts. Our evaluation on 400 human-validated questions shows that current vanilla LLMs are less reliable than retrieval augmented generation from music dictionaries. This paper suggests that the potential of LLMs in musicology requires musicology driven research that can specialized LLMs by including accurate and reliable domain knowledge.</abstract>
      <url hash="28179c5f">2024.nlp4musa-1.14</url>
      <bibkey>ramoneda-etal-2024-role</bibkey>
    </paper>
    <paper id="15">
      <title>“Does it Chug?” Towards a Data-Driven Understanding of Guitar Tone Description</title>
      <author><first>Pratik</first><last>Sutar</last></author>
      <author><first>Jason</first><last>Naradowsky</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>87–97</pages>
      <abstract/>
      <url hash="4c608ab3">2024.nlp4musa-1.15</url>
      <attachment type="Supplementary" hash="d3aabb53">2024.nlp4musa-1.15.Supplementary.pdf</attachment>
      <bibkey>sutar-etal-2024-chug</bibkey>
    </paper>
    <paper id="16">
      <title>Evaluation of Pretrained Language Models on Music Understanding</title>
      <author><first>Yannis</first><last>Vasilakis</last></author>
      <author><first>Rachel</first><last>Bittner</last></author>
      <author><first>Johan</first><last>Pauwels</last></author>
      <pages>98–106</pages>
      <abstract>Music-text multimodal systems have enabled new approaches to Music Information Research (MIR) applications. Despite the reported success, there has been little effort in evaluating the musical knowledge of Large Language Models (LLM). We demonstrate that LLMs suffer from prompt sensitivity, inability to model negation and sensitivity towards specific words. We quantified these properties as a triplet-based accuracy, evaluating the ability to model the relative similarity of labels in a hierarchical ontology. We leveraged Audioset ontology to generate triplets consisting of anchor, positive and negative label for genre/instruments sub-tree and use six general-purpose Transformer-based models. Triplets required filtering, as some were difficult to judge and therefore relatively uninformative for evaluation purposes. Despite the relatively high accuracy reported, inconsistencies are evident in all six models, suggesting that off-the-shelf LLMs need adaptation to music before use.</abstract>
      <url hash="a10cf17a">2024.nlp4musa-1.16</url>
      <bibkey>vasilakis-etal-2024-evaluation</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>FUTGA</fixed-case>: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation</title>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Zachary</first><last>Novack</last></author>
      <author><first>Amit</first><last>Namburi</last></author>
      <author><first>Jiaheng</first><last>Dai</last></author>
      <author><first>Hao-Wen</first><last>Dong</last></author>
      <author><first>Zhouhang</first><last>Xie</last></author>
      <author><first>Carol</first><last>Chen</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>107–111</pages>
      <abstract>We propose FUTGA, a model equipped with fined-grained music understanding capabilities through learning from generative augmentation with temporal compositions. We leverage existing music caption datasets and large language models (LLMs) to synthesize fine-grained music captions with structural descriptions and time boundaries for full-length songs. Augmented by the proposed synthetic dataset, FUTGA is enabled to identify the music’s temporal changes at key transition points and their musical functions, as well as generate detailed descriptions for each music segment. We further introduce a full-length music caption dataset generated by FUTGA, as the augmentation of the MusicCaps and the Song Describer datasets. The experiments demonstrate the better quality of the generated captions, which capture the time boundaries of long-form music.</abstract>
      <url hash="ae0fb87d">2024.nlp4musa-1.17</url>
      <bibkey>wu-etal-2024-futga</bibkey>
    </paper>
    <paper id="18">
      <title>The Interpretation Gap in Text-to-Music Generation Models</title>
      <author><first>Yongyi</first><last>Zang</last></author>
      <author><first>Yixiao</first><last>Zhang</last></author>
      <pages>112–118</pages>
      <abstract>Large-scale text-to-music generation models have significantly enhanced music creation capabilities, offering unprecedented creative freedom. However, their ability to collaborate effectively with human musicians remains limited. In this paper, we propose a framework to describe the musical interaction process, which includes expression, interpretation, and execution of controls. Following this framework, we argue that the primary gap between existing text-to-music models and musicians lies in the interpretation stage, where models lack the ability to interpret controls from musicians. We also propose two strategies to address this gap and call on the music information retrieval community to tackle the interpretation challenge to improve human-AI musical collaboration.</abstract>
      <url hash="ada876b7">2024.nlp4musa-1.18</url>
      <bibkey>zang-zhang-2024-interpretation</bibkey>
    </paper>
  </volume>
</collection>
