<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.tsar">
  <volume id="1" ingest-date="2025-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Text Simplification, Accessibility and Readability (TSAR 2025)</booktitle>
      <editor><first>Matthew</first><last>Shardlow</last></editor>
      <editor><first>Fernando</first><last>Alva-Manchego</last></editor>
      <editor><first>Kai</first><last>North</last></editor>
      <editor><first>Regina</first><last>Stodden</last></editor>
      <editor><first>Horacio</first><last>Saggion</last></editor>
      <editor><first>Nouran</first><last>Khallaf</last></editor>
      <editor><first>Akio</first><last>Hayakawa</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>November</month>
      <year>2025</year>
      <url hash="9ab2bc12">2025.tsar-1</url>
      <venue>tsar</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-176-6</isbn>
    </meta>
    <frontmatter>
      <url hash="c2ad8afe">2025.tsar-1.0</url>
      <bibkey>tsar-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Template-Based Text-to-Image Alignment for Language Accessibility A Study on Visualizing Text Simplifications</title>
      <author><first>Belkiss</first><last>Souayed</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Yingqiang</first><last>Gao</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>1-18</pages>
      <abstract>Individuals with intellectual disabilities often have difficulties in comprehending complex texts. While many text-to-image models prioritize photorealism over cognitive accessibility it is not clear how visual illustrations relate to text simplifications TS generated from them. This paper presents a structured vision language model VLM prompting framework for generating cognitively accessible images from simplified texts. We designed five prompt templates i.e. Basic Object Focus Contextual Scene Educational Layout Multi-Level Detail and Grid Layout each following distinct spatial arrangements while adhering to accessibility constraints such as object count limits spatial separation and content restrictions. Using 400 sentence-level TS pairs from four established text simplification datasets OneStopEnglish SimPA Wikipedia ASSET we conducted a two-phase evaluation Phase 1 assessed template effectiveness with CLIP similarity scores and Phase 2 involved expert annotation of generated images across ten visual styles by four accessibility specialists. Results show that the Basic Object Focus template achieved the highest semantic alignment indicating that visual minimalism enhances accessibility. Expert evaluation further identified Retro style as the most accessible and Wikipedia as the most effective text source. Inter-annotator agreement varied across dimensions with Text Simplicity showing strong reliability and Image Quality proving more subjective. Overall our framework offers practical guidelines for accessible content creation and underscores the importance of structured prompting in AI-generated visual accessibility tools.</abstract>
      <url hash="c044909c">2025.tsar-1.1</url>
      <bibkey>souayed-etal-2025-template</bibkey>
    </paper>
    <paper id="2">
      <title>Document-level Simplification and Illustration Generation Multimodal Coherence</title>
      <author><first>Yuhang</first><last>Liu</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Mo</first><last>Zhang</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Zhaoyi</first><last>Cheng</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>19-35</pages>
      <abstract>We present a novel method for document-level text simplification and automatic illustration generation aimed at enhancing information accessibility for individuals with cognitive impairments. While prior research has primarily focused on sentence- or paragraph-level simplification and text-to-image generation for narrative contexts this work addresses the unique challenges of simplifying long-form documents and generating semantically aligned visuals. The pipeline consists of three stages (1) discourse-aware segmentation using large language models (2) visually grounded description generation via abstraction and (3) controlled image synthesis using state-of-the-art diffusion models including DALLE 3 and FLUX1-dev. We further incorporate stylistic constraints to ensure visual coherence and we conduct a human evaluation measuring comprehension semantic alignment and visual clarity. Experimental results demonstrate that our method effectively combines simplified text and visual content with generated illustrations enhancing textual accessibility.</abstract>
      <url hash="0bbdef20">2025.tsar-1.2</url>
      <bibkey>liu-etal-2025-document</bibkey>
    </paper>
    <paper id="3">
      <title>Medical Text Simplification From Jargon Detection to Jargon-Aware Prompting</title>
      <author><first>Taiki</first><last>Papandreou</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Jan</first><last>Bakker</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Jaap</first><last>Kamps</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>36-46</pages>
      <abstract>Jargon identification is critical for improving the accessibility of biomedical texts yet models are often evaluated on isolated datasets leaving open questions about generalization. After reproducing MedReadMes jargon detection results and extending evaluation to the PLABA dataset we find that transfer learning across datasets yields only modest gains largely due to divergent annotation objectives. Through manual re-annotation we show that aligning labeling schemes improves cross-dataset performance. Building on these findings we evaluate several jargon-aware prompting strategies for LLM-based medical text simplification. Explicitly highlighting jargon in prompts does not consistently improve simplification quality. When gains occur they often trade off against readability and are model-dependent. Human evaluation indicates that simple prompting can be as effective as more complex jargon-aware instructions. We release code to facilitate further research https//anonymous.4open.science/r/tsar-anonymous-2D66F/README.md</abstract>
      <url hash="7be16887">2025.tsar-1.3</url>
      <bibkey>papandreou-etal-2025-medical</bibkey>
    </paper>
    <paper id="4">
      <title>Readability Reconsidered A Cross-Dataset Analysis of Reference-Free Metrics</title>
      <author><first>Catarina</first><last>Belem</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Parker</first><last>Glenn</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Alfy</first><last>Samuel</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Anoop</first><last>Kumar</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Daben</first><last>Liu</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>47-69</pages>
      <abstract>Automatic readability assessment plays a key role in ensuring effective communication between humans and language models. Despite significant progress the field is hindered by inconsistent definitions of readability and measurements that rely on surface-level text properties. In this work we investigate the factors shaping human perceptions of readability through the analysis of 1.2k judgments finding that beyond surface-level cues information content and topic strongly shape text comprehensibility. Furthermore we evaluate 15 popular readability metrics across 5 datasets contrasting them with 5 more nuanced model-based metrics. Our results show that four model-based metrics consistently place among the top 4 in rank correlations with human judgments while the best performing traditional metric achieves an average rank of 7.8. These findings highlight a mismatch between current readability metrics and human perceptions pointing to model-based approaches as a more promising direction.</abstract>
      <url hash="4e8c8691">2025.tsar-1.4</url>
      <bibkey>belem-etal-2025-readability</bibkey>
    </paper>
    <paper id="5">
      <title>Evaluating Health Question Answering Under Readability-Controlled Style Perturbations</title>
      <author><first>Md Mushfiqur</first><last>Rahman</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Kevin</first><last>Lybarger</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>70-86</pages>
      <abstract>Patients often ask semantically similar medical questions in linguistically diverse ways that vary in readability tone and background knowledge. A robust question answering QA system should both provide semantically consistent answers across stylistic differences and adapt its response style to match the users input however existing QA evaluations rarely test this capability creating critical gaps in QA evaluation that undermine accessibility and health literacy. We introduce SPQA an evaluation framework and benchmark that applies controlled stylistic perturbations to consumer health questions while preserving semantic intent then measures how model answers change across correctness completeness coherence fluency and linguistic adaptability using a human-validated LLM-based judge. The style axes include reading level formality and patient background knowledge all perturbations are grounded in human annotations to ensure fidelity and alignment with human judgments. Our contributions include a readability-aware evaluation methodology a style-diverse benchmark with human-grounded perturbations and an automated evaluation pipeline validated against expert judgments. Evaluation results across multiple health QA models indicate that stylistic perturbations lead to measurable performance degradation even when semantic intent is preserved during perturbation. The largest performance drops occur in answer correctness and completeness while models also show limited ability to adapt their style to match the input. These findings underscore the risk of inequitable information delivery and highlight the need for accessibility-aware QA evaluation.</abstract>
      <url hash="85ba7bd5">2025.tsar-1.5</url>
      <bibkey>rahman-lybarger-2025-evaluating</bibkey>
    </paper>
    <paper id="6">
      <title>A Multi-Agent Framework with Diagnostic Feedback for Iterative Plain Language Summary Generation from Cochrane Medical Abstracts</title>
      <author><first>Felipe</first><last>Arias Russi</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Carolina</first><last>Salazar Lara</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Ruben</first><last>Manrique</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>87-104</pages>
      <abstract>Plain Language Summaries PLS improve health literacy and enable informed healthcare decisions but writing them requires domain expertise and is time-consuming. Automated methods often prioritize efficiency over comprehension and medical documents unique simplification requirements challenge generic solutions. We present a multi-agent system for generating PLS using Cochrane PLS as proof of concept. The system uses specialized agents for information extraction writing diagnosis and evaluation integrating a medical glossary and statistical analyzer to guide revisions. We evaluated three architectural configurations on 100 Cochrane abstracts using six LLMs both proprietary and open-source. Results reveal model-dependent trade-offs between factuality and readability with the multi-agent approach showing improvements for smaller models and providing operational advantages in control and interpretability.</abstract>
      <url hash="3e007d25">2025.tsar-1.6</url>
      <bibkey>arias-russi-etal-2025-multi</bibkey>
    </paper>
    <paper id="7">
      <title>Efficient On-Device Text Simplification for Firefox with Synthetic Data Fine-Tuning</title>
      <author><first>Pablo</first><last>Romero</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author id="zihao-li"><first>Zihao</first><last>Li</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <author><first>Matthew</first><last>Shardlow</last><affiliation>&lt;Institution Name&gt;</affiliation></author>
      <pages>105-115</pages>
      <abstract>This work presents a system for on-device text simplification that enables users to process sensitive text without relying on cloud-based services. Through the use of quantization techniques and a novel approach to controllable text simplification we reduce model size by up to 75 percent with minimal performance degradation. Our models demonstrate efficient state-of-the-art results using a synthetic dataset of 2909 examples outperforming prior work trained on 300K examples. This efficiency stems from (1) a single control token strategy that precisely targets specific reading levels (2) a contrastive training approach that enriches model understanding through exposure to multiple simplification levels and (3) individual models that dedicate full parameter capacity to specific reading level transformations. Our best models achieve up to 82.18 BLEU at the Advanced level and 46.12 SARI at the Elementary level on standard benchmarks with performance preserved even after aggressive quantization. This work is implemented as a collaboration with the Mozilla AI team to process text entirely locally ensuring sensitive information never leaves the users device. We have a demonstration video https//youtu.be/TzmaxnARMzg and a web demo available at https//pablorom2004.github.io/Simplification-Web-Demo</abstract>
      <url hash="8733790b">2025.tsar-1.7</url>
      <bibkey>romero-etal-2025-efficient</bibkey>
    </paper>
    <paper id="8">
      <title>Findings of the <fixed-case>TSAR</fixed-case> 2025 Shared Task on Readability-Controlled Text Simplification</title>
      <author><first>Fernando</first><last>Alva-Manchego</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Regina</first><last>Stodden</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Joseph Marvin</first><last>Imperial</last><affiliation>[‘University of Bath’, ‘National University Philippines’]</affiliation></author>
      <author><first>Abdullah</first><last>Barayan</last><affiliation>[‘Cardiff University’, ‘King Abdulaziz University’]</affiliation></author>
      <author><first>Kai</first><last>North</last><affiliation>Cambium Assessment</affiliation></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <pages>116-130</pages>
      <abstract>This paper presents the findings of the first Shared Task on Readability-Controlled Text Simplification at TSAR 2025. The task required systems to simplify English texts to specific target readability levels of the Common European Framework of Reference for Languages (CEFR). We received 48 submissions from 20 participating teams, with approaches predominantly based on large language models (LLMs), which included iterative refinement, multi-agent setups, and LLM-as-a-judge pipelines. For this shared task, we developed a new dataset of pedagogical texts and evaluated submissions using a weighted combination of semantic similarity and CEFR-level accuracy. The results of the participating teams demonstrate that while LLMs can perform substantially well on this task, dependable and controlled simplification often requires complex, multi-iterative processes. Our findings also suggest that the capabilities of current systems are beginning to saturate existing automatic evaluation metrics, underscoring the need for reevaluation and practicality.</abstract>
      <url hash="adb3b633">2025.tsar-1.8</url>
      <bibkey>alva-manchego-etal-2025-findings</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>O</fixed-case>ne<fixed-case>NRC</fixed-case>@<fixed-case>TSAR</fixed-case>2025 Shared Task Small Models for Readability Controlled Text Simplification</title>
      <author><first>Sowmya</first><last>Vajjala</last><affiliation>N/A</affiliation></author>
      <pages>131-136</pages>
      <abstract>In this system description paper, we describe the team OneNRC’s experiments on readability controlled text simplification, focused on using smaller, quantized language models (&lt;20B). We compare these with one large proprietary model and show that the smaller models offer comparable or even better results in some experimental settings. The approach primarily comprises of prompt optimization, agentic workflow, and tool calling. The best results were achieved while using a CEFR proficiency classifier as a verification tool for the language model agent. In terms of comparison with other systems, our submission that used a quantized Gemma3:12B model that ran on a laptop achieved a rank of 9.88 among the submitted systems as per the AUTORANK framework used by the organizers. We hope these results will lead into further exploration on the usefulness of smaller models for text simplification.</abstract>
      <url hash="cff9ddb3">2025.tsar-1.9</url>
      <bibkey>vajjala-2025-onenrc</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>GRIPF</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Towards controlled <fixed-case>CEFR</fixed-case> level simplification with the help of inter-model interactions</title>
      <author><first>David</first><last>Alfter</last><affiliation>N/A</affiliation></author>
      <author><first>Sebastian</first><last>Gombert</last><affiliation>N/A</affiliation></author>
      <pages>137-148</pages>
      <abstract>In this contribution to the CEFR level simplification TSAR 2025 Shared Task, we propose two systems, EZ-SCALAR and SAGA, that implement two differing approaches to prompting LLMs for proficiency-adapted simplification. Our results place us in the middle of the participating teams, and reveal that using external lexical resources to guide simplification improves overall results.</abstract>
      <url hash="cc017cc2">2025.tsar-1.10</url>
      <bibkey>alfter-gombert-2025-gripf</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>ITU</fixed-case> <fixed-case>NLP</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task A Three-Stage Prompting Approach for <fixed-case>CEFR</fixed-case>-Oriented Text Simplification</title>
      <author><first>Kutay Arda</first><last>Dinç</last><affiliation>N/A</affiliation></author>
      <author><first>Fatih</first><last>Bektaş</last><affiliation>N/A</affiliation></author>
      <author><first>Gülşen</first><last>Eryiğit</last><affiliation>N/A</affiliation></author>
      <pages>149-154</pages>
      <abstract>Automatic Text Simplification (TS) makes complex texts more accessible but often lacks control over target readability levels. We propose a lightweight, prompt-based approach to English TS that explicitly aligns outputs with CEFR proficiency standards. Our method employs a three-stage pipeline, guided by rule-informed prompts inspired by expert strategies. In the TSAR 2025 Shared Task, our system achieved competitive performance, with stronger results at B1 level and challenges at A2 level due to over-simplification. These findings highlight the promise of prompt-based CEFR-oriented simplification and the need for more flexible constraint design.</abstract>
      <url hash="6002a3b3">2025.tsar-1.11</url>
      <bibkey>dinc-etal-2025-itu</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>STARLING</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Leveraging Alternative Generations for Readability Level Adjustment in Text Simplification</title>
      <author><first>Piotr</first><last>Przybyła</last><affiliation>N/A</affiliation></author>
      <pages>155-159</pages>
      <abstract>Readability adjustment is crucial in text simplification, as it allows to generate language appropriate to the needs of a particular group of readers. Here we present a method for simplifying a text fragment that aims for a given CEFR level, e.g. A2 or B1. The proposed approach combines prompted large language model with sentence-level adjustment of difficulty level. The work is evaluated within the framework of TSAR 2025 shared task, showing a trade-off between precise readability adjustment and faithful meaning preservation.</abstract>
      <url hash="91919a95">2025.tsar-1.12</url>
      <bibkey>przybyla-2025-starling</bibkey>
    </paper>
    <paper id="13">
      <title>task<fixed-case>G</fixed-case>en at <fixed-case>TSAR</fixed-case> 2025 Shared Task Exploring prompt strategies with linguistic knowledge</title>
      <author><first>Juan Cruz</first><last>Oviedo</last><affiliation>N/A</affiliation></author>
      <author><first>Elisabet</first><last>Comelles Pujadas</last><affiliation>N/A</affiliation></author>
      <author><first>Laura</first><last>Alonso Alemany</last><affiliation>N/A</affiliation></author>
      <author><first>Jordi</first><last>Atserias Batalla</last><affiliation>N/A</affiliation></author>
      <pages>160-172</pages>
      <abstract>TaskGen ranked as 6th best team in the TSAR 2025 shared task for English text adaptation to a target CEFR level. Our experiments consisted of prompting a Llama-3.1-8B-Instruct model with linguistic descriptors of the target level, examples of adaptations and multi-step approaches. Our best run, 13th in the overall ranking, applied an ensemble strategy using a voting mechanism to find the most adequate among 10 texts, each produced by a different prompting strategy.</abstract>
      <url hash="be0b3370">2025.tsar-1.13</url>
      <bibkey>oviedo-etal-2025-taskgen</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>E</fixed-case>asy<fixed-case>J</fixed-case>on at <fixed-case>TSAR</fixed-case> 2025 Shared Task Evaluation of Automated Text Simplification with <fixed-case>LLM</fixed-case>-as-a-Judge</title>
      <author><first>Paul-Gerhard</first><last>Barbu</last><affiliation>N/A</affiliation></author>
      <author><first>Adrianna</first><last>Lipska-Dieck</last><affiliation>N/A</affiliation></author>
      <author><first>Lena</first><last>Lindner</last><affiliation>N/A</affiliation></author>
      <pages>173-182</pages>
      <abstract>This paper presents an approach to automated text simplification for CEFR A2 and B1 levels using large language models and prompt engineering. We evaluate seven models across three prompting strategies short, descriptive, and descriptive with examples. A two-round evaluation system using LLM-as-a-Judge and traditional metrics for text simplification determines optimal model-prompt combinations for final submissions. Results demonstrate that descriptive prompts consistently outperform other strategies across all models, achieving 46-65% of first-place rankings. Qwen3 shows superior performance for A2-level simplification, while B1-level results are more balanced across models. The LLM-as-a-Judge evaluation method shows strong alignment with traditional metrics while providing enhanced explainability.</abstract>
      <url hash="6dc19809">2025.tsar-1.14</url>
      <bibkey>barbu-etal-2025-easyjon</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>HULAT</fixed-case>-<fixed-case>UC</fixed-case>3<fixed-case>M</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task A Prompt-Based Approach using Lightweight Language Models for Readability-Controlled Text Simplification</title>
      <author><first>Jesus M.</first><last>Sanchez-Gomez</last><affiliation>N/A</affiliation></author>
      <author><first>Lourdes</first><last>Moreno</last><affiliation>N/A</affiliation></author>
      <author><first>Paloma</first><last>Martínez</last><affiliation>N/A</affiliation></author>
      <author><first>Marco Antonio</first><last>Sanchez-Escudero</last><affiliation>N/A</affiliation></author>
      <pages>183-192</pages>
      <abstract>This paper describes the participation of the HULAT-UC3M team in the TSAR 2025 Shared Task on Readability-Controlled Text Simplification. Our approach uses open and lightweight Large Language Models (LLMs) with different sizes, together with two strategies for prompt engineering. The proposed system has been tested on the trial data provided, and evaluated using the official metrics CEFR Compliance, Meaning Preservation, and Similarity to References. LLaMA 3 8B model with reinforced prompts was selected as our final proposal for submission, and ranking fourteenth according to the overall metric. Finally, we discuss the main challenges that we identified in developing our approach for this task.</abstract>
      <url hash="51b4b454">2025.tsar-1.15</url>
      <bibkey>sanchez-gomez-etal-2025-hulat</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>U</fixed-case>o<fixed-case>L</fixed-case>-<fixed-case>UPF</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task A Generate-and-Select Approach for Readability-Controlled Text Simplification</title>
      <author><first>Akio</first><last>Hayakawa</last><affiliation>N/A</affiliation></author>
      <author><first>Nouran</first><last>Khallaf</last><affiliation>N/A</affiliation></author>
      <author><first>Horacio</first><last>Saggion</last><affiliation>N/A</affiliation></author>
      <author><first>Serge</first><last>Sharoff</last><affiliation>N/A</affiliation></author>
      <pages>193-210</pages>
      <abstract>The TSAR 2025 Shared Task on Readability-Controlled Text Simplification focuses on simplifying English paragraphs written at an advanced level (B2 or higher) and rewriting them to target CEFR levels (A2 or B1). The challenge is to reduce linguistic complexity without sacrificing coherence or meaning. We developed three complementary approaches based on large language models (LLMs). The first approach (Run 1) generates a diverse set of paragraph-level simplifications. It then applies filters to enforce CEFR alignment, preserve meaning, and encourage diversity, and finally selects the candidates with the lowest perceived risk. The second (Run 2) performs simplification at the sentence level, combining structured prompting, coreference resolution, and explainable AI techniques to highlight influential phrases, with candidate selection guided by automatic and LLM-based judges. The third hybrid approach (Run 3) integrates both strategies by pooling paragraph- and sentence-level simplifications, and subsequently applying the identical filtering and selection architecture used in Run 1. In the official TSAR evaluation, the hybrid system ranked 2nd overall, while its component systems also achieved competitive results.</abstract>
      <url hash="81a04be1">2025.tsar-1.16</url>
      <bibkey>hayakawa-etal-2025-uol</bibkey>
    </paper>
    <paper id="17">
      <title>Uniandes at <fixed-case>TSAR</fixed-case> 2025 Shared Task Multi-Agent <fixed-case>CEFR</fixed-case> Text Simplification with Automated Quality Assessment and Iterative Refinement</title>
      <author><first>Felipe</first><last>Arias Russi</last><affiliation>N/A</affiliation></author>
      <author><first>Kevin</first><last>Cohen Solano</last><affiliation>N/A</affiliation></author>
      <author><first>Ruben</first><last>Manrique</last><affiliation>N/A</affiliation></author>
      <pages>211-216</pages>
      <abstract>We present an agent-based system for the TSAR 2025 Shared Task on Readability-Controlled Text Simplification, which requires simplifying English paragraphs from B2+ levels to target A2 or B1 levels while preserving meaning. Our approach employs specialized agents for keyword extraction, text generation, and evaluation, coordinated through an iterative refinement loop. The system integrates a CEFR vocabulary classifier, pretrained evaluation models, and few-shot learning from trial data. Through iterative feedback between the evaluator and writer agents, our system automatically refines outputs until they meet both readability and semantic preservation constraints. This architecture achieved 4th position among participating teams, showing the effectiveness of combining specialized LLMs with automated quality control strategies for text simplification.</abstract>
      <url hash="2f4d6ba7">2025.tsar-1.17</url>
      <bibkey>arias-russi-etal-2025-uniandes</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>E</fixed-case>hi<fixed-case>M</fixed-case>e<fixed-case>NLP</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Candidate Generation via Iterative Simplification and Reranking by Readability and Semantic Similarity</title>
      <author><first>Rina</first><last>Miyata</last><affiliation>N/A</affiliation></author>
      <author><first>Koki</first><last>Horiguchi</last><affiliation>N/A</affiliation></author>
      <author><first>Risa</first><last>Kondo</last><affiliation>N/A</affiliation></author>
      <author><first>Yuki</first><last>Fujiwara</last><affiliation>N/A</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>N/A</affiliation></author>
      <pages>217-222</pages>
      <abstract>We introduce the EhiMeNLP submission, which won the TSAR 2025 Shared Task on Readability-Controlled Text Simplification. Our system employed a two-step strategy of candidate generation and reranking. For candidate generation, we simplified the given text into more readable versions by combining multiple large language models with prompts. Then, for reranking, we selected the best candidate by readability-based filtering and ranking based on semantic similarity to the original text.</abstract>
      <url hash="17c6e2e5">2025.tsar-1.18</url>
      <bibkey>miyata-etal-2025-ehimenlp</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>OUNLP</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Multi-Round Text Simplifier via Code Generation</title>
      <author><first>Cuong</first><last>Huynh</last><affiliation>N/A</affiliation></author>
      <author><first>Jie</first><last>Cao</last><affiliation>N/A</affiliation></author>
      <pages>223-230</pages>
      <abstract>This paper describes the system submission of our team OUNLP to the TSAR-2025 shared task on readability-controlled text simplification. Based on the analysis of prompt-based text simplification methods, we discovered that simplification performance is highly related to the gap between the source CEFR level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods generated via GPT-4o rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7th out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost multi-round simplification performance.</abstract>
      <url hash="e172f1aa">2025.tsar-1.19</url>
      <bibkey>huynh-cao-2025-ounlp</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>HIT</fixed-case>-<fixed-case>YOU</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Leveraging Similarity-Based Few-Shot Prompting, Round-Trip Translation, and Self-Refinement for Readability-Controlled Text Simplification</title>
      <author><first>Mao</first><last>Shimada</last><affiliation>N/A</affiliation></author>
      <author><first>Kexin</first><last>Bian</last><affiliation>N/A</affiliation></author>
      <author><first>Zhidong</first><last>Ling</last><affiliation>N/A</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>N/A</affiliation></author>
      <pages>231-241</pages>
      <abstract>We describe our submission to the TSAR 2025 shared task on readability-controlled text simplification, which evaluates systems on their ability to adjust linguistic complexity to specified CEFR levels while preserving meaning and coherence. We explored two complementary frameworks leveraging the shared task CEFR classifier as feedback. The first is an ensemble approach generating diverse candidates using multiple LLMs under zero-shot prompting with level-specific instructions and vocabulary lists, one-shot prompting, and round-trip translation. Candidates were filtered by predicted CEFR level before an LLM judge selected the final output. The second framework is a self-refinement loop, where a single candidate is iteratively revised with classifier feedback until matching the target level or reaching a maximum number of iterations. This study is among the first to apply round-trip translation and iterative self-refinement to controlled simplification, broadening the toolkit for adapting linguistic complexity.</abstract>
      <url hash="92ac2fb7">2025.tsar-1.20</url>
      <bibkey>shimada-etal-2025-hit</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>SQUREL</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task <fixed-case>CEFR</fixed-case>-Controlled Text Simplification with Prompting and Reinforcement Fine-Tuning</title>
      <author><first>Daria</first><last>Sokova</last><affiliation>N/A</affiliation></author>
      <author><first>Anastasiia</first><last>Bezobrazova</last><affiliation>N/A</affiliation></author>
      <author><first>Constantin</first><last>Orasan</last><affiliation>N/A</affiliation></author>
      <pages>242-250</pages>
      <abstract>This paper summarises the submissions of our team to the TSAR 2025 Shared Task on Readability-Controlled Text Simplification, which aims to create text simplifications balancing reduced linguistic complexity, meaning preservation, and fluency while meeting predefined target readability levels. We tested two different methods for CEFR-controlled simplification a conservative lexical pipeline relying on prompting LLMs to simplify sentences, and a setup employing reinforcement fine-tuning.</abstract>
      <url hash="a4ced9d3">2025.tsar-1.21</url>
      <bibkey>sokova-etal-2025-squrel</bibkey>
    </paper>
    <paper id="22">
      <title>Archaeology at <fixed-case>TSAR</fixed-case> 2025 Shared Task Teaching Small Models to do <fixed-case>CEFR</fixed-case> Simplifications</title>
      <author><first>Rares-Alexandru</first><last>Roscan</last><affiliation>N/A</affiliation></author>
      <author><first>Sergiu</first><last>Nisioi</last><affiliation>N/A</affiliation></author>
      <pages>251-260</pages>
      <abstract>Large language models (LLMs) have demonstrated strong performance in text simplification tasks, but their high computational cost and proprietary nature often limit practical use, especially in education. We explore open-source LLMs for CEFR-level text simplification. By reducing model size and computational requirements, our approach enables greater accessibility and deployment in educational environments. Our results show some of the lowest error rates in producing CEFR-compliant texts at TSAR 2025, using models with 8 billion and 1 billion parameters. Such approaches have the potential to democratize NLP technologies for real-world applications.</abstract>
      <url hash="159158bf">2025.tsar-1.22</url>
      <bibkey>roscan-nisioi-2025-archaeology</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>HOPE</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Balancing Control and Complexity in Readability-Controlled Text Simplification</title>
      <author><first>Sujal</first><last>Maharjan</last><affiliation>N/A</affiliation></author>
      <author><first>Astha</first><last>Shrestha</last><affiliation>N/A</affiliation></author>
      <pages>261-265</pages>
      <abstract>This paper describes our submissions to the TSAR 2025 Shared Task on Readability-Controlled Text Simplification. We present a comparative study of three architectures a rule-based baseline, a heuristic-driven expert system, and a zero-shot generative T5 pipeline with a semantic guardrail. Our analysis shows a trade-off between the controllability of rule-based systems and the fluency of generative models. In this zero-shot setting, simpler, confined systems achieved superior meaning preservation scores compared to the more powerful but less predictable generative model. We present a diagnostic failure analysis on system outputs, illustrating how different architectures result in distinct error patterns such as under-simplification, information loss via heuristics, and semantic drift.</abstract>
      <url hash="b078e58f">2025.tsar-1.23</url>
      <bibkey>maharjan-shrestha-2025-hope</bibkey>
    </paper>
    <paper id="24">
      <title>Know-<fixed-case>AI</fixed-case> at <fixed-case>TSAR</fixed-case> 2025 Shared Task Difficulty-aware Text Simplification System</title>
      <author><first>Yiheng</first><last>Wu</last><affiliation>N/A</affiliation></author>
      <author><first>Anisia</first><last>Katinskaia</last><affiliation>N/A</affiliation></author>
      <author><first>Jue</first><last>Hou</last><affiliation>N/A</affiliation></author>
      <author><first>Roman</first><last>Yangarber</last><affiliation>N/A</affiliation></author>
      <pages>266-272</pages>
      <abstract>Text simplification is an active research topic with applications in multiple domains. In a simplification pipeline, assessment of text difficulty plays a crucial role as a quality control mechanism it acts as a critic and guides models to generate text at the difficulty level required by the user. This paper presents our Difficulty-aware Text Simplification System. We evaluate our pipeline using the TSAR shared task dataset and discuss challenges in constructing corpora for training models to assess text difficulty.</abstract>
      <url hash="1a45c1ce">2025.tsar-1.24</url>
      <bibkey>wu-etal-2025-know</bibkey>
    </paper>
  </volume>
</collection>
