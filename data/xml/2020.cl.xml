<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.cl">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 46, Issue 1 - March 2020</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2020</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>On the Linguistic Representational Power of Neural Machine Translation Models</title>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>James</first><last>Glass</last></author>
      <doi>10.1162/coli_a_00367</doi>
      <abstract>Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.</abstract>
      <pages>1–52</pages>
      <url hash="72e17aed">2020.cl-1.1</url>
      <bibkey>belinkov-etal-2020-linguistic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl">Europarl</pwcdataset>
    </paper>
    <paper id="2">
      <title>The Design and Implementation of <fixed-case>X</fixed-case>iao<fixed-case>I</fixed-case>ce, an Empathetic Social Chatbot</title>
      <author><first>Li</first><last>Zhou</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Di</first><last>Li</last></author>
      <author><first>Heung-Yeung</first><last>Shum</last></author>
      <doi>10.1162/coli_a_00368</doi>
      <abstract>This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human–machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations.</abstract>
      <pages>53–93</pages>
      <url hash="5be02ff3">2020.cl-1.2</url>
      <bibkey>zhou-etal-2020-design</bibkey>
    </paper>
    <paper id="3">
      <title>An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models</title>
      <author><first>Shudong</first><last>Hao</last></author>
      <author><first>Michael J.</first><last>Paul</last></author>
      <doi>10.1162/coli_a_00369</doi>
      <abstract>Probabilistic topic modeling is a common first step in crosslingual tasks to enable knowledge transfer and extract multilingual features. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different models can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four models on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.</abstract>
      <pages>95–134</pages>
      <url hash="1a2d5806">2020.cl-1.3</url>
      <bibkey>hao-paul-2020-empirical</bibkey>
    </paper>
    <paper id="4">
      <title>Data-Driven Sentence Simplification: Survey and Benchmark</title>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <doi>10.1162/coli_a_00370</doi>
      <abstract>Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several rewriting transformations can be performed such as replacement, reordering, and splitting. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in English, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments.</abstract>
      <pages>135–187</pages>
      <url hash="dc68839b">2020.cl-1.4</url>
      <bibkey>alva-manchego-etal-2020-data</bibkey>
    </paper>
    <paper id="5">
      <title>Corpora Annotated with Negation: An Overview</title>
      <author><first>Salud María</first><last>Jiménez-Zafra</last></author>
      <author><first>Roser</first><last>Morante</last></author>
      <author><first>María Teresa</first><last>Martín-Valdivia</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <doi>10.1162/coli_a_00371</doi>
      <abstract>Negation is a universal linguistic phenomenon with a great qualitative impact on natural language processing applications. The availability of corpora annotated with negation is essential to training negation processing systems. Currently, most corpora have been annotated for English, but the presence of languages other than English on the Internet, such as Chinese or Spanish, is greater every day. In this study, we present a review of the corpora annotated with negation information in several languages with the goal of evaluating what aspects of negation have been annotated and how compatible the corpora are. We conclude that it is very difficult to merge the existing corpora because we found differences in the annotation schemes used, and most importantly, in the annotation guidelines: the way in which each corpus was tokenized and the negation elements that have been annotated. Differently than for other well established tasks like semantic role labeling or parsing, for negation there is no standard annotation scheme nor guidelines, which hampers progress in its treatment.</abstract>
      <pages>1–52</pages>
      <url hash="64d1b623">2020.cl-1.5</url>
      <bibkey>jimenez-zafra-etal-2020-corpora</bibkey>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 46, Issue 2 - June 2020</booktitle>
      <month>June</month>
      <year>2020</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction</title>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <doi>10.1162/coli_a_00373</doi>
      <abstract>We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue’s five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives.</abstract>
      <pages>249–255</pages>
      <url hash="1ea1e59e">2020.cl-2.1</url>
      <bibkey>costa-jussa-etal-2020-multilingual</bibkey>
    </paper>
    <paper id="2">
      <title>Unsupervised Word Translation with Adversarial Autoencoder</title>
      <author><first>Tasnim</first><last>Mohiuddin</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <doi>10.1162/coli_a_00374</doi>
      <abstract>Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with high- and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.</abstract>
      <pages>257–288</pages>
      <url hash="74f783bb">2020.cl-2.2</url>
      <bibkey>mohiuddin-joty-2020-unsupervised</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>L</fixed-case>ess<fixed-case>L</fixed-case>ex: Linking Multilingual Embeddings to <fixed-case>S</fixed-case>en<fixed-case>S</fixed-case>e Representations of <fixed-case>LEX</fixed-case>ical Items</title>
      <author><first>Davide</first><last>Colla</last></author>
      <author><first>Enrico</first><last>Mensa</last></author>
      <author><first>Daniele P.</first><last>Radicioni</last></author>
      <doi>10.1162/coli_a_00375</doi>
      <abstract>We present LESSLEX, a novel multilingual lexical resource. Different from the vast majority of existing approaches, we ground our embeddings on a sense inventory made available from the BabelNet semantic network. In this setting, multilingual access is governed by the mapping of terms onto their underlying sense descriptions, such that all vectors co-exist in the same semantic space. As a result, for each term we have thus the “blended” terminological vector along with those describing all senses associated to that term. LESSLEX has been tested on three tasks relevant to lexical semantics: conceptual similarity, contextual similarity, and semantic text similarity. We experimented over the principal data sets for such tasks in their multilingual and crosslingual variants, improving on or closely approaching state-of-the-art results. We conclude by arguing that LESSLEX vectors may be relevant for practical applications and for research on conceptual and lexical access and competence.</abstract>
      <pages>289–333</pages>
      <url hash="69547672">2020.cl-2.3</url>
      <bibkey>colla-etal-2020-lesslex</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>LINSPECTOR</fixed-case>: Multilingual Probing Tasks for Word Representations</title>
      <author><first>Gözde Gül</first><last>Şahin</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <author><first>Ilia</first><last>Kuznetsov</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00376</doi>
      <abstract>Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.</abstract>
      <pages>335–385</pages>
      <url hash="bcb62861">2020.cl-2.4</url>
      <bibkey>sahin-etal-2020-linspector</bibkey>
      <pwccode url="https://github.com/UKPLab/linspector" additional="true">UKPLab/linspector</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="5">
      <title>A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <doi>10.1162/coli_a_00377</doi>
      <abstract>Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning. We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability to encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks.</abstract>
      <pages>387–424</pages>
      <url hash="a8a898e3">2020.cl-2.5</url>
      <bibkey>vazquez-etal-2020-systematic</bibkey>
    </paper>
    <paper id="6">
      <title>Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework from Controlled Languages to Robust Pipelines</title>
      <author><first>Aarne</first><last>Ranta</last></author>
      <author><first>Krasimir</first><last>Angelov</last></author>
      <author><first>Normunds</first><last>Gruzitis</last></author>
      <author><first>Prasanth</first><last>Kolachina</last></author>
      <doi>10.1162/coli_a_00378</doi>
      <abstract>Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering tools and components for mobile and Web applications. On the research side, the focus in the last ten years has been on scaling up GF to wide-coverage language processing. The concept of abstract syntax offers a unified view on many other approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars, and Abstract Meaning Representations. This makes it possible for GF to utilize data from the other approaches and to build robust pipelines. In return, GF can contribute to data-driven approaches by methods to transfer resources from one language to others, to augment data by rule-based generation, to check the consistency of hand-annotated corpora, and to pipe analyses into high-precision semantic back ends. This article gives an overview of the use of abstract syntax as interlingua through both established and emerging NLP applications involving GF.</abstract>
      <pages>425–486</pages>
      <url hash="ad744fa7">2020.cl-2.6</url>
      <bibkey>ranta-etal-2020-abstract</bibkey>
    </paper>
    <paper id="7">
      <title>Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor</title>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <doi>10.1162/coli_a_00379</doi>
      <abstract>Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.</abstract>
      <pages>487–497</pages>
      <url hash="e64e92e7">2020.cl-2.7</url>
      <bibkey>nissim-etal-2020-fair</bibkey>
    </paper>
    <paper id="8">
      <title>The Limitations of Stylometry for Detecting Machine-Generated Fake News</title>
      <author><first>Tal</first><last>Schuster</last></author>
      <author><first>Roei</first><last>Schuster</last></author>
      <author><first>Darsh J.</first><last>Shah</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <doi>10.1162/coli_a_00380</doi>
      <abstract>Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.</abstract>
      <pages>499–510</pages>
      <url hash="fb11029c">2020.cl-2.8</url>
      <bibkey>schuster-etal-2020-limitations</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 46, Issue 3 - September 2020</booktitle>
      <month>September</month>
      <year>2020</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Tractable <fixed-case>L</fixed-case>exical-<fixed-case>F</fixed-case>unctional <fixed-case>G</fixed-case>rammar</title>
      <author><first>Jürgen</first><last>Wedekind</last></author>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <doi>10.1162/coli_a_00384</doi>
      <abstract>The formalism for Lexical-Functional Grammar (LFG) was introduced in the 1980s as one of the first constraint-based grammatical formalisms for natural language. It has led to substantial contributions to the linguistic literature and to the construction of large-scale descriptions of particular languages. Investigations of its mathematical properties have shown that, without further restrictions, the recognition, emptiness, and generation problems are undecidable, and that they are intractable in the worst case even with commonly applied restrictions. However, grammars of real languages appear not to invoke the full expressive power of the formalism, as indicated by the fact that algorithms and implementations for recognition and generation have been developed that run—even for broad-coverage grammars—in typically polynomial time. This article formalizes some restrictions on the notation and its interpretation that are compatible with conventions and principles that have been implicit or informally stated in linguistic theory. We show that LFG grammars that respect these restrictions, while still suitable for the description of natural languages, are equivalent to linear context-free rewriting systems and allow for tractable computation.</abstract>
      <pages>515–569</pages>
      <url hash="080de4be">2020.cl-3.1</url>
      <bibkey>wedekind-kaplan-2020-tractable</bibkey>
    </paper>
    <paper id="2">
      <title>Semantic Drift in Multilingual Representations</title>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <doi>10.1162/coli_a_00382</doi>
      <abstract>Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a methodology for comparing languages based on their organization of semantic concepts. We propose to conduct an adapted version of representational similarity analysis of a selected set of concepts in computational multilingual representations. Using this analysis method, we can reconstruct a phylogenetic tree that closely resembles those assumed by linguistic experts. These results indicate that multilingual distributional representations that are only trained on monolingual text and bilingual dictionaries preserve relations between languages without the need for any etymological information. In addition, we propose a measure to identify semantic drift between language families. We perform experiments on word-based and sentence-based multilingual models and provide both quantitative results and qualitative examples. Analyses of semantic drift in multilingual representations can serve two purposes: They can indicate unwanted characteristics of the computational models and they provide a quantitative means to study linguistic phenomena across languages.</abstract>
      <pages>571–603</pages>
      <url hash="67f25819">2020.cl-3.2</url>
      <bibkey>beinborn-choenni-2020-semantic</bibkey>
      <pwccode url="https://github.com/beinborn/SemanticDrift" additional="false">beinborn/SemanticDrift</pwccode>
    </paper>
    <paper id="3">
      <title>Sentence Meaning Representations Across Languages: What Can We Learn from Existing Frameworks?</title>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Magda</first><last>Ševčíková</last></author>
      <doi>10.1162/coli_a_00385</doi>
      <abstract>This article gives an overview of how sentence meaning is represented in eleven deep-syntactic frameworks, ranging from those based on linguistic theories elaborated for decades to rather lightweight NLP-motivated approaches. We outline the most important characteristics of each framework and then discuss how particular language phenomena are treated across those frameworks, while trying to shed light on commonalities as well as differences.</abstract>
      <pages>605–665</pages>
      <url hash="03083893">2020.cl-3.3</url>
      <bibkey>zabokrtsky-etal-2020-sentence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nombank">NomBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="4">
      <title>Predicting In-Game Actions from Interviews of <fixed-case>NBA</fixed-case> Players</title>
      <author><first>Nadav</first><last>Oved</last></author>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/coli_a_00383</doi>
      <abstract>Sports competitions are widely researched in computer and social science, with the goal of understanding how players act under uncertainty. Although there is an abundance of computational work on player metrics prediction based on past performance, very few attempts to incorporate out-of-game signals have been made. Specifically, it was previously unclear whether linguistic signals gathered from players’ interviews can add information that does not appear in performance metrics. To bridge that gap, we define text classification tasks of predicting deviations from mean in NBA players’ in-game actions, which are associated with strategic choices, player behavior, and risk, using their choice of language prior to the game. We collected a data set of transcripts from key NBA players’ pre-game interviews and their in-game performance metrics, totalling 5,226 interview-metric pairs. We design neural models for players’ action prediction based on increasingly more complex aspects of the language signals in their open-ended interviews. Our models can make their predictions based on the textual signal alone, or on a combination of that signal with signals from past-performance metrics. Our text-based models outperform strong baselines trained on performance metrics only, demonstrating the importance of language usage for action prediction. Moreover, the models that utilize both textual input and past-performance metrics produced the best results. Finally, as neural networks are notoriously difficult to interpret, we propose a method for gaining further insight into what our models have learned. Particularly, we present a latent Dirichlet allocation–based analysis, where we interpret model predictions in terms of correlated topics. We find that our best performing textual model is most associated with topics that are intuitively related to each prediction task and that better models yield higher correlation with more informative topics.1</abstract>
      <pages>667–712</pages>
      <url hash="4bd67c7a">2020.cl-3.4</url>
      <bibkey>oved-etal-2020-predicting</bibkey>
      <pwccode url="https://github.com/nadavo/mood" additional="true">nadavo/mood</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Computational Linguistics, Volume 46, Issue 4 - December 2020</booktitle>
      <month>December</month>
      <year>2020</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Sparse Transcription</title>
      <author><first>Steven</first><last>Bird</last></author>
      <doi>10.1162/coli_a_00387</doi>
      <abstract>The transcription bottleneck is often cited as a major obstacle for efforts to document the world’s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine translation, and recruit linguists to provide narrow phonetic transcriptions and sentence-aligned translations. However, I believe that these approaches are not a good fit with the available data and skills, or with long-established practices that are essentially word-based. In seeking a more effective approach, I consider a century of transcription practice and a wide range of computational approaches, before proposing a computational model based on spoken term detection that I call “sparse transcription.” This represents a shift away from current assumptions that we transcribe phones, transcribe fully, and transcribe first. Instead, sparse transcription combines the older practice of word-level transcription with interpretive, iterative, and interactive processes that are amenable to wider participation and that open the way to new methods for processing oral languages.</abstract>
      <pages>713–744</pages>
      <url hash="ad1be6ac">2020.cl-4.1</url>
      <bibkey>bird-2020-sparse</bibkey>
    </paper>
    <paper id="2">
      <title>Efficient Outside Computation</title>
      <author><first>Daniel</first><last>Gildea</last></author>
      <doi>10.1162/coli_a_00386</doi>
      <abstract>Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations. For some operations, inside values can be computed efficiently, but outside values cannot. We view out-side values as functions from inside values to the total value of all derivations, and we analyze outside computation in terms of function composition. This viewpoint helps explain why efficient outside computation is possible in many settings, despite the lack of a general outside algorithm for semiring operations.</abstract>
      <pages>745–762</pages>
      <url hash="ac8bc8f0">2020.cl-4.2</url>
      <bibkey>gildea-2020-efficient</bibkey>
    </paper>
    <paper id="3">
      <title>What Should/Do/Can <fixed-case>LSTM</fixed-case>s Learn When Parsing Auxiliary Verb Constructions?</title>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <doi>10.1162/coli_a_00392</doi>
      <abstract>There is a growing interest in investigating what neural NLP models learn about language. A prominent open question is the question of whether or not it is necessary to model hierarchical structure. We present a linguistic investigation of a neural parser adding insights to this question. We look at transitivity and agreement information of auxiliary verb constructions (AVCs) in comparison to finite main verbs (FMVs). This comparison is motivated by theoretical work in dependency grammar and in particular the work of Tesnière (1959), where AVCs and FMVs are both instances of a nucleus, the basic unit of syntax. An AVC is a dissociated nucleus; it consists of at least two words, and an FMV is its non-dissociated counterpart, consisting of exactly one word. We suggest that the representation of AVCs and FMVs should capture similar information. We use diagnostic classifiers to probe agreement and transitivity information in vectors learned by a transition-based neural parser in four typologically different languages. We find that the parser learns different information about AVCs and FMVs if only sequential models (BiLSTMs) are used in the architecture but similar information when a recursive layer is used. We find explanations for why this is the case by looking closely at how information is learned in the network and looking at what happens with different dependency representations of AVCs. We conclude that there may be benefits to using a recursive layer in dependency parsing and that we have not yet found the best way to integrate it in our parsers.</abstract>
      <pages>763–784</pages>
      <url hash="58de7bb1">2020.cl-4.3</url>
      <bibkey>de-lhoneux-etal-2020-lstms</bibkey>
      <video href="2020.cl-4.3.mp4"/>
      <pwccode url="https://github.com/mdelhoneux/avc_analyser" additional="false">mdelhoneux/avc_analyser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="4">
      <title>A Graph-Based Framework for Structured Prediction Tasks in <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Bishal</first><last>Santra</last></author>
      <author><first>Ashim</first><last>Gupta</last></author>
      <author><first>Pavankumar</first><last>Satuluri</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <doi>10.1162/coli_a_00390</doi>
      <abstract>We propose a framework using energy-based models for multiple structured prediction tasks in Sanskrit. Ours is an arc-factored model, similar to the graph-based parsing approaches, and we consider the tasks of word segmentation, morphological parsing, dependency parsing, syntactic linearization, and prosodification, a “prosody-level” task we introduce in this work. Ours is a search-based structured prediction framework, which expects a graph as input, where relevant linguistic information is encoded in the nodes, and the edges are then used to indicate the association between these nodes. Typically, the state-of-the-art models for morphosyntactic tasks in morphologically rich languages still rely on hand-crafted features for their performance. But here, we automate the learning of the feature function. The feature function so learned, along with the search space we construct, encode relevant linguistic information for the tasks we consider. This enables us to substantially reduce the training data requirements to as low as 10%, as compared to the data requirements for the neural state-of-the-art models. Our experiments in Czech and Sanskrit show the language-agnostic nature of the framework, where we train highly competitive models for both the languages. Moreover, our framework enables us to incorporate language-specific constraints to prune the search space and to filter the candidates during inference. We obtain significant improvements in morphosyntactic tasks for Sanskrit by incorporating language-specific constraints into the model. In all the tasks we discuss for Sanskrit, we either achieve state-of-the-art results or ours is the only data-driven solution for those tasks.</abstract>
      <pages>785–845</pages>
      <url hash="1ba10a98">2020.cl-4.4</url>
      <bibkey>krishna-etal-2020-graph</bibkey>
    </paper>
    <paper id="5">
      <title>Multi-<fixed-case>S</fixed-case>im<fixed-case>L</fixed-case>ex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Simon</first><last>Baker</last></author>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Ulla</first><last>Petti</last></author>
      <author><first>Ira</first><last>Leviant</last></author>
      <author><first>Kelly</first><last>Wing</last></author>
      <author><first>Olga</first><last>Majewska</last></author>
      <author><first>Eden</first><last>Bar</last></author>
      <author><first>Matt</first><last>Malone</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/coli_a_00391</doi>
      <abstract>We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex–style resources for additional languages. We make these contributions—the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.</abstract>
      <pages>847–897</pages>
      <url hash="566ee6e8">2020.cl-4.5</url>
      <bibkey>vulic-etal-2020-multi</bibkey>
      <video href="2020.cl-4.5.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/card-660">CARD-660</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/panlex">Panlex</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
  </volume>
</collection>
