<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlp4hr">
  <volume id="1" ingest-date="2024-03-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Natural Language Processing for Human Resources (NLP4HR 2024)</booktitle>
      <editor><first>Estevam</first><last>Hruschka</last></editor>
      <editor><first>Thom</first><last>Lake</last></editor>
      <editor><first>Naoki</first><last>Otani</last></editor>
      <editor><first>Tom</first><last>Mitchell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="5b838bb4">2024.nlp4hr-1</url>
      <venue>nlp4hr</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="b00febb6">2024.nlp4hr-1.0</url>
      <bibkey>nlp4hr-2024-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings</title>
      <author><first>Elena</first><last>Senger</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Mike</first><last>Zhang</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob</first><last>Goot</last></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>1-15</pages>
      <abstract>Recent years have brought significant advances to Natural Language Processing (NLP), which enabled fast progress in the field of computational job market analysis. Core tasks in this application domain are skill extraction and classification from job postings. Because of its quick growth and its interdisciplinary nature, there is no exhaustive assessment of this field. This survey aims to fill this gap by providing a comprehensive overview of deep learning methodologies, datasets, and terminologies specific to NLP-driven skill extraction. Our comprehensive cataloging of publicly available datasets addresses the lack of consolidated information on dataset creation and characteristics. Finally, the focus on terminology addresses the current lack of consistent definitions for important concepts, such as hard and soft skills, and terms relating to skill extraction and classification.</abstract>
      <url hash="462face8">2024.nlp4hr-1.1</url>
      <bibkey>senger-etal-2024-deep</bibkey>
    </paper>
    <paper id="2">
      <title>Aspect-Based Sentiment Analysis for Open-Ended <fixed-case>HR</fixed-case> Survey Responses</title>
      <author><first>Lois</first><last>Rink</last></author>
      <author><first>Job</first><last>Meijdam</last><affiliation>NA</affiliation></author>
      <author><first>David</first><last>Graus</last></author>
      <pages>16-26</pages>
      <abstract>Understanding preferences, opinions, and sentiment of the workforce is paramount for effective employee lifecycle management. Open-ended survey responses serve as a valuable source of information. This paper proposes a machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch open-ended responses in employee satisfaction surveys. Our approach aims to overcome the inherent noise and variability in these responses, enabling a comprehensive analysis of sentiments that can support employee lifecycle management. Through response clustering we identify six key aspects (salary, schedule, contact, communication, personal attention, agreements), which we validate by domain experts. We compile a dataset of 1,458 Dutch survey responses, revealing label imbalance in aspects and sentiments. We propose few-shot approaches for ABSA based on Dutch BERT models, and compare them against bag-of-words and zero-shot baselines.Our work significantly contributes to the field of ABSA by demonstrating the first successful application of Dutch pre-trained language models to aspect-based sentiment analysis in the domain of human resources (HR).</abstract>
      <url hash="dffe1ba7">2024.nlp4hr-1.2</url>
      <bibkey>rink-etal-2024-aspect</bibkey>
    </paper>
    <paper id="3">
      <title>Rethinking Skill Extraction in the Job Market Domain using Large Language Models</title>
      <author><first>Khanh</first><last>Nguyen</last></author>
      <author><first>Mike</first><last>Zhang</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Syrielle</first><last>Montariol</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <pages>27-42</pages>
      <abstract>Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.</abstract>
      <url hash="2f4cb08e">2024.nlp4hr-1.3</url>
      <bibkey>nguyen-etal-2024-rethinking</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>J</fixed-case>ob<fixed-case>S</fixed-case>kape: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching</title>
      <author><first>Antoine</first><last>Magron</last></author>
      <author><first>Anna</first><last>Dai</last></author>
      <author><first>Mike</first><last>Zhang</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Syrielle</first><last>Montariol</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <pages>43-58</pages>
      <abstract>Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.</abstract>
      <url hash="7481b3df">2024.nlp4hr-1.4</url>
      <bibkey>magron-etal-2024-jobskape</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>HR</fixed-case>-<fixed-case>M</fixed-case>ulti<fixed-case>WOZ</fixed-case>: A Task Oriented Dialogue (<fixed-case>TOD</fixed-case>) Dataset for <fixed-case>HR</fixed-case> <fixed-case>LLM</fixed-case> Agent</title>
      <author><first>Weijie</first><last>Xu</last></author>
      <author><first>Zicheng</first><last>Huang</last></author>
      <author><first>Wenxiang</first><last>Hu</last><affiliation>Amazon</affiliation></author>
      <author><first>Xi</first><last>Fang</last></author>
      <author><first>Rajesh</first><last>Cherukuri</last></author>
      <author><first>Naumaan</first><last>Nayyar</last></author>
      <author><first>Lorenzo</first><last>Malandri</last><affiliation>University of Milan - Bicocca</affiliation></author>
      <author><first>Srinivasan</first><last>Sengamedu</last><affiliation>Amazon</affiliation></author>
      <pages>59-72</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains. Our work has the following contributions:(1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferrable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.</abstract>
      <url hash="71017dde">2024.nlp4hr-1.5</url>
      <bibkey>xu-etal-2024-hr</bibkey>
    </paper>
    <paper id="6">
      <title>Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models</title>
      <author><first>Charlie</first><last>Campanella</last><affiliation>Indeed</affiliation></author>
      <author><first>Rob</first><last>Goot</last></author>
      <pages>73-77</pages>
      <abstract>Large language models have emerged as a useful technology for job matching, for both candidates and employers. Job matching is often based on a particular geographic location, such as a city or region. However, LMs have known biases, commonly derived from their training data. In this work, we aim to quantify the metropolitan size bias encoded within large language models, evaluating zero-shot salary, employer presence, and commute duration predictions in 384 of the United States’ metropolitan regions. Across all benchmarks, we observe correlations between metropolitan population and the accuracy of predictions, with the smallest 10 metropolitan regions showing upwards of 300% worse benchmark performance than the largest 10.</abstract>
      <url hash="4751dc5b">2024.nlp4hr-1.6</url>
      <bibkey>campanella-goot-2024-big</bibkey>
    </paper>
  </volume>
</collection>
