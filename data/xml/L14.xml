<?xml version='1.0' encoding='UTF-8'?>
<collection id="L14">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Ninth International Conference on Language Resources and Evaluation (<fixed-case>LREC</fixed-case>'14)</booktitle>
      <editor><first>Nicoletta</first><last>Calzolari</last></editor>
      <editor><first>Khalid</first><last>Choukri</last></editor>
      <editor><first>Thierry</first><last>Declerck</last></editor>
      <editor><first>Hrafn</first><last>Loftsson</last></editor>
      <editor><first>Bente</first><last>Maegaard</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Asuncion</first><last>Moreno</last></editor>
      <editor><first>Jan</first><last>Odijk</last></editor>
      <editor><first>Stelios</first><last>Piperidis</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Reykjavik, Iceland</address>
      <month>May</month>
      <year>2014</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <author><first>Ben</first><last>Verhoeven</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <title><fixed-case>CL</fixed-case>i<fixed-case>PS</fixed-case> Stylometry Investigation (<fixed-case>CSI</fixed-case>) corpus: A <fixed-case>D</fixed-case>utch corpus for the detection of age, gender, personality, sentiment and deception in text</title>
      <pages>3081–3085</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1_Paper.pdf</url>
      <abstract>We present the CLiPS Stylometry Investigation (CSI) corpus, a new Dutch corpus containing reviews and essays written by university students. It is designed to serve multiple purposes: detection of age, gender, authorship, personality, sentiment, deception, topic and genre. Another major advantage is its planned yearly expansion with each year’s new students. The corpus currently contains about 305,000 tokens spread over 749 documents. The average review length is 128 tokens; the average essay length is 1126 tokens. The corpus will be made available on the CLiPS website (www.clips.uantwerpen.be/datasets) and can freely be used for academic research purposes. An initial deception detection experiment was performed on this data. Deception detection is the task of automatically classifying a text as being either truthful or deceptive, in our case by examining the writing style of the author. This task has never been investigated for Dutch before. We performed a supervised machine learning experiment using the SVM algorithm in a 10-fold cross-validation setup. The only features were the token unigrams present in the training data. Using this simple method, we reached a state-of-the-art F-score of 72.2%.</abstract>
    </paper>
    <paper id="2">
      <author><first>Vasile</first><last>Rus</last></author>
      <author><first>Rajendra</first><last>Banjade</last></author>
      <author><first>Mihai</first><last>Lintean</last></author>
      <title>On Paraphrase Identification Corpora</title>
      <pages>2422–2429</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1000_Paper.pdf</url>
      <abstract>We analyze in this paper a number of data sets proposed over the last decade or so for the task of paraphrase identification. The goal of the analysis is to identify the advantages as well as shortcomings of the previously proposed data sets. Based on the analysis, we then make recommendations about how to improve the process of creating and using such data sets for evaluating in the future approaches to the task of paraphrase identification or the more general task of semantic similarity. The recommendations are meant to improve our understanding of what a paraphrase is, offer a more fair ground for comparing approaches, increase the diversity of actual linguistic phenomena that future data sets will cover, and offer ways to improve our understanding of the contributions of various modules or approaches proposed for solving the task of paraphrase identification or similar tasks.</abstract>
    </paper>
    <paper id="3">
      <author><first>Wiltrud</first><last>Kessler</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>A Corpus of Comparisons in Product Reviews</title>
      <pages>2242–2248</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1001_Paper.pdf</url>
      <abstract>Sentiment analysis (or opinion mining) deals with the task of determining the polarity of an opinionated document or sentence. Users often express sentiment about one product by comparing it to a different product. In this work, we present a corpus of comparison sentences from English camera reviews. For our purposes we define a comparison to be any statement about the similarity or difference of two entities. For each sentence we have annotated detailed information about the comparisons it contains: The comparative predicate that expresses the comparison, the type of the comparison, the two entities that are being compared, and the aspect they are compared in. The results of our agreement study show that the decision whether a sentence contains a comparison is difficult to make even for trained human annotators. Once that decision is made, we can achieve consistent results for the very detailed annotations. In total, we have annotated 2108 comparisons in 1707 sentences from camera reviews which makes our corpus the largest resource currently available. The corpus and the annotation guidelines are publicly available on our website.</abstract>
    </paper>
    <paper id="4">
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Ewa</first><last>Kijak</last></author>
      <title>Generating and using probabilistic morphological resources for the biomedical domain</title>
      <pages>3348–3354</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1003_Paper.pdf</url>
      <abstract>In most Indo-European languages, many biomedical terms are rich morphological structures composed of several constituents mainly originating from Greek or Latin. The interpretation of these compounds are keystones to access information. In this paper, we present morphological resources aiming at coping with these biomedical morphological compounds. Following previous work (Claveau et al. 2011,Claveau et al. 12), these resources are automatically built using Japanese terms in Kanjis as a pivot language and alignment techniques. We show how these alignment information can be used for segmenting compounds, attaching semantic interpretation to each part, proposing definitions (gloses) of the compounds... When possible, these tasks are compared with state-of-the-art tools, and the results show the interest of our automatically built probabilistic resources.</abstract>
    </paper>
    <paper id="5">
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Iliana</first><last>Simova</last></author>
      <author><first>Ginka</first><last>Ivanova</last></author>
      <author><first>Maria</first><last>Mateva</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <title>A System for Experiments with Dependency Parsers</title>
      <pages>4061–4065</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1005_Paper.pdf</url>
      <abstract>In this paper we present a system for experimenting with combinations of dependency parsers. The system supports initial training of different parsing models, creation of parsebank(s) with these models, and different strategies for the construction of ensemble models aimed at improving the output of the individual models by voting. The system employs two algorithms for construction of dependency trees from several parses of the same sentence and several ways for ranking of the arcs in the resulting trees. We have performed experiments with state-of-the-art dependency parsers including MaltParser, MSTParser, TurboParser, and MATEParser, on the data from the Bulgarian treebank -- BulTreeBank. Our best result from these experiments is slightly better then the best result reported in the literature for this language.</abstract>
    </paper>
    <paper id="6">
      <author><first>Thamar</first><last>Solorio</last></author>
      <author><first>Ragib</first><last>Hasan</last></author>
      <author><first>Mainul</first><last>Mizan</last></author>
      <title>Sockpuppet Detection in <fixed-case>W</fixed-case>ikipedia: A Corpus of Real-World Deceptive Writing for Linking Identities</title>
      <pages>1355–1358</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1007_Paper.pdf</url>
      <abstract>This paper describes a corpus of sockpuppet cases from Wikipedia. A sockpuppet is an online user account created with a fake identity for the purpose of covering abusive behavior and/or subverting the editing regulation process. We used a semi-automated method for crawling and curating a dataset of real sockpuppet investigation cases. To the best of our knowledge, this is the first corpus available on real-world deceptive writing. We describe the process for crawling the data and some preliminary results that can be used as baseline for benchmarking research. The dataset has been released under a Creative Commons license from our project website (http://docsig.cis.uab.edu/tools-and-datasets/).</abstract>
    </paper>
    <paper id="7">
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>Textual Emigration Analysis (<fixed-case>TEA</fixed-case>)</title>
      <pages>2089–2093</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1009_Paper.pdf</url>
      <abstract>We present a web-based application which is called TEA (Textual Emigration Analysis) as a showcase that applies textual analysis for the humanities. The TEA tool is used to transform raw text input into a graphical display of emigration source and target countries (under a global or an individual perspective). It provides emigration-related frequency information, and gives access to individual textual sources, which can be downloaded by the user. Our application is built on top of the CLARIN infrastructure which targets researchers of the humanities. In our scenario, we focus on historians, literary scientists, and other social scientists that are interested in the semantic interpretation of text. Our application processes a large set of documents to extract information about people who emigrated. The current implementation integrates two data sets: A data set from the Global Migrant Origin Database, which does not need additional processing, and a data set which was extracted from the German Wikipedia edition. The TEA tool can be accessed by using the following URL: http://clarin01.ims.uni-stuttgart.de/geovis/showcase.html</abstract>
    </paper>
    <paper id="8">
      <author><first>Xiaoyun</first><last>Wang</last></author>
      <author><first>Jinsong</first><last>Zhang</last></author>
      <author><first>Masafumi</first><last>Nishida</last></author>
      <author><first>Seiichi</first><last>Yamamoto</last></author>
      <title>Phoneme Set Design Using <fixed-case>E</fixed-case>nglish Speech Database by <fixed-case>J</fixed-case>apanese for Dialogue-Based <fixed-case>E</fixed-case>nglish <fixed-case>CALL</fixed-case> Systems</title>
      <pages>3948–3951</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/101_Paper.pdf</url>
      <abstract>This paper describes a method of generating a reduced phoneme set for dialogue-based computer assisted language learning (CALL)systems. We designed a reduced phoneme set consisting of classified phonemes more aligned with the learners speech characteristics than the canonical set of a target language. This reduced phoneme set provides an inherently more appropriate model for dealing with mispronunciation by second language speakers. In this study, we used a phonetic decision tree (PDT)-based top-down sequential splitting method to generate the reduced phoneme set and then applied this method to a translation-game type English CALL system for Japanese to determine its effectiveness. Experimental results showed that the proposed method improves the performance of recognizing non-native speech.</abstract>
    </paper>
    <paper id="9">
      <author><first>Amel</first><last>Fraisse</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <title>Toward a unifying model for Opinion, Sentiment and Emotion information extraction</title>
      <pages>3881–3886</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1010_Paper.pdf</url>
      <abstract>This paper presents a logical formalization of a set 20 semantic categories related to opinion, emotion and sentiment. Our formalization is based on the BDI model (Belief, Desire and Intetion) and constitues a first step toward a unifying model for subjective information extraction. The separability of the subjective classes that we propose was assessed both formally and on two subjective reference corpora.</abstract>
    </paper>
    <paper id="10">
      <author><first>Thorsten</first><last>Trippel</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Matej</first><last>Durco</last></author>
      <author><first>Oddrun</first><last>Ohren</last></author>
      <title>Towards automatic quality assessment of component metadata</title>
      <pages>3851–3856</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1011_Paper.pdf</url>
      <abstract>Measuring the quality of metadata is only possible by assessing the quality of the underlying schema and the metadata instance. We propose some factors that are measurable automatically for metadata according to the CMD framework, taking into account the variability of schemas that can be defined in this framework. The factors include among others the number of elements, the (re-)use of reusable components, the number of filled in elements. The resulting score can serve as an indicator of the overall quality of the CMD instance, used for feedback to metadata providers or to provide an overview of the overall quality of metadata within a reposi-tory. The score is independent of specific schemas and generalizable. An overall assessment of harvested metadata is provided in form of statistical summaries and the distribution, based on a corpus of harvested metadata. The score is implemented in XQuery and can be used in tools, editors and repositories.</abstract>
    </paper>
    <paper id="11">
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Julia</first><last>Bonn</last></author>
      <author><first>Kathryn</first><last>Conger</last></author>
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title><fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank: Semantics of New Predicate Types</title>
      <pages>3013–3019</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1012_Paper.pdf</url>
      <abstract>This research focuses on expanding PropBank, a corpus annotated with predicate argument structures, with new predicate types; namely, noun, adjective and complex predicates, such as Light Verb Constructions. This effort is in part inspired by a sister project to PropBank, the Abstract Meaning Representation project, which also attempts to capture who is doing what to whom in a sentence, but does so in a way that abstracts away from syntactic structures. For example, alternate realizations of a ‘destroying’ event in the form of either the verb ‘destroy’ or the noun ‘destruction’ would receive the same Abstract Meaning Representation. In order for PropBank to reach the same level of coverage and continue to serve as the bedrock for Abstract Meaning Representation, predicate types other than verbs, which have previously gone without annotation, must be annotated. This research describes the challenges therein, including the development of new annotation practices that walk the line between abstracting away from language-particular syntactic facts to explore deeper semantics, and maintaining the connection between semantics and syntactic structures that has proven to be very valuable for PropBank as a corpus of training data for Natural Language Processing applications.</abstract>
    </paper>
    <paper id="12">
      <author><first>Jonathan</first><last>Wright</last></author>
      <title><fixed-case>REST</fixed-case>ful Annotation and Efficient Collaboration</title>
      <pages>1692–1698</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1016_Paper.pdf</url>
      <abstract>As linguistic collection and annotation scale up and collaboration across sites increases, novel technologies are necessary to support projects. Recent events at LDC, namely the move to a web-based infrastructure, the formation of the Software Group, and our involvement in the NSF LAPPS Grid project, have converged on concerns of efficient collaboration. The underlying design of the Web, typically referred to as RESTful principles, is crucial for collaborative annotation, providing data and processing services, and participating in the Linked Data movement. This paper outlines recommendations that will facilitate such collaboration.</abstract>
    </paper>
    <paper id="13">
      <author><first>Hendrik</first><last>Buschmeier</last></author>
      <author><first>Zofia</first><last>Malisz</last></author>
      <author><first>Joanna</first><last>Skubisz</last></author>
      <author><first>Marcin</first><last>Wlodarczak</last></author>
      <author><first>Ipke</first><last>Wachsmuth</last></author>
      <author><first>Stefan</first><last>Kopp</last></author>
      <author><first>Petra</first><last>Wagner</last></author>
      <title><fixed-case>ALICO</fixed-case>: a multimodal corpus for the study of active listening</title>
      <pages>3638–3643</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1017_Paper.pdf</url>
      <abstract>The Active Listening Corpus (ALICO) is a multimodal database of spontaneous dyadic conversations with diverse speech and gestural annotations of both dialogue partners. The annotations consist of short feedback expression transcription with corresponding communicative function interpretation as well as segmentation of interpausal units, words, rhythmic prominence intervals and vowel-to-vowel intervals. Additionally, ALICO contains head gesture annotation of both interlocutors. The corpus contributes to research on spontaneous human--human interaction, on functional relations between modalities, and timing variability in dialogue. It also provides data that differentiates between distracted and attentive listeners. We describe the main characteristics of the corpus and present the most important results obtained from analyses in recent years.</abstract>
    </paper>
    <paper id="14">
      <author><first>Łukasz</first><last>Kobyliński</last></author>
      <title><fixed-case>P</fixed-case>oli<fixed-case>T</fixed-case>a: A multitagger for <fixed-case>P</fixed-case>olish</title>
      <pages>2949–2954</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1018_Paper.pdf</url>
      <abstract>Part-of-Speech (POS) tagging is a crucial task in Natural Language Processing (NLP). POS tags may be assigned to tokens in text manually, by trained linguists, or using algorithmic approaches. Particularly, in the case of annotated text corpora, the quantity of textual data makes it unfeasible to rely on manual tagging and automated methods are used extensively. The quality of such methods is of critical importance, as even 1% tagger error rate results in introducing millions of errors in a corpus consisting of a billion tokens. In case of Polish several POS taggers have been proposed to date, but even the best of the taggers achieves an accuracy of ca. 93%, as measured on the one million subcorpus of the National Corpus of Polish (NCP). As the task of tagging is an example of classification, in this article we introduce a new POS tagger for Polish, which is based on the idea of combining several classifiers to produce higher quality tagging results than using any of the taggers individually.</abstract>
    </paper>
    <paper id="15">
      <author><first>Siddharth</first><last>Jain</last></author>
      <author><first>Archna</first><last>Bhatia</last></author>
      <author><first>Angelique</first><last>Rein</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <title>A Corpus of Participant Roles in Contentious Discussions</title>
      <pages>1751–1756</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1019_Paper.pdf</url>
      <abstract>The expansion of social roles is, nowadays, a fact due to the ability of users to interact, discuss, exchange ideas and opinions, and form social networks though social media. Users in online social environment play a variety of social roles. The concept of “social role” has long been used in social science describe the intersection of behavioural, meaningful, and structural attributes that emerge regularly in particular settings. In this paper, we present a new corpus for social roles in online contentious discussions. We explore various behavioural attributes such as stubbornness, sensibility, influence, and ignorance to create a model of social roles to distinguish among various social roles participants assume in such setup. We annotate discussions drawn from two different sets of corpora in order to ensure that our model of social roles and their signals hold up in general. We discuss the various criteria for deciding values for each behavioural attributes which define the roles.</abstract>
    </paper>
    <paper id="16">
      <author><first>John</first><last>Richardson</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>Bilingual Dictionary Construction with Transliteration Filtering</title>
      <pages>1013–1017</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/102_Paper.pdf</url>
      <abstract>In this paper we present a bilingual transliteration lexicon of 170K Japanese-English technical terms in the scientific domain. Translation pairs are extracted by filtering a large list of transliteration candidates generated automatically from a phrase table trained on parallel corpora. Filtering uses a novel transliteration similarity measure based on a discriminative phrase-based machine translation approach. We demonstrate that the extracted dictionary is accurate and of high recall (F1 score 0.8). Our lexicon contains not only single words but also multi-word expressions, and is freely available. Our experiments focus on Katakana-English lexicon construction, however it would be possible to apply the proposed methods to transliteration extraction for a variety of language pairs.</abstract>
    </paper>
    <paper id="17">
      <author><first>Vera</first><last>Cabarrão</last></author>
      <author><first>Helena</first><last>Moniz</last></author>
      <author><first>Fernando</first><last>Batista</last></author>
      <author><first>Ricardo</first><last>Ribeiro</last></author>
      <author><first>Nuno</first><last>Mamede</last></author>
      <author><first>Hugo</first><last>Meinedo</last></author>
      <author><first>Isabel</first><last>Trancoso</last></author>
      <author><first>Ana Isabel</first><last>Mata</last></author>
      <author><first>David Martins</first><last>de Matos</last></author>
      <title>Revising the annotation of a Broadcast News corpus: a linguistic approach</title>
      <pages>3908–3913</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1020_Paper.pdf</url>
      <abstract>This paper presents a linguistic revision process of a speech corpus of Portuguese broadcast news focusing on metadata annotation for rich transcription, and reports on the impact of the new data on the performance for several modules. The main focus of the revision process consisted on annotating and revising structural metadata events, such as disfluencies and punctuation marks. The resultant revised data is now being extensively used, and was of extreme importance for improving the performance of several modules, especially the punctuation and capitalization modules, but also the speech recognition system, and all the subsequent modules. The resultant data has also been recently used in disfluency studies across domains.</abstract>
    </paper>
    <paper id="18">
      <author><first>Pyry</first><last>Takala</last></author>
      <author><first>Pekka</first><last>Malo</last></author>
      <author><first>Ankur</first><last>Sinha</last></author>
      <author><first>Oskar</first><last>Ahlgren</last></author>
      <title>Gold-standard for Topic-specific Sentiment Analysis of Economic Texts</title>
      <pages>2152–2157</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1021_Paper.pdf</url>
      <abstract>Public opinion, as measured by media sentiment, can be an important indicator in the financial and economic context. These are domains where traditional sentiment estimation techniques often struggle, and existing annotated sentiment text collections are of less use. Though considerable progress has been made in analyzing sentiments at sentence-level, performing topic-dependent sentiment analysis is still a relatively uncharted territory. The computation of topic-specific sentiments has commonly relied on naive aggregation methods without much consideration to the relevance of the sentences to the given topic. Clearly, the use of such methods leads to a substantial increase in noise-to-signal ratio. To foster development of methods for measuring topic-specific sentiments in documents, we have collected and annotated a corpus of financial news that have been sampled from Thomson Reuters newswire. In this paper, we describe the annotation process and evaluate the quality of the dataset using a number of inter-annotator agreement metrics. The annotations of 297 documents and over 9000 sentences can be used for research purposes when developing methods for detecting topic-wise sentiment in financial text.</abstract>
    </paper>
    <paper id="19">
      <author><first>Damir</first><last>Cavar</last></author>
      <author><first>Malgorzata</first><last>Cavar</last></author>
      <title>Visualization of Language Relations and Families: <fixed-case>M</fixed-case>ulti<fixed-case>T</fixed-case>ree</title>
      <pages>698–701</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1022_Paper.pdf</url>
      <abstract>MultiTree is an NFS-funded project collecting scholarly hypotheses about language relationships, and visualizing them on a web site in the form of trees or graphs. Two open online interfaces allow scholars, students, and the general public an easy access to search for language information or comparisons of competing hypotheses. One objective of the project was to facilitate research in historical linguistics. MultiTree has evolved to a much more powerful tool, it is not just a simple repository of scholarly information. In this paper we present the MultiTree interfaces and the impact of the project beyond the field of historical linguistics, including, among others, the use of standardized ISO language codes, and creating an interconnected database of language and dialect names, codes, publications, and authors. Further, we offer the dissemination of linguistic findings world-wide to both scholars and the general public, thus boosting the collaboration and accelerating the scientific exchange. We discuss also the ways MultiTree will develop beyond the time of the duration of the funding.</abstract>
    </paper>
    <paper id="20">
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Jan</first><last>Šnajder</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <title><fixed-case>H</fixed-case>i<fixed-case>E</fixed-case>ve: A Corpus for Extracting Event Hierarchies from News Stories</title>
      <pages>3678–3683</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1023_Paper.pdf</url>
      <abstract>In news stories, event mentions denote real-world events of different spatial and temporal granularity. Narratives in news stories typically describe some real-world event of coarse spatial and temporal granularity along with its subevents. In this work, we present HiEve, a corpus for recognizing relations of spatiotemporal containment between events. In HiEve, the narratives are represented as hierarchies of events based on relations of spatiotemporal containment (i.e., superevent―subevent relations). We describe the process of manual annotation of HiEve. Furthermore, we build a supervised classifier for recognizing spatiotemporal containment between events to serve as a baseline for future research. Preliminary experimental results are encouraging, with classifier performance reaching 58% F1-score, only 11% less than the inter annotator agreement.</abstract>
    </paper>
    <paper id="21">
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Zachary</first><last>Yocum</last></author>
      <title>Image Annotation with <fixed-case>ISO</fixed-case>-Space: Distinguishing Content from Structure</title>
      <pages>426–431</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1026_Paper.pdf</url>
      <abstract>Natural language descriptions of visual media present interesting problems for linguistic annotation of spatial information. This paper explores the use of ISO-Space, an annotation specification to capturing spatial information, for encoding spatial relations mentioned in descriptions of images. Especially, we focus on the distinction between references to representational content and structural components of images, and the utility of such a distinction within a compositional semantics. We also discuss how such a structure-content distinction within the linguistic annotation can be leveraged to compute further inferences about spatial configurations depicted by images with verbal captions. We construct a composition table to relate content-based relations to structure-based relations in the image, as expressed in the captions. While still preliminary, our initial results suggest that a weak composition table is both sound and informative for deriving new spatial relations.</abstract>
    </paper>
    <paper id="22">
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Jeremy</first><last>Leixa</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <title>The <fixed-case>ETAPE</fixed-case> speech processing evaluation</title>
      <pages>3995–3999</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1027_Paper.pdf</url>
      <abstract>The ETAPE evaluation is the third evaluation in automatic speech recognition and associated technologies in a series which started with ESTER. This evaluation proposed some new challenges, by proposing TV and radio shows with prepared and spontaneous speech, annotation and evaluation of overlapping speech, a cross-show condition in speaker diarization, and new, complex but very informative named entities in the information extraction task. This paper presents the whole campaign, including the data annotated, the metrics used and the anonymized system results. All the data created in the evaluation, hopefully including system outputs, will be distributed through the ELRA catalogue in the future.</abstract>
    </paper>
    <paper id="23">
      <author><first>David</first><last>Kamholz</last></author>
      <author><first>Jonathan</first><last>Pool</last></author>
      <author><first>Susan</first><last>Colowick</last></author>
      <title><fixed-case>P</fixed-case>an<fixed-case>L</fixed-case>ex: Building a Resource for Panlingual Lexical Translation</title>
      <pages>3145–3150</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf</url>
      <abstract>PanLex, a project of The Long Now Foundation, aims to enable the translation of lexemes among all human languages in the world. By focusing on lexemic translations, rather than grammatical or corpus data, it achieves broader lexical and language coverage than related projects. The PanLex database currently documents 20 million lexemes in about 9,000 language varieties, with 1.1 billion pairwise translations. The project primarily engages in content procurement, while encouraging outside use of its data for research and development. Its data acquisition strategy emphasizes broad, high-quality lexical and language coverage. The project plans to add data derived from 4,000 new sources to the database by the end of 2016. The dataset is publicly accessible via an HTTP API and monthly snapshots in CSV, JSON, and XML formats. Several online applications have been developed that query PanLex data. More broadly, the project aims to make a contribution to the preservation of global linguistic diversity.</abstract>
    </paper>
    <paper id="24">
      <author><first>Dan</first><last>Tufiş</last></author>
      <title>Large <fixed-case>SMT</fixed-case> data-sets extracted from <fixed-case>W</fixed-case>ikipedia</title>
      <pages>656–663</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/103_Paper.pdf</url>
      <abstract>The article presents experiments on mining Wikipedia for extracting SMT useful sentence pairs in three language pairs. Each extracted sentence pair is associated with a cross-lingual lexical similarity score based on which, several evaluations have been conducted to estimate the similarity thresholds which allow the extraction of the most useful data for training three-language pairs SMT systems. The experiments showed that for a similarity score higher than 0.7 all sentence pairs in the three language pairs were fully parallel. However, including in the training sets less parallel sentence pairs (that is with a lower similarity score) showed significant improvements in the translation quality (BLEU-based evaluations). The optimized SMT systems were evaluated on unseen test-sets also extracted from Wikipedia. As one of the main goals of our work was to help Wikipedia contributors to translate (with as little post editing as possible) new articles from major languages into less resourced languages and vice-versa, we call this type of translation experiments in-genre translation. As in the case of in-domain translation, our evaluations showed that using only in-genre training data for translating same genre new texts is better than mixing the training data with out-of-genre (even) parallel texts.</abstract>
    </paper>
    <paper id="25">
      <author><first>Valeria</first><last>de Paiva</last></author>
      <author><first>Livy</first><last>Real</last></author>
      <author><first>Alexandre</first><last>Rademaker</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <title><fixed-case>N</fixed-case>om<fixed-case>L</fixed-case>ex-<fixed-case>PT</fixed-case>: A Lexicon of <fixed-case>P</fixed-case>ortuguese Nominalizations</title>
      <pages>2851–2858</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1031_Paper.pdf</url>
      <abstract>This paper presents NomLex-PT, a lexical resource describing Portuguese nominalizations. NomLex-PT connects verbs to their nominalizations, thereby enabling NLP systems to observe the potential semantic relationships between the two words when analysing a text. NomLex-PT is freely available and encoded in RDF for easy integration with other resources. Most notably, we have integrated NomLex-PT with OpenWordNet-PT, an open Portuguese Wordnet.</abstract>
    </paper>
    <paper id="26">
      <author><first>Elisabet</first><last>Comelles</last></author>
      <author><first>Jordi</first><last>Atserias</last></author>
      <author><first>Victoria</first><last>Arranz</last></author>
      <author><first>Irene</first><last>Castellón</last></author>
      <author><first>Jordi</first><last>Sesé</last></author>
      <title><fixed-case>VERT</fixed-case>a: Facing a Multilingual Experience of a Linguistically-based <fixed-case>MT</fixed-case> Evaluation</title>
      <pages>2701–2707</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1032_Paper.pdf</url>
      <abstract>There are several MT metrics used to evaluate translation into Spanish, although most of them use partial or little linguistic information. In this paper we present the multilingual capability of VERTa, an automatic MT metric that combines linguistic information at lexical, morphological, syntactic and semantic level. In the experiments conducted we aim at identifying those linguistic features that prove the most effective to evaluate adequacy in Spanish segments. This linguistic information is tested both as independent modules (to observe what each type of feature provides) and in a combinatory fastion (where different kinds of information interact with each other). This allows us to extract the optimal combination. In addition we compare these linguistic features to those used in previous versions of VERTa aimed at evaluating adequacy for English segments. Finally, experiments show that VERTa can be easily adapted to other languages than English and that its collaborative approach correlates better with human judgements on adequacy than other well-known metrics.</abstract>
    </paper>
    <paper id="27">
      <author><first>Yifan</first><last>He</last></author>
      <author><first>Adam</first><last>Meyers</last></author>
      <title>Corpus and Method for Identifying Citations in Non-Academic Text</title>
      <pages>4316–4319</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1036_Paper.pdf</url>
      <abstract>We attempt to identify citations in non-academic text such as patents. Unlike academic articles which often provide bibliographies and follow consistent citation styles, non-academic text cites scientific research in a more ad-hoc manner. We manually annotate citations in 50 patents, train a CRF classifier to find new citations, and apply a reranker to incorporate non-local information. Our best system achieves 0.83 F-score on 5-fold cross validation.</abstract>
    </paper>
    <paper id="28">
      <author><first>Vanessa</first><last>Loza</last></author>
      <author><first>Shibamouli</first><last>Lahiri</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Po-Hsiang</first><last>Lai</last></author>
      <title>Building a Dataset for Summarization and Keyword Extraction from Emails</title>
      <pages>2441–2446</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1037_Paper.pdf</url>
      <abstract>This paper introduces a new email dataset, consisting of both single and thread emails, manually annotated with summaries and keywords. A total of 349 emails and threads have been annotated. The dataset is our first step toward developing automatic methods for summarization and keyword extraction from emails. We describe the email corpus, along with the annotation interface, annotator guidelines, and agreement studies.</abstract>
    </paper>
    <paper id="29">
      <author><first>Jordan</first><last>Schmidek</last></author>
      <author><first>Denilson</first><last>Barbosa</last></author>
      <title>Improving Open Relation Extraction via Sentence Re-Structuring</title>
      <pages>3720–3723</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1038_Paper.pdf</url>
      <abstract>Information Extraction is an important task in Natural Language Processing, consisting of finding a structured representation for the information expressed in natural language text. Two key steps in information extraction are identifying the entities mentioned in the text, and the relations among those entities. In the context of Information Extraction for the World Wide Web, unsupervised relation extraction methods, also called Open Relation Extraction (ORE) systems, have become prevalent, due to their effectiveness without domain-specific training data. In general, these systems exploit part-of-speech tags or semantic information from the sentences to determine whether or not a relation exists, and if so, its predicate. This paper discusses some of the issues that arise when even moderately complex sentences are fed into ORE systems. A process for re-structuring such sentences is discussed and evaluated. The proposed approach replaces complex sentences by several others that, together, convey the same meaning and are more amenable to extraction by current ORE systems. The results of an experimental evaluation show that this approach succeeds in reducing the processing time and increasing the accuracy of the state-of-the-art ORE systems.</abstract>
    </paper>
    <paper id="30">
      <author><first>Juan</first><last>Soler Company</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <title>How to Use less Features and Reach Better Performance in Author Gender Identification</title>
      <pages>1315–1319</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/104_Paper.pdf</url>
      <abstract>Over the last years, author profiling in general and author gender identification in particular have become a popular research area due to their potential attractive applications that range from forensic investigations to online marketing studies. However, nearly all state-of-the-art works in the area still very much depend on the datasets they were trained and tested on, since they heavily draw on content features, mostly a large number of recurrent words or combinations of words extracted from the training sets. We show that using a small number of features that mainly depend on the structure of the texts we can outperform other approaches that depend mainly on the content of the texts and that use a huge number of features in the process of identifying if the author of a text is a man or a woman. Our system has been tested against a dataset constructed for our work as well as against two datasets that were previously used in other papers.</abstract>
    </paper>
    <paper id="31">
      <author><first>Michael</first><last>Mohler</last></author>
      <author><first>Marc</first><last>Tomlinson</last></author>
      <author><first>David</first><last>Bracewell</last></author>
      <author><first>Bryan</first><last>Rink</last></author>
      <title>Semi-supervised methods for expanding psycholinguistics norms by integrating distributional similarity with the structure of <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <pages>3020–3026</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1043_Paper.pdf</url>
      <abstract>In this work, we present two complementary methods for the expansion of psycholinguistics norms. The first method is a random-traversal spreading activation approach which transfers existing norms onto semantically related terms using notions of synonymy, hypernymy, and pertainymy to approach full coverage of the English language. The second method makes use of recent advances in distributional similarity representation to transfer existing norms to their closest neighbors in a high-dimensional vector space. These two methods (along with a naive hybrid approach combining the two) have been shown to significantly outperform a state-of-the-art resource expansion system at our pilot task of imageability expansion. We have evaluated these systems in a cross-validation experiment using 8,188 norms found in existing pscholinguistics literature. We have also validated the quality of these combined norms by performing a small study using Amazon Mechanical Turk (AMT).</abstract>
    </paper>
    <paper id="32">
      <author><first>Dagmar</first><last>Jung</last></author>
      <author><first>Katarzyna</first><last>Klessa</last></author>
      <author><first>Zsuzsa</first><last>Duray</last></author>
      <author><first>Beatrix</first><last>Oszkó</last></author>
      <author><first>Mária</first><last>Sipos</last></author>
      <author><first>Sándor</first><last>Szeverényi</last></author>
      <author><first>Zsuzsa</first><last>Várnai</last></author>
      <author><first>Paul</first><last>Trilsbeek</last></author>
      <author><first>Tamás</first><last>Váradi</last></author>
      <title>Languagesindanger.eu - Including Multimedia Language Resources to disseminate Knowledge and Create Educational Material on less-Resourced Languages</title>
      <pages>530–535</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1046_Paper.pdf</url>
      <abstract>The present paper describes the development of the languagesindanger.eu interactive website as an example of including multimedia language resources toÂ disseminate knowledge and create educational material onÂ less-resourced languages. The website is a product of INNET (Innovative networking in infrastructure for endangered languages), European FP7 project. Its main functions can be summarized as related to the three following areas: (1) raising students’ awareness of language endangerment and arouse their interest in linguistic diversity, language maintenance and language documentation; (2) informing both students and teachers about these topics and show ways how they can enlarge their knowledge further with a special emphasis on information about language archives; (3) helping teachers include these topics into their classes. The website has been localized into five language versions with the intention to be accessible to both scientific and non-scientific communities such as (primarily) secondary school teachers and students, beginning university students of linguistics, journalists, the interested public, and also members of speech communities who speak minority languages.</abstract>
    </paper>
    <paper id="33">
      <author><first>Bistra</first><last>Andreeva</last></author>
      <author><first>William</first><last>Barry</last></author>
      <author><first>Jacques</first><last>Koreman</last></author>
      <title>A Cross-language Corpus for Studying the Phonetics and Phonology of Prominence</title>
      <pages>326–330</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1048_Paper.pdf</url>
      <abstract>The present article describes a corpus which was collected for the cross-language comparison of prominence. In the data analysis, the acoustic-phonetic properties of words spoken with two different levels of accentuation (de-accented and nuclear accented in non-contrastive narrow-focus) are examined in question-answer elicited sentences and iterative imitations (on the syllable da) produced by Bulgarian, Russian, French, German and Norwegian speakers (3 male and 3 female per language). Normalized parameter values allow a comparison of the properties employed in differentiating the two levels of accentuation. Across the five languages there are systematic differences in the degree to which duration, f0, intensity and spectral vowel definition change with changing prominence under different focus conditions. The link with phonological differences between the languages is discussed.</abstract>
    </paper>
    <paper id="34">
      <author><first>Benoît</first><last>Sagot</last></author>
      <title><fixed-case>D</fixed-case>e<fixed-case>L</fixed-case>ex, a freely-avaible, large-scale and linguistically grounded morphological lexicon for <fixed-case>G</fixed-case>erman</title>
      <pages>2778–2784</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/105_Paper.pdf</url>
      <abstract>We introduce DeLex, a freely-avaible, large-scale and linguistically grounded morphological lexicon for German developed within the Alexina framework. We extracted lexical information from the German wiktionary and developed a morphological inflection grammar for German, based on a linguistically sound model of inflectional morphology. Although the developement of DeLex involved some manual work, we show that is represents a good tradeoff between development cost, lexical coverage and resource accuracy.</abstract>
    </paper>
    <paper id="35">
      <author><first>Peter</first><last>Baumann</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <title>Using Resource-Rich Languages to Improve Morphological Analysis of Under-Resourced Languages</title>
      <pages>3355–3359</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1051_Paper.pdf</url>
      <abstract>The world-wide proliferation of digital communications has created the need for language and speech processing systems for under-resourced languages. Developing such systems is challenging if only small data sets are available, and the problem is exacerbated for languages with highly productive morphology. However, many under-resourced languages are spoken in multi-lingual environments together with at least one resource-rich language and thus have numerous borrowings from resource-rich languages. Based on this insight, we argue that readily available resources from resource-rich languages can be used to bootstrap the morphological analyses of under-resourced languages with complex and productive morphological systems. In a case study of two such languages, Tagalog and Zulu, we show that an easily obtainable English wordlist can be deployed to seed a morphological analysis algorithm from a small training set of conversational transcripts. Our method achieves a precision of 100% and identifies 28 and 66 of the most productive affixes in Tagalog and Zulu, respectively.</abstract>
    </paper>
    <paper id="36">
      <author><first>Clara</first><last>Bacciu</last></author>
      <author><first>Angelica Lo</first><last>Duca</last></author>
      <author><first>Andrea</first><last>Marchetti</last></author>
      <author><first>Maurizio</first><last>Tesconi</last></author>
      <title>Accommodations in Tuscany as Linked Data</title>
      <pages>3542–3545</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1052_Paper.pdf</url>
      <abstract>The OpeNER Linked Dataset (OLD) contains 19.140 entries about accommodations in Tuscany (Italy). For each accommodation, it describes the type, e.g. hotel, bed and breakfast, hostel, camping etc., and other useful information, such as a short description, the Web address, its location and the features it provides. OLD is the linked data version of the open dataset provided by Fondazione Sistema Toscana, the representative system for tourism in Tuscany. In addition, to the original dataset, OLD provides also the link of each accommodation to the most common social media (Facebook, Foursquare, Google Places and Booking). OLD exploits three common ontologies of the accommodation domain: Acco, Hontology and GoodRelations. The idea is to provide a flexible dataset, which speaks more than one ontology. OLD is available as a SPARQL node and is released under the Creative Commons release. Finally, OLD is developed within the OpeNER European project, which aims at building a set of ready to use tools to recognize and disambiguate entity mentions and perform sentiment analysis and opinion detection on texts. Within the project, OLD provides a named entity repository for entity disambiguation.</abstract>
    </paper>
    <paper id="37">
      <author><first>Przemyslaw</first><last>Lenkiewicz</last></author>
      <author><first>Olha</first><last>Shkaravska</last></author>
      <author><first>Twan</first><last>Goosen</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Menzo</first><last>Windhouwer</last></author>
      <author><first>Stephanie</first><last>Roth</last></author>
      <author><first>Olof</first><last>Olsson</last></author>
      <title>The <fixed-case>DWAN</fixed-case> framework: Application of a web annotation framework for the general humanities to the domain of language resources</title>
      <pages>3644–3649</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1053_Paper.pdf</url>
      <abstract>Researchers share large amounts of digital resources, which offer new chances for cooperation. Collaborative annotation systems are meant to support this. Often these systems are targeted at a specific task or domain, e.g., annotation of a corpus. The DWAN framework for web annotation is generic and can support a wide range of tasks and domains. A key feature of the framework is its support for caching representations of the annotated resource. This allows showing the context of the annotation even if the resource has changed or has been removed. The paper describes the design and implementation of the framework. Use cases provided by researchers are well in line with the key characteristics of the DWAN annotation framework.</abstract>
    </paper>
    <paper id="38">
      <author><first>Arjan</first><last>van Hessen</last></author>
      <author><first>Franciska</first><last>de Jong</last></author>
      <author><first>Stef</first><last>Scagliola</last></author>
      <author><first>Tanja</first><last>Petrovic</last></author>
      <title><fixed-case>C</fixed-case>roatian Memories</title>
      <pages>3920–3926</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1056_Paper.pdf</url>
      <abstract>In this contribution we describe a collection of approximately 400 video interviews recorded in the context of the project Croatian Memories (CroMe) with the objective of documenting personal war-related experiences. The value of this type of sources is threefold: they contain information that is missing in written sources, they can contribute to the process of reconciliation, and they provide a basis for reuse of data in disciplines with an interest in narrative data. The CroMe collection is not primarily designed as a linguistic corpus, but is the result of an archival effort to collect so-called oral history data. For researchers in the fields of natural language processing and speech analyÂ¬sis this type of life-stories may function as an object trouvé containing real-life language data that can prove to be useful for the purpose of modelling specific aspects of human expression and communication.</abstract>
    </paper>
    <paper id="39">
      <author><first>Deryle</first><last>Lonsdale</last></author>
      <author><first>Carl</first><last>Christensen</last></author>
      <title>Combining elicited imitation and fluency features for oral proficiency measurement</title>
      <pages>1956–1961</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1057_Paper.pdf</url>
      <abstract>The automatic grading of oral language tests has been the subject of much research in recent years. Several obstacles lie in the way of achieving this goal. Recent work suggests a testing technique called elicited imitation (EI) that can serve to accurately approximate global oral proficiency. This testing methodology, however, does not incorporate some fundamental aspects of language, such as fluency. Other work has suggested another testing technique, simulated speech (SS), as a supplement or an alternative to EI that can provide automated fluency metrics. In this work, we investigate a combination of fluency features extracted from SS tests and EI test scores as a means to more accurately predict oral language proficiency. Using machine learning and statistical modeling, we identify which features automatically extracted from SS tests best predicted hand-scored SS test results, and demonstrate the benefit of adding EI scores to these models. Results indicate that the combination of EI and fluency features do indeed more effectively predict hand-scored SS test scores. We finally discuss implications of this work for future automated oral testing scenarios.</abstract>
    </paper>
    <paper id="40">
      <author><first>Pavel</first><last>Smrz</last></author>
      <author><first>Jan</first><last>Kouril</last></author>
      <title>Semantic Search in Documents Enriched by <fixed-case>LOD</fixed-case>-based Annotations</title>
      <pages>3724–3727</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1058_Paper.pdf</url>
      <abstract>This paper deals with information retrieval on semantically enriched web-scale document collections. It particularly focuses on web-crawled content in which mentions of entities appearing in Freebase, DBpedia and other Linked Open Data resources have been identified. A special attention is paid to indexing structures and advanced query mechanisms that have been employed into a new semantic retrieval system. Scalability features are discussed together with performance statistics and results of experimental evaluation of presented approaches. Examples given to demonstrate key features of the developed solution correspond to the cultural heritage domain in which the results of our work have been primarily applied.</abstract>
    </paper>
    <paper id="41">
      <author><first>Manuel</first><last>Fiorelli</last></author>
      <author><first>Maria Teresa</first><last>Pazienza</last></author>
      <author><first>Armando</first><last>Stellato</last></author>
      <title>A Meta-data Driven Platform for Semi-automatic Configuration of Ontology Mediators</title>
      <pages>4178–4183</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1059_Paper.pdf</url>
      <abstract>Ontology mediators often demand extensive configuration, or even the adaptation of the input ontologies for remedying unsupported modeling patterns. In this paper we propose MAPLE (MAPping Architecture based on Linguistic Evidences), an architecture and software platform that semi-automatically solves this configuration problem, by reasoning on metadata about the linguistic expressivity of the input ontologies, the available mediators and other components relevant to the mediation task. In our methodology mediators should access the input ontologies through uniform interfaces abstracting many low-level details, while depending on generic third-party linguistic resources providing external information. Given a pair of ontologies to reconcile, MAPLE ranks the available mediators according to their ability to exploit most of the input ontologies content, while coping with the exhibited degree of linguistic heterogeneity. MAPLE provides the chosen mediator with concrete linguistic resources and suitable implementations of the required interfaces. The resulting mediators are more robust, as they are isolated from many low-level issues, and their applicability and performance may increase over time as new and better resources and other components are made available. To sustain this trend, we foresee the use of the Web as a large scale repository.</abstract>
    </paper>
    <paper id="42">
      <author><first>Huijing</first><last>Deng</last></author>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <title>Semantic approaches to software component retrieval with <fixed-case>E</fixed-case>nglish queries</title>
      <pages>3248–3252</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/106_Paper.pdf</url>
      <abstract>Enabling code reuse is an important goal in software engineering, and it depends crucially on effective code search interfaces. We propose to ground word meanings in source code and use such language-code mappings in order to enable a search engine for programming library code where users can pose queries in English. We exploit the fact that there are large programming language libraries which are documented both via formally specified function or method signatures as well as descriptions written in natural language. Automatically learned associations between words in descriptions and items in signatures allows us to use queries formulated in English to retrieve methods which are not documented via natural language descriptions, only based on their signatures. We show that the rankings returned by our model substantially outperforms a strong term-matching baseline.</abstract>
    </paper>
    <paper id="43">
      <author><first>Georgios</first><last>Petasis</last></author>
      <title>The Ellogon Pattern Engine: Context-free Grammars over Annotations</title>
      <pages>2460–2465</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1060_Paper.pdf</url>
      <abstract>This paper presents the pattern engine that is offered by the Ellogon language engineering platform. This pattern engine allows the application of context-free grammars over annotations, which are metadata generated during the processing of documents by natural language tools. In addition, grammar development is aided by a graphical grammar editor, giving grammar authors the capability to test and debug grammars.</abstract>
    </paper>
    <paper id="44">
      <author><first>Friedel</first><last>Wolff</last></author>
      <author><first>Laurette</first><last>Pretorius</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <title>Missed opportunities in translation memory matching</title>
      <pages>4401–4406</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1061_Paper.pdf</url>
      <abstract>A translation memory system stores a data set of source-target pairs of translations. It attempts to respond to a query in the source language with a useful target text from the data set to assist a human translator. Such systems estimate the usefulness of a target text suggestion according to the similarity of its associated source text to the source text query. This study analyses two data sets in two language pairs each to find highly similar target texts, which would be useful mutual suggestions. We further investigate which of these useful suggestions can not be selected through source text similarity, and we do a thorough analysis of these cases to categorise and quantify them. This analysis provides insight into areas where the recall of translation memory systems can be improved. Specifically, source texts with an omission, and semantically very similar source texts are some of the more frequent cases with useful target text suggestions that are not selected with the baseline approach of simple edit distance between the source texts.</abstract>
    </paper>
    <paper id="45">
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Timothy</first><last>Dozat</last></author>
      <author><first>Natalia</first><last>Silveira</last></author>
      <author><first>Katri</first><last>Haverinen</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <title>Universal <fixed-case>S</fixed-case>tanford dependencies: A cross-linguistic typology</title>
      <pages>4585–4592</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf</url>
      <abstract>Revisiting the now de facto standard Stanford dependency representation, we propose an improved taxonomy to capture grammatical relations across languages, including morphologically rich ones. We suggest a two-layered taxonomy: a set of broadly attested universal grammatical relations, to which language-specific relations can be added. We emphasize the lexicalist stance of the Stanford Dependencies, which leads to a particular, partially new treatment of compounding, prepositions, and morphology. We show how existing dependency schemes for several languages map onto the universal taxonomy proposed here and close with consideration of practical implications of dependency representation choices for NLP applications, in particular parsing.</abstract>
    </paper>
    <paper id="46">
      <author><first>Reid</first><last>Swanson</last></author>
      <author><first>Stephanie</first><last>Lukin</last></author>
      <author><first>Luke</first><last>Eisenberg</last></author>
      <author><first>Thomas</first><last>Corcoran</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <title>Getting Reliable Annotations for Sarcasm in Online Dialogues</title>
      <pages>4250–4257</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1063_Paper.pdf</url>
      <abstract>The language used in online forums differs in many ways from that of traditional language resources such as news. One difference is the use and frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the aim is to develop a theory of sarcasm in dialogue, or engineer automatic methods for reliably detecting sarcasm, a major challenge is simply the difficulty of getting enough reliably labelled examples. In this paper we describe our work on methods for achieving highly reliable sarcasm annotations from untrained annotators on Mechanical Turk. We explore the use of a number of common statistical reliability measures, such as Kappa, Karger’s, Majority Class, and EM. We show that more sophisticated measures do not appear to yield better results for our data than simple measures such as assuming that the correct label is the one that a majority of Turkers apply.</abstract>
    </paper>
    <paper id="47">
      <author><first>Johannes</first><last>Hellrich</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <author><first>Dietrich</first><last>Rebholz-Schuhmann</last></author>
      <title>Collaboratively Annotating Multilingual Parallel Corpora in the Biomedical Domain—some <fixed-case>MANTRA</fixed-case>s</title>
      <pages>4033–4040</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1064_Paper.pdf</url>
      <abstract>The coverage of multilingual biomedical resources is high for the English language, yet sparse for non-English languages―an observation which holds for seemingly well-resourced, yet still dramatically low-resourced ones such as Spanish, French or German but even more so for really under-resourced ones such as Dutch. We here present experimental results for automatically annotating parallel corpora and simultaneously acquiring new biomedical terminology for these under-resourced non-English languages on the basis of two types of language resources, namely parallel corpora (i.e. full translation equivalents at the document unit level) and (admittedly deficient) multilingual biomedical terminologies, with English as their anchor language. We automatically annotate these parallel corpora with biomedical named entities by an ensemble of named entity taggers and harmonize non-identical annotations the outcome of which is a so-called silver standard corpus. We conclude with an empirical assessment of this approach to automatically identify both known and new terms in multilingual corpora.</abstract>
    </paper>
    <paper id="48">
      <author><first>Heeyoung</first><last>Lee</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Bill</first><last>MacCartney</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <title>On the Importance of Text Analysis for Stock Price Prediction</title>
      <pages>1170–1175</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1065_Paper.pdf</url>
      <abstract>We investigate the importance of text analysis for stock price prediction. In particular, we introduce a system that forecasts companies stock price changes (UP, DOWN, STAY) in response to financial events reported in 8-K documents. Our results indicate that using text boosts prediction accuracy over 10% (relative) over a strong baseline that incorporates many financially-rooted features. This impact is most important in the short term (i.e., the next day after the financial event) but persists for up to five days.</abstract>
    </paper>
    <paper id="49">
      <author><first>David</first><last>Escudero</last></author>
      <author><first>Lourdes</first><last>Aguilar-Cuevas</last></author>
      <author><first>César</first><last>González-Ferreras</last></author>
      <author><first>Yurena</first><last>Gutiérrez-González</last></author>
      <author><first>Valentín</first><last>Cardeñoso-Payo</last></author>
      <title>On the use of a fuzzy classifier to speed up the <fixed-case>S</fixed-case>p_<fixed-case>T</fixed-case>o<fixed-case>BI</fixed-case> labeling of the Glissando <fixed-case>S</fixed-case>panish corpus</title>
      <pages>1962–1969</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1066_Paper.pdf</url>
      <abstract>In this paper, we present the application of a novel automatic prosodic labeling methodology for speeding up the manual labeling of the Glissando corpus (Spanish read news items). The methodology is based on the use of soft classification techniques. The output of the automatic system consists on a set of label candidates per word. The number of predicted candidates depends on the degree of certainty assigned by the classifier to each of the predictions. The manual transcriber checks the sets of predictions to select the correct one. We describe the fundamentals of the fuzzy classification tool and its training with a corpus labeled with Sp TOBI labels. Results show a clear coherence between the most confused labels in the output of the automatic classifier and the most confused labels detected in inter-transcriber consistency tests. More importantly, in a preliminary test, the real time ratio of the labeling process was 1:66 when the template of predictions is used and 1:80 when it is not.</abstract>
    </paper>
    <paper id="50">
      <author><first>Antonio</first><last>San Martín</last></author>
      <author><first>Marie-Claude</first><last>L’Homme</last></author>
      <title>Definition patterns for predicative terms in specialized lexical resources</title>
      <pages>3748–3755</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1067_Paper.pdf</url>
      <abstract>The research presented in this paper is part of a larger project on the semi-automatic generation of definitions of semantically-related terms in specialized resources. The work reported here involves the formulation of instructions to generate the definitions of sets of morphologically-related predicative terms, based on the definition of one of the members of the set. In many cases, it is assumed that the definition of a predicative term can be inferred by combining the definition of a related lexical unit with the information provided by the semantic relation (i.e. lexical function) that links them. In other words, terminographers only need to know the definition of “pollute” and the semantic relation that links it to other morphologically-related terms (“polluter”, “polluting”, “pollutant”, etc.) in order to create the definitions of the set. The results show that rules can be used to generate a preliminary set of definitions (based on specific lexical functions). They also show that more complex rules would need to be devised for other morphological pairs.</abstract>
    </paper>
    <paper id="51">
      <author><first>Xiao</first><last>Jiang</last></author>
      <author><first>Yufan</first><last>Guo</last></author>
      <author><first>Jeroen</first><last>Geertzen</last></author>
      <author><first>Dora</first><last>Alexopoulou</last></author>
      <author><first>Lin</first><last>Sun</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <title>Native Language Identification Using Large, Longitudinal Data</title>
      <pages>3309–3312</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1068_Paper.pdf</url>
      <abstract>Native Language Identification (NLI) is a task aimed at determining the native language (L1) of learners of second language (L2) on the basis of their written texts. To date, research on NLI has focused on relatively small corpora. We apply NLI to the recently released EFCamDat corpus which is not only multiple times larger than previous L2 corpora but also provides longitudinal data at several proficiency levels. Our investigation using accurate machine learning with a wide range of linguistic features reveals interesting patterns in the longitudinal data which are useful for both further development of NLI and its application to research on L2 acquisition.</abstract>
    </paper>
    <paper id="52">
      <author><first>Juan</first><last>Luo</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <title>Production of Phrase Tables in 11 <fixed-case>E</fixed-case>uropean Languages using an Improved Sub-sentential Aligner</title>
      <pages>664–669</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/107_Paper.pdf</url>
      <abstract>This paper is a partial report of an on-going Kakenhi project which aims to improve sub-sentential alignment and release multilingual syntactic patterns for statistical and example-based machine translation. Here we focus on improving a sub-sentential aligner which is an instance of the association approach. Phrase table is not only an essential component in the machine translation systems but also an important resource for research and usage in other domains. As part of this project, all phrase tables produced in the experiments will also be made freely available.</abstract>
    </paper>
    <paper id="53">
      <author><first>Anne</first><last>Garcia-Fernandez</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <title>Construction and Annotation of a <fixed-case>F</fixed-case>rench Folkstale Corpus</title>
      <pages>2430–2435</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1070_Paper.pdf</url>
      <abstract>In this paper, we present the digitization and annotation of a tales corpus - which is to our knowledge the only French tales corpus available and classified according to the Aarne&amp;Thompson classification - composed of historical texts (with old French parts). We first studied whether the pre-processing tools, namely OCR and PoS-tagging, have good enough accuracies to allow automatic analysis. We also manually annotated this corpus according to several types of information which could prove useful for future work: character references, episodes, and motifs. The contributions are the creation of an corpus of French tales from classical anthropology material, which will be made available to the community; the evaluation of OCR and NLP tools on this corpus; and the annotation with anthropological information.</abstract>
    </paper>
    <paper id="54">
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Federico</first><last>Boschetti</last></author>
      <author><first>Harry</first><last>Diakoff</last></author>
      <author><first>Riccardo</first><last>Del Gratta</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Gregory</first><last>Crane</last></author>
      <title>The Making of <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <pages>1140–1147</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1071_Paper.pdf</url>
      <abstract>This paper describes the process of creation and review of a new lexico-semantic resource for the classical studies: AncientGreekWordNet. The candidate sets of synonyms (synsets) are extracted from Greek-English dictionaries, on the assumption that Greek words translated by the same English word or phrase have a high probability of being synonyms or at least semantically closely related. The process of validation and the web interface developed to edit and query the resource are described in detail. The lexical coverage of Ancient Greek WordNet is illustrated and the accuracy is evaluated. Finally, scenarios for exploiting the resource are discussed.</abstract>
    </paper>
    <paper id="55">
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>William</first><last>Lewis</last></author>
      <author><first>Michael Wayne</first><last>Goodman</last></author>
      <author><first>Joshua</first><last>Crowgey</last></author>
      <author><first>Emily M.</first><last>Bender</last></author>
      <title>Enriching <fixed-case>ODIN</fixed-case></title>
      <pages>3151–3157</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1072_Paper.pdf</url>
      <abstract>In this paper, we describe the expansion of the ODIN resource, a database containing many thousands of instances of Interlinear Glossed Text (IGT) for over a thousand languages harvested from scholarly linguistic papers posted to the Web. A database containing a large number of instances of IGT, which are effectively richly annotated and heuristically aligned bitexts, provides a unique resource for bootstrapping NLP tools for resource-poor languages. To make the data in ODIN more readily consumable by tool developers and NLP researchers, we propose a new XML format for IGT, called Xigt. We call the updated release ODIN-II.</abstract>
    </paper>
    <paper id="56">
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <title><fixed-case>T</fixed-case>urkish Treebank as a Gold Standard for Morphological Disambiguation and Its Influence on Parsing</title>
      <pages>3360–3365</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1073_Paper.pdf</url>
      <abstract>So far predicted scenarios for Turkish dependency parsing have used a morphological disambiguator that is trained on the data distributed with the tool(Sak et al., 2008). Although models trained on this data have high accuracy scores on the test and development data of the same set, the accuracy drastically drops when the model is used in the preprocessing of Turkish Treebank parsing experiments. We propose to use the Turkish Treebank(Oflazer et al., 2003) as a morphological resource to overcome this problem and convert the treebank to the morphological disambiguators format. The experimental results show that we achieve improvements in disambiguating the Turkish Treebank and the results also carry over to parsing. With the help of better morphological analysis, we present the best labelled dependency parsing scores to date on Turkish.</abstract>
    </paper>
    <paper id="57">
      <author><first>Krešimir</first><last>Šojat</last></author>
      <author><first>Matea</first><last>Srebačić</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Tin</first><last>Pavelić</last></author>
      <title><fixed-case>C</fixed-case>ro<fixed-case>D</fixed-case>eri<fixed-case>V</fixed-case>: a new resource for processing <fixed-case>C</fixed-case>roatian morphology</title>
      <pages>3366–3370</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1074_Paper.pdf</url>
      <abstract>The paper deals with the processing of Croatian morphology and presents CroDeriV ― a newly developed language resource that contains data about morphological structure and derivational relatedness of verbs in Croatian. In its present shape, CroDeriV contains 14 192 Croatian verbs. Verbs in CroDeriV are analyzed for morphemes and segmented into lexical, derivational and inflectional morphemes. The structure of CroDeriV enables the detection of verbal derivational families in Croatian as well as the distribution and frequency of particular affixes and lexical morphemes. Derivational families consist of a verbal base form and all prefixed or suffixed derivatives detected in available machine readable Croatian dictionaries and corpora. Language data structured in this way was further used for the expansion of other language resources for Croatian, such as Croatian WordNet and the Croatian Morphological Lexicon. Matching the data from CroDeriV on one side and Croatian WordNet and the Croatian Morphological Lexicon on the other resulted in significant enrichment of Croatian WordNet and enlargement of the Croatian Morphological Lexicon.</abstract>
    </paper>
    <paper id="58">
      <author><first>Masaya</first><last>Yamaguchi</last></author>
      <title>Building a Database of <fixed-case>J</fixed-case>apanese Adjective Examples from Special Purpose Web Corpora</title>
      <pages>3684–3688</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1075_Paper.pdf</url>
      <abstract>It is often difficult to collect many examples for low-frequency words from a single general purpose corpus. In this paper, I present a method of building a database of Japanese adjective examples from special purpose Web corpora (SPW corpora) and investigates the characteristics of examples in the database by comparison with examples that are collected from a general purpose Web corpus (GPW corpus). My proposed method construct a SPW corpus for each adjective considering to collect examples that have the following features: (i) non-bias, (ii) the distribution of examples extracted from every SPW corpus bears much similarity to that of examples extracted from a GPW corpus. The results of experiments shows the following: (i) my proposed method can collect many examples rapidly. The number of examples extracted from SPW corpora is more than 8.0 times (median value) greater than that from the GPW corpus. (ii) the distributions of co-occurrence words for adjectives in the database are similar to those taken from the GPW corpus.</abstract>
    </paper>
    <paper id="59">
      <author><first>George</first><last>Christodoulides</last></author>
      <title><fixed-case>P</fixed-case>raaline: Integrating Tools for Speech Corpus Research</title>
      <pages>31–34</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1078_Paper.pdf</url>
      <abstract>This paper presents Praaline, an open-source software system for managing, annotating, analysing and visualising speech corpora. Researchers working with speech corpora are often faced with multiple tools and formats, and they need to work with ever-increasing amounts of data in a collaborative way. Praaline integrates and extends existing time-proven tools for spoken corpora analysis (Praat, Sonic Visualiser and a bridge to the R statistical package) in a modular system, facilitating automation and reuse. Users are exposed to an integrated, user-friendly interface from which to access multiple tools. Corpus metadata and annotations may be stored in a database, locally or remotely, and users can define the metadata and annotation structure. Users may run a customisable cascade of analysis steps, based on plug-ins and scripts, and update the database with the results. The corpus database may be queried, to produce aggregated data-sets. Praaline is extensible using Python or C++ plug-ins, while Praat and R scripts may be executed against the corpus data. A series of visualisations, editors and plug-ins are provided. Praaline is free software, released under the GPL license (www.praaline.org).</abstract>
    </paper>
    <paper id="60">
      <author><first>Dana</first><last>Dannélls</last></author>
      <author><first>Normunds</first><last>Gruzitis</last></author>
      <title>Extracting a bilingual semantic grammar from <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et-annotated corpora</title>
      <pages>2466–2473</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1079_Paper.pdf</url>
      <abstract>We present the creation of an English-Swedish FrameNet-based grammar in Grammatical Framework. The aim of this research is to make existing framenets computationally accessible for multilingual natural language applications via a common semantic grammar API, and to facilitate the porting of such grammar to other languages. In this paper, we describe the abstract syntax of the semantic grammar while focusing on its automatic extraction possibilities. We have extracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley FrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The abstract syntax defines 769 frame-specific valence patterns that cover 77,8% examples in BFN and 74,9% in SweFN belonging to the shared set of 471 frames. As a side result, we provide a unified method for comparing semantic and syntactic valence patterns across framenets.</abstract>
    </paper>
    <paper id="61">
      <author><first>Fabienne</first><last>Braune</last></author>
      <author><first>Daniel</first><last>Bauer</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <title>Mapping Between <fixed-case>E</fixed-case>nglish Strings and Reentrant Semantic Graphs</title>
      <pages>4493–4498</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1080_Paper.pdf</url>
      <abstract>We investigate formalisms for capturing the relation between semantic graphs and English strings. Semantic graph corpora have spurred recent interest in graph transduction formalisms, but it is not yet clear whether such formalisms are a good fit for natural language data―in particular, for describing how semantic reentrancies correspond to English pronouns, zero pronouns, reflexives, passives, nominalizations, etc. We introduce a data set that focuses on these problems, we build grammars to capture the graph/string relation in this data, and we evaluate those grammars for conciseness and accuracy.</abstract>
    </paper>
    <paper id="62">
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Sören</first><last>Schalowski</last></author>
      <author><first>Heike</first><last>Wiese</last></author>
      <title>The <fixed-case>K</fixed-case>iez<fixed-case>D</fixed-case>eutsch Korpus (<fixed-case>K</fixed-case>i<fixed-case>DK</fixed-case>o) Release 1.0</title>
      <pages>3927–3934</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1081_Paper.pdf</url>
      <abstract>This paper presents the first release of the KiezDeutsch Korpus (KiDKo), a new language resource with multiparty spoken dialogues of Kiezdeutsch, a newly emerging language variety spoken by adolescents from multiethnic urban areas in Germany. The first release of the corpus includes the transcriptions of the data as well as a normalisation layer and part-of-speech annotations. In the paper, we describe the main features of the new resource and then focus on automatic POS tagging of informal spoken language. Our tagger achieves an accuracy of nearly 97% on KiDKo. While we did not succeed in further improving the tagger using ensemble tagging, we present our approach to using the tagger ensembles for identifying error patterns in the automatically tagged data.</abstract>
    </paper>
    <paper id="63">
      <author><first>Gerard</first><last>de Melo</last></author>
      <title>Etymological <fixed-case>W</fixed-case>ordnet: Tracing The History of Words</title>
      <pages>1148–1154</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1083_Paper.pdf</url>
      <abstract>Research on the history of words has led to remarkable insights about language and also about the history of human civilization more generally. This paper presents the Etymological Wordnet, the first database that aims at making word origin information available as a large, machine-readable network of words in many languages. The information in this resource is obtained from Wiktionary. Extracting a network of etymological information from Wiktionary requires significant effort, as much of the etymological information is only given in prose. We rely on custom pattern matching techniques and mine a large network with over 500,000 word origin links as well as over 2 million derivational/compositional links.</abstract>
    </paper>
    <paper id="64">
      <author><first>Victoria</first><last>Rosén</last></author>
      <author><first>Petter</first><last>Haugereid</last></author>
      <author><first>Martha</first><last>Thunes</last></author>
      <author><first>Gyri S.</first><last>Losnegaard</last></author>
      <author><first>Helge</first><last>Dyvik</last></author>
      <title>The Interplay Between Lexical and Syntactic Resources in Incremental Parsebanking</title>
      <pages>1617–1624</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1085_Paper.pdf</url>
      <abstract>Automatic syntactic analysis of a corpus requires detailed lexical and morphological information that cannot always be harvested from traditional dictionaries. In building the INESS Norwegian treebank, it is often the case that necessary lexical information is missing in the morphology or lexicon. The approach used to build the treebank is incremental parsebanking; a corpus is parsed with an existing grammar, and the analyses are efficiently disambiguated by annotators. When the intended analysis is unavailable after parsing, the reason is often that necessary information is not available in the lexicon. INESS has therefore implemented a text preprocessing interface where annotators can enter unrecognized words before parsing. This may concern words that are unknown to the morphology and/or lexicon, and also words that are known, but for which important information is missing. When this information is added, either during text preprocessing or during disambiguation, the result is that after reparsing the intended analysis can be chosen and stored in the treebank. The lexical information added to the lexicon in this way may be of great interest both to lexicographers and to other language technology efforts, and the enriched lexical resource being developed will be made available at the end of the project.</abstract>
    </paper>
    <paper id="65">
      <author><first>Rafal</first><last>Rak</last></author>
      <author><first>Jacob</first><last>Carter</last></author>
      <author><first>Andrew</first><last>Rowley</last></author>
      <author><first>Riza Theresa</first><last>Batista-Navarro</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <title>Interoperability and Customisation of Annotation Schemata in Argo</title>
      <pages>3837–3842</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1086_Paper.pdf</url>
      <abstract>The process of annotating text corpora involves establishing annotation schemata which define the scope and depth of an annotation task at hand. We demonstrate this activity in Argo, a Web-based workbench for the analysis of textual resources, which facilitates both automatic and manual annotation. Annotation tasks in the workbench are defined by building workflows consisting of a selection of available elementary analytics developed in compliance with the Unstructured Information Management Architecture specification. The architecture accommodates complex annotation types that may define primitive as well as referential attributes. Argo aids the development of custom annotation schemata and supports their interoperability by featuring a schema editor and specialised analytics for schemata alignment. The schema editor is a self-contained graphical user interface for defining annotation types. Multiple heterogeneous schemata can be aligned by including one of two type mapping analytics currently offered in Argo. One is based on a simple mapping syntax and, although limited in functionality, covers most common use cases. The other utilises a well established graph query language, SPARQL, and is superior to other state-of-the-art solutions in terms of expressiveness. We argue that the customisation of annotation schemata does not need to compromise their interoperability.</abstract>
    </paper>
    <paper id="66">
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Mateusz</first><last>Kopeć</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <title><fixed-case>P</fixed-case>olish Coreference Corpus in Numbers</title>
      <pages>3234–3238</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1088_Paper.pdf</url>
      <abstract>This paper attempts a preliminary interpretation of the occurrence of different types of linguistic constructs in the manually-annotated Polish Coreference Corpus by providing analyses of various statistical properties related to mentions, clusters and near-identity links. Among others, frequency of mentions, zero subjects and singleton clusters is presented, as well as the average mention and cluster size. We also show that some coreference clustering constraints, such as gender or number agreement, are frequently not valid in case of Polish. The need for lemmatization for automatic coreference resolution is supported by an empirical study. Correlation between cluster and mention count within a text is investigated, with short characteristics of outlier cases. We also examine this correlation in each of the 14 text domains present in the corpus and show that none of them has abnormal frequency of outlier texts regarding the cluster/mention ratio. Finally, we report on our negative experiences concerning the annotation of the near-identity relation. In the conclusion we put forward some guidelines for the future research in the area.</abstract>
    </paper>
    <paper id="67">
      <author><first>Natalia</first><last>Silveira</last></author>
      <author><first>Timothy</first><last>Dozat</last></author>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <author><first>Miriam</first><last>Connor</last></author>
      <author><first>John</first><last>Bauer</last></author>
      <author><first>Chris</first><last>Manning</last></author>
      <title>A Gold Standard Dependency Corpus for <fixed-case>E</fixed-case>nglish</title>
      <pages>2897–2904</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf</url>
      <abstract>We present a gold standard annotation of syntactic dependencies in the English Web Treebank corpus using the Stanford Dependencies formalism. This resource addresses the lack of a gold standard dependency treebank for English, as well as the limited availability of gold standard syntactic annotations for English informal text genres. We also present experiments on the use of this resource, both for training dependency parsers and for evaluating the quality of different versions of the Stanford Parser, which includes a converter tool to produce dependency annotation from constituency trees. We show that training a dependency parser on a mix of newswire and web data leads to better performance on that type of data without hurting performance on newswire text, and therefore gold standard annotations for non-canonical text can be a valuable resource for parsing. Furthermore, the systematic annotation effort has informed both the SD formalism and its implementation in the Stanford Parser’s dependency converter. In response to the challenges encountered by annotators in the EWT corpus, the formalism has been revised and extended, and the converter has been improved.</abstract>
    </paper>
    <paper id="68">
      <author><first>Jan</first><last>Šnajder</last></author>
      <title><fixed-case>D</fixed-case>eriv<fixed-case>B</fixed-case>ase.hr: A High-Coverage Derivational Morphology Resource for <fixed-case>C</fixed-case>roatian</title>
      <pages>3371–3377</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1090_Paper.pdf</url>
      <abstract>Knowledge about derivational morphology has been proven useful for a number of natural language processing (NLP) tasks. We describe the construction and evaluation of DerivBase.hr, a large-coverage morphological resource for Croatian. DerivBase.hr groups 100k lemmas from web corpus hrWaC into 56k clusters of derivationally related lemmas, so-called derivational families. We focus on suffixal derivation between and within nouns, verbs, and adjectives. We propose two approaches: an unsupervised approach and a knowledge-based approach based on a hand-crafted morphology model but without using any additional lexico-semantic resources The resource acquisition procedure consists of three steps: corpus preprocessing, acquisition of an inflectional lexicon, and the induction of derivational families. We describe an evaluation methodology based on manually constructed derivational families from which we sample and annotate pairs of lemmas. We evaluate DerivBase.hr on the so-obtained sample, and show that the knowledge-based version attains good clustering quality of 81.2% precision, 76.5% recall, and 78.8% F1 -score. As with similar resources for other languages, we expect DerivBase.hr to be useful for a number of NLP tasks.</abstract>
    </paper>
    <paper id="69">
      <author><first>Simon</first><last>Scerri</last></author>
      <author><first>Behrang Q.</first><last>Zadeh</last></author>
      <author><first>Maciej</first><last>Dabrowski</last></author>
      <author><first>Ismael</first><last>Rivera</last></author>
      <title>Extracting Information for Context-aware Meeting Preparation</title>
      <pages>120–124</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1092_Paper.pdf</url>
      <abstract>People working in an office environment suffer from large volumes of information that they need to manage and access. Frequently, the problem is due to machines not being able to recognise the many implicit relationships between office artefacts, and also due to them not being aware of the context surrounding them. In order to expose these relationships and enrich artefact context, text analytics can be employed over semi-structured and unstructured content, including free text. In this paper, we explain how this strategy is applied and partly evaluated for a specific use-case: supporting the attendees of a calendar event to prepare for the meeting.</abstract>
    </paper>
    <paper id="70">
      <author><first>Kai</first><last>Hong</last></author>
      <author><first>John</first><last>Conroy</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Alex</first><last>Kulesza</last></author>
      <author><first>Hui</first><last>Lin</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <title>A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization</title>
      <pages>1608–1616</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1093_Paper.pdf</url>
      <abstract>In the period since 2004, many novel sophisticated approaches for generic multi-document summarization have been developed. Intuitive simple approaches have also been shown to perform unexpectedly well for the task. Yet it is practically impossible to compare the existing approaches directly, because systems have been evaluated on different datasets, with different evaluation measures, against different sets of comparison systems. Here we present a corpus of summaries produced by several state-of-the-art extractive summarization systems or by popular baseline systems. The inputs come from the 2004 DUC evaluation, the latest year in which generic summarization was addressed in a shared task. We use the same settings for ROUGE automatic evaluation to compare the systems directly and analyze the statistical significance of the differences in performance. We show that in terms of average scores the state-of-the-art systems appear similar but that in fact they produce very different summaries. Our corpus will facilitate future research on generic summarization and motivates the need for development of more sensitive evaluation measures and for approaches to system combination in summarization.</abstract>
    </paper>
    <paper id="71">
      <author><first>Zhiyi</first><last>Song</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Haejoong</first><last>Lee</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <author><first>Jennifer</first><last>Garland</last></author>
      <author><first>Dana</first><last>Fore</last></author>
      <author><first>Brian</first><last>Gainor</last></author>
      <author><first>Preston</first><last>Cabe</last></author>
      <author><first>Thomas</first><last>Thomas</last></author>
      <author><first>Brendan</first><last>Callahan</last></author>
      <author><first>Ann</first><last>Sawyer</last></author>
      <title>Collecting Natural <fixed-case>SMS</fixed-case> and Chat Conversations in Multiple Languages: The <fixed-case>BOLT</fixed-case> Phase 2 Corpus</title>
      <pages>1699–1704</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1094_Paper.pdf</url>
      <abstract>The DARPA BOLT Program develops systems capable of allowing English speakers to retrieve and understand information from informal foreign language genres. Phase 2 of the program required large volumes of naturally occurring informal text (SMS) and chat messages from individual users in multiple languages to support evaluation of machine translation systems. We describe the design and implementation of a robust collection system capable of capturing both live and archived SMS and chat conversations from willing participants. We also discuss the challenges recruitment at a time when potential participants have acute and growing concerns about their personal privacy in the realm of digital communication, and we outline the techniques adopted to confront those challenges. Finally, we review the properties of the resulting BOLT Phase 2 Corpus, which comprises over 6.5 million words of naturally-occurring chat and SMS in English, Chinese and Egyptian Arabic.</abstract>
    </paper>
    <paper id="72">
      <author><first>Bruno</first><last>Laranjeira</last></author>
      <author><first>Viviane</first><last>Moreira</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Maria José</first><last>Finatto</last></author>
      <title>Comparing the Quality of Focused Crawlers and of the Translation Resources Obtained from them</title>
      <pages>3572–3578</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1095_Paper.pdf</url>
      <abstract>Comparable corpora have been used as an alternative for parallel corpora as resources for computational tasks that involve domain-specific natural language processing. One way to gather documents related to a specific topic of interest is to traverse a portion of the web graph in a targeted way, using focused crawling algorithms. In this paper, we compare several focused crawling algorithms using them to collect comparable corpora on a specific domain. Then, we compare the evaluation of the focused crawling algorithms to the performance of linguistic processes executed after training with the corresponding generated corpora. Also, we propose a novel approach for focused crawling, exploiting the expressive power of multiword expressions.</abstract>
    </paper>
    <paper id="73">
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Archna</first><last>Bhatia</last></author>
      <author><first>Manaal</first><last>Faruqui</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <title>Augmenting <fixed-case>E</fixed-case>nglish Adjective Senses with Supersenses</title>
      <pages>4359–4365</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1096_Paper.pdf</url>
      <abstract>We develop a supersense taxonomy for adjectives, based on that of GermaNet, and apply it to English adjectives in WordNet using human annotation and supervised classification. Results show that accuracy for automatic adjective type classification is high, but synsets are considerably more difficult to classify, even for trained human annotators. We release the manually annotated data, the classifier, and the induced supersense labeling of 12,304 WordNet adjective synsets.</abstract>
    </paper>
    <paper id="74">
      <author><first>Christian</first><last>Buck</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Bas</first><last>van Ooyen</last></author>
      <title><fixed-case>N</fixed-case>-gram Counts and Language Models from the <fixed-case>C</fixed-case>ommon <fixed-case>C</fixed-case>rawl</title>
      <pages>3579–3584</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1097_Paper.pdf</url>
      <abstract>We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English $5$-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5-1.4 BLEU by using large language models to translate into various languages.</abstract>
    </paper>
    <paper id="75">
      <author><first>Tim</first><last>vor der Brück</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <author><first>Zahurul</first><last>Islam</last></author>
      <title><fixed-case>C</fixed-case>ol<fixed-case>L</fixed-case>ex.en: Automatically Generating and Evaluating a Full-form Lexicon for <fixed-case>E</fixed-case>nglish</title>
      <pages>3756–3760</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1099_Paper.pdf</url>
      <abstract>The paper describes a procedure for the automatic generation of a large full-form lexicon of English. We put emphasis on two statistical methods to lexicon extension and adjustment: in terms of a letter-based HMM and in terms of a detector of spelling variants and misspellings. The resulting resource, \collexen, is evaluated with respect to two tasks: text categorization and lexical coverage by example of the SUSANNE corpus and the \openanc.</abstract>
    </paper>
    <paper id="76">
      <author><first>Tamara</first><last>Polajnar</last></author>
      <author><first>Laura</first><last>Rimell</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <title>Evaluation of Simple Distributional Compositional Operations on Longer Texts</title>
      <pages>4440–4443</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/110_Paper.pdf</url>
      <abstract>Distributional semantic models have been effective at representing linguistic semantics at the word level, and more recently research has moved on to the construction of distributional representations for larger segments of text. However, it is not well understood how the composition operators that work well on short phrase-based models scale up to full-length sentences. In this paper we test several simple compositional methods on a sentence-length similarity task and discover that their performance peaks at fewer than ten operations. We also introduce a novel sentence segmentation method that reduces the number of compositional operations.</abstract>
    </paper>
    <paper id="77">
      <author><first>Horacio</first><last>Saggion</last></author>
      <title>Creating Summarization Systems with <fixed-case>SUMMA</fixed-case></title>
      <pages>4157–4163</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1102_Paper.pdf</url>
      <abstract>Automatic text summarization, the reduction of a text to its essential content is fundamental for an on-line information society. Although many summarization algorithms exist, there are few tools or infrastructures providing capabilities for developing summarization applications. This paper presents a new version of SUMMA, a text summarization toolkit for the development of adaptive summarization applications. SUMMA includes algorithms for computation of various sentence relevance features and functionality for single and multidocument summarization in various languages. It also offers methods for content-based evaluation of summaries.</abstract>
    </paper>
    <paper id="78">
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Serge</first><last>ter Braake</last></author>
      <author><first>Niels</first><last>Ockeloen</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Susan</first><last>Legêne</last></author>
      <author><first>Guus</first><last>Schreiber</last></author>
      <title><fixed-case>B</fixed-case>iography<fixed-case>N</fixed-case>et: Methodological Issues when <fixed-case>NLP</fixed-case> supports historical research</title>
      <pages>3728–3735</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1103_Paper.pdf</url>
      <abstract>When NLP is used to support research in the humanities, new methodological issues come into play. NLP methods may introduce a bias in their analysis that can influence the results of the hypothesis a humanities scholar is testing. This paper addresses this issue in the context of BiographyNet a multi-disciplinary project involving NLP, Linked Data and history. We introduce the project to the NLP community. We argue that it is essential for historians to get insight into the provenance of information, including how information was extracted from text by NLP tools.</abstract>
    </paper>
    <paper id="79">
      <author><first>Anthony</first><last>Rousseau</last></author>
      <author><first>Paul</first><last>Deléglise</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <title>Enhancing the <fixed-case>TED</fixed-case>-<fixed-case>LIUM</fixed-case> Corpus with Selected Data for Language Modeling and More <fixed-case>TED</fixed-case> Talks</title>
      <pages>3935–3939</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1104_Paper.pdf</url>
      <abstract>In this paper, we present improvements made to the TED-LIUM corpus we released in 2012. These enhancements fall into two categories. First, we describe how we filtered publicly available monolingual data and used it to estimate well-suited language models (LMs), using open-source tools. Then, we describe the process of selection we applied to new acoustic data from TED talks, providing additions to our previously released corpus. Finally, we report some experiments we made around these improvements.</abstract>
    </paper>
    <paper id="80">
      <author><first>Ajay</first><last>Dubey</last></author>
      <author><first>Parth</first><last>Gupta</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <title>Enrichment of Bilingual Dictionary through News Stream Data</title>
      <pages>3761–3765</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1105_Paper.pdf</url>
      <abstract>Bilingual dictionaries are the key component of the cross-lingual similarity estimation methods. Usually such dictionary generation is accomplished by manual or automatic means. Automatic generation approaches include to exploit parallel or comparable data to derive dictionary entries. Such approaches require large amount of bilingual data in order to produce good quality dictionary. Many time the language pair does not have large bilingual comparable corpora and in such cases the best automatic dictionary is upper bounded by the quality and coverage of such corpora. In this work we propose a method which exploits continuous quasi-comparable corpora to derive term level associations for enrichment of such limited dictionary. Though we propose our experiments for English and Hindi, our approach can be easily extendable to other languages. We evaluated dictionary by manually computing the precision. In experiments we show our approach is able to derive interesting term level associations across languages.</abstract>
    </paper>
    <paper id="81">
      <author><first>Darja</first><last>Fišer</last></author>
      <author><first>Aleš</first><last>Tavčar</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <title>slo<fixed-case>WC</fixed-case>rowd: A crowdsourcing tool for lexicographic tasks</title>
      <pages>3471–3475</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1106_Paper.pdf</url>
      <abstract>The paper presents sloWCrowd, a simple tool developed to facilitate crowdsourcing lexicographic tasks, such as error correction in automatically generated wordnets and semantic annotation of corpora. The tool is open-source, language-independent and can be adapted to a broad range of crowdsourcing tasks. Since volunteers who participate in our crowdsourcing tasks are not trained lexicographers, the tool has been designed to obtain multiple answers to the same question and compute the majority vote, making sure individual unreliable answers are discarded. We also make sure unreliable volunteers, who systematically provide unreliable answers, are not taken into account. This is achieved by measuring their accuracy against a gold standard, the questions from which are posed to the annotators on a regular basis in between the real question. We tested the tool in an extensive crowdsourcing task, i.e. error correction of the Slovene wordnet, the results of which are encouraging, motivating us to use the tool in other annotation tasks in the future as well.</abstract>
    </paper>
    <paper id="82">
      <author><first>Tomohide</first><last>Shibata</last></author>
      <author><first>Shotaro</first><last>Kohama</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>A Large Scale Database of Strongly-related Events in <fixed-case>J</fixed-case>apanese</title>
      <pages>3283–3288</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1107_Paper.pdf</url>
      <abstract>The knowledge about the relation between events is quite useful for coreference resolution, anaphora resolution, and several NLP applications such as dialogue system. This paper presents a large scale database of strongly-related events in Japanese, which has been acquired with our proposed method (Shibata and Kurohashi, 2011). In languages, where omitted arguments or zero anaphora are often utilized, such as Japanese, the coreference-based event extraction methods are hard to be applied, and so our method extracts strongly-related events in a two-phrase construct. This method first calculates the co-occurrence measure between predicate-arguments (events), and regards an event pair, whose mutual information is high, as strongly-related events. To calculate the co-occurrence measure efficiently, we adopt an association rule mining method. Then, we identify the remaining arguments by using case frames. The database contains approximately 100,000 unique events, with approximately 340,000 strongly-related event pairs, which is much larger than an existing automatically-constructed event database. We evaluated randomly-chosen 100 event pairs, and the accuracy was approximately 68%.</abstract>
    </paper>
    <paper id="83">
      <author><first>Thomas</first><last>François</last></author>
      <author><first>Nùria</first><last>Gala</last></author>
      <author><first>Patrick</first><last>Watrin</last></author>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <title><fixed-case>FLEL</fixed-case>ex: a graded lexical resource for <fixed-case>F</fixed-case>rench foreign learners</title>
      <pages>3766–3773</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1108_Paper.pdf</url>
      <abstract>In this paper we present FLELex, the first graded lexicon for French as a foreign language (FFL) that reports word frequencies by difficulty level (according to the CEFR scale). It has been obtained from a tagged corpus of 777,000 words from available textbooks and simplified readers intended for FFL learners. Our goal is to freely provide this resource to the community to be used for a variety of purposes going from the assessment of the lexical difficulty of a text, to the selection of simpler words within text simplification systems, and also as a dictionary in assistive tools for writing.</abstract>
    </paper>
    <paper id="84">
      <author><first>Lucas</first><last>Hilgert</last></author>
      <author><first>Lucelene</first><last>Lopes</last></author>
      <author><first>Artur</first><last>Freitas</last></author>
      <author><first>Renata</first><last>Vieira</last></author>
      <author><first>Denise</first><last>Hogetop</last></author>
      <author><first>Aline</first><last>Vanin</last></author>
      <title>Building Domain Specific Bilingual Dictionaries</title>
      <pages>2772–2777</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1112_Paper.pdf</url>
      <abstract>This paper proposes a method to build bilingual dictionaries for specific domains defined by a parallel corpora. The proposed method is based on an original method that is not domain specific. Both the original and the proposed methods are constructed with previously available natural language processing tools. Therefore, this paper contribution resides in the choice and parametrization of the chosen tools. To illustrate the proposed method benefits we conduct an experiment over technical manuals in English and Portuguese. The results of our proposed method were analyzed by human specialists and our results indicates significant increases in precision for unigrams and muli-grams. Numerically, the precision increase is as big as 15% according to our evaluation.</abstract>
    </paper>
    <paper id="85">
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Natalie</first><last>Kübler</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <title>A Corpus of Machine Translation Errors Extracted from Translation Students Exercises</title>
      <pages>3585–3588</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1115_Paper.pdf</url>
      <abstract>In this paper, we present a freely available corpus of automatic translations accompanied with post-edited versions, annotated with labels identifying the different kinds of errors made by the MT system. These data have been extracted from translation students exercises that have been corrected by a senior professor. This corpus can be useful for training quality estimation tools and for analyzing the types of errors made MT system.</abstract>
    </paper>
    <paper id="86">
      <author><first>Clare</first><last>Voss</last></author>
      <author><first>Stephen</first><last>Tratz</last></author>
      <author><first>Jamal</first><last>Laoudi</last></author>
      <author><first>Douglas</first><last>Briesch</last></author>
      <title>Finding <fixed-case>R</fixed-case>omanized <fixed-case>A</fixed-case>rabic Dialect in Code-Mixed Tweets</title>
      <pages>2249–2253</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1116_Paper.pdf</url>
      <abstract>Recent computational work on Arabic dialect identification has focused primarily on building and annotating corpora written in Arabic script. Arabic dialects however also appear written in Roman script, especially in social media. This paper describes our recent work developing tweet corpora and a token-level classifier that identifies a Romanized Arabic dialect and distinguishes it from French and English in tweets. We focus on Moroccan Darija, one of several spoken vernaculars in the family of Maghrebi Arabic dialects. Even given noisy, code-mixed tweets,the classifier achieved token-level recall of 93.2% on Romanized Arabic dialect, 83.2% on English, and 90.1% on French. The classifier, now integrated into our tweet conversation annotation tool (Tratz et al. 2013), has semi-automated the construction of a Romanized Arabic-dialect lexicon. Two datasets, a full list of Moroccan Darija surface token forms and a table of lexical entries derived from this list with spelling variants, as extracted from our tweet corpus collection, will be made available in the LRE MAP.</abstract>
    </paper>
    <paper id="87">
      <author><first>Nicolas</first><last>Auguin</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <title>Co-Training for Classification of Live or Studio Music Recordings</title>
      <pages>3650–3653</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1119_Paper.pdf</url>
      <abstract>The fast-spreading development of online streaming services has enabled people from all over the world to listen to music. However, it is not always straightforward for a given user to find the “right” song version he or she is looking for. As streaming services may be affected by the potential dissatisfaction among their customers, the quality of songs and the presence of tags (or labels) associated with songs returned to the users are very important. Thus, the need for precise and reliable metadata becomes paramount. In this work, we are particularly interested in distinguishing between live and studio versions of songs. Specifically, we tackle the problem in the case where very little-annotated training data are available, and demonstrate how an original co-training algorithm in a semi-supervised setting can alleviate the problem of data scarcity to successfully discriminate between live and studio music recordings.</abstract>
    </paper>
    <paper id="88">
      <author><first>Marc</first><last>Tomlinson</last></author>
      <author><first>David</first><last>Bracewell</last></author>
      <author><first>Wayne</first><last>Krug</last></author>
      <author><first>David</first><last>Hinote</last></author>
      <title>#mygoal: Finding Motivations on <fixed-case>T</fixed-case>witter</title>
      <pages>469–474</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1120_Paper.pdf</url>
      <abstract>Our everyday language reflects our psychological and cognitive state and effects the states of other individuals. In this contribution we look at the intersection between motivational state and language. We create a set of hashtags, which are annotated for the degree to which they are used by individuals to mark-up language that is indicative of a collection of factors that interact with an individual’s motivational state. We look for tags that reflect a goal mention, reward, or a perception of control. Finally, we present results for a language-model based classifier which is able to predict the presence of one of these factors in a tweet with between 69\% and 80\% accuracy on a balanced testing set. Our approach suggests that hashtags can be used to understand, not just the language of topics, but the deeper psychological and social meaning of a tweet.</abstract>
    </paper>
    <paper id="89">
      <author><first>David</first><last>Graff</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Xiaoyi</first><last>Ma</last></author>
      <author><first>Karen</first><last>Jones</last></author>
      <author><first>Ann</first><last>Sawyer</last></author>
      <title>The <fixed-case>RATS</fixed-case> Collection: Supporting <fixed-case>HLT</fixed-case> Research with Degraded Audio Data</title>
      <pages>1970–1977</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1125_Paper.pdf</url>
      <abstract>The DARPA RATS program was established to foster development of language technology systems that can perform well on speaker-to-speaker communications over radio channels that evince a wide range in the type and extent of signal variability and acoustic degradation. Creating suitable corpora to address this need poses an equally wide range of challenges for the collection, annotation and quality assessment of relevant data. This paper describes the LDCs multi-year effort to build the RATS data collection, summarizes the content and properties of the resulting corpora, and discusses the novel problems and approaches involved in ensuring that the data would satisfy its intended use, to provide speech recordings and annotations for training and evaluating HLT systems that perform 4 specific tasks on difficult radio channels: Speech Activity Detection (SAD), Language Identification (LID), Speaker Identification (SID) and Keyword Spotting (KWS).</abstract>
    </paper>
    <paper id="90">
      <author><first>Chris</first><last>Hokamp</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Peter</first><last>Schuelke</last></author>
      <title>Modeling Language Proficiency Using Implicit Feedback</title>
      <pages>3983–3986</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1126_Paper.pdf</url>
      <abstract>We describe the results of several experiments with interactive interfaces for native and L2 English students, designed to collect implicit feedback from students as they complete a reading activity. In this study, implicit means that all data is obtained without asking the user for feedback. To test the value of implicit feedback for assessing student proficiency, we collect features of user behavior and interaction, which are then used to train classification models. Based upon the feedback collected during these experiments, a students performance on a quiz and proficiency relative to other students can be accurately predicted, which is a step on the path to our goal of providing automatic feedback and unintrusive evaluation in interactive learning environments.</abstract>
    </paper>
    <paper id="91">
      <author><first>Kevin</first><last>Reschke</last></author>
      <author><first>Martin</first><last>Jankowiak</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <author><first>Daniel</first><last>Jurafsky</last></author>
      <title>Event Extraction Using Distant Supervision</title>
      <pages>4527–4531</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1127_Paper.pdf</url>
      <abstract>Distant supervision is a successful paradigm that gathers training data for information extraction systems by automatically aligning vast databases of facts with text. Previous work has demonstrated its usefulness for the extraction of binary relations such as a person’s employer or a film’s director. Here, we extend the distant supervision approach to template-based event extraction, focusing on the extraction of passenger counts, aircraft types, and other facts concerning airplane crash events. We present a new publicly available dataset and event extraction task in the plane crash domain based on Wikipedia infoboxes and newswire text. Using this dataset, we conduct a preliminary evaluation of four distantly supervised extraction models which assign named entity mentions in text to entries in the event template. Our results indicate that joint inference over sequences of candidate entity mentions is beneficial. Furthermore, we demonstrate that the Searn algorithm outperforms a linear-chain CRF and strong baselines with local inference.</abstract>
    </paper>
    <paper id="92">
      <author><first>Stefan</first><last>Ultes</last></author>
      <author><first>Hüseyin</first><last>Dikme</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <title>First Insight into Quality-Adaptive Dialogue</title>
      <pages>246–251</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/113_Paper.pdf</url>
      <abstract>While Spoken Dialogue Systems have gained in importance in recent years, most systems applied in the real world are still static and error-prone. To overcome this, the user is put into the focus of dialogue management. Hence, an approach for adapting the course of the dialogue to Interaction Quality, an objective variant of user satisfaction, is presented in this work. In general, rendering the dialogue adaptive to user satisfaction enables the dialogue system to improve the course of the dialogue and to handle problematic situations better. In this contribution, we present a pilot study of quality-adaptive dialogue. By selecting the confirmation strategy based on the current IQ value, the course of the dialogue is adapted in order to improve the overall user experience. In a user experiment comparing three different confirmation strategies in a train booking domain, the adaptive strategy performs successful and is among the two best rated strategies based on the overall user experience.</abstract>
    </paper>
    <paper id="93">
      <author><first>Antonio</first><last>Toral</last></author>
      <title><fixed-case>TLAXCALA</fixed-case>: a multilingual corpus of independent news</title>
      <pages>3689–3692</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1134_Paper.pdf</url>
      <abstract>We acquire corpora from the domain of independent news from the Tlaxcala website. We build monolingual corpora for 15 languages and parallel corpora for all the combinations of those 15 languages. These corpora include languages for which only very limited such resources exist (e.g. Tamazight). We present the acquisition process in detail and we also present detailed statistics of the produced corpora, concerning mainly quantitative dimensions such as the size of the corpora per language (for the monolingual corpora) and per language pair (for the parallel corpora). To the best of our knowledge, these are the first publicly available parallel and monolingual corpora for the domain of independent news. We also create models for unsupervised sentence splitting for all the languages of the study.</abstract>
    </paper>
    <paper id="94">
      <author><first>Sander</first><last>Wubben</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <title>Creating and using large monolingual parallel corpora for sentential paraphrase generation</title>
      <pages>4292–4299</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1135_Paper.pdf</url>
      <abstract>{In this paper we investigate the automatic generation of paraphrases by using machine translation techniques. Three contributions we make are the construction of a large paraphrase corpus for English and Dutch, a re-ranking heuristic to use machine translation for paraphrase generation and a proper evaluation methodology. A large parallel corpus is constructed by aligning clustered headlines that are scraped from a news aggregator site. To generate sentential paraphrases we use a standard phrase-based machine translation (PBMT) framework modified with a re-ranking component (henceforth PBMT-R). We demonstrate this approach for Dutch and English and evaluate by using human judgements collected from 76 participants. The judgments are compared to two automatic machine translation evaluation metrics. We observe that as the paraphrases deviate more from the source sentence, the performance of the PBMT-R system degrades less than that of the word substitution baseline system.</abstract>
    </paper>
    <paper id="95">
      <author><first>Jayendra Rakesh</first><last>Yeka</last></author>
      <author><first>Prasanth</first><last>Kolachina</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <title>Benchmarking of <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi parallel corpora</title>
      <pages>1812–1818</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1137_Paper.pdf</url>
      <abstract>In this paper we present several parallel corpora for EnglishâHindi and talk about their natures and domains. We also discuss briefly a few previous attempts in MT for translation from English to Hindi. The lack of uniformly annotated data makes it difficult to compare these attempts and precisely analyze their strengths and shortcomings. With this in mind, we propose a standard pipeline to provide uniform linguistic annotations to these resources using state-of-art NLP technologies. We conclude the paper by presenting evaluation scores of different statistical MT systems on the corpora detailed in this paper for EnglishâHindi and present the proposed plans for future work. We hope that both these annotated parallel corpora resources and MT systems will serve as benchmarks for future approaches to MT in EnglishâHindi. This was and remains the main motivation for the attempts detailed in this paper.</abstract>
    </paper>
    <paper id="96">
      <author><first>Mark</first><last>Dilsizian</last></author>
      <author><first>Polina</first><last>Yanovich</last></author>
      <author><first>Shu</first><last>Wang</last></author>
      <author><first>Carol</first><last>Neidle</last></author>
      <author><first>Dimitris</first><last>Metaxas</last></author>
      <title>A New Framework for Sign Language Recognition based on 3<fixed-case>D</fixed-case> Handshape Identification and Linguistic Modeling</title>
      <pages>1924–1929</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1138_Paper.pdf</url>
      <abstract>Current approaches to sign recognition by computer generally have at least some of the following limitations: they rely on laboratory conditions for sign production, are limited to a small vocabulary, rely on 2D modeling (and therefore cannot deal with occlusions and off-plane rotations), and/or achieve limited success. Here we propose a new framework that (1) provides a new tracking method less dependent than others on laboratory conditions and able to deal with variations in background and skin regions (such as the face, forearms, or other hands); (2) allows for identification of 3D hand configurations that are linguistically important in American Sign Language (ASL); and (3) incorporates statistical information reflecting linguistic constraints in sign production. For purposes of large-scale computer-based sign language recognition from video, the ability to distinguish hand configurations accurately is critical. Our current method estimates the 3D hand configuration to distinguish among 77 hand configurations linguistically relevant for ASL. Constraining the problem in this way makes recognition of 3D hand configuration more tractable and provides the information specifically needed for sign recognition. Further improvements are obtained by incorporation of statistical information about linguistic dependencies among handshapes within a sign derived from an annotated corpus of almost 10,000 sign tokens.</abstract>
    </paper>
    <paper id="97">
      <author><first>Karteek</first><last>Addanki</last></author>
      <author><first>Dekai</first><last>Wu</last></author>
      <title>Evaluating Improvised Hip Hop Lyrics - Challenges and Observations</title>
      <pages>4616–4623</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1139_Paper.pdf</url>
      <abstract>We investigate novel challenges involved in comparing model performance on the task of improvising responses to hip hop lyrics and discuss observations regarding inter-evaluator agreement on judging improvisation quality. We believe the analysis serves as a first step toward designing robust evaluation strategies for improvisation tasks, a relatively neglected area to date. Unlike most natural language processing tasks, improvisation tasks suffer from a high degree of subjectivity, making it difficult to design discriminative evaluation strategies to drive model development. We propose a simple strategy with fluency and rhyming as the criteria for evaluating the quality of generated responses, which we apply to both our inversion transduction grammar based FREESTYLE hip hop challenge-response improvisation system, as well as various contrastive systems. We report inter-evaluator agreement for both English and French hip hop lyrics, and analyze correlation with challenge length. We also compare the extent of agreement in evaluating fluency with that of rhyming, and quantify the difference in agreement with and without precise definitions of evaluation criteria.</abstract>
    </paper>
    <paper id="98">
      <author><first>Nathan</first><last>Green</last></author>
      <author><first>Septina Dian</first><last>Larasati</last></author>
      <title>Votter Corpus: A Corpus of Social Polling Language</title>
      <pages>3693–3697</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1143_Paper.pdf</url>
      <abstract>The Votter Corpus is a new annotated corpus of social polling questions and answers. The Votter Corpus is novel in its use of the mobile application format and novel in its coverage of specific demographics. With over 26,000 polls and close to 1 millions votes, the Votter Corpus covers everyday question and answer language, primarily for users who are female and between the ages of 13-24. The corpus is annotated by topic and by popularity of particular answers. The corpus contains many unique characteristics such as emoticons, common mobile misspellings, and images associated with many of the questions. The corpus is a collection of questions and answers from The Votter App on the Android operating system. Data is created solely on this mobile platform which differs from most social media corpora. The Votter Corpus is being made available online in XML format for research and non-commercial use. The Votter android app can be downloaded for free in most android app stores.</abstract>
    </paper>
    <paper id="99">
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <title><fixed-case>S</fixed-case>ino<fixed-case>C</fixed-case>oreferencer: An End-to-End <fixed-case>C</fixed-case>hinese Event Coreference Resolver</title>
      <pages>4532–4538</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1144_Paper.pdf</url>
      <abstract>Compared to entity coreference resolution, there is a relatively small amount of work on event coreference resolution. Much work on event coreference was done for English. In fact, to our knowledge, there are no publicly available results on Chinese event coreference resolution. This paper describes the design, implementation, and evaluation of SinoCoreferencer, an end-to-end state-of-the-art ACE-style Chinese event coreference system. We have made SinoCoreferencer publicly available, in hope to facilitate the development of high-level Chinese natural language applications that can potentially benefit from event coreference information.</abstract>
    </paper>
    <paper id="100">
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Michael</first><last>Ciul</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Ramy</first><last>Eskander</last></author>
      <title>Developing an <fixed-case>E</fixed-case>gyptian <fixed-case>A</fixed-case>rabic Treebank: Impact of Dialectal Morphology on Annotation and Tool Development</title>
      <pages>2348–2354</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1145_Paper.pdf</url>
      <abstract>This paper describes the parallel development of an Egyptian Arabic Treebank and a morphological analyzer for Egyptian Arabic (CALIMA). By the very nature of Egyptian Arabic, the data collected is informal, for example Discussion Forum text, which we use for the treebank discussed here. In addition, Egyptian Arabic, like other Arabic dialects, is sufficiently different from Modern Standard Arabic (MSA) that tools and techniques developed for MSA cannot be simply transferred over to work on Egyptian Arabic work. In particular, a morphological analyzer for Egyptian Arabic is needed to mediate between the written text and the segmented, vocalized form used for the syntactic trees. This led to the necessity of a feedback loop between the treebank team and the analyzer team, as improvements in each area were fed to the other. Therefore, by necessity, there needed to be close cooperation between the annotation team and the tool development team, which was to their mutual benefit. Collaboration on this type of challenge, where tools and resources are limited, proved to be remarkably synergistic and opens the way to further fruitful work on Arabic dialects.</abstract>
    </paper>
    <paper id="101">
      <author><first>Tatjana</first><last>Scheffler</last></author>
      <title>A <fixed-case>G</fixed-case>erman <fixed-case>T</fixed-case>witter Snapshot</title>
      <pages>2284–2289</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1146_Paper.pdf</url>
      <abstract>We present a new corpus of German tweets. Due to the relatively small number of German messages on Twitter, it is possible to collect a virtually complete snapshot of German twitter messages over a period of time. In this paper, we present our collection method which produced a 24 million tweet corpus, representing a large majority of all German tweets sent in April, 2013. Further, we analyze this representative data set and characterize the German twitterverse. While German Twitter data is similar to other Twitter data in terms of its temporal distribution, German Twitter users are much more reluctant to share geolocation information with their tweets. Finally, the corpus collection method allows for a study of discourse phenomena in the Twitter data, structured into discussion threads.</abstract>
    </paper>
    <paper id="102">
      <author><first>Helen</first><last>Hastie</last></author>
      <author><first>Anja</first><last>Belz</last></author>
      <title>A Comparative Evaluation Methodology for <fixed-case>NLG</fixed-case> in Interactive Systems</title>
      <pages>4004–4011</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1147_Paper.pdf</url>
      <abstract>Interactive systems have become an increasingly important type of application for deployment of NLG technology over recent years. At present, we do not yet have commonly agreed terminology or methodology for evaluating NLG within interactive systems. In this paper, we take steps towards addressing this gap by presenting a set of principles for designing new evaluations in our comparative evaluation methodology. We start with presenting a categorisation framework, giving an overview of different categories of evaluation measures, in order to provide standard terminology for categorising existing and new evaluation techniques. Background on existing evaluation methodologies for NLG and interactive systems is presented. The comparative evaluation methodology is presented. Finally, a methodology for comparative evaluation of NLG components embedded within interactive systems is presented in terms of the comparative evaluation methodology, using a specific task for illustrative purposes.</abstract>
    </paper>
    <paper id="103">
      <author><first>Kyoko</first><last>Ohara</last></author>
      <title>Relating Frames and Constructions in <fixed-case>J</fixed-case>apanese <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <pages>2474–2477</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1149_Paper.pdf</url>
      <abstract>Relations between frames and constructions must be made explicit in FrameNet-style linguistic resources such as Berkeley FrameNet (Fillmore &amp; Baker, 2010, Fillmore, Lee-Goldman &amp; Rhomieux, 2012), Japanese FrameNet (Ohara, 2013), and Swedish Constructicon (Lyngfelt et al., 2013). On the basis of analyses of Japanese constructions for the purpose of building a constructicon in the Japanese FrameNet project, this paper argues that constructions can be classified based on whether they evoke frames or not. By recognizing such a distinction among constructions, it becomes possible for FrameNet-style linguistic resources to have a proper division of labor between frame annotations and construction annotations. In addition to the three kinds of meaningless constructions which have been proposed already, this paper suggests there may be yet another subtype of constructions without meanings. Furthermore, the present paper adds support to the claim that there may be constructions without meanings (Fillmore, Lee-Goldman &amp; Rhomieux, 2012) in a current debate concerning whether all constructions should be seen as meaning-bearing (Goldberg, 2006: 166-182).</abstract>
    </paper>
    <paper id="104">
      <author><first>Hans-Ulrich</first><last>Krieger</last></author>
      <author><first>Thierry</first><last>Declerck</last></author>
      <title><fixed-case>TMO</fixed-case> — The Federated Ontology of the <fixed-case>T</fixed-case>rend<fixed-case>M</fixed-case>iner Project</title>
      <pages>4164–4171</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/115_Paper.pdf</url>
      <abstract>This paper describes work carried out in the European project TrendMiner which partly deals with the extraction and representation of real time information from dynamic data streams. The focus of this paper lies on the construction of an integrated ontology, TMO, the TrendMiner Ontology, that has been assembled from several independent multilingual taxonomies and ontologies which are brought together by an interface specification, expressed in OWL. Within TrendMiner, TMO serves as a common language that helps to interlink data, delivered from both symbolic and statistical components of the TrendMiner system. Very often, the extracted data is supplied as quintuples, RDF triples that are extended by two further temporal arguments, expressing the temporal extent in which an atemporal statement is true. In this paper, we will also sneak a peek on the temporal entailment rules and queries that are built into the semantic repository hosting the data and which can be used to derive useful new information.</abstract>
    </paper>
    <paper id="105">
      <author><first>Gemma Bel</first><last>Enguix</last></author>
      <author><first>Reinhard</first><last>Rapp</last></author>
      <author><first>Michael</first><last>Zock</last></author>
      <title>A Graph-Based Approach for Computing Free Word Associations</title>
      <pages>3027–3033</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1150_Paper.pdf</url>
      <abstract>A graph-based algorithm is used to analyze the co-occurrences of words in the British National Corpus. It is shown that the statistical regularities detected can be exploited to predict human word associations. The corpus-derived associations are evaluated using a large test set comprising several thousand stimulus/response pairs as collected from humans. The finding is that there is a high agreement between the two types of data. The considerable size of the test set allows us to split the stimulus words into a number of classes relating to particular word properties. For example, we construct six saliency classes, and for the words in each of these classes we compare the simulation results with the human data. It turns out that for each class there is a close relationship between the performance of our system and human performance. This is also the case for classes based on two other properties of words, namely syntactic and semantic word ambiguity. We interpret these findings as evidence for the claim that human association acquisition must be based on the statistical analysis of perceived language and that when producing associations the detected statistical regularities are replicated.</abstract>
    </paper>
    <paper id="106">
      <author><first>Roald</first><last>Eiselen</last></author>
      <author><first>Martin</first><last>Puttkammer</last></author>
      <title>Developing Text Resources for Ten <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>frican Languages</title>
      <pages>3698–3703</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1151_Paper.pdf</url>
      <abstract>The development of linguistic resources for use in natural language processing is of utmost importance for the continued growth of research and development in the field, especially for resource-scarce languages. In this paper we describe the process and challenges of simultaneously developing multiple linguistic resources for ten of the official languages of South Africa. The project focussed on establishing a set of foundational resources that can foster further development of both resources and technologies for the NLP industry in South Africa. The development efforts during the project included creating monolingual unannotated corpora, of which a subset of the corpora for each language was annotated on token, orthographic, morphological and morphosyntactic layers. The annotated subsets includes both development and test sets and were used in the creation of five core-technologies, viz. a tokeniser, sentenciser, lemmatiser, part of speech tagger and morphological decomposer for each language. We report on the quality of these tools for each language and discuss the importance of the resources within the South African context.</abstract>
    </paper>
    <paper id="107">
      <author><first>Paul</first><last>Felt</last></author>
      <author><first>Robbie</first><last>Haertel</last></author>
      <author><first>Eric</first><last>Ringger</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <title><fixed-case>M</fixed-case>omresp: A <fixed-case>B</fixed-case>ayesian Model for Multi-Annotator Document Labeling</title>
      <pages>3704–3711</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1153_Paper.pdf</url>
      <abstract>Data annotation in modern practice often involves multiple, imperfect human annotators. Multiple annotations can be used to infer estimates of the ground-truth labels and to estimate individual annotator error characteristics (or reliability). We introduce MomResp, a model that incorporates information from both natural data clusters as well as annotations from multiple annotators to infer ground-truth labels and annotator reliability for the document classification task. We implement this model and show dramatic improvements over majority vote in situations where both annotations are scarce and annotation quality is low as well as in situations where annotators disagree consistently. Because MomResp predictions are subject to label switching, we introduce a solution that finds nearly optimal predicted class reassignments in a variety of settings using only information available to the model at inference time. Although MomResp does not perform well in annotation-rich situations, we show evidence suggesting how this shortcoming may be overcome in future work.</abstract>
    </paper>
    <paper id="108">
      <author><first>Jessica</first><last>Ouyang</last></author>
      <author><first>Kathy</first><last>McKeown</last></author>
      <title>Towards Automatic Detection of Narrative Structure</title>
      <pages>4624–4631</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1154_Paper.pdf</url>
      <abstract>We present novel computational experiments using William Labov’s theory of narrative analysis. We describe his six elements of narrative structure and construct a new corpus based on his most recent work on narrative. Using this corpus, we explore the correspondence between Labovs elements of narrative structure and the implicit discourse relations of the Penn Discourse Treebank, and we construct a mapping between the elements of narrative structure and the discourse relation classes of the PDTB. We present first experiments on detecting Complicating Actions, the most common of the elements of narrative structure, achieving an f-score of 71.55. We compare the contributions of features derived from narrative analysis, such as the length of clauses and the tenses of main verbs, with those of features drawn from work on detecting implicit discourse relations. Finally, we suggest directions for future research on narrative structure, such as applications in assessing text quality and in narrative generation.</abstract>
    </paper>
    <paper id="109">
      <author><first>Anabela</first><last>Barreiro</last></author>
      <author><first>Fernando</first><last>Batista</last></author>
      <author><first>Ricardo</first><last>Ribeiro</last></author>
      <author><first>Helena</first><last>Moniz</last></author>
      <author><first>Isabel</first><last>Trancoso</last></author>
      <title><fixed-case>O</fixed-case>pen<fixed-case>L</fixed-case>ogos Semantico-Syntactic Knowledge-Rich Bilingual Dictionaries</title>
      <pages>3774–3781</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1155_Paper.pdf</url>
      <abstract>This paper presents 3 sets of OpenLogos resources, namely the English-German, the English-French, and the English-Italian bilingual dictionaries. In addition to the usual information on part-of-speech, gender, and number for nouns, offered by most dictionaries currently available, OpenLogos bilingual dictionaries have some distinctive features that make them unique: they contain cross-language morphological information (inflectional and derivational), semantico-syntactic knowledge, indication of the head word in multiword units, information about whether a source word corresponds to an homograph, information about verb auxiliaries, alternate words (i.e., predicate or process nouns), causatives, reflexivity, verb aspect, among others. The focal point of the paper will be the semantico-syntactic knowledge that is important for disambiguation and translation precision. The resources are publicly available at the METANET platform for free use by the research community.</abstract>
    </paper>
    <paper id="110">
      <author><first>Tilia</first><last>Ellendorff</last></author>
      <author><first>Fabio</first><last>Rinaldi</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <title>Using Large Biomedical Databases as Gold Annotations for Automatic Relation Extraction</title>
      <pages>3736–3741</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1156_Paper.pdf</url>
      <abstract>We show how to use large biomedical databases in order to obtain a gold standard for training a machine learning system over a corpus of biomedical text. As an example we use the Comparative Toxicogenomics Database (CTD) and describe by means of a short case study how the obtained data can be applied. We explain how we exploit the structure of the database for compiling training material and a testset. Using a Naive Bayes document classification approach based on words, stem bigrams and MeSH descriptors we achieve a macro-average F-score of 61% on a subset of 8 action terms. This outperforms a baseline system based on a lookup of stemmed keywords by more than 20%. Furthermore, we present directions of future work, taking the described system as a vantage point. Future work will be aiming towards a weakly supervised system capable of discovering complete biomedical interactions and events.</abstract>
    </paper>
    <paper id="111">
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <title>Crowdsourcing for the identification of event nominals: an experiment</title>
      <pages>1949–1955</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1157_Paper.pdf</url>
      <abstract>This paper presents the design and results of a crowdsourcing experiment on the recognition of Italian event nominals. The aim of the experiment was to assess the feasibility of crowdsourcing methods for a complex semantic task such as distinguishing the eventive interpretation of polysemous nominals taking into consideration various types of syntagmatic cues. Details on the theoretical background and on the experiment set up are provided together with the final results in terms of accuracy and inter-annotator agreement. These results are compared with the ones obtained by expert annotators on the same task. The low values in accuracy and Fleiss kappa of the crowdsourcing experiment demonstrate that crowdsourcing is not always optimal for complex linguistic tasks. On the other hand, the use of non-expert contributors allows to understand what are the most ambiguous patterns of polysemy and the most useful syntagmatic cues to be used to identify the eventive reading of nominals.</abstract>
    </paper>
    <paper id="112">
      <author><first>Jianqiang</first><last>Ma</last></author>
      <title>Automatic Refinement of Syntactic Categories in <fixed-case>C</fixed-case>hinese Word Structures</title>
      <pages>4087–4092</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1158_Paper.pdf</url>
      <abstract>Annotated word structures are useful for various Chinese NLP tasks, such as word segmentation, POS tagging and syntactic parsing. Chinese word structures are often represented by binary trees, the nodes of which are labeled with syntactic categories, due to the syntactic nature of Chinese word formation. It is desirable to refine the annotation by labeling nodes of word structure trees with more proper syntactic categories so that the combinatorial properties in the word formation process are better captured. This can lead to improved performances on the tasks that exploit word structure annotations. We propose syntactically inspired algorithms to automatically induce syntactic categories of word structure trees using POS tagged corpus and branching in existing Chinese word structure trees. We evaluate the quality of our annotation by comparing the performances of models based on our annotation and another publicly available annotation, respectively. The results on two variations of Chinese word segmentation task show that using our annotation can lead to significant performance improvements.</abstract>
    </paper>
    <paper id="113">
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Justin</first><last>Mott</last></author>
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Jennifer</first><last>Garland</last></author>
      <author><first>Colin</first><last>Warner</last></author>
      <title>Incorporating Alternate Translations into <fixed-case>E</fixed-case>nglish Translation Treebank</title>
      <pages>1863–1868</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1159_Paper.pdf</url>
      <abstract>New annotation guidelines and new processing methods were developed to accommodate English treebank annotation of a parallel English/Chinese corpus of web data that includes alternate English translations (one fluent, one literal) of expressions that are idiomatic in the Chinese source. In previous machine translation programs, alternate translations of idiomatic expressions had been present in untreebanked data only, but due to the high frequency of such expressions in informal genres such as discussion forums, machine translation system developers requested that alternatives be added to the treebanked data as well. In consultation with machine translation researchers, we chose a pragmatic approach of syntactically annotating only the fluent translation, while retaining the alternate literal translation as a segregated node in the tree. Since the literal translation alternates are often incompatible with English syntax, this approach allows us to create fluent trees without losing information. This resource is expected to support machine translation efforts, and the flexibility provided by the alternate translations is an enhancement to the treebank for this purpose.</abstract>
    </paper>
    <paper id="114">
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Beat</first><last>Kunz</last></author>
      <title><fixed-case>Z</fixed-case>morge: A <fixed-case>G</fixed-case>erman Morphological Lexicon Extracted from <fixed-case>W</fixed-case>iktionary</title>
      <pages>1063–1067</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/116_Paper.pdf</url>
      <abstract>We describe a method to automatically extract a German lexicon from Wiktionary that is compatible with the finite-state morphological grammar SMOR. The main advantage of the resulting lexicon over existing lexica for SMOR is that it is open and permissively licensed. A recall-oriented evaluation shows that a morphological analyser built with our lexicon has comparable coverage compared to existing lexica, and continues to improve as Wiktionary grows. We also describe modifications to the SMOR grammar that result in a more conventional lemmatisation of words.</abstract>
    </paper>
    <paper id="115">
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>Mohamed</first><last>Al-Badrashiny</last></author>
      <author><first>Maryam</first><last>Aminian</last></author>
      <author><first>Mohammed</first><last>Attia</last></author>
      <author><first>Heba</first><last>Elfardy</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Abdelati</first><last>Hawwari</last></author>
      <author><first>Wael</first><last>Salloum</last></author>
      <author><first>Pradeep</first><last>Dasigi</last></author>
      <author><first>Ramy</first><last>Eskander</last></author>
      <title><fixed-case>T</fixed-case>harwa: A Large Scale Dialectal <fixed-case>A</fixed-case>rabic - <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic - <fixed-case>E</fixed-case>nglish Lexicon</title>
      <pages>3782–3789</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1161_Paper.pdf</url>
      <abstract>We introduce an electronic three-way lexicon, Tharwa, comprising Dialectal Arabic, Modern Standard Arabic and English correspondents. The paper focuses on Egyptian Arabic as the first pilot dialect for the resource, with plans to expand to other dialects of Arabic in later phases of the project. We describe Tharwas creation process and report on its current status. The lexical entries are augmented with various elements of linguistic information such as POS, gender, rationality, number, and root and pattern information. The lexicon is based on a compilation of information from both monolingual and bilingual existing resources such as paper dictionaries and electronic, corpus-based dictionaries. Multiple levels of quality checks are performed on the output of each step in the creation process. The importance of this lexicon lies in the fact that it is the first resource of its kind bridging multiple variants of Arabic with English. Furthermore, it is a wide coverage lexical resource containing over 73,000 Egyptian entries. Tharwa is publicly available. We believe it will have a significant impact on both Theoretical Linguistics as well as Computational Linguistics research.</abstract>
    </paper>
    <paper id="116">
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Karlheinz</first><last>Mörth</last></author>
      <author><first>Eveline</first><last>Wandl-Vogt</last></author>
      <title>A <fixed-case>SKOS</fixed-case>-based Schema for <fixed-case>TEI</fixed-case> encoded Dictionaries at <fixed-case>ICLTT</fixed-case></title>
      <pages>414–417</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1163_Paper.pdf</url>
      <abstract>At our institutes we are working with quite some dictionaries and lexical resources in the field of less-resourced language data, like dialects and historical languages. We are aiming at publishing those lexical data in the Linked Open Data framework in order to link them with available data sets for highly-resourced languages and elevating them thus to the same digital dignity the mainstream languages have gained. In this paper we concentrate on two TEI encoded variants of the Arabic language and propose a mapping of this TEI encoded data onto SKOS, showing how the lexical entries of the two dialectal dictionaries can be linked to other language resources available in the Linked Open Data cloud.</abstract>
    </paper>
    <paper id="117">
      <author><first>Milen</first><last>Kouylekov</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <title>Semantic Technologies for Querying Linguistic Annotations: An Experiment Focusing on Graph-Structured Data</title>
      <pages>4331–4336</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1166_Paper.pdf</url>
      <abstract>With growing interest in the creation and search of linguistic annotations that form general graphs (in contrast to formally simpler, rooted trees), there also is an increased need for infrastructures that support the exploration of such representations, for example logical-form meaning representations or semantic dependency graphs. In this work, we heavily lean on semantic technologies and in particular the data model of the Resource Description Framework (RDF) to represent, store, and efficiently query very large collections of text annotated with graph-structured representations of sentence meaning.</abstract>
    </paper>
    <paper id="118">
      <author><first>Heather</first><last>Pon-Barry</last></author>
      <author><first>Stuart</first><last>Shieber</last></author>
      <author><first>Nicholas</first><last>Longenbaugh</last></author>
      <title>Eliciting and Annotating Uncertainty in Spoken Language</title>
      <pages>1978–1983</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1167_Paper.pdf</url>
      <abstract>A major challenge in the field of automatic recognition of emotion and affect in speech is the subjective nature of affect labels. The most common approach to acquiring affect labels is to ask a panel of listeners to rate a corpus of spoken utterances along one or more dimensions of interest. For applications ranging from educational technology to voice search to dictation, a speaker’s level of certainty is a primary dimension of interest. In such applications, we would like to know the speaker’s actual level of certainty, but past research has only revealed listeners’ perception of the speaker’s level of certainty. In this paper, we present a method for eliciting spoken utterances using stimuli that we design such that they have a quantitative, crowdsourced legibility score. While we cannot control a speaker’s actual internal level of certainty, the use of these stimuli provides a better estimate of internal certainty compared to existing speech corpora. The Harvard Uncertainty Speech Corpus, containing speech data, certainty annotations, and prosodic features, is made available to the research community.</abstract>
    </paper>
    <paper id="119">
      <author><first>Martin</first><last>Gleize</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <title>A hierarchical taxonomy for classifying hardness of inference tasks</title>
      <pages>3034–3040</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1168_Paper.pdf</url>
      <abstract>Exhibiting inferential capabilities is one of the major goals of many modern Natural Language Processing systems. However, if attempts have been made to define what textual inferences are, few seek to classify inference phenomena by difficulty. In this paper we propose a hierarchical taxonomy for inferences, relatively to their hardness, and with corpus annotation and system design and evaluation in mind. Indeed, a fine-grained assessment of the difficulty of a task allows us to design more appropriate systems and to evaluate them only on what they are designed to handle. Each of seven classes is described and provided with examples from different tasks like question answering, textual entailment and coreference resolution. We then test the classes of our hierarchy on the specific task of question answering. Our annotation process of the testing data at the QA4MRE 2013 evaluation campaign reveals that it is possible to quantify the contrasts in types of difficulty on datasets of the same task.</abstract>
    </paper>
    <paper id="120">
      <author><first>Michael</first><last>Rosner</last></author>
      <author><first>Kurt</first><last>Sultana</last></author>
      <title>Automatic Methods for the Extension of a Bilingual Dictionary using Comparable Corpora</title>
      <pages>3790–3797</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1169_Paper.pdf</url>
      <abstract>Bilingual dictionaries define word equivalents from one language to another, thus acting as an important bridge between languages. No bilingual dictionary is complete since languages are in a constant state of change. Additionally, dictionaries are unlikely to achieve complete coverage of all language terms. This paper investigates methods for extending dictionaries using non-aligned corpora, by finding translations through context similarity. Most methods compute word contexts from general corpora. This can lead to errors due to data sparsity. We investigate the hypothesis that this problem can be addressed by carefully choosing smaller corpora in which domain-specific terms are more predominant. We also introduce the notion of efficiency which we consider as the effort required to obtain a set of dictionary entries from a given corpus</abstract>
    </paper>
    <paper id="121">
      <author><first>Yutaka</first><last>Mitsuishi</last></author>
      <author><first>Vít</first><last>Nováček</last></author>
      <author><first>Pierre-Yves</first><last>Vandenbussche</last></author>
      <title>A Method for Building Burst-Annotated Co-Occurrence Networks for Analysing Trends in Textual Data</title>
      <pages>3742–3747</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1170_Paper.pdf</url>
      <abstract>This paper presents a method for constructing a specific type of language resources that are conveniently applicable to analysis of trending topics in time-annotated textual data. More specifically, the method consists of building a co-occurrence network from the on-line content (such as New York Times articles) that conform to key words selected by users (e.g., ‘Arab Spring’). Within the network, burstiness of the particular nodes (key words) and edges (co-occurrence relations) is computed. A service deployed on the network then facilitates exploration of the underlying text in order to identify trending topics. Using the graph structure of the network, one can assess also a broader context of the trending events. To limit the information overload of users, we filter the edges and nodes displayed by their burstiness scores to show only the presumably more important ones. The paper gives details on the proposed method, including a step-by-step walk through with plenty of real data examples. We report on a specific application of our method to the topic of `Arab Spring’ and make the language resource applied therein publicly available for experimentation. Last but not least, we outline a methodology of an ongoing evaluation of our method.</abstract>
    </paper>
    <paper id="122">
      <author><first>José Pedro</first><last>Ferreira</last></author>
      <author><first>Cristiano</first><last>Chesi</last></author>
      <author><first>Daan</first><last>Baldewijns</last></author>
      <author><first>Fernando Miguel</first><last>Pinto</last></author>
      <author><first>Margarita</first><last>Correia</last></author>
      <author><first>Daniela</first><last>Braga</last></author>
      <author><first>Hyongsil</first><last>Cho</last></author>
      <author><first>Amadeu</first><last>Ferreira</last></author>
      <author><first>Miguel</first><last>Dias</last></author>
      <title>Casa de la Lhéngua: a set of language resources and natural language processing tools for <fixed-case>M</fixed-case>irandese</title>
      <pages>536–540</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1174_Paper.pdf</url>
      <abstract>This paper describes the efforts for the construction of Language Resources and NLP tools for Mirandese, a minority language spoken in North-eastern Portugal, now available on a community-led portal, Casa de la Lhéngua. The resources were developed in the context of a collaborative citizenship project led by Microsoft, in the context of the creation of the first TTS system for Mirandese. Development efforts encompassed the compilation of a corpus with over 1M tokens, the construction of a GTP system, syllable-division, inflection and a Part-of-Speech (POS) tagger modules, leading to the creation of an inflected lexicon of about 200.000 entries with phonetic transcription, detailed POS tagging, syllable division, and stress mark-up. Alongside these tasks, which were made easier through the adaptation and reuse of existing tools for closely related languages, a casting for voice talents among the speaking community was conducted and the first speech database for speech synthesis was recorded for Mirandese. These resources were combined to fulfil the requirements of a well-tested statistical parameter synthesis model, leading to an intelligible voice font. These language resources are available freely at Casa de la Lhéngua, aiming at promoting the development of real-life applications and fostering linguistic research on Mirandese.</abstract>
    </paper>
    <paper id="123">
      <author><first>Jan</first><last>Strunk</last></author>
      <author><first>Florian</first><last>Schiel</last></author>
      <author><first>Frank</first><last>Seifart</last></author>
      <title>Untrained Forced Alignment of Transcriptions and Audio for Language Documentation Corpora using <fixed-case>W</fixed-case>eb<fixed-case>MAUS</fixed-case></title>
      <pages>3940–3947</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1176_Paper.pdf</url>
      <abstract>Language documentation projects supported by recent funding intiatives have created a large number of multimedia corpora of typologically diverse languages. Most of these corpora provide a manual alignment of transcription and audio data at the level of larger units, such as sentences or intonation units. Their usefulness both for corpus-linguistic and psycholinguistic research and for the development of tools and teaching materials could, however, be increased by achieving a more fine-grained alignment of transcription and audio at the word or even phoneme level. Since most language documentation corpora contain data on small languages, there usually do not exist any speech recognizers or acoustic models specifically trained on these languages. We therefore investigate the feasibility of untrained forced alignment for such corpora. We report on an evaluation of the tool (Web)MAUS (Kisler, 2012) on several language documentation corpora and discuss practical issues in the application of forced alignment. Our evaluation shows that (Web)MAUS with its existing acoustic models combined with simple grapheme-to-phoneme conversion can be successfully used for word-level forced alignment of a diverse set of languages without additional training, especially if a manual prealignment of larger annotation units is already avaible.</abstract>
    </paper>
    <paper id="124">
      <author><first>Lars</first><last>Hellan</last></author>
      <author><first>Dorothee</first><last>Beermann</last></author>
      <author><first>Tore</first><last>Bruland</last></author>
      <author><first>Mary Esther Kropp</first><last>Dakubu</last></author>
      <author><first>Montserrat</first><last>Marimon</last></author>
      <title><fixed-case>M</fixed-case>ulti<fixed-case>V</fixed-case>al - towards a multilingual valence lexicon</title>
      <pages>2478–2485</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1179_Paper.pdf</url>
      <abstract>MultiVal is a valence lexicon derived from lexicons of computational HPSG grammars for Norwegian, Spanish and Ga (ISO 639-3, gaa), with altogether about 22,000 verb entries and on average more than 200 valence types defined for each language. These lexical resources are mapped onto a common set of discriminants with a common array of values, and stored in a relational database linked to a web demo and a wiki presentation. Search discriminants are syntactic argument structure (SAS), functional specification, situation type and aspect, for any subset of languages, as well as the verb type systems of the grammars. Search results are lexical entries satisfying the discriminants entered, exposing the specifications from the respective provenance grammars. The Ga grammar lexicon has in turn been converted from a Ga Toolbox lexicon. Aside from the creation of such a multilingual valence resource through converging or converting existing resources, the paper also addresses a tool for the creation of such a resource as part of corpus annotation for less resourced languages.</abstract>
    </paper>
    <paper id="125">
      <author><first>Michel</first><last>Vacher</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Pedro</first><last>Chahuara</last></author>
      <author><first>François</first><last>Portet</last></author>
      <author><first>Brigitte</first><last>Meillon</last></author>
      <author><first>Nicolas</first><last>Bonnefond</last></author>
      <title>The Sweet-Home speech and multimodal corpus for home automation interaction</title>
      <pages>4499–4506</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/118_Paper.pdf</url>
      <abstract>Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes and Home Automation. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The S WEET-H OME multimodal corpus is a dataset recorded in realistic conditions in D OMUS, a fully equipped Smart Home with microphones and home automation sensors, in which participants performed Activities of Daily living (ADL). This corpus is made of a multimodal subset, a French home automation speech subset recorded in Distant Speech conditions, and two interaction subsets, the first one being recorded by 16 persons without disabilities and the second one by 6 seniors and 5 visually impaired people. This corpus was used in studies related to ADL recognition, context aware interaction and distant speech recognition applied to home automation controled through voice.</abstract>
    </paper>
    <paper id="126">
      <author><first>David</first><last>Lewis</last></author>
      <author><first>Rob</first><last>Brennan</last></author>
      <author><first>Leroy</first><last>Finn</last></author>
      <author><first>Dominic</first><last>Jones</last></author>
      <author><first>Alan</first><last>Meehan</last></author>
      <author><first>Declan</first><last>O’Sullivan</last></author>
      <author><first>Sebastian</first><last>Hellmann</last></author>
      <author><first>Felix</first><last>Sasaki</last></author>
      <title>Global Intelligent Content: Active Curation of Language Resources using Linked Data</title>
      <pages>3546–3549</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1182_Paper.pdf</url>
      <abstract>As language resources start to become available in linked data formats, it becomes relevant to consider how linked data interoperability can play a role in active language processing workflows as well as for more static language resource publishing. This paper proposes that linked data may have a valuable role to play in tracking the use and generation of language resources in such workflows in order to assess and improve the performance of the language technologies that use the resources, based on feedback from the human involvement typically required within such processes. We refer to this as Active Curation of the language resources, since it is performed systematically over language processing workflows to continuously improve the quality of the resource in specific applications, rather than via dedicated curation steps. We use modern localisation workflows, i.e. assisted by machine translation and text analytics services, to explain how linked data can support such active curation. By referencing how a suitable linked data vocabulary can be assembled by combining existing linked data vocabularies and meta-data from other multilingual content processing annotations and tool exchange standards we aim to demonstrate the relative ease with which active curation can be deployed more broadly.</abstract>
    </paper>
    <paper id="127">
      <author><first>Liviu</first><last>Dinu</last></author>
      <author><first>Alina Maria</first><last>Ciobanu</last></author>
      <title>On the <fixed-case>R</fixed-case>omance Languages Mutual Intelligibility</title>
      <pages>3313–3318</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1183_Paper.pdf</url>
      <abstract>We propose a method for computing the similarity of natural languages and for clustering them based on their lexical similarity. Our study provides evidence to be used in the investigation of the written intelligibility, i.e., the ability of people writing in different languages to understand one another without prior knowledge of foreign languages. We account for etymons and cognates, we quantify lexical similarity and we extend our analysis from words to languages. Based on the introduced methodology, we compute a matrix of Romance languages intelligibility.</abstract>
    </paper>
    <paper id="128">
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Liviu</first><last>Dinu</last></author>
      <author><first>Ionut</first><last>Sorodoc</last></author>
      <title>Aggregation methods for efficient collocation detection</title>
      <pages>4041–4045</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1184_Paper.pdf</url>
      <abstract>In this article we propose a rank aggregation method for the task of collocations detection. It consists of applying some well-known methods (e.g. Dice method, chi-square test, z-test and likelihood ratio) and then aggregating the resulting collocations rankings by rank distance and Borda score. These two aggregation methods are especially well suited for the task, since the results of each individual method naturally forms a ranking of collocations. Combination methods are known to usually improve the results, and indeed, the proposed aggregation method performs better then each individual method taken in isolation.</abstract>
    </paper>
    <paper id="129">
      <author><first>Jennifer</first><last>D’Souza</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <title>Annotating Inter-Sentence Temporal Relations in Clinical Notes</title>
      <pages>2758–2765</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1185_Paper.pdf</url>
      <abstract>Owing in part to the surge of interest in temporal relation extraction, a number of datasets manually annotated with temporal relations between event-event pairs and event-time pairs have been produced recently. However, it is not uncommon to find missing annotations in these manually annotated datasets. Many researchers attributed this problem to “annotator fatigue”. While some of these missing relations can be recovered automatically, many of them cannot. Our goals in this paper are to (1) manually annotate certain types of missing links that cannot be automatically recovered in the i2b2 Clinical Temporal Relations Challenge Corpus, one of the recently released evaluation corpora for temporal relation extraction; and (2) empirically determine the usefulness of these additional annotations. We will make our annotations publicly available, in hopes of enabling a more accurate evaluation of temporal relation extraction systems.</abstract>
    </paper>
    <paper id="130">
      <author><first>Juris</first><last>Borzovs</last></author>
      <author><first>Ilze</first><last>Ilziņa</last></author>
      <author><first>Iveta</first><last>Keiša</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <title>Terminology localization guidelines for the national scenario</title>
      <pages>4012–4017</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1189_Paper.pdf</url>
      <abstract>This paper presents a set of principles and practical guidelines for terminology work in the national scenario to ensure a harmonized approach in term localization. These linguistic principles and guidelines are elaborated by the Terminology Commission in Latvia in the domain of Information and Communication Technology (ICT). We also present a novel approach in a corpus-based selection and an evaluation of the most frequently used terms. Analysis of the terms proves that, in general, in the normative terminology work in Latvia localized terms are coined according to these guidelines. We further evaluate how terms included in the database of official terminology are adopted in the general use such as newspaper articles, blogs, forums, websites etc. Our evaluation shows that in a non-normative context the official terminology faces a strong competition from other variations of localized terms. Conclusions and recommendations from lexical analysis of localized terms are provided. We hope that presented guidelines and approach in evaluation will be useful to terminology institutions, regulative authorities and researchers in different countries that are involved in the national terminology work.</abstract>
    </paper>
    <paper id="131">
      <author><first>Claire</first><last>Brierley</last></author>
      <author><first>Majdi</first><last>Sawalha</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <title>Tools for <fixed-case>A</fixed-case>rabic Natural Language Processing: a case study in qalqalah prosody</title>
      <pages>283–287</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/119_Paper.pdf</url>
      <abstract>In this paper, we focus on the prosodic effect of qalqalah or “vibration” applied to a subset of Arabic consonants under certain constraints during correct Qur’anic recitation or taÇ§wīd, using our Boundary-Annotated Quran dataset of 77430 words (Brierley et al 2012; Sawalha et al 2014). These qalqalah events are rule-governed and are signified orthographically in the Arabic script. Hence they can be given abstract definition in the form of regular expressions and thus located and collected automatically. High frequency qalqalah content words are also found to be statistically significant discriminators or keywords when comparing Meccan and Medinan chapters in the Qur’an using a state-of-the-art Visual Analytics toolkit: Semantic Pathways. Thus we hypothesise that qalqalah prosody is one way of highlighting salient items in the text. Finally, we implement Arabic transcription technology (Brierley et al under review; Sawalha et al forthcoming) to create a qalqalah pronunciation guide where each word is transcribed phonetically in IPA and mapped to its chapter-verse ID. This is funded research under the EPSRC “Working Together” theme.</abstract>
    </paper>
    <paper id="132">
      <author><first>Ana Isabel</first><last>Mata</last></author>
      <author><first>Helena</first><last>Moniz</last></author>
      <author><first>Fernando</first><last>Batista</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <title>Teenage and adult speech in school context: building and processing a corpus of <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese</title>
      <pages>3914–3919</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1193_Paper.pdf</url>
      <abstract>We present a corpus of European Portuguese spoken by teenagers and adults in school context, CPE-FACES, with an overview of the differential characteristics of high school oral presentations and the challenges this data poses to automatic speech processing. The CPE-FACES corpus has been created with two main goals: to provide a resource for the study of prosodic patterns in both spontaneous and prepared unscripted speech, and to capture inter-speaker and speaking style variations common at school, for research on oral presentations. Research on speaking styles is still largely based on adult speech. References to teenagers are sparse and cross-analyses of speech types comparing teenagers and adults are rare. We expect CPE-FACES, currently a unique resource in this domain, will contribute to filling this gap in European Portuguese. Focusing on disfluencies and phrase-final phonetic-phonological processes we show the impact of teenage speech on the automatic segmentation of oral presentations. Analyzing fluent final intonation contours in declarative utterances, we also show that communicative situation specificities, speaker status and cross-gender differences are key factors in speaking style variation at school.</abstract>
    </paper>
    <paper id="133">
      <author><first>Archna</first><last>Bhatia</last></author>
      <author><first>Mandy</first><last>Simons</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Jordan</first><last>Bender</last></author>
      <title>A Unified Annotation Scheme for the Semantic/Pragmatic Components of Definiteness</title>
      <pages>910–916</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1194_Paper.pdf</url>
      <abstract>We present a definiteness annotation scheme that captures the semantic, pragmatic, and discourse information, which we call communicative functions, associated with linguistic descriptions such as “a story about my speech”, “the story”, “every time I give it”, “this slideshow”. A survey of the literature suggests that definiteness does not express a single communicative function but is a grammaticalization of many such functions, for example, identifiability, familiarity, uniqueness, specificity. Our annotation scheme unifies ideas from previous research on definiteness while attempting to remove redundancy and make it easily annotatable. This annotation scheme encodes the communicative functions of definiteness rather than the grammatical forms of definiteness. We assume that the communicative functions are largely maintained across languages while the grammaticalization of this information may vary. One of the final goals is to use our semantically annotated corpora to discover how definiteness is grammaticalized in different languages. We release our annotated corpora for English and Hindi, and sample annotations for Hebrew and Russian, together with an annotation manual.</abstract>
    </paper>
    <paper id="134">
      <author><first>Michaela</first><last>Regneri</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <title>Aligning Predicate-Argument Structures for Paraphrase Fragment Extraction</title>
      <pages>4300–4307</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1195_Paper.pdf</url>
      <abstract>Paraphrases and paraphrasing algorithms have been found of great importance in various natural language processing tasks. While most paraphrase extraction approaches extract equivalent sentences, sentences are an inconvenient unit for further processing, because they are too specific, and often not exact paraphrases. Paraphrase fragment extraction is a technique that post-processes sentential paraphrases and prunes them to more convenient phrase-level units. We present a new approach that uses semantic roles to extract paraphrase fragments from sentence pairs that share semantic content to varying degrees, including full paraphrases. In contrast to previous systems, the use of semantic parses allows for extracting paraphrases with high wording variance and different syntactic categories. The approach is tested on four different input corpora and compared to two previous systems for extracting paraphrase fragments. Our system finds three times as many good paraphrase fragments per sentence pair as the baselines, and at the same time outputs 30% fewer unrelated fragment pairs.</abstract>
    </paper>
    <paper id="135">
      <author><first>Sandra</first><last>Antunes</last></author>
      <author><first>Amália</first><last>Mendes</last></author>
      <title>An evaluation of the role of statistical measures and frequency for <fixed-case>MWE</fixed-case> identification</title>
      <pages>4046–4051</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1197_Paper.pdf</url>
      <abstract>We report on an experiment to evaluate the role of statistical association measures and frequency for the identification of MWE. We base our evaluation on a lexicon of 14.000 MWE comprising different types of word combinations: collocations, nominal compounds, light verbs + predicate, idioms, etc. These MWE were manually validated from a list of n-grams extracted from a 50 million word corpus of Portuguese (a subcorpus of the Reference Corpus of Contemporary Portuguese), using several criteria: syntactic fixedness, idiomaticity, frequency and Mutual Information measure, although no threshold was established, either in terms of group frequency or MI. We report on MWE that were selected on the basis of their syntactic and semantics properties while the MI or both the MI and the frequency show low values, which would constitute difficult cases to establish a cutting point. We analyze the MI values of the MWE selected in our gold dataset and, for some specific cases, compare these values with two other statistical measures.</abstract>
    </paper>
    <paper id="136">
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Dekai</first><last>Wu</last></author>
      <title>On the reliability and inter-annotator agreement of human semantic <fixed-case>MT</fixed-case> evaluation via <fixed-case>HMEANT</fixed-case></title>
      <pages>602–607</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1198_Paper.pdf</url>
      <abstract>We present analyses showing that HMEANT is a reliable, accurate and fine-grained semantic frame based human MT evaluation metric with high inter-annotator agreement (IAA) and correlation with human adequacy judgments, despite only requiring a minimal training of about 15 minutes for lay annotators. Previous work shows that the IAA on the semantic role labeling (SRL) subtask within HMEANT is over 70%. In this paper we focus on (1) the IAA on the semantic role alignment task and (2) the overall IAA of HMEANT. Our results show that the IAA on the alignment task of HMEANT is over 90% when humans align SRL output from the same SRL annotator, which shows that the instructions on the alignment task are sufficiently precise, although the overall IAA where humans align SRL output from different SRL annotators falls to only 61% due to the pipeline effect on the disagreement in the two annotation task. We show that instead of manually aligning the semantic roles using an automatic algorithm not only helps maintaining the overall IAA of HMEANT at 70%, but also provides a finer-grained assessment on the phrasal similarity of the semantic role fillers. This suggests that HMEANT equipped with automatic alignment is reliable and accurate for humans to evaluate MT adequacy while achieving higher correlation with human adequacy judgments than HTER.</abstract>
    </paper>
    <paper id="137">
      <author><first>Shikun</first><last>Zhang</last></author>
      <author><first>Wang</first><last>Ling</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <title>Dual Subtitles as Parallel Corpora</title>
      <pages>1869–1874</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1199_Paper.pdf</url>
      <abstract>In this paper, we leverage the existence of dual subtitles as a source of parallel data. Dual subtitles present viewers with two languages simultaneously, and are generally aligned in the segment level, which removes the need to automatically perform this alignment. This is desirable as extracted parallel data does not contain alignment errors present in previous work that aligns different subtitle files for the same movie. We present a simple heuristic to detect and extract dual subtitles and show that more than 20 million sentence pairs can be extracted for the Mandarin-English language pair. We also show that extracting data from this source can be a viable solution for improving Machine Translation systems in the domain of subtitles.</abstract>
    </paper>
    <paper id="138">
      <author><first>Peter</first><last>Exner</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <title><fixed-case>REFRACTIVE</fixed-case>: An Open Source Tool to Extract Knowledge from Syntactic and Semantic Relations</title>
      <pages>2584–2589</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/12_Paper.pdf</url>
      <abstract>The extraction of semantic propositions has proven instrumental in applications like IBM Watson and in Google’s knowledge graph . One of the core components of IBM Watson is the PRISMATIC knowledge base consisting of one billion propositions extracted from the English version of Wikipedia and the New York Times. However, extracting the propositions from the English version of Wikipedia is a time-consuming process. In practice, this task requires multiple machines and a computation distribution involving a good deal of system technicalities. In this paper, we describe Refractive, an open-source tool to extract propositions from a parsed corpus based on the Hadoop variant of MapReduce. While the complete process consists of a parsing part and an extraction part, we focus here on the extraction from the parsed corpus and we hope this tool will help computational linguists speed up the development of applications.</abstract>
    </paper>
    <paper id="139">
      <author><first>Guiyao</first><last>Ke</last></author>
      <author><first>Pierre-Francois</first><last>Marteau</last></author>
      <author><first>Gildas</first><last>Menier</last></author>
      <title>Variations on quantitative comparability measures and their evaluations on synthetic <fixed-case>F</fixed-case>rench-<fixed-case>E</fixed-case>nglish comparable corpora</title>
      <pages>133–139</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/120_Paper.pdf</url>
      <abstract>Following the pioneering work by \cite{Li-Gaussier-10}, we address in this paper the analysis of a family of quantitative comparability measures dedicated to the construction and evaluation of topical comparable corpora. After recalling the definition of the quantitative comparability measure proposed by \cite{Li-Gaussier-10}, we develop some variants of this measure based primarily on the consideration that the occurrence frequencies of lexical entries and the number of their translations are important. We compare the respective advantages and disadvantages of these variants in the context of an evaluation framework that is based on the progressive degradation of the Europarl parallel corpus. The degradation is obtained by replacing either deterministically or randomly a varying amount of lines in blocks that compose partitions of the initial Europarl corpus. The impact of the coverage of bilingual dictionaries on these measures is also discussed and perspectives are finally presented.</abstract>
    </paper>
    <paper id="140">
      <author><first>Liviu</first><last>Dinu</last></author>
      <author><first>Alina Maria</first><last>Ciobanu</last></author>
      <author><first>Ioana</first><last>Chitoran</last></author>
      <author><first>Vlad</first><last>Niculae</last></author>
      <title>Using a machine learning model to assess the complexity of stress systems</title>
      <pages>331–336</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1200_Paper.pdf</url>
      <abstract>We address the task of stress prediction as a sequence tagging problem. We present sequential models with averaged perceptron training for learning primary stress in Romanian words. We use character n-grams and syllable n-grams as features and we account for the consonant-vowel structure of the words. We show in this paper that Romanian stress is predictable, though not deterministic, by using data-driven machine learning techniques.</abstract>
    </paper>
    <paper id="141">
      <author><first>Ana Isabel</first><last>Mata</last></author>
      <author><first>Helena</first><last>Moniz</last></author>
      <author><first>Telmo</first><last>Móia</last></author>
      <author><first>Anabela</first><last>Gonçalves</last></author>
      <author><first>Fátima</first><last>Silva</last></author>
      <author><first>Fernando</first><last>Batista</last></author>
      <author><first>Inês</first><last>Duarte</last></author>
      <author><first>Fátima</first><last>Oliveira</last></author>
      <author><first>Isabel</first><last>Falé</last></author>
      <title>Prosodic, syntactic, semantic guidelines for topic structures across domains and corpora</title>
      <pages>1188–1193</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1201_Paper.pdf</url>
      <abstract>This paper presents the annotation guidelines applied to naturally occurring speech, aiming at an integrated account of contrast and parallel structures in European Portuguese. These guidelines were defined to allow for the empirical study of interactions among intonation and syntax-discourse patterns in selected sets of different corpora (monologues and dialogues, by adults and teenagers). In this paper we focus on the multilayer annotation process of left periphery structures by using a small sample of highly spontaneous speech in which the distinct types of topic structures are displayed. The analysis of this sample provides fundamental training and testing material for further application in a wider range of domains and corpora. The annotation process comprises the following time-linked levels (manual and automatic): phone, syllable and word level transcriptions (including co-articulation effects); tonal events and break levels; part-of-speech tagging; syntactic-discourse patterns (construction type; construction position; syntactic function; discourse function), and disfluency events as well. Speech corpora with such a multi-level annotation are a valuable resource to look into grammar module relations in language use from an integrated viewpoint. Such viewpoint is innovative in our language, and has not been often assumed by studies for other languages.</abstract>
    </paper>
    <paper id="142">
      <author><first>Kevin</first><last>Black</last></author>
      <author><first>Eric</first><last>Ringger</last></author>
      <author><first>Paul</first><last>Felt</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <author><first>Kristian</first><last>Heal</last></author>
      <author><first>Deryle</first><last>Lonsdale</last></author>
      <title>Evaluating Lemmatization Models for Machine-Assisted Corpus-Dictionary Linkage</title>
      <pages>3798–3805</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1203_Paper.pdf</url>
      <abstract>The task of corpus-dictionary linkage (CDL) is to annotate each word in a corpus with a link to an appropriate dictionary entry that documents the sense and usage of the word. Corpus-dictionary linked resources include concordances, dictionaries with word usage examples, and corpora annotated with lemmas or word-senses. Such CDL resources are essential in learning a language and in linguistic research, translation, and philology. Lemmatization is a common approximation to automating corpus-dictionary linkage, where lemmas are treated as dictionary entry headwords. We intend to use data-driven lemmatization models to provide machine assistance to human annotators in the form of pre-annotations, and thereby reduce the costs of CDL annotation. In this work we adapt the discriminative string transducer DirecTL+ to perform lemmatization for classical Syriac, a low-resource language. We compare the accuracy of DirecTL+ with the Morfette discriminative lemmatizer. DirecTL+ achieves 96.92% overall accuracy but only by a margin of 0.86% over Morfette at the cost of a longer time to train the model. Error analysis on the models provides guidance on how to apply these models in a machine assistance setting for corpus-dictionary linkage.</abstract>
    </paper>
    <paper id="143">
      <author><first>Jonathan</first><last>Washington</last></author>
      <author><first>Ilnar</first><last>Salimzyanov</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <title>Finite-state morphological transducers for three Kypchak languages</title>
      <pages>3378–3385</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1207_Paper.pdf</url>
      <abstract>This paper describes the development of free/open-source finite-state morphological transducers for three Turkic languages―Kazakh, Tatar, and Kumyk―representing one language from each of the three sub-branches of the Kypchak branch of Turkic. The finite-state toolkit used for the work is the Helsinki Finite-State Toolkit (HFST). This paper describes how the development of a transducer for each subsequent closely-related language took less development time. An evaluation is presented which shows that the transducers all have a reasonable coverage―around 90\%―on freely available corpora of the languages, and high precision over a manually verified test set.</abstract>
    </paper>
    <paper id="144">
      <author><first>Antoni</first><last>Oliver</last></author>
      <author><first>Salvador</first><last>Climent</last></author>
      <title>Automatic creation of <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>ets from parallel corpora</title>
      <pages>1112–1116</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/121_Paper.pdf</url>
      <abstract>In this paper we present the evaluation results for the creation of WordNets for five languages (Spanish, French, German, Italian and Portuguese) using an approach based on parallel corpora. We have used three very large parallel corpora for our experiments: DGT-TM, EMEA and ECB. The English part of each corpus is semantically tagged using Freeling and UKB. After this step, the process of WordNet creation is converted into a word alignment problem, where we want to alignWordNet synsets in the English part of the corpus with lemmata on the target language part of the corpus. The word alignment algorithm used in these experiments is a simple most frequent translation algorithm implemented into the WN-Toolkit. The obtained precision values are quite satisfactory, but the overall number of extracted synset-variant pairs is too low, leading into very poor recall values. In the conclusions, the use of more advanced word alignment algorithms, such as Giza++, Fast Align or Berkeley aligner is suggested.</abstract>
    </paper>
    <paper id="145">
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Mateusz</first><last>Kopeć</last></author>
      <title>The <fixed-case>P</fixed-case>olish Summaries Corpus</title>
      <pages>3712–3715</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1211_Paper.pdf</url>
      <abstract>This article presents the Polish Summaries Corpus, a new resource created to support the development and evaluation of the tools for automated single-document summarization of Polish. The Corpus contains a large number of manual summaries of news articles, with many independently created summaries for a single text. Such approach is supposed to overcome the annotator bias, which is often described as a problem during the evaluation of the summarization algorithms against a single gold standard. There are several summarizers developed specifically for Polish language, but their in-depth evaluation and comparison was impossible without a large, manually created corpus. We present in detail the process of text selection, annotation process and the contents of the corpus, which includes both abstract free-word summaries, as well as extraction-based summaries created by selecting text spans from the original document. Finally, we describe how that resource could be used not only for the evaluation of the existing summarization tools, but also for studies on the human summarization process in Polish language.</abstract>
    </paper>
    <paper id="146">
      <author><first>Tanja</first><last>Schultz</last></author>
      <author><first>Tim</first><last>Schlippe</last></author>
      <title><fixed-case>G</fixed-case>lobal<fixed-case>P</fixed-case>hone: Pronunciation Dictionaries in 20 Languages</title>
      <pages>337–341</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1212_Paper.pdf</url>
      <abstract>This paper describes the advances in the multilingual text and speech database GlobalPhone, a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. GlobalPhone was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers GlobalPhone supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages. Very recently the GlobalPhone pronunciation dictionaries have been made available for research and commercial purposes by the European Language Resources Association (ELRA).</abstract>
    </paper>
    <paper id="147">
      <author><first>Alexandru</first><last>Ceausu</last></author>
      <author><first>Sabine</first><last>Hunsicker</last></author>
      <title>Pre-ordering of phrase-based machine translation input in translation workflow</title>
      <pages>3589–3592</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1213_Paper.pdf</url>
      <abstract>Word reordering is a difficult task for decoders when the languages involved have a significant difference in syntax. Phrase-based statistical machine translation (PBSMT), preferred in commercial settings due to its maturity, is particularly prone to errors in long range reordering. Source sentence pre-ordering, as a pre-processing step before PBSMT, proved to be an efficient solution that can be achieved using limited resources. We propose a dependency-based pre-ordering model with parameters optimized using a reordering score to pre-order the source sentence. The source sentence is then translated using an existing phrase-based system. The proposed solution is very simple to implement. It uses a hierarchical phrase-based statistical machine translation system (HPBSMT) for pre-ordering, combined with a PBSMT system for the actual translation. We show that the system can provide alternate translations of less post-editing effort in a translation workflow with German as the source language.</abstract>
    </paper>
    <paper id="148">
      <author><first>Emanuele</first><last>Di Buccio</last></author>
      <author><first>Giorgio Maria</first><last>Di Nunzio</last></author>
      <author><first>Gianmaria</first><last>Silvello</last></author>
      <title>A Vector Space Model for Syntactic Distances Between Dialects</title>
      <pages>2486–2489</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1214_Paper.pdf</url>
      <abstract>Syntactic comparison across languages is essential in the research field of linguistics, e.g. when investigating the relationship among closely related languages. In IR and NLP, the syntactic information is used to understand the meaning of word occurrences according to the context in which their appear. In this paper, we discuss a mathematical framework to compute the distance between languages based on the data available in current state-of-the-art linguistic databases. This framework is inspired by approaches presented in IR and NLP.</abstract>
    </paper>
    <paper id="149">
      <author><first>Christian</first><last>Curtis</last></author>
      <title>A finite-state morphological analyzer for a <fixed-case>L</fixed-case>akota <fixed-case>HPSG</fixed-case> grammar</title>
      <pages>541–544</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1216_Paper.pdf</url>
      <abstract>This paper reports on the design and implementation of a morphophonological analyzer for Lakota, a member of the Siouan language family. The initial motivation for this work was to support development of a precision implemented grammar for Lakota on the basis of the LinGO Grammar Matrix. A finite-state transducer (FST) was developed to adapt Lakotas complex verbal morphology into a form directly usable as input to the Grammar Matrix-derived grammar. As the FST formalism can be applied in both directions, this approach also supports generative output of correct surface forms from the implemented grammar. This article describes the approach used to model Lakota verbal morphology using finite-state methods. It also discusses the results of developing a lexicon from existing text and evaluating its application to related but novel text. The analyzer presented here, along with its companion precision grammar, explores an approach that may have application in enabling machine translation for endangered and under-resourced languages.</abstract>
    </paper>
    <paper id="150">
      <author><first>Jennifer</first><last>Drexler</last></author>
      <author><first>Pushpendre</first><last>Rastogi</last></author>
      <author><first>Jacqueline</first><last>Aguilar</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <title>A <fixed-case>W</fixed-case>ikipedia-based Corpus for Contextualized Machine Translation</title>
      <pages>3593–3596</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1217_Paper.pdf</url>
      <abstract>We describe a corpus for target-contextualized machine translation (MT), where the task is to improve the translation of source documents using language models built over presumably related documents in the target language. The idea presumes a situation where most of the information about a topic is in a foreign language, yet some related target-language information is known to exist. Our corpus comprises a set of curated English Wikipedia articles describing news events, along with (i) their Spanish counterparts and (ii) some of the Spanish source articles cited within them. In experiments, we translated these Spanish documents, treating the English articles as target-side context, and evaluate the effect on translation quality when including target-side language models built over this English context and interpolated with other, separately-derived language model data. We find that even under this simplistic baseline approach, we achieve significant improvements as measured by BLEU score.</abstract>
    </paper>
    <paper id="151">
      <author><first>Spandana</first><last>Gella</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Vivi</first><last>Nastase</last></author>
      <title>Mapping <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Domains, <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Topics and <fixed-case>W</fixed-case>ikipedia Categories to Generate Multilingual Domain Specific Resources</title>
      <pages>1117–1121</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/122_Paper.pdf</url>
      <abstract>In this paper we present the mapping between WordNet domains and WordNet topics, and the emergent Wikipedia categories. This mapping leads to a coarse alignment between WordNet and Wikipedia, useful for producing domain-specific and multilingual corpora. Multilinguality is achieved through the cross-language links between Wikipedia categories. Research in word-sense disambiguation has shown that within a specific domain, relevant words have restricted senses. The multilingual, and comparable, domain-specific corpora we produce have the potential to enhance research in word-sense disambiguation and terminology extraction in different languages, which could enhance the performance of various NLP tasks.</abstract>
    </paper>
    <paper id="152">
      <author><first>Sophia</first><last>Lee</last></author>
      <author><first>Shoushan</first><last>Li</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <title>Annotating Events in an Emotion Corpus</title>
      <pages>3511–3516</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1222_Paper.pdf</url>
      <abstract>This paper presents the development of a Chinese event-based emotion corpus. It specifically describes the corpus design, collection and annotation. The proposed annotation scheme provides a consistent way of identifying some emotion-associated events (namely pre-events and post-events). Corpus data show that there are significant interactions between emotions and pre-events as well as that of between emotion and post-events. We believe that emotion as a pivot event underlies an innovative approach towards a linguistic model of emotion as well as automatic emotion detection and classification.</abstract>
    </paper>
    <paper id="153">
      <author><first>Shyam Sundar</first><last>Agrawal</last></author>
      <author><first/><last>Abhimanue</last></author>
      <author><first>Shweta</first><last>Bansal</last></author>
      <author><first>Minakshi</first><last>Mahajan</last></author>
      <title>Statistical Analysis of Multilingual Text Corpus and Development of Language Models</title>
      <pages>2436–2440</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1226_Paper.pdf</url>
      <abstract>This paper presents two studies, first a statistical analysis for three languages i.e. Hindi, Punjabi and Nepali and the other, development of language models for three Indian languages i.e. Indian English, Punjabi and Nepali. The main objective of this study is to find distinction among these languages and development of language models for their identification. Detailed statistical analysis have been done to compute the information about entropy, perplexity, vocabulary growth rate etc. Based on statistical features a comparative analysis has been done to find the similarities and differences among these languages. Subsequently an effort has been made to develop a trigram model of Indian English, Punjabi and Nepali. A corpus of 500000 words of each language has been collected and used to develop their models (unigram, bigram and trigram models). The models have been tried in two different databases- Parallel corpora of French and English and Non-parallel corpora of Indian English, Punjabi and Nepali. In the second case, the performance of the model is comparable. Usage of JAVA platform has provided a special effect for dealing with a very large database with high computational speed. Furthermore various enhancive concepts like Smoothing, Discounting, Back off, and Interpolation have been included for the designing of an effective model. The results obtained from this experiment have been described. The information can be useful for development of Automatic Speech Language Identification System.</abstract>
    </paper>
    <paper id="154">
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Denise</first><last>DiPersio</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <author><first>Andrea</first><last>Mazzucchi</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <title>New Directions for Language Resource Development and Distribution</title>
      <pages>1539–1546</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1227_Paper.pdf</url>
      <abstract>Despite the growth in the number of linguistic data centers around the world, their accomplishments and expansions and the advances they have help enable, the language resources that exist are a small fraction of those required to meet the goals of Human Language Technologies (HLT) for the worlds languages and the promises they offer: broad access to knowledge, direct communication across language boundaries and engagement in a global community. Using the Linguistic Data Consortium as a focus case, this paper sketches the progress of data centers, summarizes recent activities and then turns to several issues that have received inadequate attention and proposes some new approaches to their resolution.</abstract>
    </paper>
    <paper id="155">
      <author><first>Joseph</first><last>Mariani</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Gil</first><last>Francopoulo</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <title>Rediscovering 15 Years of Discoveries in Language Resources and Evaluation: The <fixed-case>LREC</fixed-case> Anthology Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/1228_Paper.pdf</url>
      <abstract>This paper aims at analyzing the content of the LREC conferences contained in the ELRA Anthology over the past 15 years (1998-2013). It follows similar exercises that have been conducted, such as the survey on the IEEE ICASSP conference series from 1976 to 1990, which served in the launching of the ESCA Eurospeech conference, a survey of the Association of Computational Linguistics (ACL) over 50 years of existence, which was presented at the ACL conference in 2012, or a survey over the 25 years (1987-2012) of the conferences contained in the ISCA Archive, presented at Interspeech 2013. It contains first an analysis of the evolution of the number of papers and authors over time, including the study of their gender, nationality and affiliation, and of the collaboration among authors. It then studies the funding sources of the research investigations that are reported in the papers. It conducts an analysis of the evolution of the research topics within the community over time. It finally looks at reuse and plagiarism in the papers. The survey shows the present trends in the conference series and in the Language Resources and Evaluation scientific community. Conducting this survey also demonstrated the importance of a clear and unique identification of authors, papers and other sources to facilitate the analysis. This survey is preliminary, as many other aspects also deserve attention. But we hope it will help better understanding and forging our community in the global village.</abstract>
    </paper>
    <paper id="156">
      <author><first>Kirk</first><last>Roberts</last></author>
      <author><first>Kate</first><last>Masterton</last></author>
      <author><first>Marcelo</first><last>Fiszman</last></author>
      <author><first>Halil</first><last>Kilicoglu</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last></author>
      <title>Annotating Question Decomposition on Complex Medical Questions</title>
      <pages>2598–2602</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/124_Paper.pdf</url>
      <abstract>This paper presents a method for annotating question decomposition on complex medical questions. The annotations cover multiple syntactic ways that questions can be decomposed, including separating independent clauses as well as recognizing coordinations and exemplifications. We annotate a corpus of 1,467 multi-sentence consumer health questions about genetic and rare diseases. Furthermore, we label two additional medical-specific annotations: (1) background sentences are annotated with a number of medical categories such as symptoms, treatments, and family history, and (2) the central focus of the complex question (a disease) is marked. We present simple baseline results for automatic classification of these annotations, demonstrating the challenging but important nature of this task.</abstract>
    </paper>
    <paper id="157">
      <author><first>Clarissa</first><last>Xavier</last></author>
      <author><first>Vera</first><last>Lima</last></author>
      <title>Boosting Open Information Extraction with Noun-Based Relations</title>
      <pages>96–100</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/125_Paper.pdf</url>
      <abstract>Open Information Extraction (Open IE) is a strategy for learning relations from texts, regardless the domain and without predefining these relations. Work in this area has focused mainly on verbal relations. In order to extend Open IE to extract relationships that are not expressed by verbs, we present a novel Open IE approach that extracts relations expressed in noun compounds (NCs), such as (oil, extracted from, olive) from olive oil, or in adjective-noun pairs (ANs), such as (moon, that is, gorgeous) from gorgeous moon. The approach consists of three steps: detection of NCs and ANs, interpretation of these compounds in view of corpus enrichment and extraction of relations from the enriched corpus. To confirm the feasibility of this method we created a prototype and evaluated the impact of the application of our proposal in two state-of-the-art Open IE extractors. Based on these tests we conclude that the proposed approach is an important step to fulfil the gap concerning the extraction of relations within the noun compounds and adjective-noun pairs in Open IE.</abstract>
    </paper>
    <paper id="158">
      <author><first>Krasimir</first><last>Angelov</last></author>
      <title>Bootstrapping Open-Source <fixed-case>E</fixed-case>nglish-<fixed-case>B</fixed-case>ulgarian Computational Dictionary</title>
      <pages>1018–1023</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/127_Paper.pdf</url>
      <abstract>We present an open-source English-Bulgarian dictionary which is a unification and consolidation of existing and freely available resources for the two languages. The new resource can be used as either a pair of two monolingual morphological lexicons, or as a bidirectional translation dictionary between the languages. The structure of the resource is compatible with the existing synchronous English-Bulgarian grammar in Grammatical Framework (GF). This makes it possible to immediately plug it in as a component in a grammar-based translation system that is currently under development in the same framework. This also meant that we had to enrich the dictionary with additional syntactic and semantic information that was missing in the original resources.</abstract>
    </paper>
    <paper id="159">
      <author><first>Mathieu</first><last>Mangeot</last></author>
      <title><fixed-case>M</fixed-case>otà<fixed-case>M</fixed-case>ot project: conversion of a <fixed-case>F</fixed-case>rench-<fixed-case>K</fixed-case>hmer published dictionary for building a multilingual lexical system</title>
      <pages>1024–1031</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/128_Paper.pdf</url>
      <abstract>Economic issues related to the information processing techniques are very important. The development of such technologies is a major asset for developing countries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and Thailand. The MotAMot project aims to computerize an under-resourced language: Khmer, spoken mainly in Cambodia. The main goal of the project is the development of a multilingual lexical system targeted for Khmer. The macrostructure is a pivot one with each word sense of each language linked to a pivot axi. The microstructure comes from a simplification of the explanatory and combinatory dictionary. The lexical system has been initialized with data coming mainly from the conversion of the French-Khmer bilingual dictionary of Denis Richer from Word to XML format. The French part was completed with pronunciation and parts-of-speech coming from the FeM French-english-Malay dictionary. The Khmer headwords noted in IPA in the Richer dictionary were converted to Khmer writing with OpenFST, a finite state transducer tool. The resulting resource is available online for lookup, editing, download and remote programming via a REST API on a Jibiki platform.</abstract>
    </paper>
    <paper id="160">
      <author><first>Sérgio</first><last>Curto</last></author>
      <author><first>Ana C.</first><last>Mendes</last></author>
      <author><first>Pedro</first><last>Curto</last></author>
      <author><first>Luísa</first><last>Coheur</last></author>
      <author><first>Ângela</first><last>Costa</last></author>
      <title><fixed-case>JUST</fixed-case>.<fixed-case>ASK</fixed-case>, a <fixed-case>QA</fixed-case> system that learns to answer new questions from previous interactions</title>
      <pages>2603–2607</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/130_Paper.pdf</url>
      <abstract>We present JUST.ASK, a publicly available Question Answering system, which is freely available. Its architecture is composed of the usual Question Processing, Passage Retrieval and Answer Extraction components. Several details on the information generated and manipulated by each of these components are also provided to the user when interacting with the demonstration. Since JUST.ASK also learns to answer new questions based on users feedback, (s)he is invited to identify the correct answers. These will then be used to retrieve answers to future questions.</abstract>
    </paper>
    <paper id="161">
      <author><first>Manjira</first><last>Sinha</last></author>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <author><first>Anupam</first><last>Basu</last></author>
      <title>Design and Development of an Online Computational Framework to Facilitate Language Comprehension Research on <fixed-case>I</fixed-case>ndian Languages</title>
      <pages>203–210</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/132_Paper.pdf</url>
      <abstract>In this paper we have developed an open-source online computational framework that can be used by different research groups to conduct reading researches on Indian language texts. The framework can be used to develop a large annotated Indian language text comprehension data from different user based experiments. The novelty in this framework lies in the fact that it brings different empirical data-collection techniques for text comprehension under one roof. The framework has been customized specifically to address language particularities for Indian languages. It will also offer many types of automatic analysis on the data at different levels such as full text, sentence and word level. To address the subjectivity of text difficulty perception, the framework allows to capture user background against multiple factors. The assimilated data can be automatically cross referenced against varying strata of readers.</abstract>
    </paper>
    <paper id="162">
      <author><first>Mirjam</first><last>Ernestus</last></author>
      <author><first>Lucie</first><last>Kočková-Amortová</last></author>
      <author><first>Petr</first><last>Pollak</last></author>
      <title>The Nijmegen Corpus of Casual <fixed-case>C</fixed-case>zech</title>
      <pages>365–370</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/134_Paper.pdf</url>
      <abstract>This article introduces a new speech corpus, the Nijmegen Corpus of Casual Czech (NCCCz), which contains more than 30 hours of high-quality recordings of casual conversations in Common Czech, among ten groups of three male and ten groups of three female friends. All speakers were native speakers of Czech, raised in Prague or in the region of Central Bohemia, and were between 19 and 26 years old. Every group of speakers consisted of one confederate, who was instructed to keep the conversations lively, and two speakers naive to the purposes of the recordings. The naive speakers were engaged in conversations for approximately 90 minutes, while the confederate joined them for approximately the last 72 minutes. The corpus was orthographically annotated by experienced transcribers and this orthographic transcription was aligned with the speech signal. In addition, the conversations were videotaped. This corpus can form the basis for all types of research on casual conversations in Czech, including phonetic research and research on how to improve automatic speech recognition. The corpus will be freely available.</abstract>
    </paper>
    <paper id="163">
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <title><fixed-case>M</fixed-case>odern <fixed-case>C</fixed-case>hinese Helps Archaic <fixed-case>C</fixed-case>hinese Processing: Finding and Exploiting the Shared Properties</title>
      <pages>3129–3136</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/138_Paper.pdf</url>
      <abstract>Languages change over time and ancient languages have been studied in linguistics and other related fields. A main challenge in this research area is the lack of empirical data; for instance, ancient spoken languages often leave little trace of their linguistic properties. From the perspective of natural language processing (NLP), while the NLP community has created dozens of annotated corpora, very few of them are on ancient languages. As an effort toward bridging the gap, we have created a word segmented and POS tagged corpus for Archaic Chinese using articles from Huainanzi, a book written during Chinas Western Han Dynasty (206 BC-9 AD). We then compare this corpus with the Chinese Penn Treebank (CTB), a well-known corpus for Modern Chinese, and report several interesting differences and similarities between the two corpora. Finally, we demonstrate that the CTB can be used to improve the performance of word segmenters and POS taggers for Archaic Chinese, but only through features that have similar behaviors in the two corpora.</abstract>
    </paper>
    <paper id="164">
      <author><first>Włodzimierz</first><last>Gruszczyński</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <title>Digital Library 2.0: Source of Knowledge and Research Collaboration Platform</title>
      <pages>1649–1653</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/14_Paper.pdf</url>
      <abstract>Digital libraries are frequently treated just as a new method of storage of digitized artifacts, with all consequences of transferring long-established ways of dealing with physical objects into the digital world. Such attitude improves availability, but often neglects other opportunities offered by global and immediate access, virtuality and linking ― as easy as never before. The article presents the idea of transforming a conventional digital library into knowledge source and research collaboration platform, facilitating content augmentation, interpretation and co-operation of geographically distributed researchers representing different academic fields. This concept has been verified by the process of extending descriptions stored in thematic Digital Library of Polish and Poland-related Ephemeral Prints from the 16th, 17th and 18th Centuries with extended item-associated information provided by historians, philologists, librarians and computer scientists. It resulted in associating the customary fixed metadata and digitized content with historical comments, mini-dictionaries of foreign interjections or explanation of less-known background details.</abstract>
    </paper>
    <paper id="165">
      <author><first>Kristiina</first><last>Jokinen</last></author>
      <title>Open-domain Interaction and Online Content in the <fixed-case>S</fixed-case>ami Language</title>
      <pages>517–522</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/143_Paper.pdf</url>
      <abstract>This paper presents data collection and collaborative community events organised within the project Digital Natives on the North Sami language. The project is one of the collaboration initiatives on endangered Finno-Ugric languages, supported by the larger framework between the Academy of Finland and the Hungarian Academy of Sciences. The goal of the project is to improve digital visibility and viability of the targeted Finno-Ugric languages, as well as to develop language technology tools and resources in order to assist automatic language processing and experimenting with multilingual interactive applications.</abstract>
    </paper>
    <paper id="166">
      <author><first>Akira</first><last>Utsumi</last></author>
      <title>A Character-based Approach to Distributional Semantic Models: Exploiting Kanji Characters for Constructing <fixed-case>J</fixed-case>apanese<fixed-case>W</fixed-case>ord Vectors</title>
      <pages>4444–4450</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/144_Paper.pdf</url>
      <abstract>Many Japanese words are made of kanji characters, which themselves represent meanings. However traditional word-based distributional semantic models (DSMs) do not benefit from the useful semantic information of kanji characters. In this paper, we propose a method for exploiting the semantic information of kanji characters for constructing Japanese word vectors in DSMs. In the proposed method, the semantic representations of kanji characters (i.e, kanji vectors) are constructed first using the techniques of DSMs, and then word vectors are computed by combining the vectors of constituent kanji characters using vector composition methods. The evaluation experiment using a synonym identification task demonstrates that the kanji-based DSM achieves the best performance when a kanji-kanji matrix is weighted by positive pointwise mutual information and word vectors are composed by weighted multiplication. Comparison between kanji-based DSMs and word-based DSMs reveals that our kanji-based DSMs generally outperform latent semantic analysis, and also surpasses the best score word-based DSM for infrequent words comprising only frequent kanji characters. These findings clearly indicate that kanji-based DSMs are beneficial in improvement of quality of Japanese word vectors.</abstract>
    </paper>
    <paper id="167">
      <author><first>Dasha</first><last>Bogdanova</last></author>
      <author><first>Angeliki</first><last>Lazaridou</last></author>
      <title>Cross-Language Authorship Attribution</title>
      <pages>2015–2020</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/145_Paper.pdf</url>
      <abstract>This paper presents a novel task of cross-language authorship attribution (CLAA), an extension of authorship attribution task to multilingual settings: given data labelled with authors in language X, the objective is to determine the author of a document written in language Y , where X is different from Y . We propose a number of cross-language stylometric features for the task of CLAA, such as those based on sentiment and emotional markers. We also explore an approach based on machine translation (MT) with both lexical and cross-language features. We experimentally show that MT could be used as a starting point to CLAA, since it allows good attribution accuracy to be achieved. The cross-language features provide acceptable accuracy while using jointly with MT, though do not outperform lexical features.</abstract>
    </paper>
    <paper id="168">
      <author><first>Paul</first><last>Felt</last></author>
      <author><first>Eric</first><last>Ringger</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <author><first>Kristian</first><last>Heal</last></author>
      <title>Using Transfer Learning to Assist Exploratory Corpus Annotation</title>
      <pages>140–145</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/147_Paper.pdf</url>
      <abstract>We describe an under-studied problem in language resource management: that of providing automatic assistance to annotators working in exploratory settings. When no satisfactory tagset already exists, such as in under-resourced or undocumented languages, it must be developed iteratively while annotating data. This process naturally gives rise to a sequence of datasets, each annotated differently. We argue that this problem is best regarded as a transfer learning problem with multiple source tasks. Using part-of-speech tagging data with simulated exploratory tagsets, we demonstrate that even simple transfer learning techniques can significantly improve the quality of pre-annotations in an exploratory annotation.</abstract>
    </paper>
    <paper id="169">
      <author><first>Judith</first><last>Muzerelle</last></author>
      <author><first>Anaïs</first><last>Lefeuvre</last></author>
      <author><first>Emmanuel</first><last>Schang</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Aurore</first><last>Pelletier</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <author><first>Iris</first><last>Eshkol</last></author>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <title><fixed-case>ANCOR</fixed-case>_<fixed-case>C</fixed-case>entre, a large free spoken <fixed-case>F</fixed-case>rench coreference corpus: description of the resource and reliability measures</title>
      <pages>843–847</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/150_Paper.pdf</url>
      <abstract>This article presents ANCOR_Centre, a French coreference corpus, available under the Creative Commons Licence. With a size of around 500,000 words, the corpus is large enough to serve the needs of data-driven approaches in NLP and represents one of the largest coreference resources currently available. The corpus focuses exclusively on spoken language, it aims at representing a certain variety of spoken genders. ANCOR_Centre includes anaphora as well as coreference relations which involve nominal and pronominal mentions. The paper describes into details the annotation scheme and the reliability measures computed on the resource.</abstract>
    </paper>
    <paper id="170">
      <author><first>Alex</first><last>Rudnick</last></author>
      <author><first>Taylor</first><last>Skidmore</last></author>
      <author><first>Alberto</first><last>Samaniego</last></author>
      <author><first>Michael</first><last>Gasser</last></author>
      <title><fixed-case>G</fixed-case>uampa: a Toolkit for Collaborative Translation</title>
      <pages>1659–1663</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/151_Paper.pdf</url>
      <abstract>Here we present Guampa, a new software package for online collaborative translation. This system grows out of our discussions with Guarani-language activists and educators in Paraguay, and attempts to address problems faced by machine translation researchers and by members of any community speaking an under-represented language. Guampa enables volunteers and students to work together to translate documents into heritage languages, both to make more materials available in those languages, and also to generate bitext suitable for training machine translation systems. While many approaches to crowdsourcing bitext corpora focus on Mechanical Turk and temporarily engaging anonymous workers, Guampa is intended to foster an online community in which discussions can take place, language learners can practice their translation skills, and complete documents can be translated. This approach is appropriate for the Spanish-Guarani language pair as there are many speakers of both languages, and Guarani has a dedicated activist community. Our goal is to make it easy for anyone to set up their own instance of Guampa and populate it with documents -- such as automatically imported Wikipedia articles -- to be translated for their particular language pair. Guampa is freely available and relatively easy to use.</abstract>
    </paper>
    <paper id="171">
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Menzo</first><last>Windhouwer</last></author>
      <title>Experiences with the <fixed-case>ISO</fixed-case>cat Data Category Registry</title>
      <pages>4565–4568</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/153_Paper.pdf</url>
      <abstract>The ISOcat Data Category Registry has been a joint project of both ISO TC 37 and the European CLARIN infrastructure. In this paper the experiences of using ISOcat in CLARIN are described and evaluated. This evaluation clarifies the requirements of CLARIN with regard to a semantic registry to support its semantic interoperability needs. A simpler model based on concepts instead of data cate-gories and a simpler workflow based on community recommendations will address these needs better and offer the required flexibility.</abstract>
    </paper>
    <paper id="172">
      <author><first>Menzo</first><last>Windhouwer</last></author>
      <author><first>Justin</first><last>Petro</last></author>
      <author><first>Shakila</first><last>Shayan</last></author>
      <title><fixed-case>RELISH</fixed-case> <fixed-case>LMF</fixed-case>: Unlocking the Full Power of the Lexical Markup Framework</title>
      <pages>1032–1037</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/154_Paper.pdf</url>
      <abstract>The Lexical Markup Framework (ISO 24613:2008) provides a core class diagram and various extensions as the basis for constructing lexical resources. Unfortunately the informative Document Type Definition provided by the standard and other available LMF serializations lack support for many of the powerful features of the model. This paper describes RELISH LMF, which unlocks the full power of the LMF model by providing a set of extensible modern schema modules. As use cases RELISH LL LMF and support by LEXUS, an online lexicon tool, are described.</abstract>
    </paper>
    <paper id="173">
      <author><first>Ryu</first><last>Iida</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <title>Building a Corpus of Manually Revised Texts from Discourse Perspective</title>
      <pages>936–941</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/155_Paper.pdf</url>
      <abstract>This paper presents building a corpus of manually revised texts which includes both before and after-revision information. In order to create such a corpus, we propose a procedure for revising a text from a discourse perspective, consisting of dividing a text to discourse units, organising and reordering groups of discourse units and finally modifying referring and connective expressions, each of which imposes limits on freedom of revision. Following the procedure, six revisers who have enough experience in either teaching Japanese or scoring Japanese essays revised 120 Japanese essays written by Japanese native speakers. Comparing the original and revised texts, we found some specific manual revisions frequently occurred between the original and revised texts, e.g. thesis statements were frequently placed at the beginning of a text. We also evaluate text coherence using the original and revised texts on the task of pairwise information ordering, identifying a more coherent text. The experimental results using two text coherence models demonstrated that the two models did not outperform the random baseline.</abstract>
    </paper>
    <paper id="174">
      <author><first>Matej</first><last>Ďurčo</last></author>
      <author><first>Menzo</first><last>Windhouwer</last></author>
      <title>The <fixed-case>CMD</fixed-case> Cloud</title>
      <pages>687–690</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/156_Paper.pdf</url>
      <abstract>The CLARIN Component Metadata Infrastructure (CMDI) established means for flexible resource descriptions for the domain of language resources with sound provisions for semantic interoperability weaved deeply into the meta model and the infrastructure. Based on this solid grounding, the infrastructure accommodates a growing collection of metadata records. In this paper, we give a short overview of the current status in the CMD data domain on the schema and instance level and harness the installed mechanisms for semantic interoperability to explore the similarity relations between individual profiles/schemas. We propose a method to use the semantic links shared among the profiles to generate/compile a similarity graph. This information is further rendered in an interactive graph viewer: the SMC Browser. The resulting interactive graph offers an intuitive view on the complex interrelations of the discussed dataset revealing clusters of more similar profiles. This information is useful both for metadata modellers, for metadata curation tasks as well as for general audience seeking for a ‘big picture’ of the complex CMD data domain.</abstract>
    </paper>
    <paper id="175">
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Anju</first><last>Saxena</last></author>
      <author><first>Taraka</first><last>Rama</last></author>
      <author><first>Bernard</first><last>Comrie</last></author>
      <title>Linguistic landscaping of <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sia using digital language resources: Genetic vs. areal linguistics</title>
      <pages>3137–3144</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/159_Paper.pdf</url>
      <abstract>Like many other research fields, linguistics is entering the age of big data. We are now at a point where it is possible to see how new research questions can be formulated - and old research questions addressed from a new angle or established results verified - on the basis of exhaustive collections of data, rather than small, carefully selected samples. For example, South Asia is often mentioned in the literature as a classic example of a linguistic area, but there is no systematic, empirical study substantiating this claim. Examination of genealogical and areal relationships among South Asian languages requires a large-scale quantitative and qualitative comparative study, encompassing more than one language family. Further, such a study cannot be conducted manually, but needs to draw on extensive digitized language resources and state-of-the-art computational tools. We present some preliminary results of our large-scale investigation of the genealogical and areal relationships among the languages of this region, based on the linguistic descriptions available in the 19 tomes of Grierson’s monumental “Linguistic Survey of India” (1903-1927), which is currently being digitized with the aim of turning the linguistic information in the LSI into a digital language resource suitable for a broad array of linguistic investigations.</abstract>
    </paper>
    <paper id="176">
      <author><first>Anabela</first><last>Barreiro</last></author>
      <author><first>Johanna</first><last>Monti</last></author>
      <author><first>Brigitte</first><last>Orliac</last></author>
      <author><first>Susanne</first><last>Preuß</last></author>
      <author><first>Kutz</first><last>Arrieta</last></author>
      <author><first>Wang</first><last>Ling</last></author>
      <author><first>Fernando</first><last>Batista</last></author>
      <author><first>Isabel</first><last>Trancoso</last></author>
      <title>Linguistic Evaluation of Support Verb Constructions by <fixed-case>O</fixed-case>pen<fixed-case>L</fixed-case>ogos and <fixed-case>G</fixed-case>oogle <fixed-case>T</fixed-case>ranslate</title>
      <pages>35–40</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/16_Paper.pdf</url>
      <abstract>This paper presents a systematic human evaluation of translations of English support verb constructions produced by a rule-based machine translation (RBMT) system (OpenLogos) and a statistical machine translation (SMT) system (Google Translate) for five languages: French, German, Italian, Portuguese and Spanish. We classify support verb constructions by means of their syntactic structure and semantic behavior and present a qualitative analysis of their translation errors. The study aims to verify how machine translation (MT) systems translate fine-grained linguistic phenomena, and how well-equipped they are to produce high-quality translation. Another goal of the linguistically motivated quality analysis of SVC raw output is to reinforce the need for better system hybridization, which leverages the strengths of RBMT to the benefit of SMT, especially in improving the translation of multiword units. Taking multiword units into account, we propose an effective method to achieve MT hybridization based on the integration of semantico-syntactic knowledge into SMT.</abstract>
    </paper>
    <paper id="177">
      <author><first>Michael</first><last>Kipp</last></author>
      <author><first>Levin Freiherr</first><last>von Hollen</last></author>
      <author><first>Michael Christopher</first><last>Hrstka</last></author>
      <author><first>Franziska</first><last>Zamponi</last></author>
      <title>Single-Person and Multi-Party 3<fixed-case>D</fixed-case> Visualizations for Nonverbal Communication Analysis</title>
      <pages>3393–3397</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/160_Paper.pdf</url>
      <abstract>The qualitative analysis of nonverbal communication is more and more relying on 3D recording technology. However, the human analysis of 3D data on a regular 2D screen can be challenging as 3D scenes are difficult to visually parse. To optimally exploit the full depth of the 3D data, we propose to enhance the 3D view with a number of visualizations that clarify spatial and conceptual relationships and add derived data like speed and angles. In this paper, we present visualizations for directional body motion, hand movement direction, gesture space location, and proxemic dimensions like interpersonal distance, movement and orientation. The proposed visualizations are available in the open source tool JMocap and are planned to be fully integrated into the ANVIL video annotation tool. The described techniques are intended to make annotation more efficient and reliable and may allow the discovery of entirely new phenomena.</abstract>
    </paper>
    <paper id="178">
      <author><first>Hiroaki</first><last>Shimizu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <author><first>Tomoki</first><last>Toda</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <title>Collection of a Simultaneous Translation Corpus for Comparative Analysis</title>
      <pages>670–673</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/162_Paper.pdf</url>
      <abstract>This paper describes the collection of an English-Japanese/Japanese-English simultaneous interpretation corpus. There are two main features of the corpus. The first is that professional simultaneous interpreters with different amounts of experience cooperated with the collection. By comparing data from simultaneous interpretation of each interpreter, it is possible to compare better interpretations to those that are not as good. The second is that for part of our corpus there are already translation data available. This makes it possible to compare translation data with simultaneous interpretation data. We recorded the interpretations of lectures and news, and created time-aligned transcriptions. A total of 387k words of transcribed data were collected. The corpus will be helpful to analyze differences in interpretations styles and to construct simultaneous interpretation systems.</abstract>
    </paper>
    <paper id="179">
      <author><first>Hüseyin</first><last>Çakmak</last></author>
      <author><first>Jérôme</first><last>Urbain</last></author>
      <author><first>Thierry</first><last>Dutoit</last></author>
      <author><first>Joëlle</first><last>Tilmanne</last></author>
      <title>The <fixed-case>AV</fixed-case>-<fixed-case>LASYN</fixed-case> Database : A synchronous corpus of audio and 3<fixed-case>D</fixed-case> facial marker data for audio-visual laughter synthesis</title>
      <pages>3398–3403</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/163_Paper.pdf</url>
      <abstract>A synchronous database of acoustic and 3D facial marker data was built for audio-visual laughter synthesis. Since the aim is to use this database for HMM-based modeling and synthesis, the amount of collected data from one given subject had to be maximized. The corpus contains 251 utterances of laughter from one male participant. Laughter was elicited with the help of humorous videos. The resulting database is synchronous between modalities (audio and 3D facial motion capture data). Visual 3D data is available in common formats such as BVH and C3D with head motion and facial deformation independently available. Data is segmented and audio has been annotated. Phonetic transcriptions are available in the HTK-compatible format. Principal component analysis has been conducted on visual data and has shown that a dimensionality reduction might be relevant. The corpus may be obtained under a research license upon request to authors.</abstract>
    </paper>
    <paper id="180">
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Andrei</first><last>Malchanau</last></author>
      <author><first>Harry</first><last>Bunt</last></author>
      <title>Interoperability of Dialogue Corpora through <fixed-case>ISO</fixed-case> 24617-2-based Querying</title>
      <pages>4407–4414</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/165_Paper.pdf</url>
      <abstract>This paper explores a way of achieving interoperability: developing a query format for accessing existing annotated corpora whose expressions make use of the annotation language defined by the standard. The interpretation of expressions in the query implements a mapping from ISO 24617-2 concepts to those of the annotation scheme used in the corpus. We discuss two possible ways to query existing annotated corpora using DiAML. One way is to transform corpora into DiAML compliant format, and subsequently query these data using XQuery or XPath. The second approach is to define a DiAML query that can be directly used to retrieve requested information from the annotated data. Both approaches are valid. The first one presents a standard way of querying XML data. The second approach is a DiAML-oriented querying of dialogue act annotated data, for which we designed an interface. The proposed approach is tested on two important types of existing dialogue corpora: spoken two-person dialogue corpora collected and annotated within the HCRC Map Task paradigm, and multiparty face-to-face dialogues of the AMI corpus. We present the results and evaluate them with respect to accuracy and completeness through statistical comparisons between retrieved and manually constructed reference annotations.</abstract>
    </paper>
    <paper id="181">
      <author><first>Catia</first><last>Cucchiarini</last></author>
      <author><first>Steve</first><last>Bodnar</last></author>
      <author><first>Bart Penning</first><last>de Vries</last></author>
      <author><first>Roeland</first><last>van Hout</last></author>
      <author><first>Helmer</first><last>Strik</last></author>
      <title><fixed-case>ASR</fixed-case>-based <fixed-case>CALL</fixed-case> systems and learner speech data: new resources and opportunities for research and development in second language learning</title>
      <pages>2708–2714</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/168_Paper.pdf</url>
      <abstract>In this paper we describe the language resources developed within the project Feedback and the Acquisition of Syntax in Oral Proficiency (FASOP), which is aimed at investigating the effectiveness of various forms of practice and feedback on the acquisition of syntax in second language (L2) oral proficiency, as well as their interplay with learner characteristics such as education level, learner motivation and confidence. For this purpose, use is made of a Computer Assisted Language Learning (CALL) system that employs Automatic Speech Recognition (ASR) technology to allow spoken interaction and to create an experimental environment that guarantees as much control over the language learning setting as possible. The focus of the present paper is on the resources that are being produced in FASOP. In line with the theme of this conference, we present the different types of resources developed within this project and the way in which these could be used to pursue innovative research in second language acquisition and to develop and improve ASR-based language learning applications.</abstract>
    </paper>
    <paper id="182">
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Martin</first><last>Gropp</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Gregor</first><last>Eigner</last></author>
      <author><first>Mario</first><last>Topf</last></author>
      <author><first>Stefan</first><last>Srb</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <author><first>Blaise</first><last>Potard</last></author>
      <author><first>John</first><last>Dines</last></author>
      <author><first>Olivier</first><last>Deroo</last></author>
      <author><first>Ronny</first><last>Egeler</last></author>
      <author><first>Uwe</first><last>Meinz</last></author>
      <author><first>Steffen</first><last>Liersch</last></author>
      <author><first>Anna</first><last>Schmidt</last></author>
      <title>The <fixed-case>DBOX</fixed-case> Corpus Collection of Spoken Human-Human and Human-Machine Dialogues</title>
      <pages>252–258</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/169_Paper.pdf</url>
      <abstract>This paper describes the data collection and annotation carried out within the DBOX project ( Eureka project, number E! 7152). This project aims to develop interactive games based on spoken natural language human-computer dialogues, in 3 European languages: English, German and French. We collect the DBOX data continuously. We first start with human-human Wizard of Oz experiments to collect human-human data in order to model natural human dialogue behaviour, for better understanding of phenomena of human interactions and predicting interlocutors actions, and then replace the human Wizard by an increasingly advanced dialogue system, using evaluation data for system improvement. The designed dialogue system relies on a Question-Answering (QA) approach, but showing truly interactive gaming behaviour, e.g., by providing feedback, managing turns and contact, producing social signals and acts, e.g., encouraging vs. downplaying, polite vs. rude, positive vs. negative attitude towards players or their actions, etc. The DBOX dialogue corpus has required substantial investment. We expect it to have a great impact on the rest of the project. The DBOX project consortium will continue to maintain the corpus and to take an interest in its growth, e.g., expand to other languages. The resulting corpus will be publicly released.</abstract>
    </paper>
    <paper id="183">
      <author><first>Thomas</first><last>Schmidt</last></author>
      <title>The Database for Spoken <fixed-case>G</fixed-case>erman — <fixed-case>DGD</fixed-case>2</title>
      <pages>1451–1457</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/171_Paper.pdf</url>
      <abstract>The Database for Spoken German (Datenbank für Gesprochenes Deutsch, DGD2, http://dgd.ids-mannheim.de) is the central platform for publishing and disseminating spoken language corpora from the Archive of Spoken German (Archiv für Gesprochenes Deutsch, AGD, http://agd.ids-mannheim.de) at the Institute for the German Language in Mannheim. The corpora contained in the DGD2 come from a variety of sources, some of them in-house projects, some of them external projects. Most of the corpora were originally intended either for research into the (dialectal) variation of German or for studies in conversation analysis and related fields. The AGD has taken over the task of permanently archiving these resources and making them available for reuse to the research community. To date, the DGD2 offers access to 19 different corpora, totalling around 9000 speech events, 2500 hours of audio recordings or 8 million transcribed words. This paper gives an overview of the data made available via the DGD2, of the technical basis for its implementation, and of the most important functionalities it offers. The paper concludes with information about the users of the database and future plans for its development.</abstract>
    </paper>
    <paper id="184">
      <author><first>Liviu</first><last>Dinu</last></author>
      <author><first>Alina Maria</first><last>Ciobanu</last></author>
      <title>Building a Dataset of Multilingual Cognates for the <fixed-case>R</fixed-case>omanian Lexicon</title>
      <pages>1038–1043</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/175_Paper.pdf</url>
      <abstract>Identifying cognates is an interesting task with applications in numerous research areas, such as historical and comparative linguistics, language acquisition, cross-lingual information retrieval, readability and machine translation. We propose a dictionary-based approach to identifying cognates based on etymology and etymons. We account for relationships between languages and we extract etymology-related information from electronic dictionaries. We employ the dataset of cognates that we obtain as a gold standard for evaluating to which extent orthographic methods can be used to detect cognate pairs. The question that arises is whether they are able to discriminate between cognates and non-cognates, given the orthographic changes undergone by foreign words when entering new languages. We investigate some orthographic approaches widely used in this research area and some original metrics as well. We run our experiments on the Romanian lexicon, but the method we propose is adaptable to any language, as far as resources are available.</abstract>
    </paper>
    <paper id="185">
      <author><first>Giuseppe</first><last>Rizzo</last></author>
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Raphaël</first><last>Troncy</last></author>
      <title>Benchmarking the Extraction and Disambiguation of Named Entities on the Semantic Web</title>
      <pages>4593–4600</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/176_Paper.pdf</url>
      <abstract>Named entity recognition and disambiguation are of primary importance for extracting information and for populating knowledge bases. Detecting and classifying named entities has traditionally been taken on by the natural language processing community, whilst linking of entities to external resources, such as those in DBpedia, has been tackled by the Semantic Web community. As these tasks are treated in different communities, there is as yet no oversight on the performance of these tasks combined. We present an approach that combines the state-of-the art from named entity recognition in the natural language processing domain and named entity linking from the semantic web community. We report on experiments and results to gain more insights into the strengths and limitations of current approaches on these tasks. Our approach relies on the numerous web extractors supported by the NERD framework, which we combine with a machine learning algorithm to optimize recognition and linking of named entities. We test our approach on four standard data sets that are composed of two diverse text types, namely newswire and microposts.</abstract>
    </paper>
    <paper id="186">
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Kit</first><last>Cho</last></author>
      <author><first>G. Aaron</first><last>Broadwell</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>John</first><last>Lien</last></author>
      <author><first>Sarah</first><last>Taylor</last></author>
      <author><first>Laurie</first><last>Feldman</last></author>
      <author><first>Boris</first><last>Yamrom</last></author>
      <author><first>Nick</first><last>Webb</last></author>
      <author><first>Umit</first><last>Boz</last></author>
      <author><first>Ignacio</first><last>Cases</last></author>
      <author><first>Ching-sheng</first><last>Lin</last></author>
      <title>Automatic Expansion of the <fixed-case>MRC</fixed-case> Psycholinguistic Database Imageability Ratings</title>
      <pages>2800–2805</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/178_Paper.pdf</url>
      <abstract>Recent studies in metaphor extraction across several languages (Broadwell et al., 2013; Strzalkowski et al., 2013) have shown that word imageability ratings are highly correlated with the presence of metaphors in text. Information about imageability of words can be obtained from the MRC Psycholinguistic Database (MRCPD) for English words and Léxico Informatizado del Español Programa (LEXESP) for Spanish words, which is a collection of human ratings obtained in a series of controlled surveys. Unfortunately, word imageability ratings were collected for only a limited number of words: 9,240 words in English, 6,233 in Spanish; and are unavailable at all in the other two languages studied: Russian and Farsi. The present study describes an automated method for expanding the MRCPD by conferring imageability ratings over the synonyms and hyponyms of existing MRCPD words, as identified in Wordnet. The result is an expanded MRCPD+ database with imagea-bility scores for more than 100,000 words. The appropriateness of this expansion process is assessed by examining the structural coherence of the expanded set and by validating the expanded lexicon against human judgment. Finally, the performance of the metaphor extraction system is shown to improve significantly with the expanded database. This paper describes the process for English MRCPD+ and the resulting lexical resource. The process is analogous for other languages.</abstract>
    </paper>
    <paper id="187">
      <author><first>Riyaz Ahmad</first><last>Bhat</last></author>
      <author><first>Shahid Mushtaq</first><last>Bhat</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <title>Towards building a <fixed-case>K</fixed-case>ashmiri Treebank: Setting up the Annotation Pipeline</title>
      <pages>748–752</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/18_Paper.pdf</url>
      <abstract>Kashmiri is a resource poor language with very less computational and language resources available for its text processing. As the main contribution of this paper, we present an initial version of the Kashmiri Dependency Treebank. The treebank consists of 1,000 sentences (17,462 tokens), annotated with part-of-speech (POS), chunk and dependency information. The treebank has been manually annotated using the Paninian Computational Grammar (PCG) formalism (Begum et al., 2008; Bharati et al., 2009). This version of Kashmiri treebank is an extension of its earlier verion of 500 sentences (Bhat, 2012), a pilot experiment aimed at defining the annotation guidelines on a small subset of Kashmiri corpora. In this paper, we have refined the guidelines with some significant changes and have carried out inter-annotator agreement studies to ascertain its quality. We also present a dependency parsing pipeline, consisting of a tokenizer, a stemmer, a POS tagger, a chunker and an inter-chunk dependency parser. It, therefore, constitutes the first freely available, open source dependency parser of Kashmiri, setting the initial baseline for Kashmiri dependency parsing.</abstract>
    </paper>
    <paper id="188">
      <author><first>Olga</first><last>Uryupina</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Aliaksei</first><last>Severyn</last></author>
      <author><first>Agata</first><last>Rotondi</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <title><fixed-case>S</fixed-case>en<fixed-case>T</fixed-case>ube: A Corpus for Sentiment Analysis on <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube Social Media</title>
      <pages>4244–4249</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/180_Paper.pdf</url>
      <abstract>In this paper we present SenTube -- a dataset of user-generated comments on YouTube videos annotated for information content and sentiment polarity. It contains annotations that allow to develop classifiers for several important NLP tasks: (i) sentiment analysis, (ii) text categorization (relatedness of a comment to video and/or product), (iii) spam detection, and (iv) prediction of comment informativeness. The SenTube corpus favors the development of research on indexing and searching YouTube videos exploiting information derived from comments. The corpus will cover several languages: at the moment, we focus on English and Italian, with Spanish and Dutch parts scheduled for the later stages of the project. For all the languages, we collect videos for the same set of products, thus offering possibilities for multi- and cross-lingual experiments. The paper provides annotation guidelines, corpus statistics and annotator agreement details.</abstract>
    </paper>
    <paper id="189">
      <author><first>Carlos Daniel</first><last>Hernandez Mena</last></author>
      <author><first>Abel Herrera</first><last>Camacho</last></author>
      <title><fixed-case>CIEMPIESS</fixed-case>: A New Open-Sourced <fixed-case>M</fixed-case>exican <fixed-case>S</fixed-case>panish Radio Corpus</title>
      <pages>371–375</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/182_Paper.pdf</url>
      <abstract>Corpus de Investigación en Español de México del Posgrado de Ingeniería Eléctrica y Servicio Social” (CIEMPIESS) is a new open-sourced corpus extracted from Spanish spoken FM podcasts in the dialect of the center of Mexico. The CIEMPIESS corpus was designed to be used in the field of automatic speech recongnition (ASR) and it is provided with two different kind of pronouncing dictionaries, one of them containing the phonemes of Mexican Spanish and the other containing this same phonemes plus allophones. Corpus annotation took into account the tonic vowel of every word and the four different sounds that letter “x” presents in the Spanish language. CIEMPIESS corpus is also provided with two different language models extracted from electronic newsletters, one of them takes into account the tonic vowels but not the other one. Both the dictionaries and the language models allow users to experiment different scenarios for the recognition task in order to adequate the corpus to their needs.</abstract>
    </paper>
    <paper id="190">
      <author><first>Arantza</first><last>del Pozo</last></author>
      <author><first>Carlo</first><last>Aliprandi</last></author>
      <author><first>Aitor</first><last>Álvarez</last></author>
      <author><first>Carlos</first><last>Mendes</last></author>
      <author><first>Joao P.</first><last>Neto</last></author>
      <author><first>Sérgio</first><last>Paulo</last></author>
      <author><first>Nicola</first><last>Piccinini</last></author>
      <author><first>Matteo</first><last>Raffaelli</last></author>
      <title><fixed-case>SAVAS</fixed-case>: Collecting, Annotating and Sharing Audiovisual Language Resources for Automatic Subtitling</title>
      <pages>432–436</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/183_Paper.pdf</url>
      <abstract>This paper describes the data collection, annotation and sharing activities carried out within the FP7 EU-funded SAVAS project. The project aims to collect, share and reuse audiovisual language resources from broadcasters and subtitling companies to develop large vocabulary continuous speech recognisers in specific domains and new languages, with the purpose of solving the automated subtitling needs of the media industry.</abstract>
    </paper>
    <paper id="191">
      <author><first>Peter</first><last>Fankhauser</last></author>
      <author><first>Jörg</first><last>Knappen</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <title>Exploring and Visualizing Variation in Language Resources</title>
      <pages>4125–4128</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/185_Paper.pdf</url>
      <abstract>Language resources are often compiled for the purpose of variational analysis, such as studying differences between genres, registers, and disciplines, regional and diachronic variation, influence of gender, cultural context, etc. Often the sheer number of potentially interesting contrastive pairs can get overwhelming due to the combinatorial explosion of possible combinations. In this paper, we present an approach that combines well understood techniques for visualization heatmaps and word clouds with intuitive paradigms for exploration drill down and side by side comparison to facilitate the analysis of language variation in such highly combinatorial situations. Heatmaps assist in analyzing the overall pattern of variation in a corpus, and word clouds allow for inspecting variation at the level of words.</abstract>
    </paper>
    <paper id="192">
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Wei</first><last>Gao</last></author>
      <title>Simple Effective Microblog Named Entity Recognition: <fixed-case>A</fixed-case>rabic as an Example</title>
      <pages>2513–2517</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/186_Paper.pdf</url>
      <abstract>Despite many recent papers on Arabic Named Entity Recognition (NER) in the news domain, little work has been done on microblog NER. NER on microblogs presents many complications such as informality of language, shortened named entities, brevity of expressions, and inconsistent capitalization (for cased languages). We introduce simple effective language-independent approaches for improving NER on microblogs, based on using large gazetteers, domain adaptation, and a two-pass semi-supervised method. We use Arabic as an example language to compare the relative effectiveness of the approaches and when best to use them. We also present a new dataset for the task. Results of combining the proposed approaches show an improvement of 35.3 F-measure points over a baseline system trained on news data and an improvement of 19.9 F-measure points over the same system but trained on microblog data.</abstract>
    </paper>
    <paper id="193">
      <author><first>Miguel B.</first><last>Almeida</last></author>
      <author><first>Mariana S. C.</first><last>Almeida</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <author><first>Helena</first><last>Figueira</last></author>
      <author><first>Pedro</first><last>Mendes</last></author>
      <author><first>Cláudia</first><last>Pinto</last></author>
      <title>Priberam Compressive Summarization Corpus: A New Multi-Document Summarization Corpus for <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese</title>
      <pages>146–152</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/187_Paper.pdf</url>
      <abstract>In this paper, we introduce the Priberam Compressive Summarization Corpus, a new multi-document summarization corpus for European Portuguese. The corpus follows the format of the summarization corpora for English in recent DUC and TAC conferences. It contains 80 manually chosen topics referring to events occurred between 2010 and 2013. Each topic contains 10 news stories from major Portuguese newspapers, radio and TV stations, along with two human generated summaries up to 100 words. Apart from the language, one important difference from the DUC/TAC setup is that the human summaries in our corpus are \emph{compressive}: the annotators performed only sentence and word deletion operations, as opposed to generating summaries from scratch. We use this corpus to train and evaluate learning-based extractive and compressive summarization systems, providing an empirical comparison between these two approaches. The corpus is made freely available in order to facilitate research on automatic summarization.</abstract>
    </paper>
    <paper id="194">
      <author><first>Chantal</first><last>van Son</last></author>
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Hope and Fear: How Opinions Influence Factuality</title>
      <pages>3857–3864</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/188_Paper.pdf</url>
      <abstract>Both sentiment and event factuality are fundamental information levels for our understanding of events mentioned in news texts. Most research so far has focused on either modeling opinions or factuality. In this paper, we propose a model that combines the two for the extraction and interpretation of perspectives on events. By doing so, we can explain the way people perceive changes in (their belief of) the world as a function of their fears of changes to the bad or their hopes of changes to the good. This study seeks to examine the effectiveness of this approach by applying factuality annotations, based on FactBank, on top of the MPQA Corpus, a corpus containing news texts annotated for sentiments and other private states. Our findings suggest that this approach can be valuable for the understanding of perspectives, but that there is still some work to do on the refinement of the integration.</abstract>
    </paper>
    <paper id="195">
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <title>Linking Pictographs to Synsets: <fixed-case>S</fixed-case>clera2<fixed-case>C</fixed-case>ornetto</title>
      <pages>3404–3410</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/189_Paper.pdf</url>
      <abstract>Social inclusion of people with Intellectual and Developmental Disabilities can be promoted by offering them ways to independently use the internet. People with reading or writing disabilities can use pictographs instead of text. We present a resource in which we have linked a set of 5710 pictographs to lexical-semantic concepts in Cornetto, a Wordnet-like database for Dutch. We show that, by using this resource in a text-to-pictograph translation system, we can greatly improve the coverage comparing with a baseline where words are converted into pictographs only if the word equals the filename.</abstract>
    </paper>
    <paper id="196">
      <author><first>Mohamed</first><last>Morchid</last></author>
      <author><first>Georges</first><last>Linarès</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <title>Characterizing and Predicting Bursty Events: The Buzz Case Study on <fixed-case>T</fixed-case>witter</title>
      <pages>2766–2771</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/19_Paper.pdf</url>
      <abstract>The prediction of bursty events on the Internet is a challenging task. Difficulties are due to the diversity of information sources, the size of the Internet, dynamics of popularity, user behaviors... On the other hand, Twitter is a structured and limited space. In this paper, we present a new method for predicting bursty events using content-related indices. Prediction is performed by a neural network that combines three features in order to predict the number of retweets of a tweet on the Twitter platform. The indices are related to popularity, expressivity and singularity. Popularity index is based on the analysis of RSS streams. Expressivity uses a dictionary that contains words annotated in terms of expressivity load. Singularity represents outlying topic association estimated via a Latent Dirichlet Allocation (LDA) model. Experiments demonstrate the effectiveness of the proposal with a 72% F-measure prediction score for the tweets that have been forwarded at least 60 times.</abstract>
    </paper>
    <paper id="197">
      <author><first>Hans-Ulrich</first><last>Krieger</last></author>
      <author><first>Christian</first><last>Spurk</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Frank</first><last>Müller</last></author>
      <author><first>Thomas</first><last>Tolxdorff</last></author>
      <title>Information Extraction from <fixed-case>G</fixed-case>erman Patient Records via Hybrid Parsing and Relation Extraction Strategies</title>
      <pages>2043–2048</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/190_Paper.pdf</url>
      <abstract>In this paper, we report on first attempts and findings to analyzing German patient records, using a hybrid parsing architecture and a combination of two relation extraction strategies. On a practical level, we are interested in the extraction of concepts and relations among those concepts, a necessary cornerstone for building medical information systems. The parsing pipeline consists of a morphological analyzer, a robust chunk parser adapted to Latin phrases used in medical diagnosis, a repair rule stage, and a probabilistic context-free parser that respects the output from the chunker. The relation extraction stage is a combination of two systems: SProUT, a shallow processor which uses hand-written rules to discover relation instances from local text units and DARE which extracts relation instances from complete sentences, using rules that are learned in a bootstrapping process, starting with semantic seeds. Two small experiments have been carried out for the parsing pipeline and the relation extraction stage.</abstract>
    </paper>
    <paper id="198">
      <author><first>Dietmar</first><last>Schabus</last></author>
      <author><first>Michael</first><last>Pucher</last></author>
      <author><first>Phil</first><last>Hoole</last></author>
      <title>The <fixed-case>MMASCS</fixed-case> multi-modal annotated synchronous corpus of audio, video, facial motion and tongue motion data of normal, fast and slow speech</title>
      <pages>3411–3416</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/192_Paper.pdf</url>
      <abstract>In this paper, we describe and analyze a corpus of speech data that we have recorded in multiple modalities simultaneously: facial motion via optical motion capturing, tongue motion via electro-magnetic articulography, as well as conventional video and high-quality audio. The corpus consists of 320 phonetically diverse sentences uttered by a male Austrian German speaker at normal, fast and slow speaking rate. We analyze the influence of speaking rate on phone durations and on tongue motion. Furthermore, we investigate the correlation between tongue and facial motion. The data corpus is available free of charge for research use, including phonetic annotations and a playback software which visualizes the 3D data, from the website http://cordelia.ftw.at/mmascs</abstract>
    </paper>
    <paper id="199">
      <author><first>Lucie</first><last>Poláková</last></author>
      <author><first>Pavlína</first><last>Jínová</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <title>Genres in the <fixed-case>P</fixed-case>rague Discourse Treebank</title>
      <pages>1320–1326</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/195_Paper.pdf</url>
      <abstract>We present the project of classification of Prague Discourse Treebank documents (Czech journalistic texts) for their genres. Our main interest lies in opening the possibility to observe how text coherence is realized in different types (in the genre sense) of language data and, in the future, in exploring the ways of using genres as a feature for multi-sentence-level language technologies. In the paper, we first describe the motivation and the concept of the genre annotation, and briefly introduce the Prague Discourse Treebank. Then, we elaborate on the process of manual annotation of genres in the treebank, from the annotators’ manual work to post-annotation checks and to the inter-annotator agreement measurements. The annotated genres are subsequently analyzed together with discourse relations (already annotated in the treebank) ― we present distributions of the annotated genres and results of studying distinctions of distributions of discourse relations across the individual genres.</abstract>
    </paper>
    <paper id="200">
      <author><first>Joris</first><last>Pelemans</last></author>
      <author><first>Kris</first><last>Demuynck</last></author>
      <author><first>Hugo</first><last>Van hamme</last></author>
      <author><first>Patrick</first><last>Wambacq</last></author>
      <title>Speech Recognition Web Services for <fixed-case>D</fixed-case>utch</title>
      <pages>3041–3044</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/196_Paper.pdf</url>
      <abstract>In this paper we present 3 applications in the domain of Automatic Speech Recognition for Dutch, all of which are developed using our in-house speech recognition toolkit SPRAAK. The speech-to-text transcriber is a large vocabulary continuous speech recognizer, optimized for Southern Dutch. It is capable to select components and adjust parameters on the fly, based on the observed conditions in the audio and was recently extended with the capability of adding new words to the lexicon. The grapheme-to-phoneme converter generates possible pronunciations for Dutch words, based on lexicon lookup and linguistic rules. The speech-text alignment system takes audio and text as input and constructs a time aligned output where every word receives exact begin and end times. All three of the applications (and others) are freely available, after registration, as a web application on http://www.spraak.org/webservice/ and in addition, can be accessed as a web service in automated tools.</abstract>
    </paper>
    <paper id="201">
      <author><first>Angela</first><last>Costa</last></author>
      <author><first>Tiago</first><last>Luís</last></author>
      <author><first>Luísa</first><last>Coheur</last></author>
      <title>Translation errors from <fixed-case>E</fixed-case>nglish to <fixed-case>P</fixed-case>ortuguese: an annotated corpus</title>
      <pages>1231–1234</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/199_Paper.pdf</url>
      <abstract>Analysing the translation errors is a task that can help us finding and describing translation problems in greater detail, but can also suggest where the automatic engines should be improved. Having these aims in mind we have created a corpus composed of 150 sentences, 50 from the TAP magazine, 50 from a TED talk and the other 50 from the from the TREC collection of factoid questions. We have automatically translated these sentences from English into Portuguese using Google Translate and Moses. After we have analysed the errors and created the error annotation taxonomy, the corpus was annotated by a linguist native speaker of Portuguese. Although Google’s overall performance was better in the translation task (we have also calculated the BLUE and NIST scores), there are some error types that Moses was better at coping with, specially discourse level errors.</abstract>
    </paper>
    <paper id="202">
      <author><first>Fadoua Ataa</first><last>Allah</last></author>
      <author><first>Siham</first><last>Boulaknadel</last></author>
      <title><fixed-case>A</fixed-case>mazigh Verb Conjugator</title>
      <pages>1051–1055</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/2_Paper.pdf</url>
      <abstract>With the aim of preserving the Amazigh heritage from being threatened with disappearance, it seems suitable to provide Amazigh with required resources to confront the stakes of access to the domain of New Information and Communication Technologies (ICT). In this context and in the perspective to build linguistic resources and natural language processing tools for this language, we have undertaken to develop an online conjugating tool that generates the inflectional forms of the Amazigh verbs. This tool is based on novel linguistically motivated morphological rules describing the verbal paradigm for all the Moroccan Amazigh varieties. Furthermore, it is based on the notion of morphological tree structure and uses transformational rules which are attached to the leaf nodes. Each rule may have numerous mutually exclusive clauses, where each part of a clause is a regular expression pattern that is matched against the radical pattern. This tool is an interactive conjugator that provides exhaustive coverage of linguistically accurate conjugation paradigms for over 3584 Armazigh verbs. It has been made simple and easy to use and designed from the ground up to be a highly effective learning aid that stimulates a desire to learn.</abstract>
    </paper>
    <paper id="203">
      <author><first>Pawel</first><last>Kamocki</last></author>
      <title>The liability of service providers in e-Research Infrastructures: killing the messenger?</title>
      <pages>4220–4224</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/202_Paper.pdf</url>
      <abstract>Hosting Providers play an essential role in the development of Internet services such as e-Research Infrastructures. In order to promote the development of such services, legislators on both sides of the Atlantic Ocean introduced safe harbour provisions to protect Service Providers (a category which includes Hosting Providers) from legal claims (e.g. of copyright infringement). Relevant provisions can be found in Â§ 512 of the United States Copyright Act and in art. 14 of the Directive 2000/31/EC (and its national implementations). The cornerstone of this framework is the passive role of the Hosting Provider through which he has no knowledge of the content that he hosts. With the arrival of Web 2.0, however, the role of Hosting Providers on the Internet changed; this change has been reflected in court decisions that have reached varying conclusions in the last few years. The purpose of this article is to present the existing framework (including recent case law from the US, Germany and France).</abstract>
    </paper>
    <paper id="204">
      <author><first>Quentin</first><last>Pradet</last></author>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <title>Adapting <fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et to <fixed-case>F</fixed-case>rench using existing resources</title>
      <pages>1122–1126</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/203_Paper.pdf</url>
      <abstract>VerbNet is an English lexical resource for verbs that has proven useful for English NLP due to its high coverage and coherent classification. Such a resource doesnt exist for other languages, despite some (mostly automatic and unsupervised) attempts. We show how to semi-automatically adapt VerbNet using existing resources designed for diï¬erent purposes. This study focuses on French and uses two French resources: a semantic lexicon (Les Verbes Français) and a syntactic lexicon (Lexique-Grammaire).</abstract>
    </paper>
    <paper id="205">
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Thomas</first><last>Meyer</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>F</fixed-case>rench Verb Phrase Alignment in <fixed-case>E</fixed-case>uroparl for Tense Translation Modeling</title>
      <pages>674–681</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/205_Paper.pdf</url>
      <abstract>This paper presents a method for verb phrase (VP) alignment in an English-French parallel corpus and its use for improving statistical machine translation (SMT) of verb tenses. The method starts from automatic word alignment performed with GIZA++, and relies on a POS tagger and a parser, in combination with several heuristics, in order to identify non-contiguous components of VPs, and to label the aligned VPs with their tense and voice on each side. This procedure is applied to the Europarl corpus, leading to the creation of a smaller, high-precision parallel corpus with about 320,000 pairs of finite VPs, which is made publicly available. This resource is used to train a tense predictor for translation from English into French, based on a large number of surface features. Three MT systems are compared: (1) a baseline phrase-based SMT; (2) a tense-aware SMT system using the above predictions within a factored translation model; and (3) a system using oracle predictions from the aligned VPs. For several tenses, such as the French “imparfait”, the tense-aware SMT system improves significantly over the baseline and is closer to the oracle system.</abstract>
    </paper>
    <paper id="206">
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <title>The evolving infrastructure for language resources and the role for data scientists</title>
      <pages>608–612</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/206_Paper.pdf</url>
      <abstract>In the context of ongoing developments as regards the creation of a sustainable, interoperable language resource infrastructure and spreading ideas of the need for open access, not only of research publications but also of the underlying data, various issues present themselves which require that different stakeholders reconsider their positions. In the present paper we relate the experiences from the CLARIN-NL data curation service (DCS) over the two years that it has been operational, and the future role we envisage for expertise centres like the DCS in the evolving infrastructure.</abstract>
    </paper>
    <paper id="207">
      <author><first>Attila</first><last>Novák</last></author>
      <title>A New Form of Humor — Mapping Constraint-Based Computational Morphologies to a Finite-State Representation</title>
      <pages>1068–1073</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/207_Paper.pdf</url>
      <abstract>MorphoLogic’s Humor morphological analyzer engine has been used for the development of several high-quality computational morphologies, among them ones for complex agglutinative languages. However, Humor’s closed source licensing scheme has been an obstacle to making these resources widely available. Moreover, there are other limitations of the rule-based Humor engine: lack of support for morphological guessing and for the integration of frequency information or other weighting of the models. These problems were solved by converting the databases to a finite-state representation that allows for morphological guessing and the addition of weights. Moreover, it has open-source implementations.</abstract>
    </paper>
    <paper id="208">
      <author><first>Matti</first><last>Karppa</last></author>
      <author><first>Ville</first><last>Viitaniemi</last></author>
      <author><first>Marcos</first><last>Luzardo</last></author>
      <author><first>Jorma</first><last>Laaksonen</last></author>
      <author><first>Tommi</first><last>Jantunen</last></author>
      <title><fixed-case>SLM</fixed-case>otion - An extensible sign language oriented video analysis tool</title>
      <pages>1886–1891</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/209_Paper.pdf</url>
      <abstract>We present a software toolkit called SLMotion which provides a framework for automatic and semiautomatic analysis, feature extraction and annotation of individual sign language videos, and which can easily be adapted to batch processing of entire sign language corpora. The program follows a modular design, and exposes a Numpy-compatible Python application programming interface that makes it easy and convenient to extend its functionality through scripting. The program includes support for exporting the annotations in ELAN format. The program is released as free software, and is available for GNU/Linux and MacOS platforms.</abstract>
    </paper>
    <paper id="209">
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>Constructing a <fixed-case>C</fixed-case>hinese—<fixed-case>J</fixed-case>apanese Parallel Corpus from <fixed-case>W</fixed-case>ikipedia</title>
      <pages>642–647</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/21_Paper.pdf</url>
      <abstract>Parallel corpora are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese―Japanese. As comparable corpora are far more available, many studies have been conducted to automatically construct parallel corpora from comparable corpora. This paper presents a robust parallel sentence extraction system for constructing a Chinese―Japanese parallel corpus from Wikipedia. The system is inspired by previous studies that mainly consist of a parallel sentence candidate filter and a binary classifier for parallel sentence identification. We improve the system by using the common Chinese characters for filtering and two novel feature sets for classification. Experiments show that our system performs significantly better than the previous studies for both accuracy in parallel sentence extraction and SMT performance. Using the system, we construct a Chinese―Japanese parallel corpus with more than 126k highly accurate parallel sentences from Wikipedia. The constructed parallel corpus is freely available at http://orchid.kuee.kyoto-u.ac.jp/~chu/resource/wiki_zh_ja.tgz.</abstract>
    </paper>
    <paper id="210">
      <author><first>Michael</first><last>Carl</last></author>
      <author><first>Mercedes Martínez</first><last>García</last></author>
      <author><first>Bartolomé</first><last>Mesa-Lao</last></author>
      <title><fixed-case>CFT</fixed-case>13: A resource for research into the post-editing process</title>
      <pages>1757–1764</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/210_Paper.pdf</url>
      <abstract>This paper describes the most recent dataset that has been added to the CRITT Translation Process Research Database (TPR-DB). Under the name CFT13, this new study contains user activity data (UAD) in the form of key-logging and eye-tracking collected during the second CasMaCat field trial in June 2013. The CFT13 is a publicly available resource featuring a number of simple and compound process and product units suited to investigate human-computer interaction while post-editing machine translation outputs.</abstract>
    </paper>
    <paper id="211">
      <author><first>Ludger</first><last>Zeevaert</last></author>
      <title>Mörkum Njálu. An annotated corpus to analyse and explain grammatical divergences between 14th-century manuscripts of Njál’s saga.</title>
      <pages>981–987</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/211_Paper.pdf</url>
      <abstract>The work of the research project Variance of Njáls saga at the Árni Magnússon Institute for Icelandic Studies in Reykjavík relies mainly on an annotated XML-corpus of manuscripts of Brennu-Njáls saga or The Story of Burnt Njál, an Icelandic prose narrative from the end of the 13th century. One part of the project is devoted to linguistic variation in the earliest transmission of the text in parchment manuscripts and fragments from the 14th century. The article gives a short overview over the design of the corpus that has to serve quite different purposes from palaeographic over stemmatological to literary research. It focuses on features important for the analysis of certain linguistic variables and the challenge lying in their implementation in a corpus consisting of close transcriptions of medieval manuscripts and gives examples for the use of the corpus for linguistic research in the frame of the project that mainly consists of the analysis of different grammatical/syntactic constructions that are often referred to in connection with stylistic research (narrative inversion, historical present tense, indirect-speech constructions).</abstract>
    </paper>
    <paper id="212">
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Adrian</first><last>Leeman</last></author>
      <author><first>Marie-José</first><last>Kolly</last></author>
      <author><first>Ingrid</first><last>Hove</last></author>
      <author><first>Ibrahim</first><last>Almajai</last></author>
      <author><first>Volker</first><last>Dellwo</last></author>
      <author><first>Steven</first><last>Moran</last></author>
      <title>A Crowdsourcing Smartphone Application for <fixed-case>S</fixed-case>wiss <fixed-case>G</fixed-case>erman: Putting Language Documentation in the Hands of the Users</title>
      <pages>3444–3447</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/214_Paper.pdf</url>
      <abstract>This contribution describes an on-going projects a smartphone application called Voice Ãpp, which is a follow-up of a previous application called Dialäkt Ãpp. The main purpose of both apps is to identify the users Swiss German dialect on the basis of the dialectal variations of 15 words. The result is returned as one or more geographical points on a map. In Dialäkt Ãpp, launched in 2013, the user provides his or her own pronunciation through buttons, while the Voice Ãpp, currently in development, asks users to pronounce the word and uses speech recognition techniques to identify the variants and localize the user. This second app is more challenging from a technical point of view but nevertheless recovers the nature of dialect variation of spoken language. Besides, the Voice Ãpp takes its users on a journey in which they explore the individuality of their own voices, answering questions such as: How high is my voice? How fast do I speak? Do I speak faster than users in the neighbouring city?</abstract>
    </paper>
    <paper id="213">
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Philip</first><last>Ogren</last></author>
      <author><first>Lee</first><last>Becker</last></author>
      <title><fixed-case>C</fixed-case>lear<fixed-case>TK</fixed-case> 2.0: Design Patterns for Machine Learning in <fixed-case>UIMA</fixed-case></title>
      <pages>3289–3293</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/218_Paper.pdf</url>
      <abstract>ClearTK adds machine learning functionality to the UIMA framework, providing wrappers to popular machine learning libraries, a rich feature extraction library that works across different classifiers, and utilities for applying and evaluating machine learning models. Since its inception in 2008, ClearTK has evolved in response to feedback from developers and the community. This evolution has followed a number of important design principles including: conceptually simple annotator interfaces, readable pipeline descriptions, minimal collection readers, type system agnostic code, modules organized for ease of import, and assisting user comprehension of the complex UIMA framework.</abstract>
    </paper>
    <paper id="214">
      <author><first>Inès</first><last>Zribi</last></author>
      <author><first>Rahma</first><last>Boujelbane</last></author>
      <author><first>Abir</first><last>Masmoudi</last></author>
      <author><first>Mariem</first><last>Ellouze</last></author>
      <author><first>Lamia</first><last>Belguith</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <title>A Conventional Orthography for <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic</title>
      <pages>2355–2361</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/219_Paper.pdf</url>
      <abstract>Tunisian Arabic is a dialect of the Arabic language spoken in Tunisia. Tunisian Arabic is an under-resourced language. It has neither a standard orthography nor large collections of written text and dictionaries. Actually, there is no strict separation between Modern Standard Arabic, the official language of the government, media and education, and Tunisian Arabic; the two exist on a continuum dominated by mixed forms. In this paper, we present a conventional orthography for Tunisian Arabic, following a previous effort on developing a conventional orthography for Dialectal Arabic (or CODA) demonstrated for Egyptian Arabic. We explain the design principles of CODA and provide a detailed description of its guidelines as applied to Tunisian Arabic.</abstract>
    </paper>
    <paper id="215">
      <author><first>Thomas</first><last>Mayer</last></author>
      <author><first>Michael</first><last>Cysouw</last></author>
      <title>Creating a massively parallel <fixed-case>B</fixed-case>ible corpus</title>
      <pages>3158–3163</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/220_Paper.pdf</url>
      <abstract>We present our ongoing effort to create a massively parallel Bible corpus. While an ever-increasing number of Bible translations is available in electronic form on the internet, there is no large-scale parallel Bible corpus that allows language researchers to easily get access to the texts and their parallel structure for a large variety of different languages. We report on the current status of the corpus, with over 900 translations in more than 830 language varieties. All translations are tokenized (e.g., separating punctuation marks) and Unicode normalized. Mainly due to copyright restrictions only portions of the texts are made publicly available. However, we provide co-occurrence information for each translation in a (sparse) matrix format. All word forms in the translation are given together with their frequency and the verses in which they occur.</abstract>
    </paper>
    <paper id="216">
      <author><first>Reinhard</first><last>Rapp</last></author>
      <title>Corpus-Based Computation of Reverse Associations</title>
      <pages>1380–1386</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/221_Paper.pdf</url>
      <abstract>According to psychological learning theory an important principle governing language acquisition is co-occurrence. For example, when we perceive language, our brain seems to unconsciously analyze and store the co-occurrence patterns of the words. And during language production, these co-occurrence patterns are reproduced. The applicability of this principle is particularly obvious in the case of word associations. There is evidence that the associative responses people typically come up with upon presentation of a stimulus word are often words which frequently co-occur with it. It is thus possible to predict a response by looking at co-occurrence data. The work presented here is along these lines. However, it differs from most previous work in that it investigates the direction from the response to the stimulus rather than vice-versa, and that it also deals with the case when several responses are known. Our results indicate that it is possible to predict a stimulus word from its responses, and that it helps if several responses are given.</abstract>
    </paper>
    <paper id="217">
      <author><first>Palmira</first><last>Marrafa</last></author>
      <author><first>Raquel</first><last>Amaro</last></author>
      <author><first>Sara</first><last>Mendes</last></author>
      <title><fixed-case>L</fixed-case>ex<fixed-case>T</fixed-case>ec — a rich language resource for technical domains in <fixed-case>P</fixed-case>ortuguese</title>
      <pages>1044–1050</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/222_Paper.pdf</url>
      <abstract>The growing amount of available information and the importance given to the access to technical information enhance the potential role of NLP applications in enabling users to deal with information for a variety of knowledge domains. In this process, language resources are crucial. This paper presents Lextec, a rich computational language resource for technical vocabulary in Portuguese. Encoding a representative set of terms for ten different technical domains, this concept-based relational language resource combines a wide range of linguistic information by integrating each entry in a domain-specific wordnet and associating it with a precise definition for each lexicalization in the technical domain at stake, illustrative texts and information for translation into English.</abstract>
    </paper>
    <paper id="218">
      <author><first>Blanca</first><last>Arias</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Mercè</first><last>Lorente</last></author>
      <author><first>Montserrat</first><last>Marimón</last></author>
      <author><first>Alba</first><last>Milà</last></author>
      <author><first>Jorge</first><last>Vivaldi</last></author>
      <author><first>Muntsa</first><last>Padró</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Imanol</first><last>Larrea</last></author>
      <title>Boosting the creation of a treebank</title>
      <pages>775–781</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/225_Paper.pdf</url>
      <abstract>In this paper we present the results of an ongoing experiment of bootstrapping a Treebank for Catalan by using a Dependency Parser trained with Spanish sentences. In order to save time and cost, our approach was to profit from the typological similarities between Catalan and Spanish to create a first Catalan data set quickly by automatically: (i) annotating with a de-lexicalized Spanish parser, (ii) manually correcting the parses, and (iii) using the Catalan corrected sentences to train a Catalan parser. The results showed that the number of parsed sentences required to train a Catalan parser is about 1000 that were achieved in 4 months, with 2 annotators.</abstract>
    </paper>
    <paper id="219">
      <author><first>Eric</first><last>Sanders</last></author>
      <author><first>Ineke</first><last>van de Craats</last></author>
      <author><first>Vanja</first><last>de Lint</last></author>
      <title>The <fixed-case>D</fixed-case>utch <fixed-case>LESLLA</fixed-case> Corpus</title>
      <pages>2715–2718</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/227_Paper.pdf</url>
      <abstract>This paper describes the Dutch LESLLA data and its curation. LESLLA stands for Low-Educated Second Language and Literacy Acquisition. The data was collected for research in this field and would have been disappeared if it were not saved. Within the CLARIN project Data Curation Service the data was made into a spoken language resource and made available to other researchers.</abstract>
    </paper>
    <paper id="220">
      <author><first>Tomáš</first><last>Jelínek</last></author>
      <title>Improvements to Dependency Parsing Using Automatic Simplification of Data</title>
      <pages>73–77</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/228_Paper.pdf</url>
      <abstract>In dependency parsing, much effort is devoted to the development of new methods of language modeling and better feature settings. Less attention is paid to actual linguistic data and how appropriate they are for automatic parsing: linguistic data can be too complex for a given parser, morphological tags may not reflect well syntactic properties of words, a detailed, complex annotation scheme may be ill suited for automatic parsing. In this paper, I present a study of this problem on the following case: automatic dependency parsing using the data of the Prague Dependency Treebank with two dependency parsers: MSTParser and MaltParser. I show that by means of small, reversible simplifications of the text and of the annotation, a considerable improvement of parsing accuracy can be achieved. In order to facilitate the task of language modeling performed by the parser, I reduce variability of lemmas and forms in the text. I modify the system of morphological annotation to adapt it better for parsing. Finally, the dependency annotation scheme is also partially modified. All such modifications are automatic and fully reversible: after the parsing is done, the original data and structures are automatically restored. With MaltParser, I achieve an 8.3% error rate reduction.</abstract>
    </paper>
    <paper id="221">
      <author><first>Claudiu</first><last>Mihăilă</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <title>The Meta-knowledge of Causality in Biomedical Scientific Discourse</title>
      <pages>1984–1991</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/23_Paper.pdf</url>
      <abstract>Causality lies at the heart of biomedical knowledge, being involved in diagnosis, pathology or systems biology. Thus, automatic causality recognition can greatly reduce the human workload by suggesting possible causal connections and aiding in the curation of pathway models. For this, we rely on corpora that are annotated with classified, structured representations of important facts and findings contained within text. However, it is impossible to correctly interpret these annotations without additional information, e.g., classification of an event as fact, hypothesis, experimental result or analysis of results, confidence of authors about the validity of their analyses etc. In this study, we analyse and automatically detect this type of information, collectively termed meta-knowledge (MK), in the context of existing discourse causality annotations. Our effort proves the feasibility of identifying such pieces of information, without which the understanding of causal relations is limited.</abstract>
    </paper>
    <paper id="222">
      <author><first>Wolfgang</first><last>Maier</last></author>
      <author><first>Miriam</first><last>Kaeshammer</last></author>
      <author><first>Peter</first><last>Baumann</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <title>Discosuite - A parser test suite for <fixed-case>G</fixed-case>erman discontinuous structures</title>
      <pages>2905–2912</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/230_Paper.pdf</url>
      <abstract>Parser evaluation traditionally relies on evaluation metrics which deliver a single aggregate score over all sentences in the parser output, such as PARSEVAL. However, for the evaluation of parser performance concerning a particular phenomenon, a test suite of sentences is needed in which this phenomenon has been identified. In recent years, the parsing of discontinuous structures has received a rising interest. Therefore, in this paper, we present a test suite for testing the performance of dependency and constituency parsers on non-projective dependencies and discontinuous constituents for German. The test suite is based on the newly released TIGER treebank version 2.2. It provides a unique possibility of benchmarking parsers on non-local syntactic relationships in German, for constituents and dependencies. We include a linguistic analysis of the phenomena that cause discontinuity in the TIGER annotation, thereby closing gaps in previous literature. The linguistic phenomena we investigate include extraposition, a placeholder/repeated element construction, topicalization, scrambling, local movement, parentheticals, and fronting of pronouns.</abstract>
    </paper>
    <paper id="223">
      <author><first>Francesco</first><last>Barbieri</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <title>Modelling Irony in <fixed-case>T</fixed-case>witter: Feature Analysis and Evaluation</title>
      <pages>4258–4264</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/231_Paper.pdf</url>
      <abstract>Irony, a creative use of language, has received scarce attention from the computational linguistics research point of view. We propose an automatic system capable of detecting irony with good accuracy in the social network Twitter. Twitter allows users to post short messages (140 characters) which usually do not follow the expected rules of the grammar, users tend to truncate words and use particular punctuation. For these reason automatic detection of Irony in Twitter is not trivial and requires specific linguistic tools. We propose in this paper a new set of experiments to assess the relevance of the features included in our model. Our model does not include words or sequences of words as features, aiming to detect inner characteristic of Irony.</abstract>
    </paper>
    <paper id="224">
      <author><first>Gregor</first><last>Titze</last></author>
      <author><first>Volha</first><last>Bryl</last></author>
      <author><first>Cäcilia</first><last>Zirn</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <title><fixed-case>DB</fixed-case>pedia Domains: augmenting <fixed-case>DB</fixed-case>pedia with domain information</title>
      <pages>1438–1442</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/233_Paper.pdf</url>
      <abstract>We present an approach for augmenting DBpedia, a very large ontology lying at the heart of the Linked Open Data (LOD) cloud, with domain information. Our approach uses the thematic labels provided for DBpedia entities by Wikipedia categories, and groups them based on a kernel based k-means clustering algorithm. Experiments on gold-standard data show that our approach provides a first solution to the automatic annotation of DBpedia entities with domain labels, thus providing the largest LOD domain-annotated ontology to date.</abstract>
    </paper>
    <paper id="225">
      <author><first>Mathieu</first><last>Chollet</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <author><first>Catherine</first><last>Pelachaud</last></author>
      <title>Mining a multimodal corpus for non-verbal behavior sequences conveying attitudes</title>
      <pages>3417–3424</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/235_Paper.pdf</url>
      <abstract>Interpersonal attitudes are expressed by non-verbal behaviors on a variety of different modalities. The perception of these behaviors is influenced by how they are sequenced with other behaviors from the same person and behaviors from other interactants. In this paper, we present a method for extracting and generating sequences of non-verbal signals expressing interpersonal attitudes. These sequences are used as part of a framework for non-verbal expression with Embodied Conversational Agents that considers different features of non-verbal behavior: global behavior tendencies, interpersonal reactions, sequencing of non-verbal signals, and communicative intentions. Our method uses a sequence mining technique on an annotated multimodal corpus to extract sequences characteristic of different attitudes. New sequences of non-verbal signals are generated using a probabilistic model, and evaluated using the previously mined sequences.</abstract>
    </paper>
    <paper id="226">
      <author><first>Cyril</first><last>Grouin</last></author>
      <title>Biomedical entity extraction using machine-learning based approaches</title>
      <pages>2518–2523</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/236_Paper.pdf</url>
      <abstract>In this paper, we present the experiments we made to process entities from the biomedical domain. Depending on the task to process, we used two distinct supervised machine-learning techniques: Conditional Random Fields to perform both named entity identification and classification, and Maximum Entropy to classify given entities. Machine-learning approaches outperformed knowledge-based techniques on categories where sufficient annotated data was available. We showed that the use of external features (unsupervised clusters, information from ontology and taxonomy) improved the results significantly.</abstract>
    </paper>
    <paper id="227">
      <author><first>Achim</first><last>Stein</last></author>
      <title>Parsing Heterogeneous Corpora with a Rich Dependency Grammar</title>
      <pages>2879–2886</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/239_Paper.pdf</url>
      <abstract>Grammar models conceived for parsing purposes are often poorer than models that are motivated linguistically. We present a grammar model which is linguistically satisfactory and based on the principles of traditional dependency grammar. We show how a state-of-the-art dependency parser (mate tools) performs with this model, trained on the Syntactic Reference Corpus of Medieval French (SRCMF), a manually annotated corpus of medieval (Old French) texts. We focus on the problems caused by small and heterogeneous training sets typical for corpora of older periods. The result is the first publicly available dependency parser for Old French. On a 90/10 training/evaluation split of eleven OF texts (206000 words), we obtained an UAS of 89.68% and a LAS of 82.62%. Three experiments showed how heterogeneity, typical of medieval corpora, affects the parsing results: (a) a ‘one-on-one’ cross evaluation for individual texts, (b) a ‘leave-one-out’ cross evaluation, and (c) a prose/verse cross evaluation.</abstract>
    </paper>
    <paper id="228">
      <author><first>Eckhard</first><last>Bick</last></author>
      <title><fixed-case>ML</fixed-case>-Optimization of Ported Constraint Grammars</title>
      <pages>4483–4487</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/24_Paper.pdf</url>
      <abstract>In this paper, we describe how a Constraint Grammar with linguist-written rules can be optimized and ported to another language using a Machine Learning technique. The effects of rule movements, sorting, grammar-sectioning and systematic rule modifications are discussed and quantitatively evaluated. Statistical information is used to provide a baseline and to enhance the core of manual rules. The best-performing parameter combinations achieved part-of-speech F-scores of over 92 for a grammar ported from English to Danish, a considerable advance over both the statistical baseline (85.7), and the raw ported grammar (86.1). When the same technique was applied to an existing native Danish CG, error reduction was 10% (F=96.94).</abstract>
    </paper>
    <paper id="229">
      <author><first>Samira</first><last>Shaikh</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>George Aaron</first><last>Broadwell</last></author>
      <author><first>Boris</first><last>Yamrom</last></author>
      <author><first>Sarah</first><last>Taylor</last></author>
      <author><first>Laurie</first><last>Feldman</last></author>
      <author><first>Kit</first><last>Cho</last></author>
      <author><first>Umit</first><last>Boz</last></author>
      <author><first>Ignacio</first><last>Cases</last></author>
      <author><first>Yuliya</first><last>Peshkova</last></author>
      <author><first>Ching-Sheng</first><last>Lin</last></author>
      <title>A Multi-Cultural Repository of Automatically Discovered Linguistic and Conceptual Metaphors</title>
      <pages>2495–2500</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/241_Paper.pdf</url>
      <abstract>In this article, we present details about our ongoing work towards building a repository of Linguistic and Conceptual Metaphors. This resource is being developed as part of our research effort into the large-scale detection of metaphors from unrestricted text. We have stored a large amount of automatically extracted metaphors in American English, Mexican Spanish, Russian and Iranian Farsi in a relational database, along with pertinent metadata associated with these metaphors. A substantial subset of the contents of our repository has been systematically validated via rigorous social science experiments. Using information stored in the repository, we are able to posit certain claims in a cross-cultural context about how peoples in these cultures (America, Mexico, Russia and Iran) view particular concepts related to Governance and Economic Inequality through the use of metaphor. Researchers in the field can use this resource as a reference of typical metaphors used across these cultures. In addition, it can be used to recognize metaphors of the same form or pattern, in other domains of research.</abstract>
    </paper>
    <paper id="230">
      <author><first>Haritz</first><last>Salaberri</last></author>
      <author><first>Olatz</first><last>Arregi</last></author>
      <author><first>Beñat</first><last>Zapirain</last></author>
      <title>First approach toward Semantic Role Labeling for <fixed-case>B</fixed-case>asque</title>
      <pages>1387–1393</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/242_Paper.pdf</url>
      <abstract>In this paper, we present the first Semantic Role Labeling system developed for Basque. The system is implemented using machine learning techniques and trained with the Reference Corpus for the Processing of Basque (EPEC). In our experiments the classifier that offers the best results is based on Support Vector Machines. Our system achieves 84.30 F1 score in identifying the PropBank semantic role for a given constituent and 82.90 F1 score in identifying the VerbNet role. Our study establishes a baseline for Basque SRL. Although there are no directly comparable systems for English we can state that the results we have achieved are quite good. In addition, we have performed a Leave-One-Out feature selection procedure in order to establish which features are the worthiest regarding argument classification. This will help smooth the way for future stages of Basque SRL and will help draw some of the guidelines of our research.</abstract>
    </paper>
    <paper id="231">
      <author><first>Lianet Sepúlveda</first><last>Torres</last></author>
      <author><first>Magali Sanches</first><last>Duran</last></author>
      <author><first>Sandra</first><last>Aluísio</last></author>
      <title>Generating a Lexicon of Errors in <fixed-case>P</fixed-case>ortuguese to Support an Error Identification System for <fixed-case>S</fixed-case>panish Native Learners</title>
      <pages>3952–3957</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/247_Paper.pdf</url>
      <abstract>Portuguese is a less resourced language in what concerns foreign language learning. Aiming to inform a module of a system designed to support scientific written production of Spanish native speakers learning Portuguese, we developed an approach to automatically generate a lexicon of wrong words, reproducing language transfer errors made by such foreign learners. Each item of the artificially generated lexicon contains, besides the wrong word, the respective Spanish and Portuguese correct words. The wrong word is used to identify the interlanguage error and the correct Spanish and Portuguese forms are used to generate the suggestions. Keeping control of the correct word forms, we can provide correction or, at least, useful suggestions for the learners. We propose to combine two automatic procedures to obtain the error correction: i) a similarity measure and ii) a translation algorithm based on aligned parallel corpus. The similarity-based method achieved a precision of 52%, whereas the alignment-based method achieved a precision of 90%. In this paper we focus only on interlanguage errors involving suffixes that have different forms in both languages. The approach, however, is very promising to tackle other types of errors, such as gender errors.</abstract>
    </paper>
    <paper id="232">
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Michael</first><last>Färber</last></author>
      <author><first>Achim</first><last>Rettinger</last></author>
      <title>x<fixed-case>L</fixed-case>i<fixed-case>D</fixed-case>-Lexica: Cross-lingual Linked Data Lexica</title>
      <pages>2101–2105</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/248_Paper.pdf</url>
      <abstract>In this paper, we introduce our cross-lingual linked data lexica, called xLiD-Lexica, which are constructed by exploiting the multilingual Wikipedia and linked data resources from Linked Open Data (LOD). We provide the cross-lingual groundings of linked data resources from LOD as RDF data, which can be easily integrated into the LOD data sources. In addition, we build a SPARQL endpoint over our xLiD-Lexica to allow users to easily access them using SPARQL query language. Multilingual and cross-lingual information access can be facilitated by the availability of such lexica, e.g., allowing for an easy mapping of natural language expressions in different languages to linked data resources from LOD. Many tasks in natural language processing, such as natural language generation, cross-lingual entity linking, text annotation and question answering, can benefit from our xLiD-Lexica.</abstract>
    </paper>
    <paper id="233">
      <author><first>Yuan</first><last>Luo</last></author>
      <author><first>Thomas</first><last>Boucher</last></author>
      <author><first>Tolga</first><last>Oral</last></author>
      <author><first>David</first><last>Osofsky</last></author>
      <author><first>Sara</first><last>Weber</last></author>
      <title>A Study on Expert Sourcing Enterprise Question Collection and Classification</title>
      <pages>181–188</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/25_Paper.pdf</url>
      <abstract>Large enterprises, such as IBM, accumulate petabytes of free-text data within their organizations. To mine this big data, a critical ability is to enable meaningful question answering beyond keywords search. In this paper, we present a study on the characteristics and classification of IBM sales questions. The characteristics are analyzed both semantically and syntactically, from where a question classification guideline evolves. We adopted an enterprise level expert sourcing approach to gather questions, annotate questions based on the guideline and manage the quality of annotations via enhanced inter-annotator agreement analysis. We developed a question feature extraction system and experimented with rule-based, statistical and hybrid question classifiers. We share our annotated corpus of questions and report our experimental results. Statistical classifiers separately based on n-grams and hand-crafted rule features give reasonable macro-f1 scores at 61.7% and 63.1% respectively. Rule based classifier gives a macro-f1 at 77.1%. The hybrid classifier with n-gram and rule features using a second guess model further improves the macro-f1 to 83.9%.</abstract>
    </paper>
    <paper id="234">
      <author><first>Hong</first><last>Li</last></author>
      <author><first>Sebastian</first><last>Krause</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Robert</first><last>Hummel</last></author>
      <author><first>Veselina</first><last>Mironova</last></author>
      <title>Annotating Relation Mentions in Tabloid Press</title>
      <pages>3253–3257</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/250_Paper.pdf</url>
      <abstract>This paper presents a new resource for the training and evaluation needed by relation extraction experiments. The corpus consists of annotations of mentions for three semantic relations: marriage, parent―child, siblings, selected from the domain of biographic facts about persons and their social relationships. The corpus contains more than one hundred news articles from Tabloid Press. In the current corpus, we only consider the relation mentions occurring in the individual sentences. We provide multi-level annotations which specify the marked facts from relation, argument, entity, down to the token level, thus allowing for detailed analysis of linguistic phenomena and their interactions. A generic markup tool Recon developed at the DFKI LT lab has been utilised for the annotation task. The corpus has been annotated by two human experts, supported by additional conflict resolution conducted by a third expert. As shown in the evaluation, the annotation is of high quality as proved by the stated inter-annotator agreements both on sentence level and on relationmention level. The current corpus is already in active use in our research for evaluation of the relation extraction performance of our automatically learned extraction patterns.</abstract>
    </paper>
    <paper id="235">
      <author><first>Chetana</first><last>Gavankar</last></author>
      <author><first>Ashish</first><last>Kulkarni</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <title>Efficient Reuse of Structured and Unstructured Resources for Ontology Population</title>
      <pages>3654–3660</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/251_Paper.pdf</url>
      <abstract>We study the problem of ontology population for a domain ontology and present solutions based on semi-automatic techniques. A domain ontology for an organization, often consists of classes whose instances are either specific to, or independent of the organization. E.g. in an academic domain ontology, classes like Professor, Department could be organization (university) specific, while Conference, Programming languages are organization independent. This distinction allows us to leverage data sources both―within the organization and those in the Internet ― to extract entities and populate an ontology. We propose techniques that build on those for open domain IE. Together with user input, we show through comprehensive evaluation, how these semi-automatic techniques achieve high precision. We experimented with the academic domain and built an ontology comprising of over 220 classes. Intranet documents from five universities formed our organization specific corpora and we used open domain knowledge bases like Wikipedia, Linked Open Data, and web pages from the Internet as the organization independent data sources. The populated ontology that we built for one of the universities comprised of over 75,000 instances. We adhere to the semantic web standards and tools and make the resources available in the OWL format. These could be useful for applications such as information extraction, text annotation, and information retrieval.</abstract>
    </paper>
    <paper id="236">
      <author><first>Marie</first><last>Kopřivová</last></author>
      <author><first>Hana</first><last>Goláňová</last></author>
      <author><first>Petra</first><last>Klimešová</last></author>
      <author><first>David</first><last>Lukeš</last></author>
      <title>Mapping Diatopic and Diachronic Variation in Spoken <fixed-case>C</fixed-case>zech: The <fixed-case>ORTOFON</fixed-case> and <fixed-case>DIALEKT</fixed-case> Corpora</title>
      <pages>376–382</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/252_Paper.pdf</url>
      <abstract>ORTOFON and DIALEKT are two corpora of spoken Czech (recordings + transcripts) which are currently being built at the Institute of the Czech National Corpus. The first one (ORTOFON) continues the tradition of the CNC’s ORAL series of spoken corpora by focusing on collecting recordings of unscripted informal spoken interactions (“prototypically spoken texts”), but also provides new features, most notably an annotation scheme with multiple tiers per speaker, including orthographic and phonetic transcripts and allowing for a more precise treatment of overlapping speech. Rich speaker- and situation-related metadata are also collected for possible use as factors in sociolinguistic analyses. One of the stated goals is to make the data in the corpus balanced with respect to a subset of these. The second project, DIALEKT, consists in annotating (in a way partially compatible with the ORTOFON corpus) and providing electronic access to historical (1960s--80s) dialect recordings, mainly of a monological nature, from all over the Czech Republic. The goal is to integrate both corpora into one map-based browsing interface, allowing an intuitive and informative spatial visualization of query results or dialect feature maps, confrontation with isoglosses previously established through the effort of dialectologists etc.</abstract>
    </paper>
    <paper id="237">
      <author><first>Patrick</first><last>Schone</last></author>
      <author><first>Heath</first><last>Nielson</last></author>
      <author><first>Mark</first><last>Ward</last></author>
      <title>Corpus and Evaluation of Handwriting Recognition of Historical Genealogical Records</title>
      <pages>153–159</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/253_Paper.pdf</url>
      <abstract>Over the last few decades, significant strides have been made in handwriting recognition (HR), which is the automatic transcription of handwritten documents. HR often focuses on modern handwritten material, but in the electronic age, the volume of handwritten material is rapidly declining. However, we believe HR is on the verge of having major application to historical record collections. In recent years, archives and genealogical organizations have conducted huge campaigns to transcribe valuable historical record content with such transcription being largely done through human-intensive labor. HR has the potential of revolutionizing these transcription endeavors. To test the hypothesis that this technology is close to applicability, and to provide a testbed for reducing any accuracy gaps, we have developed an evaluation paradigm for historical record handwriting recognition. We created a huge test corpus consisting of four historical data collections of four differing genres and three languages. In this paper, we provide the details of these extensive resources which we intend to release to the research community for further study. Since several research organizations have already participated in this evaluation, we also show initial results and comparisons to human levels of performance.</abstract>
    </paper>
    <paper id="238">
      <author><first>Ildikó</first><last>Pilán</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <title>Reusing <fixed-case>S</fixed-case>wedish <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et for training semantic roles</title>
      <pages>1359–1363</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/254_Paper.pdf</url>
      <abstract>In this article we present the first experiences of reusing the Swedish FrameNet (SweFN) as a resource for training semantic roles. We give an account of the procedure we used to adapt SweFN to the needs of students of Linguistics in the form of an automatically generated exercise. During this adaptation, the mapping of the fine-grained distinction of roles from SweFN into learner-friendlier coarse-grained roles presented a major challenge. Besides discussing the details of this mapping, we describe the resulting multiple-choice exercise and its graphical user interface. The exercise was made available through Lärka, an online platform for students of Linguistics and learners of Swedish as a second language. We outline also aspects underlying the selection of the incorrect answer options which include semantic as well as frequency-based criteria. Finally, we present our own observations and initial user feedback about the applicability of such a resource in the pedagogical domain. Students’ answers indicated an overall positive experience, the majority found the exercise useful for learning semantic roles.</abstract>
    </paper>
    <paper id="239">
      <author><first>Kay</first><last>Berkling</last></author>
      <author><first>Johanna</first><last>Fay</last></author>
      <author><first>Masood</first><last>Ghayoomi</last></author>
      <author><first>Katrin</first><last>Hein</last></author>
      <author><first>Rémi</first><last>Lavalley</last></author>
      <author><first>Ludwig</first><last>Linhuber</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <title>A Database of Freely Written Texts of <fixed-case>G</fixed-case>erman School Students for the Purpose of Automatic Spelling Error Classification</title>
      <pages>1212–1217</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/255_Paper.pdf</url>
      <abstract>The spelling competence of school students is best measured on freely written texts, instead of pre-determined, dictated texts. Since the analysis of the error categories in these kinds of texts is very labor intensive and costly, we are working on an automatic systems to perform this task. The modules of the systems are derived from techniques from the area of natural language processing, and are learning systems that need large amounts of training data. To obtain the data necessary for training and evaluating the resulting system, we conducted data collection of freely written, German texts by school children. 1,730 students from grade 1 through 8 participated in this data collection. The data was transcribed electronically and annotated with their corrected version. This resulted in a total of 14,563 sentences that can now be used for research regarding spelling diagnostics. Additional meta-data was collected regarding writers’ language biography, teaching methodology, age, gender, and school year. In order to do a detailed manual annotation of the categories of the spelling errors committed by the students we developed a tool specifically tailored to the task.</abstract>
    </paper>
    <paper id="240">
      <author><first>Christian</first><last>Haenig</last></author>
      <author><first>Andreas</first><last>Niekler</last></author>
      <author><first>Carsten</first><last>Wuensch</last></author>
      <title><fixed-case>PACE</fixed-case> Corpus: a multilingual corpus of Polarity-annotated textual data from the domains Automotive and <fixed-case>CE</fixed-case>llphone</title>
      <pages>2219–2224</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/258_Paper.pdf</url>
      <abstract>In this paper, we describe a publicly available multilingual evaluation corpus for phrase-level Sentiment Analysis that can be used to evaluate real world applications in an industrial context. This corpus contains data from English and German Internet forums (1000 posts each) focusing on the automotive domain. The major topic of the corpus is connecting and using cellphones to/in cars. The presented corpus contains different types of annotations: objects (e.g. my car, my new cellphone), features (e.g. address book, sound quality) and phrase-level polarities (e.g. the best possible automobile, big problem). Each of the posts has been annotated by at least four different annotators ― these annotations are retained in their original form. The reliability of the annotations is evaluated by inter-annotator agreement scores. Besides the corpus data and format, we provide comprehensive corpus statistics. This corpus is one of the first lexical resources focusing on real world applications that analyze the voice of the customer which is crucial for various industrial use cases.</abstract>
    </paper>
    <paper id="241">
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>Viktor</first><last>Varga</last></author>
      <author><first>Katalin Ilona</first><last>Simkó</last></author>
      <author><first>János</first><last>Zsibrita</last></author>
      <author><first>Ágoston</first><last>Nagy</last></author>
      <author><first>Richárd</first><last>Farkas</last></author>
      <author><first>János</first><last>Csirik</last></author>
      <title><fixed-case>S</fixed-case>zeged Corpus 2.5: Morphological Modifications in a Manually <fixed-case>POS</fixed-case>-tagged <fixed-case>H</fixed-case>ungarian Corpus</title>
      <pages>1074–1078</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/262_Paper.pdf</url>
      <abstract>The Szeged Corpus is the largest manually annotated database containing the possible morphological analyses and lemmas for each word form. In this work, we present its latest version, Szeged Corpus 2.5, in which the new harmonized morphological coding system of Hungarian has been employed and, on the other hand, the majority of misspelled words have been corrected and tagged with the proper morphological code. New morphological codes are introduced for participles, causative / modal / frequentative verbs, adverbial pronouns and punctuation marks, moreover, the distinction between common and proper nouns is eliminated. We also report some statistical data on the frequency of the new morphological codes. The new version of the corpus made it possible to train magyarlanc, a data-driven POS-tagger of Hungarian on a dataset with the new harmonized codes. According to the results, magyarlanc is able to achieve a state-of-the-art accuracy score on the 2.5 version as well.</abstract>
    </paper>
    <paper id="242">
      <author><first>Pierre André</first><last>Ménard</last></author>
      <author><first>Caroline</first><last>Barrière</last></author>
      <title>Linked Open Data and Web Corpus Data for noun compound bracketing</title>
      <pages>702–709</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/263_Paper.pdf</url>
      <abstract>This research provides a comparison of a linked open data resource (DBpedia) and web corpus data resources (Google Web Ngrams and Google Books Ngrams) for noun compound bracketing. Large corpus statistical analysis has often been used for noun compound bracketing, and our goal is to introduce a linked open data (LOD) resource for such task. We show its particularities and its performance on the task. Results obtained on resources tested individually are promising, showing a potential for DBpedia to be included in future hybrid systems.</abstract>
    </paper>
    <paper id="243">
      <author><first>João</first><last>Freitas</last></author>
      <author><first>António</first><last>Teixeira</last></author>
      <author><first>Miguel</first><last>Dias</last></author>
      <title>Multimodal Corpora for Silent Speech Interaction</title>
      <pages>4507–4511</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/264_Paper.pdf</url>
      <abstract>A Silent Speech Interface (SSI) allows for speech communication to take place in the absence of an acoustic signal. This type of interface is an alternative to conventional Automatic Speech Recognition which is not adequate for users with some speech impairments or in the presence of environmental noise. The work presented here produces the conditions to explore and analyze complex combinations of input modalities applicable in SSI research. By exploring non-invasive and promising modalities, we have selected the following sensing technologies used in human-computer interaction: Video and Depth input, Ultrasonic Doppler sensing and Surface Electromyography. This paper describes a novel data collection methodology where these independent streams of information are synchronously acquired with the aim of supporting research and development of a multimodal SSI. The reported recordings were divided into two rounds: a first one where the acquired data was silently uttered and a second round where speakers pronounced the scripted prompts in an audible and normal tone. In the first round of recordings, a total of 53.94 minutes were captured where 30.25% was estimated to be silent speech. In the second round of recordings, a total of 30.45 minutes were obtained and 30.05% of the recordings were audible speech.</abstract>
    </paper>
    <paper id="244">
      <author><first>Tomoko</first><last>Izumi</last></author>
      <author><first>Tomohide</first><last>Shibata</last></author>
      <author><first>Hisako</first><last>Asano</last></author>
      <author><first>Yoshihiro</first><last>Matsuo</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>Constructing a Corpus of <fixed-case>J</fixed-case>apanese Predicate Phrases for Synonym/Antonym Relations</title>
      <pages>1394–1400</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/267_Paper.pdf</url>
      <abstract>We construct a large corpus of Japanese predicate phrases for synonym-antonym relations. The corpus consists of 7,278 pairs of predicates such as receive-permission (ACC) vs. obtain-permission (ACC), in which each predicate pair is accompanied by a noun phrase and case information. The relations are categorized as synonyms, entailment, antonyms, or unrelated. Antonyms are further categorized into three different classes depending on their aspect of oppositeness. Using the data as a training corpus, we conduct the supervised binary classification of synonymous predicates based on linguistically-motivated features. Combining features that are characteristic of synonymous predicates with those that are characteristic of antonymous predicates, we succeed in automatically identifying synonymous predicates at the high F-score of 0.92, a 0.4 improvement over the baseline method of using the Japanese WordNet. The results of an experiment confirm that the quality of the corpus is high enough to achieve automatic classification. To the best of our knowledge, this is the first and the largest publicly available corpus of Japanese predicate phrases for synonym-antonym relations.</abstract>
    </paper>
    <paper id="245">
      <author><first>Evelina</first><last>Rennes</last></author>
      <author><first>Arne</first><last>Jönsson</last></author>
      <title>The Impact of Cohesion Errors in Extraction Based Summaries</title>
      <pages>1575–1582</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/27_Paper.pdf</url>
      <abstract>We present results from an eye tracking study of automatic text summarization. Automatic text summarization is a growing field due to the modern world’s Internet based society, but to automatically create perfect summaries is challenging. One problem is that extraction based summaries often have cohesion errors. By the usage of an eye tracking camera, we have studied the nature of four different types of cohesion errors occurring in extraction based summaries. A total of 23 participants read and rated four different texts and marked the most difficult areas of each text. Statistical analysis of the data revealed that absent cohesion or context and broken anaphoric reference (pronouns) caused some disturbance in reading, but that the impact is restricted to the effort to read rather than the comprehension of the text. However, erroneous anaphoric references (pronouns) were not always detected by the participants which poses a problem for automatic text summarizers. The study also revealed other potential disturbing factors.</abstract>
    </paper>
    <paper id="246">
      <author><first>Lanjun</first><last>Zhou</last></author>
      <author><first>Binyang</first><last>Li</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <title>The <fixed-case>CUHK</fixed-case> Discourse <fixed-case>T</fixed-case>ree<fixed-case>B</fixed-case>ank for <fixed-case>C</fixed-case>hinese: Annotating Explicit Discourse Connectives for the <fixed-case>C</fixed-case>hinese <fixed-case>T</fixed-case>ree<fixed-case>B</fixed-case>ank</title>
      <pages>942–949</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/270_Paper.pdf</url>
      <abstract>The lack of open discourse corpus for Chinese brings limitations for many natural language processing tasks. In this work, we present the first open discourse treebank for Chinese, namely, the Discourse Treebank for Chinese (DTBC). At the current stage, we annotated explicit intra-sentence discourse connectives, their corresponding arguments and senses for all 890 documents of the Chinese Treebank 5. We started by analysing the characteristics of discourse annotation for Chinese, adapted the annotation scheme of Penn Discourse Treebank 2 (PDTB2) to Chinese language while maintaining the compatibility as far as possible. We made adjustments to 3 essential aspects according to the previous study of Chinese linguistics. They are sense hierarchy, argument scope and semantics of arguments. Agreement study showed that our annotation scheme could achieve highly reliable results.</abstract>
    </paper>
    <paper id="247">
      <author><first>Kugatsu</first><last>Sadamitsu</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Yoshihiro</first><last>Matsuo</last></author>
      <title>Extraction of Daily Changing Words for Question Answering</title>
      <pages>2608–2612</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/271_Paper.pdf</url>
      <abstract>This paper proposes a method for extracting Daily Changing Words (DCWs), words that indicate which questions are real-time dependent. Our approach is based on two types of template matching using time and named entity slots from large size corpora and adding simple filtering methods from news corpora. Extracted DCWs are utilized for detecting and sorting real-time dependent questions. Experiments confirm that our DCW method achieves higher accuracy in detecting real-time dependent questions than existing word classes and a simple supervised machine learning approach.</abstract>
    </paper>
    <paper id="248">
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Declan</first><last>Groves</last></author>
      <title><fixed-case>MTW</fixed-case>atch: A Tool for the Analysis of Noisy Parallel Data</title>
      <pages>41–45</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/272_Paper.pdf</url>
      <abstract>State-of-the-art statistical machine translation (SMT) technique requires a good quality parallel data to build a translation model. The availability of large parallel corpora has rapidly increased over the past decade. However, often these newly developed parallel data contains contain significant noise. In this paper, we describe our approach for classifying good quality parallel sentence pairs from noisy parallel data. We use 10 different features within a Support Vector Machine (SVM)-based model for our classification task. We report a reasonably good classification accuracy and its positive effect on overall MT accuracy.</abstract>
    </paper>
    <paper id="249">
      <author><first>Martin</first><last>Riedl</last></author>
      <author><first>Richard</first><last>Steuer</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <title>Distributed Distributional Similarities of <fixed-case>G</fixed-case>oogle <fixed-case>B</fixed-case>ooks Over the Centuries</title>
      <pages>1401–1405</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/274_Paper.pdf</url>
      <abstract>This paper introduces a distributional thesaurus and sense clusters computed on the complete Google Syntactic N-grams, which is extracted from Google Books, a very large corpus of digitized books published between 1520 and 2008. We show that a thesaurus computed on such a large text basis leads to much better results than using smaller corpora like Wikipedia. We also provide distributional thesauri for equal-sized time slices of the corpus. While distributional thesauri can be used as lexical resources in NLP tasks, comparing word similarities over time can unveil sense change of terms across different decades or centuries, and can serve as a resource for diachronic lexicography. Thesauri and clusters are available for download.</abstract>
    </paper>
    <paper id="250">
      <author><first>Saba</first><last>Urooj</last></author>
      <author><first>Sarmad</first><last>Hussain</last></author>
      <author><first>Asad</first><last>Mustafa</last></author>
      <author><first>Rahila</first><last>Parveen</last></author>
      <author><first>Farah</first><last>Adeeba</last></author>
      <author><first>Tafseer</first><last>Ahmed Khan</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <author><first>Annette</first><last>Hautli</last></author>
      <title>The <fixed-case>CLE</fixed-case> <fixed-case>U</fixed-case>rdu <fixed-case>POS</fixed-case> Tagset</title>
      <pages>2920–2925</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/275_Paper.pdf</url>
      <abstract>The paper presents a design schema and details of a new Urdu POS tagset. This tagset is designed due to challenges encountered in working with existing tagsets for Urdu. It uses tags that judiciously incorporate information about special morpho-syntactic categories found in Urdu. With respect to the overall naming schema and the basic divisions, the tagset draws on the Penn Treebank and a Common Tagset for Indian Languages. The resulting CLE Urdu POS Tagset consists of 12 major categories with subdivisions, resulting in 32 tags. The tagset has been used to tag 100k words of the CLE Urdu Digest Corpus, giving a tagging accuracy of 96.8%.</abstract>
    </paper>
    <paper id="251">
      <author><first>Darina</first><last>Benikova</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Marc</first><last>Reznicek</last></author>
      <title><fixed-case>N</fixed-case>o<fixed-case>S</fixed-case>ta-<fixed-case>D</fixed-case> Named Entity Annotation for <fixed-case>G</fixed-case>erman: Guidelines and Dataset</title>
      <pages>2524–2531</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf</url>
      <abstract>We describe the annotation of a new dataset for German Named Entity Recognition (NER). The need for this dataset is motivated by licensing issues and consistency issues of existing datasets. We describe our approach to creating annotation guidelines based on linguistic and semantic considerations, and how we iteratively refined and tested them in the early stages of annotation in order to arrive at the largest publicly available dataset for German NER, consisting of over 31,000 manually annotated sentences (over 591,000 tokens) from German Wikipedia and German online news. We provide a number of statistics on the dataset, which indicate its high quality, and discuss legal aspects of distributing the data as a compilation of citations. The data is released under the permissive CC-BY license, and will be fully available for download in September 2014 after it has been used for the GermEval 2014 shared task on NER. We further provide the full annotation guidelines and links to the annotation tool used for the creation of this resource.</abstract>
    </paper>
    <paper id="252">
      <author><first>Yi-Fen</first><last>Liu</last></author>
      <author><first>Shu-Chuan</first><last>Tseng</last></author>
      <author><first>J.-S. Roger</first><last>Jang</last></author>
      <title>Phone Boundary Annotation in Conversational Speech</title>
      <pages>848–853</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/277_Paper.pdf</url>
      <abstract>Phone-aligned spoken corpora are indispensable language resources for quantitative linguistic analyses and automatic speech systems. However, producing this type of data resources is not an easy task due to high costs of time and man power as well as difficulties of applying valid annotation criteria and achieving reliable inter-labelers consistency. Among different types of spoken corpora, conversational speech that is often filled with extreme reduction and varying pronunciation variants is particularly challenging. By adopting a combined verification procedure, we obtained reasonably good annotation results. Preliminary phone boundaries that were automatically generated by a phone aligner were provided to human labelers for verifying. Instead of making use of the visualization of acoustic cues, the labelers should solely rely on their perceptual judgments to locate a position that best separates two adjacent phones. Impressionistic judgments in cases of reduction and segment deletion were helpful and necessary, as they balanced subtle nuance caused by differences in perception.</abstract>
    </paper>
    <paper id="253">
      <author><first>Mayumi</first><last>Bono</last></author>
      <author><first>Kouhei</first><last>Kikuchi</last></author>
      <author><first>Paul</first><last>Cibulka</last></author>
      <author><first>Yutaka</first><last>Osugi</last></author>
      <title>A Colloquial Corpus of <fixed-case>J</fixed-case>apanese <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage: Linguistic Resources for Observing Sign Language Conversations</title>
      <pages>1898–1904</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/278_Paper.pdf</url>
      <abstract>We began building a corpus of Japanese Sign Language (JSL) in April 2011. The purpose of this project was to increase awareness of sign language as a distinctive language in Japan. This corpus is beneficial not only to linguistic research but also to hearing-impaired and deaf individuals, as it helps them to recognize and respect their linguistic differences and communication styles. This is the first large-scale JSL corpus developed for both academic and public use. We collected data in three ways: interviews (for introductory purposes only), dialogues, and lexical elicitation. In this paper, we focus particularly on data collected during a dialogue to discuss the application of conversation analysis (CA) to signed dialogues and signed conversations. Our annotation scheme was designed not only to elucidate theoretical issues related to grammar and linguistics but also to clarify pragmatic and interactional phenomena related to the use of JSL.</abstract>
    </paper>
    <paper id="254">
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <author><first>Elżbieta</first><last>Hajnicz</last></author>
      <author><first>Agnieszka</first><last>Patejuk</last></author>
      <author><first>Marcin</first><last>Woliński</last></author>
      <author><first>Filip</first><last>Skwarski</last></author>
      <author><first>Marek</first><last>Świdziński</last></author>
      <title><fixed-case>W</fixed-case>alenty: Towards a comprehensive valence dictionary of <fixed-case>P</fixed-case>olish</title>
      <pages>2785–2792</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/279_Paper.pdf</url>
      <abstract>This paper presents Walenty, a comprehensive valence dictionary of Polish, with a number of novel features, as compared to other such dictionaries. The notion of argument is based on the coordination test and takes into consideration the possibility of diverse morphosyntactic realisations. Some aspects of the internal structure of phraseological (idiomatic) arguments are handled explicitly. While the current version of the dictionary concentrates on syntax, it already contains some semantic features, including semantically defined arguments, such as locative, temporal or manner, as well as control and raising, and work on extending it with semantic roles and selectional preferences is in progress. Although Walenty is still being intensively developed, it is already by far the largest Polish valence dictionary, with around 8600 verbal lemmata and almost 39 000 valence schemata. The dictionary is publicly available on the Creative Commons BY SA licence and may be downloaded from http://zil.ipipan.waw.pl/Walenty.</abstract>
    </paper>
    <paper id="255">
      <author><first>Balamurali</first><last>A.R</last></author>
      <title>Can the Crowd be Controlled?: A Case Study on Crowd Sourcing and Automatic Validation of Completed Tasks based on User Modeling</title>
      <pages>189–195</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/28_Paper.pdf</url>
      <abstract>Annotation is an essential step in the development cycle of many Natural Language Processing (NLP) systems. Lately, crowd-sourcing has been employed to facilitate large scale annotation at a reduced cost. Unfortunately, verifying the quality of the submitted annotations is a daunting task. Existing approaches address this problem either through sampling or redundancy. However, these approaches do have a cost associated with it. Based on the observation that a crowd-sourcing worker returns to do a task that he has done previously, a novel framework for automatic validation of crowd-sourced task is proposed in this paper. A case study based on sentiment analysis is presented to elucidate the framework and its feasibility. The result suggests that validation of the crowd-sourced task can be automated to a certain extent.</abstract>
    </paper>
    <paper id="256">
      <author><first>Thomas</first><last>Bögel</last></author>
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Michael</first><last>Gertz</last></author>
      <title>Computational Narratology: Extracting Tense Clusters from Narrative Texts</title>
      <pages>950–955</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/280_Paper.pdf</url>
      <abstract>Computational Narratology is an emerging field within the Digital Humanities. In this paper, we tackle the problem of extracting temporal information as a basis for event extraction and ordering, as well as further investigations of complex phenomena in narrative texts. While most existing systems focus on news texts and extract explicit temporal information exclusively, we show that this approach is not feasible for narratives. Based on tense information of verbs, we define temporal clusters as an annotation task and validate the annotation schema by showing that the task can be performed with high inter-annotator agreement. To alleviate and reduce the manual annotation effort, we propose a rule-based approach to robustly extract temporal clusters using a multi-layered and dynamic NLP pipeline that combines off-the-shelf components in a heuristic setting. Comparing our results against human judgements, our system is capable of predicting the tense of verbs and sentences with very high reliability: for the most prevalent tense in our corpus, more than 95% of all verbs are annotated correctly.</abstract>
    </paper>
    <paper id="257">
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <author><first>Ilze</first><last>Auziņa</last></author>
      <author><first>Kārlis</first><last>Goba</last></author>
      <title>Designing the <fixed-case>L</fixed-case>atvian Speech Recognition Corpus</title>
      <pages>1547–1553</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/284_Paper.pdf</url>
      <abstract>In this paper the authors present the first Latvian speech corpus designed specifically for speech recognition purposes. The paper outlines the decisions made in the corpus designing process through analysis of related work on speech corpora creation for different languages. The authors provide also guidelines that were used for the creation of the Latvian speech recognition corpus. The corpus creation guidelines are fairly general for them to be re-used by other researchers when working on different language speech recognition corpora. The corpus consists of two parts ― an orthographically annotated corpus containing 100 hours of orthographically transcribed audio data and a phonetically annotated corpus containing 4 hours of phonetically transcribed audio data. Metadata files in XML format provide additional details about the speakers, noise levels, speech styles, etc. The speech recognition corpus is phonetically balanced and phonetically rich and the paper describes also the methodology how the phonetical balancedness has been assessed.</abstract>
    </paper>
    <paper id="258">
      <author><first>Pavel</first><last>Vondřička</last></author>
      <title>Aligning parallel texts with <fixed-case>I</fixed-case>nter<fixed-case>T</fixed-case>ext</title>
      <pages>1875–1879</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/285_Paper.pdf</url>
      <abstract>InterText is a flexible manager and editor for alignment of parallel texts aimed both at individual and collaborative creation of parallel corpora of any size or translational memories. It is available in two versions: as a multi-user server application with a web-based interface and as a native desktop application for personal use. Both versions are able to cooperate with each other. InterText can process plain text or custom XML documents, deploy existing automatic aligners and provide a comfortable interface for manual post-alignment correction of both the alignment and the text contents and segmentation of the documents. One language version may be aligned with several other versions (using stand-off alignment) and the application ensures consistency between them. The server version supports different user levels and privileges and it can also track changes made to the texts for easier supervision. It also allows for batch import, alignment and export and can be connected to other tools and scripts for better integration in a more complex project workflow.</abstract>
    </paper>
    <paper id="259">
      <author><first>Panot</first><last>Chaimongkol</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <author><first>Yuka</first><last>Tateisi</last></author>
      <title>Corpus for Coreference Resolution on Scientific Papers</title>
      <pages>3187–3190</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/286_Paper.pdf</url>
      <abstract>The ever-growing number of published scientific papers prompts the need for automatic knowledge extraction to help scientists keep up with the state-of-the-art in their respective fields. To construct a good knowledge extraction system, annotated corpora in the scientific domain are required to train machine learning models. As described in this paper, we have constructed an annotated corpus for coreference resolution in multiple scientific domains, based on an existing corpus. We have modified the annotation scheme from Message Understanding Conference to better suit scientific texts. Then we applied that to the corpus. The annotated corpus is then compared with corpora in general domains in terms of distribution of resolution classes and performance of the Stanford Dcoref coreference resolver. Through these comparisons, we have demonstrated quantitatively that our manually annotated corpus differs from a general-domain corpus, which suggests deep differences between general-domain texts and scientific texts and which shows that different approaches can be made to tackle coreference resolution for general texts and scientific texts.</abstract>
    </paper>
    <paper id="260">
      <author><first>Ingrid</first><last>Falk</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Christophe</first><last>Gérard</last></author>
      <title>From Non Word to New Word: Automatically Identifying Neologisms in <fixed-case>F</fixed-case>rench Newspapers</title>
      <pages>4337–4344</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/288_Paper.pdf</url>
      <abstract>In this paper we present a statistical machine learning approach to formal neologism detection going some way beyond the use of exclusion lists. We explore the impact of three groups of features: form related, morpho-lexical and thematic features. The latter type of features has not yet been used in this kind of application and represents a way to access the semantic context of new words. The results suggest that form related features are helpful at the overall classification task, while morpho-lexical and thematic features better single out true neologisms.</abstract>
    </paper>
    <paper id="261">
      <author><first>Nancy</first><last>Underwood</last></author>
      <author><first>Bartolomé</first><last>Mesa-Lao</last></author>
      <author><first>Mercedes García</first><last>Martínez</last></author>
      <author><first>Michael</first><last>Carl</last></author>
      <author><first>Vicent</first><last>Alabau</last></author>
      <author><first>Jesús</first><last>González-Rubio</last></author>
      <author><first>Luis A.</first><last>Leiva</last></author>
      <author><first>Germán</first><last>Sanchis-Trilles</last></author>
      <author><first>Daniel</first><last>Ortíz-Martínez</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <title>Evaluating the effects of interactivity in a post-editing workbench</title>
      <pages>553–559</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/289_Paper.pdf</url>
      <abstract>This paper describes the field trial and subsequent evaluation of a post-editing workbench which is currently under development in the EU-funded CasMaCat project. Based on user evaluations of the initial prototype of the workbench, this second prototype of the workbench includes a number of interactive features designed to improve productivity and user satisfaction. Using CasMaCat’s own facilities for logging keystrokes and eye tracking, data were collected from nine post-editors in a professional setting. These data were then used to investigate the effects of the interactive features on productivity, quality, user satisfaction and cognitive load as reflected in the post-editors gaze activity. These quantitative results are combined with the qualitative results derived from user questionnaires and interviews conducted with all the participants.</abstract>
    </paper>
    <paper id="262">
      <author><first>Georgios</first><last>Paltoglou</last></author>
      <title>Using <fixed-case>T</fixed-case>witter and Sentiment Analysis for event detection</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/29_Paper.pdf</url>
    </paper>
    <paper id="263">
      <author><first>Thomas</first><last>Schmidt</last></author>
      <title>The Research and Teaching Corpus of Spoken <fixed-case>G</fixed-case>erman — <fixed-case>FOLK</fixed-case></title>
      <pages>383–387</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/290_Paper.pdf</url>
      <abstract>FOLK is the “Forschungs- und Lehrkorpus Gesprochenes Deutsch (FOLK)” (eng.: research and teaching corpus of spoken German). The project has set itself the aim of building a corpus of German conversations which a) covers a broad range of interaction types in private, institutional and public settings, b) is sufficiently large and diverse and of sufficient quality to support different qualitative and quantitative research approaches, c) is transcribed, annotated and made accessible according to current technological standards, and d) is available to the scientific community on a sound legal basis and without unnecessary restrictions of usage. This paper gives an overview of the corpus design, the strategies for acquisition of a diverse range of interaction data, and the corpus construction workflow from recording via transcription an annotation to dissemination.</abstract>
    </paper>
    <paper id="264">
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last></author>
      <author><first>Peter</first><last>Fankhauser</last></author>
      <author><first>Hannah</first><last>Kermes</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Noam</first><last>Ordan</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <title>Data Mining with Shallow vs. Linguistic Features to Study Diversification of Scientific Registers</title>
      <pages>1327–1334</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/291_Paper.pdf</url>
      <abstract>We present a methodology to analyze the linguistic evolution of scientific registers with data mining techniques, comparing the insights gained from shallow vs. linguistic features. The focus is on selected scientific disciplines at the boundaries to computer science (computational linguistics, bioinformatics, digital construction, microelectronics). The data basis is the English Scientific Text Corpus (SCITEX) which covers a time range of roughly thirty years (1970/80s to early 2000s) (Degaetano-Ortlieb et al., 2013; Teich and Fankhauser, 2010). In particular, we investigate the diversification of scientific registers over time. Our theoretical basis is Systemic Functional Linguistics (SFL) and its specific incarnation of register theory (Halliday and Hasan, 1985). In terms of methods, we combine corpus-based methods of feature extraction and data mining techniques.</abstract>
    </paper>
    <paper id="265">
      <author><first>Hassan</first><last>Saif</last></author>
      <author><first>Miriam</first><last>Fernandez</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <author><first>Harith</first><last>Alani</last></author>
      <title>On Stopwords, Filtering and Data Sparsity for Sentiment Analysis of <fixed-case>T</fixed-case>witter</title>
      <pages>810–817</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf</url>
      <abstract>Sentiment classification over Twitter is usually affected by the noisy nature (abbreviations, irregular forms) of tweets data. A popular procedure to reduce the noise of textual data is to remove stopwords by using pre-compiled stopword lists or more sophisticated methods for dynamic stopword identification. However, the effectiveness of removing stopwords in the context of Twitter sentiment classification has been debated in the last few years. In this paper we investigate whether removing stopwords helps or hampers the effectiveness of Twitter sentiment classification methods. To this end, we apply six different stopword identification methods to Twitter data from six different datasets and observe how removing stopwords affects two well-known supervised sentiment classification methods. We assess the impact of removing stopwords by observing fluctuations on the level of data sparsity, the size of the classifier’s feature space and its classification performance. Our results show that using pre-compiled lists of stopwords negatively impacts the performance of Twitter sentiment classification approaches. On the other hand, the dynamic generation of stopword lists, by removing those infrequent terms appearing only once in the corpus, appears to be the optimal method to maintaining a high classification performance while reducing the data sparsity and shrinking the feature space.</abstract>
    </paper>
    <paper id="266">
      <author><first>Patrik</first><last>Lambert</last></author>
      <author><first>Carlos</first><last>Rodríguez-Penagos</last></author>
      <title>Adapting Freely Available Resources to Build an Opinion Mining Pipeline in <fixed-case>P</fixed-case>ortuguese</title>
      <pages>2225–2228</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/293_Paper.pdf</url>
      <abstract>We present a complete UIMA-based pipeline for sentiment analysis in Portuguese news using freely available resources and a minimal set of manually annotated training data. We obtained good precision on binary classification but concluded that news feed is a challenging environment to detect the extent of opinionated text.</abstract>
    </paper>
    <paper id="267">
      <author><first>Milena</first><last>Hnátková</last></author>
      <author><first>Michal</first><last>Křen</last></author>
      <author><first>Pavel</first><last>Procházka</last></author>
      <author><first>Hana</first><last>Skoumalová</last></author>
      <title>The <fixed-case>SYN</fixed-case>-series corpora of written <fixed-case>C</fixed-case>zech</title>
      <pages>160–164</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/294_Paper.pdf</url>
      <abstract>The paper overviews the SYN series of synchronic corpora of written Czech compiled within the framework of the Czech National Corpus project. It describes their design and processing with a focus on the annotation, i.e. lemmatization and morphological tagging. The paper also introduces SYN2013PUB, a new 935-million newspaper corpus of Czech published in 2013 as the most recent addition to the SYN series before planned revision of its architecture. SYN2013PUB can be seen as a completion of the series in terms of titles and publication dates of major Czech newspapers that are now covered by complete volumes in comparable proportions. All SYN-series corpora can be characterized as traditional, with emphasis on cleared copyright issues, well-defined composition, reliable metadata and high-quality data processing; their overall size currently exceeds 2.2 billion running words.</abstract>
    </paper>
    <paper id="268">
      <author><first>Liane</first><last>Guillou</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Aaron</first><last>Smith</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <title><fixed-case>P</fixed-case>ar<fixed-case>C</fixed-case>or 1.0: A Parallel Pronoun-Coreference Corpus to Support Statistical <fixed-case>MT</fixed-case></title>
      <pages>3191–3198</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/298_Paper.pdf</url>
      <abstract>We present ParCor, a parallel corpus of texts in which pronoun coreference ― reduced coreference in which pronouns are used as referring expressions ― has been annotated. The corpus is intended to be used both as a resource from which to learn systematic differences in pronoun use between languages and ultimately for developing and testing informed Statistical Machine Translation systems aimed at addressing the problem of pronoun coreference in translation. At present, the corpus consists of a collection of parallel English-German documents from two different text genres: TED Talks (transcribed planned speech), and EU Bookshop publications (written text). All documents in the corpus have been manually annotated with respect to the type and location of each pronoun and, where relevant, its antecedent. We provide details of the texts that we selected, the guidelines and tools used to support annotation and some corpus statistics. The texts in the corpus have already been translated into many languages, and we plan to expand the corpus into these other languages, as well as other genres, in the future.</abstract>
    </paper>
    <paper id="269">
      <author><first>Johann-Mattis</first><last>List</last></author>
      <author><first>Jelena</first><last>Prokić</last></author>
      <title>A Benchmark Database of Phonetic Alignments in Historical Linguistics and Dialectology</title>
      <pages>288–294</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/299_Paper.pdf</url>
      <abstract>In the last two decades, alignment analyses have become an important technique in quantitative historical linguistics and dialectology. Phonetic alignment plays a crucial role in the identification of regular sound correspondences and deeper genealogical relations between and within languages and language families. Surprisingly, up to today, there are no easily accessible benchmark data sets for phonetic alignment analyses. Here we present a publicly available database of manually edited phonetic alignments which can serve as a platform for testing and improving the performance of automatic alignment algorithms. The database consists of a great variety of alignments drawn from a large number of different sources. The data is arranged in a such way that typical problems encountered in phonetic alignment analyses (metathesis, diversity of phonetic sequences) are represented and can be directly tested.</abstract>
    </paper>
    <paper id="270">
      <author><first>Xavier</first><last>Tannier</last></author>
      <title>Extracting News Web Page Creation Time with <fixed-case>DCTF</fixed-case>inder</title>
      <pages>2037–2042</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/3_Paper.pdf</url>
      <abstract>Web pages do not offer reliable metadata concerning their creation date and time. However, getting the document creation time is a necessary step for allowing to apply temporal normalization systems to web pages. In this paper, we present DCTFinder, a system that parses a web page and extracts from its content the title and the creation date of this web page. DCTFinder combines heuristic title detection, supervised learning with Conditional Random Fields (CRFs) for document date extraction, and rule-based creation time recognition. Using such a system allows further deep and efficient temporal analysis of web pages. Evaluation on three corpora of English and French web pages indicates that the tool can extract document creation times with reasonably high accuracy (between 87 and 92\%). DCTFinder is made freely available on http://sourceforge.net/projects/dctfinder/, as well as all resources (vocabulary and annotated documents) built for training and evaluating the system in English and French, and the English trained model itself.</abstract>
    </paper>
    <paper id="271">
      <author><first>Karel</first><last>Kučera</last></author>
      <author><first>Martin</first><last>Stluka</last></author>
      <title>Corpus of 19th-century <fixed-case>C</fixed-case>zech Texts: Problems and Solutions</title>
      <pages>165–168</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/300_Paper.pdf</url>
      <abstract>Although the Czech language of the 19th century represents the roots of modern Czech and many features of the 20th- and 21st-century language cannot be properly understood without this historical background, the 19th-century Czech has not been thoroughly and consistently researched so far. The long-term project of a corpus of 19th-century Czech printed texts, currently in its third year, is intended to stimulate the research as well as to provide a firm material basis for it. The reason why, in our opinion, the project is worth mentioning is that it is faced with an unusual concentration of problems following mostly from the fact that the 19th century was arguably the most tumultuous period in the history of Czech, as well as from the fact that Czech is a highly inflectional language with a long history of sound changes, orthography reforms and rather discontinuous development of its vocabulary. The paper will briefly characterize the general background of the problems and present the reasoning behind the solutions that have been implemented in the ongoing project.</abstract>
    </paper>
    <paper id="272">
      <author><first>Julien</first><last>Velcin</last></author>
      <author><first>Young-Min</first><last>Kim</last></author>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Jean-Yves</first><last>Dormagen</last></author>
      <author><first>Eric</first><last>SanJuan</last></author>
      <author><first>Leila</first><last>Khouas</last></author>
      <author><first>Anne</first><last>Peradotto</last></author>
      <author><first>Stephane</first><last>Bonnevay</last></author>
      <author><first>Claude</first><last>Roux</last></author>
      <author><first>Julien</first><last>Boyadjian</last></author>
      <author><first>Alejandro</first><last>Molina</last></author>
      <author><first>Marie</first><last>Neihouser</last></author>
      <title>Investigating the Image of Entities in Social Media: Dataset Design and First Results</title>
      <pages>818–822</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/302_Paper.pdf</url>
      <abstract>The objective of this paper is to describe the design of a dataset that deals with the image (i.e., representation, web reputation) of various entities populating the Internet: politicians, celebrities, companies, brands etc. Our main contribution is to build and provide an original annotated French dataset. This dataset consists of 11527 manually annotated tweets expressing the opinion on specific facets (e.g., ethic, communication, economic project) describing two French policitians over time. We believe that other researchers might benefit from this experience, since designing and implementing such a dataset has proven quite an interesting challenge. This design comprises different processes such as data selection, formal definition and instantiation of an image. We have set up a full open-source annotation platform. In addition to the dataset design, we present the first results that we obtained by applying clustering methods to the annotated dataset in order to extract the entity images.</abstract>
    </paper>
    <paper id="273">
      <author><first>Per Erik</first><last>Solberg</last></author>
      <author><first>Arne</first><last>Skjærholt</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Kristin</first><last>Hagen</last></author>
      <author><first>Janne Bondi</first><last>Johannessen</last></author>
      <title>The <fixed-case>N</fixed-case>orwegian Dependency Treebank</title>
      <pages>789–795</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/303_Paper.pdf</url>
      <abstract>The Norwegian Dependency Treebank is a new syntactic treebank for Norwegian Bokmäl and Nynorsk with manual syntactic and morphological annotation, developed at the National Library of Norway in collaboration with the University of Oslo. It is the first publically available treebank for Norwegian. This paper presents the core principles behind the syntactic annotation and how these principles were employed in certain specific cases. We then present the selection of texts and distribution between genres, as well as the annotation process and an evaluation of the inter-annotator agreement. Finally, we present the first results of data-driven dependency parsing of Norwegian, contrasting four state-of-the-art dependency parsers trained on the treebank. The consistency and the parsability of this treebank is shown to be comparable to other large treebank initiatives.</abstract>
    </paper>
    <paper id="274">
      <author><first>Maik</first><last>Stührenberg</last></author>
      <title>Extending standoff annotation</title>
      <pages>169–174</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/308_Paper.pdf</url>
      <abstract>Textual information is sometimes accompanied by additional encodings (such as visuals). These multimodal documents may be interesting objects of investigation for linguistics. Another class of complex documents are pre-annotated documents. Classic XML inline annotation often fails for both document classes because of overlapping markup. However, standoff annotation, that is the separation of primary data and markup, is a valuable and common mechanism to annotate multiple hierarchies and/or read-only primary data. We demonstrate an extended version of the XStandoff meta markup language, that allows the definition of segments in spatial and pre-annotated primary data. Together with the ability to import already established (linguistic) serialization formats as annotation levels and layers in an XStandoff instance, we are able to annotate a variety of primary data files, including text, audio, still and moving images. Application scenarios that may benefit from using XStandoff are the analyzation of multimodal documents such as instruction manuals, or sports match analysis, or the less destructive cleaning of web pages.</abstract>
    </paper>
    <paper id="275">
      <author><first>László</first><last>Laki</last></author>
      <author><first>György</first><last>Orosz</last></author>
      <title>An efficient language independent toolkit for complete morphological disambiguation</title>
      <pages>1625–1630</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/309_Paper.pdf</url>
      <abstract>In this paper a Moses SMT toolkit-based language-independent complete morphological annotation tool is presented called HuLaPos2. Our system performs PoS tagging and lemmatization simultaneously. Amongst others, the algorithm used is able to handle phrases instead of unigrams, and can perform the tagging in a not strictly left-to-right order. With utilizing these gains, our system outperforms the HMM-based ones. In order to handle the unknown words, a suffix-tree based guesser was integrated into HuLaPos2. To demonstrate the performance of our system it was compared with several systems in different languages and PoS tag sets. In general, it can be concluded that the quality of HuLaPos2 is comparable with the state-of-the-art systems, and in the case of PoS tagging it outperformed many available systems.</abstract>
    </paper>
    <paper id="276">
      <author><first>Peter</first><last>Spyns</last></author>
      <author><first>Remco</first><last>van Veenendaal</last></author>
      <title>A decade of <fixed-case>HLT</fixed-case> Agency activities in the Low Countries: from resource maintenance (<fixed-case>BLARK</fixed-case>) to service offerings (<fixed-case>BLAISE</fixed-case>)</title>
      <pages>2158–2165</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/31_Paper.pdf</url>
      <abstract>In this paper we report on the Flemish-Dutch Agency for Human Language Technologies (HLT Agency or TST-Centrale in Dutch) in the Low Countries. We present its activities in its first decade of existence. The main goal of the HLT Agency is to ensure the sustainability of linguistic resources for Dutch. 10 years after its inception, the HLT Agency faces new challenges and opportunities. An important contextual factor is the rise of the infrastructure networks and proliferation of resource centres. We summarise some lessons learnt and we propose as future work to define and build for Dutch (which by extension can apply to any national language) a set of Basic LAnguage Infrastructure SErvices (BLAISE). As a conclusion, we state that the HLT Agency, also by its peculiar institutional status, has fulfilled and still is fulfilling an important role in maintaining Dutch as a digitally fully fledged functional language.</abstract>
    </paper>
    <paper id="277">
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Sarah</first><last>Fünfer</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <title>A Corpus of Spontaneous Speech in Lectures: The <fixed-case>KIT</fixed-case> Lecture Corpus for Spoken Language Processing and Translation</title>
      <pages>1554–1559</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/311_Paper.pdf</url>
      <abstract>With the increasing number of applications handling spontaneous speech, the needs to process spoken languages become stronger. Speech disfluency is one of the most challenging tasks to deal with in automatic speech processing. As most applications are trained with well-formed, written texts, many issues arise when processing spontaneous speech due to its distinctive characteristics. Therefore, more data with annotated speech disfluencies will help the adaptation of natural language processing applications, such as machine translation systems. In order to support this, we have annotated speech disfluencies in German lectures at KIT. In this paper we describe how we annotated the disfluencies in the data and provide detailed statistics on the size of the corpus and the speakers. Moreover, machine translation performance on a source text including disfluencies is compared to the results of the translation of a source text without different sorts of disfluencies or no disfluencies at all.</abstract>
    </paper>
    <paper id="278">
      <author><first>Niklas</first><last>Vanhainen</last></author>
      <author><first>Giampiero</first><last>Salvi</last></author>
      <title>Free Acoustic and Language Models for Large Vocabulary Continuous Speech Recognition in <fixed-case>S</fixed-case>wedish</title>
      <pages>388–392</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/312_Paper.pdf</url>
      <abstract>This paper presents results for large vocabulary continuous speech recognition (LVCSR) in Swedish. We trained acoustic models on the public domain NST Swedish corpus and made them freely available to the community. The training procedure corresponds to the reference recogniser (RefRec) developed for the SpeechDat databases during the COST249 action. We describe the modifications we made to the procedure in order to train on the NST database, and the language models we created based on the N-gram data available at the Norwegian Language Council. Our tests include medium vocabulary isolated word recognition and LVCSR. Because no previous results are available for LVCSR in Swedish, we use as baseline the performance of the SpeechDat models on the same tasks. We also compare our best results to the ones obtained in similar conditions on resource rich languages such as American English. We tested the acoustic models with HTK and Julius and plan to make them available in CMU Sphinx format as well in the near future. We believe that the free availability of these resources will boost research in speech and language technology in Swedish, even in research groups that do not have resources to develop ASR systems.</abstract>
    </paper>
    <paper id="279">
      <author><first>Begüm</first><last>Erten</last></author>
      <author><first>Cem</first><last>Bozsahin</last></author>
      <author><first>Deniz</first><last>Zeyrek</last></author>
      <title><fixed-case>T</fixed-case>urkish Resources for Visual Word Recognition</title>
      <pages>2106–2110</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/316_Paper.pdf</url>
      <abstract>We report two tools to conduct psycholinguistic experiments on Turkish words. KelimetriK allows experimenters to choose words based on desired orthographic scores of word frequency, bigram and trigram frequency, ON, OLD20, ATL and subset/superset similarity. Turkish version of Wuggy generates pseudowords from one or more template words using an efficient method. The syllabified version of the words are used as the input, which are decomposed into their sub-syllabic components. The bigram frequency chains are constructed by the entire words’ onset, nucleus and coda patterns. Lexical statistics of stems and their syllabification are compiled by us from BOUN corpus of 490 million words. Use of these tools in some experiments is shown.</abstract>
    </paper>
    <paper id="280">
      <author><first>Eshrag</first><last>Refaee</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <title>An <fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter Corpus for Subjectivity and Sentiment Analysis</title>
      <pages>2268–2273</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/317_Paper.pdf</url>
      <abstract>We present a newly collected data set of 8,868 gold-standard annotated Arabic feeds. The corpus is manually labelled for subjectivity and sentiment analysis (SSA) ( = 0:816). In addition, the corpus is annotated with a variety of motivated feature-sets that have previously shown positive impact on performance. The paper highlights issues posed by twitter as a genre, such as mixture of language varieties and topic-shifts. Our next step is to extend the current corpus, using online semi-supervised learning. A first sub-corpus will be released via the ELRA repository as part of this submission.</abstract>
    </paper>
    <paper id="281">
      <author><first>Massimo</first><last>Moneglia</last></author>
      <author><first>Susan</first><last>Brown</last></author>
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Gloria</first><last>Gagliardi</last></author>
      <author><first>Fahad</first><last>Khan</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Alessandro</first><last>Panunzi</last></author>
      <title>The <fixed-case>IMAGACT</fixed-case> Visual Ontology. An Extendable Multilingual Infrastructure for the representation of lexical encoding of Action</title>
      <pages>3425–3432</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/318_Paper.pdf</url>
      <abstract>Action verbs have many meanings, covering actions in different ontological types. Moreover, each language categorizes action in its own way. One verb can refer to many different actions and one action can be identified by more than one verb. The range of variations within and across languages is largely unknown, causing trouble for natural language processing tasks. IMAGACT is a corpus-based ontology of action concepts, derived from English and Italian spontaneous speech corpora, which makes use of the universal language of images to identify the different action types extended by verbs referring to action in English, Italian, Chinese and Spanish. This paper presents the infrastructure and the various linguistic information the user can derive from it. IMAGACT makes explicit the variation of meaning of action verbs within one language and allows comparisons of verb variations within and across languages. Because the action concepts are represented with videos, extension into new languages beyond those presently implemented in IMAGACT is done using competence-based judgments by mother-tongue informants without intense lexicographic work involving underdetermined semantic description</abstract>
    </paper>
    <paper id="282">
      <author><first>Martin</first><last>Benjamin</last></author>
      <title>Collaboration in the Production of a Massively Multilingual Lexicon</title>
      <pages>211–215</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/319_Paper.pdf</url>
      <abstract>This paper discusses the multiple approaches to collaboration that the Kamusi Project is employing in the creation of a massively multilingual lexical resource. The projects data structure enables the inclusion of large amounts of rich data within each sense-specific entry, with transitive concept-based links across languages. Data collection involves mining existing data sets, language experts using an online editing system, crowdsourcing, and games with a purpose. The paper discusses the benefits and drawbacks of each of these elements, and the steps the project is taking to account for those. Special attention is paid to guiding crowd members with targeted questions that produce results in a specific format. Collaboration is seen as an essential method for generating large amounts of linguistic data, as well as for validating the data so it can be considered trustworthy.</abstract>
    </paper>
    <paper id="283">
      <author><first>François</first><last>Salmon</last></author>
      <author><first>Félicien</first><last>Vallet</last></author>
      <title>An Effortless Way To Create Large-Scale Datasets For Famous Speakers</title>
      <pages>348–352</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/32_Paper.pdf</url>
      <abstract>The creation of large-scale multimedia datasets has become a scientific matter in itself. Indeed, the fully-manual annotation of hundreds or thousands of hours of video and/or audio turns out to be practically infeasible. In this paper, we propose an extremly handy approach to automatically construct a database of famous speakers from TV broadcast news material. We then run a user experiment with a correctly designed tool that demonstrates that very reliable results can be obtained with this method. In particular, a thorough error analysis demonstrates the value of the approach and provides hints for the improvement of the quality of the dataset.</abstract>
    </paper>
    <paper id="284">
      <author><first>Bogdan</first><last>Ludusan</last></author>
      <author><first>Maarten</first><last>Versteegh</last></author>
      <author><first>Aren</first><last>Jansen</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Xuan-Nga</first><last>Cao</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <title>Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems</title>
      <pages>560–567</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/320_Paper.pdf</url>
      <abstract>The unsupervised discovery of linguistic terms from either continuous phoneme transcriptions or from raw speech has seen an increasing interest in the past years both from a theoretical and a practical standpoint. Yet, there exists no common accepted evaluation method for the systems performing term discovery. Here, we propose such an evaluation toolbox, drawing ideas from both speech technology and natural language processing. We first transform the speech-based output into a symbolic representation and compute five types of evaluation metrics on this representation: the quality of acoustic matching, the quality of the clusters found, and the quality of the alignment with real words (type, token, and boundary scores). We tested our approach on two term discovery systems taking speech as input, and one using symbolic input. The latter was run using both the gold transcription and a transcription obtained from an automatic speech recognizer, in order to simulate the case when only imperfect symbolic information is available. The results obtained are analysed through the use of the proposed evaluation metrics and the implications of these metrics are discussed.</abstract>
    </paper>
    <paper id="285">
      <author><first>Dietmar</first><last>Rösner</last></author>
      <author><first>Rafael</first><last>Friesen</last></author>
      <author><first>Stephan</first><last>Günther</last></author>
      <author><first>Rico</first><last>Andrich</last></author>
      <title>Modeling and evaluating dialog success in the <fixed-case>LAST</fixed-case> <fixed-case>MINUTE</fixed-case> corpus</title>
      <pages>259–265</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/321_Paper.pdf</url>
      <abstract>The LAST MINUTE corpus comprises records and transcripts of naturalistic problem solving dialogs between N = 130 subjects and a companion system simulated in a Wizard of Oz experiment. Our goal is to detect dialog situations where subjects might break up the dialog with the system which might happen when the subject is unsuccessful. We present a dialog act based representation of the dialog courses in the problem solving phase of the experiment and propose and evaluate measures for dialog success or failure derived from this representation. This dialog act representation refines our previous coarse measure as it enables the correct classification of many dialog sequences that were ambiguous before. The dialog act representation is useful for the identification of different subject groups and the exploration of interesting dialog courses in the corpus. We find young females to be most successful in the challenging last part of the problem solving phase and young subjects to have the initiative in the dialog more often than the elderly.</abstract>
    </paper>
    <paper id="286">
      <author><first>Maxim</first><last>Sidorov</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <author><first>Alexander</first><last>Schmitt</last></author>
      <title>Comparison of Gender- and Speaker-adaptive Emotion Recognition</title>
      <pages>3476–3480</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/322_Paper.pdf</url>
      <abstract>Deriving the emotion of a human speaker is a hard task, especially if only the audio stream is taken into account. While state-of-the-art approaches already provide good results, adaptive methods have been proposed in order to further improve the recognition accuracy. A recent approach is to add characteristics of the speaker, e.g., the gender of the speaker. In this contribution, we argue that adding information unique for each speaker, i.e., by using speaker identification techniques, improves emotion recognition simply by adding this additional information to the feature vector of the statistical classification algorithm. Moreover, we compare this approach to emotion recognition adding only the speaker gender being a non-unique speaker attribute. We justify this by performing adaptive emotion recognition using both gender and speaker information on four different corpora of different languages containing acted and non-acted speech. The final results show that adding speaker information significantly outperforms both adding gender information and solely using a generic speaker-independent approach.</abstract>
    </paper>
    <paper id="287">
      <author><first>Siân</first><last>Alsop</last></author>
      <author><first>Hilary</first><last>Nesi</last></author>
      <title>The pragmatic annotation of a corpus of academic lectures</title>
      <pages>1560–1563</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/323_Paper.pdf</url>
      <abstract>This paper will describe a process of pragmatic annotation (c.f. Simpson-Vlach and Leicher 2006) which systematically identifies pragmatic meaning in spoken text. The annotation of stretches of text that perform particular pragmatic functions allows conclusions to be drawn across data sets at a different level than that of the individual lexical item, or structural content. The annotation of linguistic features, which cannot be identified by purely objective means, is distinguished here from structural mark-up of speaker identity, turns, pauses etc. The features annotated are explaining, housekeeping, humour, storytelling and summarising. Twenty-two subcategories are attributed to these elements. Data is from the Engineering Lecture Corpus (ELC), which includes 76 English-medium engineering lectures from the UK, New Zealand and Malaysia. The annotation allows us to compare differences in the use of these discourse features across cultural subcorpora. Results show that cultural context does impact on the linguistic realisation of commonly occurring discourse features in engineering lectures.</abstract>
    </paper>
    <paper id="288">
      <author><first>Dorte Haltrup</first><last>Hansen</last></author>
      <author><first>Lene</first><last>Offersgaard</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <title>Using <fixed-case>TEI</fixed-case>, <fixed-case>CMDI</fixed-case> and <fixed-case>ISO</fixed-case>cat in <fixed-case>CLARIN</fixed-case>-<fixed-case>DK</fixed-case></title>
      <pages>613–618</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/325_Paper.pdf</url>
      <abstract>This paper presents the challenges and issues encountered in the conversion of TEI header metadata into the CMDI format. The work is carried out in the Danish research infrastructure, CLARIN-DK, in order to enable the exchange of language resources nationally as well as internationally, in particular with other partners of CLARIN ERIC. The paper describes the task of converting an existing TEI specification applied to all the text resources deposited in DK-CLARIN. During the task we have tried to reuse and share CMDI profiles and components in the CLARIN Component Registry, as well as linking the CMDI components and elements to the relevant data categories in the ISOcat Data Category Registry. The conversion of the existing metadata into the CMDI format turned out not to be a trivial task and the experience and insights gained from this work have resulted in a proposal for a work flow for future use. We also present a core TEI header metadata set.</abstract>
    </paper>
    <paper id="289">
      <author><first>Sabrina</first><last>Campano</last></author>
      <author><first>Jessica</first><last>Durand</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <title>Comparative analysis of verbal alignment in human-human and human-agent interactions</title>
      <pages>4415–4422</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/327_Paper.pdf</url>
      <abstract>Engagement is an important feature in human-human and human-agent interaction. In this paper, we investigate lexical alignment as a cue of engagement, relying on two different corpora : CID and SEMAINE. Our final goal is to build a virtual conversational character that could use alignment strategies to maintain user’s engagement. To do so, we investigate two alignment processes : shared vocabulary and other-repetitions. A quantitative and qualitative approach is proposed to characterize these aspects in human-human (CID) and human-operator (SEMAINE) interactions. Our results show that these processes are observable in both corpora, indicating a stable pattern that can be further modelled in conversational agents.</abstract>
    </paper>
    <paper id="290">
      <author><first>Mircea</first><last>Petic</last></author>
      <author><first>Daniela</first><last>Gîfu</last></author>
      <title>Transliteration and alignment of parallel texts from <fixed-case>C</fixed-case>yrillic to <fixed-case>L</fixed-case>atin</title>
      <pages>1819–1823</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/328_Paper.pdf</url>
      <abstract>This article describes a methodology of recovering and preservation of old Romanian texts and problems related to their recognition. Our focus is to create a gold corpus for Romanian language (the novella Sania), for both alphabets used in Transnistria ― Cyrillic and Latin. The resource is available for similar researches. This technology is based on transliteration and semiautomatic alignment of parallel texts at the level of letter/lexem/multiwords. We have analysed every text segment present in this corpus and discovered other conventions of writing at the level of transliteration, academic norms and editorial interventions. These conventions allowed us to elaborate and implement some new heuristics that make a correct automatic transliteration process. Sometimes the words of Latin script are modified in Cyrillic script from semantic reasons (for instance, editor’s interpretation). Semantic transliteration is seen as a good practice in introducing multiwords from Cyrillic to Latin. Not only does it preserve how a multiwords sound in the source script, but also enables the translator to modify in the original text (here, choosing the most common sense of an expression). Such a technology could be of interest to lexicographers, but also to specialists in computational linguistics to improve the actual transliteration standards.</abstract>
    </paper>
    <paper id="291">
      <author><first>Corina</first><last>Dima</last></author>
      <author><first>Verena</first><last>Henrich</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <author><first>Christina</first><last>Hoppermann</last></author>
      <title>How to Tell a Schneemann from a Milchmann: An Annotation Scheme for Compound-Internal Relations</title>
      <pages>1194–1201</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/329_Paper.pdf</url>
      <abstract>This paper presents a language-independent annotation scheme for the semantic relations that link the constituents of noun-noun compounds, such as Schneemann ‘snow man’ or Milchmann ‘milk man’. The annotation scheme is hybrid in the sense that it assigns each compound a two-place label consisting of a semantic property and a prepositional paraphrase. The resulting inventory combines the insights of previous annotation schemes that rely exclusively on either semantic properties or prepositions, thus avoiding the known weaknesses that result from using only one of the two label types. The proposed annotation scheme has been used to annotate a set of 5112 German noun-noun compounds. A release of the dataset is currently being prepared and will be made available via the CLARIN Center Tübingen. In addition to the presentation of the hybrid annotation scheme, the paper also reports on an inter-annotator agreement study that has resulted in a substantial agreement among annotators.</abstract>
    </paper>
    <paper id="292">
      <author><first>Susana</first><last>Bautista</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <title>Can Numerical Expressions Be Simpler? Implementation and Demostration of a Numerical Simplification System for <fixed-case>S</fixed-case>panish</title>
      <pages>956–962</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/330_Paper.pdf</url>
      <abstract>Information in newspapers is often showed in the form of numerical expressions which present comprehension problems for many people, including people with disabilities, illiteracy or lack of access to advanced technology. The purpose of this paper is to motivate, describe, and demonstrate a rule-based lexical component that simplifies numerical expressions in Spanish texts. We propose an approach that makes news articles more accessible to certain readers by rewriting difficult numerical expressions in a simpler way. We will showcase the numerical simplification system with a live demo based on the execution of our components over different texts, and which will consider both successful and unsuccessful simplification cases.</abstract>
    </paper>
    <paper id="293">
      <author><first>Anita</first><last>Rácz</last></author>
      <author><first>István</first><last>Nagy T.</last></author>
      <author><first>Veronika</first><last>Vincze</last></author>
      <title>4<fixed-case>FX</fixed-case>: Light Verb Constructions in a Multilingual Parallel Corpus</title>
      <pages>710–715</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/331_Paper.pdf</url>
      <abstract>In this paper, we describe 4FX, a quadrilingual (English-Spanish-German-Hungarian) parallel corpus annotated for light verb constructions. We present the annotation process, and report statistical data on the frequency of LVCs in each language. We also offer inter-annotator agreement rates and we highlight some interesting facts and tendencies on the basis of comparing multilingual data from the four corpora. According to the frequency of LVC categories and the calculated Kendalls coefficient for the four corpora, we found that Spanish and German are very similar to each other, Hungarian is also similar to both, but German differs from all these three. The qualitative and quantitative data analysis might prove useful in theoretical linguistic research for all the four languages. Moreover, the corpus will be an excellent testbed for the development and evaluation of machine learning based methods aiming at extracting or identifying light verb constructions in these four languages.</abstract>
    </paper>
    <paper id="294">
      <author><first>Fritz</first><last>Kliche</last></author>
      <author><first>André</first><last>Blessing</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Jonathan</first><last>Sonntag</last></author>
      <title>The e<fixed-case>I</fixed-case>dentity Text Exploration Workbench</title>
      <pages>691–697</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/332_Paper.pdf</url>
      <abstract>We work on tools to explore text contents and metadata of newspaper articles as provided by news archives. Our tool components are being integrated into an “Exploration Workbench” for Digital Humanities researchers. Next to the conversion of different data formats and character encodings, a prominent feature of our design is its “Wizard” function for corpus building: Researchers import raw data and define patterns to extract text contents and metadata. The Workbench also comprises different tools for data cleaning. These include filtering of off-topic articles, duplicates and near-duplicates, corrupted and empty articles. We currently work on ca. 860.000 newspaper articles from different media archives, provided in different data formats. We index the data with state-of-the-art systems to allow for large scale information retrieval. We extract metadata on publishing dates, author names, newspaper sections, etc., and split articles into segments such as headlines, subtitles, paragraphs, etc. After cleaning the data and compiling a thematically homogeneous corpus, the sample can be used for quantitative analyses which are not affected by noise. Users can retrieve sets of articles on different topics, issues or otherwise defined research questions (“subcorpora”) and investigate quantitatively their media attention on the timeline (“Issue Cycles”).</abstract>
    </paper>
    <paper id="295">
      <author><first>Nesrine</first><last>Fourati</last></author>
      <author><first>Catherine</first><last>Pelachaud</last></author>
      <title><fixed-case>E</fixed-case>milya: Emotional body expression in daily actions database</title>
      <pages>3486–3493</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/334_Paper.pdf</url>
      <abstract>The studies of bodily expression of emotion have been so far mostly focused on body movement patterns associated with emotional expression. Recently, there is an increasing interest on the expression of emotion in daily actions, called also non-emblematic movements (such as walking or knocking at the door). Previous studies were based on database limited to a small range of movement tasks or emotional states. In this paper, we describe our new database of emotional body expression in daily actions, where 11 actors express 8 emotions in 7 actions. We use motion capture technology to record body movements, but we recorded as well synchronized audio-visual data to enlarge the use of the database for different research purposes. We investigate also the matching between the expressed emotions and the perceived ones through a perceptive study. The first results of this study are discussed in this paper.</abstract>
    </paper>
    <paper id="296">
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <title>Using Stem-Templates to Improve <fixed-case>A</fixed-case>rabic <fixed-case>POS</fixed-case> and Gender/Number Tagging</title>
      <pages>2926–2931</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/335_Paper.pdf</url>
      <abstract>This paper presents an end-to-end automatic processing system for Arabic. The system performs: correction of common spelling errors pertaining to different forms of alef, ta marbouta and ha, and alef maqsoura and ya; context sensitive word segmentation into underlying clitics, POS tagging, and gender and number tagging of nouns and adjectives. We introduce the use of stem templates as a feature to improve POS tagging by 0.5\% and to help ascertain the gender and number of nouns and adjectives. For gender and number tagging, we report accuracies that are significantly higher on previously unseen words compared to a state-of-the-art system.</abstract>
    </paper>
    <paper id="297">
      <author><first>Shaoda</first><last>He</last></author>
      <author><first>Xiaojun</first><last>Zou</last></author>
      <author><first>Liumingjing</first><last>Xiao</last></author>
      <author><first>Junfeng</first><last>Hu</last></author>
      <title>Construction of Diachronic Ontologies from People’s Daily of Fifty Years</title>
      <pages>3258–3263</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/337_Paper.pdf</url>
      <abstract>This paper presents an Ontology Learning From Text (OLFT) method follows the well-known OLFT cake layer framework. Based on the distributional similarity, the proposed method generates multi-level ontologies from comparatively small corpora with the aid of HITS algorithm. Currently, this method covers terms extraction, synonyms recognition, concepts discovery and concepts hierarchical clustering. Among them, both concepts discovery and concepts hierarchical clustering are aided by the HITS authority, which is obtained from the HITS algorithm by an iteratively recommended way. With this method, a set of diachronic ontologies is constructed for each year based on People’s Daily corpora of fifty years (i.e., from 1947 to 1996). Preliminary experiments show that our algorithm outperforms the Google’s RNN and K-means based algorithm in both concepts discovery and concepts hierarchical clustering.</abstract>
    </paper>
    <paper id="298">
      <author><first>Jonathan</first><last>Chevelu</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <title><fixed-case>ROOTS</fixed-case>: a toolkit for easy, fast and consistent processing of large sequential annotated data collections</title>
      <pages>619–626</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/338_Paper.pdf</url>
      <abstract>The development of new methods for given speech and natural language processing tasks usually consists in annotating large corpora of data before applying machine learning techniques to train models or to extract information. Beyond scientific aspects, creating and managing such annotated data sets is a recurrent problem. While using human annotators is obviously expensive in time and money, relying on automatic annotation processes is not a simple solution neither. Typically, the high diversity of annotation tools and of data formats, as well as the lack of efficient middleware to interface them all together, make such processes very complex and painful to design. To circumvent this problem, this paper presents the toolkit ROOTS, a freshly released open source toolkit (http://roots-toolkit.gforge.inria.fr) for easy, fast and consistent management of heterogeneously annotated data. ROOTS is designed to efficiently handle massive complex sequential data and to allow quick and light prototyping, as this is often required for research purposes. To illustrate these properties, three sample applications are presented in the field of speech and language processing, though ROOTS can more generally be easily extended to other application domains.</abstract>
    </paper>
    <paper id="299">
      <author><first>Martin</first><last>Jansche</last></author>
      <title>Computer-Aided Quality Assurance of an <fixed-case>I</fixed-case>celandic Pronunciation Dictionary</title>
      <pages>2111–2114</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/339_Paper.pdf</url>
      <abstract>We propose a model-driven method for ensuring the quality of pronunciation dictionaries. The key ingredient is computing an alignment between letter strings and phoneme strings, a standard technique in pronunciation modeling. The novel aspect of our method is the use of informative, parametric alignment models which are refined iteratively as they are tested against the data. We discuss the use of alignment failures as a signal for detecting and correcting problematic dictionary entries. We illustrate this method using an existing pronunciation dictionary for Icelandic. Our method is completely general and has been applied in the construction of pronunciation dictionaries for commercially deployed speech recognition systems in several languages.</abstract>
    </paper>
    <paper id="300">
      <author><first>Ismail</first><last>El Maarouf</last></author>
      <author><first>Jane</first><last>Bradbury</last></author>
      <author><first>Vít</first><last>Baisa</last></author>
      <author><first>Patrick</first><last>Hanks</last></author>
      <title>Disambiguating Verbs by Collocation: Corpus Lexicography meets Natural Language Processing</title>
      <pages>1001–1006</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/34_Paper.pdf</url>
      <abstract>This paper reports the results of Natural Language Processing (NLP) experiments in semantic parsing, based on a new semantic resource, the Pattern Dictionary of English Verbs (PDEV) (Hanks, 2013). This work is set in the DVC (Disambiguating Verbs by Collocation) project , a project in Corpus Lexicography aimed at expanding PDEV to a large scale. This project springs from a long-term collaboration of lexicographers with computer scientists which has given rise to the design and maintenance of specific, adapted, and user-friendly editing and exploration tools. Particular attention is drawn on the use of NLP deep semantic methods to help in data processing. Possible contributions of NLP include pattern disambiguation, the focus of this article. The present article explains how PDEV differs from other lexical resources and describes its structure in detail. It also presents new classification experiments on a subset of 25 verbs. The SVM model obtained a micro-average F1 score of 0.81.</abstract>
    </paper>
    <paper id="301">
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>János</first><last>Zsibrita</last></author>
      <author><first>Péter</first><last>Durst</last></author>
      <author><first>Martina Katalin</first><last>Szabó</last></author>
      <title>Automatic Error Detection concerning the Definite and Indefinite Conjugation in the <fixed-case>H</fixed-case>un<fixed-case>L</fixed-case>earner Corpus</title>
      <pages>3958–3962</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/340_Paper.pdf</url>
      <abstract>In this paper we present the results of automatic error detection, concerning the definite and indefinite conjugation in the extended version of the HunLearner corpus, the learners corpus of the Hungarian language. We present the most typical structures that trigger definite or indefinite conjugation in Hungarian and we also discuss the most frequent types of errors made by language learners in the corpus texts. We also illustrate the error types with sentences taken from the corpus. Our results highlight grammatical structures that might pose problems for learners of Hungarian, which can be fruitfully applied in the teaching and practicing of such constructions from the language teachers or learners point of view. On the other hand, these results may be exploited in extending the functionalities of a grammar checker, concerning the definiteness of the verb. Our automatic system was able to achieve perfect recall, i.e. it could find all the mismatches between the type of the object and the conjugation of the verb, which is promising for future studies in this area.</abstract>
    </paper>
    <paper id="302">
      <author><first>Maxim</first><last>Sidorov</last></author>
      <author><first>Christina</first><last>Brester</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Eugene</first><last>Semenkin</last></author>
      <title>Speech-Based Emotion Recognition: Feature Selection by Self-Adaptive Multi-Criteria Genetic Algorithm</title>
      <pages>3481–3485</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/341_Paper.pdf</url>
      <abstract>Automated emotion recognition has a number of applications in Interactive Voice Response systems, call centers, etc. While employing existing feature sets and methods for automated emotion recognition has already achieved reasonable results, there is still a lot to do for improvement. Meanwhile, an optimal feature set, which should be used to represent speech signals for performing speech-based emotion recognition techniques, is still an open question. In our research, we tried to figure out the most essential features with self-adaptive multi-objective genetic algorithm as a feature selection technique and a probabilistic neural network as a classifier. The proposed approach was evaluated using a number of multi-languages databases (English, German), which were represented by 37- and 384-dimensional feature sets. According to the obtained results, the developed technique allows to increase the emotion recognition performance by up to 26.08 relative improvement in accuracy. Moreover, emotion recognition performance scores for all applied databases are improved.</abstract>
    </paper>
    <paper id="303">
      <author><first>Stefan</first><last>Höfler</last></author>
      <author><first>Kyoko</first><last>Sugisaki</last></author>
      <title>Constructing and exploiting an automatically annotated resource of legislative texts</title>
      <pages>175–180</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/345_Paper.pdf</url>
      <abstract>In this paper, we report on the construction of a resource of Swiss legislative texts that is automatically annotated with structural, morphosyntactic and content-related information, and we discuss the exploitation of this resource for the purposes of legislative drafting, legal linguistics and translation and for the evaluation of legislation. Our resource is based on the classified compilation of Swiss federal legislation. All texts contained in the classified compilation exist in German, French and Italian, some of them are also available in Romansh and English. Our resource is currently being exploited (a) as a testing environment for developing methods of automated style checking for legislative drafts, (b) as the basis of a statistical multilingual word concordance, and (c) for the empirical evaluation of legislation. The paper describes the domain- and language-specific procedures that we have implemented to provide the automatic annotations needed for these applications.</abstract>
    </paper>
    <paper id="304">
      <author><first>Roman</first><last>Schneider</last></author>
      <title><fixed-case>G</fixed-case>enitiv<fixed-case>DB</fixed-case> — a Corpus-Generated Database for <fixed-case>G</fixed-case>erman Genitive Classification</title>
      <pages>988–994</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/346_Paper.pdf</url>
      <abstract>We present a novel NLP resource for the explanation of linguistic phenomena, built and evaluated exploring very large annotated language corpora. For the compilation, we use the German Reference Corpus (DeReKo) with more than 5 billion word forms, which is the largest linguistic resource worldwide for the study of contemporary written German. The result is a comprehensive database of German genitive formations, enriched with a broad range of intra- und extralinguistic metadata. It can be used for the notoriously controversial classification and prediction of genitive endings (short endings, long endings, zero-marker). We also evaluate the main factors influencing the use of specific endings. To get a general idea about a factors influences and its side effects, we calculate chi-square-tests and visualize the residuals with an association plot. The results are evaluated against a gold standard by implementing tree-based machine learning algorithms. For the statistical analysis, we applied the supervised LMT Logistic Model Trees algorithm, using the WEKA software. We intend to use this gold standard to evaluate GenitivDB, as well as to explore methodologies for a predictive genitive model.</abstract>
    </paper>
    <paper id="305">
      <author><first>Jana</first><last>Šindlerová</last></author>
      <author><first>Zdeňka</first><last>Urešová</last></author>
      <author><first>Eva</first><last>Fucikova</last></author>
      <title>Resources in Conflict: A Bilingual Valency Lexicon vs. a Bilingual Treebank vs. a Linguistic Theory</title>
      <pages>2490–2494</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/349_Paper.pdf</url>
      <abstract>In this paper, we would like to exemplify how a syntactically annotated bilingual treebank can help us in exploring and revising a developed linguistic theory. On the material of the Prague Czech-English Dependency Treebank we observe sentences in which an Addressee argument in one language is linked translationally to a Patient argument in the other one, and make generalizations about the theoretical grounds of the argument non-correspondences and its relations to the valency theory beyond the annotation practice. Exploring verbs of three semantic classes (Judgement verbs, Teaching verbs and Attempt Suasion verbs) we claim that the Functional Generative Description argument labelling is highly dependent on the morphosyntactic realization of the individual participants, which then results in valency frame differences. Nevertheless, most of the differences can be overcome without substantial changes to the linguistic theory itself.</abstract>
    </paper>
    <paper id="306">
      <author><first>Roser</first><last>Saurí</last></author>
      <author><first>Judith</first><last>Domingo</last></author>
      <author><first>Toni</first><last>Badia</last></author>
      <title>The <fixed-case>N</fixed-case>ew<fixed-case>S</fixed-case>o<fixed-case>M</fixed-case>e Corpus: A Unifying Opinion Annotation Framework across Genres and in Multiple Languages</title>
      <pages>2229–2236</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/350_Paper.pdf</url>
      <abstract>We present the NewSoMe (News and Social Media) Corpus, a set of subcorpora with annotations on opinion expressions across genres (news reports, blogs, product reviews and tweets) and covering multiple languages (English, Spanish, Catalan and Portuguese). NewSoMe is the result of an effort to increase the opinion corpus resources available in languages other than English, and to build a unifying annotation framework for analyzing opinion in different genres, including controlled text, such as news reports, as well as different types of user generated contents (UGC). Given the broad design of the resource, most of the annotation effort were carried out resorting to crowdsourcing platforms: Amazon Mechanical Turk and CrowdFlower. This created an excellent opportunity to research on the feasibility of crowdsourcing methods for annotating big amounts of text in different languages.</abstract>
    </paper>
    <paper id="307">
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Yuchen</first><last>Zhang</last></author>
      <title>Buy one get one free: Distant annotation of <fixed-case>C</fixed-case>hinese tense, event type and modality</title>
      <pages>1412–1416</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/353_Paper.pdf</url>
      <abstract>We describe a “distant annotation” method where we mark up the semantic tense, event type, and modality of Chinese events via a word-aligned parallel corpus. We first map Chinese verbs to their English counterparts via word alignment, and then annotate the resulting English text spans with coarse-grained categories for semantic tense, event type, and modality that we believe apply to both English and Chinese. Because English has richer morpho-syntactic indicators for semantic tense, event type and modality than Chinese, our intuition is that this distant annotation approach will yield more consistent annotation than if we annotate the Chinese side directly. We report experimental results that show stable annotation agreement statistics and that event type and modality have significant influence on tense prediction. We also report the size of the annotated corpus that we have obtained, and how different domains impact annotation consistency.</abstract>
    </paper>
    <paper id="308">
      <author><first>Kodai</first><last>Takahashi</last></author>
      <author><first>Masashi</first><last>Inoue</last></author>
      <title>Multimodal dialogue segmentation with gesture post-processing</title>
      <pages>3433–3437</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/354_Paper.pdf</url>
      <abstract>We investigate an automatic dialogue segmentation method using both verbal and non-verbal modalities. Dialogue contents are used for the initial segmentation of dialogue; then, gesture occurrences are used to remove the incorrect segment boundaries. A unique characteristic of our method is to use verbal and non-verbal information separately. We use a three-party dialogue that is rich in gesture as data. The transcription of the dialogue is segmented into topics without prior training by using the TextTiling and U00 algorithm. Some candidates for segment boundaries - where the topic continues - are irrelevant. Those boundaries can be found and removed by locating gestures that stretch over the boundary candidates. This ltering improves the segmentation accuracy of text-only segmentation.</abstract>
    </paper>
    <paper id="309">
      <author><first>André</first><last>Bittar</last></author>
      <author><first>Luca</first><last>Dini</last></author>
      <author><first>Sigrid</first><last>Maurel</last></author>
      <author><first>Mathieu</first><last>Ruhlmann</last></author>
      <title>The Dangerous Myth of the Star System</title>
      <pages>2237–2241</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/356_Paper.pdf</url>
      <abstract>In recent years we have observed two parallel trends in computational linguistics research and e-commerce development. On the research side, there has been an increasing interest in algorithms and approaches that are able to capture the polarity of opinions expressed by users on products, institutions and services. On the other hand, almost all big e-commerce and aggregator sites are by now providing users the possibility of writing comments and expressing their appreciation with a numeric score (usually represented as a number of stars). This generates the impression that the work carried out in the research community is made partially useless (at least for economic exploitation) by an evolution in web practices. In this paper we describe an experiment on a large corpus which shows that the score judgments provided by users are often conflicting with the text contained in the opinion, and to such a point that a rule-based opinion mining system can be demonstrated to perform better than the users themselves in ranking their opinions.</abstract>
    </paper>
    <paper id="310">
      <author><first>Haibo</first><last>Li</last></author>
      <author><first>Masato</first><last>Hagiwara</last></author>
      <author><first>Qi</first><last>Li</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <title>Comparison of the Impact of Word Segmentation on Name Tagging for <fixed-case>C</fixed-case>hinese and <fixed-case>J</fixed-case>apanese</title>
      <pages>2532–2536</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/358_Paper.pdf</url>
      <abstract>Word Segmentation is usually considered an essential step for many Chinese and Japanese Natural Language Processing tasks, such as name tagging. This paper presents several new observations and analysis on the impact of word segmentation on name tagging; (1). Due to the limitation of current state-of-the-art Chinese word segmentation performance, a character-based name tagger can outperform its word-based counterparts for Chinese but not for Japanese; (2). It is crucial to keep segmentation settings (e.g. definitions, specifications, methods) consistent between training and testing for name tagging; (3). As long as (2) is ensured, the performance of word segmentation does not have appreciable impact on Chinese and Japanese name tagging.</abstract>
    </paper>
    <paper id="311">
      <author><first>Verginica Barbu</first><last>Mititelu</last></author>
      <author><first>Elena</first><last>Irimia</last></author>
      <author><first>Dan</first><last>Tufiș</last></author>
      <title><fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>o<fixed-case>L</fixed-case>a — The Reference Corpus of Contemporary <fixed-case>R</fixed-case>omanian Language</title>
      <pages>1235–1239</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/360_Paper.pdf</url>
      <abstract>We present the project of creating CoRoLa, a reference corpus of contemporary Romanian (from 1945 onwards). In the international context, the project finds its place among the initiatives of gathering huge collections of texts, of pre-processing and annotating them at several levels, and also of documenting them with metadata (CMDI). Our project is a joined effort of two institutes of the Romanian Academy. We foresee a corpus of more than 500 million word forms, covering all functional styles of the language. Although the vast majority of texts will be in written form, we target about 300 hours of oral texts, too, obligatorily with associated transcripts. Most of the texts will be from books, while the rest will be harvested from newspapers, booklets, technical reports, etc. The pre-processing includes cleaning the data and harmonising the diacritics, sentence splitting and tokenization. Annotation will be done at a morphological level in a first stage, followed by lemmatization, with the possibility of adding syntactic, semantic and discourse annotation in a later stage. A core of CoRoLa is described in the article. The target users of our corpus will be researchers in linguistics and language processing, teachers of Romanian, students.</abstract>
    </paper>
    <paper id="312">
      <author><first>Tibor</first><last>Kiss</last></author>
      <author><first>Francis Jeffry</first><last>Pelletier</last></author>
      <author><first>Tobias</first><last>Stadtfeld</last></author>
      <title>Building a reference lexicon for countability in <fixed-case>E</fixed-case>nglish</title>
      <pages>995–1000</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/361_Paper.pdf</url>
      <abstract>The present paper describes the construction of a resource to determine the lexical preference class of a large number of English noun-senses ($\approx$ 14,000) with respect to the distinction between mass and count interpretations. In constructing the lexicon, we have employed a questionnaire-based approach based on existing resources such as the Open ANC (\url{http://www.anc.org}) and WordNet \cite{Miller95}. The questionnaire requires annotators to answer six questions about a noun-sense pair. Depending on the answers, a given noun-sense pair can be assigned to fine-grained noun classes, spanning the area between count and mass. The reference lexicon contains almost 14,000 noun-sense pairs. An initial data set of 1,000 has been annotated together by four native speakers, while the remaining 12,800 noun-sense pairs have been annotated in parallel by two annotators each. We can confirm the general feasibility of the approach by reporting satisfactory values between 0.694 and 0.755 in inter-annotator agreement using Krippendorff’s $\alpha$.</abstract>
    </paper>
    <paper id="313">
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <title>The <fixed-case>LIMA</fixed-case> Multilingual Analyzer Made Free: <fixed-case>FLOSS</fixed-case> Resources Adaptation and Correction</title>
      <pages>2932–2937</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/362_Paper.pdf</url>
      <abstract>At CEA LIST, we have decided to release our multilingual analyzer LIMA as Free software. As we were not proprietary of all the language resources used we had to select and adapt free ones in order to attain results good enough and equivalent to those obtained with our previous ones. For English and French, we found and adapted a full-form dictionary and an annotated corpus for learning part-of-speech tagging models.</abstract>
    </paper>
    <paper id="314">
      <author><first>Marco</first><last>Marelli</last></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <author><first>Roberto</first><last>Zamparelli</last></author>
      <title>A <fixed-case>SICK</fixed-case> cure for the evaluation of compositional distributional semantic models</title>
      <pages>216–223</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf</url>
      <abstract>Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes.</abstract>
    </paper>
    <paper id="315">
      <author><first>Bayu</first><last>Rahayudi</last></author>
      <author><first>Ronald</first><last>Poppe</last></author>
      <author><first>Dirk</first><last>Heylen</last></author>
      <title>Twente Debate Corpus — A Multimodal Corpus for Head Movement Analysis</title>
      <pages>4184–4188</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/364_Paper.pdf</url>
      <abstract>This paper introduces a multimodal discussion corpus for the study into head movement and turn-taking patterns in debates. Given that participants either acted alone or in a pair, cooperation and competition and their nonverbal correlates can be analyzed. In addition to the video and audio of the recordings, the corpus contains automatically estimated head movements, and manual annotations of who is speaking and who is looking where. The corpus consists of over 2 hours of debates, in 6 groups with 18 participants in total. We describe the recording setup and present initial analyses of the recorded data. We found that the person who acted as single debater speaks more and also receives more attention compared to the other debaters, also when corrected for the time speaking. We also found that a single debater was more likely to speak after a team debater. Future work will be aimed at further analysis of the relation between speaking and looking patterns, the outcome of the debate and perceived dominance of the debaters.</abstract>
    </paper>
    <paper id="316">
      <author><first>Annika</first><last>Hämäläinen</last></author>
      <author><first>Jairo</first><last>Avelar</last></author>
      <author><first>Silvia</first><last>Rodrigues</last></author>
      <author><first>Miguel Sales</first><last>Dias</last></author>
      <author><first>Artur</first><last>Kolesiński</last></author>
      <author><first>Tibor</first><last>Fegyó</last></author>
      <author><first>Géza</first><last>Németh</last></author>
      <author><first>Petra</first><last>Csobánka</last></author>
      <author><first>Karine</first><last>Lan</last></author>
      <author><first>David</first><last>Hewson</last></author>
      <title>The <fixed-case>EASR</fixed-case> Corpora of <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese, <fixed-case>F</fixed-case>rench, <fixed-case>H</fixed-case>ungarian and <fixed-case>P</fixed-case>olish Elderly Speech</title>
      <pages>1458–1464</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/365_Paper.pdf</url>
      <abstract>Currently available speech recognisers do not usually work well with elderly speech. This is because several characteristics of speech (e.g. fundamental frequency, jitter, shimmer and harmonic noise ratio) change with age and because the acoustic models used by speech recognisers are typically trained with speech collected from younger adults only. To develop speech-driven applications capable of successfully recognising elderly speech, this type of speech data is needed for training acoustic models from scratch or for adapting acoustic models trained with younger adults speech. However, the availability of suitable elderly speech corpora is still very limited. This paper describes an ongoing project to design, collect, transcribe and annotate large elderly speech corpora for four European languages: Portuguese, French, Hungarian and Polish. The Portuguese, French and Polish corpora contain read speech only, whereas the Hungarian corpus also contains spontaneous command and control type of speech. Depending on the language in question, the corpora contain 76 to 205 hours of speech collected from 328 to 986 speakers aged 60 and over. The final corpora will come with manually verified orthographic transcriptions, as well as annotations for filled pauses, noises and damaged words.</abstract>
    </paper>
    <paper id="317">
      <author><first>Matteo</first><last>Abrate</last></author>
      <author><first>Angelo Mario</first><last>Del Grosso</last></author>
      <author><first>Emiliano</first><last>Giovannetti</last></author>
      <author><first>Angelica Lo</first><last>Duca</last></author>
      <author><first>Damiana</first><last>Luzzi</last></author>
      <author><first>Lorenzo</first><last>Mancini</last></author>
      <author><first>Andrea</first><last>Marchetti</last></author>
      <author><first>Irene</first><last>Pedretti</last></author>
      <author><first>Silvia</first><last>Piccini</last></author>
      <title>Sharing Cultural Heritage: the Clavius on the Web Project</title>
      <pages>627–634</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/368_Paper.pdf</url>
      <abstract>In the last few years the amount of manuscripts digitized and made available on the Web has been constantly increasing. However, there is still a considarable lack of results concerning both the explicitation of their content and the tools developed to make it available. The objective of the Clavius on the Web project is to develop a Web platform exposing a selection of Christophorus Clavius letters along with three different levels of analysis: linguistic, lexical and semantic. The multilayered annotation of the corpus involves a XML-TEI encoding followed by a tokenization step where each token is univocally identified through a CTS urn notation and then associated to a part-of-speech and a lemma. The text is lexically and semantically annotated on the basis of a lexicon and a domain ontology, the former structuring the most relevant terms occurring in the text and the latter representing the domain entities of interest (e.g. people, places, etc.). Moreover, each entity is connected to linked and non linked resources, including DBpedia and VIAF. Finally, the results of the three layers of analysis are gathered and shown through interactive visualization and storytelling techniques. A demo version of the integrated architecture was developed.</abstract>
    </paper>
    <paper id="318">
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Jingjing</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Yu</last></author>
      <author><first>Dimitris</first><last>Metaxas</last></author>
      <author><first>Carol</first><last>Neidle</last></author>
      <title>3<fixed-case>D</fixed-case> Face Tracking and Multi-Scale, Spatio-temporal Analysis of Linguistically Significant Facial Expressions and Head Positions in <fixed-case>ASL</fixed-case></title>
      <pages>4512–4518</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/370_Paper.pdf</url>
      <abstract>Essential grammatical information is conveyed in signed languages by clusters of events involving facial expressions and movements of the head and upper body. This poses a significant challenge for computer-based sign language recognition. Here, we present new methods for the recognition of nonmanual grammatical markers in American Sign Language (ASL) based on: (1) new 3D tracking methods for the estimation of 3D head pose and facial expressions to determine the relevant low-level features; (2) methods for higher-level analysis of component events (raised/lowered eyebrows, periodic head nods and head shakes) used in grammatical markings―with differentiation of temporal phases (onset, core, offset, where appropriate), analysis of their characteristic properties, and extraction of corresponding features; (3) a 2-level learning framework to combine low- and high-level features of differing spatio-temporal scales. This new approach achieves significantly better tracking and recognition results than our previous methods.</abstract>
    </paper>
    <paper id="319">
      <author><first>Leah</first><last>Geer</last></author>
      <author><first>Jonathan</first><last>Keane</last></author>
      <title>Exploring factors that contribute to successful fingerspelling comprehension</title>
      <pages>1905–1910</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/371_Paper.pdf</url>
      <abstract>Using a novel approach, we examine which cues in a fingerspelling stream, namely holds or transitions, allow for more successful comprehension by students learning American Sign Language (ASL). Sixteen university-level ASL students participated in this study. They were shown video clips of a native signer fingerspelling common English words. Clips were modified in the following ways: all were slowed down to half speed, one-third of the clips were modified to black out the transition portion of the fingerspelling stream, and one-third modified to have holds blacked out. The remaining third of clips were free of blacked out portions, which we used to establish a baseline of comprehension. Research by Wilcox (1992), among others, suggested that transitions provide more rich information, and thus items with the holds blacked out should be easier to comprehend than items with the transitions blacked out. This was not found to be the case here. Students achieved higher comprehension scores when hold information was provided. Data from this project can be used to design training tools to help students become more proficient at fingerspelling comprehension, a skill with which most students struggle.</abstract>
    </paper>
    <paper id="320">
      <author><first>Nobal</first><last>Niraula</last></author>
      <author><first>Vasile</first><last>Rus</last></author>
      <author><first>Rajendra</first><last>Banjade</last></author>
      <author><first>Dan</first><last>Stefanescu</last></author>
      <author><first>William</first><last>Baggett</last></author>
      <author><first>Brent</first><last>Morgan</last></author>
      <title>The <fixed-case>DARE</fixed-case> Corpus: A Resource for Anaphora Resolution in Dialogue Based Intelligent Tutoring Systems</title>
      <pages>3199–3203</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/372_Paper.pdf</url>
      <abstract>We describe the DARE corpus, an annotated data set focusing on pronoun resolution in tutorial dialogue. Although data sets for general purpose anaphora resolution exist, they are not suitable for dialogue based Intelligent Tutoring Systems. To the best of our knowledge, no data set is currently available for pronoun resolution in dialogue based intelligent tutoring systems. The described DARE corpus consists of 1,000 annotated pronoun instances collected from conversations between high-school students and the intelligent tutoring system DeepTutor. The data set is publicly available.</abstract>
    </paper>
    <paper id="321">
      <author><first>Mikel</first><last>Forcada</last></author>
      <title>On the annotation of <fixed-case>TMX</fixed-case> translation memories for advanced leveraging in computer-aided translation</title>
      <pages>4374–4378</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/373_Paper.pdf</url>
      <abstract>The term advanced leveraging refers to extensions beyond the current usage of translation memory (TM) in computer-aided translation (CAT). One of these extensions is the ability to identify and use matches on the sub-segment level ― for instance, using sub-sentential elements when segments are sentences― to help the translator when a reasonable fuzzy-matched proposal is not available; some such functionalities have started to become available in commercial CAT tools. Resources such as statistical word aligners, external machine translation systems, glossaries and term bases could be used to identify and annotate segment-level translation units at the sub-segment level, but there is currently no single, agreed standard supporting the interchange of sub-segmental annotation of translation memories to create a richer translation resource. This paper discusses the capabilities and limitations of some current standards, envisages possible alternatives, and ends with a tentative proposal which slightly abuses (repurposes) the usage of existing elements in the TMX standard.</abstract>
    </paper>
    <paper id="322">
      <author><first>Shannon</first><last>Hennig</last></author>
      <author><first>Ryad</first><last>Chellali</last></author>
      <author><first>Nick</first><last>Campbell</last></author>
      <title>The <fixed-case>D</fixed-case>-<fixed-case>ANS</fixed-case> corpus: the <fixed-case>D</fixed-case>ublin-Autonomous Nervous System corpus of biosignal and multimodal recordings of conversational speech</title>
      <pages>3438–3443</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/374_Paper.pdf</url>
      <abstract>Biosignals, such as electrodermal activity (EDA) and heart rate, are increasingly being considered as potential data sources to provide information about the temporal fluctuations in affective experience during human interaction. This paper describes an English-speaking, multiple session corpus of small groups of people engaged in informal, unscripted conversation while wearing wireless, wrist-based EDA sensors. Additionally, one participant per recording session wore a heart rate monitor. This corpus was collected in order to observe potential interactions between various social and communicative phenomena and the temporal dynamics of the recorded biosignals. Here we describe the communicative context, technical set-up, synchronization process, and challenges in collecting and utilizing such data. We describe the segmentation and annotations to date, including laughter annotations, and how the research community can access and collaborate on this corpus now and in the future. We believe this corpus is particularly relevant to researchers interested in unscripted social conversation as well as to researchers with a specific interest in observing the dynamics of biosignals during informal social conversation rich with examples of laughter, conversational turn-taking, and non-task-based interaction.</abstract>
    </paper>
    <paper id="323">
      <author><first>Andrea</first><last>Moro</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <author><first>Francesco Maria</first><last>Tucci</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <title>Annotating the <fixed-case>MASC</fixed-case> Corpus with <fixed-case>B</fixed-case>abel<fixed-case>N</fixed-case>et</title>
      <pages>4214–4219</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/375_Paper.pdf</url>
      <abstract>In this paper we tackle the problem of automatically annotating, with both word senses and named entities, the MASC 3.0 corpus, a large English corpus covering a wide range of genres of written and spoken text. We use BabelNet 2.0, a multilingual semantic network which integrates both lexicographic and encyclopedic knowledge, as our sense/entity inventory together with its semantic structure, to perform the aforementioned annotation task. Word sense annotated corpora have been around for more than twenty years, helping the development of Word Sense Disambiguation algorithms by providing both training and testing grounds. More recently Entity Linking has followed the same path, with the creation of huge resources containing annotated named entities. However, to date, there has been no resource that contains both kinds of annotation. In this paper we present an automatic approach for performing this annotation, together with its output on the MASC corpus. We use this corpus because its goal of integrating different types of annotations goes exactly in our same direction. Our overall aim is to stimulate research on the joint exploitation and disambiguation of word senses and named entities. Finally, we estimate the quality of our annotations using both manually-tagged named entities and word senses, obtaining an accuracy of roughly 70% for both named entities and word sense annotations.</abstract>
    </paper>
    <paper id="324">
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Khalil</first><last>Sima’an</last></author>
      <title>All Fragments Count in Parser Evaluation</title>
      <pages>78–82</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/376_Paper.pdf</url>
      <abstract>PARSEVAL, the default paradigm for evaluating constituency parsers, calculates parsing success (Precision/Recall) as a function of the number of matching labeled brackets across the test set. Nodes in constituency trees, however, are connected together to reflect important linguistic relations such as predicate-argument and direct-dominance relations between categories. In this paper, we present FREVAL, a generalization of PARSEVAL, where the precision and recall are calculated not only for individual brackets, but also for co-occurring, connected brackets (i.e. fragments). FREVAL fragments precision (FLP) and recall (FLR) interpolate the match across the whole spectrum of fragment sizes ranging from those consisting of individual nodes (labeled brackets) to those consisting of full parse trees. We provide evidence that FREVAL is informative for inspecting relative parser performance by comparing a range of existing parsers.</abstract>
      <revision id="1" href="L14-1324v1" hash="e3b486bc"/>
      <revision id="2" href="L14-1324v2" hash="19420704" date="2020-07-07">Changed the name of one of the authors.</revision>
    </paper>
    <paper id="325">
      <author><first>Juan María</first><last>Garrido</last></author>
      <author><first>Yesika</first><last>Laplaza</last></author>
      <author><first>Benjamin</first><last>Kolz</last></author>
      <author><first>Miquel</first><last>Cornudella</last></author>
      <title><fixed-case>T</fixed-case>ex<fixed-case>AF</fixed-case>on 2.0: A text processing tool for the generation of expressive speech in <fixed-case>TTS</fixed-case> applications</title>
      <pages>3494–3500</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/377_Paper.pdf</url>
      <abstract>This paper presents TexAfon 2.0, an improved version of the text processing tool TexAFon, specially oriented to the generation of synthetic speech with expressive content. TexAFon is a text processing module in Catalan and Spanish for TTS systems, which performs all the typical tasks needed for the generation of synthetic speech from text: sentence detection, pre-processing, phonetic transcription, syllabication, prosodic segmentation and stress prediction. These improvements include a new normalisation module for the standardisation on chat text in Spanish, a module for the detection of the expressed emotions in the input text, and a module for the automatic detection of the intended speech acts, which are briefly described in the paper. The results of the evaluations carried out for each module are also presented.</abstract>
    </paper>
    <paper id="326">
      <author><first>Mojgan</first><last>Seraji</last></author>
      <author><first>Carina</first><last>Jahani</last></author>
      <author><first>Beáta</first><last>Megyesi</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <title>A <fixed-case>P</fixed-case>ersian Treebank with <fixed-case>S</fixed-case>tanford Typed Dependencies</title>
      <pages>796–801</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/378_Paper.pdf</url>
      <abstract>We present the Uppsala Persian Dependency Treebank (UPDT) with a syntactic annotation scheme based on Stanford Typed Dependencies. The treebank consists of 6,000 sentences and 151,671 tokens with an average sentence length of 25 words. The data is from different genres, including newspaper articles and fiction, as well as technical descriptions and texts about culture and art, taken from the open source Uppsala Persian Corpus (UPC). The syntactic annotation scheme is extended for Persian to include all syntactic relations that could not be covered by the primary scheme developed for English. In addition, we present open source tools for automatic analysis of Persian containing a text normalizer, a sentence segmenter and tokenizer, a part-of-speech tagger, and a parser. The treebank and the parser have been developed simultaneously in a bootstrapping procedure. The result of a parsing experiment shows an overall labeled attachment score of 82.05% and an unlabeled attachment score of 85.29%. The treebank is freely available as an open source resource.</abstract>
    </paper>
    <paper id="327">
      <author><first>Marion</first><last>Baranes</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <title>A Language-independent Approach to Extracting Derivational Relations from an Inflectional Lexicon</title>
      <pages>2793–2799</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/379_Paper.pdf</url>
      <abstract>In this paper, we describe and evaluate an unsupervised method for acquiring pairs of lexical entries belonging to the same morphological family, i.e., derivationally related words, starting from a purely inflectional lexicon. Our approach relies on transformation rules that relate lexical entries with the one another, and which are automatically extracted from the inflected lexicon based on surface form analogies and on part-of-speech information. It is generic enough to be applied to any language with a mainly concatenative derivational morphology. Results were obtained and evaluated on English, French, German and Spanish. Precision results are satisfying, and our French results favorably compare with another resource, although its construction relied on manually developed lexicographic information whereas our approach only requires an inflectional lexicon.</abstract>
    </paper>
    <paper id="328">
      <author><first>Dilek</first><last>Küçük</last></author>
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <title>Named Entity Recognition on <fixed-case>T</fixed-case>urkish Tweets</title>
      <pages>450–454</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/380_Paper.pdf</url>
      <abstract>Various recent studies show that the performance of named entity recognition (NER) systems developed for well-formed text types drops significantly when applied to tweets. The only existing study for the highly inflected agglutinative language Turkish reports a drop in F-Measure from 91% to 19% when ported from news articles to tweets. In this study, we present a new named entity-annotated tweet corpus and a detailed analysis of the various tweet-specific linguistic phenomena. We perform comparative NER experiments with a rule-based multilingual NER system adapted to Turkish on three corpora: a news corpus, our new tweet corpus, and another tweet corpus. Based on the analysis and the experimentation results, we suggest system features required to improve NER results for social media like Twitter.</abstract>
    </paper>
    <paper id="329">
      <author><first>Anne</first><last>Lacheret</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Julie</first><last>Beliao</last></author>
      <author><first>Anne</first><last>Dister</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Nicolas</first><last>Obin</last></author>
      <author><first>Paola</first><last>Pietrandrea</last></author>
      <author><first>Atanas</first><last>Tchobanov</last></author>
      <title><fixed-case>R</fixed-case>hapsodie: a Prosodic-Syntactic Treebank for Spoken <fixed-case>F</fixed-case>rench</title>
      <pages>295–301</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/381_Paper.pdf</url>
      <abstract>The main objective of the Rhapsodie project (ANR Rhapsodie 07 Corp-030-01) was to define rich, explicit, and reproducible schemes for the annotation of prosody and syntax in different genres (Â± spontaneous, Â± planned, face-to-face interviews vs. broadcast, etc.), in order to study the prosody/syntax/discourse interface in spoken French, and their roles in the segmentation of speech into discourse units (Lacheret, Kahane, &amp; Pietrandrea forthcoming). We here describe the deliverable, a syntactic and prosodic treebank of spoken French, composed of 57 short samples of spoken French (5 minutes long on average, amounting to 3 hours of speech and 33000 words), orthographically and phonetically transcribed. The transcriptions and the annotations are all aligned on the speech signal: phonemes, syllables, words, speakers, overlaps. This resource is freely available at www.projet-rhapsodie.fr. The sound samples (wav/mp3), the acoustic analysis (original F0 curve manually corrected and automatic stylized F0, pitch format), the orthographic transcriptions (txt), the microsyntactic annotations (tabular format), the macrosyntactic annotations (txt, tabular format), the prosodic annotations (xml, textgrid, tabular format), and the metadata (xml and html) can be freely downloaded under the terms of the Creative Commons licence Attribution - Noncommercial - Share Alike 3.0 France. The metadata are encoded in the IMDI-CMFI format and can be parsed on line.</abstract>
    </paper>
    <paper id="330">
      <author><first>Montserrat</first><last>Marimon</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Beatriz</first><last>Fisas</last></author>
      <author><first>Blanca</first><last>Arias</last></author>
      <author><first>Silvia</first><last>Vázquez</last></author>
      <author><first>Jorge</first><last>Vivaldi</last></author>
      <author><first>Carlos</first><last>Morell</last></author>
      <author><first>Mercè</first><last>Lorente</last></author>
      <title>The <fixed-case>IULA</fixed-case> <fixed-case>S</fixed-case>panish <fixed-case>LSP</fixed-case> Treebank</title>
      <pages>782–788</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/382_Paper.pdf</url>
      <abstract>This paper presents the IULA Spanish LSP Treebank, a dependency treebank of over 41,000 sentences of different domains (Law, Economy, Computing Science, Environment, and Medicine), developed in the framework of the European project METANET4U. Dependency annotations in the treebank were automatically derived from manually selected parses produced by an HPSG-grammar by a deterministic conversion algorithm that used the identifiers of grammar rules to identify the heads, the dependents, and some dependency types that were directly transferred onto the dependency structure (e.g., subject, specifier, and modifier), and the identifiers of the lexical entries to identify the argument-related dependency functions (e.g. direct object, indirect object, and oblique complement). The treebank is accessible with a browser that provides concordance-based search functions and delivers the results in two formats: (i) a column-based format, in the style of CoNLL-2006 shared task, and (ii) a dependency graph, where dependency relations are noted by an oriented arrow which goes from the dependent node to the head node. The IULA Spanish LSP Treebank is the first technical corpus of Spanish annotated at surface syntactic level following the dependency grammar theory. The treebank has been made publicly and freely available from the META-SHARE platform with a Creative Commons CC-by licence.</abstract>
    </paper>
    <paper id="331">
      <author><first>Maria</first><last>Goryainova</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <title>Morpho-Syntactic Study of Errors from Speech Recognition System</title>
      <pages>3045–3049</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/383_Paper.pdf</url>
      <abstract>The study provides an original standpoint of the speech transcription errors by focusing on the morpho-syntactic features of the erroneous chunks and of the surrounding left and right context. The typology concerns the forms, the lemmas and the POS involved in erroneous chunks, and in the surrounding contexts. Comparison with error free contexts are also provided. The study is conducted on French. Morpho-syntactic analysis underlines that three main classes are particularly represented in the erroneous chunks: (i) grammatical words (to, of, the), (ii) auxiliary verbs (has, is), and (iii) modal verbs (should, must). Such items are widely encountered in the ASR outputs as frequent candidates to transcription errors. The analysis of the context points out that some left 3-grams contexts (e.g., repetitions, that is disfluencies, bracketing formulas such as “cest”, etc.) may be better predictors than others. Finally, the surface analysis conducted through a Levensthein distance analysis, highlighted that the most common distance is of 2 characters and mainly involves differences between inflected forms of a unique item.</abstract>
    </paper>
    <paper id="332">
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Zdeňka</first><last>Urešová</last></author>
      <author><first>Xiuhong</first><last>Zhang</last></author>
      <title>Not an Interlingua, But Close: Comparison of <fixed-case>E</fixed-case>nglish <fixed-case>AMR</fixed-case>s to <fixed-case>C</fixed-case>hinese and <fixed-case>C</fixed-case>zech</title>
      <pages>1765–1772</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/384_Paper.pdf</url>
      <abstract>Abstract Meaning Representations (AMRs) are rooted, directional and labeled graphs that abstract away from morpho-syntactic idiosyncrasies such as word category (verbs and nouns), word order, and function words (determiners, some prepositions). Because these syntactic idiosyncrasies account for many of the cross-lingual differences, it would be interesting to see if this representation can serve, e.g., as a useful, minimally divergent transfer layer in machine translation. To answer this question, we have translated 100 English sentences that have existing AMRs into Chinese and Czech to create AMRs for them. A cross-linguistic comparison of English to Chinese and Czech AMRs reveals both cases where the AMRs for the language pairs align well structurally and cases of linguistic divergence. We found that the level of compatibility of AMR between English and Chinese is higher than between English and Czech. We believe this kind of comparison is beneficial to further refining the annotation standards for each of the three languages and will lead to more compatible annotation guidelines between the languages.</abstract>
    </paper>
    <paper id="333">
      <author><first>Adam</first><last>Meyers</last></author>
      <author><first>Giancarlo</first><last>Lee</last></author>
      <author><first>Angus</first><last>Grieve-Smith</last></author>
      <author><first>Yifan</first><last>He</last></author>
      <author><first>Harriet</first><last>Taber</last></author>
      <title>Annotating Relations in Scientific Articles</title>
      <pages>4601–4608</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/385_Paper.pdf</url>
      <abstract>Relations (ABBREVIATE, EXEMPLIFY, ORIGINATE, REL_WORK, OPINION) between entities (citations, jargon, people, organizations) are annotated for PubMed scientific articles. We discuss our specifications, pre-processing and evaluation</abstract>
    </paper>
    <paper id="334">
      <author><first>Prescott</first><last>Klassen</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Lucy</first><last>Vanderwende</last></author>
      <author><first>Meliha</first><last>Yetisgen</last></author>
      <title>Annotating Clinical Events in Text Snippets for Phenotype Detection</title>
      <pages>2753–2757</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/386_Paper.pdf</url>
      <abstract>Early detection and treatment of diseases that onset after a patient is admitted to a hospital, such as pneumonia, is critical to improving and reducing costs in healthcare. NLP systems that analyze the narrative data embedded in clinical artifacts such as x-ray reports can help support early detection. In this paper, we consider the importance of identifying the change of state for events - in particular, clinical events that measure and compare the multiple states of a patients health across time. We propose a schema for event annotation comprised of five fields and create preliminary annotation guidelines for annotators to apply the schema. We then train annotators, measure their performance, and finalize our guidelines. With the complete guidelines, we then annotate a corpus of snippets extracted from chest x-ray reports in order to integrate the annotations as a new source of features for classification tasks.</abstract>
    </paper>
    <paper id="335">
      <author><first>Pablo</first><last>Ruiz</last></author>
      <author><first>Aitor</first><last>Álvarez</last></author>
      <author><first>Haritz</first><last>Arzelus</last></author>
      <title>Phoneme Similarity Matrices to Improve Long Audio Alignment for Automatic Subtitling</title>
      <pages>437–442</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/387_Paper.pdf</url>
      <abstract>Long audio alignment systems for Spanish and English are presented, within an automatic subtitling application. Language-specific phone decoders automatically recognize audio contents at phoneme level. At the same time, language-dependent grapheme-to-phoneme modules perform a transcription of the script for the audio. A dynamic programming algorithm (Hirschberg’s algorithm) finds matches between the phonemes automatically recognized by the phone decoder and the phonemes in the scripts transcription. Alignment accuracy is evaluated when scoring alignment operations with a baseline binary matrix, and when scoring alignment operations with several continuous-score matrices, based on phoneme similarity as assessed through comparing multivalued phonological features. Alignment accuracy results are reported at phoneme, word and subtitle level. Alignment accuracy when using the continuous scoring matrices based on phonological similarity was clearly higher than when using the baseline binary matrix.</abstract>
    </paper>
    <paper id="336">
      <author><first>Maria Evangelia</first><last>Chatzimina</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <title>Use of unsupervised word classes for entity recognition: Application to the detection of disorders in clinical reports</title>
      <pages>3264–3271</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/389_Paper.pdf</url>
      <abstract>Unsupervised word classes induced from unannotated text corpora are increasingly used to help tasks addressed by supervised classification, such as standard named entity detection. This paper studies the contribution of unsupervised word classes to a medical entity detection task with two specific objectives: How do unsupervised word classes compare to available knowledge-based semantic classes? Does syntactic information help produce unsupervised word classes with better properties? We design and test two syntax-based methods to produce word classes: one applies the Brown clustering algorithm to syntactic dependencies, the other collects latent categories created by a PCFG-LA parser. When added to non-semantic features, knowledge-based semantic classes gain 7.28 points of F-measure. In the same context, basic unsupervised word classes gain 4.16pt, reaching 60% of the contribution of knowledge-based semantic classes and outperforming Wikipedia, and adding PCFG-LA unsupervised word classes gain one more point at 5.11pt, reaching 70%. Unsupervised word classes could therefore provide a useful semantic back-off in domains where no knowledge-based semantic classes are available. The combination of both knowledge-based and basic unsupervised classes gains 8.33pt. Therefore, unsupervised classes are still useful even when rich knowledge-based classes exist.</abstract>
    </paper>
    <paper id="337">
      <author><first>Eva</first><last>Hajičová</last></author>
      <title>Three dimensions of the so-called “interoperability” of annotation schemes”</title>
      <pages>4559–4564</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/39_Paper.pdf</url>
      <abstract>Interoperability of annotation schemes is one of the key words in the discussions about annotation of corpora. In the present contribution, we propose to look at the so-called interoperability from (at least) three angles, namely (i) as a relation (and possible interaction or cooperation) of different annotation schemes for different layers or phenomena of a single language, (ii) the possibility to annotate different languages by a single (modified or not) annotation scheme, and (iii) the relation between different annotation schemes for a single language, or for a single phenomenon or layer of the same language. The pros and cons of each of these aspects are discussed as well as their contribution to linguistic studies and natural language processing. It is stressed that a communication and collaboration between different annotation schemes requires an explicit specification and consistency of each of the schemes.</abstract>
    </paper>
    <paper id="338">
      <author><first>Miriam</first><last>Kaeshammer</last></author>
      <author><first>Anika</first><last>Westburg</last></author>
      <title>On Complex Word Alignment Configurations</title>
      <pages>1773–1780</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/390_Paper.pdf</url>
      <abstract>Resources of manual word alignments contain configurations that are beyond the alignment capacity of current translation models, hence the term complex alignment configuration. They have been the matter of some debate in the machine translation community, as they call for more powerful translation models that come with further complications. In this work we investigate instances of complex alignment configurations in data sets of four different language pairs to shed more light on the nature and cause of those configurations. For the English-German alignments from Padó and Lapata (2006), for instance, we find that only a small fraction of the complex configurations are due to real annotation errors. While a third of the complex configurations in this data set could be simplified when annotating according to a different style guide, the remaining ones are phenomena that one would like to be able to generate during translation. Those instances are mainly caused by the different word order of English and German. Our findings thus motivate further research in the area of translation beyond phrase-based and context-free translation modeling.</abstract>
    </paper>
    <paper id="339">
      <author><first>Dimitrios</first><last>Kokkinakis</last></author>
      <author><first>Jyrki</first><last>Niemi</last></author>
      <author><first>Sam</first><last>Hardwick</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <title><fixed-case>HFST</fixed-case>-<fixed-case>S</fixed-case>we<fixed-case>NER</fixed-case> — A New <fixed-case>NER</fixed-case> Resource for <fixed-case>S</fixed-case>wedish</title>
      <pages>2537–2543</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/391_Paper.pdf</url>
      <abstract>Named entity recognition (NER) is a knowledge-intensive information extraction task that is used for recognizing textual mentions of entities that belong to a predefined set of categories, such as locations, organizations and time expressions. NER is a challenging, difficult, yet essential preprocessing technology for many natural language processing applications, and particularly crucial for language understanding. NER has been actively explored in academia and in industry especially during the last years due to the advent of social media data. This paper describes the conversion, modeling and adaptation of a Swedish NER system from a hybrid environment, with integrated functionality from various processing components, to the Helsinki Finite-State Transducer Technology (HFST) platform. This new HFST-based NER (HFST-SweNER) is a full-fledged open source implementation that supports a variety of generic named entity types and consists of multiple, reusable resource layers, e.g., various n-gram-based named entity lists (gazetteers).</abstract>
    </paper>
    <paper id="340">
      <author><first>Motaz</first><last>Saad</last></author>
      <author><first>David</first><last>Langlois</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <title>Building and Modelling Multilingual Subjective Corpora</title>
      <pages>3086–3091</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/392_Paper.pdf</url>
      <abstract>Building multilingual opinionated models requires multilingual corpora annotated with opinion labels. Unfortunately, such kind of corpora are rare. We consider opinions in this work as subjective or objective. In this paper, we introduce an annotation method that can be reliably transferred across topic domains and across languages. The method starts by building a classifier that annotates sentences into subjective/objective label using a training data from “movie reviews” domain which is in English language. The annotation can be transferred to another language by classifying English sentences in parallel corpora and transferring the same annotation to the same sentences of the other language. We also shed the light on the link between opinion mining and statistical language modelling, and how such corpora are useful for domain specific language modelling. We show the distinction between subjective and objective sentences which tends to be stable across domains and languages. Our experiments show that language models trained on objective (respectively subjective) corpus lead to better perplexities on objective (respectively subjective) test.</abstract>
    </paper>
    <paper id="341">
      <author><first>Barbara</first><last>Schuppler</last></author>
      <author><first>Martin</first><last>Hagmueller</last></author>
      <author><first>Juan A.</first><last>Morales-Cordovilla</last></author>
      <author><first>Hannes</first><last>Pessentheiner</last></author>
      <title><fixed-case>GRASS</fixed-case>: the Graz corpus of Read And Spontaneous Speech</title>
      <pages>1465–1470</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/394_Paper.pdf</url>
      <abstract>This paper provides a description of the preparation, the speakers, the recordings, and the creation of the orthographic transcriptions of the first large scale speech database for Austrian German. It contains approximately 1900 minutes of (read and spontaneous) speech produced by 38 speakers. The corpus consists of three components. First, the Conversation Speech (CS) component contains free conversations of one hour length between friends, colleagues, couples, or family members. Second, the Commands Component (CC) contains commands and keywords which were either read or elicited by pictures. Third, the Read Speech (RS) component contains phonetically balanced sentences and digits. The speech of all components has been recorded at super-wideband quality in a soundproof recording-studio with head-mounted microphones, large-diaphragm microphones, a laryngograph, and with a video camera. The orthographic transcriptions, which have been created and subsequently corrected manually, contain approximately 290 000 word tokens from 15 000 different word types.</abstract>
    </paper>
    <paper id="342">
      <author><first>Menzo</first><last>Windhouwer</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <title>Linguistic resources and cats: how to use <fixed-case>ISO</fixed-case>cat, <fixed-case>REL</fixed-case>cat and <fixed-case>SCHEMA</fixed-case>cat</title>
      <pages>3806–3810</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/396_Paper.pdf</url>
      <abstract>Within the European CLARIN infrastructure ISOcat is used to enable both humans and computer programs to find specific resources even when they use different terminology or data structures. In order to do so, it should be clear which concepts are used in these resources, both at the level of metadata for the resource as well as its content, and what is meant by them. The concepts can be specified in ISOcat. SCHEMAcat enables us to relate the concepts used by a resource, while RELcat enables to type these relationships and add relationships beyond resource boundaries. This way these three registries together allow us (and the programs) to find what we are looking for.</abstract>
    </paper>
    <paper id="343">
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Jens</first><last>Allwood</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <title>Bring vs. <fixed-case>MTR</fixed-case>oget: Evaluating automatic thesaurus translation</title>
      <pages>2115–2121</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/397_Paper.pdf</url>
      <abstract>Evaluation of automatic language-independent methods for language technology resource creation is difficult, and confounded by a largely unknown quantity, viz. to what extent typological differences among languages are significant for results achieved for one language or language pair to be applicable across languages generally. In the work presented here, as a simplifying assumption, language-independence is taken as axiomatic within certain specified bounds. We evaluate the automatic translation of Roget’s “Thesaurus” from English into Swedish using an independently compiled Roget-style Swedish thesaurus, S.C. Bring’s “Swedish vocabulary arranged into conceptual classes” (1930). Our expectation is that this explicit evaluation of one of the thesaureses created in the MTRoget project will provide a good estimate of the quality of the other thesauruses created using similar methods.</abstract>
    </paper>
    <paper id="344">
      <author><first>Paula</first><last>Lopez-Otero</last></author>
      <author><first>Laura</first><last>Docio-Fernandez</last></author>
      <author><first>Carmen</first><last>Garcia-Mateo</last></author>
      <title>Introducing a Framework for the Evaluation of Music Detection Tools</title>
      <pages>568–572</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/398_Paper.pdf</url>
      <abstract>The huge amount of multimedia information available nowadays makes its manual processing prohibitive, requiring tools for automatic labelling of these contents. This paper describes a framework for assessing a music detection tool; this framework consists of a database, composed of several hours of radio recordings that include different types of radio programmes, and a set of evaluation measures for evaluating the performance of a music detection tool in detail. A tool for automatically detecting music in audio streams, with application to music information retrieval tasks, is presented as well. The aim of this tool is to discard the audio excerpts that do not contain music in order to avoid their unnecessary processing. This tool applies fingerprinting to different acoustic features extracted from the audio signal in order to remove perceptual irrelevancies, and a support vector machine is trained for classifying these fingerprints in classes music and no-music. The validity of this tool is assessed in the proposed evaluation framework.</abstract>
    </paper>
    <paper id="345">
      <author><first>Tristan</first><last>Miller</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <title><fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et—<fixed-case>W</fixed-case>ikipedia—<fixed-case>W</fixed-case>iktionary: Construction of a Three-way Alignment</title>
      <pages>2094–2100</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/4_Paper.pdf</url>
      <abstract>The coverage and quality of conceptual information contained in lexical semantic resources is crucial for many tasks in natural language processing. Automatic alignment of complementary resources is one way of improving this coverage and quality; however, past attempts have always been between pairs of specific resources. In this paper we establish some set-theoretic conventions for describing concepts and their alignments, and use them to describe a method for automatically constructing n-way alignments from arbitrary pairwise alignments. We apply this technique to the production of a three-way alignment from previously published WordNet-Wikipedia and WordNet-Wiktionary alignments. We then present a quantitative and informal qualitative analysis of the aligned resource. The three-way alignment was found to have greater coverage, an enriched sense representation, and coarser sense granularity than both the original resources and their pairwise alignments, though this came at the cost of accuracy. An evaluation of the induced word sense clusters in a word sense disambiguation task showed that they were no better than random clusters of equivalent granularity. However, use of the alignments to enrich a sense inventory with additional sense glosses did significantly improve the performance of a baseline knowledge-based WSD algorithm.</abstract>
    </paper>
    <paper id="346">
      <author><first>Cristina</first><last>Grisot</last></author>
      <author><first>Thomas</first><last>Meyer</last></author>
      <title>Cross-linguistic annotation of narrativity for <fixed-case>E</fixed-case>nglish/<fixed-case>F</fixed-case>rench verb tense disambiguation</title>
      <pages>963–966</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/400_Paper.pdf</url>
      <abstract>This paper presents manual and automatic annotation experiments for a pragmatic verb tense feature (narrativity) in English/French parallel corpora. The feature is considered to play an important role for translating English Simple Past tense into French, where three different tenses are available. Whether the French Passe Ì Compose Ì, Passe Ì Simple or Imparfait should be used is highly dependent on a longer-range context, in which either narrative events ordered in time or mere non-narrative state of affairs in the past are described. This longer-range context is usually not available to current machine translation (MT) systems, that are trained on parallel corpora. Annotating narrativity prior to translation is therefore likely to help current MT systems. Our experiments show that narrativity can be reliably identified with kappa-values of up to 0.91 in manual annotation and with F1 scores of up to 0.72 in automatic annotation.</abstract>
    </paper>
    <paper id="347">
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Sabine</first><last>Hunsicker</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Cindy</first><last>Tscherwinka</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <title>The tara<fixed-case>XÜ</fixed-case> corpus of human-annotated machine translations</title>
      <pages>2679–2682</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/401_Paper.pdf</url>
      <abstract>Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. This paper describes the corpus developed as a result of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing.</abstract>
    </paper>
    <paper id="348">
      <author><first>Mahmoud</first><last>El-Haj</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Steve</first><last>Young</last></author>
      <author><first>Martin</first><last>Walker</last></author>
      <title>Detecting Document Structure in a Very Large Corpus of <fixed-case>UK</fixed-case> Financial Reports</title>
      <pages>1335–1338</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/402_Paper.pdf</url>
      <abstract>In this paper we present the evaluation of our automatic methods for detecting and extracting document structure in annual financial reports. The work presented is part of the Corporate Financial Information Environment (CFIE) project in which we are using Natural Language Processing (NLP) techniques to study the causes and consequences of corporate disclosure and financial reporting outcomes. We aim to uncover the determinants of financial reporting quality and the factors that influence the quality of information disclosed to investors beyond the financial statements. The CFIE consists of the supply of information by firms to investors, and the mediating influences of information intermediaries on the timing, relevance and reliability of information available to investors. It is important to compare and contrast specific elements or sections of each annual financial report across our entire corpus rather than working at the full document level. We show that the values of some metrics e.g. readability will vary across sections, thus improving on previous research research based on full texts.</abstract>
    </paper>
    <paper id="349">
      <author><first>Dan</first><last>Ștefănescu</last></author>
      <author><first>Rajendra</first><last>Banjade</last></author>
      <author><first>Vasile</first><last>Rus</last></author>
      <title>Latent Semantic Analysis Models on <fixed-case>W</fixed-case>ikipedia and <fixed-case>TASA</fixed-case></title>
      <pages>1417–1422</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/403_Paper.pdf</url>
      <abstract>This paper introduces a collection of freely available Latent Semantic Analysis models built on the entire English Wikipedia and the TASA corpus. The models differ not only on their source, Wikipedia versus TASA, but also on the linguistic items they focus on: all words, content-words, nouns-verbs, and main concepts. Generating such models from large datasets (e.g. Wikipedia), that can provide a large coverage for the actual vocabulary in use, is computationally challenging, which is the reason why large LSA models are rarely available. Our experiments show that for the task of word-to-word similarity, the scores assigned by these models are strongly correlated with human judgment, outperforming many other frequently used measures, and comparable to the state of the art.</abstract>
    </paper>
    <paper id="350">
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Audronė</first><last>Bielevičienė</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>António</first><last>Branco</last></author>
      <author><first>Gerhard</first><last>Budin</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <author><first>Radovan</first><last>Garabík</last></author>
      <author><first>Marko</first><last>Grobelnik</last></author>
      <author><first>Carmen</first><last>García-Mateo</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Inma</first><last>Hernáez</last></author>
      <author><first>John</first><last>Judge</last></author>
      <author><first>Svetla</first><last>Koeva</last></author>
      <author><first>Simon</first><last>Krek</last></author>
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <author><first>Joseph</first><last>Mariani</last></author>
      <author><first>John</first><last>McNaught</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Asunción</first><last>Moreno</last></author>
      <author><first>Jan</first><last>Odijk</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Piotr</first><last>Pęzik</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <author><first>Eiríkur</first><last>Rögnvaldsson</last></author>
      <author><first>Michael</first><last>Rosner</last></author>
      <author><first>Bolette</first><last>Pedersen</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <author><first>Koenraad</first><last>De Smedt</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Paul</first><last>Thompson</last></author>
      <author><first>Dan</first><last>Tufiş</last></author>
      <author><first>Tamás</first><last>Váradi</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <author><first>Kadri</first><last>Vider</last></author>
      <author><first>Jolanta</first><last>Zabarskaite</last></author>
      <title>The Strategic Impact of <fixed-case>META</fixed-case>-<fixed-case>NET</fixed-case> on the Regional, National and International Level</title>
      <pages>1517–1524</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/405_Paper.pdf</url>
      <abstract>This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiatives work throughout Europe in order to boost progress and innovation in our field.</abstract>
    </paper>
    <paper id="351">
      <author><first>Alan</first><last>Akbik</last></author>
      <author><first>Thilo</first><last>Michael</last></author>
      <title>The Weltmodell: A Data-Driven Commonsense Knowledge Base</title>
      <pages>3272–3276</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/409_Paper.pdf</url>
      <abstract>We present the Weltmodell, a commonsense knowledge base that was automatically generated from aggregated dependency parse fragments gathered from over 3.5 million English language books. We leverage the magnitude and diversity of this dataset to arrive at close to ten million distinct N-ary commonsense facts using techniques from open-domain Information Extraction (IE). Furthermore, we compute a range of measures of association and distributional similarity on this data. We present the results of our efforts using a browsable web demonstrator and publicly release all generated data for use and discussion by the research community. In this paper, we give an overview of our knowledge acquisition method and representation model, and present our web demonstrator.</abstract>
    </paper>
    <paper id="352">
      <author><first>Florian</first><last>Schiel</last></author>
      <author><first>Thomas</first><last>Kisler</last></author>
      <title><fixed-case>G</fixed-case>erman Alcohol Language Corpus - the Question of Dialect</title>
      <pages>353–356</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/41_Paper.pdf</url>
      <abstract>Speech uttered under the influence of alcohol is known to deviate from the speech of the same person when sober. This is an important feature in forensic investigations and could also be used to detect intoxication in the automotive environment. Aside from acoustic-phonetic features and speech content which have already been studied by others in this contribution we address the question whether speakers use dialectal variation or dialect words more frequently when intoxicated than when sober. We analyzed 300,000 recorded word tokens in read and spontaneous speech uttered by 162 female and male speakers within the German Alcohol Language Corpus. We found that contrary to our expectations the frequency of dialectal forms decreases significantly when speakers are under the influence. We explain this effect with a compensatory over-shoot mechanism: speakers are aware of their intoxication and that they are being monitored. In forensic analysis of speech this `awareness factor’ must be taken into account.</abstract>
    </paper>
    <paper id="353">
      <author><first>Koenraad</first><last>De Smedt</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <author><first>Bolette</first><last>Pedersen</last></author>
      <author><first>Costanza</first><last>Navarretta</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <author><first>Markéta</first><last>Lopatková</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Gisle</first><last>Andersen</last></author>
      <author><first>Przemyslaw</first><last>Lenkiewicz</last></author>
      <title><fixed-case>CLARA</fixed-case>: A New Generation of Researchers in Common Language Resources and Their Applications</title>
      <pages>2166–2174</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/410_Paper.pdf</url>
      <abstract>CLARA (Common Language Resources and Their Applications) is a Marie Curie Initial Training Network which ran from 2009 until 2014 with the aim of providing researcher training in crucial areas related to language resources and infrastructure. The scope of the project was broad and included infrastructure design, lexical semantic modeling, domain modeling, multimedia and multimodal communication, applications, and parsing technologies and grammar models. An international consortium of 9 partners and 12 associate partners employed researchers in 19 new positions and organized a training program consisting of 10 thematic courses and summer/winter schools. The project has resulted in new theoretical insights as well as new resources and tools. Most importantly, the project has trained a new generation of researchers who can perform advanced research and development in language resources and technologies.</abstract>
    </paper>
    <paper id="354">
      <author><first>Nathan</first><last>Hartmann</last></author>
      <author><first>Lucas</first><last>Avanço</last></author>
      <author><first>Pedro</first><last>Balage</last></author>
      <author><first>Magali</first><last>Duran</last></author>
      <author><first>Maria</first><last>das Graças Volpe Nunes</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <author><first>Sandra</first><last>Aluísio</last></author>
      <title>A Large Corpus of Product Reviews in <fixed-case>P</fixed-case>ortuguese: Tackling Out-Of-Vocabulary Words</title>
      <pages>3865–3871</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/413_Paper.pdf</url>
      <abstract>Web 2.0 has allowed a never imagined communication boom. With the widespread use of computational and mobile devices, anyone, in practically any language, may post comments in the web. As such, formal language is not necessarily used. In fact, in these communicative situations, language is marked by the absence of more complex syntactic structures and the presence of internet slang, with missing diacritics, repetitions of vowels, and the use of chat-speak style abbreviations, emoticons and colloquial expressions. Such language use poses severe new challenges for Natural Language Processing (NLP) tools and applications, which, so far, have focused on well-written texts. In this work, we report the construction of a large web corpus of product reviews in Brazilian Portuguese and the analysis of its lexical phenomena, which support the development of a lexical normalization tool for, in future work, subsidizing the use of standard NLP products for web opinion mining and summarization purposes.</abstract>
    </paper>
    <paper id="355">
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Abhijit</first><last>Mishra</last></author>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Ritesh</first><last>Shah</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <title>Shata-Anuvadak: Tackling Multiway Translation of <fixed-case>I</fixed-case>ndian Languages</title>
      <pages>1781–1787</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/414_Paper.pdf</url>
      <abstract>We present a compendium of 110 Statistical Machine Translation systems built from parallel corpora of 11 Indian languages belonging to both Indo-Aryan and Dravidian families. We analyze the relationship between translation accuracy and the language families involved. We feel that insights obtained from this analysis will provide guidelines for creating machine translation systems of specific Indian language pairs. We build phrase based systems and some extensions. Across multiple languages, we show improvements on the baseline phrase based systems using these extensions: (1) source side reordering for English-Indian language translation, and (2) transliteration of untranslated words for Indian language-Indian language translation. These enhancements harness shared characteristics of Indian languages. To stimulate similar innovation widely in the NLP community, we have made the trained models for these language pairs publicly available.</abstract>
    </paper>
    <paper id="356">
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <author><first>Steven</first><last>Krauwer</last></author>
      <title>The <fixed-case>CLARIN</fixed-case> Research Infrastructure: Resources and Tools for e<fixed-case>H</fixed-case>umanities Scholars</title>
      <pages>1525–1531</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/415_Paper.pdf</url>
      <abstract>CLARIN is the short name for the Common Language Resources and Technology Infrastructure, which aims at providing easy and sustainable access for scholars in the humanities and social sciences to digital language data and advanced tools to discover, explore, exploit, annotate, analyse or combine them, independent of where they are located. CLARIN is in the process of building a networked federation of European data repositories, service centers and centers of expertise, with single sign-on access for all members of the academic community in all participating countries. Tools and data from different centers will be interoperable so that data collections can be combined and tools from different sources can be chained to perform complex operations to support researchers in their work. Interoperability of language resources and tools in the federation of CLARIN Centers is ensured by adherence to TEI and ISO standards for text encoding, by the use of persistent identifiers, and by the observance of common protocols. The purpose of the present paper is to give an overview of language resources, tools, and services that CLARIN presently offers.</abstract>
    </paper>
    <paper id="357">
      <author><first>Renlong</first><last>Ai</last></author>
      <author><first>Marcela</first><last>Charfuelan</last></author>
      <author><first>Walter</first><last>Kasper</last></author>
      <author><first>Tina</first><last>Klüwer</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <author><first>Sandra</first><last>Gasber</last></author>
      <author><first>Philip</first><last>Gienandt</last></author>
      <title><fixed-case>S</fixed-case>printer: Language Technologies for Interactive and Multimedia Language Learning</title>
      <pages>2733–2738</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/416_Paper.pdf</url>
      <abstract>Modern language learning courses are no longer exclusively based on books or face-to-face lectures. More and more lessons make use of multimedia and personalized learning methods. Many of these are based on e-learning solutions. Learning via the Internet provides 7/24 services that require sizeable human resources. Therefore we witness a growing economic pressure to employ computer-assisted methods for improving language learning in quality, efficiency and scalability. In this paper, we will address three applications of language technologies for language learning: 1) Methods and strategies for pronunciation training in second language learning, e.g., multimodal feedback via visualization of sound features, speech verification and prosody transplantation; 2) Dialogue-based language learning games; 3) Application of parsing and generation technologies to the automatic generation of paraphrases for the semi-automatic production of learning material.</abstract>
    </paper>
    <paper id="358">
      <author><first>Wushouer</first><last>Mairidan</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <author><first>Donghui</first><last>Lin</last></author>
      <author><first>Katsutoshi</first><last>Hirayama</last></author>
      <title>Bilingual Dictionary Induction as an Optimization Problem</title>
      <pages>2122–2129</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/417_Paper.pdf</url>
      <abstract>Bilingual dictionaries are vital in many areas of natural language processing, but such resources are rarely available for lower-density language pairs, especially for those that are closely related. Pivot-based induction consists of using a third language to bridge a language pair. As an approach to create new dictionaries, it can generate wrong translations due to polysemy and ambiguous words. In this paper we propose a constraint approach to pivot-based dictionary induction for the case of two closely related languages. In order to take into account the word senses, we use an approach based on semantic distances, in which possibly missing translations are considered, and instance of induction is encoded as an optimization problem to generate new dictionary. Evaluations show that the proposal achieves 83.7% accuracy and approximately 70.5% recall, thus outperforming the baseline pivot-based method.</abstract>
    </paper>
    <paper id="359">
      <author><first>Brian</first><last>MacWhinney</last></author>
      <author><first>Davida</first><last>Fromm</last></author>
      <title>Two Approaches to Metaphor Detection</title>
      <pages>2501–2506</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/419_Paper.pdf</url>
      <abstract>Methods for automatic detection and interpretation of metaphors have focused on analysis and utilization of the ways in which metaphors violate selectional preferences (Martin, 2006). Detection and interpretation processes that rely on this method can achieve wide coverage and may be able to detect some novel metaphors. However, they are prone to high false alarm rates, often arising from imprecision in parsing and supporting ontological and lexical resources. An alternative approach to metaphor detection emphasizes the fact that many metaphors become conventionalized collocations, while still preserving their active metaphorical status. Given a large enough corpus for a given language, it is possible to use tools like SketchEngine (Kilgariff, Rychly, Smrz, &amp; Tugwell, 2004) to locate these high frequency metaphors for a given target domain. In this paper, we examine the application of these two approaches and discuss their relative strengths and weaknesses for metaphors in the target domain of economic inequality in English, Spanish, Farsi, and Russian.</abstract>
    </paper>
    <paper id="360">
      <author><first>Shinsuke</first><last>Mori</last></author>
      <author><first>Hideki</first><last>Ogura</last></author>
      <author><first>Tetsuro</first><last>Sasada</last></author>
      <title>A <fixed-case>J</fixed-case>apanese Word Dependency Corpus</title>
      <pages>753–758</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/42_Paper.pdf</url>
      <abstract>In this paper, we present a corpus annotated with dependency relationships in Japanese. It contains about 30 thousand sentences in various domains. Six domains in Balanced Corpus of Contemporary Written Japanese have part-of-speech and pronunciation annotation as well. Dictionary example sentences have pronunciation annotation and cover basic vocabulary in Japanese with English sentence equivalent. Economic newspaper articles also have pronunciation annotation and the topics are similar to those of Penn Treebank. Invention disclosures do not have other annotation, but it has a clear application, machine translation. The unit of our corpus is word like other languages contrary to existing Japanese corpora whose unit is phrase called bunsetsu. Each sentence is manually segmented into words. We first present the specification of our corpus. Then we give a detailed explanation about our standard of word dependency. We also report some preliminary results of an MST-based dependency parser on our corpus.</abstract>
    </paper>
    <paper id="361">
      <author><first>Hege</first><last>Fromreide</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <title>Crowdsourcing and annotating <fixed-case>NER</fixed-case> for <fixed-case>T</fixed-case>witter #drift</title>
      <pages>2544–2547</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/421_Paper.pdf</url>
      <abstract>We present two new NER datasets for Twitter; a manually annotated set of 1,467 tweets (kappa=0.942) and a set of 2,975 expert-corrected, crowdsourced NER annotated tweets from the dataset described in Finin et al. (2010). In our experiments with these datasets, we observe two important points: (a) language drift on Twitter is significant, and while off-the-shelf systems have been reported to perform well on in-sample data, they often perform poorly on new samples of tweets, (b) state-of-the-art performance across various datasets can be obtained from crowdsourced annotations, making it more feasible to “catch up” with language drift.</abstract>
    </paper>
    <paper id="362">
      <author><first>Sebastian</first><last>Krause</last></author>
      <author><first>Hong</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Robert</first><last>Hummel</last></author>
      <author><first>Luise</first><last>Spielhagen</last></author>
      <title>Language Resources and Annotation Tools for Cross-Sentence Relation Extraction</title>
      <pages>4320–4325</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/422_Paper.pdf</url>
      <abstract>In this paper, we present a novel combination of two types of language resources dedicated to the detection of relevant relations (RE) such as events or facts across sentence boundaries. One of the two resources is the sar-graph, which aggregates for each target relation ten thousands of linguistic patterns of semantically associated relations that signal instances of the target relation (Uszkoreit and Xu, 2013). These have been learned from the Web by intra-sentence pattern extraction (Krause et al., 2012) and after semantic filtering and enriching have been automatically combined into a single graph. The other resource is cockrACE, a specially annotated corpus for the training and evaluation of cross-sentence RE. By employing our powerful annotation tool Recon, annotators mark selected entities and relations (including events), coreference relations among these entities and events, and also terms that are semantically related to the relevant relations and events. This paper describes how the two resources are created and how they complement each other.</abstract>
    </paper>
    <paper id="363">
      <author><first>Alain</first><last>Couillault</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <author><first>Hugues</first><last>de Mazancourt</last></author>
      <title>Evaluating corpora documentation with regards to the Ethics and Big Data Charter</title>
      <pages>4225–4229</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/424_Paper.pdf</url>
      <abstract>The authors have written the Ethic and Big Data Charter in collaboration with various agencies, private bodies and associations. This Charter aims at describing any large or complex resources, and in particular language resources, from a legal and ethical viewpoint and ensuring the transparency of the process of creating and distributing such resources. We propose in this article an analysis of the documentation coverage of the most frequently mentioned language resources with regards to the Charter, in order to show the benefit it offers.</abstract>
    </paper>
    <paper id="364">
      <author><first>Ahmet</first><last>Aker</last></author>
      <author><first>Monica</first><last>Paramita</last></author>
      <author><first>Emma</first><last>Barker</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Bootstrapping Term Extractors for Multiple Languages</title>
      <pages>483–489</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/425_Paper.pdf</url>
      <abstract>Terminology extraction resources are needed for a wide range of human language technology applications, including knowledge management, information extraction, semantic search, cross-language information retrieval and automatic and assisted translation. We create a low cost method for creating terminology extraction resources for 21 non-English EU languages. Using parallel corpora and a projection method, we create a General POS Tagger for these languages. We also investigate the use of EuroVoc terms and Wikipedia corpus to automatically create term grammar for each language. Our results show that these automatically generated resources can assist term extraction process with similar performance to manually generated resources. All resources resulted in this experiment are freely available for download.</abstract>
    </paper>
    <paper id="365">
      <author><first>Els</first><last>Lefever</last></author>
      <author><first>Marjan</first><last>Van de Kauter</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Evaluation of Automatic Hypernym Extraction from Technical Corpora in <fixed-case>E</fixed-case>nglish and <fixed-case>D</fixed-case>utch</title>
      <pages>490–497</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/426_Paper.pdf</url>
      <abstract>In this research, we evaluate different approaches for the automatic extraction of hypernym relations from English and Dutch technical text. The detected hypernym relations should enable us to semantically structure automatically obtained term lists from domain- and user-specific data. We investigated three different hypernymy extraction approaches for Dutch and English: a lexico-syntactic pattern-based approach, a distributional model and a morpho-syntactic method. To test the performance of the different approaches on domain-specific data, we collected and manually annotated English and Dutch data from two technical domains, viz. the dredging and financial domain. The experimental results show that especially the morpho-syntactic approach obtains good results for automatic hypernym extraction from technical and domain-specific texts.</abstract>
    </paper>
    <paper id="366">
      <author><first>Bartosz</first><last>Broda</last></author>
      <author><first>Bartłomiej</first><last>Nitoń</last></author>
      <author><first>Włodzimierz</first><last>Gruszczyński</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <title>Measuring Readability of <fixed-case>P</fixed-case>olish Texts: Baseline Experiments</title>
      <pages>573–580</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/427_Paper.pdf</url>
      <abstract>Measuring readability of a text is the first sensible step to its simplification. In this paper we present an overview of the most common approaches to automatic measuring of readability. Of the described ones, we implemented and evaluated: Gunning FOG index, Flesch-based Pisarek method. We also present two other approaches. The first one is based on measuring distributional lexical similarity of a target text and comparing it to reference texts. In the second one, we propose a novel method for automation of Taylor test ― which, in its base form, requires performing a large amount of surveys. The automation of Taylor test is performed using a technique called statistical language modelling. We have developed a free on-line web-based system and constructed plugins for the most common text editors, namely Microsoft Word and OpenOffice.org. Inner workings of the system are described in detail. Finally, extensive evaluations are performed for Polish ― a Slavic, highly inflected language. We show that Pisareks method is highly correlated to Gunning FOG Index, even if different in form, and that both the similarity-based approach and automated Taylor test achieve high accuracy. Merits of using either of them are discussed.</abstract>
    </paper>
    <paper id="367">
      <author><first>Raphael</first><last>Winkelmann</last></author>
      <author><first>Georg</first><last>Raess</last></author>
      <title>Introducing a web application for labeling, visualizing speech and correcting derived speech signals</title>
      <pages>4129–4133</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/429_Paper.pdf</url>
      <abstract>The advent of HTML5 has sparked a great increase in interest in the web as a development platform for a variety of different research applications. Due to its ability to easily deploy software to remote clients and the recent development of standardized browser APIs, we argue that the browser has become a good platform to develop a speech labeling tool for. This paper introduces a preliminary version of an open-source client-side web application for labeling speech data, visualizing speech and segmentation information and manually correcting derived speech signals such as formant trajectories. The user interface has been designed to be as user-friendly as possible in order to make the sometimes tedious task of transcribing as easy and efficient as possible. The future integration into the next iteration of the EMU speech database management system and its general architecture will also be outlined, as the work presented here is only one of several components contributing to the future system.</abstract>
    </paper>
    <paper id="368">
      <author><first>Lise</first><last>Rebout</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <title>An Iterative Approach for Mining Parallel Sentences in a Comparable Corpus</title>
      <pages>648–655</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/43_Paper.pdf</url>
      <abstract>We describe an approach for mining parallel sentences in a collection of documents in two languages. While several approaches have been proposed for doing so, our proposal differs in several respects. First, we use a document level classifier in order to focus on potentially fruitful document pairs, an understudied approach. We show that mining less, but more parallel documents can lead to better gains in machine translation. Second, we compare different strategies for post-processing the output of a classifier trained to recognize parallel sentences. Last, we report a simple bootstrapping experiment which shows that promising sentence pairs extracted in a first stage can help to mine new sentence pairs in a second stage. We applied our approach on the English-French Wikipedia. Gains of a statistical machine translation (SMT) engine are analyzed along different test sets.</abstract>
    </paper>
    <paper id="369">
      <author><first>Mohamed</first><last>Elmahdy</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Eiman</first><last>Mustafawi</last></author>
      <title>Development of a <fixed-case>TV</fixed-case> Broadcasts Speech Recognition System for Qatari <fixed-case>A</fixed-case>rabic</title>
      <pages>3057–3061</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/430_Paper.pdf</url>
      <abstract>A major problem with dialectal Arabic speech recognition is due to the sparsity of speech resources. In this paper, a transfer learning framework is proposed to jointly use a large amount of Modern Standard Arabic (MSA) data and little amount of dialectal Arabic data to improve acoustic and language modeling. The Qatari Arabic (QA) dialect has been chosen as a typical example for an under-resourced Arabic dialect. A wide-band speech corpus has been collected and transcribed from several Qatari TV series and talk-show programs. A large vocabulary speech recognition baseline system was built using the QA corpus. The proposed MSA-based transfer learning technique was performed by applying orthographic normalization, phone mapping, data pooling, acoustic model adaptation, and system combination. The proposed approach can achieve more than 28% relative reduction in WER.</abstract>
    </paper>
    <paper id="370">
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Kais</first><last>Dukes</last></author>
      <title>Can Crowdsourcing be used for Effective Annotation of <fixed-case>A</fixed-case>rabic?</title>
      <pages>224–228</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/431_Paper.pdf</url>
      <abstract>Crowdsourcing has been used recently as an alternative to traditional costly annotation by many natural language processing groups. In this paper, we explore the use of Amazon Mechanical Turk (AMT) in order to assess the feasibility of using AMT workers (also known as Turkers) to perform linguistic annotation of Arabic. We used a gold standard data set taken from the Quran corpus project annotated with part-of-speech and morphological information. An Arabic language qualification test was used to filter out potential non-qualified participants. Two experiments were performed, a part-of-speech tagging task in where the annotators were asked to choose a correct word-category from a multiple choice list and case ending identification task. The results obtained so far showed that annotating Arabic grammatical case is harder than POS tagging, and crowdsourcing for Arabic linguistic annotation requiring expert annotators could be not as effective as other crowdsourcing experiments requiring less expertise and qualifications.</abstract>
    </paper>
    <paper id="371">
      <author><first>Hanae</first><last>Koiso</last></author>
      <author><first>Yasuharu</first><last>Den</last></author>
      <author><first>Ken’ya</first><last>Nishikawa</last></author>
      <author><first>Kikuo</first><last>Maekawa</last></author>
      <title>Design and development of an <fixed-case>RDB</fixed-case> version of the Corpus of Spontaneous <fixed-case>J</fixed-case>apanese</title>
      <pages>1471–1476</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/432_Paper.pdf</url>
      <abstract>In this paper, we describe the design and development of a new version of the Corpus of Spontaneous Japanese (CSJ), which is a large-scale spoken corpus released in 2004. CSJ contains various annotations that are represented in XML format (CSJ-XML). CSJ-XML, however, is very complicated and suffers from some problems. To overcome this problem, we have developed and released, in 2013, a relational database version of CSJ (CSJ-RDB). CSJ-RDB is based on an extension of the segment and link-based annotation scheme, which we adapted to handle multi-channel and multi-modal streams. Because this scheme adopts a stand-off framework, CSJ-RDB can represent three hierarchical structures at the same time: inter-pausal-unit-top, clause-top, and intonational-phrase-top. CSJ-RDB consists of five different types of tables: segment, unaligned-segment, link, relation, and meta-information tables. The database was automatically constructed from annotation files extracted from CSJ-XML by using general-purpose corpus construction tools. CSJ-RDB enables us to easily and efficiently conduct complex searches required for corpus-based studies of spoken language.</abstract>
    </paper>
    <paper id="372">
      <author><first>Mohamed</first><last>Elmahdy</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Eiman</first><last>Mustafawi</last></author>
      <title>Automatic Long Audio Alignment and Confidence Scoring for Conversational <fixed-case>A</fixed-case>rabic Speech</title>
      <pages>3062–3066</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/434_Paper.pdf</url>
      <abstract>In this paper, a framework for long audio alignment for conversational Arabic speech is proposed. Accurate alignments help in many speech processing tasks such as audio indexing, speech recognizer acoustic model (AM) training, audio summarizing and retrieving, etc. We have collected more than 1,400 hours of conversational Arabic besides the corresponding human generated non-aligned transcriptions. Automatic audio segmentation is performed using a split and merge approach. A biased language model (LM) is trained using the corresponding text after a pre-processing stage. Because of the dominance of non-standard Arabic in conversational speech, a graphemic pronunciation model (PM) is utilized. The proposed alignment approach is performed in two passes. Firstly, a generic standard Arabic AM is used along with the biased LM and the graphemic PM in a fast speech recognition pass. In a second pass, a more restricted LM is generated for each audio segment, and unsupervised acoustic model adaptation is applied. The recognizer output is aligned with the processed transcriptions using Levenshtein algorithm. The proposed approach resulted in an initial alignment accuracy of 97.8-99.0% depending on the amount of disfluencies. A confidence scoring metric is proposed to accept/reject aligner output. Using confidence scores, it was possible to reject the majority of mis-aligned segments resulting in alignment accuracy of 99.0-99.8% depending on the speech domain and the amount of disfluencies.</abstract>
    </paper>
    <paper id="373">
      <author><first>Dirk</first><last>Goldhahn</last></author>
      <author><first>Uwe</first><last>Quasthoff</last></author>
      <title>Vocabulary-Based Language Similarity using Web Corpora</title>
      <pages>3294–3299</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/435_Paper.pdf</url>
      <abstract>This paper will focus on the evaluation of automatic methods for quantifying language similarity. This is achieved by ascribing language similarity to the similarity of text corpora. This corpus similarity will first be determined by the resemblance of the vocabulary of languages. Thereto words or parts of them such as letter n-grams are examined. Extensions like transliteration of the text data will ensure the independence of the methods from text characteristics such as the writing system used. Further analyzes will show to what extent knowledge about the distribution of words in parallel text can be used in the context of language similarity.</abstract>
    </paper>
    <paper id="374">
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <author><first>Luciano</first><last>Serafini</last></author>
      <author><first>Pim</first><last>Stouten</last></author>
      <author><first>Francis</first><last>Irving</last></author>
      <author><first>Willem</first><last>Van Hage</last></author>
      <title><fixed-case>N</fixed-case>ews<fixed-case>R</fixed-case>eader: recording history from daily news streams</title>
      <pages>2000–2007</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/436_Paper.pdf</url>
      <abstract>The European project NewsReader develops technology to process daily news streams in 4 languages, extracting what happened, when, where and who was involved. NewsReader does not just read a single newspaper but massive amounts of news coming from thousands of sources. It compares the results across sources to complement information and determine where they disagree. Furthermore, it merges news of today with previous news, creating a long-term history rather than separate events. The result is stored in a KnowledgeStore, that cumulates information over time, producing an extremely large knowledge graph that is visualized using new techniques to provide more comprehensive access. We present the first version of the system and the results of processing first batches of data.</abstract>
    </paper>
    <paper id="375">
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <title>A set of open source tools for <fixed-case>T</fixed-case>urkish natural language processing</title>
      <pages>1079–1086</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/437_Paper.pdf</url>
      <abstract>This paper introduces a set of freely available, open-source tools for Turkish that are built around TRmorph, a morphological analyzer introduced earlier in Coltekin (2010). The article first provides an update on the analyzer, which includes a complete rewrite using a different finite-state description language and tool set as well as major tagset changes to comply better with the state-of-the-art computational processing of Turkish and the user requests received so far. Besides these major changes to the analyzer, this paper introduces tools for morphological segmentation, stemming and lemmatization, guessing unknown words, grapheme to phoneme conversion, hyphenation and a morphological disambiguation.</abstract>
    </paper>
    <paper id="376">
      <author><first>Tjerk</first><last>Hagemeijer</last></author>
      <author><first>Michel</first><last>Généreux</last></author>
      <author><first>Iris</first><last>Hendrickx</last></author>
      <author><first>Amália</first><last>Mendes</last></author>
      <author><first>Abigail</first><last>Tiny</last></author>
      <author><first>Armando</first><last>Zamora</last></author>
      <title>The <fixed-case>G</fixed-case>ulf of <fixed-case>G</fixed-case>uinea Creole Corpora</title>
      <pages>523–529</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/438_Paper.pdf</url>
      <abstract>We present the process of building linguistic corpora of the Portuguese-related Gulf of Guinea creoles, a cluster of four historically related languages: Santome, Angolar, Principense and Fa dAmbô. We faced the typical difficulties of languages lacking an official status, such as lack of standard spelling, language variation, lack of basic language instruments, and small data sets, which comprise data from the late 19th century to the present. In order to tackle these problems, the compiled written and transcribed spoken data collected during field work trips were adapted to a normalized spelling that was applied to the four languages. For the corpus compilation we followed corpus linguistics standards. We recorded meta data for each file and added morphosyntactic information based on a part-of-speech tag set that was designed to deal with the specificities of these languages. The corpora of three of the four creoles are already available and searchable via an online web interface.</abstract>
    </paper>
    <paper id="377">
      <author><first>Ville</first><last>Viitaniemi</last></author>
      <author><first>Tommi</first><last>Jantunen</last></author>
      <author><first>Leena</first><last>Savolainen</last></author>
      <author><first>Matti</first><last>Karppa</last></author>
      <author><first>Jorma</first><last>Laaksonen</last></author>
      <title><fixed-case>S</fixed-case>-pot - a benchmark in spotting signs within continuous signing</title>
      <pages>1892–1897</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/440_Paper.pdf</url>
      <abstract>In this paper we present S-pot, a benchmark setting for evaluating the performance of automatic spotting of signs in continuous sign language videos. The benchmark includes 5539 video files of Finnish Sign Language, ground truth sign spotting results, a tool for assessing the spottings against the ground truth, and a repository for storing information on the results. In addition we will make our sign detection system and results made with it publicly available as a baseline for comparison and further developments.</abstract>
    </paper>
    <paper id="378">
      <author><first>Masood</first><last>Ghayoomi</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>Converting an <fixed-case>HPSG</fixed-case>-based Treebank into its Parallel Dependency-based Treebank</title>
      <pages>802–809</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/441_Paper.pdf</url>
      <abstract>A treebank is an important language resource for supervised statistical parsers. The parser induces the grammatical properties of a language from this language resource and uses the model to parse unseen data automatically. Since developing such a resource is very time-consuming and tedious, one can take advantage of already extant resources by adapting them to a particular application. This reduces the amount of human effort required to develop a new language resource. In this paper, we introduce an algorithm to convert an HPSG-based treebank into its parallel dependency-based treebank. With this converter, we can automatically create a new language resource from an existing treebank developed based on a grammar formalism. Our proposed algorithm is able to create both projective and non-projective dependency trees.</abstract>
    </paper>
    <paper id="379">
      <author><first>Iñaki</first><last>Alegria</last></author>
      <author><first>Nora</first><last>Aranberri</last></author>
      <author><first>Pere</first><last>Comas</last></author>
      <author><first>Víctor</first><last>Fresno</last></author>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <author><first>Lluis</first><last>Padró</last></author>
      <author><first>Iñaki</first><last>San Vicente</last></author>
      <author><first>Jordi</first><last>Turmo</last></author>
      <author><first>Arkaitz</first><last>Zubiaga</last></author>
      <title><fixed-case>T</fixed-case>weet<fixed-case>N</fixed-case>orm_es: an annotated corpus for <fixed-case>S</fixed-case>panish microtext normalization</title>
      <pages>2274–2278</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/442_Paper.pdf</url>
      <abstract>In this paper we introduce TweetNorm_es, an annotated corpus of tweets in Spanish language, which we make publicly available under the terms of the CC-BY license. This corpus is intended for development and testing of microtext normalization systems. It was created for Tweet-Norm, a tweet normalization workshop and shared task, and is the result of a joint annotation effort from different research groups. In this paper we describe the methodology defined to build the corpus as well as the guidelines followed in the annotation process. We also present a brief overview of the Tweet-Norm shared task, as the first evaluation environment where the corpus was used.</abstract>
    </paper>
    <paper id="380">
      <author><first>Elżbieta</first><last>Hajnicz</last></author>
      <title>The Procedure of Lexico-Semantic Annotation of Składnica Treebank</title>
      <pages>2290–2297</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/444_Paper.pdf</url>
      <abstract>In this paper, the procedure of lexico-semantic annotation of Składnica Treebank using Polish WordNet is presented. Other semantically annotated corpora, in particular treebanks, are outlined first. Resources involved in annotation as well as a tool called Semantikon used for it are described. The main part of the paper is the analysis of the applied procedure. It consists of the basic and correction phases. During basic phase all nouns, verbs and adjectives are annotated with wordnet senses. The annotation is performed independently by two linguists. During the correction phase, conflicts are resolved by the linguist supervising the process. Multi-word units obtain special tags, synonyms and hypernyms are used for senses absent in Polish WordNet. Additionally, each sentence receives its general assessment. Finally, some statistics of the results of annotation are given, including inter-annotator agreement. The final resource is represented in XML files preserving the structure of Składnica.</abstract>
    </paper>
    <paper id="381">
      <author><first>Júlia</first><last>Pajzs</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Mohamed</first><last>Ebrahim</last></author>
      <author><first>Leonida</first><last>Della Rocca</last></author>
      <author><first>Stefano</first><last>Bucci</last></author>
      <author><first>Eszter</first><last>Simon</last></author>
      <author><first>Tamás</first><last>Váradi</last></author>
      <title>Media monitoring and information extraction for the highly inflected agglutinative language <fixed-case>H</fixed-case>ungarian</title>
      <pages>2049–2056</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/449_Paper.pdf</url>
      <abstract>The Europe Media Monitor (EMM) is a fully-automatic system that analyses written online news by gathering articles in over 70 languages and by applying text analysis software for currently 21 languages, without using linguistic tools such as parsers, part-of-speech taggers or morphological analysers. In this paper, we describe the effort of adding to EMM Hungarian text mining tools for news gathering; document categorisation; named entity recognition and classification for persons, organisations and locations; name lemmatisation; quotation recognition; and cross-lingual linking of related news clusters. The major challenge of dealing with the Hungarian language is its high degree of inflection and agglutination. We present several experiments where we apply linguistically light-weight methods to deal with inflection and we propose a method to overcome the challenges. We also present detailed frequency lists of Hungarian person and location name suffixes, as found in real-life news texts. This empirical data can be used to draw further conclusions and to improve existing Named Entity Recognition software. Within EMM, the solutions described here will also be applied to other morphologically complex languages such as those of the Slavic language family. The media monitoring and analysis system EMM is freely accessible online via the web page http://emm.newsbrief.eu/overview.html.</abstract>
    </paper>
    <paper id="382">
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <title><fixed-case>F</fixed-case>rench Resources for Extraction and Normalization of Temporal Expressions with <fixed-case>H</fixed-case>eidel<fixed-case>T</fixed-case>ime</title>
      <pages>3239–3243</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/45_Paper.pdf</url>
      <abstract>In this paper, we describe the development of French resources for the extraction and normalization of temporal expressions with HeidelTime, a open-source multilingual, cross-domain temporal tagger. HeidelTime extracts temporal expressions from documents and normalizes them according to the TIMEX3 annotation standard. Several types of temporal expressions are extracted: dates, times, durations and temporal sets. French resources have been evaluated in two different ways: on the French TimeBank corpus, a corpus of newspaper articles in French annotated according to the ISO-TimeML standard, and on a user application for automatic building of event timelines. Results on the French TimeBank are quite satisfaying as they are comparable to those obtained by HeidelTime in English and Spanish on newswire articles. Concerning the user application, we used two temporal taggers for the preprocessing of the corpus in order to compare their performance and results show that the performances of our application on French documents are better with HeidelTime. The French resources and evaluation scripts are publicly available with HeidelTime.</abstract>
    </paper>
    <paper id="383">
      <author><first>Maarten</first><last>Truyens</last></author>
      <author><first>Patrick</first><last>Van Eecke</last></author>
      <title>Legal aspects of text mining</title>
      <pages>2182–2186</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/452_Paper.pdf</url>
      <abstract>Unlike data mining, text mining has received only limited attention in legal circles. Nevertheless, interesting legal stumbling blocks exist, both with respect to the data collection and data sharing phases, due to the strict rules of copyright and database law. Conflicts are particularly likely when content is extracted from commercial databases, and when texts that have a minimal level of creativity are stored in a permanent way. In all circumstances, even with non-commercial research, license agreements and website terms of use can impose further restrictions. Accordingly, only for some delineated areas (very old texts for which copyright expired, legal statutes, texts in the public domain) strong legal certainty can be obtained without case-by-case assessments. As a result, while prior permission is certainly not required in all cases, many researchers tend to err on the side of caution, and seek permission from publishers, institutions and individual authors before including texts in their corpora, although this process can be difficult and very time-consuming. In the United States, the legal assessment is very different, due to the open-ended nature and flexibility offered by the “fair use” doctrine.</abstract>
    </paper>
    <paper id="384">
      <author><first>Angelina</first><last>Ivanova</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <title>Treelet Probabilities for <fixed-case>HPSG</fixed-case> Parsing and Error Correction</title>
      <pages>2887–2892</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/453_Paper.pdf</url>
      <abstract>Most state-of-the-art parsers take an approach to produce an analysis for any input despite errors. However, small grammatical mistakes in a sentence often cause parser to fail to build a correct syntactic tree. Applications that can identify and correct mistakes during parsing are particularly interesting for processing user-generated noisy content. Such systems potentially could take advantage of linguistic depth of broad-coverage precision grammars. In order to choose the best correction for an utterance, probabilities of parse trees of different sentences should be comparable which is not supported by discriminative methods underlying parsing software for processing deep grammars. In the present work we assess the treelet model for determining generative probabilities for HPSG parsing with error correction. In the first experiment the treelet model is applied to the parse selection task and shows superior exact match accuracy than the baseline and PCFG. In the second experiment it is tested for the ability to score the parse tree of the correct sentence higher than the constituency tree of the original version of the sentence containing grammatical error.</abstract>
    </paper>
    <paper id="385">
      <author><first>Abir</first><last>Masmoudi</last></author>
      <author><first>Mariem Ellouze</first><last>Khmekhem</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Lamia Hadrich</first><last>Belguith</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <title>A Corpus and Phonetic Dictionary for <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic Speech Recognition</title>
      <pages>306–310</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/454_Paper.pdf</url>
      <abstract>In this paper we describe an effort to create a corpus and phonetic dictionary for Tunisian Arabic Automatic Speech Recognition (ASR). The corpus, named TARIC (Tunisian Arabic Railway Interaction Corpus) has a collection of audio recordings and transcriptions from dialogues in the Tunisian Railway Transport Network. The phonetic (or pronunciation) dictionary is an important ASR component that serves as an intermediary between acoustic models and language models in ASR systems. The method proposed in this paper, to automatically generate a phonetic dictionary, is rule based. For that reason, we define a set of pronunciation rules and a lexicon of exceptions. To determine the performance of our phonetic rules, we chose to evaluate our pronunciation dictionary on two types of corpora. The word error rate of word grapheme-to-phoneme mapping is around 9%.</abstract>
    </paper>
    <paper id="386">
      <author><first>Marie-Claude</first><last>L’Homme</last></author>
      <author><first>Benoît</first><last>Robichaud</last></author>
      <author><first>Carlos Subirats</first><last>Rüggeberg</last></author>
      <title>Discovering frames in specialized domains</title>
      <pages>1364–1371</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/455_Paper.pdf</url>
      <abstract>This paper proposes a method for discovering semantic frames (Fillmore, 1982, 1985; Fillmore et al., 2003) in specialized domains. It is assumed that frames are especially relevant for capturing the lexical structure in specialized domains and that they complement structures such as ontologies that appear better suited to represent specific relationships between entities. The method we devised is based on existing lexical entries recorded in a specialized database related to the field of the environment (erode, impact, melt, recycling, warming). The frames and the data encoded in FrameNet are used as a reference. Selected information was extracted automatically from the database on the environment (and, when possible, compared to FrameNet), and presented to a linguist who analyzed this information to discover potential frames. Several different frames were discovered with this method. About half of them correspond to frames already described in FrameNet; some new frames were also defined and part of these might be specific to the field of the environment.</abstract>
    </paper>
    <paper id="387">
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Brian</first><last>MacWhinney</last></author>
      <author><first>Davida</first><last>Fromm</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <author><first>Weston</first><last>Feely</last></author>
      <author><first>Robert</first><last>Frederking</last></author>
      <author><first>Anatole</first><last>Gershman</last></author>
      <author><first>Carlos</first><last>Ramirez</last></author>
      <title>Resources for the Detection of Conventionalized Metaphors in Four Languages</title>
      <pages>498–501</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/457_Paper.pdf</url>
      <abstract>This paper describes a suite of tools for extracting conventionalized metaphors in English, Spanish, Farsi, and Russian. The method depends on three significant resources for each language: a corpus of conventionalized metaphors, a table of conventionalized conceptual metaphors (CCM table), and a set of extraction rules. Conventionalized metaphors are things like “escape from poverty” and “burden of taxation”. For each metaphor, the CCM table contains the metaphorical source domain word (such as “escape”) the target domain word (such as “poverty”) and the grammatical construction in which they can be found. The extraction rules operate on the output of a dependency parser and identify the grammatical configurations (such as a verb with a prepositional phrase complement) that are likely to contain conventional metaphors. We present results on detection rates for conventional metaphors and analysis of the similarity and differences of source domains for conventional metaphors in the four languages.</abstract>
    </paper>
    <paper id="388">
      <author><first>Jan</first><last>Odijk</last></author>
      <title><fixed-case>CLARIN</fixed-case>-<fixed-case>NL</fixed-case>: Major results</title>
      <pages>2187–2193</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/459_Paper.pdf</url>
      <abstract>In this paper I provide a high level overview of the major results of CLARIN-NL so far. I will show that CLARIN-NL is starting to provide the data, facilities and services in the CLARIN infrastructure to carry out humanities research supported by large amounts of data and tools. These services have easy interfaces and easy search options (no technical background needed). Still some training is required, to understand both the possibilities and the limitations of the data and the tools. Actual use of the facilities leads to suggestions for improvements and to suggestions for new functionality. All researchers are therefore invited to start using the elements in the CLARIN infrastructure offered by CLARIN-NL. Though I will show that a lot has been achieved in the CLARIN-NL project, I will also provide a long list of functionality and interoperability cases that have not been dealt with in CLARIN-NL and must remain for future work.</abstract>
    </paper>
    <paper id="389">
      <author><first>Hugo Gonçalo</first><last>Oliveira</last></author>
      <author><first>Inês</first><last>Coelho</last></author>
      <author><first>Paulo</first><last>Gomes</last></author>
      <title>Exploiting <fixed-case>P</fixed-case>ortuguese Lexical Knowledge Bases for Answering Open Domain Cloze Questions Automatically</title>
      <pages>4202–4209</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/46_Paper.pdf</url>
      <abstract>We present the task of answering cloze questions automatically and how it can be tackled by exploiting lexical knowledge bases (LKBs). This task was performed in what can be seen as an indirect evaluation of Portuguese LKB. We introduce the LKBs used and the algorithms applied, and then report on the obtained results and draw some conclusions: LKBs are definitely useful resources for this challenging task, and exploiting them, especially with PageRanking-based algorithms, clearly improves the baselines. Moreover, larger LKB, created automatically and not sense-aware led to the best results, as opposed to handcrafted LKB structured on synsets.</abstract>
    </paper>
    <paper id="390">
      <author><first>Yuka</first><last>Tateisi</last></author>
      <author><first>Yo</first><last>Shidahara</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <title>Annotation of Computer Science Papers for Semantic Relation Extrac-tion</title>
      <pages>1423–1429</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/461_Paper.pdf</url>
      <abstract>We designed a new annotation scheme for formalising relation structures in research papers, through the investigation of computer science papers. The annotation scheme is based on the hypothesis that identifying the role of entities and events that are described in a paper is useful for intelligent information retrieval in academic literature, and the role can be determined by the relationship between the author and the described entities or events, and relationships among them. Using the scheme, we have annotated research abstracts from the IPSJ Journal published in Japanese by the Information Processing Society of Japan. On the basis of the annotated corpus, we have developed a prototype information extraction system which has the facility to classify sentences according to the relationship between entities mentioned, to help find the role of the entity in which the searcher is interested.</abstract>
    </paper>
    <paper id="391">
      <author><first>Wan Yu</first><last>Ho</last></author>
      <author><first>Christine</first><last>Kng</last></author>
      <author><first>Shan</first><last>Wang</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <title>Identifying Idioms in <fixed-case>C</fixed-case>hinese Translations</title>
      <pages>716–721</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/462_Paper.pdf</url>
      <abstract>Optimally, a translated text should preserve information while maintaining the writing style of the original. When this is not possible, as is often the case with figurative speech, a common practice is to simplify and make explicit the implications. However, in our investigations of translations from English to another language, English-to-Chinese texts were often found to include idiomatic expressions (usually in the form of Chengyu æè ̄) where there were originally no idiomatic, metaphorical, or even figurative expressions. We have created an initial small lexicon of Chengyu, with which we can use to find all occurrences of Chengyu in a given corpus, and will continue to expand the database. By examining the rates and patterns of occurrence across four genres in the NTU Multilingual Corpus, a resource may be created to aid machine translation or, going further, predict Chinese translational trends in any given genre.</abstract>
    </paper>
    <paper id="392">
      <author><first>Thierry</first><last>Etchegoyhen</last></author>
      <author><first>Lindsay</first><last>Bywood</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <author><first>Panayota</first><last>Georgakopoulou</last></author>
      <author><first>Jie</first><last>Jiang</last></author>
      <author><first>Gerard</first><last>van Loenhout</last></author>
      <author><first>Arantza</first><last>del Pozo</last></author>
      <author><first>Mirjam Sepesy</first><last>Maučec</last></author>
      <author><first>Anja</first><last>Turner</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <title>Machine Translation for Subtitling: A Large-Scale Evaluation</title>
      <pages>46–53</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/463_Paper.pdf</url>
      <abstract>This article describes a large-scale evaluation of the use of Statistical Machine Translation for professional subtitling. The work was carried out within the FP7 EU-funded project SUMAT and involved two rounds of evaluation: a quality evaluation and a measure of productivity gain/loss. We present the SMT systems built for the project and the corpora they were trained on, which combine professionally created and crowd-sourced data. Evaluation goals, methodology and results are presented for the eleven translation pairs that were evaluated by professional subtitlers. Overall, a majority of the machine translated subtitles received good quality ratings. The results were also positive in terms of productivity, with a global gain approaching 40%. We also evaluated the impact of applying quality estimation and filtering of poor MT output, which resulted in higher productivity gains for filtered files as opposed to fully machine-translated files. Finally, we present and discuss feedback from the subtitlers who participated in the evaluation, a key aspect for any eventual adoption of machine translation technology in professional subtitling.</abstract>
    </paper>
    <paper id="393">
      <author><first>Elisabetta</first><last>Jezek</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <author><first>Anna</first><last>Feltracco</last></author>
      <author><first>Alessia</first><last>Bianchini</last></author>
      <author><first>Octavian</first><last>Popescu</last></author>
      <title><fixed-case>T</fixed-case>-<fixed-case>PAS</fixed-case>; A resource of Typed Predicate Argument Structures for linguistic analysis and semantic processing</title>
      <pages>890–895</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/465_Paper.pdf</url>
      <abstract>The goal of this paper is to introduce T-PAS, a resource of typed predicate argument structures for Italian, acquired from corpora by manual clustering of distributional information about Italian verbs, to be used for linguistic analysis and semantic processing tasks. T-PAS is the first resource for Italian in which semantic selection properties and sense-in-context distinctions of verbs are characterized fully on empirical ground. In the paper, we first describe the process of pattern acquisition and corpus annotation (section 2) and its ongoing evaluation (section 3). We then demonstrate the benefits of pattern tagging for NLP purposes (section 4), and discuss current effort to improve the annotation of the corpus (section 5). We conclude by reporting on ongoing experiments using semiautomatic techniques for extending coverage (section 6).</abstract>
    </paper>
    <paper id="394">
      <author><first>Kara</first><last>Warburton</last></author>
      <title>Narrowing the Gap Between Termbases and Corpora in Commercial Environments</title>
      <pages>722–727</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/466_Paper.pdf</url>
      <abstract>Terminological resources offer potential to support applications beyond translation, such as controlled authoring and indexing, which are increasingly of interest to commercial enterprises. The ad-hoc semasiological approach adopted by commercial terminographers diverges considerably from methodologies prescribed by conventional theory. The notion of termhood in such production-oriented environments is driven by pragmatic criteria such as frequency and repurposability of the terminological unit. A high degree of correspondence between the commercial corpus and the termbase is desired. Research carried out at the City University of Hong Kong using four IT companies as case studies revealed a large gap between corpora and termbases. Problems in selecting terms and in encoding them properly in termbases account for a significant portion of this gap. A rigorous corpus-based approach to term selection would significantly reduce this gap and improve the effectiveness of commercial termbases. In particular, single-word terms (keywords) identified by comparison to a reference corpus offer great potential for identifying important multi-word terms in this context. We conclude that terminography for production purposes should be more corpus-based than is currently the norm.</abstract>
    </paper>
    <paper id="395">
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <title>Author-Specific Sentiment Aggregation for Polarity Prediction of Reviews</title>
      <pages>3092–3099</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/467_Paper.pdf</url>
      <abstract>In this work, we propose an author-specific sentiment aggregation model for polarity prediction of reviews using an ontology. We propose an approach to construct a Phrase Annotated Author Specific Sentiment Ontology Tree (PASOT), where the facet nodes are annotated with opinion phrases of the author, used to describe the facets, as well as the author’s preference for the facets. We show that an author-specific aggregation of sentiment over an ontology fares better than a flat classification model, which does not take the domain-specific facet importance or author-specific facet preference into account. We compare our approach to supervised classification using Support Vector Machines, as well as other baselines from previous works, where we achieve an accuracy improvement of 7.55% over the SVM baseline. Furthermore, we also show the effectiveness of our approach in capturing thwarting in reviews, achieving an accuracy improvement of 11.53% over the SVM baseline.</abstract>
    </paper>
    <paper id="396">
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <title>Clustering of Multi-Word Named Entity variants: Multilingual Evaluation</title>
      <pages>2548–2553</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/468_Paper.pdf</url>
      <abstract>Multi-word entities, such as organisation names, are frequently written in many different ways. We have previously automatically identified over one million acronym pairs in 22 languages, consisting of their short form (e.g. EC) and their corresponding long forms (e.g. European Commission, European Union Commission). In order to automatically group such long form variants as belonging to the same entity, we cluster them, using bottom-up hierarchical clustering and pair-wise string similarity metrics. In this paper, we address the issue of how to evaluate the named entity variant clusters automatically, with minimal human annotation effort. We present experiments that make use of Wikipedia redirection tables and we show that this method produces good results.</abstract>
    </paper>
    <paper id="397">
      <author><first>Richard</first><last>Sproat</last></author>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>HyunJeong</first><last>Choe</last></author>
      <author><first>David</first><last>Huynh</last></author>
      <author><first>Linne</first><last>Ha</last></author>
      <author><first>Ravindran</first><last>Rajakumar</last></author>
      <author><first>Evelyn</first><last>Wenzel-Grondie</last></author>
      <title>A Database for Measuring Linguistic Information Content</title>
      <pages>967–974</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/47_Paper.pdf</url>
      <abstract>Which languages convey the most information in a given amount of space? This is a question often asked of linguists, especially by engineers who often have some information theoretic measure of information in mind, but rarely define exactly how they would measure that information. The question is, in fact remarkably hard to answer, and many linguists consider it unanswerable. But it is a question that seems as if it ought to have an answer. If one had a database of close translations between a set of typologically diverse languages, with detailed marking of morphosyntactic and morphosemantic features, one could hope to quantify the differences between how these different languages convey information. Since no appropriate database exists we decided to construct one. The purpose of this paper is to present our work on the database, along with some preliminary results. We plan to release the dataset once complete.</abstract>
    </paper>
    <paper id="398">
      <author><first>Noushin Rezapour</first><last>Asheghi</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <title>Designing and Evaluating a Reliable Corpus of Web Genres via Crowd-Sourcing</title>
      <pages>1339–1346</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/470_Paper.pdf</url>
      <abstract>Research in Natural Language Processing often relies on a large collection of manually annotated documents. However, currently there is no reliable genre-annotated corpus of web pages to be employed in Automatic Genre Identification (AGI). In AGI, documents are classified based on their genres rather than their topics or subjects. The major shortcoming of available web genre collections is their relatively low inter-coder agreement. Reliability of annotated data is an essential factor for reliability of the research result. In this paper, we present the first web genre corpus which is reliably annotated. We developed precise and consistent annotation guidelines which consist of well-defined and well-recognized categories. For annotating the corpus, we used crowd-sourcing which is a novel approach in genre annotation. We computed the overall as well as the individual categories’ chance-corrected inter-annotator agreement. The results show that the corpus has been annotated reliably.</abstract>
    </paper>
    <paper id="399">
      <author><first>Héctor Martínez</first><last>Alonso</last></author>
      <author><first>Lauren</first><last>Romeo</last></author>
      <title>Crowdsourcing as a preprocessing for complex semantic annotation tasks</title>
      <pages>229–234</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/471_Paper.pdf</url>
      <abstract>This article outlines a methodology that uses crowdsourcing to reduce the workload of experts for complex semantic tasks. We split turker-annotated datasets into a high-agreement block, which is not modified, and a low-agreement block, which is re-annotated by experts. The resulting annotations have higher observed agreement. We identify different biases in the annotation for both turkers and experts.</abstract>
    </paper>
    <paper id="400">
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <title>Automatic Annotation of Machine Translation Datasets with Binary Quality Judgements</title>
      <pages>1788–1792</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/473_Paper.pdf</url>
      <abstract>The automatic estimation of machine translation (MT) output quality is an active research area due to its many potential applications (e.g. aiding human translation and post-editing, re-ranking MT hypotheses, MT system combination). Current approaches to the task rely on supervised learning methods for which high-quality labelled data is fundamental. In this framework, quality estimation (QE) has been mainly addressed as a regression problem where models trained on (source, target) sentence pairs annotated with continuous scores (in the [0-1] interval) are used to assign quality scores (in the same interval) to unseen data. Such definition of the problem assumes that continuous scores are informative and easily interpretable by different users. These assumptions, however, conflict with the subjectivity inherent to human translation and evaluation. On one side, the subjectivity of human judgements adds noise and biases to annotations based on scaled values. This problem reduces the usability of the resulting datasets, especially in application scenarios where a sharp distinction between good and bad translations is needed. On the other side, continuous scores are not always sufficient to decide whether a translation is actually acceptable or not. To overcome these issues, we present an automatic method for the annotation of (source, target) pairs with binary judgements that reflect an empirical, and easily interpretable notion of quality. The method is applied to annotate with binary judgements three QE datasets for different language combinations. The three datasets are combined in a single resource, called BinQE, which can be freely downloaded from http://hlt.fbk.eu/technologies/binqe.</abstract>
    </paper>
    <paper id="401">
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Emilia</first><last>Verzeni</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <title>Semantic Clustering of Pivot Paraphrases</title>
      <pages>4270–4275</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/475_Paper.pdf</url>
      <abstract>Paraphrases extracted from parallel corpora by the pivot method (Bannard and Callison-Burch, 2005) constitute a valuable resource for multilingual NLP applications. In this study, we analyse the semantics of unigram pivot paraphrases and use a graph-based sense induction approach to unveil hidden sense distinctions in the paraphrase sets. The comparison of the acquired senses to gold data from the Lexical Substitution shared task (McCarthy and Navigli, 2007) demonstrates that sense distinctions exist in the paraphrase sets and highlights the need for a disambiguation step in applications using this resource.</abstract>
    </paper>
    <paper id="402">
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <title>When <fixed-case>POS</fixed-case> data sets don’t add up: Combatting sample bias</title>
      <pages>4472–4475</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/476_Paper.pdf</url>
      <abstract>Several works in Natural Language Processing have recently looked into part-of-speech annotation of Twitter data and typically used their own data sets. Since conventions on Twitter change rapidly, models often show sample bias. Training on a combination of the existing data sets should help overcome this bias and produce more robust models than any trained on the individual corpora. Unfortunately, combining the existing corpora proves difficult: many of the corpora use proprietary tag sets that have little or no overlap. Even when mapped to a common tag set, the different corpora systematically differ in their treatment of various tags and tokens. This includes both pre-processing decisions, as well as default labels for frequent tokens, thus exhibiting data bias and label bias, respectively. Only if we address these biases can we combine the existing data sets to also overcome sample bias. We present a systematic study of several Twitter POS data sets, the problems of label and data bias, discuss their effects on model performance, and show how to overcome them to learn models that perform well on various test sets, achieving relative error reduction of up to 21%.</abstract>
    </paper>
    <paper id="403">
      <author><first>Matthew</first><last>Shardlow</last></author>
      <title>Out in the Open: Finding and Categorising Errors in the Lexical Simplification Pipeline</title>
      <pages>1583–1590</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/479_Paper.pdf</url>
      <abstract>Lexical simplification is the task of automatically reducing the complexity of a text by identifying difficult words and replacing them with simpler alternatives. Whilst this is a valuable application of natural language generation, rudimentary lexical simplification systems suffer from a high error rate which often results in nonsensical, non-simple text. This paper seeks to characterise and quantify the errors which occur in a typical baseline lexical simplification system. We expose 6 distinct categories of error and propose a classification scheme for these. We also quantify these errors for a moderate size corpus, showing the magnitude of each error type. We find that for 183 identified simplification instances, only 19 (10.38%) result in a valid simplification, with the rest causing errors of varying gravity.</abstract>
    </paper>
    <paper id="404">
      <author><first>Mark</first><last>Finlayson</last></author>
      <author><first>Jeffry</first><last>Halverson</last></author>
      <author><first>Steven</first><last>Corman</last></author>
      <title>The N2 corpus: A semantically annotated collection of Islamist extremist stories</title>
      <pages>896–902</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/48_Paper.pdf</url>
      <abstract>We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually double-annotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42,480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure.</abstract>
    </paper>
    <paper id="405">
      <author><first>Robert</first><last>Remus</last></author>
      <author><first>Dominique</first><last>Ziegelmayer</last></author>
      <title>Learning from Domain Complexity</title>
      <pages>2021–2028</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/480_Paper.pdf</url>
      <abstract>Sentiment analysis is genre and domain dependent, i.e. the same method performs differently when applied to text that originates from different genres and domains. Intuitively, this is due to different language use in different genres and domains. We measure such differences in a sentiment analysis gold standard dataset that contains texts from 1 genre and 10 domains. Differences in language use are quantified using certain language statistics, viz. domain complexity measures. We investigate 4 domain complexity measures: percentage of rare words, word richness, relative entropy and corpus homogeneity. We relate domain complexity measurements to performance of a standard machine learning-based classifier and find strong correlations. We show that we can accurately estimate its performance based on domain complexity using linear regression models fitted using robust loss functions. Moreover, we illustrate how domain complexity may guide us in model selection, viz. in deciding what word n-gram order to employ in a discriminative model and whether to employ aggressive or conservative word n-gram feature selection.</abstract>
    </paper>
    <paper id="406">
      <author><first>Ahmed</first><last>Abbasi</last></author>
      <author><first>Ammar</first><last>Hassan</last></author>
      <author><first>Milan</first><last>Dhar</last></author>
      <title>Benchmarking <fixed-case>T</fixed-case>witter Sentiment Analysis Tools</title>
      <pages>823–829</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/483_Paper.pdf</url>
      <abstract>Twitter has become one of the quintessential social media platforms for user-generated content. Researchers and industry practitioners are increasingly interested in Twitter sentiments. Consequently, an array of commercial and freely available Twitter sentiment analysis tools have emerged, though it remains unclear how well these tools really work. This study presents the findings of a detailed benchmark analysis of Twitter sentiment analysis tools, incorporating 20 tools applied to 5 different test beds. In addition to presenting detailed performance evaluation results, a thorough error analysis is used to highlight the most prevalent challenges facing Twitter sentiment analysis tools. The results have important implications for various stakeholder groups, including social media analytics researchers, NLP developers, and industry managers and practitioners using social media sentiments as input for decision-making.</abstract>
    </paper>
    <paper id="407">
      <author><first>Camille</first><last>Fauth</last></author>
      <author><first>Anne</first><last>Bonneau</last></author>
      <author><first>Frank</first><last>Zimmerer</last></author>
      <author><first>Juergen</first><last>Trouvain</last></author>
      <author><first>Bistra</first><last>Andreeva</last></author>
      <author><first>Vincent</first><last>Colotte</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <author><first>Denis</first><last>Jouvet</last></author>
      <author><first>Jeanin</first><last>Jügler</last></author>
      <author><first>Yves</first><last>Laprie</last></author>
      <author><first>Odile</first><last>Mella</last></author>
      <author><first>Bernd</first><last>Möbius</last></author>
      <title>Designing a Bilingual Speech Corpus for <fixed-case>F</fixed-case>rench and <fixed-case>G</fixed-case>erman Language Learners: a Two-Step Process</title>
      <pages>1477–1482</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/484_Paper.pdf</url>
      <abstract>We present the design of a corpus of native and non-native speech for the language pair French-German, with a special emphasis on phonetic and prosodic aspects. To our knowledge there is no suitable corpus, in terms of size and coverage, currently available for the target language pair. To select the target L1-L2 interference phenomena we prepare a small preliminary corpus (corpus1), which is analyzed for coverage and cross-checked jointly by French and German experts. Based on this analysis, target phenomena on the phonetic and phonological level are selected on the basis of the expected degree of deviation from the native performance and the frequency of occurrence. 14 speakers performed both L2 (either French or German) and L1 material (either German or French). This allowed us to test, recordings duration, recordings material, the performance of our automatic aligner software. Then, we built corpus2 taking into account what we learned about corpus1. The aims are the same but we adapted speech material to avoid too long recording sessions. 100 speakers will be recorded. The corpus (corpus1 and corpus2) will be prepared as a searchable database, available for the scientific community after completion of the project.</abstract>
    </paper>
    <paper id="408">
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Lorenzo</first><last>Gatti</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Oliviero</first><last>Stock</last></author>
      <title>Creative language explorations through a high-expressivity <fixed-case>N</fixed-case>-grams query language</title>
      <pages>4326–4330</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/486_Paper.pdf</url>
      <abstract>In computation linguistics a combination of syntagmatic and paradigmatic features is often exploited. While the first aspects are typically managed by information present in large n-gram databases, domain and ontological aspects are more properly modeled by lexical ontologies such as WordNet and semantic similarity spaces. This interconnection is even stricter when we are dealing with creative language phenomena, such as metaphors, prototypical properties, puns generation, hyperbolae and other rhetorical phenomena. This paper describes a way to focus on and accomplish some of these tasks by exploiting NgramQuery, a generalized query language on Google N-gram database. The expressiveness of this query language is boosted by plugging semantic similarity acquired both from corpora (e.g. LSA) and from WordNet, also integrating operators for phonetics and sentiment analysis. The paper reports a number of examples of usage in some creative language tasks.</abstract>
    </paper>
    <paper id="409">
      <author><first>Reinhard</first><last>Rapp</last></author>
      <title>Using Word Familiarities and Word Associations to Measure Corpus Representativeness</title>
      <pages>2029–2036</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/492_Paper.pdf</url>
      <abstract>The definition of corpus representativeness used here assumes that a representative corpus should reflect as well as possible the average language use a native speaker encounters in everyday life over a longer period of time. As it is not practical to observe people’s language input over years, we suggest to utilize two types of experimental data capturing two forms of human intuitions: Word familiarity norms and word association norms. If it is true that human language acquisition is corpus-based, such data should reflect people’s perceived language input. Assuming so, we compute a representativeness score for a corpus by extracting word frequency and word association statistics from it and by comparing these statistics to the human data. The higher the similarity, the more representative the corpus should be for the language environments of the test persons. We present results for five different corpora and for truncated versions thereof. The results confirm the expectation that corpus size and corpus balance are crucial aspects for corpus representativeness.</abstract>
    </paper>
    <paper id="410">
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Corentin</first><last>Ribeyre</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <title>Deep Syntax Annotation of the Sequoia <fixed-case>F</fixed-case>rench Treebank</title>
      <pages>2298–2305</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/494_Paper.pdf</url>
      <abstract>We define a deep syntactic representation scheme for French, which abstracts away from surface syntactic variation and diathesis alternations, and describe the annotation of deep syntactic representations on top of the surface dependency trees of the Sequoia corpus. The resulting deep-annotated corpus, named deep-sequoia, is freely available, and hopefully useful for corpus linguistics studies and for training deep analyzers to prepare semantic analysis.</abstract>
    </paper>
    <paper id="411">
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Pascal</first><last>Amsili</last></author>
      <author><first>Lucie</first><last>Barque</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <author><first>Marianne</first><last>Djemaa</last></author>
      <author><first>Pauline</first><last>Haas</last></author>
      <author><first>Richard</first><last>Huyghe</last></author>
      <author><first>Yvette Yannick</first><last>Mathieu</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Laure</first><last>Vieu</last></author>
      <title>Developing a <fixed-case>F</fixed-case>rench <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et: Methodology and First results</title>
      <pages>1372–1379</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/496_Paper.pdf</url>
      <abstract>The Asfalda project aims to develop a French corpus with frame-based semantic annotations and automatic tools for shallow semantic analysis. We present the first part of the project: focusing on a set of notional domains, we delimited a subset of English frames, adapted them to French data when necessary, and developed the corresponding French lexicon. We believe that working domain by domain helped us to enforce the coherence of the resulting resource, and also has the advantage that, though the number of frames is limited (around a hundred), we obtain full coverage within a given domain.</abstract>
    </paper>
    <paper id="412">
      <author><first>Marta</first><last>Sabou</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <author><first>Arno</first><last>Scharl</last></author>
      <title>Corpus Annotation through Crowdsourcing: Towards Best Practice Guidelines</title>
      <pages>859–866</pages>
      <abstract>Crowdsourcing is an emerging collaborative approach that can be used for the acquisition of annotated corpora and a wide range of other linguistic resources. Although the use of this approach is intensifying in all its key genres (paid-for crowdsourcing, games with a purpose, volunteering-based approaches), the community still lacks a set of best-practice guidelines similar to the annotation best practices for traditional, expert-based corpus acquisition. In this paper we focus on the use of crowdsourcing methods for corpus acquisition and propose a set of best practice guidelines based in our own experiences in this area and an overview of related literature. We also introduce GATE Crowd, a plugin of the GATE platform that relies on these guidelines and offers tool support for using crowdsourcing in a more principled and efficient manner.</abstract>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/497_Paper.pdf</url>
    </paper>
    <paper id="413">
      <author><first>Ioannis</first><last>Korkontzelos</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <title>Locating Requests among Open Source Software Communication Messages</title>
      <pages>1347–1354</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/498_Paper.pdf</url>
      <abstract>As a first step towards assessing the quality of support offered online for Open Source Software (OSS), we address the task of locating requests, i.e., messages that raise an issue to be addressed by the OSS community, as opposed to any other message. We present a corpus of online communication messages randomly sampled from newsgroups and bug trackers, manually annotated as requests or non-requests. We identify several linguistically shallow, content-based heuristics that correlate with the classification and investigate the extent to which they can serve as independent classification criteria. Then, we train machine-learning classifiers on these heuristics. We experiment with a wide range of settings, such as different learners, excluding some heuristics and adding unigram features of various parts-of-speech and frequency. We conclude that some heuristics can perform well, while their accuracy can be improved further using machine learning, at the cost of obtaining manual annotations.</abstract>
    </paper>
    <paper id="414">
      <author><first>Kateřina</first><last>Rysová</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <title>Valency and Word Order in <fixed-case>C</fixed-case>zech — A Corpus Probe</title>
      <pages>975–980</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/50_Paper.pdf</url>
      <abstract>We present a part of broader research on word order aiming at finding factors influencing word order in Czech (i.e. in an inflectional language) and their intensity. The main aim of the paper is to test a hypothesis that obligatory adverbials (in terms of the valency) follow the non-obligatory (i.e. optional) ones in the surface word order. The determined hypothesis was tested by creating a list of features for the decision trees algorithm and by searching in data of the Prague Dependency Treebank using the search tool PML Tree Query. Apart from the valency, our experiment also evaluates importance of several other features, such as argument length and deep syntactic function. Neither of the used methods has proved the given hypothesis but according to the results, there are several other features that influence word order of contextually non-bound free modifiers of a verb in Czech, namely position of the sentence in the text, form and length of the verb modifiers (the whole subtrees), and the semantic dependency relation (functor) of the modifiers.</abstract>
    </paper>
    <paper id="415">
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Hans-Ulrich</first><last>Krieger</last></author>
      <title>Harmonization of <fixed-case>G</fixed-case>erman Lexical Resources for Opinion Mining</title>
      <pages>3872–3876</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/500_Paper.pdf</url>
      <abstract>We present on-going work on the harmonization of existing German lexical resources in the field of opinion and sentiment mining. The input of our harmonization effort consisted in four distinct lexicons of German word forms, encoded either as lemmas or as full forms, marked up with polarity features, at distinct granularity levels. We describe how the lexical resources have been mapped onto each other, generating a unique list of entries, with unified Part-of-Speech information and basic polarity features. Future work will be dedicated to the comparison of the harmonized lexicon with German corpora annotated with polarity information. We are further aiming at both linking the harmonized German lexical resources with similar resources in other languages and publishing the resulting set of lexical data in the context of the Linguistic Linked Open Data cloud.</abstract>
    </paper>
    <paper id="416">
      <author><first>Magda</first><last>Ševčíková</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <title>Word-Formation Network for <fixed-case>C</fixed-case>zech</title>
      <pages>1087–1093</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/501_Paper.pdf</url>
      <abstract>In the present paper, we describe the development of the lexical network DeriNet, which captures core word-formation relations on the set of around 266 thousand Czech lexemes. The network is currently limited to derivational relations because derivation is the most frequent and most productive word-formation process in Czech. This limitation is reflected in the architecture of the network: each lexeme is allowed to be linked up with just a single base word; composition as well as combined processes (composition with derivation) are thus not included. After a brief summarization of theoretical descriptions of Czech derivation and the state of the art of NLP approaches to Czech derivation, we discuss the linguistic background of the network and introduce the formal structure of the network and the semi-automatic annotation procedure. The network was initialized with a set of lexemes whose existence was supported by corpus evidence. Derivational links were created using three sources of information: links delivered by a tool for morphological analysis, links based on an automatically discovered set of derivation rules, and on a grammar-based set of rules. Finally, we propose some research topics which could profit from the existence of such lexical network.</abstract>
    </paper>
    <paper id="417">
      <author><first>Jamie</first><last>Bost</last></author>
      <author><first>Johanna</first><last>Moore</last></author>
      <title>An Analysis of Older Users’ Interactions with Spoken Dialogue Systems</title>
      <pages>1176–1181</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/502_Paper.pdf</url>
      <abstract>This study explores communication differences between older and younger users with a task-oriented spoken dialogue system. Previous analyses of the MATCH corpus show that older users have significantly longer dialogues than younger users and that they are less satisfied with the system. Open questions remain regarding the relationship between information recall and cognitive abilities. This study documents a length annotation scheme designed to explore causes of additional length in the dialogues and the relationships between length, cognitive abilities, user satisfaction, and information recall. Results show that primary causes of older users additional length include using polite vocabulary, providing additional information relevant to the task, and using full sentences to respond to the system. Regression models were built to predict length from cognitive abilities and user satisfaction from length. Overall, users with higher cognitive ability scores had shorter dialogues than users with lower cognitive ability scores, and users with shorter dialogues were more satisfied with the system than users with longer dialogues. Dialogue length and cognitive abilities were significantly correlated with information recall. Overall, older users tended to use a human-to-human communication style with the system, whereas younger users tended to adopt a factual interaction style.</abstract>
    </paper>
    <paper id="418">
      <author><first>Martin</first><last>Volk</last></author>
      <author><first>Johannes</first><last>Graën</last></author>
      <author><first>Elena</first><last>Callegaro</last></author>
      <title>Innovations in Parallel Corpus Search Tools</title>
      <pages>3172–3178</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/504_Paper.pdf</url>
      <abstract>Recent years have seen an increased interest in and availability of parallel corpora. Large corpora from international organizations (e.g. European Union, United Nations, European Patent Office), or from multilingual Internet sites (e.g. OpenSubtitles) are now easily available and are used for statistical machine translation but also for online search by different user groups. This paper gives an overview of different usages and different types of search systems. In the past, parallel corpus search systems were based on sentence-aligned corpora. We argue that automatic word alignment allows for major innovations in searching parallel corpora. Some online query systems already employ word alignment for sorting translation variants, but none supports the full query functionality that has been developed for parallel treebanks. We propose to develop such a system for efficiently searching large parallel corpora with a powerful query language.</abstract>
    </paper>
    <paper id="419">
      <author><first>Joaquim</first><last>Moré</last></author>
      <author><first>Salvador</first><last>Climent</last></author>
      <title>Machine Translationness: Machine-likeness in Machine Translation Evaluation</title>
      <pages>54–61</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/506_Paper.pdf</url>
      <abstract>Machine translationness (MTness) is the linguistic phenomena that make machine translations distinguishable from human translations. This paper intends to present MTness as a research object and suggests an MT evaluation method based on determining whether the translation is machine-like instead of determining its human-likeness as in evaluation current approaches. The method rates the MTness of a translation with a metric, the MTS (Machine Translationness Score). The MTS calculation is in accordance with the results of an experimental study on machine translation perception by common people. MTS proved to correlate well with human ratings on translation quality. Besides, our approach allows the performance of cheap evaluations since expensive resources (e.g. reference translations, training corpora) are not needed. The paper points out the challenge of dealing with MTness as an everyday phenomenon caused by the massive use of MT.</abstract>
    </paper>
    <paper id="420">
      <author><first>Mikaël</first><last>Morardo</last></author>
      <author><first>Éric</first><last>Villemonte de la Clergerie</last></author>
      <title>Towards an environment for the production and the validation of lexical semantic resources</title>
      <pages>867–874</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/507_Paper.pdf</url>
      <abstract>We present the components of a processing chain for the creation, visualization, and validation of lexical resources (formed of terms and relations between terms). The core of the chain is a component for building lexical networks relying on Harris’ distributional hypothesis applied on the syntactic dependencies produced by the French parser FRMG on large corpora. Another important aspect concerns the use of an online interface for the visualization and collaborative validation of the resulting resources.</abstract>
    </paper>
    <paper id="421">
      <author><first>Jonathan</first><last>Gratch</last></author>
      <author><first>Ron</first><last>Artstein</last></author>
      <author><first>Gale</first><last>Lucas</last></author>
      <author><first>Giota</first><last>Stratou</last></author>
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Angela</first><last>Nazarian</last></author>
      <author><first>Rachel</first><last>Wood</last></author>
      <author><first>Jill</first><last>Boberg</last></author>
      <author><first>David</first><last>DeVault</last></author>
      <author><first>Stacy</first><last>Marsella</last></author>
      <author><first>David</first><last>Traum</last></author>
      <author><first>Skip</first><last>Rizzo</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <title>The Distress Analysis Interview Corpus of human and computer interviews</title>
      <pages>3123–3128</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/508_Paper.pdf</url>
      <abstract>The Distress Analysis Interview Corpus (DAIC) contains clinical interviews designed to support the diagnosis of psychological distress conditions such as anxiety, depression, and post traumatic stress disorder. The interviews are conducted by humans, human controlled agents and autonomous agents, and the participants include both distressed and non-distressed individuals. Data collected include audio and video recordings and extensive questionnaire responses; parts of the corpus have been transcribed and annotated for a variety of verbal and non-verbal features. The corpus has been used to support the creation of an automated interviewer agent, and for research on the automatic identification of psychological distress.</abstract>
    </paper>
    <paper id="422">
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Tatsuya</first><last>Watanabe</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <title>Representing Multimodal Linguistic Annotated data</title>
      <pages>3386–3392</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/51_Paper.pdf</url>
      <abstract>The question of interoperability for linguistic annotated resources covers different aspects. First, it requires a representation framework making it possible to compare, and eventually merge, different annotation schema. In this paper, a general description level representing the multimodal linguistic annotations is proposed. It focuses on time representation and on the data content representation: This paper reconsiders and enhances the current and generalized representation of annotations. An XML schema of such annotations is proposed. A Python API is also proposed. This framework is implemented in a multi-platform software and distributed under the terms of the GNU Public License.</abstract>
    </paper>
    <paper id="423">
      <author><first>Timur</first><last>Gilmanov</last></author>
      <author><first>Olga</first><last>Scrivner</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <title><fixed-case>SWIFT</fixed-case> Aligner, A Multifunctional Tool for Parallel Corpora: Visualization, Word Alignment, and (Morpho)-Syntactic Cross-Language Transfer</title>
      <pages>2913–2919</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/510_Paper.pdf</url>
      <abstract>It is well known that word aligned parallel corpora are valuable linguistic resources. Since many factors affect automatic alignment quality, manual post-editing may be required in some applications. While there are several state-of-the-art word-aligners, such as GIZA++ and Berkeley, there is no simple visual tool that would enable correcting and editing aligned corpora of different formats. We have developed SWIFT Aligner, a free, portable software that allows for visual representation and editing of aligned corpora from several most commonly used formats: TALP, GIZA, and NAACL. In addition, our tool has incorporated part-of-speech and syntactic dependency transfer from an annotated source language into an unannotated target language, by means of word-alignment.</abstract>
    </paper>
    <paper id="424">
      <author><first>Rosemary</first><last>Orr</last></author>
      <author><first>Marijn</first><last>Huijbregts</last></author>
      <author><first>Roeland</first><last>van Beek</last></author>
      <author><first>Lisa</first><last>Teunissen</last></author>
      <author><first>Kate</first><last>Backhouse</last></author>
      <author><first>David</first><last>van Leeuwen</last></author>
      <title>Semi-automatic annotation of the <fixed-case>UCU</fixed-case> accents speech corpus</title>
      <pages>1483–1487</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/511_Paper.pdf</url>
      <abstract>Annotation and labeling of speech tasks in large multitask speech corpora is a necessary part of preparing a corpus for distribution. We address three approaches to annotation and labeling: manual, semi automatic and automatic procedures for labeling the UCU Accent Project speech data, a multilingual multitask longitudinal speech corpus. Accuracy and minimal time investment are the priorities in assessing the efficacy of each procedure. While manual labeling based on aural and visual input should produce the most accurate results, this approach is error-prone because of its repetitive nature. A semi automatic event detection system requiring manual rejection of false alarms and location and labeling of misses provided the best results. A fully automatic system could not be applied to entire speech recordings because of the variety of tasks and genres. However, it could be used to annotate separate sentences within a specific task. Acoustic confidence measures can correctly detect sentences that do not match the text with an EER of 3.3%</abstract>
    </paper>
    <paper id="425">
      <author><first>Daniela</first><last>Amaral</last></author>
      <author><first>Evandro</first><last>Fonseca</last></author>
      <author><first>Lucelene</first><last>Lopes</last></author>
      <author><first>Renata</first><last>Vieira</last></author>
      <title>Comparative Analysis of <fixed-case>P</fixed-case>ortuguese Named Entities Recognition Tools</title>
      <pages>2554–2558</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/513_Paper.pdf</url>
      <abstract>This paper describes an experiment to compare four tools to recognize named entities in Portuguese texts. The experiment was made over the HAREM corpora, a golden standard for named entities recognition in Portuguese. The tools experimented are based on natural language processing techniques and also machine learning. Specifically, one of the tools is based on Conditional random fields, an unsupervised machine learning model that has being used to named entities recognition in several languages, while the other tools follow more traditional natural language approaches. The comparison results indicate advantages for different tools according to the different classes of named entities. Despite of such balance among tools, we conclude pointing out foreseeable advantages to the machine learning based tool.</abstract>
    </paper>
    <paper id="426">
      <author><first>Ana Lúcia</first><last>Santos</last></author>
      <author><first>Michel</first><last>Généreux</last></author>
      <author><first>Aida</first><last>Cardoso</last></author>
      <author><first>Celina</first><last>Agostinho</last></author>
      <author><first>Silvana</first><last>Abalada</last></author>
      <title>A corpus of <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese child and child-directed speech</title>
      <pages>1488–1491</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/514_Paper.pdf</url>
      <abstract>We present a corpus of child and child-directed speech of European Portuguese. This corpus results from the expansion of an already existing database (Santos, 2006). It includes around 52 hours of child-adult interaction and now contains 27,595 child utterances and 70,736 adult utterances. The corpus was transcribed according to the CHILDES system (Child Language Data Exchange System) and using the CLAN software (MacWhinney, 2000). The corpus itself represents a valuable resource for the study of lexical, syntax and discourse acquisition. In this paper, we also show how we used an existing part-of-speech tagger trained on written material (Généreux, Hendrickx &amp; Mendes, 2012) to automatically lemmatize and tag child and child-directed speech and generate a line with part-of-speech information compatible with the CLAN interface. We show that a POS-tagger trained on the analysis of written language can be exploited for the treatment of spoken material with minimal effort, with only a small number of written rules assisting the statistical model.</abstract>
    </paper>
    <paper id="427">
      <author><first>Guntis</first><last>Barzdins</last></author>
      <author><first>Didzis</first><last>Gosko</last></author>
      <author><first>Laura</first><last>Rituma</last></author>
      <author><first>Peteris</first><last>Paikens</last></author>
      <title>Using C5.0 and Exhaustive Search for Boosting Frame-Semantic Parsing Accuracy</title>
      <pages>4476–4482</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/515_Paper.pdf</url>
      <abstract>Frame-semantic parsing is a kind of automatic semantic role labeling performed according to the FrameNet paradigm. The paper reports a novel approach for boosting frame-semantic parsing accuracy through the use of the C5.0 decision tree classifier, a commercial version of the popular C4.5 decision tree classifier, and manual rule enhancement. Additionally, the possibility to replace C5.0 by an exhaustive search based algorithm (nicknamed C6.0) is described, leading to even higher frame-semantic parsing accuracy at the expense of slightly increased training time. The described approach is particularly efficient for languages with small FrameNet annotated corpora as it is for Latvian, which is used for illustration. Frame-semantic parsing accuracy achieved for Latvian through the C6.0 algorithm is on par with the state-of-the-art English frame-semantic parsers. The paper includes also a frame-semantic parsing use-case for extracting structured information from unstructured newswire texts, sometimes referred to as bridging of the semantic gap.</abstract>
    </paper>
    <paper id="428">
      <author><first>Verena</first><last>Lyding</last></author>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Egon</first><last>Stemle</last></author>
      <title>‘inter<fixed-case>H</fixed-case>ist’ Ì¶ an interactive visual interface for corpus exploration</title>
      <pages>635–641</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/517_Paper.pdf</url>
      <abstract>In this article, we present interHist, a compact visualization for the interactive exploration of results to complex corpus queries. Integrated with a search interface to the PAISA corpus of Italian web texts, interHist aims at facilitating the exploration of large results sets to linguistic corpus searches. This objective is approached by providing an interactive visual overview of the data, which supports the user-steered navigation by means of interactive filtering. It allows to dynamically switch between an overview on the data and a detailed view on results in their immediate textual context, thus helping to detect and inspect relevant hits more efficiently. We provide background information on corpus linguistics and related work on visualizations for language and linguistic data. We introduce the architecture of interHist, by detailing the data structure it relies on, describing the visualization design and providing technical details of the implementation and its integration with the corpus querying environment. Finally, we illustrate its usage by presenting a use case for the analysis of the composition of Italian noun phrases.</abstract>
    </paper>
    <paper id="429">
      <author><first>Rodrigo</first><last>Boos</last></author>
      <author><first>Kassius</first><last>Prestes</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <title>Identification of Multiword Expressions in the br<fixed-case>W</fixed-case>a<fixed-case>C</fixed-case></title>
      <pages>728–735</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/518_Paper.pdf</url>
      <abstract>Although corpus size is a well known factor that affects the performance of many NLP tasks, for many languages large freely available corpora are still scarce. In this paper we describe one effort to build a very large corpus for Brazilian Portuguese, the brWaC, generated following the Web as Corpus kool initiative. To indirectly assess the quality of the resulting corpus we examined the impact of corpus origin in a specific task, the identification of Multiword Expressions with association measures, against a standard corpus. Focusing on nominal compounds, the expressions obtained from each corpus are of comparable quality and indicate that corpus origin has no impact on this task.</abstract>
    </paper>
    <paper id="430">
      <author><first>Lis</first><last>Pereira</last></author>
      <author><first>Elga</first><last>Strafella</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <title>Collocation or Free Combination? — Applying Machine Translation Techniques to identify collocations in <fixed-case>J</fixed-case>apanese</title>
      <pages>736–739</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/519_Paper.pdf</url>
      <abstract>This work presents an initial investigation on how to distinguish collocations from free combinations. The assumption is that, while free combinations can be literally translated, the overall meaning of collocations is different from the sum of the translation of its parts. Based on that, we verify whether a machine translation system can help us perform such distinction. Results show that it improves the precision compared with standard methods of collocation identification through statistical association measures.</abstract>
    </paper>
    <paper id="431">
      <author><first>Adam</first><last>Kilgarriff</last></author>
      <author><first>Pavel</first><last>Rychlý</last></author>
      <author><first>Miloš</first><last>Jakubíček</last></author>
      <author><first>Vojtěch</first><last>Kovář</last></author>
      <author><first>Vít</first><last>Baisa</last></author>
      <author><first>Lucia</first><last>Kocincová</last></author>
      <title>Extrinsic Corpus Evaluation with a Collocation Dictionary Task</title>
      <pages>545–552</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/52_Paper.pdf</url>
      <abstract>The NLP researcher or application-builder often wonders “what corpus should I use, or should I build one of my own? If I build one of my own, how will I know if I have done a good job?” Currently there is very little help available for them. They are in need of a framework for evaluating corpora. We develop such a framework, in relation to corpora which aim for good coverage of `general language’. The task we set is automatic creation of a publication-quality collocations dictionary. For a sample of 100 headwords of Czech and 100 of English, we identify a gold standard dataset of (ideally) all the collocations that should appear for these headwords in such a dictionary. The datasets are being made available alongside this paper. We then use them to determine precision and recall for a range of corpora, with a range of parameters.</abstract>
    </paper>
    <paper id="432">
      <author><first>Dominique</first><last>Estival</last></author>
      <author><first>Steve</first><last>Cassidy</last></author>
      <author><first>Felicity</first><last>Cox</last></author>
      <author><first>Denis</first><last>Burnham</last></author>
      <title><fixed-case>A</fixed-case>us<fixed-case>T</fixed-case>alk: an audio-visual corpus of <fixed-case>A</fixed-case>ustralian <fixed-case>E</fixed-case>nglish</title>
      <pages>3105–3109</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/520_Paper.pdf</url>
      <abstract>This paper describes the AusTalk corpus, which was designed and created through the Big ASC, a collaborative project with the two main goals of providing a standardised infrastructure for audio-visual recordings in Australia and of producing a large audio-visual corpus of Australian English, with 3 hours of AV recordings for 1000 speakers. We first present the overall project, then describe the corpus itself and its components, the strict data collection protocol with high levels of standardisation and automation, and the processes put in place for quality control. We also discuss the annotation phase of the project, along with its goals and challenges; a major contribution of the project has been to explore procedures for automating annotations and we present our solutions. We conclude with the current status of the corpus and with some examples of research already conducted with this new resource. AusTalk is one of the corpora included in the HCS vLab, which is briefly sketched in the conclusion.</abstract>
    </paper>
    <paper id="433">
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Spencer</first><last>Onuffer</last></author>
      <author><first>Nora</first><last>Kazour</last></author>
      <author><first>Emily</first><last>Danchik</last></author>
      <author><first>Michael T.</first><last>Mordowanec</last></author>
      <author><first>Henrietta</first><last>Conrad</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <title>Comprehensive Annotation of Multiword Expressions in a Social Web Corpus</title>
      <pages>455–461</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/521_Paper.pdf</url>
      <abstract>Multiword expressions (MWEs) are quite frequent in languages such as English, but their diversity, the scarcity of individual MWE types, and contextual ambiguity have presented obstacles to corpus-based studies and NLP systems addressing them as a class. Here we advocate for a comprehensive annotation approach: proceeding sentence by sentence, our annotators manually group tokens into MWEs according to guidelines that cover a broad range of multiword phenomena. Under this scheme, we have fully annotated an English web corpus for multiword expressions, including those containing gaps.</abstract>
    </paper>
    <paper id="434">
      <author><first>Leonardo Sameshima</first><last>Taba</last></author>
      <author><first>Helena</first><last>Caseli</last></author>
      <title>Automatic semantic relation extraction from <fixed-case>P</fixed-case>ortuguese texts</title>
      <pages>2739–2746</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/522_Paper.pdf</url>
      <abstract>Nowadays we are facing a growing demand for semantic knowledge in computational applications, particularly in Natural Language Processing (NLP). However, there aren’t sufficient human resources to produce that knowledge at the same rate of its demand. Considering the Portuguese language, which has few resources in the semantic area, the situation is even more alarming. Aiming to solve that problem, this work investigates how some semantic relations can be automatically extracted from Portuguese texts. The two main approaches investigated here are based on (i) textual patterns and (ii) machine learning algorithms. Thus, this work investigates how and to which extent these two approaches can be applied to the automatic extraction of seven binary semantic relations (is-a, part-of, location-of, effect-of, property-of, made-of and used-for) in Portuguese texts. The results indicate that machine learning, in particular Support Vector Machines, is a promising technique for the task, although textual patterns presented better results for the used-for relation.</abstract>
    </paper>
    <paper id="435">
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Kemal</first><last>Oflazer</last></author>
      <title>A Multidialectal Parallel Corpus of <fixed-case>A</fixed-case>rabic</title>
      <pages>1240–1245</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/523_Paper.pdf</url>
      <abstract>The daily spoken variety of Arabic is often termed the colloquial or dialect form of Arabic. There are many Arabic dialects across the Arab World and within other Arabic speaking communities. These dialects vary widely from region to region and to a lesser extent from city to city in each region. The dialects are not standardized, they are not taught, and they do not have official status. However they are the primary vehicles of communication (face-to-face and recently, online) and have a large presence in the arts as well. In this paper, we present the first multidialectal Arabic parallel corpus, a collection of 2,000 sentences in Standard Arabic, Egyptian, Tunisian, Jordanian, Palestinian and Syrian Arabic, in addition to English. Such parallel data does not exist naturally, which makes this corpus a very valuable resource that has many potential applications such as Arabic dialect identification and machine translation.</abstract>
    </paper>
    <paper id="436">
      <author><first>Costanza</first><last>Navarretta</last></author>
      <author><first>Magdalena</first><last>Lis</last></author>
      <title>Transfer learning of feedback head expressions in <fixed-case>D</fixed-case>anish and <fixed-case>P</fixed-case>olish comparable multimodal corpora</title>
      <pages>3597–3603</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/525_Paper.pdf</url>
      <abstract>The paper is an investigation of the reusability of the annotations of head movements in a corpus in a language to predict the feedback functions of head movements in a comparable corpus in another language. The two corpora consist of naturally occurring triadic conversations in Danish and Polish, which were annotated according to the same scheme. The intersection of common annotation features was used in the experiments. A Naïve Bayes classifier was trained on the annotations of a corpus and tested on the annotations of the other corpus. Training and test datasets were then reversed and the experiments repeated. The results show that the classifier identifies more feedback behaviours than the majority baseline in both cases and the improvements are significant. The performance of the classifier decreases significantly compared with the results obtained when training and test data belong to the same corpus. Annotating multimodal data is resource consuming, thus the results are promising. However, they also confirm preceding studies that have identified both similarities and differences in the use of feedback head movements in different languages. Since our datasets are small and only regard a communicative behaviour in two languages, the experiments should be tested on more data types.</abstract>
    </paper>
    <paper id="437">
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Filip</first><last>Klubička</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Sergio</first><last>Ortiz-Rojas</last></author>
      <author><first>Vassilis</first><last>Papavassiliou</last></author>
      <author><first>Prokopis</first><last>Prokopidis</last></author>
      <title>Comparing two acquisition systems for automatically building an <fixed-case>E</fixed-case>nglish—<fixed-case>C</fixed-case>roatian parallel corpus from multilingual websites</title>
      <pages>1252–1258</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/529_Paper.pdf</url>
      <abstract>In this paper we compare two tools for automatically harvesting bitexts from multilingual websites: bitextor and ILSP-FC. We used both tools for crawling 21 multilingual websites from the tourism domain to build a domain-specific English―Croatian parallel corpus. Different settings were tried for both tools and 10,662 unique document pairs were obtained. A sample of about 10% of them was manually examined and the success rate was computed on the collection of pairs of documents detected by each setting. We compare the performance of the settings and the amount of different corpora detected by each setting. In addition, we describe the resource obtained, both by the settings and through the human evaluation, which has been released as a high-quality parallel corpus.</abstract>
    </paper>
    <paper id="438">
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <author><first>Atefeh</first><last>Farzindar</last></author>
      <title>Hashtag Occurrences, Layout and Translation: A Corpus-driven Analysis of Tweets Published by the <fixed-case>C</fixed-case>anadian Government</title>
      <pages>2254–2261</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/53_Paper.pdf</url>
      <abstract>We present an aligned bilingual corpus of 8758 tweet pairs in French and English, derived from Canadian government agencies. Hashtags appear in a tweet’s prologue, announcing its topic, or in the tweet’s text in lieu of traditional words, or in an epilogue. Hashtags are words prefixed with a pound sign in 80% of the cases. The rest is mostly multiword hashtags, for which we describe a segmentation algorithm. A manual analysis of the bilingual alignment of 5000 hashtags shows that 5% (French) to 18% (English) of them don’t have a counterpart in their containing tweet’s translation. This analysis shows that 80% of multiword hashtags are correctly translated by humans, and that the mistranslation of the rest may be due to incomplete translation directives regarding social media. We show how these resources and their analysis can guide the design of a machine translation pipeline, and its evaluation. A baseline system implementing a tweet-specific tokenizer yields promising results. The system is improved by translating epilogues, prologues, and text separately. We attempt to feed the SMT engine with the original hashtag and some alternatives (“dehashed” version or a segmented version of multiword hashtags), but translation quality improves at the cost of hashtag recall.</abstract>
    </paper>
    <paper id="439">
      <author><first>Siim</first><last>Orasmaa</last></author>
      <title>Towards an Integration of Syntactic and Temporal Annotations in <fixed-case>E</fixed-case>stonian</title>
      <pages>1259–1266</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/530_Paper.pdf</url>
      <abstract>We investigate the question how manually created syntactic annotations can be used to analyse and improve consistency in manually created temporal annotations. Our work introduces an annotation project for Estonian, where temporal annotations in TimeML framework were manually added to a corpus containing gold standard morphological and dependency syntactic annotations. In the first part of our work, we evaluate the consistency of manual temporal annotations, focusing on event annotations. We use syntactic annotations to distinguish different event annotation models, and we observe highest inter-annotator agreements on models representing prototypical events (event verbs and events being part of the syntactic predicate of clause). In the second part of our work, we investigate how to improve consistency between syntactic and temporal annotations. We test on whether syntactic annotations can be used to validate temporal annotations: to find missing or partial annotations. Although the initial results indicate that such validation is promising, we also note that a better bridging between temporal (semantic) and syntactic annotations is needed for a complete automatic validation.</abstract>
    </paper>
    <paper id="440">
      <author><first>Emanuele</first><last>Bastianelli</last></author>
      <author><first>Giuseppe</first><last>Castellucci</last></author>
      <author><first>Danilo</first><last>Croce</last></author>
      <author><first>Luca</first><last>Iocchi</last></author>
      <author><first>Roberto</first><last>Basili</last></author>
      <author><first>Daniele</first><last>Nardi</last></author>
      <title><fixed-case>H</fixed-case>u<fixed-case>RIC</fixed-case>: a Human Robot Interaction Corpus</title>
      <pages>4519–4526</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/531_Paper.pdf</url>
      <abstract>Recent years show the development of large scale resources (e.g. FrameNet for the Frame Semantics) that supported the definition of several state-of-the-art approaches in Natural Language Processing. However, the reuse of existing resources in heterogeneous domains such as Human Robot Interaction is not straightforward. The generalization offered by many data driven methods is strongly biased by the employed data, whose performance in out-of-domain conditions exhibit large drops. In this paper, we present the Human Robot Interaction Corpus (HuRIC). It is made of audio files paired with their transcriptions referring to commands for a robot, e.g. in a home environment. The recorded sentences are annotated with different kinds of linguistic information, ranging from morphological and syntactic information to rich semantic information, according to the Frame Semantics, to characterize robot actions, and Spatial Semantics, to capture the robot environment. All texts are represented through the Abstract Meaning Representation, to adopt a simple but expressive representation of commands, that can be easily translated into the internal representation of the robot.</abstract>
    </paper>
    <paper id="441">
      <author><first>Joke</first><last>Daems</last></author>
      <author><first>Lieve</first><last>Macken</last></author>
      <author><first>Sonia</first><last>Vandepitte</last></author>
      <title>On the origin of errors: A fine-grained analysis of <fixed-case>MT</fixed-case> and <fixed-case>PE</fixed-case> errors and their relationship</title>
      <pages>62–66</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/532_Paper.pdf</url>
      <abstract>In order to improve the symbiosis between machine translation (MT) system and post-editor, it is not enough to know that the output of one system is better than the output of another system. A fine-grained error analysis is needed to provide information on the type and location of errors occurring in MT and the corresponding errors occurring after post-editing (PE). This article reports on a fine-grained translation quality assessment approach which was applied to machine translated-texts and the post-edited versions of these texts, made by student post-editors. By linking each error to the corresponding source text-passage, it is possible to identify passages that were problematic in MT, but not after PE, or passages that were problematic even after PE. This method provides rich data on the origin and impact of errors, which can be used to improve post-editor training as well as machine translation systems. We present the results of a pilot experiment on the post-editing of newspaper articles and highlight the advantages of our approach.</abstract>
    </paper>
    <paper id="442">
      <author><first>Giampiero</first><last>Salvi</last></author>
      <author><first>Niklas</first><last>Vanhainen</last></author>
      <title>The <fixed-case>W</fixed-case>ave<fixed-case>S</fixed-case>urfer Automatic Speech Recognition Plugin</title>
      <pages>3067–3071</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/533_Paper.pdf</url>
      <abstract>This paper presents a plugin that adds automatic speech recognition (ASR) functionality to the WaveSurfer sound manipulation and visualisation program. The plugin allows the user to run continuous speech recognition on spoken utterances, or to align an already available orthographic transcription to the spoken material. The plugin is distributed as free software and is based on free resources, namely the Julius speech recognition engine and a number of freely available ASR resources for different languages. Among these are the acoustic and language models we have created for Swedish using the NST database.</abstract>
    </paper>
    <paper id="443">
      <author><first>Matěj</first><last>Korvas</last></author>
      <author><first>Ondřej</first><last>Plátek</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Lukáš</first><last>Žilka</last></author>
      <author><first>Filip</first><last>Jurčíček</last></author>
      <title>Free <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>zech telephone speech corpus shared under the <fixed-case>CC</fixed-case>-<fixed-case>BY</fixed-case>-<fixed-case>SA</fixed-case> 3.0 license</title>
      <pages>4423–4428</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/535_Paper.pdf</url>
      <abstract>We present a dataset of telephone conversations in English and Czech, developed for training acoustic models for automatic speech recognition (ASR) in spoken dialogue systems (SDSs). The data comprise 45 hours of speech in English and over 18 hours in Czech. Large part of the data, both audio and transcriptions, was collected using crowdsourcing, the rest are transcriptions by hired transcribers. We release the data together with scripts for data pre-processing and building acoustic models using the HTK and Kaldi ASR toolkits. We publish also the trained models described in this paper. The data are released under the CC-BY-SA~3.0 license, the scripts are licensed under Apache~2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.</abstract>
    </paper>
    <paper id="444">
      <author><first>Antje</first><last>Schlaf</last></author>
      <author><first>Claudia</first><last>Bobach</last></author>
      <author><first>Matthias</first><last>Irmer</last></author>
      <title>Creating a Gold Standard Corpus for the Extraction of Chemistry-Disease Relations from Patent Texts</title>
      <pages>2057–2061</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/536_Paper.pdf</url>
      <abstract>This paper describes the creation of a gold standard for chemistry-disease relations in patent texts. We start with an automated annotation of named entities of the domains chemistry (e.g. propranolol) and diseases (e.g. hypertension) as well as of related domains like methods and substances. After that, domain-relevant relations between these entities, e.g. propranolol treats hypertension, have been manually annotated. The corpus is intended to be suitable for developing and evaluating relation extraction methods. In addition, we present two reasoning methods of high precision for automatically extending the set of extracted relations. Chain reasoning provides a method to infer and integrate additional, indirectly expressed relations occurring in relation chains. Enumeration reasoning exploits the frequent occurrence of enumerations in patents and automatically derives additional relations. These two methods are applicable both for verifying and extending the manually annotated data as well as for potential improvements of automatic relation extraction.</abstract>
    </paper>
    <paper id="445">
      <author><first>Anna</first><last>Polychroniou</last></author>
      <author><first>Hugues</first><last>Salamin</last></author>
      <author><first>Alessandro</first><last>Vinciarelli</last></author>
      <title>The <fixed-case>SSPN</fixed-case>et-Mobile Corpus: Social Signal Processing Over Mobile Phones.</title>
      <pages>1492–1498</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/537_Paper.pdf</url>
      <abstract>This article presents the SSPNet-Mobile Corpus, a collection of 60 mobile phone calls between unacquainted individuals (120 subjects). The corpus is designed to support research on non-verbal behavior and it has been manually annotated into conversational topics and behavioral events (laughter, fillers, back-channel, etc.). Furthermore, the corpus includes, for each subject, psychometric questionnaires measuring personality, conflict attitude and interpersonal attraction. Besides presenting the main characteristics of the corpus (scenario, subjects, experimental protocol, sensing approach, psychometric measurements), the paper reviews the main results obtained so far using the data.</abstract>
    </paper>
    <paper id="446">
      <author><first>Alina</first><last>Wróblewska</last></author>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <title>Projection-based Annotation of a <fixed-case>P</fixed-case>olish Dependency Treebank</title>
      <pages>2306–2312</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/538_Paper.pdf</url>
      <abstract>This paper presents an approach of automatic annotation of sentences with dependency structures. The approach builds on the idea of cross-lingual dependency projection. The presented method of acquiring dependency trees involves a weighting factor in the processes of projecting source dependency relations to target sentences and inducing well-formed target dependency trees from sets of projected dependency relations. Using a parallel corpus, source trees are transferred onto equivalent target sentences via an extended set of alignment links. Projected arcs are initially weighted according to the certainty of word alignment links. Then, arc weights are recalculated using a method based on the EM selection algorithm. Maximum spanning trees selected from EM-scored digraphs and labelled with appropriate grammatical functions constitute a target dependency treebank. Extrinsic evaluation shows that parsers trained on such a treebank may perform comparably to parsers trained on a manually developed treebank.</abstract>
    </paper>
    <paper id="447">
      <author><first>Gianluca</first><last>Lebani</last></author>
      <author><first>Veronica</first><last>Viola</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <title>Bootstrapping an <fixed-case>I</fixed-case>talian <fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et: data-driven analysis of verb alternations</title>
      <pages>1127–1134</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/541_Paper.pdf</url>
      <abstract>The goal of this paper is to propose a classification of the syntactic alternations admitted by the most frequent Italian verbs. The data-driven two-steps procedure exploited and the structure of the identified classes of alternations are presented in depth and discussed. Even if this classification has been developed with a practical application in mind, namely the semi-automatic building of a VerbNet-like lexicon for Italian verbs, partly following the methodology proposed in the context of the VerbNet project, its availability may have a positive impact on several related research topics and Natural Language Processing tasks</abstract>
    </paper>
    <paper id="448">
      <author><first>Arda</first><last>Çelebi</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <title>Self-training a Constituency Parser using n-gram Trees</title>
      <pages>2893–2896</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/543_Paper.pdf</url>
      <abstract>In this study, we tackle the problem of self-training a feature-rich discriminative constituency parser. We approach the self-training problem with the assumption that while the full sentence parse tree produced by a parser may contain errors, some portions of it are more likely to be correct. We hypothesize that instead of feeding the parser the guessed full sentence parse trees of its own, we can break them down into smaller ones, namely n-gram trees, and perform self-training on them. We build an n-gram parser and transfer the distinct expertise of the $n$-gram parser to the full sentence parser by using the Hierarchical Joint Learning (HJL) approach. The resulting jointly self-trained parser obtains slight improvement over the baseline.</abstract>
    </paper>
    <paper id="449">
      <author><first>Bushra</first><last>Jawaid</last></author>
      <author><first>Amir</first><last>Kamran</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <title>A Tagged Corpus and a Tagger for <fixed-case>U</fixed-case>rdu</title>
      <pages>2938–2943</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/544_Paper.pdf</url>
      <abstract>In this paper, we describe a release of a sizeable monolingual Urdu corpus automatically tagged with part-of-speech tags. We extend the work of Jawaid and Bojar (2012) who use three different taggers and then apply a voting scheme to disambiguate among the different choices suggested by each tagger. We run this complex ensemble on a large monolingual corpus and release the tagged corpus. Additionally, we use this data to train a single standalone tagger which will hopefully significantly simplify Urdu processing. The standalone tagger obtains the accuracy of 88.74% on test data.</abstract>
    </paper>
    <paper id="450">
      <author><first>Kostadin</first><last>Cholakov</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Judith</first><last>Eckle-Kohler</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <title>Lexical Substitution Dataset for <fixed-case>G</fixed-case>erman</title>
      <pages>1406–1411</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/545_Paper.pdf</url>
      <abstract>This article describes a lexical substitution dataset for German. The whole dataset contains 2,040 sentences from the German Wikipedia, with one target word in each sentence. There are 51 target nouns, 51 adjectives, and 51 verbs randomly selected from 3 frequency groups based on the lemma frequency list of the German WaCKy corpus. 200 sentences have been annotated by 4 professional annotators and the remaining sentences by 1 professional annotator and 5 additional annotators who have been recruited via crowdsourcing. The resulting dataset can be used to evaluate not only lexical substitution systems, but also different sense inventories and word sense disambiguation systems.</abstract>
    </paper>
    <paper id="451">
      <author><first>Lauren</first><last>Romeo</last></author>
      <author><first>Sara</first><last>Mendes</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <title>A cascade approach for complex-type classification</title>
      <pages>4451–4458</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/546_Paper.pdf</url>
      <abstract>The work detailed in this paper describes a 2-step cascade approach for the classification of complex-type nominals. We describe an experiment that demonstrates how a cascade approach performs when the task consists in distinguishing nominals from a given complex-type from any other noun in the language. Overall, our classifier successfully identifies very specific and not highly frequent lexical items such as complex-types with high accuracy, and distinguishes them from those instances that are not complex types by using lexico-syntactic patterns indicative of the semantic classes corresponding to each of the individual sense components of the complex type. Although there is still room for improvement with regard to the coverage of the classifiers developed, the cascade approach increases the precision of classification of the complex-type nouns that are covered in the experiment presented.</abstract>
    </paper>
    <paper id="452">
      <author><first>Cédric</first><last>Lopez</last></author>
      <author><first>Frédérique</first><last>Segond</last></author>
      <author><first>Olivier</first><last>Hondermarck</last></author>
      <author><first>Paolo</first><last>Curtoni</last></author>
      <author><first>Luca</first><last>Dini</last></author>
      <title>Generating a Resource for Products and Brandnames Recognition. Application to the Cosmetic Domain.</title>
      <pages>2559–2564</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/549_Paper.pdf</url>
      <abstract>Named Entity Recognition task needs high-quality and large-scale resources. In this paper, we present RENCO, a based-rules system focused on the recognition of entities in the Cosmetic domain (brandnames, product names, â¦). RENCO has two main objectives: 1) Generating resources for named entity recognition; 2) Mining new named entities relying on the previous generated resources. In order to build lexical resources for the cosmetic domain, we propose a system based on local lexico-syntactic rules complemented by a learning module. As the outcome of the system, we generate both a simple lexicon and a structured lexicon. Results of the evaluation show that even if RENCO outperforms a classic Conditional Random Fields algorithm, both systems should combine their respective strengths.</abstract>
    </paper>
    <paper id="453">
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <title>Annotation of specialized corpora using a comprehensive entity and relation scheme</title>
      <pages>1267–1274</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/552_Paper.pdf</url>
      <abstract>Annotated corpora are essential resources for many applications in Natural Language Processing. They provide insight on the linguistic and semantic characteristics of the genre and domain covered, and can be used for the training and evaluation of automatic tools. In the biomedical domain, annotated corpora of English texts have become available for several genres and subfields. However, very few similar resources are available for languages other than English. In this paper we present an effort to produce a high-quality corpus of clinical documents in French, annotated with a comprehensive scheme of entities and relations. We present the annotation scheme as well as the results of a pilot annotation study covering 35 clinical documents in a variety of subfields and genres. We show that high inter-annotator agreement can be achieved using a complex annotation scheme.</abstract>
    </paper>
    <paper id="454">
      <author><first>Katarzyna</first><last>Klessa</last></author>
      <author><first>Dafydd</first><last>Gibbon</last></author>
      <title>Annotation Pro + <fixed-case>TGA</fixed-case>: automation of speech timing analysis</title>
      <pages>1499–1505</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/553_Paper.pdf</url>
      <abstract>This paper reports on two tools for the automatic statistical analysis of selected properties of speech timing on the basis of speech annotation files. The tools, one online (TGA, Time Group Analyser) and one offline (Annotation Pro+TGA), are intended to support the rapid analysis of speech timing data without the need to create specific scripts or spreadsheet functions for this purpose. The software calculates, inter alia, mean, median, rPVI, nPVI, slope and intercept functions within interpausal groups, provides visualisations of timing patterns, as well as correlations between these, and parses interpausal groups into hierarchies based on duration relations. Although many studies, especially in speech technology, use computational means, enquiries have shown that a large number of phoneticians and phonetics students do not have script creation skills and therefore use traditional copy+spreadsheet techniques, which are slow, preclude the analysis of large data sets, and are prone to inconsistencies. The present tools have been tested in a number of studies on English, Mandarin and Polish, and are introduced here with reference to results from these studies.</abstract>
    </paper>
    <paper id="455">
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Valeria</first><last>Quochi</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Jason</first><last>Utt</last></author>
      <title>Polysemy Index for Nouns: an Experiment on <fixed-case>I</fixed-case>talian using the <fixed-case>PAROLE</fixed-case> <fixed-case>SIMPLE</fixed-case> <fixed-case>CLIPS</fixed-case> Lexical Database</title>
      <pages>2955–2963</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/556_Paper.pdf</url>
      <abstract>An experiment is presented to induce a set of polysemous basic type alternations (such as Animal-Food, or Building-Institution) by deriving them from the sense alternations found in an existing lexical resource. The paper builds on previous work and applies those results to the Italian lexicon PAROLE SIMPLE CLIPS. The new results show how the set of frequent type alternations that can be induced from the lexicon is partly different from the set of polysemy relations selected and explicitely applied by lexicographers when building it. The analysis of mismatches shows that frequent type alternations do not always correpond to prototypical polysemy relations, nevertheless the proposed methodology represents a useful tool offered to lexicographers to systematically check for possible gaps in their resource.</abstract>
    </paper>
    <paper id="456">
      <author><first>Ahmed</first><last>Salama</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Behrang</first><last>Mohit</last></author>
      <author><first>Kemal</first><last>Oflazer</last></author>
      <title><fixed-case>Y</fixed-case>ou<fixed-case>DACC</fixed-case>: the <fixed-case>Y</fixed-case>outube Dialectal <fixed-case>A</fixed-case>rabic Comment Corpus</title>
      <pages>1246–1251</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/558_Paper.pdf</url>
      <abstract>This paper presents YOUDACC, an automatically annotated large-scale multi-dialectal Arabic corpus collected from user comments on Youtube videos. Our corpus covers different groups of dialects: Egyptian (EG), Gulf (GU), Iraqi (IQ), Maghrebi (MG) and Levantine (LV). We perform an empirical analysis on the crawled corpus and demonstrate that our location-based proposed method is effective for the task of dialect labeling.</abstract>
    </paper>
    <paper id="457">
      <author><first>Dan</first><last>Flickinger</last></author>
      <author><first>Emily M.</first><last>Bender</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <title>Towards an Encyclopedia of Compositional Semantics: Documenting the Interface of the <fixed-case>E</fixed-case>nglish <fixed-case>R</fixed-case>esource <fixed-case>G</fixed-case>rammar</title>
      <pages>875–881</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/562_Paper.pdf</url>
      <abstract>We motivate and describe the design and development of an emerging encyclopedia of compositional semantics, pursuing three objectives. We first seek to compile a comprehensive catalogue of interoperable semantic analyses, i.e., a precise characterization of meaning representations for a broad range of common semantic phenomena. Second, we operationalize the discovery of semantic phenomena and their definition in terms of what we call their semantic fingerprint, a formal account of the building blocks of meaning representation involved and their configuration. Third, we ground our work in a carefully constructed semantic test suite of minimal exemplars for each phenomenon, along with a `target’ fingerprint that enables automated regression testing. We work towards these objectives by codifying and documenting the body of knowledge that has been constructed in a long-term collaborative effort, the development of the LinGO English Resource Grammar. Documentation of its semantic interface is a prerequisite to use by non-experts of the grammar and the analyses it produces, but this effort also advances our own understanding of relevant interactions among phenomena, as well as of areas for future work in the grammar.</abstract>
    </paper>
    <paper id="458">
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Laure</first><last>Vieu</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Guido</first><last>Vetere</last></author>
      <title>Enriching the “Senso Comune” Platform with Automatically Acquired Data</title>
      <pages>2130–2137</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/563_Paper.pdf</url>
      <abstract>This paper reports on research activities on automatic methods for the enrichment of the Senso Comune platform. At this stage of development, we will report on two tasks, namely word sense alignment with MultiWordNet and automatic acquisition of Verb Shallow Frames from sense annotated data in the MultiSemCor corpus. The results obtained are satisfying. We achieved a final F-measure of 0.64 for noun sense alignment and a F-measure of 0.47 for verb sense alignment, and an accuracy of 68\% on the acquisition of Verb Shallow Frames.</abstract>
    </paper>
    <paper id="459">
      <author><first>Christoph</first><last>Draxler</last></author>
      <title>Online experiments with the Percy software framework - experiences and some early results</title>
      <pages>235–240</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/564_Paper.pdf</url>
      <abstract>In early 2012 the online perception experiment software Percy was deployed on a production server at our lab. Since then, 38 experiments have been made publicly available, with a total of 3078 experiment sessions. In the course of time, the software has been continuously updated and extended to adapt to changing user requirements. Web-based editors for the structure and layout of the experiments have been developed. This paper describes the system architecture, presents usage statistics, discusses typical characteristics of online experiments, and gives an outlook on ongoing work. webapp.phonetik.uni-muenchen.de/WebExperiment lists all currently active experiments.</abstract>
    </paper>
    <paper id="460">
      <author><first>Onno</first><last>Crasborn</last></author>
      <author><first>Han</first><last>Sloetjes</last></author>
      <title>Improving the exploitation of linguistic annotations in <fixed-case>ELAN</fixed-case></title>
      <pages>3604–3608</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/567_Paper.pdf</url>
      <abstract>This paper discusses some improvements in recent and planned versions of the multimodal annotation tool ELAN, which are targeted at improving the usability of annotated files. Increased support for multilingual documents is provided, by allowing for multilingual vocabularies and by specifying a language per document, annotation layer (tier) or annotation. In addition, improvements in the search possibilities and the display of the results have been implemented, which are especially relevant in the interpretation of the results of complex multi-tier searches.</abstract>
    </paper>
    <paper id="461">
      <author><first>Simon</first><last>Fuller</last></author>
      <author><first>Phil</first><last>Maguire</last></author>
      <author><first>Philippe</first><last>Moser</last></author>
      <title>A Deep Context Grammatical Model For Authorship Attribution</title>
      <pages>4488–4492</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/568_Paper.pdf</url>
      <abstract>We define a variable-order Markov model, representing a Probabilistic Context Free Grammar, built from the sentence-level, de-lexicalized parse of source texts generated by a standard lexicalized parser, which we apply to the authorship attribution task. First, we motivate this model in the context of previous research on syntactic features in the area, outlining some of the general strengths and limitations of the overall approach. Next we describe the procedure for building syntactic models for each author based on training cases. We then outline the attribution process - assigning authorship to the model which yields the highest probability for the given test case. We demonstrate the efficacy for authorship attribution over different Markov orders and compare it against syntactic features trained by a linear kernel SVM. We find that the model performs somewhat less successfully than the SVM over similar features. In the conclusion, we outline how we plan to employ the model for syntactic evaluation of literary texts.</abstract>
    </paper>
    <paper id="462">
      <author><first>Teresa</first><last>Herrmann</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <title>Manual Analysis of Structurally Informed Reordering in <fixed-case>G</fixed-case>erman-<fixed-case>E</fixed-case>nglish Machine Translation</title>
      <pages>4379–4386</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/569_Paper.pdf</url>
      <abstract>Word reordering is a difficult task for translation. Common automatic metrics such as BLEU have problems reflecting improvements in target language word order. However, it is a crucial aspect for humans when deciding on translation quality. This paper presents a detailed analysis of a structure-aware reordering approach applied in a German-to-English phrase-based machine translation system. We compare the translation outputs of two translation systems applying reordering rules based on parts-of-speech and syntax trees on a sentence-by-sentence basis. For each sentence-pair we examine the global translation performance and classify local changes in the translated sentences. This analysis is applied to three data sets representing different genres. While the improvement in BLEU differed substantially between the data sets, the manual evaluation showed that both global translation performance as well as individual types of improvements and degradations exhibit a similar behavior throughout the three data sets. We have observed that for 55-64% of the sentences with different translations, the translation produced using the tree-based reordering was considered to be the better translation. As intended by the investigated reordering model, most improvements are achieved by improving the position of the verb or being able to translate a verb that could not be translated before.</abstract>
    </paper>
    <paper id="463">
      <author><first>Gabriele</first><last>Pallotti</last></author>
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Fabio</first><last>Affè</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Stefania</first><last>Ferrari</last></author>
      <title>Presenting a system of human-machine interaction for performing map tasks.</title>
      <pages>3963–3966</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/570_Paper.pdf</url>
      <abstract>A system for human machine interaction is presented, that offers second language learners of Italian the possibility of assessing their competence by performing a map task, namely by guiding the a virtual follower through a map with written instructions in natural language. The underlying natural language processing algorithm is described, and the map authoring infrastructure is presented.</abstract>
    </paper>
    <paper id="464">
      <author><first>Moritz</first><last>Wittmann</last></author>
      <author><first>Marion</first><last>Weller</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <title>Automatic Extraction of Synonyms for <fixed-case>G</fixed-case>erman Particle Verbs from Parallel Data with Distributional Similarity as a Re-Ranking Feature</title>
      <pages>1430–1437</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/574_Paper.pdf</url>
      <abstract>We present a method for the extraction of synonyms for German particle verbs based on a word-aligned German-English parallel corpus: by translating the particle verb to a pivot, which is then translated back, a set of synonym candidates can be extracted and ranked according to the respective translation probabilities. In order to deal with separated particle verbs, we apply re-ordering rules to the German part of the data. In our evaluation against a gold standard, we compare different pre-processing strategies (lemmatized vs. inflected forms) and introduce language model scores of synonym candidates in the context of the input particle verb as well as distributional similarity as additional re-ranking criteria. Our evaluation shows that distributional similarity as a re-ranking feature is more robust than language model scores and leads to an improved ranking of the synonym candidates. In addition to evaluating against a gold standard, we also present a small-scale manual evaluation.</abstract>
    </paper>
    <paper id="465">
      <author><first>Layla</first><last>El Asri</last></author>
      <author><first>Rémi</first><last>Lemonnier</last></author>
      <author><first>Romain</first><last>Laroche</last></author>
      <author><first>Olivier</first><last>Pietquin</last></author>
      <author><first>Hatim</first><last>Khouzaimi</last></author>
      <title><fixed-case>NASTIA</fixed-case>: Negotiating Appointment Setting Interface</title>
      <pages>266–271</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/575_Paper.pdf</url>
      <abstract>This paper describes a French Spoken Dialogue System (SDS) named NASTIA (Negotiating Appointment SeTting InterfAce). Appointment scheduling is a hybrid task halfway between slot-filling and negotiation. NASTIA implements three different negotiation strategies. These strategies were tested on 1734 dialogues with 385 users who interacted at most 5 times with the SDS and gave a rating on a scale of 1 to 10 for each dialogue. Previous appointment scheduling systems were evaluated with the same experimental protocol. NASTIA is different from these systems in that it can adapt its strategy during the dialogue. The highest system task completion rate with these systems was 81% whereas NASTIA had an 88% average and its best performing strategy even reached 92%. This strategy also significantly outperformed previous systems in terms of overall user rating with an average of 8.28 against 7.40. The experiment also enabled highlighting global recommendations for building spoken dialogue systems.</abstract>
    </paper>
    <paper id="466">
      <author><first>Layla</first><last>El Asri</last></author>
      <author><first>Romain</first><last>Laroche</last></author>
      <author><first>Olivier</first><last>Pietquin</last></author>
      <title><fixed-case>DINASTI</fixed-case>: Dialogues with a Negotiating Appointment Setting Interface</title>
      <pages>272–278</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/576_Paper.pdf</url>
      <abstract>This paper describes the DINASTI (DIalogues with a Negotiating Appointment SeTting Interface) corpus, which is composed of 1734 dialogues with the French spoken dialogue system NASTIA (Negotiating Appointment SeTting InterfAce). NASTIA is a reinforcement learning-based system. The DINASTI corpus was collected while the system was following a uniform policy. Each entry of the corpus is a system-user exchange annotated with 120 automatically computable features.The corpus contains a total of 21587 entries, with 385 testers. Each tester performed at most five scenario-based interactions with NASTIA. The dialogues last an average of 10.82 dialogue turns, with 4.45 reinforcement learning decisions. The testers filled an evaluation questionnaire after each dialogue. The questionnaire includes three questions to measure task completion. In addition, it comprises 7 Likert-scaled items evaluating several aspects of the interaction, a numerical overall evaluation on a scale of 1 to 10, and a free text entry. Answers to this questionnaire are provided with DINASTI. This corpus is meant for research on reinforcement learning modelling for dialogue management.</abstract>
    </paper>
    <paper id="467">
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <author><first>Marina</first><last>Valeeva</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <title><fixed-case>LQVS</fixed-case>umm: A Corpus of Linguistic Quality Violations in Multi-Document Summarization</title>
      <pages>1591–1599</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/578_Paper.pdf</url>
      <abstract>We present LQVSumm, a corpus of about 2000 automatically created extractive multi-document summaries from the TAC 2011 shared task on Guided Summarization, which we annotated with several types of linguistic quality violations. Examples for such violations include pronouns that lack antecedents or ungrammatical clauses. We give details on the annotation scheme and show that inter-annotator agreement is good given the open-ended nature of the task. The annotated summaries have previously been scored for Readability on a numeric scale by human annotators in the context of the TAC challenge; we show that the number of instances of violations of linguistic quality of a summary correlates with these intuitively assigned numeric scores. On a system-level, the average number of violations marked in a system’s summaries achieves higher correlation with the Readability scores than current supervised state-of-the-art methods for assigning a single readability score to a summary. It is our hope that our corpus facilitates the development of methods that not only judge the linguistic quality of automatically generated summaries as a whole, but which also allow for detecting, labeling, and fixing particular violations in a text.</abstract>
    </paper>
    <paper id="468">
      <author><first>Manfred</first><last>Stede</last></author>
      <author><first>Arne</first><last>Neumann</last></author>
      <title><fixed-case>P</fixed-case>otsdam Commentary Corpus 2.0: Annotation for Discourse Research</title>
      <pages>925–929</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/579_Paper.pdf</url>
      <abstract>We present a revised and extended version of the Potsdam Commentary Corpus, a collection of 175 German newspaper commentaries (op-ed pieces) that has been annotated with syntax trees and three layers of discourse-level information: nominal coreference,connectives and their arguments (similar to the PDTB, Prasad et al. 2008), and trees reflecting discourse structure according to Rhetorical Structure Theory (Mann/Thompson 1988). Connectives have been annotated with the help of a semi-automatic tool, Conano (Stede/Heintze 2004), which identifies most connectives and suggests arguments based on their syntactic category. The other layers have been created manually with dedicated annotation tools. The corpus is made available on the one hand as a set of original XML files produced with the annotation tools, based on identical tokenization. On the other hand, it is distributed together with the open-source linguistic database ANNIS3 (Chiarcos et al. 2008; Zeldes et al. 2009), which provides multi-layer search functionality and layer-specific visualization modules. This allows for comfortable qualitative evaluation of the correlations between annotation layers.</abstract>
    </paper>
    <paper id="469">
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Franck</first><last>Sajous</last></author>
      <author><first>Basilio</first><last>Calderone</last></author>
      <title><fixed-case>GLÀFF</fixed-case>, a Large Versatile <fixed-case>F</fixed-case>rench Lexicon</title>
      <pages>1007–1012</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/58_Paper.pdf</url>
      <abstract>This paper introduces GLAFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLAFF contains, for each entry, inflectional features and phonemic transcriptions. It distinguishes itself from the other available French lexicons by its size, its potential for constant updating and its copylefted license. We explain how we have built GLAFF and compare it to other known resources in terms of coverage and quality of the phonemic transcriptions. We show that its size and quality are strong assets that could allow GLAFF to become a reference lexicon for French NLP and linguistics. Moreover, other derived lexicons can easily be based on GLAFF to satisfy specific needs of various fields such as psycholinguistics.</abstract>
    </paper>
    <paper id="470">
      <author><first>Ahti</first><last>Lohk</last></author>
      <author><first>Kaarel</first><last>Allik</last></author>
      <author><first>Heili</first><last>Orav</last></author>
      <author><first>Leo</first><last>Võhandu</last></author>
      <title>Dense Components in the Structure of <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <pages>1135–1139</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/582_Paper.pdf</url>
      <abstract>This paper introduces a test-pattern named a dense component for checking inconsistencies in the hierarchical structure of a wordnet. Dense component (viewed as substructure) points out the cases of regular polysemy in the context of multiple inheritance. Definition of the regular polysemy is redefined ― instead of lexical units there are used lexical concepts (synsets). All dense components are evaluated by expert lexicographer. Based on this experiment we give an overview of the inconsistencies which the test-pattern helps to detect. Special attention is turned to all different kind of corrections made by lexicographer. Authors of this paper find that the greatest benefit of the use of dense components is helping to detect if the regular polysemy is justified or not. In-depth analysis has been performed for Estonian Wordnet Version 66. Some comparative figures are also given for the Estonian Wordnet (EstWN) Version 67 and Princeton WordNet (PrWN) Version 3.1. Analysing hierarchies only hypernym-relations are used.</abstract>
    </paper>
    <paper id="471">
      <author><first>Lauren</first><last>Romeo</last></author>
      <author><first>Gianluca</first><last>Lebani</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <title>Choosing which to use? A study of distributional models for nominal lexical semantic classification</title>
      <pages>4366–4373</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/583_Paper.pdf</url>
      <abstract>This paper empirically evaluates the performances of different state-of-the-art distributional models in a nominal lexical semantic classification task. We consider models that exploit various types of distributional features, which thereby provide different representations of nominal behavior in context. The experiments presented in this work demonstrate the advantages and disadvantages of each model considered. This analysis also considers a combined strategy that we found to be capable of leveraging the bottlenecks of each model, especially when large robust data is not available.</abstract>
    </paper>
    <paper id="472">
      <author><first>Jens</first><last>Forster</last></author>
      <author><first>Christoph</first><last>Schmidt</last></author>
      <author><first>Oscar</first><last>Koller</last></author>
      <author><first>Martin</first><last>Bellgardt</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <title>Extensions of the Sign Language Recognition and Translation Corpus <fixed-case>RWTH</fixed-case>-<fixed-case>PHOENIX</fixed-case>-Weather</title>
      <pages>1911–1916</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/585_Paper.pdf</url>
      <abstract>This paper introduces the RWTH-PHOENIX-Weather 2014, a video-based, large vocabulary, German sign language corpus which has been extended over the last two years, tripling the size of the original corpus. The corpus contains weather forecasts simultaneously interpreted into sign language which were recorded from German public TV and manually annotated using glosses on the sentence level and semi-automatically transcribed spoken German extracted from the videos using the open-source speech recognition system RASR. Spatial annotations of the signers’ hands as well as shape and orientation annotations of the dominant hand have been added for more than 40k respectively 10k video frames creating one of the largest corpora allowing for quantitative evaluation of object tracking algorithms. Further, over 2k signs have been annotated using the SignWriting annotation system, focusing on the shape, orientation, movement as well as spatial contacts of both hands. Finally, extended recognition and translation setups are defined, and baseline results are presented.</abstract>
    </paper>
    <paper id="473">
      <author><first>Sara</first><last>Candeias</last></author>
      <author><first>Dirce</first><last>Celorico</last></author>
      <author><first>Jorge</first><last>Proença</last></author>
      <author><first>Arlindo</first><last>Veiga</last></author>
      <author><first>Carla</first><last>Lopes</last></author>
      <author><first>Fernando</first><last>Perdigão</last></author>
      <title><fixed-case>HESITA</fixed-case>(te) in <fixed-case>P</fixed-case>ortuguese</title>
      <pages>1564–1567</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/587_Paper.pdf</url>
      <abstract>Hesitations, so-called disfluencies, are a characteristic of spontaneous speech, playing a primary role in its structure, reflecting aspects of the language production and the management of inter-communication. In this paper we intend to present a database of hesitations in European Portuguese speech - HESITA - as a relevant base of work to study a variety of speech phenomena. Patterns of hesitations, hesitation distribution according to speaking style, and phonetic properties of the fillers are some of the characteristics we extrapolated from the HESITA database. This database also represents an important resource for improvement in synthetic speech naturalness as well as in robust acoustic modelling for automatic speech recognition. The HESITA database is the output of a project in the speech-processing field for European Portuguese held by an interdisciplinary group in intimate articulation between engineering tools and experience and the linguistic approach.</abstract>
    </paper>
    <paper id="474">
      <author><first>Sameh</first><last>Alansary</last></author>
      <title><fixed-case>MUHIT</fixed-case>: A Multilingual Harmonized Dictionary</title>
      <pages>2138–2145</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/588_Paper.pdf</url>
      <abstract>This paper discusses a trial to build a multilingual harmonized dictionary that contains more than 40 languages, with special reference to Arabic which represents about 20% of the whole size of the dictionary. This dictionary is called MUHIT which is an interactive multilingual dictionary application. It is a web application that makes it easily accessible to all users. MUHIT is developed within the Universal Networking Language (UNL) framework by the UNDL Foundation, in cooperation with Bibliotheca Alexandrina (BA). This application targets to serve specialists and non-specialists. It provides users with full linguistic description to each lexical item. This free application is useful to many NLP tasks such as multilingual translation and cross-language synonym search. This dictionary is built depending on WordNet and corpus based approaches, in a specially designed linguistic environment called UNLariam that is developed by the UNLD foundation. This dictionary is the first launched application by the UNLD foundation.</abstract>
    </paper>
    <paper id="475">
      <author><first>Maddalen</first><last>Lopez de Lacalle</last></author>
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <title>Predicate Matrix: extending <fixed-case>S</fixed-case>em<fixed-case>L</fixed-case>ink through <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et mappings</title>
      <pages>903–909</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/589_Paper.pdf</url>
      <abstract>This paper presents the Predicate Matrix v1.1, a new lexical resource resulting from the integration of multiple sources of predicate information including FrameNet, VerbNet, PropBank and WordNet. We start from the basis of SemLink. Then, we use advanced graph-based algorithms to further extend the mapping coverage of SemLink. Second, we also exploit the current content of SemLink to infer new role mappings among the different predicate schemas. As a result, we have obtained a new version of the Predicate Matrix which largely extends the current coverage of SemLink and the previous version of the Predicate Matrix.</abstract>
    </paper>
    <paper id="476">
      <author><first>AiTi</first><last>Aw</last></author>
      <author><first>Sharifah Mahani</first><last>Aljunied</last></author>
      <author><first>Nattadaporn</first><last>Lertcheva</last></author>
      <author><first>Sasiwimon</first><last>Kalunsima</last></author>
      <title><fixed-case>T</fixed-case>a<fixed-case>LAP</fixed-case>i — A <fixed-case>T</fixed-case>hai Linguistically Annotated Corpus for Language Processing</title>
      <pages>125–132</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/59_Paper.pdf</url>
      <abstract>This paper discusses a Thai corpus, TaLAPi, fully annotated with word segmentation (WS), part-of-speech (POS) and named entity (NE) information with the aim to provide a high-quality and sufficiently large corpus for real-life implementation of Thai language processing tools. The corpus contains 2,720 articles (1,043,471words) from the entertainment and lifestyle (NE&amp;L) domain and 5,489 articles (3,181,487 words) in the news (NEWS) domain, with a total of 35 POS tags and 10 named entity categories. In particular, we present an approach to segment and tag foreign and loan words expressed in transliterated or original form in Thai text corpora. We see this as an area for study as adapted and un-adapted foreign language sequences have not been well addressed in the literature and this poses a challenge to the annotation process due to the increasing use and adoption of foreign words in the Thai language nowadays. To reduce the ambiguities in POS tagging and to provide rich information for facilitating Thai syntactic analysis, we adapted the POS tags used in ORCHID and propose a framework to tag Thai text and also addresses the tagging of loan and foreign words based on the proposed segmentation strategy. TaLAPi also includes a detailed guideline for tagging the 10 named entity categories</abstract>
    </paper>
    <paper id="477">
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Giulia</first><last>Venturi</last></author>
      <author><first>Andrea</first><last>Cimino</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <title><fixed-case>T</fixed-case>2<fixed-case>K</fixed-case>^2: a System for Automatically Extracting and Organizing Knowledge from Texts</title>
      <pages>2062–2070</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/590_Paper.pdf</url>
      <abstract>In this paper, we present T2K^2, a suite of tools for automatically extracting domain―specific knowledge from collections of Italian and English texts. T2K^2 (Text―To―Knowledge v2) relies on a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine learning which are dynamically integrated to provide an accurate and incremental representation of the content of vast repositories of unstructured documents. Extracted knowledge ranges from domain―specific entities and named entities to the relations connecting them and can be used for indexing document collections with respect to different information types. T2K^2 also includes linguistic profiling functionalities aimed at supporting the user in constructing the acquisition corpus, e.g. in selecting texts belonging to the same genre or characterized by the same degree of specialization or in monitoring the added value of newly inserted documents. T2K^2 is a web application which can be accessed from any browser through a personal account which has been tested in a wide range of domains.</abstract>
    </paper>
    <paper id="478">
      <author><first>Giovanni</first><last>Costantini</last></author>
      <author><first>Iacopo</first><last>Iaderola</last></author>
      <author><first>Andrea</first><last>Paoloni</last></author>
      <author><first>Massimiliano</first><last>Todisco</last></author>
      <title><fixed-case>EMOVO</fixed-case> Corpus: an <fixed-case>I</fixed-case>talian Emotional Speech Database</title>
      <pages>3501–3504</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/591_Paper.pdf</url>
      <abstract>This article describes the first emotional corpus, named EMOVO, applicable to Italian language,. It is a database built from the voices of up to 6 actors who played 14 sentences simulating 6 emotional states (disgust, fear, anger, joy, surprise, sadness) plus the neutral state. These emotions are the well-known Big Six found in most of the literature related to emotional speech. The recordings were made with professional equipment in the Fondazione Ugo Bordoni laboratories. The paper also describes a subjective validation test of the corpus, based on emotion-discrimination of two sentences carried out by two different groups of 24 listeners. The test was successful because it yielded an overall recognition accuracy of 80%. It is observed that emotions less easy to recognize are joy and disgust, whereas the most easy to detect are anger, sadness and the neutral state.</abstract>
    </paper>
    <paper id="479">
      <author><first>Arfath</first><last>Pasha</last></author>
      <author><first>Mohamed</first><last>Al-Badrashiny</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>Ahmed</first><last>El Kholy</last></author>
      <author><first>Ramy</first><last>Eskander</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Manoj</first><last>Pooleery</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Ryan</first><last>Roth</last></author>
      <title><fixed-case>MADAMIRA</fixed-case>: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of <fixed-case>A</fixed-case>rabic</title>
      <pages>1094–1101</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/593_Paper.pdf</url>
      <abstract>In this paper, we present MADAMIRA, a system for morphological analysis and disambiguation of Arabic that combines some of the best aspects of two previously commonly used systems for Arabic processing, MADA (Habash and Rambow, 2005; Habash et al., 2009; Habash et al., 2013) and AMIRA (Diab et al., 2007). MADAMIRA improves upon the two systems with a more streamlined Java implementation that is more robust, portable, extensible, and is faster than its ancestors by more than an order of magnitude. We also discuss an online demo (see http://nlp.ldeo.columbia.edu/madamira/) that highlights these aspects.</abstract>
    </paper>
    <paper id="480">
      <author><first>Ritesh</first><last>Kumar</last></author>
      <title>Developing Politeness Annotated Corpus of <fixed-case>H</fixed-case>indi Blogs</title>
      <pages>1275–1280</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/594_Paper.pdf</url>
      <abstract>In this paper I discuss the creation and annotation of a corpus of Hindi blogs. The corpus consists of a total of over 479,000 blog posts and blog comments. It is annotated with the information about the politeness level of each blog post and blog comment. The annotation is carried out using four levels of politeness ― neutral, appropriate, polite and impolite. For the annotation, three classifiers ― were trained and tested maximum entropy (MaxEnt), Support Vector Machines (SVM) and C4.5 - using around 30,000 manually annotated texts. Among these, C4.5 gave the best accuracy. It achieved an accuracy of around 78% which is within 2% of the human accuracy during annotation. Consequently this classifier is used to annotate the rest of the corpus</abstract>
    </paper>
    <paper id="481">
      <author><first>Weston</first><last>Feely</last></author>
      <author><first>Mehdi</first><last>Manshadi</last></author>
      <author><first>Robert</first><last>Frederking</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <title>The <fixed-case>CMU</fixed-case> <fixed-case>METAL</fixed-case> <fixed-case>F</fixed-case>arsi <fixed-case>NLP</fixed-case> Approach</title>
      <pages>4052–4055</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/596_Paper.pdf</url>
      <abstract>While many high-quality tools are available for analyzing major languages such as English, equivalent freely-available tools for important but lower-resourced languages such as Farsi are more difficult to acquire and integrate into a useful NLP front end. We report here on an accurate and efficient Farsi analysis front end that we have assembled, which may be useful to others who wish to work with written Farsi. The pre-existing components and resources that we incorporated include the Carnegie Mellon TurboParser and TurboTagger (Martins et al., 2010) trained on the Dadegan Treebank (Rasooli et al., 2013), the Uppsala Farsi text normalizer PrePer (Seraji, 2013), the Uppsala Farsi tokenizer (Seraji et al., 2012a), and Jon Dehdaris PerStem (Jadidinejad et al., 2010). This set of tools (combined with additional normalization and tokenization modules that we have developed and made available) achieves a dependency parsing labeled attachment score of 89.49%, unlabeled attachment score of 92.19%, and label accuracy score of 91.38% on a held-out parsing test data set. All of the components and resources used are freely available. In addition to describing the components and resources, we also explain the rationale for our choices.</abstract>
    </paper>
    <paper id="482">
      <author><first>Bart</first><last>Desmet</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Recognising suicidal messages in <fixed-case>D</fixed-case>utch social media</title>
      <pages>830–835</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/597_Paper.pdf</url>
      <abstract>Early detection of suicidal thoughts is an important part of effective suicide prevention. Such thoughts may be expressed online, especially by young people. This paper presents on-going work on the automatic recognition of suicidal messages in social media. We present experiments for automatically detecting relevant messages (with suicide-related content), and those containing suicide threats. A sample of 1357 texts was annotated in a corpus of 2674 blog posts and forum messages from Netlog, indicating relevance, origin, severity of suicide threat and risks as well as protective factors. For the classification experiments, Naive Bayes, SVM and KNN algorithms are combined with shallow features, i.e. bag-of-words of word, lemma and character ngrams, and post length. The best relevance classification is achieved by using SVM with post length, lemma and character ngrams, resulting in an F-score of 85.6% (78.7% precision and 93.8% recall). For the second task (threat detection), a cascaded setup which first filters out irrelevant messages with SVM and then predicts the severity with KNN, performs best: 59.2% F-score (69.5% precision and 51.6% recall).</abstract>
    </paper>
    <paper id="483">
      <author><first>Maximilian</first><last>Köper</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <title>A Rank-based Distance Measure to Detect Polysemy and to Determine Salient Vector-Space Features for <fixed-case>G</fixed-case>erman Prepositions</title>
      <pages>4459–4466</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/599_Paper.pdf</url>
      <abstract>This paper addresses vector space models of prepositions, a notoriously ambiguous word class. We propose a rank-based distance measure to explore the vector-spatial properties of the ambiguous objects, focusing on two research tasks: (i) to distinguish polysemous from monosemous prepositions in vector space; and (ii) to determine salient vector-space features for a classification of preposition senses. The rank-based measure predicts the polysemy vs. monosemy of prepositions with a precision of up to 88%, and suggests preposition-subcategorised nouns as more salient preposition features than preposition-subcategorising verbs.</abstract>
    </paper>
    <paper id="484">
      <author><first>Rosalee</first><last>Wolfe</last></author>
      <author><first>John</first><last>McDonald</last></author>
      <author><first>Larwan</first><last>Berke</last></author>
      <author><first>Marie</first><last>Stumbo</last></author>
      <title>Expanding n-gram analytics in <fixed-case>ELAN</fixed-case> and a case study for sign synthesis</title>
      <pages>1880–1885</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/6_Paper.pdf</url>
      <abstract>Corpus analysis is a powerful tool for signed language synthesis. A new extension to ELAN offers expanded n-gram analysis tools including improved search capabilities and an extensive library of statistical measures of association for n-grams. Uncovering and exploring coarticulatory timing effects via corpus analysis requires n-gram analysis to discover the most frequently occurring bigrams. This paper presents an overview of the new tools and a case study in American Sign Language synthesis that exploits these capabilities for computing more natural timing in generated sentences. The new extension provides a time-saving convenience for language researchers using ELAN.</abstract>
    </paper>
    <paper id="485">
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Huan-Yuan</first><last>Chen</last></author>
      <author><first>Chang-Sheng</first><last>Yu</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <author><first>Po-Ching</first><last>Lee</last></author>
      <author><first>Chun-Hsun</first><last>Chen</last></author>
      <title>Sentence Rephrasing for Parsing Sentences with <fixed-case>OOV</fixed-case> Words</title>
      <pages>2859–2862</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/60_Paper.pdf</url>
      <abstract>This paper addresses the problems of out-of-vocabulary (OOV) words, named entities in particular, in dependency parsing. The OOV words, whose word forms are unknown to the learning-based parser, in a sentence may decrease the parsing performance. To deal with this problem, we propose a sentence rephrasing approach to replace each OOV word in a sentence with a popular word of the same named entity type in the training set, so that the knowledge of the word forms can be used for parsing. The highest-frequency-based rephrasing strategy and the information-retrieval-based rephrasing strategy are explored to select the word to replace, and the Chinese Treebank 6.0 (CTB6) corpus is adopted to evaluate the feasibility of the proposed sentence rephrasing strategies. Experimental results show that rephrasing some specific types of OOV words such as Corporation, Organization, and Competition increases the parsing performances. This methodology can be applied to domain adaptation to deal with OOV problems.</abstract>
    </paper>
    <paper id="486">
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <author><first>Paul</first><last>Bédaride</last></author>
      <title>Mapping the Lexique des Verbes du Français (Lexicon of <fixed-case>F</fixed-case>rench Verbs) to a <fixed-case>NLP</fixed-case> lexicon using examples</title>
      <pages>2806–2810</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/602_Paper.pdf</url>
      <abstract>This article presents experiments aiming at mapping the Lexique des Verbes du Français (Lexicon of French Verbs) to FRILEX, a Natural Language Processing (NLP) lexicon based on D ICOVALENCE. The two resources (Lexicon of French Verbs and D ICOVALENCE) were built by linguists, based on very different theories, which makes a direct mapping nearly impossible. We chose to use the examples provided in one of the resource to find implicit links between the two and make them explicit.</abstract>
    </paper>
    <paper id="487">
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Julien</first><last>Grosjean</last></author>
      <author><first>Stéfan</first><last>Darmoni</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <title>Language Resources for <fixed-case>F</fixed-case>rench in the Biomedical Domain</title>
      <pages>2146–2151</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/604_Paper.pdf</url>
      <abstract>The biomedical domain offers a wealth of linguistic resources for Natural Language Processing, including terminologies and corpora. While many of these resources are prominently available for English, other languages including French benefit from substantial coverage thanks to the contribution of an active community over the past decades. However, access to terminological resources in languages other than English may not be as straight-forward as access to their English counterparts. Herein, we review the extent of resource coverage for French and give pointers to access French-language resources. We also discuss the sources and methods for making additional material available for French.</abstract>
    </paper>
    <paper id="488">
      <author><first>Adriane</first><last>Boyd</last></author>
      <author><first>Jirka</first><last>Hana</last></author>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <author><first>Katrin</first><last>Wisniewski</last></author>
      <author><first>Andrea</first><last>Abel</last></author>
      <author><first>Karin</first><last>Schöne</last></author>
      <author><first>Barbora</first><last>Štindlová</last></author>
      <author><first>Chiara</first><last>Vettori</last></author>
      <title>The <fixed-case>MERLIN</fixed-case> corpus: Learner language and the <fixed-case>CEFR</fixed-case></title>
      <pages>1281–1288</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/606_Paper.pdf</url>
      <abstract>The MERLIN corpus is a written learner corpus for Czech, German,and Italian that has been designed to illustrate the Common European Framework of Reference for Languages (CEFR) with authentic learner data. The corpus contains 2,290 learner texts produced in standardized language certifications covering CEFR levels A1-C1. The MERLIN annotation scheme includes a wide range of language characteristics that enable research into the empirical foundations of the CEFR scales and provide language teachers, test developers, and Second Language Acquisition researchers with concrete examples of learner performance and progress across multiple proficiency levels. For computational linguistics, it provide a range of authentic learner data for three target languages, supporting a broadening of the scope of research in areas such as automatic proficiency classification or native language identification. The annotated corpus and related information will be freely available as a corpus resource and through a freely accessible, didactically-oriented online platform.</abstract>
    </paper>
    <paper id="489">
      <author><first>Yvonne</first><last>Adesam</last></author>
      <author><first>Malin</first><last>Ahlberg</last></author>
      <author><first>Peter</first><last>Andersson</last></author>
      <author><first>Gerlof</first><last>Bouma</last></author>
      <author><first>Markus</first><last>Forsberg</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <title>Computer-aided morphology expansion for Old <fixed-case>S</fixed-case>wedish</title>
      <pages>1102–1105</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/607_Paper.pdf</url>
      <abstract>In this paper we describe and evaluate a tool for paradigm induction and lexicon extraction that has been applied to Old Swedish. The tool is semi-supervised and uses a small seed lexicon and unannotated corpora to derive full inflection tables for input lemmata. In the work presented here, the tool has been modified to deal with the rich spelling variation found in Old Swedish texts. We also present some initial experiments, which are the first steps towards creating a large-scale morphology for Old Swedish.</abstract>
    </paper>
    <paper id="490">
      <author><first>Bushra</first><last>Jawaid</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <title>Two-Step Machine Translation with Lattices</title>
      <pages>682–686</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/610_Paper.pdf</url>
      <abstract>The idea of two-step machine translation was introduced to divide the complexity of the search space into two independent steps: (1) lexical translation and reordering, and (2) conjugation and declination in the target language. In this paper, we extend the two-step machine translation structure by replacing state-of-the-art phrase-based machine translation with the hierarchical machine translation in the 1st step. We further extend the fixed string-based input format of the 2nd step with word lattices (Dyer et al., 2008); this provides the 2nd step with the opportunity to choose among a sample of possible reorderings instead of relying on the single best one as produced by the 1st step.</abstract>
    </paper>
    <paper id="491">
      <author><first>Björn</first><last>Schuller</last></author>
      <author><first>Felix</first><last>Friedmann</last></author>
      <author><first>Florian</first><last>Eyben</last></author>
      <title>The <fixed-case>M</fixed-case>unich Biovoice Corpus: Effects of Physical Exercising, Heart Rate, and Skin Conductance on Human Speech Production</title>
      <pages>1506–1510</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/611_Paper.pdf</url>
      <abstract>We introduce a spoken language resource for the analysis of impact that physical exercising has on human speech production. In particular, the database provides heart rate and skin conductance measurement information alongside the audio recordings. It contains recordings from 19 subjects in a relaxed state and after exercising. The audio material includes breathing, sustained vowels, and read text. Further, we describe pre-extracted audio-features from our openSMILE feature extractor together with baseline performances for the recognition of high and low heart rate using these features. The baseline results clearly show the feasibility of automatic estimation of heart rate from the human voice, in particular from sustained vowels. Both regression - in order to predict the exact heart rate value - and a binary classification setting for high and low heart rate classes are investigated. Finally, we give tendencies on feature group relevance in the named contexts of heart rate estimation and skin conductivity estimation.</abstract>
    </paper>
    <paper id="492">
      <author><first>Luz</first><last>Rello</last></author>
      <author><first>Ricardo</first><last>Baeza-Yates</last></author>
      <author><first>Joaquim</first><last>Llisterri</last></author>
      <title><fixed-case>D</fixed-case>ys<fixed-case>L</fixed-case>ist: An Annotated Resource of Dyslexic Errors</title>
      <pages>1289–1296</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/612_Paper.pdf</url>
      <abstract>We introduce a language resource for Spanish, DysList, composed of a list of unique errors extracted from a collection of texts written by people with dyslexia. Each of the errors was annotated with a set of characteristics as well as visual and phonetic features. To the best of our knowledge this is the largest resource of this kind, especially given the difficulty of finding texts written by people with dyslexia</abstract>
    </paper>
    <paper id="493">
      <author><first>Raymond</first><last>Shen</last></author>
      <author><first>Hideaki</first><last>Kikuchi</last></author>
      <title>Estimation of Speaking Style in Speech Corpora Focusing on speech transcriptions</title>
      <pages>2747–2752</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/616_Paper.pdf</url>
      <abstract>Recent developments in computer technology have allowed the construction and widespread application of large-scale speech corpora. To foster ease of data retrieval for people interested in utilising these speech corpora, we attempt to characterise speaking style across some of them. In this paper, we first introduce the 3 scales of speaking style proposed by Eskenazi in 1993. We then use morphological features extracted from speech transcriptions that have proven effective in style discrimination and author identification in the field of natural language processing to construct an estimation model of speaking style. More specifically, we randomly choose transcriptions from various speech corpora as text stimuli with which to conduct a rating experiment on speaking style perception; then, using the features extracted from those stimuli and the rating results, we construct an estimation model of speaking style by a multi-regression analysis. After the cross validation (leave-1-out), the results show that among the 3 scales of speaking style, the ratings of 2 scales can be estimated with high accuracies, which prove the effectiveness of our method in the estimation of speaking style.</abstract>
    </paper>
    <paper id="494">
      <author><first>Anne</first><last>Garcia-Fernandez</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <title>Evaluation of different strategies for domain adaptation in opinion mining</title>
      <pages>3877–3880</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/617_Paper.pdf</url>
      <abstract>The work presented in this article takes place in the field of opinion mining and aims more particularly at finding the polarity of a text by relying on machine learning methods. In this context, it focuses on studying various strategies for adapting a statistical classifier to a new domain when training data only exist for one or several other domains. This study shows more precisely that a self-training procedure consisting in enlarging the initial training corpus with texts from the target domain that were reliably classified by the classifier is the most successful and stable strategy for the tested domains. Moreover, this strategy gets better results in most cases than (Blitzer et al., 2007)’s method on the same evaluation corpus while it is more simple.</abstract>
    </paper>
    <paper id="495">
      <author><first>Travis</first><last>Goodwin</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <title>Clinical Data-Driven Probabilistic Graph Processing</title>
      <pages>101–108</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/618_Paper.pdf</url>
      <abstract>Electronic Medical Records (EMRs) encode an extraordinary amount of medical knowledge. Collecting and interpreting this knowledge, however, belies a significant level of clinical understanding. Automatically capturing the clinical information is crucial for performing comparative effectiveness research. In this paper, we present a data-driven approach to model semantic dependencies between medical concepts, qualified by the beliefs of physicians. The dependencies, captured in a patient cohort graph of clinical pictures and therapies is further refined into a probabilistic graphical model which enables efficient inference of patient-centered treatment or test recommendations (based on probabilities). To perform inference on the graphical model, we describe a technique of smoothing the conditional likelihood of medical concepts by their semantically-similar belief values. The experimental results, as compared against clinical guidelines are very promising.</abstract>
    </paper>
    <paper id="496">
      <author><first>Muntsa</first><last>Padró</last></author>
      <author><first>Marco</first><last>Idiart</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <title>Comparing Similarity Measures for Distributional Thesauri</title>
      <pages>2964–2971</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/619_Paper.pdf</url>
      <abstract>Distributional thesauri have been applied for a variety of tasks involving semantic relatedness. In this paper, we investigate the impact of three parameters: similarity measures, frequency thresholds and association scores. We focus on the robustness and stability of the resulting thesauri, measuring inter-thesaurus agreement when testing different parameter values. The results obtained show that low-frequency thresholds affect thesaurus quality more than similarity measures, with more agreement found for increasing thresholds.These results indicate the sensitivity of distributional thesauri to frequency. Nonetheless, the observed differences do not transpose over extrinsic evaluation using TOEFL-like questions. While this may be specific to the task, we argue that a careful examination of the stability of distributional resources prior to application is needed.</abstract>
    </paper>
    <paper id="497">
      <author><first>Cheikh M. Bamba</first><last>Dione</last></author>
      <title>Pruning the Search Space of the <fixed-case>W</fixed-case>olof <fixed-case>LFG</fixed-case> Grammar Using a Probabilistic and a Constraint Grammar Parser</title>
      <pages>2863–2870</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/62_Paper.pdf</url>
      <abstract>This paper presents a method for greatly reducing parse times in LFG by integrating a Constraint Grammar parser into a probabilistic context-free grammar. The CG parser is used in the pre-processing phase to reduce morphological and lexical ambiguity. Similarly, the c-structure pruning mechanism of XLE is used in the parsing phase to discard low-probability c-structures, before f-annotations are solved. The experiment results show a considerable increase in parsing efficiency and robustness in the annotation of Wolof running text. The Wolof CG parser indicated an f-score of 90% for morphological disambiguation and a speedup of ca. 40%, while the c-structure pruning method increased the speed of the Wolof grammar by over 36%. On a small amount of data, CG disambiguation and c-structure pruning allowed for a speedup of 58%, however with a substantial drop in parse accuracy of 3.62.</abstract>
    </paper>
    <paper id="498">
      <author><first>Maha</first><last>Althobaiti</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <title><fixed-case>A</fixed-case>ra<fixed-case>NLP</fixed-case>: a <fixed-case>J</fixed-case>ava-based Library for the Processing of <fixed-case>A</fixed-case>rabic Text.</title>
      <pages>4134–4138</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/621_Paper.pdf</url>
      <abstract>We present a free, Java-based library named “AraNLP” that covers various Arabic text preprocessing tools. Although a good number of tools for processing Arabic text already exist, integration and compatibility problems continually occur. AraNLP is an attempt to gather most of the vital Arabic text preprocessing tools into one library that can be accessed easily by integrating or accurately adapting existing tools and by developing new ones when required. The library includes a sentence detector, tokenizer, light stemmer, root stemmer, part-of speech tagger (POS-tagger), word segmenter, normalizer, and a punctuation and diacritic remover.</abstract>
    </paper>
    <paper id="499">
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Annie</first><last>Zaenen</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title>Criteria for Identifying and Annotating Caused Motion Constructions in Corpus Data</title>
      <pages>1297–1304</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/624_Paper.pdf</url>
      <abstract>While natural language processing performance has been improved through the recognition that there is a relationship between the semantics of the verb and the syntactic context in which the verb is realized, sentences where the verb does not conform to the expected syntax-semantic patterning behavior remain problematic. For example, in the sentence The crowed laughed the clown off the stage, a verb of non-verbal communication laugh is used in a caused motion construction and gains a motion entailment that is atypical given its inherent lexical semantics. This paper focuses on our efforts at defining the semantic types and varieties of caused motion constructions (CMCs) through an iterative annotation process and establishing annotation guidelines based on these criteria to aid in the production of a consistent and reliable annotation. The annotation will serve as training and test data for classifiers for CMCs, and the CMC definitions developed throughout this study will be used in extending VerbNet to handle representations of sentences in which a verb is used in a syntactic context that is atypical for its lexical semantics.</abstract>
    </paper>
    <paper id="500">
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <title>Web-imageability of the Behavioral Features of Basic-level Concepts</title>
      <pages>3609–3614</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/627_Paper.pdf</url>
      <abstract>The recent research direction toward multimodal semantic representation would be further advanced, if we could have a machinery to collect adequate images from the Web, given a target concept. With this motivation, this paper particularly investigates into the Web imageabilities of the behavioral features (e.g. beaver builds dams) of a basic-level concept (beaver). The term Web-imageability denotes how adequately the images acquired from the Web deliver the intended meaning of a complex concept. The primary contributions made in this paper are twofold: (1) beaver building dams-type queries can better yield relevant Web images, suggesting that the present participle form (-ing form) of a verb (building), as a query component, is more effective than the base form; (2) the behaviors taken by animate beings are likely to be more depicted on the Web, particularly if the behaviors are, in a sense, inherent to animate beings (e.g.,motion, consumption), while the creation-type behaviors of inanimate beings are not. The paper further analyzes linguistic annotations that were independently given to some of the images, and discusses an aspect of the semantic gap between image and language.</abstract>
    </paper>
    <paper id="501">
      <author><first>Steve</first><last>Cassidy</last></author>
      <author><first>Dominique</first><last>Estival</last></author>
      <author><first>Timothy</first><last>Jones</last></author>
      <author><first>Denis</first><last>Burnham</last></author>
      <author><first>Jared</first><last>Burghold</last></author>
      <title>The Alveo Virtual Laboratory: A Web Based Repository <fixed-case>API</fixed-case></title>
      <pages>1–7</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/628_Paper.pdf</url>
      <abstract>The Human Communication Science Virtual Laboratory (HCS vLab) is an eResearch project funded under the Australian Government NeCTAR program to build a platform for collaborative eResearch around data representing human communication and the tools that researchers use in their analysis. The human communication science field is broadly defined to encompass the study of language from various perspectives but also includes research on music and various other forms of human expression. This paper outlines the core architecture of the HCS vLab and in particular, highlights the web based API that provides access to data and tools to authenticated users.</abstract>
    </paper>
    <paper id="502">
      <author><first>Chris</first><last>Culy</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Ulla</first><last>König-Cardanobile</last></author>
      <title>A Compact Interactive Visualization of Dependency Treebank Query Results</title>
      <pages>759–766</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/63_Paper.pdf</url>
      <abstract>One of the challenges of corpus querying is making sense of the results of a query, especially when a large number of results and linguistically annotated data are concerned. While the most widespread tools for querying syntactically annotated corpora tend to focus on single occurrences, one aspect that is not fully exploited yet in this area is that language is a complex system whose units are connected to each other at both microscopic (the single occurrences) and macroscopic level (the whole system itself). Assuming that language is a system, we describe a tool (using the DoubleTreeJS visualization) to visualize the results of querying dependency treebanks by forming a node from a single item type, and building a network in which the heads and the dependents of the central node are respectively the left and the right vertices of the tree, which are connected to the central node by dependency relations. One case study is presented, consisting in the exploitation of DoubleTreeJS for supporting one assumption in theoretical linguistics with evidence provided by the data of a dependency treebank of Medieval Latin.</abstract>
    </paper>
    <paper id="503">
      <author><first>Irina</first><last>Temnikova</last></author>
      <author><first>Andrea</first><last>Varga</last></author>
      <author><first>Dogan</first><last>Biyikli</last></author>
      <title>Building a Crisis Management Term Resource for Social Media: The Case of Floods and Protests</title>
      <pages>740–747</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/630_Paper.pdf</url>
      <abstract>Extracting information from social media is being currently exploited for a variety of tasks, including the recognition of emergency events in Twitter. This is done in order to supply Crisis Management agencies with additional crisis information. The existing approaches, however, mostly rely on geographic location and hashtags/keywords, obtained via a manual Twitter search. As we expect that Twitter crisis terminology would differ from existing crisis glossaries, we start collecting a specialized terminological resource to support this task. The aim of this resource is to contain sets of crisis-related Twitter terms which are the same for different instances of the same type of event. This article presents a preliminary investigation of the nature of terms used in four events of two crisis types, tests manual and automatic ways to collect these terms and comes up with an initial collection of terms for these two types of events. As contributions, a novel annotation schema is presented, along with important insights into the differences in annotations between different specialists, descriptive term statistics, and performance results of existing automatic terminology recognition approaches for this task.</abstract>
    </paper>
    <paper id="504">
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Zhiye</first><last>Fei</last></author>
      <author><first>Aaron</first><last>Dai</last></author>
      <author><first>Mark</first><last>Sammons</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Stephen</first><last>Mayhew</last></author>
      <title><fixed-case>I</fixed-case>LLINOIS<fixed-case>C</fixed-case>LOUD<fixed-case>NLP</fixed-case>: Text Analytics Services in the Cloud</title>
      <pages>14–21</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/632_Paper.pdf</url>
      <abstract>Natural Language Processing (NLP) continues to grow in popularity in a range of research and commercial applications. However, installing, maintaining, and running NLP tools can be time consuming, and many commercial and research end users have only intermittent need for large processing capacity. This paper describes ILLINOISCLOUDNLP, an on-demand framework built around NLPCURATOR and Amazon Web Services Elastic Compute Cloud (EC2). This framework provides a simple interface to end users via which they can deploy one or more NLPCURATOR instances on EC2, upload plain text documents, specify a set of Text Analytics tools (NLP annotations) to apply, and process and store or download the processed data. It can also allow end users to use a model trained on their own data: ILLINOISCLOUDNLP takes care of training, hosting, and applying it to new data just as it does with existing models within NLPCURATOR. As a representative use case, we describe our use of ILLINOISCLOUDNLP to process 3.05 million documents used in the 2012 and 2013 Text Analysis Conference Knowledge Base Population tasks at a relatively deep level of processing, in approximately 20 hours, at an approximate cost of US$500; this is about 20 times faster than doing so on a single server and requires no human supervision and no NLP or Machine Learning expertise.</abstract>
    </paper>
    <paper id="505">
      <author><first>Satoshi</first><last>Sato</last></author>
      <title>Text Readability and Word Distribution in <fixed-case>J</fixed-case>apanese</title>
      <pages>2811–2815</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/633_Paper.pdf</url>
      <abstract>This paper reports the relation between text readability and word distribution in the Japanese language. There was no similar study in the past due to three major obstacles: (1) unclear definition of Japanese “word”, (2) no balanced corpus, and (3) no readability measure. Compilation of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) and development of a readability predictor remove these three obstacles and enable this study. First, we have counted the frequency of each word in each text in the corpus. Then we have calculated the frequency rank of words both in the whole corpus and in each of three readability bands. Three major findings are: (1) the proportion of high-frequent words to tokens in Japanese is lower than that in English; (2) the type-coverage curve of words in the difficult-band draws an unexpected shape; (3) the size of the intersection between high-frequent words in the easy-band and these in the difficult-band is unexpectedly small.</abstract>
    </paper>
    <paper id="506">
      <author><first>Julie</first><last>Hochgesang</last></author>
      <title>The Use of a <fixed-case>F</fixed-case>ile<fixed-case>M</fixed-case>aker Pro Database in Evaluating Sign Language Notation Systems</title>
      <pages>1917–1923</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/634_Paper.pdf</url>
      <abstract>In this paper, FileMaker Pro has been used to create a database in order to evaluate sign language notation systems used for representing hand configurations. The database cited in this paper focuses on child acquisition data, particularly the dataset of one child and one adult productions of the same American Sign Language (ASL) signs produced in a two-year span. The hand configurations in selected signs have been coded using Stokoe notation (Stokoe, Casterline &amp; Croneberg, 1965), the Hamburg Notation System or HamNoSys (Prillwitz et al, 1989), the revised Prosodic Model Handshape Coding system or PM (Eccarius &amp; Brentari, 2008) and Sign Language Phonetic Annotation or SLPA, a notation system that has grown from the Movement-Hold Model (Johnson &amp; Liddell, 2010, 2011a, 2011b, 2012). Data was pulled from ELAN transcripts, organized and notated in a FileMaker Pro database created to investigate the representativeness of each system. Representativeness refers to the ability of the notation system to represent the hand configurations in the dataset. This paper briefly describes the design of the FileMaker Pro database intended to provide both quantitative and qualitative information in order to allow the sign language researcher to examine the representativeness of sign language notation systems.</abstract>
    </paper>
    <paper id="507">
      <author><first>Octavian</first><last>Popescu</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Patrick</first><last>Hanks</last></author>
      <title>Mapping <fixed-case>CPA</fixed-case> Patterns onto <fixed-case>O</fixed-case>nto<fixed-case>N</fixed-case>otes Senses</title>
      <pages>882–889</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/636_Paper.pdf</url>
      <abstract>In this paper we present an alignment experiment between patterns of verb use discovered by Corpus Pattern Analysis (CPA; Hanks 2004, 2008, 2012) and verb senses in OntoNotes (ON; Hovy et al. 2006, Weischedel et al. 2011). We present a probabilistic approach for mapping one resource into the other. Firstly we introduce a basic model, based on conditional probabilities, which determines for any given sentence the best CPA pattern match. On the basis of this model, we propose a joint source channel model (JSCM) that computes the probability of compatibility of semantic types between a verb phrase and a pattern, irrespective of whether the verb phrase is a norm or an exploitation. We evaluate the accuracy of the proposed mapping using cluster similarity metrics based on entropy.</abstract>
    </paper>
    <paper id="508">
      <author><first>Emily M.</first><last>Bender</last></author>
      <title>Language <fixed-case>C</fixed-case>o<fixed-case>LLAGE</fixed-case>: Grammatical Description with the <fixed-case>L</fixed-case>in<fixed-case>GO</fixed-case> Grammar Matrix</title>
      <pages>2447–2451</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/639_Paper.pdf</url>
      <abstract>Language CoLLAGE is a collection of grammatical descriptions developed in the context of a grammar engineering graduate course with the LinGO Grammar Matrix. These grammatical descriptions include testsuites in well-formed interlinear glossed text (IGT) format, high-level grammatical characterizations called choices files, HPSG grammar fragments (capable of parsing and generation), and documentation. As of this writing, Language CoLLAGE includes resources for 52 typologically and areally diverse languages and this number is expected to grow over time. The resources for each language cover a similar range of core grammatical phenomena and are implemented in a uniform framework, compatible with the DELPH-IN suite of processing tools.</abstract>
    </paper>
    <paper id="509">
      <author><first>Silvia Rodríguez</first><last>Vázquez</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Anton</first><last>Bolfing</last></author>
      <title>Applying Accessibility-Oriented Controlled Language (<fixed-case>CL</fixed-case>) Rules to Improve Appropriateness of Text Alternatives for Images: an Exploratory Study</title>
      <pages>4139–4146</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/640_Paper.pdf</url>
      <abstract>At present, inappropriate text alternatives for images in the Web continue to pose web accessibility barriers for people with special needs. Although research efforts have been devoted to define how to write text equivalents for visual content in websites, existing guidelines often lack direct linguistic-oriented recommendations. Similarly, most web accessibility evaluation tools just provide users with an automated functionality to check the presence of text alternatives within the element, rather than a platform to verify their content. This paper presents an overview of the findings from an exploratory study carried out to investigate if the appropriateness level of text alternatives for images in French can be improved when applying controlled language (CL) rules. Results gathered suggest that using accessibility-oriented alt style rules can have a significant impact on text alternatives appropriateness. Although more data would be needed to draw further conclusions about our proposal, this preliminary study already offers an interest insight into the potential use of CL checkers such as Acrolinx for language-based web accessibility evaluation.</abstract>
    </paper>
    <paper id="510">
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <title>A Multi-Dialect, Multi-Genre Corpus of Informal Written <fixed-case>A</fixed-case>rabic</title>
      <pages>241–245</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/641_Paper.pdf</url>
      <abstract>This paper presents a multi-dialect, multi-genre, human annotated corpus of dialectal Arabic. We collected utterances in five Arabic dialects: Levantine, Gulf, Egyptian, Iraqi and Maghrebi. We scraped newspaper websites for user commentary and Twitter for two distinct types of dialectal content. To the best of the authors knowledge, this work is the most diverse corpus of dialectal Arabic in both the source of the content and the number of dialects. Every utterance in the corpus was human annotated on Amazons Mechanical Turk; this stands in contrast to Al-Sabbagh and Girju (2012) where only a small subset was human annotated in order to train a classifier to automatically annotate the remainder of the corpus. We provide a discussion of the methodology used for the annotation in addition to the performance of the individual workers. We extend the Arabic dialect identification task to the Iraqi and Maghrebi dialects and improve the results of Zaidan and Callison-Burch (2011a) on Levantine, Gulf and Egyptian.</abstract>
    </paper>
    <paper id="511">
      <author><first>Elisa</first><last>Omodei</last></author>
      <author><first>Jean-Philippe</first><last>Cointet</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <title>Reconstructing the Semantic Landscape of Natural Language Processing</title>
      <pages>2972–2978</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/643_Paper.pdf</url>
      <abstract>This paper investigates the evolution of the computational linguistics domain through a quantitative analysis of the ACL Anthology (containing around 12,000 papers published between 1985 and 2008). Our approach combines complex system methods with natural language processing techniques. We reconstruct the socio-semantic landscape of the domain by inferring a co-authorship and a semantic network from the analysis of the corpus. First, keywords are extracted using a hybrid approach mixing linguistic patterns with statistical information. Then, the semantic network is built using a co-occurrence analysis of these keywords within the corpus. Combining temporal and network analysis techniques, we are able to examine the main evolutions of the field and the more active subfields over time. Lastly we propose a model to explore the mutual influence of the social and the semantic network over time, leading to a socio-semantic co-evolutionary system.</abstract>
    </paper>
    <paper id="512">
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Gleb</first><last>Satyukov</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Marit</first><last>Nijsen</last></author>
      <title>Discovering and Visualising Stories in News</title>
      <pages>3277–3282</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/645_Paper.pdf</url>
      <abstract>Daily news streams often revolve around topics that span over a longer period of time such as the global financial crisis or the healthcare debate in the US. The length and depth of these stories can be such that they become difficult to track for information specialists who need to reconstruct exactly what happened for policy makers and companies. We present a framework to model stories from news: we describe the characteristics that make up interesting stories, how these translate to filters on our data and we present a first use case in which we detail the steps to visualising story lines extracted from news articles about the global automotive industry.</abstract>
    </paper>
    <paper id="513">
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Jun</first><last>Araki</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <title>Supervised Within-Document Event Coreference using Information Propagation</title>
      <pages>4539–4544</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/646_Paper.pdf</url>
      <abstract>Event coreference is an important task for full text analysis. However, previous work uses a variety of approaches, sources and evaluation, making the literature confusing and the results incommensurate. We provide a description of the differences to facilitate future research. Second, we present a supervised method for event coreference resolution that uses a rich feature set and propagates information alternatively between events and their arguments, adapting appropriately for each type of argument.</abstract>
    </paper>
    <paper id="514">
      <author><first>Ana</first><last>Aguiar</last></author>
      <author><first>Mariana</first><last>Kaiseler</last></author>
      <author><first>Hugo</first><last>Meinedo</last></author>
      <author><first>Pedro</first><last>Almeida</last></author>
      <author><first>Mariana</first><last>Cunha</last></author>
      <author><first>Jorge</first><last>Silva</last></author>
      <title><fixed-case>VOCE</fixed-case> Corpus: Ecologically Collected Speech Annotated with Physiological and Psychological Stress Assessments</title>
      <pages>1568–1574</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/647_Paper.pdf</url>
      <abstract>Public speaking is a widely requested professional skill, and at the same time an activity that causes one of the most common adult phobias (Miller and Stone, 2009). It is also known that the study of stress under laboratory conditions, as it is most commonly done, may provide only limited ecological validity (Wilhelm and Grossman, 2010). Previously, we introduced an inter-disciplinary methodology to enable collecting a large amount of recordings under consistent conditions (Aguiar et al., 2013). This paper introduces the VOCE corpus of speech annotated with stress indicators under naturalistic public speaking (PS) settings, and makes it available at http://paginas.fe.up.pt/~voce/articles.html. The novelty of this corpus is that the recordings are carried out in objectively stressful PS situations, as recommended in (Zanstra and Johnston, 2011). The current database contains a total of 38 recordings, 13 of which contain full psychologic and physiologic annotation. We show that the collected recordings validate the assumptions of the methodology, namely that participants experience stress during the PS events. We describe the various metrics that can be used for physiologic and psychologic annotation, and we characterise the sample collected so far, providing evidence that demographics do not affect the relevant psychologic or physiologic annotation. The collection activities are on-going, and we expect to increase the number of complete recordings in the corpus to 30 by June 2014.</abstract>
    </paper>
    <paper id="515">
      <author><first>Shinsuke</first><last>Mori</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <title>Language Resource Addition: Dictionary or Corpus?</title>
      <pages>1631–1636</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/648_Paper.pdf</url>
      <abstract>In this paper, we investigate the relative effect of two strategies of language resource additions to the word segmentation problem and part-of-speech tagging problem in Japanese. The first strategy is adding entries to the dictionary and the second is adding annotated sentences to the training corpus. The experimental results showed that the annotated sentence addition to the training corpus is better than the entries addition to the dictionary. And the annotated sentence addition is efficient especially when we add new words with contexts of three real occurrences as partially annotated sentences. According to this knowledge, we executed annotation on the invention disclosure texts and observed word segmentation accuracy.</abstract>
    </paper>
    <paper id="516">
      <author><first>Luca</first><last>Cristoforetti</last></author>
      <author><first>Mirco</first><last>Ravanelli</last></author>
      <author><first>Maurizio</first><last>Omologo</last></author>
      <author><first>Alessandro</first><last>Sosi</last></author>
      <author><first>Alberto</first><last>Abad</last></author>
      <author><first>Martin</first><last>Hagmueller</last></author>
      <author><first>Petros</first><last>Maragos</last></author>
      <title>The <fixed-case>DIRHA</fixed-case> simulated corpus</title>
      <pages>2629–2634</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/650_Paper.pdf</url>
      <abstract>This paper describes a multi-microphone multi-language acoustic corpus being developed under the EC project Distant-speech Interaction for Robust Home Applications (DIRHA). The corpus is composed of several sequences obtained by convolution of dry acoustic events with more than 9000 impulse responses measured in a real apartment equipped with 40 microphones. The acoustic events include in-domain sentences of different typologies uttered by native speakers in four different languages and non-speech events representing typical domestic noises. To increase the realism of the resulting corpus, background noises were recorded in the real home environment and then added to the generated sequences. The purpose of this work is to describe the simulation procedure and the data sets that were created and used to derive the corpus. The corpus contains signals of different characteristics making it suitable for various multi-microphone signal processing and distant speech recognition tasks.</abstract>
    </paper>
    <paper id="517">
      <author><first>Daniel</first><last>Hladek</last></author>
      <author><first>Jan</first><last>Stas</last></author>
      <author><first>Jozef</first><last>Juhar</last></author>
      <title>The <fixed-case>S</fixed-case>lovak Categorized News Corpus</title>
      <pages>1705–1708</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/656_Paper.pdf</url>
      <abstract>The presented corpus aims to be the first attempt to create a representative sample of the contemporary Slovak language from various domains with easy searching and automated processing. This first version of the corpus contains words and automatic morphological and named entity annotations and transcriptions of abbreviations and numerals. Integral part of the proposed paper is a word boundary and sentence boundary detection algorithm that utilizes characteristic features of the language.</abstract>
    </paper>
    <paper id="518">
      <author><first>Uwe</first><last>Quasthoff</last></author>
      <author><first>Dirk</first><last>Goldhahn</last></author>
      <author><first>Thomas</first><last>Eckart</last></author>
      <author><first>Erla</first><last>Hallsteinsdóttir</last></author>
      <author><first>Sabine</first><last>Fiedler</last></author>
      <title>High Quality Word Lists as a Resource for Multiple Purposes</title>
      <pages>2816–2819</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/657_Paper.pdf</url>
      <abstract>Since 2011 the comprehensive, electronically available sources of the Leipzig Corpora Collection have been used consistently for the compilation of high quality word lists. The underlying corpora include newspaper texts, Wikipedia articles and other randomly collected Web texts. For many of the languages featured in this collection, it is the first comprehensive compilation to use a large-scale empirical base. The word lists have been used to compile dictionaries with comparable frequency data in the Frequency Dictionaries series. This includes frequency data of up to 1,000,000 word forms presented in alphabetical order. This article provides an introductory description of the data and the methodological approach used. In addition, language-specific statistical information is provided with regard to letters, word structure and structural changes. Such high quality word lists also provide the opportunity to explore comparative linguistic topics and such monolingual issues as studies of word formation and frequency-based examinations of lexical areas for use in dictionaries or language teaching. The results presented here can provide initial suggestions for subsequent work in several areas of research.</abstract>
    </paper>
    <paper id="519">
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <title>A Quality-based Active Sample Selection Strategy for Statistical Machine Translation</title>
      <pages>2690–2695</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/658_Paper.pdf</url>
      <abstract>This paper presents a new active learning technique for machine translation based on quality estimation of automatically translated sentences. It uses an error-driven strategy, i.e., it assumes that the more errors an automatically translated sentence contains, the more informative it is for the translation system. Our approach is based on a quality estimation technique which involves a wider range of features of the source text, automatic translation, and machine translation system compared to previous work. In addition, we enhance the machine translation system training data with post-edited machine translations of the sentences selected, instead of simulating this using previously created reference translations. We found that re-training systems with additional post-edited data yields higher quality translations regardless of the selection strategy used. We relate this to the fact that post-editions tend to be closer to source sentences as compared to references, making the rule extraction process more reliable.</abstract>
    </paper>
    <paper id="520">
      <author><first>Juri</first><last>Ganitkevitch</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <title>The Multilingual Paraphrase Database</title>
      <pages>4276–4283</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/659_Paper.pdf</url>
      <abstract>We release a massive expansion of the paraphrase database (PPDB) that now includes a collection of paraphrases in 23 different languages. The resource is derived from large volumes of bilingual parallel data. Our collection is extracted and ranked using state of the art methods. The multilingual PPDB has over a billion paraphrase pairs in total, covering the following languages: Arabic, Bulgarian, Chinese, Czech, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Polish, Portugese, Romanian, Russian, Slovak, Slovenian, and Swedish.</abstract>
    </paper>
    <paper id="521">
      <author><first>Menno</first><last>van Zaanen</last></author>
      <author><first>Gerhard</first><last>van Huyssteen</last></author>
      <author><first>Suzanne</first><last>Aussems</last></author>
      <author><first>Chris</first><last>Emmery</last></author>
      <author><first>Roald</first><last>Eiselen</last></author>
      <title>The Development of <fixed-case>D</fixed-case>utch and <fixed-case>A</fixed-case>frikaans Language Resources for Compound Boundary Analysis.</title>
      <pages>1056–1062</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/66_Paper.pdf</url>
      <abstract>In most languages, new words can be created through the process of compounding, which combines two or more words into a new lexical unit. Whereas in languages such as English the components that make up a compound are separated by a space, in languages such as Finnish, German, Afrikaans and Dutch these components are concatenated into one word. Compounding is very productive and leads to practical problems in developing machine translators and spelling checkers, as newly formed compounds cannot be found in existing lexicons. The Automatic Compound Processing (AuCoPro) project deals with the analysis of compounds in two closely-related languages, Afrikaans and Dutch. In this paper, we present the development and evaluation of two datasets, one for each language, that contain compound words with annotated compound boundaries. Such datasets can be used to train classifiers to identify the compound components in novel compounds. We describe the process of annotation and provide an overview of the annotation guidelines as well as global properties of the datasets. The inter-rater agreements between the annotators are considered highly reliable. Furthermore, we show the usability of these datasets by building an initial automatic compound boundary detection system, which assigns compound boundaries with approximately 90% accuracy.</abstract>
    </paper>
    <paper id="522">
      <author><first>Lluís</first><last>Padró</last></author>
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Xavier</first><last>Carreras</last></author>
      <author><first>Blaz</first><last>Fortuna</last></author>
      <author><first>Esteban</first><last>García-Cuesta</last></author>
      <author><first>Zhixing</first><last>Li</last></author>
      <author><first>Tadej</first><last>Štajner</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <title>Language Processing Infrastructure in the <fixed-case>XL</fixed-case>ike Project</title>
      <pages>3811–3816</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/660_Paper.pdf</url>
      <abstract>This paper presents the linguistic analysis tools and its infrastructure developed within the XLike project. The main goal of the implemented tools is to provide a set of functionalities for supporting some of the main objectives of XLike, such as enabling cross-lingual services for publishers, media monitoring or developing new business intelligence applications. The services cover seven major and minor languages: English, German, Spanish, Chinese, Catalan, Slovenian, and Croatian. These analyzers are provided as web services following a lightweight SOA architecture approach, and they are publically callable and are catalogued in META-SHARE.</abstract>
    </paper>
    <paper id="523">
      <author><first>Gregor</first><last>Thurmair</last></author>
      <title>Conceptual transfer: Using local classifiers for transfer selection</title>
      <pages>4387–4393</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/661_Paper.pdf</url>
      <abstract>A key challenge for Machine Translation is transfer selection, i.e. to find the right translation for a given word from a set of alternatives (1:n). This problem becomes the more important the larger the dictionary is, as the number of alternatives increases. The contribution presents a novel approach for transfer selection, called conceptual transfer, where selection is done using classifiers based on the conceptual context of a translation candidate on the source language side. Such classifiers are built automatically by parallel corpus analysis: Creating subcorpora for each translation of a 1:n package, and identifying correlating concepts in these subcorpora as features of the classifier. The resulting resource can easily be linked to transfer components of MT systems as it does not depend on internal analysis structures. Tests show that conceptual transfer outperforms the selection techniques currently used in operational MT systems.</abstract>
    </paper>
    <paper id="524">
      <author><first>Marta</first><last>Villegas</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <title>Metadata as Linked Open Data: mapping disparate <fixed-case>XML</fixed-case> metadata registries into one <fixed-case>RDF</fixed-case>/<fixed-case>OWL</fixed-case> registry.</title>
      <pages>393–400</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/664_Paper.pdf</url>
      <abstract>The proliferation of different metadata schemas and models pose serious problems of interoperability. Maintaining isolated repositories with overlapping data is costly in terms of time and effort. In this paper, we describe how we have achieved a Linked Open Data version of metadata descriptions coming from heterogeneous sources, originally encoded in XML. The resulting model is much simpler than the original XSD schema and avoids problems typical of XML syntax, such as semantic ambiguity and order constraint. Moreover, the open world assumption of RDF/OWL allows to naturally integrate objects from different schemas and to add further extensions, facilitating merging of different models as well as linking to external data. Apart from the advantages in terms of interoperability and maintainability, the merged repository enables end-users to query multiple sources using a unified schema and is able to present them with implicit knowledge derived from the linked data. The approach we present here is easily scalable to any number of sources and schemas.</abstract>
    </paper>
    <paper id="525">
      <author><first>Grégoire</first><last>Détrez</last></author>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
      <author><first>Aarne</first><last>Ranta</last></author>
      <title>Sharing resources between free/open-source rule-based machine translation systems: Grammatical Framework and Apertium</title>
      <pages>4394–4400</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/665_Paper.pdf</url>
      <abstract>In this paper, we describe two methods developed for sharing linguistic data between two free and open source rule based machine translation systems: Apertium, a shallow-transfer system; and Grammatical Framework (GF), which performs a deeper syntactic transfer. In the first method, we describe the conversion of lexical data from Apertium to GF, while in the second one we automatically extract Apertium shallow-transfer rules from a GF bilingual grammar. We evaluated the resulting systems in a English-Spanish translation context, and results showed the usefulness of the resource sharing and confirmed the a-priori strong and weak points of the systems involved. \\</abstract>
    </paper>
    <paper id="526">
      <author><first>Georgios</first><last>Petasis</last></author>
      <title>Annotating Arguments: The <fixed-case>NOMAD</fixed-case> Collaborative Annotation Tool</title>
      <pages>1930–1937</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/669_Paper.pdf</url>
      <abstract>The huge amount of the available information in the Web creates the need for effective information extraction systems that are able to produce metadata that satisfy user’s information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn or evaluate extraction models. The production of such corpora can be significantly facilitated by annotation tools, which provide user-friendly facilities and enable annotators to annotate documents according to a predefined annotation schema. However, the construction of annotation tools that operate in a distributed environment is a challenging task: the majority of these tools are implemented as Web applications, having to cope with the capabilities offered by browsers. This paper describes the NOMAD collaborative annotation tool, which implements an alternative architecture: it remains a desktop application, fully exploiting the advantages of desktop applications, but provides collaborative annotation through the use of a centralised server for storing both the documents and their metadata, and instance messaging protocols for communicating events among all annotators. The annotation tool is implemented as a component of the Ellogon language engineering platform, exploiting its extensive annotation engine, its cross-platform abilities and its linguistic processing components, if such a need arises. Finally, the NOMAD annotation tool is distributed with an open source license, as part of the Ellogon platform.</abstract>
    </paper>
    <paper id="527">
      <author><first>Diana</first><last>Maynard</last></author>
      <author><first>Mark</first><last>Greenwood</last></author>
      <title>Who cares about Sarcastic Tweets? Investigating the Impact of Sarcasm on Sentiment Analysis.</title>
      <pages>4238–4243</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/67_Paper.pdf</url>
      <abstract>Sarcasm is a common phenomenon in social media, and is inherently difficult to analyse, not just automatically but often for humans too. It has an important effect on sentiment, but is usually ignored in social media analysis, because it is considered too tricky to handle. While there exist a few systems which can detect sarcasm, almost no work has been carried out on studying the effect that sarcasm has on sentiment in tweets, and on incorporating this into automatic tools for sentiment analysis. We perform an analysis of the effect of sarcasm scope on the polarity of tweets, and have compiled a number of rules which enable us to improve the accuracy of sentiment analysis when sarcasm is known to be present. We consider in particular the effect of sentiment and sarcasm contained in hashtags, and have developed a hashtag tokeniser for GATE, so that sentiment and sarcasm found within hashtags can be detected more easily. According to our experiments, the hashtag tokenisation achieves 98% Precision, while the sarcasm detection achieved 91% Precision and polarity detection 80%.</abstract>
    </paper>
    <paper id="528">
      <author><first>Xabier</first><last>Artola</last></author>
      <author><first>Zuhaitz</first><last>Beloki</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <title>A stream computing approach towards scalable <fixed-case>NLP</fixed-case></title>
      <pages>8–13</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/670_Paper.pdf</url>
      <abstract>Computational power needs have grown dramatically in recent years. This is also the case in many language processing tasks, due to overwhelming quantities of textual information that must be processed in a reasonable time frame. This scenario has led to a paradigm shift in the computing architectures and large-scale data processing strategies used in the NLP field. In this paper we describe a series of experiments carried out in the context of the NewsReader project with the goal of analyzing the scaling capabilities of the language processing pipeline used in it. We explore the use of Storm in a new approach for scalable distributed language processing across multiple machines and evaluate its effectiveness and efficiency when processing documents on a medium and large scale. The experiments have shown that there is a big room for improvement regarding language processing performance when adopting parallel architectures, and that we might expect even better results with the use of large clusters with many processing nodes.</abstract>
    </paper>
    <paper id="529">
      <author><first>Þórdís</first><last>Úlfarsdóttir</last></author>
      <title><fixed-case>ISLEX</fixed-case> — a Multilingual Web Dictionary</title>
      <pages>2820–2825</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/672_Paper.pdf</url>
      <abstract>ISLEX is a multilingual Scandinavian dictionary, with Icelandic as a source language and Danish, Norwegian, Swedish, Faroese and Finnish as target languages. Within ISLEX are in fact contained several independent, bilingual dictionaries. While Faroese and Finnish are still under construction, the other languages were opened to the public on the web in November 2011. The use of the dictionary is free of charge and it has been extremely well received by its users. The result of the project is threefold. Firstly, some long awaited Icelandic-Scandinavian dictionaries have been published on the digital medium. Secondly, the project has been an important experience in Nordic language collaboration by jointly building such a work in six countries simultaneously, by academic institutions in Iceland, Denmark, Norway, Sweden, The Faroe Islands and Finland. Thirdly, the work has resulted in a compilation of structured linguistic data of the Nordic languages. This data is suitable for use in further lexicographic work and in various language technology projects.</abstract>
    </paper>
    <paper id="530">
      <author><first>Manuela</first><last>Sanguinetti</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Loredana</first><last>Cupi</last></author>
      <title>Exploiting catenae in a parallel treebank alignment</title>
      <pages>1824–1831</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/674_Paper.pdf</url>
      <abstract>This paper aims to introduce the issues related to the syntactic alignment of a dependency-based multilingual parallel treebank, ParTUT. Our approach to the task starts from a lexical mapping and then attempts to expand it using dependency relations. In developing the system, however, we realized that the only dependency relations between the individual nodes were not sufficient to overcome some translation divergences, or shifts, especially in the absence of a direct lexical mapping and a different syntactic realization. For this purpose, we explored the use of a novel syntactic notion introduced in dependency theoretical framework, i.e. that of catena (Latin for “chain”), which is intended as a group of words that are continuous with respect to dominance. In relation to the task of aligning parallel dependency structures, catenae can be used to explain and identify those cases of one-to-many or many-to-many correspondences, typical of several translation shifts, that cannot be detected by means of direct word-based mappings or bare syntactic relations. The paper presented here describes the overall structure of the alignment system as it has been currently designed, how catenae are extracted from the parallel resource, and their potential relevance to the completion of tree alignment in ParTUT sentences.</abstract>
    </paper>
    <paper id="531">
      <author><first>Irina</first><last>Temnikova</last></author>
      <author><first>William A.</first><last>Baumgartner Jr.</last></author>
      <author><first>Negacy D.</first><last>Hailu</last></author>
      <author><first>Ivelina</first><last>Nikolova</last></author>
      <author><first>Tony</first><last>McEnery</last></author>
      <author><first>Adam</first><last>Kilgarriff</last></author>
      <author><first>Galia</first><last>Angelova</last></author>
      <author><first>K. Bretonnel</first><last>Cohen</last></author>
      <title>Sublanguage Corpus Analysis Toolkit: A tool for assessing the representativeness and sublanguage characteristics of corpora</title>
      <pages>1714–1718</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/675_Paper.pdf</url>
      <abstract>Sublanguages are varieties of language that form subsets of the general language, typically exhibiting particular types of lexical, semantic, and other restrictions and deviance. SubCAT, the Sublanguage Corpus Analysis Toolkit, assesses the representativeness and closure properties of corpora to analyze the extent to which they are either sublanguages, or representative samples of the general language. The current version of SubCAT contains scripts and applications for assessing lexical closure, morphological closure, sentence type closure, over-represented words, and syntactic deviance. Its operation is illustrated with three case studies concerning scientific journal articles, patents, and clinical records. Materials from two language families are analyzed―English (Germanic), and Bulgarian (Slavic). The software is available at sublanguage.sourceforge.net under a liberal Open Source license.</abstract>
    </paper>
    <paper id="532">
      <author><first>Violeta</first><last>Seretan</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Johanna</first><last>Gerlach</last></author>
      <title>A Large-Scale Evaluation of Pre-editing Strategies for Improving User-Generated Content Translation</title>
      <pages>1793–1799</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/676_Paper.pdf</url>
      <abstract>The user-generated content represents an increasing share of the information available today. To make this type of content instantly accessible in another language, the ACCEPT project focuses on developing pre-editing technologies for correcting the source text in order to increase its translatability. Linguistically-informed pre-editing rules have been developed for English and French for the two domains considered by the project, namely, the technical domain and the healthcare domain. In this paper, we present the evaluation experiments carried out to assess the impact of the proposed pre-editing rules on translation quality. Results from a large-scale evaluation campaign show that pre-editing helps indeed attain a better translation quality for a high proportion of the data, the difference with the number of cases where the adverse effect is observed being statistically significant. The ACCEPT pre-editing technology is freely available online and can be used in any Web-based environment to enhance the translatability of user-generated content so that it reaches a broader audience.</abstract>
    </paper>
    <paper id="533">
      <author><first>Sigrún</first><last>Helgadóttir</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <author><first>Eiríkur</first><last>Rögnvaldsson</last></author>
      <title>Correcting Errors in a New Gold Standard for Tagging <fixed-case>I</fixed-case>celandic Text</title>
      <pages>2944–2948</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/677_Paper.pdf</url>
      <abstract>In this paper, we describe the correction of PoS tags in a new Icelandic corpus, MIM-GOLD, consisting of about 1 million tokens sampled from the Tagged Icelandic Corpus, MÍM, released in 2013. The goal is to use the corpus, among other things, as a new gold standard for training and testing PoS taggers. The construction of the corpus was first described in 2010 together with preliminary work on error detection and correction. In this paper, we describe further the correction of tags in the corpus. We describe manual correction and a method for semi-automatic error detection and correction. We show that, even after manual correction, the number of tagging errors in the corpus can be reduced significantly by applying our semi-automatic detection and correction method. After the semi-automatic error correction, preliminary evaluation of tagging accuracy shows very low error rates. We hope that the existence of the corpus will make it possible to improve PoS taggers for Icelandic text.</abstract>
    </paper>
    <paper id="534">
      <author><first>Béatrice</first><last>Daille</last></author>
      <author><first>Amir</first><last>Hazem</last></author>
      <title>Semi-compositional Method for Synonym Extraction of Multi-Word Terms</title>
      <pages>1202–1207</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/679_Paper.pdf</url>
      <abstract>Automatic synonyms and semantically related word extraction is a challenging task, useful in many NLP applications such as question answering, search query expansion, text summarization, etc. While different studies addressed the task of word synonym extraction, only a few investigations tackled the problem of acquiring synonyms of multi-word terms (MWT) from specialized corpora. To extract pairs of synonyms of multi-word terms, we propose in this paper an unsupervised semi-compositional method that makes use of distributional semantics and exploit the compositional property shared by most MWT. We show that our method outperforms significantly the state-of-the-art.</abstract>
    </paper>
    <paper id="535">
      <author><first>Matúš</first><last>Pleva</last></author>
      <author><first>Jozef</first><last>Juhár</last></author>
      <title><fixed-case>TUKE</fixed-case>-<fixed-case>BN</fixed-case>ews-<fixed-case>SK</fixed-case>: <fixed-case>S</fixed-case>lovak Broadcast News Corpus Construction and Evaluation</title>
      <pages>1709–1713</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/680_Paper.pdf</url>
      <abstract>This article presents an overview of the existing acoustical corpuses suitable for broadcast news automatic transcription task in the Slovak language. The TUKE-BNews-SK database created in our department was built to support the application development for automatic broadcast news processing and spontaneous speech recognition of the Slovak language. The audio corpus is composed of 479 Slovak TV broadcast news shows from public Slovak television called STV1 or Jednotka containing 265 hours of material and 186 hours of clean transcribed speech (4 hours subset extracted for testing purposes). The recordings were manually transcribed using Transcriber tool modified for Slovak annotators and automatic Slovak spell checking. The corpus design, acquisition, annotation scheme and pronunciation transcription is described together with corpus statistics and tools used. Finally the evaluation procedure using automatic speech recognition is presented on the broadcast news and parliamentary speeches test sets.</abstract>
    </paper>
    <paper id="536">
      <author><first>Csaba</first><last>Oravecz</last></author>
      <author><first>Tamás</first><last>Váradi</last></author>
      <author><first>Bálint</first><last>Sass</last></author>
      <title>The <fixed-case>H</fixed-case>ungarian <fixed-case>G</fixed-case>igaword Corpus</title>
      <pages>1719–1723</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/681_Paper.pdf</url>
      <abstract>The paper reports on the development of the Hungarian Gigaword Corpus (HGC), an extended new edition of the Hungarian National Corpus, with upgraded and redesigned linguistic annotation and an increased size of 1.5 billion tokens. Issues concerning the standard steps of corpus collection and preparation are discussed with special emphasis on linguistic analysis and annotation due to Hungarian having some challenging characteristics with respect to computational processing. As the HGC is designed to serve as a resource for a wide range of linguistic research as well as for the interested public, a number of issues had to be resolved which were raised by trying to find a balance between the above two application areas. The following main objectives have been defined for the development of the HGC, focusing on the pivotal concept of increase in: - size: extending the corpus to minimum 1 billion words, - quality: using new technology for development and analysis, - coverage and representativity: taking new samples of language use and including further variants (transcribed spoken language data and user generated content (social media) from the internet in particular).</abstract>
    </paper>
    <paper id="537">
      <author><first>Kunal</first><last>Sachdeva</last></author>
      <author><first>Rishabh</first><last>Srivastava</last></author>
      <author><first>Sambhav</first><last>Jain</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <title><fixed-case>H</fixed-case>indi to <fixed-case>E</fixed-case>nglish Machine Translation: Using Effective Selection in Multi-Model <fixed-case>SMT</fixed-case></title>
      <pages>1807–1811</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/682_Paper.pdf</url>
      <abstract>Recent studies in machine translation support the fact that multi-model systems perform better than the individual models. In this paper, we describe a Hindi to English statistical machine translation system and improve over the baseline using multiple translation models. We have considered phrase based as well as hierarchical models and enhanced over both these baselines using a regression model. The system is trained over textual as well as syntactic features extracted from source and target of the aforementioned translations. Our system shows significant improvement over the baseline systems for both automatic as well as human evaluations. The proposed methodology is quite generic and easily be extended to other language pairs as well.</abstract>
    </paper>
    <paper id="538">
      <author><first>Maria Pia</first><last>di Buono</last></author>
      <author><first>Mario</first><last>Monteleone</last></author>
      <title>From Natural Language to Ontology Population in the Cultural Heritage Domain. A Computational Linguistics-based approach.</title>
      <pages>3661–3666</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/686_Paper.pdf</url>
      <abstract>This paper presents an on-going Natural Language Processing (NLP) research based on Lexicon-Grammar (LG) and aimed at improving knowledge management of Cultural Heritage (CH) domain. We intend to demonstrate how our language formalization technique can be applied for both processing and populating a domain ontology. We also use NLP techniques for text extraction and mining to fill information gaps and improve access to cultural resources. The Linguistic Resources (LRs, i.e. electronic dictionaries) we built can be used in the structuring of effective Knowledge Management Systems (KMSs). In order to apply to Parts of Speech (POS) the classes and properties defined by the Conseil Interational des Musees (CIDOC) Conceptual Reference Model (CRM), we use Finite State Transducers/Automata (FSTs/FSA) and their variables built in the form of graphs. FSTs/FSA are also used for analysing corpora in order to retrieve recursive sentence structures, in which combinatorial and semantic constraints identify properties and denote relationship. Besides, FSTs/FSA are also used to match our electronic dictionary entries (ALUs, or Atomic Linguistic Units) to RDF subject, object and predicate (SKOS Core Vocabulary). This matching of linguistic data to RDF and their translation into SPARQL/SERQL path expressions allows the use ALUs to process natural-language queries.</abstract>
    </paper>
    <paper id="539">
      <author><first>Stephen</first><last>Wattam</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Marc</first><last>Alexander</last></author>
      <author><first>Jean</first><last>Anderson</last></author>
      <title>Experiences with Parallelisation of an Existing <fixed-case>NLP</fixed-case> Pipeline: Tagging Hansard</title>
      <pages>4093–4096</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/687_Paper.pdf</url>
      <abstract>This poster describes experiences processing the two-billion-word Hansard corpus using a fairly standard NLP pipeline on a high performance cluster. Herein we report how we were able to parallelise and apply a traditional single-threaded batch-oriented application to a platform that differs greatly from that for which it was originally designed. We start by discussing the tagging toolchain, its specific requirements and properties, and its performance characteristics. This is contrasted with a description of the cluster on which it was to run, and specific limitations are discussed such as the overhead of using SAN-based storage. We then go on to discuss the nature of the Hansard corpus, and describe which properties of this corpus in particular prove challenging for use on the system architecture used. The solution for tagging the corpus is then described, along with performance comparisons against a naive run on commodity hardware. We discuss the gains and benefits of using high-performance machinery rather than relatively cheap commodity hardware. Our poster provides a valuable scenario for large scale NLP pipelines and lessons learnt from the experience.</abstract>
    </paper>
    <paper id="540">
      <author><first>Younggyun</first><last>Hahm</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <author><first>Kyungtae</first><last>Lim</last></author>
      <author><first>Youngsik</first><last>Kim</last></author>
      <author><first>Dosam</first><last>Hwang</last></author>
      <author><first>Key-Sun</first><last>Choi</last></author>
      <title>Named Entity Corpus Construction using <fixed-case>W</fixed-case>ikipedia and <fixed-case>DB</fixed-case>pedia Ontology</title>
      <pages>2565–2569</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/688_Paper.pdf</url>
      <abstract>In this paper, we propose a novel method to automatically build a named entity corpus based on the DBpedia ontology. Since most of named entity recognition systems require time and effort consuming annotation tasks as training data. Work on NER has thus for been limited on certain languages like English that are resource-abundant in general. As an alternative, we suggest that the NE corpus generated by our proposed method, can be used as training data. Our approach introduces Wikipedia as a raw text and uses the DBpedia data set for named entity disambiguation. Our method is language-independent and easy to be applied to many different languages where Wikipedia and DBpedia are provided. Throughout the paper, we demonstrate that our NE corpus is of comparable quality even to the manually annotated NE corpus.</abstract>
    </paper>
    <paper id="541">
      <author><first>Zoraida</first><last>Callejas</last></author>
      <author><first>Brian</first><last>Ravenet</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <author><first>Catherine</first><last>Pelachaud</last></author>
      <title>A model to generate adaptive multimodal job interviews with a virtual recruiter</title>
      <pages>3615–3619</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/689_Paper.pdf</url>
      <abstract>This paper presents an adaptive model of multimodal social behavior for embodied conversational agents. The context of this research is the training of youngsters for job interviews in a serious game where the agent plays the role of a virtual recruiter. With the proposed model the agent is able to adapt its social behavior according to the anxiety level of the trainee and a predefined difficulty level of the game. This information is used to select the objective of the system (to challenge or comfort the user), which is achieved by selecting the complexity of the next question posed and the agent’s verbal and non-verbal behavior. We have carried out a perceptive study that shows that the multimodal behavior of an agent implementing our model successfully conveys the expected social attitudes.</abstract>
    </paper>
    <paper id="542">
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <title>The <fixed-case>SET</fixed-case>imes.<fixed-case>HR</fixed-case> Linguistically Annotated Corpus of <fixed-case>C</fixed-case>roatian</title>
      <pages>1724–1727</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/690_Paper.pdf</url>
      <abstract>We present SETimes.HR ― the first linguistically annotated corpus of Croatian that is freely available for all purposes. The corpus is built on top of the SETimes parallel corpus of nine Southeast European languages and English. It is manually annotated for lemmas, morphosyntactic tags, named entities and dependency syntax. We couple the corpus with domain-sensitive test sets for Croatian and Serbian to support direct model transfer evaluation between these closely related languages. We build and evaluate statistical models for lemmatization, morphosyntactic tagging, named entity recognition and dependency parsing on top of SETimes.HR and the test sets, providing the state of the art in all the tasks. We make all resources presented in the paper freely available under a very permissive licensing scheme.</abstract>
    </paper>
    <paper id="543">
      <author><first>Jerid</first><last>Francom</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <author><first>Adam</first><last>Ussishkin</last></author>
      <title><fixed-case>ACTIV</fixed-case>-<fixed-case>ES</fixed-case>: a comparable, cross-dialect corpus of ‘everyday’ <fixed-case>S</fixed-case>panish from <fixed-case>A</fixed-case>rgentina, <fixed-case>M</fixed-case>exico, and <fixed-case>S</fixed-case>pain</title>
      <pages>1733–1737</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/691_Paper.pdf</url>
      <abstract>Corpus resources for Spanish have proved invaluable for a number of applications in a wide variety of fields. However, a majority of resources are based on formal, written language and/or are not built to model language variation between varieties of the Spanish language, despite the fact that most language in everyday use is informal/ dialogue-based and shows rich regional variation. This paper outlines the development and evaluation of the ACTIV-ES corpus, a first-step to produce a comparable, cross-dialect corpus representative of the everyday language of various regions of the Spanish-speaking world.</abstract>
    </paper>
    <paper id="544">
      <author><first>Van-Minh</first><last>Pho</last></author>
      <author><first>Thibault</first><last>André</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Gabriel</first><last>Illouz</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <title>Multiple Choice Question Corpus Analysis for Distractor Characterization</title>
      <pages>4284–4291</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/692_Paper.pdf</url>
      <abstract>In this paper, we present a study of MCQ aiming to define criteria in order to automatically select distractors. We are aiming to show that distractor editing follows rules like syntactic and semantic homogeneity according to associated answer, and the possibility to automatically identify this homogeneity. Manual analysis shows that homogeneity rule is respected to edit distractors and automatic analysis shows the possibility to reproduce these criteria. These ones can be used in future works to automatically select distractors, with the combination of other criteria.</abstract>
    </paper>
    <paper id="545">
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Daša</first><last>Berović</last></author>
      <author><first>Danijela</first><last>Merkler</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <title><fixed-case>C</fixed-case>roatian Dependency Treebank 2.0: New Annotation Guidelines for Improved Parsing</title>
      <pages>2313–2319</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/694_Paper.pdf</url>
      <abstract>We present a new version of the Croatian Dependency Treebank. It constitutes a slight departure from the previously closely observed Prague Dependency Treebank syntactic layer annotation guidelines as we introduce a new subset of syntactic tags on top of the existing tagset. These new tags are used in explicit annotation of subordinate clauses via subordinate conjunctions. Introducing the new annotation to Croatian Dependency Treebank, we also modify head attachment rules addressing subordinate conjunctions and subordinate clause predicates. In an experiment with data-driven dependency parsing, we show that implementing these new annotation guidelines leeds to a statistically significant improvement in parsing accuracy. We also observe a substantial improvement in inter-annotator agreement, facilitating more consistent annotation in further treebank development.</abstract>
    </paper>
    <paper id="546">
      <author><first>Roberto</first><last>Gretter</last></author>
      <title><fixed-case>E</fixed-case>uronews: a multilingual speech corpus for <fixed-case>ASR</fixed-case></title>
      <pages>2635–2638</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/695_Paper.pdf</url>
      <abstract>In this paper we present a multilingual speech corpus, designed for Automatic Speech Recognition (ASR) purposes. Data come from the portal Euronews and were acquired both from the Web and from TV. The corpus includes data in 10 languages (Arabic, English, French, German, Italian, Polish, Portuguese, Russian, Spanish and Turkish) and was designed both to train AMs and to evaluate ASR performance. For each language, the corpus is composed of about 100 hours of speech for training (60 for Polish) and about 4 hours, manually transcribed, for testing. Training data include the audio, some reference text, the ASR output and their alignment. We plan to make public at least part of the benchmark in view of a multilingual ASR benchmark for IWSLT 2014.</abstract>
    </paper>
    <paper id="547">
      <author><first>Masood</first><last>Ghayoomi</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <title>Constituency Parsing of <fixed-case>B</fixed-case>ulgarian: Word- vs Class-based Parsing</title>
      <pages>4056–4060</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/696_Paper.pdf</url>
      <abstract>In this paper, we report the obtained results of two constituency parsers trained with BulTreeBank, an HPSG-based treebank for Bulgarian. To reduce the data sparsity problem, we propose using the Brown word clustering to do an off-line clustering and map the words in the treebank to create a class-based treebank. The observations show that when the classes outnumber the POS tags, the results are better. Since this approach adds on another dimension of abstraction (in comparison to the lemma), its coarse-grained representation can be used further for training statistical parsers.</abstract>
    </paper>
    <paper id="548">
      <author><first>Maike</first><last>Paetzel</last></author>
      <author><first>David Nicolas</first><last>Racca</last></author>
      <author><first>David</first><last>DeVault</last></author>
      <title>A Multimodal Corpus of Rapid Dialogue Games</title>
      <pages>4189–4195</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/697_Paper.pdf</url>
      <abstract>This paper presents a multimodal corpus of spoken human-human dialogues collected as participants played a series of Rapid Dialogue Games (RDGs). The corpus consists of a collection of about 11 hours of spoken audio, video, and Microsoft Kinect data taken from 384 game interactions (dialogues). The games used for collecting the corpus required participants to give verbal descriptions of linguistic expressions or visual images and were specifically designed to engage players in a fast-paced conversation under time pressure. As a result, the corpus contains many examples of participants attempting to communicate quickly in specific game situations, and it also includes a variety of spontaneous conversational phenomena such as hesitations, filled pauses, overlapping speech, and low-latency responses. The corpus has been created to facilitate research in incremental speech processing for spoken dialogue systems. Potentially, the corpus could be used in several areas of speech and language research, including speech recognition, natural language understanding, natural language generation, and dialogue management.</abstract>
    </paper>
    <paper id="549">
      <author><first>Juan Rafael</first><last>Orozco-Arroyave</last></author>
      <author><first>Julián David</first><last>Arias-Londoño</last></author>
      <author><first>Jesús Francisco</first><last>Vargas-Bonilla</last></author>
      <author><first>María Claudia</first><last>González-Rátiva</last></author>
      <author><first>Elmar</first><last>Nöth</last></author>
      <title>New <fixed-case>S</fixed-case>panish speech corpus database for the analysis of people suffering from <fixed-case>P</fixed-case>arkinson’s disease</title>
      <pages>342–347</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/7_Paper.pdf</url>
      <abstract>Parkinsons disease (PD) is the second most prevalent neurodegenerative disorder after Alzheimer’s, affecting about 1% of the people older than 65 and about 89% of the people with PD develop different speech disorders. Different researchers are currently working in the analysis of speech of people with PD, including the study of different dimensions in speech such as phonation, articulation,prosody and intelligibility. The study of phonation and articulation has been addressed mainly considering sustained vowels; however, the analysis of prosody and intelligibility requires the inclusion of words, sentences and monologue. In this paper we present a new database with speech recordings of 50 patients with PD and their respective healthy controls, matched by age and gender. All of the participants are Spanish native speakers and the recordings were collected following a protocol that considers both technical requirements and several recommendations given by experts in linguistics, phoniatry and neurology. This corpus includes tasks such as sustained phonations of the vowels, diadochokinetic evaluation, 45 words, 10 sentences, a reading text and a monologue. The paper also includes results of the characterization of the Spanish vowels considering different measures used in other works to characterize different speech impairments.</abstract>
    </paper>
    <paper id="550">
      <author><first>Scott</first><last>Martens</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <title><fixed-case>T</fixed-case>homas <fixed-case>A</fixed-case>quinas in the <fixed-case>T</fixed-case>ü<fixed-case>NDRA</fixed-case>: Integrating the Index <fixed-case>T</fixed-case>homisticus Treebank into <fixed-case>CLARIN</fixed-case>-<fixed-case>D</fixed-case></title>
      <pages>767–774</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/70_Paper.pdf</url>
      <abstract>This paper describes the integration of the Index Thomisticus Treebank (IT-TB) into the web-based treebank search and visualization application TueNDRA (Tuebingen aNnotated Data Retrieval &amp; Analysis). TueNDRA was originally designed to provide access via the Internet to constituency treebanks and to tools for searching and visualizing them, as well as tabulating statistics about their contents. TueNDRA has now been extended to also provide full support for dependency treebanks with non-projective dependencies, in order to integrate the IT-TB and future treebanks with similar properties. These treebanks are queried using an adapted form of the TIGERSearch query language, which can search both hierarchical and sequential information in treebanks in a single query. As a web application, making the IT-TB accessible via TueNDRA makes the treebank and the tools to use of it available to a large community without having to distribute software and show users how to install it.</abstract>
    </paper>
    <paper id="551">
      <author><first>Peter</first><last>Anick</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <title>Identification of Technology Terms in Patents</title>
      <pages>2008–2014</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/701_Paper.pdf</url>
      <abstract>Natural language analysis of patents holds promise for the development of tools designed to assist analysts in the monitoring of emerging technologies. One component of such tools is the identification of technology terms. We describe an approach to the discovery of technology terms using supervised machine learning and evaluate its performance on subsets of patents in three languages: English, German, and Chinese.</abstract>
    </paper>
    <paper id="552">
      <author><first>Tomáš</first><last>Kliegr</last></author>
      <author><first>Ondřej</first><last>Zamazal</last></author>
      <title>Towards Linked Hypernyms Dataset 2.0: complementing <fixed-case>DB</fixed-case>pedia with hypernym discovery</title>
      <pages>3517–3523</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/703_Paper.pdf</url>
      <abstract>This paper presents a statistical type inference algorithm for ontology alignment, which assigns DBpedia entities with a new type (class). To infer types for a specific entity, the algorithm first identifies types that co-occur with the type the entity already has, and subsequently prunes the set of candidates for the most confident one. The algorithm has one parameter for balancing specificity/reliability of the resulting type selection. The proposed algorithm is used to complement the types in the LHD dataset, which is RDF knowledge base populated by identifying hypernyms from the free text of Wikipedia articles. The majority of types assigned to entities in LHD 1.0 are DBpedia resources. Through the statistical type inference, the number of entities with a type from DBpedia Ontology is increased significantly: by 750 thousand entities for the English dataset, 200.000 for Dutch and 440.000 for German. The accuracy of the inferred types is at 0.65 for English (as compared to 0.86 for LHD 1.0 types). A byproduct of the mapping process is a set of 11.000 mappings from DBpedia resources to DBpedia Ontology classes with associated confidence values. The number of the resulting mappings is an order of magnitude larger than what can be achieved with standard ontology alignment algorithms (Falcon, LogMapLt and YAM++), which do not utilize the type co-occurrence information. The presented algorithm is not restricted to the LHD dataset, it can be used to address generic type inference problems in presence of class membership information for a large number of instances.</abstract>
    </paper>
    <paper id="553">
      <author><first>Eduard</first><last>Bejček</last></author>
      <author><first>Václava</first><last>Kettnerová</last></author>
      <author><first>Markéta</first><last>Lopatková</last></author>
      <title>Automatic Mapping Lexical Resources: A Lexical Unit as the Keystone</title>
      <pages>2826–2832</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/704_Paper.pdf</url>
      <abstract>This paper presents the fully automatic linking of two valency lexicons of Czech verbs: VALLEX and PDT-VALLEX. Despite the same theoretical background adopted by these lexicons and the same linguistic phenomena they focus on, the fully automatic mapping of these resouces is not straightforward. We demonstrate that converting these lexicons into a common format represents a relatively easy part of the task whereas the automatic identification of pairs of corresponding valency frames (representing lexical units of verbs) poses difficulties. The overall achieved precision of 81% can be considered satisfactory. However, the higher number of lexical units a verb has, the lower the precision of their automatic mapping usually is. Moreover, we show that especially (i) supplementing further information on lexical units and (ii) revealing and reconciling regular discrepancies in their annotations can greatly assist in the automatic merging.</abstract>
    </paper>
    <paper id="554">
      <author><first>Kris</first><last>Heylen</last></author>
      <author><first>Stephen</first><last>Bond</last></author>
      <author><first>Dirk</first><last>De Hertog</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Hendrik</first><last>Kockaert</last></author>
      <title><fixed-case>T</fixed-case>erm<fixed-case>W</fixed-case>ise: A <fixed-case>CAT</fixed-case>-tool with Context-Sensitive Terminological Support.</title>
      <pages>4018–4022</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/706_Paper.pdf</url>
      <abstract>Increasingly, large bilingual document collections are being made available online, especially in the legal domain. This type of Big Data is a valuable resource that specialized translators exploit to search for informative examples of how domain-specific expressions should be translated. However, general purpose search engines are not optimized to retrieve previous translations that are maximally relevant to a translator. In this paper, we report on the TermWise project, a cooperation of terminologists, corpus linguists and computer scientists, that aims to leverage big online translation data for terminological support to legal translators at the Belgian Federal Ministry of Justice. The project developed dedicated knowledge extraction algorithms and a server-based tool to provide translators with the most relevant previous translations of domain-specific expressions relative to the current translation assignment. The functionality is implemented an extra database, a Term&amp;Phrase Memory, that is meant to be integrated with existing Computer Assisted Translation tools. In the paper, we give an overview of the system, give a demo of the user interface, we present a user-based evaluation by translators and discuss how the tool is part of the general evolution towards exploiting Big Data in translation.</abstract>
    </paper>
    <paper id="555">
      <author><first>Irina</first><last>Galinskaya</last></author>
      <author><first>Valentin</first><last>Gusev</last></author>
      <author><first>Elena</first><last>Mescheryakova</last></author>
      <author><first>Mariya</first><last>Shmatova</last></author>
      <title>Measuring the Impact of Spelling Errors on the Quality of Machine Translation</title>
      <pages>2683–2689</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/708_Paper.pdf</url>
      <abstract>In this paper we show how different types of spelling errors influence the quality of machine translation. We also propose a method to evaluate the impact of spelling errors correction on translation quality without expensive manual work of providing reference translations.</abstract>
    </paper>
    <paper id="556">
      <author><first>Sakriani</first><last>Sakti</last></author>
      <author><first>Keigo</first><last>Kubo</last></author>
      <author><first>Sho</first><last>Matsumiya</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Tomoki</first><last>Toda</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <author><first>Fumihiro</first><last>Adachi</last></author>
      <author><first>Ryosuke</first><last>Isotani</last></author>
      <title>Towards Multilingual Conversations in the Medical Domain: Development of Multilingual Medical Data and A Network-based <fixed-case>ASR</fixed-case> System</title>
      <pages>2639–2643</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/709_Paper.pdf</url>
      <abstract>This paper outlines the recent development on multilingual medical data and multilingual speech recognition system for network-based speech-to-speech translation in the medical domain. The overall speech-to-speech translation (S2ST) system was designed to translate spoken utterances from a given source language into a target language in order to facilitate multilingual conversations and reduce the problems caused by language barriers in medical situations. Our final system utilizes a weighted finite-state transducers with n-gram language models. Currently, the system successfully covers three languages: Japanese, English, and Chinese. The difficulties involved in connecting Japanese, English and Chinese speech recognition systems through Web servers will be discussed, and the experimental results in simulated medical conversation will also be presented.</abstract>
    </paper>
    <paper id="557">
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Roxane</first><last>Bertrand</last></author>
      <author><first>Mathilde</first><last>Guardiola</last></author>
      <title>Automatic detection of other-repetition occurrences: application to <fixed-case>F</fixed-case>rench conversational Speech</title>
      <pages>836–842</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/71_Paper.pdf</url>
      <abstract>This paper investigates the discursive phenomenon called other-repetitions (OR), particularly in the context of spontaneous French dialogues. It focuses on their automatic detection and characterization. A method is proposed to retrieve automatically OR: this detection is based on rules that are applied on the lexical material only. This automatic detection process has been used to label other-repetitions on 8 dialogues of CID - Corpus of Interactional Data. Evaluations performed on one speaker are good with a F1-measure of 0.85. Retrieved OR occurrences are then statistically described: number of words, distance, etc.</abstract>
    </paper>
    <paper id="558">
      <author><first>Andrej</first><last>Žgank</last></author>
      <author><first>Ana Zwitter</first><last>Vitez</last></author>
      <author><first>Darinka</first><last>Verdonik</last></author>
      <title>The <fixed-case>S</fixed-case>lovene <fixed-case>BNSI</fixed-case> Broadcast News database and reference speech corpus <fixed-case>GOS</fixed-case>: Towards the uniform guidelines for future work</title>
      <pages>2644–2647</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/710_Paper.pdf</url>
      <abstract>The aim of the paper is to search for common guidelines for the future development of speech databases for less resourced languages in order to make them the most useful for both main fields of their use, linguistic research and speech technologies. We compare two standards for creating speech databases, one followed when developing the Slovene speech database for automatic speech recognition ― BNSI Broadcast News, the other followed when developing the Slovene reference speech corpus GOS, and outline possible common guidelines for future work. We also present an add-on for the GOS corpus, which enables its usage for automatic speech recognition.</abstract>
    </paper>
    <paper id="559">
      <author><first>Roberto</first><last>Bartolini</last></author>
      <author><first>Valeria</first><last>Quochi</last></author>
      <author><first>Irene</first><last>De Felice</last></author>
      <author><first>Irene</first><last>Russo</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <title>From Synsets to Videos: Enriching <fixed-case>I</fixed-case>tal<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Multimodally</title>
      <pages>3110–3117</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/712_Paper.pdf</url>
      <abstract>The paper describes the multimodal enrichment of ItalWordNet action verbs entries by means of an automatic mapping with an ontology of action types instantiated by video scenes (ImagAct). The two resources present important differences as well as interesting complementary features, such that a mapping of these two resources can lead to a an enrichment of IWN, through the connection between synsets and videos apt to illustrate the meaning described by glosses. Here, we describe an approach inspired by ontology matching methods for the automatic mapping of ImagAct video scened onto ItalWordNet sense. The experiments described in the paper are conducted on Italian, but the same methodology can be extended to other languages for which WordNets have been created, since ImagAct is done also for English, Chinese and Spanish. This source of multimodal information can be exploited to design second language learning tools, as well as for language grounding in video action recognition and potentially for robotics.</abstract>
    </paper>
    <paper id="560">
      <author><first>Vidas</first><last>Daudaravičius</last></author>
      <title>Language Editing Dataset of Academic Texts</title>
      <pages>1738–1742</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/714_Paper.pdf</url>
      <abstract>We describe the VTeX Language Editing Dataset of Academic Texts (LEDAT), a dataset of text extracts from scientific papers that were edited by professional native English language editors at VTeX. The goal of the LEDAT is to provide a large data resource for the development of language evaluation and grammar error correction systems for the scientific community. We describe the data collection and the compilation process of the LEDAT. The new dataset can be used in many NLP studies and applications where deeper knowledge of the academic language and language editing is required. The dataset can be used also as a knowledge base of English academic language to support many writers of scientific papers.</abstract>
    </paper>
    <paper id="561">
      <author><first>Matti</first><last>Varjokallio</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <title>A Toolkit for Efficient Learning of Lexical Units for Speech Recognition</title>
      <pages>3072–3075</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/715_Paper.pdf</url>
      <abstract>String segmentation is an important and recurring problem in natural language processing and other domains. For morphologically rich languages, the amount of different word forms caused by morphological processes like agglutination, compounding and inflection, may be huge and causes problems for traditional word-based language modeling approach. Segmenting text into better modelable units is thus an important part of the modeling task. This work presents methods and a toolkit for learning segmentation models from text. The methods may be applied to lexical unit selection for speech recognition and also other segmentation tasks.</abstract>
    </paper>
    <paper id="562">
      <author><first>Yuichi</first><last>Ishimoto</last></author>
      <author><first>Tomoyuki</first><last>Tsuchiya</last></author>
      <author><first>Hanae</first><last>Koiso</last></author>
      <author><first>Yasuharu</first><last>Den</last></author>
      <title>Towards Automatic Transformation between Different Transcription Conventions: Prediction of Intonation Markers from Linguistic and Acoustic Features</title>
      <pages>311–315</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/716_Paper.pdf</url>
      <abstract>Because of the tremendous effort required for recording and transcription, large-scale spoken language corpora have been hardly developed in Japanese, with a notable exception of the Corpus of Spontaneous Japanese (CSJ). Various research groups have individually developed conversation corpora in Japanese, but these corpora are transcribed by different conventions and have few annotations in common, and some of them lack fundamental annotations, which are prerequisites for conversation research. To solve this situation by sharing existing conversation corpora that cover diverse styles and settings, we have tried to automatically transform a transcription made by one convention into that made by another convention. Using a conversation corpus transcribed in both the Conversation-Analysis-style (CA-style) and CSJ-style, we analyzed the correspondence between CA’s `intonation markers’ and CSJ’s `tone labels,’ and constructed a statistical model that converts tone labels into intonation markers with reference to linguistic and acoustic features of the speech. The result showed that there is considerable variance in intonation marking even between trained transcribers. The model predicted with 85% accuracy the presence of the intonation markers, and classified the types of the markers with 72% accuracy.</abstract>
    </paper>
    <paper id="563">
      <author><first>Hiroaki</first><last>Noguchi</last></author>
      <author><first>Yasuhiro</first><last>Katagiri</last></author>
      <author><first>Yasuharu</first><last>Den</last></author>
      <title><fixed-case>J</fixed-case>apanese conversation corpus for training and evaluation of backchannel prediction model.</title>
      <pages>4429–4433</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/717_Paper.pdf</url>
      <abstract>In this paper, we propose an experimental method for building a specialized corpus for training and evaluating backchannel prediction models of spoken dialogue. To develop a backchannel prediction model using a machine learning technique, it is necessary to discriminate between the timings of the interlocutor s speech when more listeners commonly respond with backchannels and the timings when fewer listeners do so. The proposed corpus indicates the normative timings for backchannels in each speech with millisecond accuracy. In the proposed method, we first extracted each speech comprising a single turn from recorded conversation. Second, we presented these speeches as stimuli to 89 participants and asked them to respond by key hitting whenever they thought it appropriate to respond with a backchannel. In this way, we collected 28983 responses. Third, we applied the Gaussian mixture model to the temporal distribution of the responses and estimated the center of Gaussian distribution, that is, the backchannel relevance place (BRP), in each case. Finally, we synthesized 10 pairs of stereo speech stimuli and asked 19 participants to rate each on a 7-point scale of naturalness. The results show that backchannels inserted at BRPs were significantly higher than those in the original condition.</abstract>
    </paper>
    <paper id="564">
      <author><first>Jan</first><last>Gorisch</last></author>
      <author><first>Corine</first><last>Astésano</last></author>
      <author><first>Ellen Gurman</first><last>Bard</last></author>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <title>Aix Map Task corpus: The <fixed-case>F</fixed-case>rench multimodal corpus of task-oriented dialogue</title>
      <pages>2648–2652</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/719_Paper.pdf</url>
      <abstract>This paper introduces the Aix Map Task corpus, a corpus of audio and video recordings of task-oriented dialogues. It was modelled after the original HCRC Map Task corpus. Lexical material was designed for the analysis of speech and prosody, as described in Astésano et al. (2007). The design of the lexical material, the protocol and some basic quantitative features of the existing corpus are presented. The corpus was collected under two communicative conditions, one audio-only condition and one face-to-face condition. The recordings took place in a studio and a sound attenuated booth respectively, with head-set microphones (and in the face-to-face condition with two video cameras). The recordings have been segmented into Inter-Pausal-Units and transcribed using transcription conventions containing actual productions and canonical forms of what was said. It is made publicly available online.</abstract>
    </paper>
    <paper id="565">
      <author><first>Heike</first><last>Zinsmeister</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Kathrin</first><last>Beck</last></author>
      <title>Adapting a part-of-speech tagset to non-standard text: The case of <fixed-case>STTS</fixed-case></title>
      <pages>4097–4104</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/721_Paper.pdf</url>
      <abstract>The Stuttgart-Tübingen TagSet (STTS) is a de-facto standard for the part-of-speech tagging of German texts. Since its first publication in 1995, STTS has been used in a variety of annotation projects, some of which have adapted the tagset slightly for their specific needs. Recently, the focus of many projects has shifted from the analysis of newspaper text to that of non-standard varieties such as user-generated content, historical texts, and learner language. These text types contain linguistic phenomena that are missing from or are only suboptimally covered by STTS; in a community effort, German NLP researchers have therefore proposed additions to and modifications of the tagset that will handle these phenomena more appropriately. In addition, they have discussed alternative ways of tag assignment in terms of bipartite tags (stem, token) for historical texts and tripartite tags (lexicon, morphology, distribution) for learner texts. In this article, we report on this ongoing activity, addressing methodological issues and discussing selected phenomena and their treatment in the tagset adaptation process.</abstract>
    </paper>
    <paper id="566">
      <author><first>Milan</first><last>Rusko</last></author>
      <author><first>Sakhia</first><last>Darjaa</last></author>
      <author><first>Marián</first><last>Trnka</last></author>
      <author><first>Marián</first><last>Ritomský</last></author>
      <author><first>Róbert</first><last>Sabo</last></author>
      <title>Alert!... Calm Down, There is Nothing to Worry About. Warning and Soothing Speech Synthesis.</title>
      <pages>1182–1187</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/722_Paper.pdf</url>
      <abstract>Presence of appropriate acoustic cues of affective features in the synthesized speech can be a prerequisite for the proper evaluation of the semantic content by the message recipient. In the recent work the authors have focused on the research of expressive speech synthesis capable of generating naturally sounding synthetic speech at various levels of arousal. The synthesizer should be able to produce speech in Slovak in different styles from extremely urgent warnings, insisting messages, alerts, through comments, and neutral style speech to soothing messages and very calm speech. A three-step method was used for recording both - the high-activation and low-activation expressive speech databases. The acoustic properties of the obtained databases are discussed. Several synthesizers with different levels of arousal were designed using these databases and their outputs are compared to the original voice of the voice talent. A possible ambiguity of acoustic cues is pointed out and the relevance of the semantic meaning of the sentences both in the sentence set for the speech database recording and in the set for subjective synthesizer testing is discussed.</abstract>
    </paper>
    <paper id="567">
      <author><first>Valia</first><last>Kordoni</last></author>
      <author><first>Iliana</first><last>Simova</last></author>
      <title>Multiword Expressions in Machine Translation</title>
      <pages>1208–1211</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/723_Paper.pdf</url>
      <abstract>This work describes an experimental evaluation of the significance of phrasal verb treatment for obtaining better quality statistical machine translation (SMT) results. The importance of the detection and special treatment of phrasal verbs is measured in the context of SMT, where the word-for-word translation of these units often produces incoherent results. Two ways of integrating phrasal verb information in a phrase-based SMT system are presented. Automatic and manual evaluations of the results reveal improvements in the translation quality in both experiments.</abstract>
    </paper>
    <paper id="568">
      <author><first>Christian</first><last>Girardi</last></author>
      <author><first>Manuela</first><last>Speranza</last></author>
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <title><fixed-case>CROMER</fixed-case>: a Tool for Cross-Document Event and Entity Coreference</title>
      <pages>3204–3208</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/726_Paper.pdf</url>
      <abstract>In this paper we present CROMER (CROss-document Main Events and entities Recognition), a novel tool to manually annotate event and entity coreference across clusters of documents. The tool has been developed so as to handle large collections of documents, perform collaborative annotation (several annotators can work on the same clusters), and enable the linking of the annotated data to external knowledge sources. Given the availability of semantic information encoded in Semantic Web resources, this tool is designed to support annotators in linking entities and events to DBPedia and Wikipedia, so as to facilitate the automatic retrieval of additional semantic information. In this way, event modelling and chaining is made easy, while guaranteeing the highest interconnection with external resources. For example, the tool can be easily linked to event models such as the Simple Event Model [Van Hage et al , 2011] and the Grounded Annotation Framework [Fokkens et al. 2013].</abstract>
    </paper>
    <paper id="569">
      <author><first>Tiberiu</first><last>Boroș</last></author>
      <author><first>Adriana</first><last>Stan</last></author>
      <author><first>Oliver</first><last>Watts</last></author>
      <author><first>Stefan Daniel</first><last>Dumitrescu</last></author>
      <title><fixed-case>RSS</fixed-case>-<fixed-case>TOBI</fixed-case> - A Prosodically Enhanced <fixed-case>R</fixed-case>omanian Speech Corpus</title>
      <pages>316–320</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/727_Paper.pdf</url>
      <abstract>This paper introduces a recent development of a Romanian Speech corpus to include prosodic annotations of the speech data in the form of ToBI labels. We describe the methodology of determining the required pitch patterns that are common for the Romanian language, annotate the speech resource, and then provide a comparison of two text-to-speech synthesis systems to establish the benefits of using this type of information to our speech resource. The result is a publicly available speech dataset which can be used to further develop speech synthesis systems or to automatically learn the prediction of ToBI labels from text in Romanian language.</abstract>
    </paper>
    <paper id="570">
      <author><first>Artūrs</first><last>Znotiņš</last></author>
      <author><first>Pēteris</first><last>Paikens</last></author>
      <title>Coreference Resolution for <fixed-case>L</fixed-case>atvian</title>
      <pages>3209–3213</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/729_Paper.pdf</url>
      <abstract>Coreference resolution (CR) is a current problem in natural language processing (NLP) research and it is a key task in applications such as question answering, text summarization and information extraction for which text understanding is of crucial importance. We describe an implementation of coreference resolution tools for Latvian language, developed as a part of a tool chain for newswire text analysis but usable also as a separate, publicly available module. LVCoref is a rule based CR system that uses entity centric model that encourages the sharing of information across all mentions that point to the same real-world entity. The system is developed to provide starting ground for further experiments and generate a reference baseline to be compared with more advanced rule-based and machine learning based future coreference resolvers. It now reaches 66.6 F-score using predicted mentions and 78.1% F-score using gold mentions. This paper describes current efforts to create a CR system and to improve NER performance for Latvian. Task also includes creation of the corpus of manually annotated coreference relations.</abstract>
    </paper>
    <paper id="571">
      <author><first>Elena</first><last>Mitocariu</last></author>
      <author><first>Daniel</first><last>Anechitei</last></author>
      <author><first>Dan</first><last>Cristea</last></author>
      <title>How Could Veins Speed Up The Process Of Discourse Parsing</title>
      <pages>2871–2878</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/73_Paper.pdf</url>
      <abstract>In this paper we propose a method of reducing the search space of a discourse parsing process, while keeping unaffected its capacity to generate cohesive and coherent tree structures. The parsing method uses Veins Theory (VT), by developing incrementally a forest of parallel discourse trees, evaluating them on cohesion and coherence criteria and keeping only the most promising structures to go on with at each step. The incremental development is constrained by two general principles, well known in discourse parsing: sequentiality of the terminal nodes and attachment restricted to the right frontier. A set of formulas rooted on VT helps to guess the most promising nodes of the right frontier where an attachment can be made, thus avoiding an exhaustive generation of the whole search space and in the same time maximizing the coherence of the discourse structures. We report good results of applying this approach, representing a significant improvement in discourse parsing process.</abstract>
    </paper>
    <paper id="572">
      <author><first>Nitsan</first><last>Chrizman</last></author>
      <author><first>Alon</first><last>Itai</last></author>
      <title>How to construct a multi-lingual domain ontology</title>
      <pages>4345–4350</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/730_Paper.pdf</url>
      <abstract>The research focuses on automatic construction of multi-lingual domain-ontologies, i.e., creating a DAG (directed acyclic graph) consisting of concepts relating to a specific domain and the relations between them. The domain example on which the research performed is “Organized Crime”. The contribution of the work is the investigation of and comparison between several data sources and methods to create multi-lingual ontologies. The first subtask was to extract the domain’s concepts. The best source turned out to be Wikepedias articles that are under the catgegory. The second task was to create an English ontology, i.e., the relationships between the concepts. Again the relationships between concepts and the hierarchy were derived from Wikipedia. The final task was to create an ontology for a language with far fewer resources (Hebrew). The task was accomplished by deriving the concepts from the Hebrew Wikepedia and assessing their relevance and the relationships between them from the English ontology.</abstract>
    </paper>
    <paper id="573">
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <title>Automatic language identity tagging on word and sentence-level in multilingual text sources: a case-study on <fixed-case>L</fixed-case>uxembourgish</title>
      <pages>3300–3304</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/732_Paper.pdf</url>
      <abstract>Luxembourgish, embedded in a multilingual context on the divide between Romance and Germanic cultures, remains one of Europe’s under-described languages. This is due to the fact that the written production remains relatively low, and linguistic knowledge and resources, such as lexica and pronunciation dictionaries, are sparse. The speakers or writers will frequently switch between Luxembourgish, German, and French, on a per-sentence basis, as well as on a sub-sentence level. In order to build resources like lexicons, and especially pronunciation lexicons, or language models needed for natural language processing tasks such as automatic speech recognition, language used in text corpora should be identified. In this paper, we present the design of a manually annotated corpus of mixed language sentences as well as the tools used to select these sentences. This corpus of difficult sentences was used to test a word-based language identification system. This language identification system was used to select textual data extracted from the web, in order to build a lexicon and language models. This lexicon and language model were used in an Automatic Speech Recognition system for the Luxembourgish language which obtain a 25\% WER on the Quaero development data.</abstract>
    </paper>
    <paper id="574">
      <author><first>Orphée</first><last>De Clercq</last></author>
      <author><first>Sarah</first><last>Schulz</last></author>
      <author><first>Bart</first><last>Desmet</last></author>
      <author><first>Véronique</first><last>Hoste</last></author>
      <title>Towards Shared Datasets for Normalization Research</title>
      <pages>1218–1223</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/733_Paper.pdf</url>
      <abstract>In this paper we present a Dutch and English dataset that can serve as a gold standard for evaluating text normalization approaches. With the combination of text messages, message board posts and tweets, these datasets represent a variety of user generated content. All data was manually normalized to their standard form using newly-developed guidelines. We perform automatic lexical normalization experiments on these datasets using statistical machine translation techniques. We focus on both the word and character level and find that we can improve the BLEU score with ca. 20% for both languages. In order for this user generated content data to be released publicly to the research community some issues first need to be resolved. These are discussed in closer detail by focussing on the current legislation and by investigating previous similar data collection projects. With this discussion we hope to shed some light on various difficulties researchers are facing when trying to share social media data.</abstract>
    </paper>
    <paper id="575">
      <author><first>Nicolas</first><last>Pécheux</last></author>
      <author><first>Alexander</first><last>Allauzen</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <title>Rule-based Reordering Space in Statistical Machine Translation</title>
      <pages>1800–1806</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/735_Paper.pdf</url>
      <abstract>In Statistical Machine Translation (SMT), the constraints on word reorderings have a great impact on the set of potential translations that are explored. Notwithstanding computationnal issues, the reordering space of a SMT system needs to be designed with great care: if a larger search space is likely to yield better translations, it may also lead to more decoding errors, because of the added ambiguity and the interaction with the pruning strategy. In this paper, we study this trade-off using a state-of-the art translation system, where all reorderings are represented in a word lattice prior to decoding. This allows us to directly explore and compare different reordering spaces. We study in detail a rule-based preordering system, varying the length or number of rules, the tagset used, as well as contrasting with oracle settings and purely combinatorial subsets of permutations. We focus on two language pairs: English-French, a close language pair and English-German, known to be a more challenging reordering pair.</abstract>
    </paper>
    <paper id="576">
      <author><first>Luis Javier</first><last>Rodríguez-Fuentes</last></author>
      <author><first>Mikel</first><last>Penagarikano</last></author>
      <author><first>Amparo</first><last>Varona</last></author>
      <author><first>Mireia</first><last>Diez</last></author>
      <author><first>Germán</first><last>Bordel</last></author>
      <title><fixed-case>KALAKA</fixed-case>-3: a database for the recognition of spoken <fixed-case>E</fixed-case>uropean languages on <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube audios</title>
      <pages>443–449</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/736_Paper.pdf</url>
      <abstract>This paper describes the main features of KALAKA-3, a speech database specifically designed for the development and evaluation of language recognition systems. The database provides TV broadcast speech for training, and audio data extracted from YouTube videos for tuning and testing. The database was created to support the Albayzin 2012 Language Recognition Evaluation, which featured two language recognition tasks, both dealing with European languages. The first one involved six target languages (Basque, Catalan, English, Galician, Portuguese and Spanish) for which there was plenty of training data, whereas the second one involved four target languages (French, German, Greek and Italian) for which no training data was provided. Two separate sets of YouTube audio files were provided to test the performance of language recognition systems on both tasks. To allow open-set tests, these datasets included speech in 11 additional (Out-Of-Set) European languages. The paper also presents a summary of the results attained in the evaluation, along with the performance of state-of-the-art systems on the four evaluation tracks defined on the database, which demonstrates the extreme difficulty of some of them. As far as we know, this is the first database specifically designed to benchmark spoken language recognition technology on YouTube audios.</abstract>
    </paper>
    <paper id="577">
      <author><first>Andrew</first><last>Gargett</last></author>
      <author><first>John</first><last>Barnden</last></author>
      <title>Mining Online Discussion Forums for Metaphors</title>
      <pages>2507–2512</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/737_Paper.pdf</url>
      <abstract>We present an approach to mining online forums for figurative language such as metaphor. We target in particular online discussions within the illness and the political conflict domains, with a view to constructing corpora of Metaphor in Illness Discussion, andMetaphor in Political Conflict Discussion. This paper reports on our ongoing efforts to combine manual and automatic detection strategies for labelling the corpora, and present some initial results from our work showing that metaphor use is not independent of illness domain.</abstract>
    </paper>
    <paper id="578">
      <author><first>Theodosia</first><last>Togia</last></author>
      <author><first>Ann</first><last>Copestake</last></author>
      <title><fixed-case>T</fixed-case>ag<fixed-case>NT</fixed-case>ext: A parallel corpus for the induction of resource-specific non-taxonomical relations from tagged images</title>
      <pages>3448–3455</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/738_Paper.pdf</url>
      <abstract>When producing textual descriptions, humans express propositions regarding an object; but what do they express when annotating a document with simple tags? To answer this question, we have studied what users of tagging systems would have said if they were to describe a resource with fully fledged text. In particular, our work attempts to answer the following questions: if users were to use full descriptions, would their current tags be words present in these hypothetical sentences? If yes, what kind of language would connect these words? Such questions, although central to the problem of extracting binary relations between tags, have been sidestepped in the existing literature, which has focused on a small subset of possible inter-tag relations, namely hierarchical ones (e.g. “car” --is-a-- “vehicle”), as opposed to non-taxonomical relations (e.g. “woman” --wears-- “hat”). TagNText is the first attempt to construct a parallel corpus of tags and textual descriptions with respect to particular resources. The corpus provides enough data for the researcher to gain an insight into the nature of underlying relations, as well as the tools and methodology for constructing larger-scale parallel corpora that can aid non-taxonomical relation extraction.</abstract>
    </paper>
    <paper id="579">
      <author><first>Carmen</first><last>García-Mateo</last></author>
      <author><first>Antonio</first><last>Cardenal</last></author>
      <author><first>Xosé Luis</first><last>Regueira</last></author>
      <author><first>Elisa Fernández</first><last>Rei</last></author>
      <author><first>Marta</first><last>Martinez</last></author>
      <author><first>Roberto</first><last>Seara</last></author>
      <author><first>Rocío</first><last>Varela</last></author>
      <author><first>Noemí</first><last>Basanta</last></author>
      <title><fixed-case>CORILGA</fixed-case>: a <fixed-case>G</fixed-case>alician Multilevel Annotated Speech Corpus for Linguistic Analysis</title>
      <pages>2653–2657</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/739_Paper.pdf</url>
      <abstract>This paper describes the CORILGA (Corpus Oral Informatizado da Lingua Galega). CORILGA is a large high-quality corpus of spoken Galician from the 1960s up to present-day, including both formal and informal spoken language from both standard and non-standard varieties, and across different generations and social levels. The corpus will be available to the research community upon completion. Galician is one of the EU languages that needs further research before highly effective language technology solutions can be implemented. A software repository for speech resources in Galician is also described. The repository includes a structured database, a graphical interface and processing tools. The use of a database enables to perform search in a simple and fast way based in a number of different criteria. The web-based user interface facilitates users the access to the different materials. Last but not least a set of transcription-based modules for automatic speech recognition has been developed, thus facilitating the orthographic labelling of the recordings.</abstract>
    </paper>
    <paper id="580">
      <author><first>Akira</first><last>Fujita</last></author>
      <author><first>Akihiro</first><last>Kameda</last></author>
      <author><first>Ai</first><last>Kawazoe</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <title>Overview of <fixed-case>T</fixed-case>odai Robot Project and Evaluation Framework of its <fixed-case>NLP</fixed-case>-based Problem Solving</title>
      <pages>2590–2597</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/74_Paper.pdf</url>
      <abstract>We introduce the organization of the Todai Robot Project and discuss its achievements. The Todai Robot Project task focuses on benchmarking NLP systems for problem solving. This task encourages NLP-based systems to solve real high-school examinations. We describe the details of the method to manage question resources and their correct answers, answering tools and participation by researchers in the task. We also analyse the answering accuracy of the developed systems by comparing the systems answers with answers given by human test-takers.</abstract>
    </paper>
    <paper id="581">
      <author><first>Demulier</first><last>Virginie</last></author>
      <author><first>Elisabetta</first><last>Bevacqua</last></author>
      <author><first>Florian</first><last>Focone</last></author>
      <author><first>Tom</first><last>Giraud</last></author>
      <author><first>Pamela</first><last>Carreno</last></author>
      <author><first>Brice</first><last>Isableu</last></author>
      <author><first>Sylvie</first><last>Gibet</last></author>
      <author><first>Pierre</first><last>De Loor</last></author>
      <author><first>Jean-Claude</first><last>Martin</last></author>
      <title>A Database of Full Body Virtual Interactions Annotated with Expressivity Scores</title>
      <pages>3505–3510</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/741_Paper.pdf</url>
      <abstract>Recent technologies enable the exploitation of full body expressions in applications such as interactive arts but are still limited in terms of dyadic subtle interaction patterns. Our project aims at full body expressive interactions between a user and an autonomous virtual agent. The currently available databases do not contain full body expressivity and interaction patterns via avatars. In this paper, we describe a protocol defined to collect a database to study expressive full-body dyadic interactions. We detail the coding scheme for manually annotating the collected videos. Reliability measures for global annotations of expressivity and interaction are also provided.</abstract>
    </paper>
    <paper id="582">
      <author><first>Piotr</first><last>Bański</last></author>
      <author><first>Nils</first><last>Diewald</last></author>
      <author><first>Michael</first><last>Hanl</last></author>
      <author><first>Marc</first><last>Kupietz</last></author>
      <author><first>Andreas</first><last>Witt</last></author>
      <title>Access control by query rewriting: the case of <fixed-case>K</fixed-case>or<fixed-case>AP</fixed-case></title>
      <pages>3817–3822</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/743_Paper.pdf</url>
      <abstract>We present an approach to an aspect of managing complex access scenarios to large and heterogeneous corpora that involves handling user queries that, intentionally or due to the complexity of the queried resource, target texts or annotations outside of the given users permissions. We first outline the overall architecture of the corpus analysis platform KorAP, devoting some attention to the way in which it handles multiple query languages, by implementing ISO CQLF (Corpus Query Lingua Franca), which in turn constitutes a component crucial for the functionality discussed here. Next, we look at query rewriting as it is used by KorAP and zoom in on one kind of this procedure, namely the rewriting of queries that is forced by data access restrictions.</abstract>
    </paper>
    <paper id="583">
      <author><first>Igor</first><last>Odriozola</last></author>
      <author><first>Inma</first><last>Hernaez</last></author>
      <author><first>María Inés</first><last>Torres</last></author>
      <author><first>Luis Javier</first><last>Rodriguez-Fuentes</last></author>
      <author><first>Mikel</first><last>Penagarikano</last></author>
      <author><first>Eva</first><last>Navas</last></author>
      <title><fixed-case>B</fixed-case>asque Speecon-like and <fixed-case>B</fixed-case>asque <fixed-case>S</fixed-case>peech<fixed-case>D</fixed-case>at <fixed-case>MDB</fixed-case>-600: speech databases for the development of <fixed-case>ASR</fixed-case> technology for <fixed-case>B</fixed-case>asque</title>
      <pages>2658–2665</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/744_Paper.pdf</url>
      <abstract>This paper introduces two databases specifically designed for the development of ASR technology for the Basque language: the Basque Speecon-like database and the Basque SpeechDat MDB-600 database. The former was recorded in an office environment according to the Speecon specifications, whereas the later was recorded through mobile telephones according to the SpeechDat specifications. Both databases were created under an initiative that the Basque Government started in 2005, a program called ADITU, which aimed at developing speech technologies for Basque. The databases belong to the Basque Government. A comprehensive description of both databases is provided in this work, highlighting the differences with regard to their corresponding standard specifications. The paper also presents several initial experimental results for both databases with the purpose of validating their usefulness for the development of speech recognition technology. Several applications already developed with the Basque Speecon-like database are also described. Authors aim to make these databases widely known to the community as well, and foster their use by other groups.</abstract>
    </paper>
    <paper id="584">
      <author><first>Andrew</first><last>Gargett</last></author>
      <author><first>Sam</first><last>Hellmuth</last></author>
      <author><first>Ghazi</first><last>AlGethami</last></author>
      <title><fixed-case>D</fixed-case>i<fixed-case>VE</fixed-case>-<fixed-case>A</fixed-case>rabic: <fixed-case>G</fixed-case>ulf <fixed-case>A</fixed-case>rabic Dialogue in a Virtual Environment</title>
      <pages>4434–4439</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/745_Paper.pdf</url>
      <abstract>Documentation of communicative behaviour across languages seems at a crossroads. While methods for collecting data on spoken or written communication, backed up by computational techniques, are evolving, the actual data being collected remain largely the same. Inspired by the efforts of some innovative researchers who are directly tackling the various obstacles to investigating language in the field (e.g. see various papers collected in Enfield &amp; Stivers 2007), we report here about ongoing work to solve the general problem of collecting in situ data for situated linguistic interaction. The initial stages of this project have involved employing a portable format designed to increase range and flexibility of doing such collections in the field. Our motivation is to combine this with a parallel data set for a typologically distinct language, in order to contribute a parallel corpus of situated language use.</abstract>
    </paper>
    <paper id="585">
      <author><first>Coline</first><last>Claude-Lachenaud</last></author>
      <author><first>Éric</first><last>Charton</last></author>
      <author><first>Benoît</first><last>Ozell</last></author>
      <author><first>Michel</first><last>Gagnon</last></author>
      <title>A multimodal interpreter for 3<fixed-case>D</fixed-case> visualization and animation of verbal concepts</title>
      <pages>3620–3627</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/747_Paper.pdf</url>
      <abstract>We present an algorithm intended to visually represent the sense of verb related to an object described in a text sequence, as a movement in 3D space. We describe a specific semantic analyzer, based on a standard verbal ontology, dedicated to the interpretation of action verbs as spatial actions. Using this analyzer, our system build a generic 3D graphical path for verbal concepts allowing space representation, listed as SelfMotion concepts in the FrameNet ontology project. The object movement is build by first extracting the words and enriching them with the semantic analyzer. Then, weight tables, necessary to obtain characteristics values (orientation, shape, trajectory...) for the verb are used in order to get a 3D path, as realist as possible. The weight tables were created to make parallel between features defined for SelfMotion verbal concept (some provided by FrameNet, other determined during the project) and values used in the final algorithm used to create 3D moving representations from input text. We evaluate our analyzer on a corpus of short sentences and presents our results.</abstract>
    </paper>
    <paper id="586">
      <author><first>Tobias</first><last>Bocklet</last></author>
      <author><first>Andreas</first><last>Maier</last></author>
      <author><first>Korbinian</first><last>Riedhammer</last></author>
      <author><first>Ulrich</first><last>Eysholdt</last></author>
      <author><first>Elmar</first><last>Nöth</last></author>
      <title>Erlangen-<fixed-case>CLP</fixed-case>: A Large Annotated Corpus of Speech from Children with Cleft Lip and Palate</title>
      <pages>2671–2674</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/748_Paper.pdf</url>
      <abstract>In this paper we describe Erlangen-CLP, a large speech database of children with Cleft Lip and Palate. More than 800 German children with CLP (most of them between 4 and 18 years old) and 380 age matched control speakers spoke the semi-standardized PLAKSS test that consists of words with all German phonemes in different positions. So far 250 CLP speakers were manually transcribed, 120 of these were analyzed by a speech therapist and 27 of them by four additional therapists. The tharapists marked 6 different processes/criteria like pharyngeal backing and hypernasality which typically occur in speech of people with CLP. We present detailed statistics about the the marked processes and the inter-rater agreement.</abstract>
    </paper>
    <paper id="587">
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <author><first>Fabien</first><last>Gandon</last></author>
      <title>Classifying Inconsistencies in <fixed-case>DB</fixed-case>pedia Language Specific Chapters</title>
      <pages>1443–1450</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/750_Paper.pdf</url>
      <abstract>This paper proposes a methodology to identify and classify the semantic relations holding among the possible different answers obtained for a certain query on DBpedia language specific chapters. The goal is to reconcile information provided by language specific DBpedia chapters to obtain a consistent results set. Starting from the identified semantic relations between two pieces of information, we further classify them as positive or negative, and we exploit bipolar abstract argumentation to represent the result set as a unique graph, where using argumentation semantics we are able to detect the (possible multiple) consistent sets of elements of the query result. We experimented with the proposed methodology over a sample of triples extracted from 10 DBpedia ontology properties. We define the LingRel ontology to represent how the extracted information from different chapters is related to each other, and we map the properties of the LingRel ontology to the properties of the SIOC-Argumentation ontology to built argumentation graphs. The result is a pilot resource that can be profitably used both to train and to evaluate NLP applications querying linked data in detecting the semantic relations among the extracted values, in order to output consistent information sets.</abstract>
    </paper>
    <paper id="588">
      <author><first>Anindya</first><last>Roy</last></author>
      <author><first>Camille</first><last>Guinaudeau</last></author>
      <author><first>Hervé</first><last>Bredin</last></author>
      <author><first>Claude</first><last>Barras</last></author>
      <title><fixed-case>TVD</fixed-case>: A Reproducible and Multiply Aligned <fixed-case>TV</fixed-case> Series Dataset</title>
      <pages>418–425</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/751_Paper.pdf</url>
      <abstract>We introduce a new dataset built around two TV series from different genres, The Big Bang Theory, a situation comedy and Game of Thrones, a fantasy drama. The dataset has multiple tracks extracted from diverse sources, including dialogue (manual and automatic transcripts, multilingual subtitles), crowd-sourced textual descriptions (brief episode summaries, longer episode outlines) and various metadata (speakers, shots, scenes). The paper describes the dataset and provide tools to reproduce it for research purposes provided one has legally acquired the DVD set of the series. Tools are also provided to temporally align a major subset of dialogue and description tracks, in order to combine complementary information present in these tracks for enhanced accessibility. For alignment, we consider tracks as comparable corpora and first apply an existing algorithm for aligning such corpora based on dynamic time warping and TFIDF-based similarity scores. We improve this baseline algorithm using contextual information, WordNet-based word similarity and scene location information. We report the performance of these algorithms on a manually aligned subset of the data. To highlight the interest of the database, we report a use case involving rich speech retrieval and propose other uses.</abstract>
    </paper>
    <paper id="589">
      <author><first>Cédric</first><last>Lopez</last></author>
      <author><first>Reda</first><last>Bestandji</last></author>
      <author><first>Mathieu</first><last>Roche</last></author>
      <author><first>Rachel</first><last>Panckhurst</last></author>
      <title>Towards Electronic <fixed-case>SMS</fixed-case> Dictionary Construction: An Alignment-based Approach</title>
      <pages>2833–2838</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/753_Paper.pdf</url>
      <abstract>In this paper, we propose a method for aligning text messages (entitled AlignSMS) in order to automatically build an SMS dictionary. An extract of 100 text messages from the 88milSMS corpus (Panckhurst el al., 2013, 2014) was used as an initial test. More than 90,000 authentic text messages in French were collected from the general public by a group of academics in the south of France in the context of the sud4science project (http://www.sud4science.org). This project is itself part of a vast international SMS data collection project, entitled sms4science (http://www.sms4science.org, Fairon et al. 2006, Cougnon, 2014). After corpus collation, pre-processing and anonymisation (Accorsi et al., 2012, Patel et al., 2013), we discuss how raw anonymised text messages can be transcoded into normalised text messages, using a statistical alignment method. The future objective is to set up a hybrid (symbolic/statistic) approach based on both grammar rules and our statistical AlignSMS method.</abstract>
    </paper>
    <paper id="590">
      <author><first>Olivier</first><last>Ferret</last></author>
      <title>Compounds and distributional thesauri</title>
      <pages>2979–2984</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/754_Paper.pdf</url>
      <abstract>The building of distributional thesauri from corpora is a problem that was the focus of a significant number of articles, starting with (Grefenstette, 1994} and followed by (Lin, 1998}, (Curran and Moens, 2002) or (Heylen and Peirsman, 2007). However, in all these cases, only single terms were considered. More recently, the topic of compositionality in the framework of distributional semantic representations has come to the surface and was investigated for building the semantic representation of phrases or even sentences from the representation of their words. However, this work was not done until now with the objective of building distributional thesauri. In this article, we investigate the impact of the introduction of compounds for achieving such building. More precisely, we consider compounds as undividable lexical units and evaluate their influence according to three different roles: as features in the distributional contexts of single terms, as possible neighbors of single term entries and finally, as entries of a thesaurus. This investigation was conducted through an intrinsic evaluation for a large set of nominal English single terms and compounds with various frequencies.</abstract>
    </paper>
    <paper id="591">
      <author><first>Antonio</first><last>Balvet</last></author>
      <author><first>Dejan</first><last>Stosic</last></author>
      <author><first>Aleksandra</first><last>Miletic</last></author>
      <title><fixed-case>TALC</fixed-case>-sef A Manually-Revised <fixed-case>POS</fixed-case>-<fixed-case>TA</fixed-case>gged Literary Corpus in <fixed-case>S</fixed-case>erbian, <fixed-case>E</fixed-case>nglish and <fixed-case>F</fixed-case>rench</title>
      <pages>4105–4110</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/755_Paper.pdf</url>
      <abstract>In this paper, we present a parallel literary corpus for Serbian, English and French, the TALC-sef corpus. The corpus includes a manually-revised pos-tagged reference Serbian corpus of over 150,000 words. The initial objective was to devise a reference parallel corpus in the three languages, both for literary and linguistic studies. The French and English sub-corpora had been pos-tagged from the onset, using TreeTagger (Schmid, 1994), but the corpus lacked, until now, a tagged version of the Serbian sub-corpus. Here, we present the original parallel literary corpus, then we address issues related to pos-tagging a large collection of Serbian text: from the conception of an appropriate tagset for Serbian, to the choice of an automatic pos-tagger adapted to the task, and then to some quantitative and qualitative results. We then move on to a discussion of perspectives in the near future for further annotations of the whole parallel corpus.</abstract>
    </paper>
    <paper id="592">
      <author><first>Shinsuke</first><last>Goto</last></author>
      <author><first>Donghui</first><last>Lin</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <title>Crowdsourcing for Evaluating Machine Translation Quality</title>
      <pages>3456–3463</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/756_Paper.pdf</url>
      <abstract>The recent popularity of machine translation has increased the demand for the evaluation of translations. However, the traditional evaluation approach, manual checking by a bilingual professional, is too expensive and too slow. In this study, we confirm the feasibility of crowdsourcing by analyzing the accuracy of crowdsourcing translation evaluations. We compare crowdsourcing scores to professional scores with regard to three metrics: translation-score, sentence-score, and system-score. A Chinese to English translation evaluation task was designed using around the NTCIR-9 PATENT parallel corpus with the goal being 5-range evaluations of adequacy and fluency. The experiment shows that the average score of crowdsource workers well matches professional evaluation results. The system-score comparison strongly indicates that crowdsourcing can be used to find the best translation system given the input of 10 source sentence.</abstract>
    </paper>
    <paper id="593">
      <author><first>Billy T.M.</first><last>Wong</last></author>
      <author><first>Ian C.</first><last>Chow</last></author>
      <author><first>Jonathan J.</first><last>Webster</last></author>
      <author><first>Hengbin</first><last>Yan</last></author>
      <title>The Halliday Centre Tagger: An Online Platform for Semi-automatic Text Annotation and Analysis</title>
      <pages>1664–1667</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/758_Paper.pdf</url>
      <abstract>This paper reports the latest development of The Halliday Centre Tagger (the Tagger), an online platform provided with semi-automatic features to facilitate text annotation and analysis. The Tagger is featured for its web-based architecture with all functionalities and file storage space provided online, and a theory-neutral design where users can define their own labels for annotating various kinds of linguistic information. The Tagger is currently optimized for text annotation of Systemic Functional Grammar (SFG), providing by default a pre-defined set of SFG grammatical features, and the function of automatic identification of process types for English verbs. Apart from annotation, the Tagger also offers the features of visualization and summarization to aid text analysis. The visualization feature combines and illustrates multi-dimensional layers of annotation in a unified way of presentation, while the summarization feature categorizes annotated entries according to different SFG systems, i.e., transitivity, theme, logical-semantic relations, etc. Such features help users identify grammatical patterns in an annotated text.</abstract>
    </paper>
    <paper id="594">
      <author><first>Shinsuke</first><last>Mori</last></author>
      <author><first>Hirokuni</first><last>Maeta</last></author>
      <author><first>Yoko</first><last>Yamakata</last></author>
      <author><first>Tetsuro</first><last>Sasada</last></author>
      <title>Flow Graph Corpus from Recipe Texts</title>
      <pages>2370–2377</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/763_Paper.pdf</url>
      <abstract>In this paper, we present our attempt at annotating procedural texts with a flow graph as a representation of understanding. The domain we focus on is cooking recipe. The flow graphs are directed acyclic graphs with a special root node corresponding to the final dish. The vertex labels are recipe named entities, such as foods, tools, cooking actions, etc. The arc labels denote relationships among them. We converted 266 Japanese recipe texts into flow graphs manually. 200 recipes are randomly selected from a web site and 66 are of the same dish. We detail the annotation framework and report some statistics on our corpus. The most typical usage of our corpus may be automatic conversion from texts to flow graphs which can be seen as an entire understanding of procedural texts. With our corpus, one can also try word segmentation, named entity recognition, predicate-argument structure analysis, and coreference resolution.</abstract>
    </paper>
    <paper id="595">
      <author><first>Johannes</first><last>Kirschnick</last></author>
      <author><first>Alan</first><last>Akbik</last></author>
      <author><first>Holmer</first><last>Hemsen</last></author>
      <title><fixed-case>F</fixed-case>reepal: A Large Collection of Deep Lexico-Syntactic Patterns for Relation Extraction</title>
      <pages>2071–2075</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/764_Paper.pdf</url>
      <abstract>The increasing availability and maturity of both scalable computing architectures and deep syntactic parsers is opening up new possibilities for Relation Extraction (RE) on large corpora of natural language text. In this paper, we present Freepal, a resource designed to assist with the creation of relation extractors for more than 5,000 relations defined in the Freebase knowledge base (KB). The resource consists of over 10 million distinct lexico-syntactic patterns extracted from dependency trees, each of which is assigned to one or more Freebase relations with different confidence strengths. We generate the resource by executing a large-scale distant supervision approach on the ClueWeb09 corpus to extract and parse over 260 million sentences labeled with Freebase entities and relations. We make Freepal freely available to the research community, and present a web demonstrator to the dataset, accessible from free-pal.appspot.com.</abstract>
    </paper>
    <paper id="596">
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Marie-Amélie</first><last>Botalla</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <title>Correcting and Validating Syntactic Dependency in the Spoken <fixed-case>F</fixed-case>rench Treebank Rhapsodie</title>
      <pages>2320–2325</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/766_Paper.pdf</url>
      <abstract>This article presents the methods, results, and precision of the syntactic annotation process of the Rhapsodie Treebank of spoken French. The Rhapsodie Treebank is an 33,000 word corpus annotated for prosody and syntax, licensed in its entirety under Creative Commons. The syntactic annotation contains two levels: a macro-syntactic level, containing a segmentation into illocutionary units (including discourse markers, parentheses â¦) and a micro-syntactic level including dependency relations and various paradigmatic structures, called pile constructions, the latter being particularly frequent and diverse in spoken language. The micro-syntactic annotation process, presented in this paper, includes a semi-automatic preparation of the transcription, the application of a syntactic dependency parser, transcoding of the parsing results to the Rhapsodie annotation scheme, manual correction by multiple annotators followed by a validation process, and finally the application of coherence rules that check common errors. The good inter-annotator agreement scores are presented and analyzed in greater detail. The article also includes the list of functions used in the dependency annotation and for the distinction of various pile constructions and presents the ideas underlying these choices.</abstract>
    </paper>
    <paper id="597">
      <author><first>Marcin</first><last>Woliński</last></author>
      <title>Morfeusz Reloaded</title>
      <pages>1106–1111</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/768_Paper.pdf</url>
      <abstract>The paper presents recent developments in Morfeusz ― a morphological analyser for Polish. The program, being already a fundamental resource for processing Polish, has been reimplemented with some important changes in the tagset, some new options, added information on proper names, and ability to perform simple prefix derivation. The present version of Morfeusz (including its dictionaries) is made available under the very liberal 2-clause BSD license. The program can be downloaded from http://sgjp.pl/morfeusz/.</abstract>
    </paper>
    <paper id="598">
      <author><first>Mauro</first><last>Dragoni</last></author>
      <author><first>Alessio</first><last>Bosca</last></author>
      <author><first>Matteo</first><last>Casu</last></author>
      <author><first>Andi</first><last>Rexha</last></author>
      <title>Modeling, Managing, Exposing, and Linking Ontologies with a <fixed-case>W</fixed-case>iki-based Tool</title>
      <pages>1668–1675</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/769_Paper.pdf</url>
      <abstract>In the last decade, the need of having effective and useful tools for the creation and the management of linguistic resources significantly increased. One of the main reasons is the necessity of building linguistic resources (LRs) that, besides the goal of expressing effectively the domain that users want to model, may be exploited in several ways. In this paper we present a wiki-based collaborative tool for modeling ontologies, and more in general any kind of linguistic resources, called MoKi. This tool has been customized in the context of an EU-funded project for addressing three important aspects of LRs modeling: (i) the exposure of the created LRs, (ii) for providing features for linking the created resources to external ones, and (iii) for producing multilingual LRs in a safe manner.</abstract>
    </paper>
    <paper id="599">
      <author><first>Kasia</first><last>Budzynska</last></author>
      <author><first>Mathilde</first><last>Janier</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <author><first>Patrick</first><last>Saint-Dizier</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <author><first>Olena</first><last>Yakorska</last></author>
      <title>A Model for Processing Illocutionary Structures and Argumentation in Debates</title>
      <pages>917–924</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/77_Paper.pdf</url>
      <abstract>In this paper, we briefly present the objectives of Inference Anchoring Theory (IAT) and the formal structure which is proposed for dialogues. Then, we introduce our development corpus, and a computational model designed for the identification of discourse minimal units in the context of argumentation and the illocutionary force associated with each unit. We show the categories of resources which are needed and how they can be reused in different contexts.</abstract>
    </paper>
    <paper id="600">
      <author><first>Deryle</first><last>Lonsdale</last></author>
      <author><first>Benjamin</first><last>Millard</last></author>
      <title>Student achievement and <fixed-case>F</fixed-case>rench sentence repetition test scores</title>
      <pages>2719–2725</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/770_Paper.pdf</url>
      <abstract>Sentence repetition (SR) tests are one way of probing a language learner’s oral proficiency. Test-takers listen to a set of carefully engineered sentences of varying complexity one-by-one, and then try to repeat them back as exactly as possible. In this paper we explore how well an SR test that we have developed for French corresponds with the test-taker’s achievement levels, represented by proficiency interview scores and by college class enrollment. We describe how we developed our SR test items using various language resources, and present pertinent facts about the test administration. The responses were scored by humans and also by a specially designed automatic speech recognition (ASR) engine; we sketch both scoring approaches. Results are evaluated in several ways: correlations between human and ASR scores, item response analysis to quantify the relative difficulty of the items, and criterion-referenced analysis setting thresholds of consistency across proficiency levels. We discuss several observations and conclusions prompted by the analyses, and suggestions for future work.</abstract>
    </paper>
    <paper id="601">
      <author><first>Daniel</first><last>Luzzati</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <author><first>Eric</first><last>Bilinski</last></author>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Juliette</first><last>Kahn</last></author>
      <author><first>Carole</first><last>Lailler</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <title>Human annotation of <fixed-case>ASR</fixed-case> error regions: Is “gravity” a sharable concept for human annotators?</title>
      <pages>3050–3056</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/771_Paper.pdf</url>
      <abstract>This paper is concerned with human assessments of the severity of errors in ASR outputs. We did not design any guidelines so that each annotator involved in the study could consider the “seriousness” of an ASR error using their own scientific background. Eight human annotators were involved in an annotation task on three distinct corpora, one of the corpora being annotated twice, hiding this annotation in duplicate to the annotators. None of the computed results (inter-annotator agreement, edit distance, majority annotation) allow any strong correlation between the considered criteria and the level of seriousness to be shown, which underlines the difficulty for a human to determine whether a ASR error is serious or not.</abstract>
    </paper>
    <paper id="602">
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Luka</first><last>Nerima</last></author>
      <author><first>Lorenza</first><last>Russo</last></author>
      <author><first>Maria</first><last>Ivanova</last></author>
      <author><first>Eric</first><last>Wehrli</last></author>
      <title><fixed-case>S</fixed-case>wiss<fixed-case>A</fixed-case>dmin: A multilingual tagged parallel corpus of press releases</title>
      <pages>1832–1836</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/772_Paper.pdf</url>
      <abstract>SwissAdmin is a new multilingual corpus of press releases from the Swiss Federal Administration, available in German, French, Italian and English. We provide SwissAdmin in three versions: (i) plain texts of approximately 6 to 8 million words per language; (ii) sentence-aligned bilingual texts for each language pair; (iii) a part-of-speech-tagged version consisting of annotations in both the Universal tagset and the richer Fips tagset, along with grammatical functions, verb valencies and collocations. The SwissAdmin corpus is freely available at www.latl.unige.ch/swissadmin.</abstract>
    </paper>
    <paper id="603">
      <author><first>Anna</first><last>Vernerová</last></author>
      <author><first>Václava</first><last>Kettnerová</last></author>
      <author><first>Markéta</first><last>Lopatková</last></author>
      <title>To Pay or to Get Paid: Enriching a Valency Lexicon with Diatheses</title>
      <pages>2452–2459</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/773_Paper.pdf</url>
      <abstract>Valency lexicons typically describe only unmarked usages of verbs (the active form); however, verbs prototypically enter different surface structures. In this paper, we focus on the so-called diatheses, i.e., the relations between different surface syntactic manifestations of verbs that are brought about by changes in the morphological category of voice, e.g., the passive diathesis. The change in voice of a verb is prototypically associated with shifts of some of its valency complementations in the surface structure. These shifts are implied by changes in morphemic forms of the involved valency complementations and are regular enough to be captured by syntactic rules. However, as diatheses are lexically conditioned, their applicability to an individual lexical unit of a verb is not predictable from its valency frame alone. In this work, we propose a representation of this linguistic phenomenon in a valency lexicon of Czech verbs, VALLEX, with the aim to enhance this lexicon with the information on individual types of Czech diatheses. In order to reduce the amount of necessary manual annotation, a semi-automatic method is developed. This method draws evidence from a large morphologically annotated corpus, relying on grammatical constraints on the applicability of individual types of diatheses.</abstract>
    </paper>
    <paper id="604">
      <author><first>Liang</first><last>Tian</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <author><first>Paulo</first><last>Quaresma</last></author>
      <author><first>Francisco</first><last>Oliveira</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Shuo</first><last>Li</last></author>
      <author><first>Yiming</first><last>Wang</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <title><fixed-case>UM</fixed-case>-Corpus: A Large <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>hinese Parallel Corpus for Statistical Machine Translation</title>
      <pages>1837–1842</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/774_Paper.pdf</url>
      <abstract>Parallel corpus is a valuable resource for cross-language information retrieval and data-driven natural language processing systems, especially for Statistical Machine Translation (SMT). However, most existing parallel corpora to Chinese are subject to in-house use, while others are domain specific and limited in size. To a certain degree, this limits the SMT research. This paper describes the acquisition of a large scale and high quality parallel corpora for English and Chinese. The corpora constructed in this paper contain about 15 million English-Chinese (E-C) parallel sentences, and more than 2 million training data and 5,000 testing sentences are made publicly available. Different from previous work, the corpus is designed to embrace eight different domains. Some of them are further categorized into different topics. The corpus will be released to the research community, which is available at the NLP2CT website.</abstract>
    </paper>
    <paper id="605">
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>Josu</first><last>Bermudez</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <title><fixed-case>IXA</fixed-case> pipeline: Efficient and Ready to Use Multilingual <fixed-case>NLP</fixed-case> tools</title>
      <pages>3823–3828</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/775_Paper.pdf</url>
      <abstract>IXA pipeline is a modular set of Natural Language Processing tools (or pipes) which provide easy access to NLP technology. It offers robust and efficient linguistic annotation to both researchers and non-NLP experts with the aim of lowering the barriers of using NLP technology either for research purposes or for small industrial developers and SMEs. IXA pipeline can be used “as is” or exploit its modularity to pick and change different components. Given its open-source nature, it can also be modified and extended for it to work with other languages. This paper describes the general data-centric architecture of IXA pipeline and presents competitive results in several NLP annotations for English and Spanish.</abstract>
    </paper>
    <paper id="606">
      <author><first>Suguru</first><last>Matsuyoshi</last></author>
      <author><first>Ryo</first><last>Otsuki</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <title>Annotating the Focus of Negation in <fixed-case>J</fixed-case>apanese Text</title>
      <pages>1743–1750</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/777_Paper.pdf</url>
      <abstract>This paper proposes an annotation scheme for the focus of negation in Japanese text. Negation has its scope and the focus within the scope. The scope of negation is the part of the sentence that is negated; the focus is the part of the scope that is most prominently or explicitly negated. In natural language processing, correct interpretation of negated statements requires precise detection of the focus of negation in the statements. As a foundation for developing a negation focus detector for Japanese, we have annotated textdata of “Rakuten Travel: User review data” and the newspaper subcorpus of the “Balanced Corpus of Contemporary Written Japanese” with labels proposed in our annotation scheme. We report 1,327 negation cues and the foci in the corpora, and present classification of these foci based on syntactic types and semantic types. We also propose a system for detecting the focus of negation in Japanese using 16 heuristic rules and report the performance of the system.</abstract>
    </paper>
    <paper id="607">
      <author><first>Mohamed</first><last>Sherif</last></author>
      <author><first>Sandro</first><last>Coelho</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <author><first>Sebastian</first><last>Hellmann</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Martin</first><last>Brümmer</last></author>
      <author><first>Andreas</first><last>Both</last></author>
      <title><fixed-case>NIF</fixed-case>4<fixed-case>OGGD</fixed-case> - <fixed-case>NLP</fixed-case> Interchange Format for Open <fixed-case>G</fixed-case>erman Governmental Data</title>
      <pages>3524–3528</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/780_Paper.pdf</url>
      <abstract>In the last couple of years the amount of structured open government data has increased significantly. Already now, citizens are able to leverage the advantages of open data through increased transparency and better opportunities to take part in governmental decision making processes. Our approach increases the interoperability of existing but distributed open governmental datasets by converting them to the RDF-based NLP Interchange Format (NIF). Furthermore, we integrate the converted data into a geodata store and present a user interface for querying this data via a keyword-based search. The language resource generated in this project is publicly available for download and also via a dedicated SPARQL endpoint.</abstract>
    </paper>
    <paper id="608">
      <author><first>Alessio</first><last>Bosca</last></author>
      <author><first>Matteo</first><last>Casu</last></author>
      <author><first>Matteo</first><last>Dragoni</last></author>
      <author><first>Nikolaos</first><last>Marianos</last></author>
      <title>A Gold Standard for <fixed-case>CLIR</fixed-case> evaluation in the Organic Agriculture Domain</title>
      <pages>3667–3670</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/781_Paper.pdf</url>
      <abstract>We present a gold standard for the evaluation of Cross Language Information Retrieval systems in the domain of Organic Agriculture and AgroEcology. The presented resource is free to use for research purposes and it includes a collection of multilingual documents annotated with respect to a domain ontology, the ontology used for annotating the resources, a set of 48 queries in 12 languages and a gold standard with the correct resources for the proposed queries. The goal of this work consists in contributing to the research community with a resource for evaluating multilingual retrieval algorithms, with particular focus on domain adaptation strategies for general purpose multilingual information retrieval systems and on the effective exploitation of semantic annotations. Domain adaptation is in fact an important activity for tuning the retrieval system, reducing the ambiguities and improving the precision of information retrieval. Domain ontologies constitute a diffuse practice for defining the conceptual space of a corpus and mapping resources to specific topics and in our lab we propose as well to investigate and evaluate the impact of this information in enhancing the retrieval of contents. An initial experiment is described, giving a baseline for further research with the proposed gold standard.</abstract>
    </paper>
    <paper id="609">
      <author><first>Senka</first><last>Drobac</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <author><first>Tommi</first><last>Pirinen</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <title>Heuristic Hyper-minimization of Finite State Lexicons</title>
      <pages>3319–3324</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/784_Paper.pdf</url>
      <abstract>Flag diacritics, which are special multi-character symbols executed at runtime, enable optimising finite-state networks by combining identical sub-graphs of its transition graph. Traditionally, the feature has required linguists to devise the optimisations to the graph by hand alongside the morphological description. In this paper, we present a novel method for discovering flag positions in morphological lexicons automatically, based on the morpheme structure implicit in the language description. With this approach, we have gained significant decrease in the size of finite-state networks while maintaining reasonable application speed. The algorithm can be applied to any language description, where the biggest achievements are expected in large and complex morphologies. The most noticeable reduction in size we got with a morphological transducer for Greenlandic, whose original size is on average about 15 times larger than other morphologies. With the presented hyper-minimization method, the transducer is reduced to 10,1% of the original size, with lookup speed decreased only by 9,5%.</abstract>
    </paper>
    <paper id="610">
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Harris</first><last>Papageorgiou</last></author>
      <author><first>Christian</first><last>Spurk</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Riccardo</first><last>del Gratta</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <author><first>Christian</first><last>Girardi</last></author>
      <title><fixed-case>META</fixed-case>-<fixed-case>SHARE</fixed-case>: One year after</title>
      <pages>1532–1538</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/786_Paper.pdf</url>
      <abstract>This paper presents META-SHARE (www.meta-share.eu), an open language resource infrastructure, and its usage since its Europe-wide deployment in early 2013. META-SHARE is a network of repositories that store language resources (data, tools and processing services) documented with high-quality metadata, aggregated in central inventories allowing for uniform search and access. META-SHARE was developed by META-NET (www.meta-net.eu) and aims to serve as an important component of a language technology marketplace for researchers, developers, professionals and industrial players, catering for the full development cycle of language technology, from research through to innovative products and services. The observed usage in its initial steps, the steadily increasing number of network nodes, resources, users, queries, views and downloads are all encouraging and considered as supportive of the choices made so far. In tandem, take-up activities like direct linking and processing of datasets by language processing services as well as metadata transformation to RDF are expected to open new avenues for data and resources linking and boost the organic growth of the infrastructure while facilitating language technology deployment by much wider research communities and industrial sectors.</abstract>
    </paper>
    <paper id="611">
      <author><first>Claudia</first><last>Baur</last></author>
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Nikos</first><last>Tsourakis</last></author>
      <title>Using a Serious Game to Collect a Child Learner Speech Corpus</title>
      <pages>2726–2732</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/787_Paper.pdf</url>
      <abstract>We present an English-L2 child learner speech corpus, produced by 14 year old Swiss German-L1 students in their third year of learning English, which is currently in the process of being collected. The collection method uses a web-enabled multimodal language game implemented using the CALL-SLT platform, in which subjects hold prompted conversations with an animated agent. Prompts consist of a short animated Engligh-language video clip together with a German-language piece of text indicating the semantic content of the requested response. Grammar-based speech understanding is used to decide whether responses are accepted or rejected, and dialogue flow is controlled using a simple XML-based scripting language; the scripts are written to allow multiple dialogue paths, the choice being made randomly. The system is gamified using a score-and-badge framework with four levels of badges. We describe the application, the data collection and annotation procedures, and the initial tranche of data. The full corpus, when complete, should contain at least 5,000 annotated utterances.</abstract>
    </paper>
    <paper id="612">
      <author><first>Riccardo</first><last>Del Gratta</last></author>
      <author><first>Gabriella</first><last>Pardelli</last></author>
      <author><first>Sara</first><last>Goggi</last></author>
      <title>The <fixed-case>LRE</fixed-case> Map disclosed</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/788_Paper.pdf</url>
      <abstract>This paper describes a serialization of the LRE Map database according to the RDF model. Due to the peculiar nature of the LRE Map, many ontologies are necessary to model the map in RDF, including newly created and reused ontologies. The importance of having the LRE Map in RDF and its connections to other open resources is also addressed.</abstract>
    </paper>
    <paper id="613">
      <author><first>Evgeny</first><last>Stepanov</last></author>
      <author><first>Giuseppe</first><last>Riccardi</last></author>
      <author><first>Ali Orkan</first><last>Bayer</last></author>
      <title>The Development of the Multilingual <fixed-case>LUNA</fixed-case> Corpus for Spoken Language System Porting</title>
      <pages>2675–2678</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/789_Paper.pdf</url>
      <abstract>The development of annotated corpora is a critical process in the development of speech applications for multiple target languages. While the technology to develop a monolingual speech application has reached satisfactory results (in terms of performance and effort), porting an existing application from a source language to a target language is still a very expensive task. In this paper we address the problem of creating multilingual aligned corpora and its evaluation in the context of a spoken language understanding (SLU) porting task. We discuss the challenges of the manual creation of multilingual corpora, as well as present the algorithms for the creation of multilingual SLU via Statistical Machine Translation (SMT).</abstract>
    </paper>
    <paper id="614">
      <author><first>Magdaléna</first><last>Rysová</last></author>
      <title>Verbs of Saying with a Textual Connecting Function in the <fixed-case>P</fixed-case>rague Discourse Treebank</title>
      <pages>930–935</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/79_Paper.pdf</url>
      <abstract>The paper tries to contribute to the general discussion on discourse connectives, concretely to the question whether it is meaningful to distinguish two separate groups of connectives ― i.e. “classical” connectives limited to few predefined classes like conjunctions or adverbs (e.g. “but”) vs. alternative lexicalizations of connectives (i.e. unrestricted expressions and phrases like “the reason is”, “he added”, “the condition was” etc.). In this respect, the paper focuses on one group of these broader connectives in Czech ― the selected verbs of saying “doplnit/doplňovat” (“to complement”), “upřesnit/upřesňovat” (“to specify”), “dodat/dodávat” (“to add”), “pokračovat” (“to continue”) ― and analyses their occurrence and function in texts from the Prague Discourse Treebank. The paper demonstrates that these verbs of saying have a special place within the other connectives, as they contain two items ― e.g. “he added” means “and he said” so the verb “to add” contains an information about the relation to the previous context (“and”) plus the verb of saying (“to say”). This information led us to a more general observation, i.e. discourse connectives in broader sense do not necessarily connect two pieces of a text but some of them carry the second argument right in their semantics, which “classical” connectives can never do.</abstract>
    </paper>
    <paper id="615">
      <author><first>Marc</first><last>Poch</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Sergio</first><last>Espeja</last></author>
      <author><first>Felipe</first><last>Navío</last></author>
      <title>Ranking Job Offers for Candidates: learning hidden knowledge from Big Data</title>
      <pages>2076–2082</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/791_Paper.pdf</url>
      <abstract>This paper presents a system for suggesting a ranked list of appropriate vacancy descriptions to job seekers in a job board web site. In particular our work has explored the use of supervised classifiers with the objective of learning implicit relations which cannot be found with similarity or pattern based search methods that rely only on explicit information. Skills, names of professions and degrees, among other examples, are expressed in different languages, showing high variation and the use of ad-hoc resources to trace the relations is very costly. This implicit information is unveiled when a candidate applies for a job and therefore it is information that can be used for learning a model to predict new cases. The results of our experiments, which combine different clustering, classification and ranking methods, show the validity of the approach.</abstract>
    </paper>
    <paper id="616">
      <author><first>Valérie</first><last>Hanoka</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <title>An Open-Source Heavily Multilingual Translation Graph Extracted from Wiktionaries and Parallel Corpora</title>
      <pages>3179–3186</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/792_Paper.pdf</url>
      <abstract>This paper describes YaMTG (Yet another Multilingual Translation Graph), a new open-source heavily multilingual translation database (over 664 languages represented) built using several sources, namely various wiktionaries and the OPUS parallel corpora (Tiedemann, 2009). We detail the translation extraction process for 21 wiktionary language editions, and provide an evaluation of the translations contained in YaMTG.</abstract>
    </paper>
    <paper id="617">
      <author><first>Claudia</first><last>Borg</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <title>Crowd-sourcing evaluation of automatically acquired, morphologically related word groupings</title>
      <pages>3325–3332</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/793_Paper.pdf</url>
      <abstract>The automatic discovery and clustering of morphologically related words is an important problem with several practical applications. This paper describes the evaluation of word clusters carried out through crowd-sourcing techniques for the Maltese language. The hybrid (Semitic-Romance) nature of Maltese morphology, together with the fact that no large-scale lexical resources are available for Maltese, make this an interesting and challenging problem.</abstract>
    </paper>
    <paper id="618">
      <author><first>Auður</first><last>Hauksdóttir</last></author>
      <title>An Innovative World Language Centre : Challenges for the Use of Language Technology</title>
      <pages>2194–2198</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/795_Paper.pdf</url>
      <abstract>The Vigdis International Centre of Multilingualism and Intercultural Understanding at the University of Iceland will work under the auspices of UNESCO. The main objective of the Centre is to promote linguistic diversity and to raise awareness of the importance of multilingualism. The focus will be on research on translations, foreign language learning, language policy and language planning. The centre will also serve as a platform for promoting collaborative activities on languages and cultures, in particular, organizing exhibitions and other events aimed at both the academic community and the general public. The Centre will work in close collaboration with the national and international research community. The Centre aims to create state-of-the-art infrastructure, using Language Technology resources in research and academic studies, in particular in translations and language learning (Computer-Assisted Language Learning). In addition, the centre will provide scholars with a means to conduct corpus-based research for synchronic investigations and for comparative studies. The Centre will also function as a repository for language data corpora. Facilities will be provided so that these corpora can be used by the research community on site and online. Computer technology resources will also be exploited in creating tools and exhibitions for the general audience.</abstract>
    </paper>
    <paper id="619">
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <title>A language-independent and fully unsupervised approach to lexicon induction and part-of-speech tagging for closely related languages</title>
      <pages>502–508</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/797_Paper.pdf</url>
      <abstract>In this paper, we describe our generic approach for transferring part-of-speech annotations from a resourced language towards an etymologically closely related non-resourced language, without using any bilingual (i.e., parallel) data. We first induce a translation lexicon from monolingual corpora, based on cognate detection followed by cross-lingual contextual similarity. Second, POS information is transferred from the resourced language along translation pairs to the non-resourced language and used for tagging the corpus. We evaluate our methods on three language families, consisting of five Romance languages, three Germanic languages and five Slavic languages. We obtain tagging accuracies of up to 91.6%.</abstract>
    </paper>
    <paper id="620">
      <author><first>David</first><last>Tavarez</last></author>
      <author><first>Eva</first><last>Navas</last></author>
      <author><first>Daniel</first><last>Erro</last></author>
      <author><first>Ibon</first><last>Saratxaga</last></author>
      <author><first>Inma</first><last>Hernaez</last></author>
      <title>New bilingual speech databases for audio diarization</title>
      <pages>2666–2670</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/799_Paper.pdf</url>
      <abstract>This paper describes the process of collecting and recording two new bilingual speech databases in Spanish and Basque. They are designed primarily for speaker diarization in two different application domains: broadcast news audio and recorded meetings. First, both databases have been manually segmented. Next, several diarization experiments have been carried out in order to evaluate them. Our baseline speaker diarization system has been applied to both databases with around 30% of DER for broadcast news audio and 40% of DER for recorded meetings. Also, the behavior of the system when different languages are used by the same speaker has been tested.</abstract>
    </paper>
    <paper id="621">
      <author><first>Mohamed</first><last>Morchid</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <author><first>Georges</first><last>Linarès</last></author>
      <title>A <fixed-case>LDA</fixed-case>-Based Topic Classification Approach From Highly Imperfect Automatic Transcriptions</title>
      <pages>1309–1314</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/8_Paper.pdf</url>
      <abstract>Although the current transcription systems could achieve high recognition performance, they still have a lot of difficulties to transcribe speech in very noisy environments. The transcription quality has a direct impact on classification tasks using text features. In this paper, we propose to identify themes of telephone conversation services with the classical Term Frequency-Inverse Document Frequency using Gini purity criteria (TF-IDF-Gini) method and with a Latent Dirichlet Allocation (LDA) approach. These approaches are coupled with a Support Vector Machine (SVM) classification to resolve theme identification problem. Results show the effectiveness of the proposed LDA-based method compared to the classical TF-IDF-Gini approach in the context of highly imperfect automatic transcriptions. Finally, we discuss the impact of discriminative and non-discriminative words extracted by both methods in terms of transcription accuracy.</abstract>
    </paper>
    <paper id="622">
      <author><first>Cristina Sánchez</first><last>Marco</last></author>
      <title>An open source part-of-speech tagger for <fixed-case>N</fixed-case>orwegian: Building on existing language resources</title>
      <pages>4111–4117</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/801_Paper.pdf</url>
      <abstract>This paper presents an open source part-of-speech tagger for the Norwegian language. It describes how an existing language processing library (FreeLing) was used to build a new part-of-speech tagger for this language. This part-of-speech tagger has been built on already available resources, in particular a Norwegian dictionary and gold standard corpus, which were partly customized for the purposes of this paper. The results of a careful evaluation show that this tagger yields an accuracy close to state-of-the-art taggers for other languages.</abstract>
    </paper>
    <paper id="623">
      <author><first>Ahmet</first><last>Aker</last></author>
      <author><first>Monica</first><last>Paramita</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Bilingual dictionaries for all <fixed-case>EU</fixed-case> languages</title>
      <pages>2839–2845</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/803_Paper.pdf</url>
      <abstract>Bilingual dictionaries can be automatically generated using the GIZA++ tool. However, these dictionaries contain a lot of noise, because of which the quality of outputs of tools relying on the dictionaries are negatively affected. In this work we present three different methods for cleaning noise from automatically generated bilingual dictionaries: LLR, pivot and translation based approach. We have applied these approaches on the GIZA++ dictionaries -- dictionaries covering official EU languages -- in order to remove noise. Our evaluation showed that all methods help to reduce noise. However, the best performance is achieved using the transliteration based approach. We provide all bilingual dictionaries (the original GIZA++ dictionaries and the cleaned ones) free for download. We also provide the cleaning tools and scripts for free download.</abstract>
    </paper>
    <paper id="624">
      <author><first>Martin</first><last>Reynaert</last></author>
      <title>Synergy of Nederlab and</title>
      <pages>1224–1230</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/804_Paper.pdf</url>
      <abstract>In two concurrent projects in the Netherlands we are further developing TICCL or Text-Induced Corpus Clean-up. In project Nederlab TICCL is set to work on diachronic Dutch text. To this end it has been equipped with the largest diachronic lexicon and a historical name list developed at the Institute for Dutch Lexicology or INL. In project @PhilosTEI TICCL will be set to work on a fair range of European languages. We present a new implementation in C++ of the system which has been tailored to be easily adaptable to different languages. We further revisit prior work on diachronic Portuguese in which it was compared to VARD2 which had been manually adapted to Portuguese. This tested the new mechanisms for ranking correction candidates we have devised. We then move to evaluating the new TICCL port on a very large corpus of Dutch books known as EDBO, digitized by the Dutch National Library. The results show that TICCL scales to the largest corpus sizes and performs excellently raising the quality of the Gold Standard EDBO book by about 20% to 95% word accuracy. Simultaneous unsupervised post-correction of 10,000 digitized books is now a real option.</abstract>
    </paper>
    <paper id="625">
      <author><first>Raphael</first><last>Rubino</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <title>Quality Estimation for Synthetic Parallel Data Generation</title>
      <pages>1843–1849</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/807_Paper.pdf</url>
      <abstract>This paper presents a novel approach for parallel data generation using machine translation and quality estimation. Our study focuses on pivot-based machine translation from English to Croatian through Slovene. We generate an English―Croatian version of the Europarl parallel corpus based on the English―Slovene Europarl corpus and the Apertium rule-based translation system for Slovene―Croatian. These experiments are to be considered as a first step towards the generation of reliable synthetic parallel data for under-resourced languages. We first collect small amounts of aligned parallel data for the Slovene―Croatian language pair in order to build a quality estimation system for sentence-level Translation Edit Rate (TER) estimation. We then infer TER scores on automatically translated Slovene to Croatian sentences and use the best translations to build an English―Croatian statistical MT system. We show significant improvement in terms of automatic metrics obtained on two test sets using our approach compared to a random selection of synthetic parallel data.</abstract>
    </paper>
    <paper id="626">
      <author><first>Janine</first><last>Pimentel</last></author>
      <title>Adding a Third Language to a Lexical Resource Describing Legal Terminology: the assignment of equivalents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/808_Paper.pdf</url>
    </paper>
    <paper id="627">
      <author><first>Wolfgang</first><last>Seeker</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>An Out-of-Domain Test Suite for Dependency Parsing of <fixed-case>G</fixed-case>erman</title>
      <pages>4066–4073</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/809_Paper.pdf</url>
      <abstract>We present a dependency conversion of five German test sets from five different genres. The dependency representation is made as similar as possible to the dependency representation of TiGer, one of the two big syntactic treebanks of German. The purpose of these test sets is to enable researchers to test dependency parsing models on several different data sets from different text genres. We discuss some easy to compute statistics to demonstrate the variation and differences in the test sets and provide some baseline experiments where we test the effect of additional lexical knowledge on the out-of-domain performance of two state-of-the-art dependency parsers. Finally, we demonstrate with three small experiments that text normalization may be an important step in the standard processing pipeline when applied in an out-of-domain setting.</abstract>
    </paper>
    <paper id="628">
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Francesco</first><last>Cecconi</last></author>
      <author><first>Daniele</first><last>Vannella</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <title>Representing Multilingual Data as Linked Data: the Case of <fixed-case>B</fixed-case>abel<fixed-case>N</fixed-case>et 2.0</title>
      <pages>401–408</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/810_Paper.pdf</url>
      <abstract>Recent years have witnessed a surge in the amount of semantic information published on the Web. Indeed, the Web of Data, a subset of the Semantic Web, has been increasing steadily in both volume and variety, transforming the Web into a ‘global database’ in which resources are linked across sites. Linguistic fields -- in a broad sense -- have not been left behind, and we observe a similar trend with the growth of linguistic data collections on the so-called ‘Linguistic Linked Open Data (LLOD) cloud’. While both Semantic Web and Natural Language Processing communities can obviously take advantage of this growing and distributed linguistic knowledge base, they are today faced with a new challenge, i.e., that of facilitating multilingual access to the Web of data. In this paper we present the publication of BabelNet 2.0, a wide-coverage multilingual encyclopedic dictionary and ontology, as Linked Data. The conversion made use of lemon, a lexicon model for ontologies particularly well-suited for this enterprise. The result is an interlinked multilingual (lexical) resource which can not only be accessed on the LOD, but also be used to enrich existing datasets with linguistic information, or to support the process of mapping datasets across languages.</abstract>
    </paper>
    <paper id="629">
      <author><first>George</first><last>Kiomourtzis</last></author>
      <author><first>George</first><last>Giannakopoulos</last></author>
      <author><first>Georgios</first><last>Petasis</last></author>
      <author><first>Pythagoras</first><last>Karampiperis</last></author>
      <author><first>Vangelis</first><last>Karkaletsis</last></author>
      <title><fixed-case>NOMAD</fixed-case>: Linguistic Resources and Tools Aimed at Policy Formulation and Validation</title>
      <pages>3464–3470</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/813_Paper.pdf</url>
      <abstract>The NOMAD project (Policy Formulation and Validation through non Moderated Crowd-sourcing) is a project that supports policy making, by providing rich, actionable information related to how citizens perceive different policies. NOMAD automatically analyzes citizen contributions to the informal web (e.g. forums, social networks, blogs, newsgroups and wikis) using a variety of tools. These tools comprise text retrieval, topic classification, argument detection and sentiment analysis, as well as argument summarization. NOMAD provides decision-makers with a full arsenal of solutions starting from describing a domain and a policy to applying content search and acquisition, categorization and visualization. These solutions work in a collaborative manner in the policy-making arena. NOMAD, thus, embeds editing, analysis and visualization technologies into a concrete framework, applicable in a variety of policy-making and decision support settings In this paper we provide an overview of the linguistic tools and resources of NOMAD.</abstract>
    </paper>
    <paper id="630">
      <author><first>Lina</first><last>Henriksen</last></author>
      <author><first>Dorte Haltrup</first><last>Hansen</last></author>
      <author><first>Bente</first><last>Maegaard</last></author>
      <author><first>Bolette Sandford</first><last>Pedersen</last></author>
      <author><first>Claus</first><last>Povlsen</last></author>
      <title>Encompassing a spectrum of <fixed-case>LT</fixed-case> users in the <fixed-case>CLARIN</fixed-case>-<fixed-case>DK</fixed-case> Infrastructure</title>
      <pages>2175–2181</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/814_Paper.pdf</url>
      <abstract>CLARIN-DK is a platform with language resources constituting the Danish part of the European infrastructure CLARIN ERIC. Unlike some other language based infrastructures CLARIN-DK is not solely a repository for upload and storage of data, but also a platform of web services permitting the user to process data in various ways. This involves considerable complications in relation to workflow requirements. The CLARIN-DK interface must guide the user to perform the necessary steps of a workflow; even when the user is inexperienced and perhaps has an unclear conception of the requested results. This paper describes a user driven approach to creating a user interface specification for CLARIN-DK. We indicate how different user profiles determined different crucial interface design options. We also describe some use cases established in order to give illustrative examples of how the platform may facilitate research.</abstract>
    </paper>
    <paper id="631">
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Thierry</first><last>Bazillon</last></author>
      <author><first>Jose</first><last>Deulofeu</last></author>
      <author><first>Andre</first><last>Valli</last></author>
      <title>Automatically enriching spoken corpora with syntactic information for linguistic studies</title>
      <pages>854–858</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/816_Paper.pdf</url>
      <abstract>Syntactic parsing of speech transcriptions faces the problem of the presence of disfluencies that break the syntactic structure of the utterances. We propose in this paper two solutions to this problem. The first one relies on a disfluencies predictor that detects disfluencies and removes them prior to parsing. The second one integrates the disfluencies in the syntactic structure of the utterances and train a disfluencies aware parser.</abstract>
    </paper>
    <paper id="632">
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <title>Propa-<fixed-case>L</fixed-case>: a semantic filtering service from a lexical network created using Games With A Purpose</title>
      <pages>1676–1681</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/817_Paper.pdf</url>
      <abstract>This article presents Propa-L, a freely accessible Web service that allows to semantically filter a lexical network. The language resources behind the service are dynamic and created through Games With A Purpose. We show an example of application of this service: the generation of a list of keywords for parental filtering on the Web, but many others can be envisaged. Moreover, the propagation algorithm we present here can be applied to any lexical network, in any language.</abstract>
    </paper>
    <paper id="633">
      <author><first>Maria</first><last>Simi</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <title>Less is More? Towards a Reduced Inventory of Categories for Training a Parser for the <fixed-case>I</fixed-case>talian <fixed-case>S</fixed-case>tanford Dependencies</title>
      <pages>83–90</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf</url>
      <abstract>Stanford Dependencies (SD) represent nowadays a de facto standard as far as dependency annotation is concerned. The goal of this paper is to explore pros and cons of different strategies for generating SD annotated Italian texts to enrich the existing Italian Stanford Dependency Treebank (ISDT). This is done by comparing the performance of a statistical parser (DeSR) trained on a simpler resource (the augmented version of the Merged Italian Dependency Treebank or MIDT+) and whose output was automatically converted to SD, with the results of the parser directly trained on ISDT. Experiments carried out to test reliability and effectiveness of the two strategies show that the performance of a parser trained on the reduced dependencies repertoire, whose output can be easily converted to SD, is slightly higher than the performance of a parser directly trained on ISDT. A non-negligible advantage of the first strategy for generating SD annotated texts is that semi-automatic extensions of the training resource are more easily and consistently carried out with respect to a reduced dependency tag set. Preliminary experiments carried out for generating the collapsed and propagated SD representation are also reported.</abstract>
    </paper>
    <paper id="634">
      <author><first>Mark</first><last>Cieliebak</last></author>
      <author><first>Oliver</first><last>Dürr</last></author>
      <author><first>Fatih</first><last>Uzdilli</last></author>
      <title>Meta-Classifiers Easily Improve Commercial Sentiment Detection Tools</title>
      <pages>3100–3104</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/820_Paper.pdf</url>
      <abstract>In this paper, we analyze the quality of several commercial tools for sentiment detection. All tools are tested on nearly 30,000 short texts from various sources, such as tweets, news, reviews etc. The best commercial tools have average accuracy of 60%. We then apply machine learning techniques (Random Forests) to combine all tools, and show that this results in a meta-classifier that improves the overall performance significantly.</abstract>
    </paper>
    <paper id="635">
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title><fixed-case>U</fixed-case>nix<fixed-case>M</fixed-case>an Corpus: A Resource for Language Learning in the <fixed-case>U</fixed-case>nix Domain</title>
      <pages>2985–2989</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/823_Paper.pdf</url>
      <abstract>We present a new resource, the UnixMan Corpus, for studying language learning it the domain of Unix utility manuals. The corpus is built by mining Unix (and other Unix related) man pages for parallel example entries, consisting of English textual descriptions with corresponding command examples. The commands provide a grounded and ambiguous semantics for the textual descriptions, making the corpus of interest to work on Semantic Parsing and Grounded Language Learning. In contrast to standard resources for Semantic Parsing, which tend to be restricted to a small number of concepts and relations, the UnixMan Corpus spans a wide variety of utility genres and topics, and consists of hundreds of command and domain entity types. The semi-structured nature of the manuals also makes it easy to exploit other types of relevant information for Grounded Language Learning. We describe the details of the corpus and provide preliminary classification results.</abstract>
    </paper>
    <paper id="636">
      <author><first>Jonathan</first><last>Sonntag</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <title><fixed-case>G</fixed-case>ra<fixed-case>PAT</fixed-case>: a Tool for Graph Annotations</title>
      <pages>4147–4151</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/824_Paper.pdf</url>
      <abstract>We introduce GraPAT, a web-based annotation tool for building graph structures over text. Graphs have been demonstrated to be relevant in a variety of quite diverse annotation efforts and in different NLP applications, and they serve to model annotators intuitions quite closely. In particular, in this paper we discuss the implementation of graph annotations for sentiment analysis, argumentation structure, and rhetorical text structures. All of these scenarios can create certain problems for existing annotation tools, and we show how GraPAT can help to overcome such difficulties.</abstract>
    </paper>
    <paper id="637">
      <author><first>Antonio</first><last>Pareja-Lora</last></author>
      <author><first>Guillermo</first><last>Cárcamo-Escorza</last></author>
      <author><first>Alicia</first><last>Ballesteros-Calvo</last></author>
      <title>Standardisation and Interoperation of Morphosyntactic and Syntactic Annotation Tools for <fixed-case>S</fixed-case>panish and their Annotations</title>
      <pages>4118–4124</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/826_Paper.pdf</url>
      <abstract>Linguistic annotation tools and linguistic annotations are scarcely syntactically and/or semantically interoperable. Their low interoperability usually results from the number of factors taken into account in their development and design. These include (i) the type of phenomena annotated (either morphosyntactic, syntactic, semantic, etc.); (ii) how these phenomena are annotated (e.g., the particular guidelines and/or schema used to encode the annotations); and (iii) the languages (Java, C++, etc.) and technologies (as standalone programs, as APIs, as web services, etc.) used to develop them. This low level of interoperability makes it difficult to reuse both the linguistic annotation tools and their annotations in new scenarios, e.g., in natural language processing (NLP) pipelines. In spite of this, developing new linguistic tools from scratch is quite a high time-consuming task that also entails a very high cost. Therefore, cost-effective ways to systematically reuse linguistic tools and annotations must be found urgently. A traditional way to overcome reuse and/or interoperability problems is standardisation. In this paper, we present a web service version of FreeLing that provides standard-compliant morpho-syntactic and syntactic annotations for Spanish, according to several ISO linguistic annotation standards and standard drafts.</abstract>
    </paper>
    <paper id="638">
      <author><first>Gongye</first><last>Jin</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>A Framework for Compiling High Quality Knowledge Resources From Raw Corpora</title>
      <pages>109–114</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/828_Paper.pdf</url>
      <abstract>The identification of various types of relations is a necessary step to allow computers to understand natural language text. In particular, the clarification of relations between predicates and their arguments is essential because predicate-argument structures convey most of the information in natural languages. To precisely capture these relations, wide-coverage knowledge resources are indispensable. Such knowledge resources can be derived from automatic parses of raw corpora, but unfortunately parsing still has not achieved a high enough performance for precise knowledge acquisition. We present a framework for compiling high quality knowledge resources from raw corpora. Our proposed framework selects high quality dependency relations from automatic parses and makes use of them for not only the calculation of fundamental distributional similarity but also the acquisition of knowledge such as case frames.</abstract>
    </paper>
    <paper id="639">
      <author><first>Jason</first><last>Utt</last></author>
      <author><first>Sylvia</first><last>Springorum</last></author>
      <author><first>Maximilian</first><last>Köper</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <title>Fuzzy <fixed-case>V</fixed-case>-Measure - An Evaluation Method for Cluster Analyses of Ambiguous Data</title>
      <pages>581–587</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/829_Paper.pdf</url>
      <abstract>This paper discusses an extension of the V-measure (Rosenberg and Hirschberg, 2007), an entropy-based cluster evaluation metric. While the original work focused on evaluating hard clusterings, we introduce the Fuzzy V-measure which can be used on data that is inherently ambiguous. We perform multiple analyses varying the sizes and ambiguity rates and show that while entropy-based measures in general tend to suffer when ambiguity increases, a measure with desirable properties can be derived from these in a straightforward manner.</abstract>
    </paper>
    <paper id="640">
      <author><first>Guoyu</first><last>Tang</last></author>
      <author><first>Yunqing</first><last>Xia</last></author>
      <author><first>Weizhi</first><last>Wang</last></author>
      <author><first>Raymond</first><last>Lau</last></author>
      <author><first>Fang</first><last>Zheng</last></author>
      <title>Clustering tweets using<fixed-case>W</fixed-case>ikipedia concepts</title>
      <pages>2262–2267</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/83_Paper.pdf</url>
      <abstract>Two challenging issues are notable in tweet clustering. Firstly, the sparse data problem is serious since no tweet can be longer than 140 characters. Secondly, synonymy and polysemy are rather common because users intend to present a unique meaning with a great number of manners in tweets. Enlightened by the recent research which indicates Wikipedia is promising in representing text, we exploit Wikipedia concepts in representing tweets with concept vectors. We address the polysemy issue with a Bayesian model, and the synonymy issue by exploiting the Wikipedia redirections. To further alleviate the sparse data problem, we further make use of three types of out-links in Wikipedia. Evaluation on a twitter dataset shows that the concept model outperforms the traditional VSM model in tweet clustering.</abstract>
    </paper>
    <paper id="641">
      <author><first>Maria</first><last>Koutsombogera</last></author>
      <author><first>Samer Al</first><last>Moubayed</last></author>
      <author><first>Bajibabu</first><last>Bollepalli</last></author>
      <author><first>Ahmed Hussen</first><last>Abdelaziz</last></author>
      <author><first>Martin</first><last>Johansson</last></author>
      <author><first>José David Aguas</first><last>Lopes</last></author>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <author><first>Catharine</first><last>Oertel</last></author>
      <author><first>Kalin</first><last>Stefanov</last></author>
      <author><first>Gül</first><last>Varol</last></author>
      <title>The Tutorbot Corpus — A Corpus for Studying Tutoring Behaviour in Multiparty Face-to-Face Spoken Dialogue</title>
      <pages>4196–4201</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/832_Paper.pdf</url>
      <abstract>This paper describes a novel experimental setup exploiting state-of-the-art capture equipment to collect a multimodally rich game-solving collaborative multiparty dialogue corpus. The corpus is targeted and designed towards the development of a dialogue system platform to explore verbal and nonverbal tutoring strategies in multiparty spoken interactions. The dialogue task is centered on two participants involved in a dialogue aiming to solve a card-ordering game. The participants were paired into teams based on their degree of extraversion as resulted from a personality test. With the participants sits a tutor that helps them perform the task, organizes and balances their interaction and whose behavior was assessed by the participants after each interaction. Different multimodal signals captured and auto-synchronized by different audio-visual capture technologies, together with manual annotations of the tutors behavior constitute the Tutorbot corpus. This corpus is exploited to build a situated model of the interaction based on the participants temporally-changing state of attention, their conversational engagement and verbal dominance, and their correlation with the verbal and visual feedback and conversation regulatory actions generated by the tutor.</abstract>
    </paper>
    <paper id="642">
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Darja</first><last>Fišer</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <title><fixed-case>T</fixed-case>weet<fixed-case>C</fixed-case>a<fixed-case>T</fixed-case>: a tool for building <fixed-case>T</fixed-case>witter corpora of smaller languages</title>
      <pages>2279–2283</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/834_Paper.pdf</url>
      <abstract>This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using the Twitter search API and a set of seed terms, the tool identifies users tweeting in the language of interest together with their friends and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected corpus is also described, which filters out users that tweet predominantly in a foreign language thus further cleans the collected corpora. Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported.</abstract>
    </paper>
    <paper id="643">
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Vojtěch</first><last>Diatka</last></author>
      <author><first>Pavel</first><last>Rychlý</last></author>
      <author><first>Pavel</first><last>Straňák</last></author>
      <author><first>Vít</first><last>Suchomel</last></author>
      <author><first>Aleš</first><last>Tamchyna</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <title><fixed-case>H</fixed-case>ind<fixed-case>E</fixed-case>n<fixed-case>C</fixed-case>orp - <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish and <fixed-case>H</fixed-case>indi-only Corpus for Machine Translation</title>
      <pages>3550–3555</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/835_Paper.pdf</url>
      <abstract>We present HindEnCorp, a parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences. Both the corpora are freely available for non-commercial research and their preliminary release has been used by numerous participants of the WMT 2014 shared translation task.</abstract>
    </paper>
    <paper id="644">
      <author><first>Silvia</first><last>Necşulescu</last></author>
      <author><first>Sara</first><last>Mendes</last></author>
      <author><first>Núria</first><last>Bel</last></author>
      <title>Combining dependency information and generalization in a pattern-based approach to the classification of lexical-semantic relation instances</title>
      <pages>4308–4315</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/837_Paper.pdf</url>
      <abstract>This work addresses the classification of word pairs as instances of lexical-semantic relations. The classification is approached by leveraging patterns of co-occurrence contexts from corpus data. The significance of using dependency information, of augmenting the set of dependency paths provided to the system, and of generalizing patterns using part-of-speech information for the classification of lexical-semantic relation instances is analyzed. Results show that dependency information is decisive to achieve better results both in precision and recall, while generalizing features based on dependency information by replacing lexical forms with their part-of-speech increases the coverage of classification systems. Our experiments also make apparent that approaches based on the context where word pairs co-occur are upper-bound-limited by the times these appear in the same sentence. Therefore strategies to use information across sentence boundaries are necessary.</abstract>
    </paper>
    <paper id="645">
      <author><first>Aimilios</first><last>Chalamandaris</last></author>
      <author><first>Pirros</first><last>Tsiakoulis</last></author>
      <author><first>Sotiris</first><last>Karabetsos</last></author>
      <author><first>Spyros</first><last>Raptis</last></author>
      <title>Using Audio Books for Training a Text-to-Speech System</title>
      <pages>3076–3080</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/838_Paper.pdf</url>
      <abstract>Creating new voices for a TTS system often requires a costly procedure of designing and recording an audio corpus, a time consuming and effort intensive task. Using publicly available audiobooks as the raw material of a spoken corpus for such systems creates new perspectives regarding the possibility of creating new synthetic voices quickly and with limited effort. This paper addresses the issue of creating new synthetic voices based on audiobook data in an automated method. As an audiobook includes several types of speech, such as narration, character playing etc., special care is given in identifying the data subset that leads to a more neutral and general purpose synthetic voice. The main goal is to identify and address the effect the audiobook speech diversity on the resulting TTS system. Along with the methodology for coping with this diversity in the speech data, we also describe a set of experiments performed in order to investigate the efficiency of different approaches for automatic data pruning. Further plans for exploiting the diversity of the speech incorporated in an audiobook are also described in the final section and conclusions are drawn.</abstract>
    </paper>
    <paper id="646">
      <author><first>Agata</first><last>Cybulska</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <title>Using a sledgehammer to crack a nut? Lexical diversity and event coreference resolution</title>
      <pages>4545–4552</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/840_Paper.pdf</url>
      <abstract>In this paper we examine the representativeness of the EventCorefBank (ECB, Bejan and Harabagiu, 2010) with regards to the language population of large-volume streams of news. The ECB corpus is one of the data sets used for evaluation of the task of event coreference resolution. Our analysis shows that the ECB in most cases covers one seminal event per domain, what considerably simplifies event and so language diversity that one comes across in the news. We augmented the corpus with a new corpus component, consisting of 502 texts, describing different instances of event types that were already captured by the 43 topics of the ECB, making it more representative of news articles on the web. The new “ECB+” corpus is available for further research.</abstract>
    </paper>
    <paper id="647">
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <title>ca<fixed-case>W</fixed-case>a<fixed-case>C</fixed-case> – A web corpus of <fixed-case>C</fixed-case>atalan and its application to language modeling and machine translation</title>
      <pages>1728–1732</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/841_Paper.pdf</url>
      <abstract>In this paper we present the construction process of a web corpus of Catalan built from the content of the .cat top-level domain. For collecting and processing data we use the Brno pipeline with the spiderling crawler and its accompanying tools. To the best of our knowledge the corpus represents the largest existing corpus of Catalan containing 687 million words, which is a significant increase given that until now the biggest corpus of Catalan, CuCWeb, counts 166 million words. We evaluate the resulting resource on the tasks of language modeling and statistical machine translation (SMT) by calculating LM perplexity and incorporating the LM in the SMT pipeline. We compare language models trained on different subsets of the resource with those trained on the Catalan Wikipedia and the target side of the parallel data used to train the SMT system.</abstract>
    </paper>
    <paper id="648">
      <author><first>Marc</first><last>Kupietz</last></author>
      <author><first>Harald</first><last>Lüngen</last></author>
      <title>Recent Developments in <fixed-case>D</fixed-case>e<fixed-case>R</fixed-case>e<fixed-case>K</fixed-case>o</title>
      <pages>2378–2385</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/842_Paper.pdf</url>
      <abstract>This paper gives an overview of recent developments in the German Reference Corpus DeReKo in terms of growth, maximising relevant corpus strata, metadata, legal issues, and its current and future research interface. Due to the recent acquisition of new licenses, DeReKo has grown by a factor of four in the first half of 2014, mostly in the area of newspaper text, and presently contains over 24 billion word tokens. Other strata, like fictional texts, web corpora, in particular CMC texts, and spoken but conceptually written texts have also increased significantly. We report on the newly acquired corpora that led to the major increase, on the principles and strategies behind our corpus acquisition activities, and on our solutions for the emerging legal, organisational, and technical challenges.</abstract>
    </paper>
    <paper id="649">
      <author><first>Shu-Kai</first><last>Hsieh</last></author>
      <title>Why <fixed-case>C</fixed-case>hinese Web-as-Corpus is Wacky? Or: How Big Data is Killing <fixed-case>C</fixed-case>hinese Corpus Linguistics</title>
      <pages>2386–2389</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/843_Paper.pdf</url>
      <abstract>This paper aims to examine and evaluate the current development of using Web-as-Corpus (WaC) paradigm in Chinese corpus linguistics. I will argue that the unstable notion of wordhood in Chinese and the resulting diverse ideas of implementing word segmentation systems have posed great challenges for those who are keen on building web-scaled corpus data. Two lexical measures are proposed to illustrate the issues and methodological discussions are provided.</abstract>
    </paper>
    <paper id="650">
      <author><first>Tafseer</first><last>Ahmed Khan</last></author>
      <title>Automatic acquisition of <fixed-case>U</fixed-case>rdu nouns (along with gender and irregular plurals)</title>
      <pages>2846–2850</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/844_Paper.pdf</url>
      <abstract>The paper describes a set of methods to automatically acquire the Urdu nouns (and its gender) on the basis of inflectional and contextual clues. The algorithms used are a blend of computer’s brute force on the corpus and careful design of distinguishing rules on the basis linguistic knowledge. As there are homograph inflections for Urdu nouns, adjectives and verbs, we compare potential inflectional forms with paradigms of inflections in strict order and gives best guess (of part of speech) for the word. We also worked on irregular plurals i.e. the plural forms that are borrowed from Arabic, Persian and English. Evaluation shows that not all the borrowed rules have same productivity in Urdu. The commonly used borrowed plural rules are shown in the result.</abstract>
    </paper>
    <paper id="651">
      <author><first>Clare</first><last>Llewellyn</last></author>
      <author><first>Claire</first><last>Grover</last></author>
      <author><first>Jon</first><last>Oberlander</last></author>
      <author><first>Ewan</first><last>Klein</last></author>
      <title>Re-using an Argument Corpus to Aid in the Curation of Social Media Collections</title>
      <pages>462–468</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/845_Paper.pdf</url>
      <abstract>This work investigates how automated methods can be used to classify social media text into argumentation types. In particular it is shown how supervised machine learning was used to annotate a Twitter dataset (London Riots) with argumentation classes. An investigation of issues arising from a natural inconsistency within social media data found that machine learning algorithms tend to over fit to the data because Twitter contains a lot of repetition in the form of retweets. It is also noted that when learning argumentation classes we must be aware that the classes will most likely be of very different sizes and this must be kept in mind when analysing the results. Encouraging results were found in adapting a model from one domain of Twitter data (London Riots) to another (OR2012). When adapting a model to another dataset the most useful feature was punctuation. It is probable that the nature of punctuation in Twitter language, the very specific use in links, indicates argumentation class.</abstract>
    </paper>
    <paper id="652">
      <author><first>Raivis</first><last>Skadiņš</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Roberts</first><last>Rozis</last></author>
      <author><first>Daiga</first><last>Deksne</last></author>
      <title>Billions of Parallel Words for Free: Building and Using the <fixed-case>EU</fixed-case> Bookshop Corpus</title>
      <pages>1850–1855</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/846_Paper.pdf</url>
      <abstract>The European Union is a great source of high quality documents with translations into several languages. Parallel corpora from its publications are frequently used in various tasks, machine translation in particular. A source that has not systematically been explored yet is the EU Bookshop ― an online service and archive of publications from various European institutions. The service contains a large body of publications in the 24 official of the EU. This paper describes our efforts in collecting those publications and converting them to a format that is useful for natural language processing in particular statistical machine translation. We report our procedure of crawling the website and various pre-processing steps that were necessary to clean up the data after the conversion from the original PDF files. Furthermore, we demonstrate the use of this dataset in training SMT models for English, French, German, Spanish, and Latvian.</abstract>
    </paper>
    <paper id="653">
      <author><first>Isa</first><last>Maks</last></author>
      <author><first>Ruben</first><last>Izquierdo</last></author>
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Andoni</first><last>Azpeitia</last></author>
      <title>Generating Polarity Lexicons with <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et propagation in 5 languages</title>
      <pages>1155–1161</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/847_Paper.pdf</url>
      <abstract>In this paper we focus on the creation of general-purpose (as opposed to domain-specific) polarity lexicons in five languages: French, Italian, Dutch, English and Spanish using WordNet propagation. WordNet propagation is a commonly used method to generate these lexicons as it gives high coverage of general purpose language and the semantically rich WordNets where concepts are organised in synonym , antonym and hyperonym/hyponym structures seem to be well suited to the identification of positive and negative words. However, WordNets of different languages may vary in many ways such as the way they are compiled, the number of synsets, number of synonyms and number of semantic relations they include. In this study we investigate whether this variability translates into differences of performance when these WordNets are used for polarity propagation. Although many variants of the propagation method are developed for English, little is known about how they perform with WordNets of other languages. We implemented a propagation algorithm and designed a method to obtain seed lists similar with respect to quality and size, for each of the five languages. We evaluated the results against gold standards also developed according to a common method in order to achieve as less variance as possible between the different languages.</abstract>
    </paper>
    <paper id="654">
      <author><first>Mara</first><last>Chinea Rios</last></author>
      <author><first>Germán</first><last>Sanchis-Trilles</last></author>
      <author><first>Daniel</first><last>Ortiz-Martínez</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <title>Online optimisation of log-linear weights in interactive machine translation</title>
      <pages>3556–3559</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/848_Paper.pdf</url>
      <abstract>Whenever the quality provided by a machine translation system is not enough, a human expert is required to correct the sentences provided by the machine translation system. In such a setup, it is crucial that the system is able to learn from the errors that have already been corrected. In this paper, we analyse the applicability of discriminative ridge regression for learning the log-linear weights of a state-of-the-art machine translation system underlying an interactive machine translation framework, with encouraging results.</abstract>
    </paper>
    <paper id="655">
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Thomas</first><last>Bögel</last></author>
      <author><first>Julian</first><last>Zell</last></author>
      <author><first>Ayser</first><last>Armiti</last></author>
      <author><first>Tran Van</first><last>Canh</last></author>
      <author><first>Michael</first><last>Gertz</last></author>
      <title>Extending <fixed-case>H</fixed-case>eidel<fixed-case>T</fixed-case>ime for Temporal Expressions Referring to Historic Dates</title>
      <pages>2390–2397</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/849_Paper.pdf</url>
      <abstract>Research on temporal tagging has achieved a lot of attention during the last years. However, most of the work focuses on processing news-style documents. Thus, references to historic dates are often not well handled by temporal taggers although they frequently occur in narrative-style documents about history, e.g., in many Wikipedia articles. In this paper, we present the AncientTimes corpus containing documents about different historic time periods in eight languages, in which we manually annotated temporal expressions. Based on this corpus, we explain the challenges of temporal tagging documents about history. Furthermore, we use the corpus to extend our multilingual, cross-domain temporal tagger HeidelTime to extract and normalize temporal expressions referring to historic dates, and to demonstrate HeidelTime’s new capabilities. Both, the AncientTimes corpus as well as the new HeidelTime version are made publicly available.</abstract>
    </paper>
    <paper id="656">
      <author><first>Roman</first><last>Klinger</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <title>The <fixed-case>USAGE</fixed-case> review corpus for fine grained multi lingual opinion analysis</title>
      <pages>2211–2218</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/85_Paper.pdf</url>
      <abstract>Opinion mining has received wide attention in recent years. Models for this task are typically trained or evaluated with a manually annotated dataset. However, fine-grained annotation of sentiments including information about aspects and their evaluation is very labour-intensive. The data available so far is limited. Contributing to this situation, this paper describes the Bielefeld University Sentiment Analysis Corpus for German and English (USAGE), which we offer freely to the community and which contains the annotation of product reviews from Amazon with both aspects and subjective phrases. It provides information on segments in the text which denote an aspect or a subjective evaluative phrase which refers to the aspect. Relations and coreferences are explicitly annotated. This dataset contains 622 English and 611 German reviews, allowing to investigate how to port sentiment analysis systems across languages and domains. We describe the methodology how the corpus was created and provide statistics including inter-annotator agreement. We further provide figures for a baseline system and results for German and English as well as in a cross-domain setting. The results are encouraging in that they show that aspects and phrases can be extracted robustly without the need of tuning to a particular type of products.</abstract>
    </paper>
    <paper id="657">
      <author><first>Nadjet</first><last>Bouayad-Agha</last></author>
      <author><first>Alicia</first><last>Burga</last></author>
      <author><first>Gerard</first><last>Casamayor</last></author>
      <author><first>Joan</first><last>Codina</last></author>
      <author><first>Rogelio</first><last>Nazar</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <title>An Exercise in Reuse of Resources: Adapting General Discourse Coreference Resolution for Detecting Lexical Chains in Patent Documentation</title>
      <pages>3214–3221</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/850_Paper.pdf</url>
      <abstract>The Stanford Coreference Resolution System (StCR) is a multi-pass, rule-based system that scored best in the CoNLL 2011 shared task on general discourse coreference resolution. We describe how the StCR has been adapted to the specific domain of patents and give some cues on how it can be adapted to other domains. We present a linguistic analysis of the patent domain and how we were able to adapt the rules to the domain and to expand coreferences with some lexical chains. A comparative evaluation shows an improvement of the coreference resolution system, denoting that (i) StCR is a valuable tool across different text genres; (ii) specialized discourse NLP may significantly benefit from general discourse NLP research.</abstract>
    </paper>
    <paper id="658">
      <author><first>Bernardo</first><last>Severo</last></author>
      <author><first>Cassia</first><last>Trojahn</last></author>
      <author><first>Renata</first><last>Vieira</last></author>
      <title><fixed-case>VOAR</fixed-case>: A Visual and Integrated Ontology Alignment Environment</title>
      <pages>3671–3677</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/851_Paper.pdf</url>
      <abstract>Ontology alignment is a key process for enabling interoperability between ontology-based systems in the Linked Open Data age. From two input ontologies, this process generates an alignment (set of correspondences) between them. In this paper we present VOAR, a new web-based environment for ontology alignment visualization and manipulation. Within this graphical environment, users can manually create/edit correspondences and apply a set of operations on alignments (filtering, merge, difference, etc.). VOAR allows invoking external ontology matching systems that implement a specific alignment interface, so that the generated alignments can be manipulated within the environment. Evaluating multiple alignments together against a reference one can also be carried out, using classical evaluation metrics (precision, recall and f-measure). The status of each correspondence with respect to its presence or absence in reference alignment is visually represented. Overall, the main new aspect of VOAR is the visualization and manipulation of alignments at schema level, in an integrated, visual and web-based environment.</abstract>
    </paper>
    <paper id="659">
      <author><first>Thomas</first><last>Eckart</last></author>
      <author><first>Erla</first><last>Hallsteinsdóttir</last></author>
      <author><first>Sigrún</first><last>Helgadóttir</last></author>
      <author><first>Uwe</first><last>Quasthoff</last></author>
      <author><first>Dirk</first><last>Goldhahn</last></author>
      <title>A 500 Million Word <fixed-case>POS</fixed-case>-Tagged <fixed-case>I</fixed-case>celandic Corpus</title>
      <pages>2398–2402</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/852_Paper.pdf</url>
      <abstract>The new POS-tagged Icelandic corpus of the Leipzig Corpora Collection is an extensive resource for the analysis of the Icelandic language. As it contains a large share of all Web documents hosted under the .is top-level domain, it is especially valuable for investigations on modern Icelandic and non-standard language varieties. The corpus is accessible via a dedicated web portal and large shares are available for download. Focus of this paper will be the description of the tagging process and evaluation of statistical properties like word form frequencies and part of speech tag distributions. The latter will be in particular compared with values from the Icelandic Frequency Dictionary (IFD) Corpus.</abstract>
    </paper>
    <paper id="660">
      <author><first>Chahinez</first><last>Benkoussas</last></author>
      <author><first>Hussam</first><last>Hamdan</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Elodie</first><last>Faath</last></author>
      <title>A Collection of Scholarly Book Reviews from the Platforms of electronic sources in Humanities and Social Sciences <fixed-case>O</fixed-case>pen<fixed-case>E</fixed-case>dition.org</title>
      <pages>4172–4177</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/853_Paper.pdf</url>
      <abstract>In this paper, we present our contribution for the automatic construction of the Scholarly Book Reviews corpora from two different sources, the OpenEdition platform which is dedicated to electronic resources in the humanities and social sciences, and the Web. The main target is the collect of reviews in order to provide automatic links between each review and its potential book in the future. For these purposes, we propose different document representations and we apply some supervised approaches for binary genre classification before evaluating their impact.</abstract>
    </paper>
    <paper id="661">
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <author><first>Eiríkur</first><last>Rögnvaldsson</last></author>
      <author><first>Einar Freyr</first><last>Sigurðsson</last></author>
      <author><first>Joel C.</first><last>Wallenberg</last></author>
      <title>Rapid Deployment of Phrase Structure Parsing for Related Languages: A Case Study of Insular Scandinavian</title>
      <pages>91–95</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/855_Paper.pdf</url>
      <abstract>This paper presents ongoing work that aims to improve machine parsing of Faroese using a combination of Faroese and Icelandic training data. We show that even if we only have a relatively small parsed corpus of one language, namely 53,000 words of Faroese, we can obtain better results by adding information about phrase structure from a closely related language which has a similar syntax. Our experiment uses the Berkeley parser. We demonstrate that the addition of Icelandic data without any other modification to the experimental setup results in an f-measure improvement from 75.44% to 78.05% in Faroese and an improvement in part-of-speech tagging accuracy from 88.86% to 90.40%.</abstract>
    </paper>
    <paper id="662">
      <author><first>Michael</first><last>Röder</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <author><first>Sebastian</first><last>Hellmann</last></author>
      <author><first>Daniel</first><last>Gerber</last></author>
      <author><first>Andreas</first><last>Both</last></author>
      <title>N³ - A Collection of Datasets for Named Entity Recognition and Disambiguation in the <fixed-case>NLP</fixed-case> Interchange Format</title>
      <pages>3529–3533</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/856_Paper.pdf</url>
      <abstract>Extracting Linked Data following the Semantic Web principle from unstructured sources has become a key challenge for scientific research. Named Entity Recognition and Disambiguation are two basic operations in this extraction process. One step towards the realization of the Semantic Web vision and the development of highly accurate tools is the availability of data for validating the quality of processes for Named Entity Recognition and Disambiguation as well as for algorithm tuning. This article presents three novel, manually curated and annotated corpora (N3). All of them are based on a free license and stored in the NLP Interchange Format to leverage the Linked Data character of our datasets.</abstract>
    </paper>
    <paper id="663">
      <author><first>Valentín</first><last>Cardeñoso-Payo</last></author>
      <author><first>César</first><last>González-Ferreras</last></author>
      <author><first>David</first><last>Escudero</last></author>
      <title>Assessment of Non-native Prosody for <fixed-case>S</fixed-case>panish as <fixed-case>L</fixed-case>2 using quantitative scores and perceptual evaluation</title>
      <pages>3967–3972</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/857_Paper.pdf</url>
      <abstract>In this work we present SAMPLE, a new pronunciation database of Spanish as L2, and first results on the automatic assessment of Non-native prosody. Listen and repeat and read tasks are carried out by native and foreign speakers of Spanish. The corpus has been designed to support comparative studies and evaluation of automatic pronunciation error assessment both at phonetic and prosodic level. Four expert evaluators have annotated utterances with perceptual scores related to prosodic aspects of speech, intelligibility, phonetic quality and global proficiency level in Spanish. From each utterance, we computed several prosodic features and ASR scores. A correlation study over subjective and quantitative measures is carried out. An estimation of the prediction of perceptual scores from speech features is shown.</abstract>
    </paper>
    <paper id="664">
      <author><first>Michael</first><last>Stadtschnitzer</last></author>
      <author><first>Jochen</first><last>Schwenninger</last></author>
      <author><first>Daniel</first><last>Stein</last></author>
      <author><first>Joachim</first><last>Koehler</last></author>
      <title>Exploiting the large-scale <fixed-case>G</fixed-case>erman Broadcast Corpus to boost the Fraunhofer <fixed-case>IAIS</fixed-case> Speech Recognition System</title>
      <pages>3887–3890</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/858_Paper.pdf</url>
      <abstract>In this paper we describe the large-scale German broadcast corpus (GER-TV1000h) containing more than 1,000 hours of transcribed speech data. This corpus is unique in the German language corpora domain and enables significant progress in tuning the acoustic modelling of German large vocabulary continuous speech recognition (LVCSR) systems. The exploitation of this huge broadcast corpus is demonstrated by optimizing and improving the Fraunhofer IAIS speech recognition system. Due to the availability of huge amount of acoustic training data new training strategies are investigated. The performance of the automatic speech recognition (ASR) system is evaluated on several datasets and compared to previously published results. It can be shown that the word error rate (WER) using a larger corpus can be reduced by up to 9.1 \% relative. By using both larger corpus and recent training paradigms the WER was reduced by up to 35.8 \% relative and below 40 \% absolute even for spontaneous dialectal speech in noisy conditions, making the ASR output a useful resource for subsequent tasks like named entity recognition also in difficult acoustic situations.</abstract>
    </paper>
    <paper id="665">
      <author><first>Natalia</first><last>Loukachevitch</last></author>
      <author><first>Aleksey</first><last>Alekseev</last></author>
      <title>Summarizing News Clusters on the Basis of Thematic Chains</title>
      <pages>1600–1607</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/859_Paper.pdf</url>
      <abstract>In this paper we consider a method for extraction of sets of semantically similar language expressions representing different partici-pants of the text story ― thematic chains. The method is based on the structural organization of news clusters and exploits comparison of various contexts of words. The word contexts are used as a basis for extracting multiword expressions and constructing thematic chains. The main difference of thematic chains in comparison with lexical chains is the basic principle of their construction: thematic chains are intended to model different participants (concrete or abstract) of the situation described in the analyzed texts, what means that elements of the same thematic chain cannot often co-occur in the same sentences of the texts under consideration. We evaluate our method on the multi-document summarization task</abstract>
    </paper>
    <paper id="666">
      <author><first>Kilian A.</first><last>Foth</last></author>
      <author><first>Arne</first><last>Köhn</last></author>
      <author><first>Niels</first><last>Beuck</last></author>
      <author><first>Wolfgang</first><last>Menzel</last></author>
      <title>Because Size Does Matter: The <fixed-case>H</fixed-case>amburg Dependency Treebank</title>
      <pages>2326–2333</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/860_Paper.pdf</url>
      <abstract>We present the Hamburg Dependency Treebank (HDT), which to our knowledge is the largest dependency treebank currently available. It consists of genuine dependency annotations, i. e. they have not been transformed from phrase structures. We explore characteristics of the treebank and compare it against others. To exemplify the benefit of large dependency treebanks, we evaluate different parsers on the HDT. In addition, a set of tools will be described which help working with and searching in the treebank.</abstract>
    </paper>
    <paper id="667">
      <author><first>Vincenzo</first><last>Galatà</last></author>
      <author><first>Alberto</first><last>Benin</last></author>
      <author><first>Piero</first><last>Cosi</last></author>
      <author><first>Giuseppe Riccardo</first><last>Leone</last></author>
      <author><first>Giulio</first><last>Paci</last></author>
      <author><first>Giacomo</first><last>Sommavilla</last></author>
      <author><first>Fabio</first><last>Tesser</last></author>
      <title>Discovering the <fixed-case>I</fixed-case>talian literature: interactive access to audio indexed text resources</title>
      <pages>4152–4156</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/862_Paper.pdf</url>
      <abstract>In this paper we present a web interface to study Italian through the access to read Italian literature. The system allows to browse the content, search for specific words and listen to the correct pronunciation produced by native speakers in a given context. This work aims at providing people who are interested in learning Italian with a new way of exploring the Italian culture and literature through a web interface with a search module. By submitting a query, users may browse and listen to the results through several modalities including: a) the voice of a native speaker: if an indexed audio track is available, the user can listen either to the query terms or to the whole context in which they appear (sentence, paragraph, verse); b) a synthetic voice: the user can listen to the results read by a text-to-speech system; c) an avatar: the user can listen to and look at a talking head reading the paragraph and visually reproducing real speech articulatory movements. In its up to date version, different speech technologies currently being developed at ISTC-CNR are implemented into a single framework. The system will be described in detail and hints for future work are discussed.</abstract>
    </paper>
    <paper id="668">
      <author><first>Jorge</first><last>Gracia</last></author>
      <author><first>Elena</first><last>Montiel-Ponsoda</last></author>
      <author><first>Daniel</first><last>Vila-Suero</last></author>
      <author><first>Guadalupe</first><last>Aguado-de-Cea</last></author>
      <title>Enabling Language Resources to Expose Translations as Linked Data on the Web</title>
      <pages>409–413</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/863_Paper.pdf</url>
      <abstract>Language resources, such as multilingual lexica and multilingual electronic dictionaries, contain collections of lexical entries in several languages. Having access to the corresponding explicit or implicit translation relations between such entries might be of great interest for many NLP-based applications. By using Semantic Web-based techniques, translations can be available on the Web to be consumed by other (semantic enabled) resources in a direct manner, not relying on application-specific formats. To that end, in this paper we propose a model for representing translations as linked data, as an extension of the lemon model. Our translation module represents some core information associated to term translations and does not commit to specific views or translation theories. As a proof of concept, we have extracted the translations of the terms contained in Terminesp, a multilingual terminological database, and represented them as linked data. We have made them accessible on the Web both for humans (via a Web interface) and software agents (with a SPARQL endpoint).</abstract>
    </paper>
    <paper id="669">
      <author><first>Judit</first><last>Ács</last></author>
      <title>Pivot-based multilingual dictionary building using <fixed-case>W</fixed-case>iktionary</title>
      <pages>1938–1942</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/864_Paper.pdf</url>
      <abstract>We describe a method for expanding existing dictionaries in several languages by discovering previously non-existent links between translations. We call this method triangulation and we present and compare several variations of it. We assess precision manually, and recall by comparing the extracted dictionaries with independently obtained basic vocabulary sets. We featurize the translation candidates and train a maximum entropy classifier to identify correct translations in the noisy data.</abstract>
    </paper>
    <paper id="670">
      <author><first>Andrea</first><last>Glaser</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <title>Exploring the utility of coreference chains for improved identification of personal names</title>
      <pages>2570–2577</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/865_Paper.pdf</url>
      <abstract>Identifying the real world entity that a proper name refers to is an important task in many NLP applications. Context plays an important role in disambiguating entities with the same names. In this paper, we discuss a dataset and experimental set-up that allows us to systematically explore the effects of different sizes and types of context in this disambiguation task. We create context by first identifying coreferent expressions in the document and then combining sentences these expressions occur in to one informative context. We apply different filters to obtain different levels of coreference-based context. Since hand-labeling a dataset of a decent size is expensive, we investigate the usefulness of an automatically created pseudo-ambiguity dataset. The results on this pseudo-ambiguity dataset show that using coreference-based context performs better than using a fixed window of context around the entity. The insights taken from the pseudo data experiments can be used to predict how the method works with real data. In our experiments on real data we obtain comparable results.</abstract>
    </paper>
    <paper id="671">
      <author><first>Tatiana</first><last>Erekhinskaya</last></author>
      <author><first>Meghana</first><last>Satpute</last></author>
      <author><first>Dan</first><last>Moldovan</last></author>
      <title>Multilingual e<fixed-case>X</fixed-case>tended <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Knowledge Base: Semantic Parsing and Translation of Glosses</title>
      <pages>2990–2994</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/866_Paper.pdf</url>
      <abstract>This paper presents a method to create WordNet-like lexical resources for different languages. Instead of directly translating glosses from one language to another, we perform first semantic parsing of WordNet glosses and then translate the resulting semantic representation. The proposed approach simplifies the machine translation of the glosses. The approach provides ready to use semantic representation of glosses in target languages instead of just plain text.</abstract>
    </paper>
    <paper id="672">
      <author><first>Manel</first><last>Zarrouk</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <title>Relation Inference in Lexical Networks ... with Refinements</title>
      <pages>2995–3000</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/867_Paper.pdf</url>
      <abstract>Improving lexical networks quality is an important issue in the creation process of these language resources. This can be done by automatically inferring new relations from already existing ones with the purpose of (1) densifying the relations to cover the eventual lack of information and (2) detecting errors. In this paper, we devise such an approach applied to the JeuxDeMots lexical network, which is a freely available lexical and semantic resource for French. We first present the principles behind the lexical network construction with crowdsourcing and games with a purpose and illustrated them with JeuxDeMots (JDM). Then, we present the outline of an elicitation engine based on an inference engine using schemes like deduction, induction and abduction which will be referenced and briefly presented and we will especially highlight the new scheme (Relation Inference Scheme with Refinements) added to our system. An experiment showing the relevance of this scheme is then presented.</abstract>
    </paper>
    <paper id="673">
      <author><first>Verónica</first><last>Pérez-Rosas</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Alexis</first><last>Narvaez</last></author>
      <author><first>Mihai</first><last>Burzo</last></author>
      <title>A Multimodal Dataset for Deception Detection</title>
      <pages>3118–3122</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/869_Paper.pdf</url>
      <abstract>This paper presents the construction of a multimodal dataset for deception detection, including physiological, thermal, and visual responses of human subjects under three deceptive scenarios. We present the experimental protocol, as well as the data acquisition process. To evaluate the usefulness of the dataset for the task of deception detection, we present a statistical analysis of the physiological and thermal modalities associated with the deceptive and truthful conditions. Initial results show that physiological and thermal responses can differentiate between deceptive and truthful states.</abstract>
    </paper>
    <paper id="674">
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Tea</first><last>Pršir</last></author>
      <author><first>Antoine</first><last>Auchlin</last></author>
      <title><fixed-case>C</fixed-case>-<fixed-case>P</fixed-case>hono<fixed-case>G</fixed-case>enre: a 7-hours corpus of 7 speaking styles in <fixed-case>F</fixed-case>rench: relations between situational features and prosodic properties</title>
      <pages>302–305</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/870_Paper.pdf</url>
      <abstract>Phonogenres, or speaking styles, are typified acoustic images associated to types of language activities, causing prosodic and phonostylistic variations. This communication presents a large speech corpus (7 hours) in French, extending a previous work by Goldman et al. (2011a), Simon et al. (2010), with a greater number and complementary repertoire of considered phonogenres. The corpus is available with segmentation at phonetic, syllabic and word levels, as well as manual annotation. Segmentations and annotations were achieved semi-automatically, through a set of Praat implemented tools, and manual steps. The phonogenres are also described with a reduced set of situational dimensions as in Lucci (1983) and Koch &amp; Oesterreichers (2001). A preliminary acoustic study, joining rhythmical comparative measurements (Dellwo 2010) to Goldman et al.s (2007a) ProsoReport, reports acoustic differences between phonogenres.</abstract>
    </paper>
    <paper id="675">
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Francisco</first><last>Guzman</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Stephan</first><last>Vogel</last></author>
      <title>The <fixed-case>AMARA</fixed-case> Corpus: Building Parallel Language Resources for the Educational Domain</title>
      <pages>1856–1862</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/877_Paper.pdf</url>
      <abstract>This paper presents the AMARA corpus of on-line educational content: a new parallel corpus of educational video subtitles, multilingually aligned for 20 languages, i.e. 20 monolingual corpora and 190 parallel corpora. This corpus includes both resource-rich languages such as English and Arabic, and resource-poor languages such as Hindi and Thai. In this paper, we describe the gathering, validation, and preprocessing of a large collection of parallel, community-generated subtitles. Furthermore, we describe the methodology used to prepare the data for Machine Translation tasks. Additionally, we provide a document-level, jointly aligned development and test sets for 14 language pairs, designed for tuning and testing Machine Translation systems. We provide baseline results for these tasks, and highlight some of the challenges we face when building machine translation systems for educational content.</abstract>
    </paper>
    <paper id="676">
      <author><first>Lauma</first><last>Pretkalniņa</last></author>
      <author><first>Artūrs</first><last>Znotiņš</last></author>
      <author><first>Laura</first><last>Rituma</last></author>
      <author><first>Didzis</first><last>Goško</last></author>
      <title>Dependency parsing representation effects on the accuracy of semantic applications — an example of an inflective language</title>
      <pages>4074–4081</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/879_Paper.pdf</url>
      <abstract>In this paper we investigate how different dependency representations of a treebank influence the accuracy of the dependency parser trained on this treebank and the impact on several parser applications: named entity recognition, coreference resolution and limited semantic role labeling. For these experiments we use Latvian Treebank, whose native annotation format is dependency based hybrid augmented with phrase-like elements. We explore different representations of coordinations, complex predicates and punctuation mark attachment. Our experiments shows that parsers trained on the variously transformed treebanks vary significantly in their accuracy, but the best-performing parser as measured by attachment score not always leads to best accuracy for an end application.</abstract>
    </paper>
    <paper id="677">
      <author><first>Guiyao</first><last>Ke</last></author>
      <author><first>Pierre-Francois</first><last>Marteau</last></author>
      <title>Co-clustering of bilingual datasets as a mean for assisting the construction of thematic bilingual comparable corpora</title>
      <pages>1992–1999</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/88_Paper.pdf</url>
      <abstract>We address in this paper the assisted construction of bilingual thematic comparable corpora by means of co-clustering bilingual documents collected from raw sources such as the Web. The proposed approach is based on a quantitative comparability measure and a co-clustering approach which allow to mix similarity measures existing in each of the two linguistic spaces with a “thematic” comparability measure that defines a mapping between these two spaces. With the improvement of the co-clustering ($k$-medoids) performance we get, we use a comparability threshold and a manual verification to ensure the good and robust alignment of co-clusters (co-medoids). Finally, from any available raw corpus, we enrich the aligned clusters in order to provide “thematic” comparable corpora of good quality and controlled size. On a case study that exploit raw web data, we show that this approach scales reasonably well and is quite suited for the construction of thematic comparable corpora of good quality.</abstract>
    </paper>
    <paper id="678">
      <author><first>Paweł</first><last>Kędzia</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <title>Ruled-based, Interlingual Motivated Mapping of pl<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et onto <fixed-case>SUMO</fixed-case> Ontology</title>
      <pages>4351–4358</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/882_Paper.pdf</url>
      <abstract>In this paper we study a rule-based approach to mapping plWordNet onto SUMO Upper Ontology on the basis of the already existing mappings: plWordNet -- the Princeton WordNet -- SUMO. Information acquired from the inter-lingual relations between plWordNet and Princeton WordNet and the relations between Princeton WordNet and SUMO ontology are used in the proposed rules. Several mapping rules together with the matching examples are presented. The automated mapping results were evaluated in two steps, (i) we automatically checked formal correctness of the mappings for the pairs of plWordNet synset and SUMO concept, (ii) a subset of 160 mapping examples was manually checked by two+one linguists. We analyzed types of the mapping errors and their causes. The proposed rules expressed very high precision, especially when the errors in the resources are taken into account. Because both wordnets were constructed independently and as a result the obtained rules are not trivial and they reveal the differences between both wordnets and both languages.</abstract>
    </paper>
    <paper id="679">
      <author><first>Pollet</first><last>Samvelian</last></author>
      <author><first>Pegah</first><last>Faghiri</last></author>
      <author><first>Sarra El</first><last>Ayari</last></author>
      <title>Extending the coverage of a <fixed-case>MWE</fixed-case> database for <fixed-case>P</fixed-case>ersian <fixed-case>CP</fixed-case>s exploiting valency alternations</title>
      <pages>4023–4026</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/883_Paper.pdf</url>
      <abstract>PersPred is a manually elaborated multilingual syntactic and semantic Lexicon for Persian Complex Predicates (CPs), referred to also as Light Verb Constructions (LVCs) or Compound Verbs. CPs constitutes the regular and the most common way of expressing verbal concepts in Persian, which has only around 200 simplex verbs. CPs can be defined as multi-word sequences formed by a verb and a non-verbal element and functioning in many respects as a simplex verb. Bonami &amp; Samvelain (2010) and Samvelian &amp; Faghiri (to appear) extendedly argue that Persian CPs are MWEs and consequently must be listed. The first delivery of PersPred, contains more than 600 combinations of the verb zadan hit with a noun, presented in a spreadsheet. In this paper we present a semi-automatic method used to extend the coverage of PersPred 1.0, which relies on the syntactic information on valency alternations already encoded in the database. Given the importance of CPs in the verbal lexicon of Persian and the fact that lexical resources cruelly lack for Persian, this method can be further used to achieve our goal of making PersPred an appropriate resource for NLP applications.</abstract>
    </paper>
    <paper id="680">
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <author><first>Magdalena</first><last>Wolska</last></author>
      <title>Finding a Tradeoff between Accuracy and Rater’s Workload in Grading Clustered Short Answers</title>
      <pages>588–595</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/887_Paper.pdf</url>
      <abstract>n this paper we investigate the potential of answer clustering for semi-automatic scoring of short answer questions for German as a foreign language. We use surface features like word and character n-grams to cluster answers to listening comprehension exercises per question and simulate having human graders only label one answer per cluster and then propagating this label to all other members of the cluster. We investigate various ways to select this single item to be labeled and find that choosing the item closest to the centroid of a cluster leads to improved (simulated) grading accuracy over random item selection. Averaged over all questions, we can reduce a teachers workload to labeling only 40% of all different answers for a question, while still maintaining a grading accuracy of more than 85%.</abstract>
    </paper>
    <paper id="681">
      <author><first>Ilaine</first><last>Wang</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <title>Macrosyntactic Segmenters of a <fixed-case>F</fixed-case>rench Spoken Corpus</title>
      <pages>3891–3896</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/889_Paper.pdf</url>
      <abstract>The aim of this paper is to describe an automated process to segment spoken French transcribed data into macrosyntactic units. While sentences are delimited by punctuation marks for written data, there is no obvious hint nor limit to major units for speech. As a reference, we used the manual annotation of macrosyntactic units based on illocutionary as well as syntactic criteria and developed for the Rhapsodie corpus, a 33.000 words prosodic and syntactic treebank. Our segmenters were built using machine learning methods as supervised classifiers~: segmentation is about identifying the boundaries of units, which amounts to classifying each interword space. We trained six different models on Rhapsodie using different sets of features, including prosodic and morphosyntactic cues, on the assumption that their combination would be relevant for the task. Both types of cues could be resulting either from manual annotation/correction or from fully automated processes, which comparison might help determine the cost of manual effort, especially for the 3M words of spoken French of the Orfeo project those experiments are contributing to.</abstract>
    </paper>
    <paper id="682">
      <author><first>Jetske</first><last>Klatter</last></author>
      <author><first>Roeland</first><last>van Hout</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Paula</first><last>Fikkert</last></author>
      <author><first>Anne</first><last>Baker</last></author>
      <author><first>Jan</first><last>de Jong</last></author>
      <author><first>Frank</first><last>Wijnen</last></author>
      <author><first>Eric</first><last>Sanders</last></author>
      <author><first>Paul</first><last>Trilsbeek</last></author>
      <title>Vulnerability in Acquisition, Language Impairments in <fixed-case>D</fixed-case>utch: Creating a <fixed-case>VALID</fixed-case> Data Archive</title>
      <pages>357–364</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/89_Paper.pdf</url>
      <abstract>The VALID Data Archive is an open multimedia data archive (under construction) with data from speakers suffering from language impairments. We report on a pilot project in the CLARIN-NL framework in which five data resources were curated. For all data sets concerned, written informed consent from the participants or their caretakers has been obtained. All materials were anonymized. The audio files were converted into wav (linear PCM) files and the transcriptions into CHAT or ELAN format. Research data that consisted of test, SPSS and Excel files were documented and converted into CSV files. All data sets obtained appropriate CMDI metadata files. A new CMDI metadata profile for this type of data resources was established and care was taken that ISOcat metadata categories were used to optimize interoperability. After curation all data are deposited at the Max Planck Institute for Psycholinguistics Nijmegen where persistent identifiers are linked to all resources. The content of the transcriptions in CHAT and plain text format can be searched with the TROVA search engine.</abstract>
    </paper>
    <paper id="683">
      <author><first>Anders</first><last>Björkelund</last></author>
      <author><first>Kerstin</first><last>Eckart</last></author>
      <author><first>Arndt</first><last>Riester</last></author>
      <author><first>Nadja</first><last>Schauffler</last></author>
      <author><first>Katrin</first><last>Schweitzer</last></author>
      <title>The Extended <fixed-case>DIRNDL</fixed-case> Corpus as a Resource for Coreference and Bridging Resolution</title>
      <pages>3222–3228</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/891_Paper.pdf</url>
      <abstract>DIRNDL is a spoken and written corpus based on German radio news, which features coreference and information-status annotation (including bridging anaphora and their antecedents), as well as prosodic information. We have recently extended DIRNDL with a fine-grained two-dimensional information status labeling scheme. We have also applied a state-of-the-art part-of-speech and morphology tagger to the corpus, as well as highly accurate constituency and dependency parsers. In the light of this development we believe that DIRNDL is an interesting resource for NLP researchers working on automatic coreference and bridging resolution. In order to enable and promote usage of the data, we make it available for download in an accessible tabular format, compatible with the formats used in the CoNLL and SemEval shared tasks on automatic coreference resolution.</abstract>
    </paper>
    <paper id="684">
      <author><first>Elena</first><last>Volodina</last></author>
      <author><first>Ildikó</first><last>Pilán</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Therese Lindström</first><last>Tiedemann</last></author>
      <title>A flexible language learning platform based on language resources and web services</title>
      <pages>3973–3978</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/892_Paper.pdf</url>
      <abstract>We present Lärka, the language learning platform of Spräkbanken (the Swedish Language Bank). It consists of an exercise generator which reuses resources available through Spräkbanken: mainly Korp, the corpus infrastructure, and Karp, the lexical infrastructure. Through Lärka we reach new user groups ― students and teachers of Linguistics as well as second language learners and their teachers ― and this way bring Spräkbanken’s resources in a relevant format to them. Lärka can therefore be viewed as an case of real-life language resource evaluation with end users. In this article we describe Lärka’s architecture, its user interface, and the five exercise types that have been released for users so far. The first user evaluation following in-class usage with students of linguistics, speech therapy and teacher candidates are presented. The outline of future work concludes the paper.</abstract>
    </paper>
    <paper id="685">
      <author><first>Christian</first><last>Chiarcos</last></author>
      <title>Towards interoperable discourse annotation. Discourse features in the Ontologies of Linguistic Annotation</title>
      <pages>4569–4577</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/893_Paper.pdf</url>
      <abstract>This paper describes the extension of the Ontologies of Linguistic Annotation (OLiA) with respect to discourse features. The OLiA ontologies provide a a terminology repository that can be employed to facilitate the conceptual (semantic) interoperability of annotations of discourse phenomena as found in the most important corpora available to the community, including OntoNotes, the RST Discourse Treebank and the Penn Discourse Treebank. Along with selected schemes for information structure and coreference, discourse relations are discussed with special emphasis on the Penn Discourse Treebank and the RST Discourse Treebank. For an example contained in the intersection of both corpora, I show how ontologies can be employed to generalize over divergent annotation schemes.</abstract>
    </paper>
    <paper id="686">
      <author><first>Patrick</first><last>Littell</last></author>
      <author><first>Kaitlyn</first><last>Price</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <title>Morphological parsing of <fixed-case>S</fixed-case>wahili using crowdsourced lexical resources</title>
      <pages>3333–3339</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/896_Paper.pdf</url>
      <abstract>We describe a morphological analyzer for the Swahili language, written in an extension of XFST/LEXC intended for the easy declaration of morphophonological patterns and importation of lexical resources. Our analyzer was supplemented extensively with data from the Kamusi Project (kamusi.org), a user-contributed multilingual dictionary. Making use of this resource allowed us to achieve wide lexical coverage quickly, but the heterogeneous nature of user-contributed content also poses some challenges when adapting it for use in an expert system.</abstract>
    </paper>
    <paper id="687">
      <author><first>Eric</first><last>Charton</last></author>
      <author><first>Marie-Jean</first><last>Meurs</last></author>
      <author><first>Ludovic</first><last>Jean-Louis</last></author>
      <author><first>Michel</first><last>Gagnon</last></author>
      <title>Improving Entity Linking using Surface Form Refinement</title>
      <pages>4609–4615</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/899_Paper.pdf</url>
      <abstract>In this paper, we present an algorithm for improving named entity resolution and entity linking by using surface form generation and rewriting. Surface forms consist of a word or a group of words that matches lexical units like Paris or New York City. Used as matching sequences to select candidate entries in a knowledge base, they contribute to the disambiguation of those candidates through similarity measures. In this context, misspelled textual sequences (entities) can be impossible to identify due to the lack of available matching surface forms. To address this problem, we propose an algorithm for surface form refinement based on Wikipedia resources. The approach extends the surface form coverage of our entity linking system, and rewrites or reformulates misspelled mentions (entities) prior to starting the annotation process. The algorithm is evaluated on the corpus associated with the monolingual English entity linking task of NIST KBP 2013. We show that the algorithm improves the entity linking system performance.</abstract>
    </paper>
    <paper id="688">
      <author><first>Victoria</first><last>Arranz</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Valérie</first><last>Mapelli</last></author>
      <author><first>Hélène</first><last>Mazo</last></author>
      <title><fixed-case>ELRA</fixed-case>’s Consolidated Services for the <fixed-case>HLT</fixed-case> Community</title>
      <pages>1511–1516</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/9_Paper.pdf</url>
      <abstract>This paper emphasises on ELRAs contribution to the HLT field thanks to the consolidation of its services since LREC 2012. Among the most recent contributions is the establishment of the International Standard Language Resource Number (ISLRN), with the creation and exploitation of an associated web portal to enable the procurement of unique identifiers for Language Resources. Interoperability, consolidation and synchronization remain also a strong focus in ELRAs cataloguing work, in particular with ELRAs involvement in the META-SHARE project, whose platform is to become ELRAs next instrument of sharing LRs. Since last LREC, ELRA has continued its action to offer free LRs to the research community. Cooperation is another watchword within ELRAs activities on multiple aspects: 1) at the legal level, ELRA is supporting the EC in identifying the gaps to be fulfilled to reach harmonized copyright regulations for the HLT community in Europe; 2) at the production level, ELRA is participating in several international projects, in the field of LR production and evaluation of technologies; 3) at the communication level, ELRA has organised the NLP12 meeting with the aim of boosting co-operation and strengthening the bridges between various communities.</abstract>
    </paper>
    <paper id="689">
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title>Single Classifier Approach for Verb Sense Disambiguation based on Generalized Features</title>
      <pages>4210–4213</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/90_Paper.pdf</url>
      <abstract>We present a supervised method for verb sense disambiguation based on VerbNet. Most previous supervised approaches to verb sense disambiguation create a classifier for each verb that reaches a frequency threshold. These methods, however, have a significant practical problem that they cannot be applied to rare or unseen verbs. In order to overcome this problem, we create a single classifier to be applied to rare or unseen verbs in a new text. This single classifier also exploits generalized semantic features of a verb and its modifiers in order to better deal with rare or unseen verbs. Our experimental results show that the proposed method achieves equivalent performance to per-verb classifiers, which cannot be applied to unseen verbs. Our classifier could be utilized to improve the classifications in lexical resources of verbs, such as VerbNet, in a semi-automatic manner and to possibly extend the coverage of these resources to new verbs.</abstract>
    </paper>
    <paper id="690">
      <author><first>Raquel</first><last>Amaro</last></author>
      <title>Extracting semantic relations from <fixed-case>P</fixed-case>ortuguese corpora using lexical-syntactic patterns</title>
      <pages>3001–3005</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/900_Paper.pdf</url>
      <abstract>The growing investment on automatic extraction procedures, together with the need for extensive resources, makes semi-automatic construction a new viable and efficient strategy for developing of language resources, combining accuracy, size, coverage and applicability. These assumptions motivated the work depicted in this paper, aiming at the establishment and use of lexical-syntactic patterns for extracting semantic relations for Portuguese from corpora, part of a larger ongoing project for the semi-automatic extension of WordNet.PT. 26 lexical-syntactic patterns were established, covering hypernymy/hyponymy and holonymy/meronymy relations between nominal items, and over 34 000 contexts were manually analyzed to evaluate the productivity of each pattern. The set of patterns and respective examples are given, as well as data concerning the extraction of relations - right hits, wrong hits and related hits-, and the total of occurrences of each pattern in CPRC. Although language-dependent, and thus clearly of obvious interest for the development of lexical resources for Portuguese, the results depicted in this paper are also expected to be helpful as a basis for the establishment of patterns for related languages such as Spanish, Catalan, French or Italian.</abstract>
    </paper>
    <paper id="691">
      <author><first>Artem</first><last>Ostankov</last></author>
      <author><first>Florian</first><last>Röhrbein</last></author>
      <author><first>Ulli</first><last>Waltinger</last></author>
      <title><fixed-case>L</fixed-case>inked<fixed-case>H</fixed-case>ealth<fixed-case>A</fixed-case>nswers: Towards Linked Data-driven Question Answering for the Health Care Domain</title>
      <pages>2613–2620</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/902_Paper.pdf</url>
      <abstract>This paper presents Linked Health Answers, a natural language question answering systems that utilizes health data drawn from the Linked Data Cloud. The contributions of this paper are three-fold: Firstly, we review existing state-of-the-art NLP platforms and components, with a special focus on components that allow or support an automatic SPARQL construction. Secondly, we present the implemented architecture of the Linked Health Answers systems. Thirdly, we propose an statistical bootstrap approach for the identification and disambiguation of RDF-based predicates using a machine learning-based classifier. The evaluation focuses on predicate detection in sentence statements, as well as within the scenario of natural language questions.</abstract>
    </paper>
    <paper id="692">
      <author><first>David</first><last>Jurgens</last></author>
      <title>An analysis of ambiguity in word sense annotations</title>
      <pages>3006–3012</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/904_Paper.pdf</url>
      <abstract>Word sense annotation is a challenging task where annotators distinguish which meaning of a word is present in a given context. In some contexts, a word usage may elicit multiple interpretations, resulting either in annotators disagreeing or in allowing the usage to be annotated with multiple senses. While some works have allowed the latter, the extent to which multiple sense annotations are needed has not been assessed. The present work analyzes a dataset of instances annotated with multiple WordNet senses to assess the causes of the multiple interpretations and their relative frequencies, along with the effect of the multiple senses on the contextual interpretation. We show that contextual underspecification is the primary cause of multiple interpretations but that syllepsis still accounts for more than a third of the cases. In addition, we show that sense coarsening can only partially remove the need for labeling instances with multiple senses and we provide suggestions for how future sense annotation guidelines might be developed to account for this need.</abstract>
    </paper>
    <paper id="693">
      <author><first>Iolanda</first><last>Alfano</last></author>
      <author><first>Francesco</first><last>Cutugno</last></author>
      <author><first>Aurelio</first><last>De Rosa</last></author>
      <author><first>Claudio</first><last>Iacobini</last></author>
      <author><first>Renata</first><last>Savy</last></author>
      <author><first>Miriam</first><last>Voghera</last></author>
      <title><fixed-case>VOLIP</fixed-case>: a corpus of spoken <fixed-case>I</fixed-case>talian and a virtuous example of reuse of linguistic resources</title>
      <pages>3897–3901</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/906_Paper.pdf</url>
      <abstract>The corpus VoLIP (The Voice of LIP) is an Italian speech resource which associates the audio signals to the orthographic transcriptions of the LIP Corpus. The LIP Corpus was designed to represent diaphasic, diatopic and diamesic variation. The Corpus was collected in the early 90s to compile a frequency lexicon of spoken Italian and its size was tailored to produce a reliable frequency lexicon for the first 3,000 lemmas. Therefore, it consists of about 500,000 word tokens for 60 hours of recording. The speech materials belong to five different text registers and they were collected in four different cities. Thanks to a modern technological approach VoLIP web service allows users to search the LIP corpus using IMDI metadata, lexical or morpho-syntactic entry keys, receiving as result the audio portions aligned to the corresponding required entry. The VoLIP corpus is freely available at the URL http://www.parlaritaliano.it.</abstract>
    </paper>
    <paper id="694">
      <author><first>Carla Parra</first><last>Escartín</last></author>
      <title>Chasing the Perfect Splitter: A Comparison of Different Compound Splitting Tools</title>
      <pages>3340–3347</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/909_Paper.pdf</url>
      <abstract>This paper reports on the evaluation of two compound splitters for German. Compounding is a very frequent phenomenon in German and thus efficient ways of detecting and correctly splitting compound words are needed for natural language processing applications. This paper presents different strategies for compound splitting, focusing on German. Four compound splitters for German are presented. Two of them were used in Statistical Machine Translation (SMT) experiments, obtaining very similar qualitative scores in terms of BLEU and TER and therefore a thorough evaluation of both has been carried out.</abstract>
    </paper>
    <paper id="695">
      <author><first>Homa B.</first><last>Hashemi</last></author>
      <author><first>Rebecca</first><last>Hwa</last></author>
      <title>A Comparison of <fixed-case>MT</fixed-case> Errors and <fixed-case>ESL</fixed-case> Errors</title>
      <pages>2696–2700</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/911_Paper.pdf</url>
      <abstract>Generating fluent and grammatical sentences is a major goal for both Machine Translation (MT) and second-language Grammar Error Correction (GEC), but there have not been a lot of cross-fertilization between the two research communities. Arguably, an automatic translate-to-English system might be seen as an English as a Second Language (ESL) writer whose native language is the source language. This paper investigates whether research findings from the GEC community may help with characterizing MT error analysis. We describe a method for the automatic classification of MT errors according to English as a Second Language (ESL) error categories and conduct a large comparison experiment that includes both high-performing and low-performing translate-to-English MT systems for several source languages. Comparing the distribution of MT error types for all the systems suggests that MT systems have fairly similar distributions regardless of their source languages, and the high-performing MT systems have error distributions that are more similar to those of the low-performing MT systems than to those of ESL learners with the same L1.</abstract>
    </paper>
    <paper id="696">
      <author><first>Philippe</first><last>Martin</last></author>
      <title>New functions for a multipurpose multimodal tool for phonetic and linguistic analysis of very large speech corpora</title>
      <pages>3628–3632</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/912_Paper.pdf</url>
      <abstract>The increased interest for linguistic analysis of spontaneous (i.e. non-prepared) speech from various points of view (semantic, syntactic, morphologic, phonologic and intonative) lead to the development of ever more sophisticated dedicated tools. Although the software Praat emerged as the de facto standard for the analysis of spoken data, its use for intonation studies is often felt as not optimal, notably for its limited capabilities in fundamental frequency tracking. This paper presents some of the recently implemented features of the software WinPitch, developed with the analysis of spontaneous speech in mind (and notably for the C-ORAL-ROM project 10 years ago). Among many features, WinPitch includes a set of multiple pitch tracking algorithms aimed to obtain reliable pitch curves in adverse recording conditions (echo, filtering, poor signal to noise ratio, etc.). Others functions of WinPitch incorporate an integrated concordancer, an on the fly text-sound aligner, and routines for EEG analysis.</abstract>
    </paper>
    <paper id="697">
      <author><first>Paul</first><last>Buitelaar</last></author>
      <author><first>Georgeta</first><last>Bordea</last></author>
      <author><first>Barry</first><last>Coughlan</last></author>
      <title>Hot Topics and Schisms in <fixed-case>NLP</fixed-case>: Community and Trend Analysis with Saffron on <fixed-case>ACL</fixed-case> and <fixed-case>LREC</fixed-case> Proceedings</title>
      <pages>2083–2088</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/913_Paper.pdf</url>
      <abstract>In this paper we present a comparative analysis of two series of conferences in the field of Computational Linguistics, the LREC conference and the ACL conference. Conference proceedings were analysed using Saffron by performing term extraction and topical hierarchy construction with the goal of analysing topic trends and research communities. The system aims to provide insight into a research community and to guide publication and participation strategies, especially of novice researchers.</abstract>
    </paper>
    <paper id="698">
      <author><first>Ann</first><last>Irvine</last></author>
      <author><first>Joshua</first><last>Langfus</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <title>The <fixed-case>A</fixed-case>merican Local News Corpus</title>
      <pages>1305–1308</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/914_Paper.pdf</url>
      <abstract>We present the American Local News Corpus (ALNC), containing over 4 billion words of text from 2,652 online newspapers in the United States. Each article in the corpus is associated with a timestamp, state, and city. All 50 U.S. states and 1,924 cities are represented. We detail our method for taking daily snapshots of thousands of local and national newspapers and present two example corpus analyses. The first explores how different sports are talked about over time and geography. The second compares per capita murder rates with news coverage of murders across the 50 states. The ALNC is about the same size as the Gigaword corpus and is growing continuously. Version 1.0 is available for research use.</abstract>
    </paper>
    <paper id="699">
      <author><first>Rudolf</first><last>Rosa</last></author>
      <author><first>Jan</first><last>Mašek</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <title><fixed-case>H</fixed-case>amle<fixed-case>DT</fixed-case> 2.0: Thirty Dependency Treebanks Stanfordized</title>
      <pages>2334–2341</pages>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/915_Paper.pdf</url>
      <abstract>We present HamleDT 2.0 (HArmonized Multi-LanguagE Dependency Treebank). HamleDT 2.0 is a collection of 30 existing treebanks harmonized into a common annotation style, the Prague Dependencies, and further transformed into Stanford Dependencies, a treebank annotation style that became popular in recent years. We use the newest basic Universal Stanford Dependencies, without added language-specific subtypes. We describe both of the annotation styles, including adjustments that were necessary to make, and provide details about the conversion process. We also discuss the differences between the two styles, evaluating their advantages and disadvantages, and note the effects of the differences on the conversion. We regard the stanfordization as generally successful, although we admit several shortcomings, especially in the distinction between direct and indirect objects, that have to be addressed in future. We release part of HamleDT 2.0 freely; we are not allowed to redistribute the whole dataset, but we do provide the conversion pipeline.</abstract>
    </paper>
    <paper id="700">
      <author><first>Shan</first><last>Wang</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <title>Building The Sense-Tagged Multilingual Parallel Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/916_Paper.pdf</url>
      <abstract>Sense-annotated parallel corpora play a crucial role in natural language processing. This paper introduces our progress in creating such a corpus for Asian languages using English as a pivot, which is the first such corpus for these languages. Two sets of tools have been developed for sequential and targeted tagging, which are also easy to set up for any new language in addition to those we are annotating. This paper also briefly presents the general guidelines for doing this project. The current results of monolingual sense-tagging and multilingual linking are illustrated, which indicate the differences among genres and language pairs. All the tools, guidelines and the manually annotated corpus will be freely available at compling.ntu.edu.sg/ntumc.</abstract>
    </paper>
    <paper id="701">
      <author><first>Marcos</first><last>Garcia</last></author>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <title>Multilingual corpora with coreferential annotation of person entities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/918_Paper.pdf</url>
      <abstract>This paper presents three corpora with coreferential annotation of person entities for Portuguese, Galician and Spanish. They contain coreference links between several types of pronouns (including elliptical, possessive, indefinite, demonstrative, relative and personal clitic and non-clitic pronouns) and nominal phrases (including proper nouns). Some statistics have been computed, showing distributional aspects of coreference both in journalistic and in encyclopedic texts. Furthermore, the paper shows the importance of coreference resolution for a task such as Information Extraction, by evaluating the output of an Open Information Extraction system on the annotated corpora. The corpora are freely distributed in two formats: (i) the SemEval-2010 and (ii) the brat rapid annotation tool, so they can be enlarged and improved collaboratively.</abstract>
    </paper>
    <paper id="702">
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <title><fixed-case>SANA</fixed-case>: A Large Scale Multi-Genre, Multi-Dialect Lexicon for <fixed-case>A</fixed-case>rabic Subjectivity and Sentiment Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/919_Paper.pdf</url>
      <abstract>The computational treatment of subjectivity and sentiment in natural language is usually significantly improved by applying features exploiting lexical resources where entries are tagged with semantic orientation (e.g., positive, negative values). In spite of the fair amount of work on Arabic sentiment analysis over the past few years (e.g., (Abbasi et al., 2008; Abdul-Mageed et al., 2014; Abdul-Mageed et al., 2012; Abdul-Mageed and Diab, 2012a; Abdul-Mageed and Diab, 2012b; Abdul-Mageed et al., 2011a; Abdul-Mageed and Diab, 2011)), the language remains under-resourced as to these polarity repositories compared to the English language. In this paper, we report efforts to build and present SANA, a large-scale, multi-genre, multi-dialect multi-lingual lexicon for the subjectivity and sentiment analysis of the Arabic language and dialects.</abstract>
    </paper>
    <paper id="703">
      <author><first>Behrang</first><last>Zadeh</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <title>Evaluation of Technology Term Recognition with Random Indexing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/920_Paper.pdf</url>
      <abstract>In this paper, we propose a method that combines the principles of automatic term recognition and the distributional hypothesis to identify technology terms from a corpus of scientific publications. We employ the random indexing technique to model terms’ surrounding words, which we call the context window, in a vector space at reduced dimension. The constructed vector space and a set of reference vectors, which represents manually annotated technology terms, in a k-nearest-neighbour voting classification scheme are used for term classification. In this paper, we examine a number of parameters that influence the obtained results. First, we inspect several context configurations, i.e. the effect of the context window size, the direction in which co-occurrence counts are collected, and information about the order of words within the context windows. Second, in the k-nearest-neighbour voting scheme, we study the role that neighbourhood size selection plays, i.e. the value of k. The obtained results are similar to word space models. The performed experiments suggest the best performing context are small (i.e. not wider than 3 words), are extended in both directions and encode the word order information. Moreover, the accomplished experiments suggest that the obtained results, to a great extent, are independent of the value of k.</abstract>
    </paper>
    <paper id="704">
      <author><first>Stefan</first><last>Bott</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <title>Optimizing a Distributional Semantic Model for the Prediction of <fixed-case>G</fixed-case>erman Particle Verb Compositionality</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/921_Paper.pdf</url>
      <abstract>In the work presented here we assess the degree of compositionality of German Particle Verbs with a Distributional Semantics Model which only relies on word window information and has no access to syntactic information as such. Our method only takes the lexical distributional distance between the Particle Verb to its Base Verb as a predictor for compositionality. We show that the ranking of distributional similarity correlates significantly with the ranking of human judgements on semantic compositionality for a series of Particle Verbs and the Base Verbs they are derived from. We also investigate the influence of further linguistic factors, such as the ambiguity and the overall frequency of the verbs and a syntactically separate occurrences of verbs and particles that causes difficulties for the correct lemmatization of Particle Verbs. We analyse in how far these factors may influence the success with which the compositionality of the Particle Verbs may be predicted.</abstract>
    </paper>
    <paper id="705">
      <author><first>Anik</first><last>Dey</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <title>A <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Code-Switching Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/922_Paper.pdf</url>
      <abstract>The aim of this paper is to investigate the rules and constraints of code-switching (CS) in Hindi-English mixed language data. In this paper, well discuss how we collected the mixed language corpus. This corpus is primarily made up of student interview speech. The speech was manually transcribed and verified by bilingual speakers of Hindi and English. The code-switching cases in the corpus are discussed and the reasons for code-switching are explained.</abstract>
    </paper>
    <paper id="706">
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Di</first><last>Wang</last></author>
      <author><first>Keith</first><last>Suderman</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>Jonathan</first><last>Wright</last></author>
      <title>The Language Application Grid</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/926_Paper.pdf</url>
      <abstract>The Language Application (LAPPS) Grid project is establishing a framework that enables language service discovery, composition, and reuse and promotes sustainability, manageability, usability, and interoperability of natural language Processing (NLP) components. It is based on the service-oriented architecture (SOA), a more recent, web-oriented version of the pipeline architecture that has long been used in NLP for sequencing loosely-coupled linguistic analyses. The LAPPS Grid provides access to basic NLP processing tools and resources and enables pipelining such tools to create custom NLP applications, as well as composite services such as question answering and machine translation together with language resources such as mono- and multi-lingual corpora and lexicons that support NLP. The transformative aspect of the LAPPS Grid is that it orchestrates access to and deployment of language resources and processing functions available from servers around the globe and enables users to add their own language resources, services, and even service grids to satisfy their particular needs.</abstract>
    </paper>
    <paper id="707">
      <author><first>George</first><last>Christodoulides</last></author>
      <author><first>Mathieu</first><last>Avanzi</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <title><fixed-case>D</fixed-case>is<fixed-case>M</fixed-case>o: A Morphosyntactic, Disfluency and Multi-Word Unit Annotator. An Evaluation on a Corpus of <fixed-case>F</fixed-case>rench Spontaneous and Read Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/929_Paper.pdf</url>
      <abstract>We present DisMo, a multi-level annotator for spoken language corpora that integrates part-of-speech tagging with basic disfluency detection and annotation, and multi-word unit recognition. DisMo is a hybrid system that uses a combination of lexical resources, rules, and statistical models based on Conditional Random Fields (CRF). In this paper, we present the first public version of DisMo for French. The system is trained and its performance evaluated on a 57k-token corpus, including different varieties of French spoken in three countries (Belgium, France and Switzerland). DisMo supports a multi-level annotation scheme, in which the tokenisation to minimal word units is complemented with multi-word unit groupings (each having associated POS tags), as well as separate levels for annotating disfluencies and discourse phenomena. We present the systems architecture, linguistic resources and its hierarchical tag-set. Results show that DisMo achieves a precision of 95% (finest tag-set) to 96.8% (coarse tag-set) in POS-tagging non-punctuated, sound-aligned transcriptions of spoken French, while also offering substantial possibilities for automated multi-level annotation.</abstract>
    </paper>
    <paper id="708">
      <author><first>Trang Mai</first><last>Xuan</last></author>
      <author><first>Yohei</first><last>Murakami</last></author>
      <author><first>Donghui</first><last>Lin</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <title>Integration of Workflow and Pipeline for Language Service Composition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/930_Paper.pdf</url>
      <abstract>Integrating language resources and language services is a critical part of building natural language processing applications. Service workflow and processing pipeline are two approaches for sharing and combining language resources. Workflow languages focus on expressive power of the languages to describe variety of workflow patterns to meet users’ needs. Users can combine those language services in service workflows to meet their requirements. The workflows can be accessible in distributed manner and can be invoked independently of the platforms. However, workflow languages lack of pipelined execution support to improve performance of workflows. Whereas, the processing pipeline provides a straightforward way to create a sequence of linguistic processing to analyze large amounts of text data. It focuses on using pipelined execution and parallel execution to improve throughput of pipelines. However, the resulting pipelines are standalone applications, i.e., software tools that are accessible only via local machine and that can only be run with the processing pipeline platforms. In this paper we propose an integration framework of the two approaches so that each offests the disadvantages of the other. We then present a case study wherein two representative frameworks, the Language Grid and UIMA, are integrated.</abstract>
    </paper>
    <paper id="709">
      <author><first>Klim</first><last>Peshkov</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <title>Segmentation evaluation metrics, a comparison grounded on prosodic and discourse units</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/931_Paper.pdf</url>
      <abstract>Knowledge on evaluation metrics and best practices of using them have improved fast in the recent years Fort et al. (2012). However, the advances concern mostly evaluation of classification related tasks. Segmentation tasks have received less attention. Nevertheless, there are crucial in a large number of linguistic studies. A range of metrics is available (F-score on boundaries, F-score on units, WindowDiff ((WD), Boundary Similarity (BS) but it is still relatively difficult to interpret these metrics on various linguistic segmentation tasks, such as prosodic and discourse segmentation. In this paper, we consider real segmented datasets (introduced in Peshkov et al. (2012)) as references which we deteriorate in different ways (random addition of boundaries, random removal boundaries, near-miss errors introduction). This provide us with various measures on controlled datasets and with an interesting benchmark for various linguistic segmentation tasks.</abstract>
    </paper>
    <paper id="710">
      <author><first>Andrea</first><last>Abel</last></author>
      <author><first>Aivars</first><last>Glaznieks</last></author>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Egon</first><last>Stemle</last></author>
      <title><fixed-case>K</fixed-case>o<fixed-case>K</fixed-case>o: an <fixed-case>L</fixed-case>1 Learner Corpus for <fixed-case>G</fixed-case>erman</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/934_Paper.pdf</url>
      <abstract>We introduce the KoKo corpus, a collection of German L1 learner texts annotated with learner errors, along with the methods and tools used in its construction and evaluation. The corpus contains both texts and corresponding survey information from 1,319 pupils and amounts to around 716,000 tokens. The evaluation of the performed transcriptions and annotations shows an accuracy of orthographic error annotations of approximately 80% as well as high accuracies of transcriptions (&gt;99%), automatic tokenisation (&gt;99%), sentence splitting (&gt;96%) and POS-tagging (&gt;94%). The KoKo corpus will be published at the end of 2014. It will be the first accessible linguistically annotated German L1 learner corpus and a valuable source for research on L1 learner language as well as for teachers of German as L1, in particular with regards to writing skills.</abstract>
    </paper>
    <paper id="711">
      <author><first>Petra</first><last>Barančíková</last></author>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <author><first>Aleš</first><last>Tamchyna</last></author>
      <title>Improving Evaluation of <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech <fixed-case>MT</fixed-case> through Paraphrasing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/935_Paper.pdf</url>
      <abstract>In this paper, we present a method of improving the accuracy of machine translation evaluation of Czech sentences. Given a reference sentence, our algorithm transforms it by targeted paraphrasing into a new synthetic reference sentence that is closer in wording to the machine translation output, but at the same time preserves the meaning of the original reference sentence. Grammatical correctness of the new reference sentence is provided by applying Depfix on newly created paraphrases. Depfix is a system for post-editing English-to-Czech machine translation outputs. We adjusted it to fix the errors in paraphrased sentences. Due to a noisy source of our paraphrases, we experiment with adding word alignment. However, the alignment reduces the number of paraphrases found and the best results were achieved by a simple greedy method with only one-word paraphrases thanks to their intensive filtering. BLEU scores computed using these new reference sentences show significantly higher correlation with human judgment than scores computed on the original reference sentences.</abstract>
    </paper>
    <paper id="712">
      <author><first>Erik</first><last>Faessler</last></author>
      <author><first>Johannes</first><last>Hellrich</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <title>Disclose Models, Hide the Data - How to Make Use of Confidential Corpora without Seeing Sensitive Raw Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/936_Paper.pdf</url>
      <abstract>Confidential corpora from the medical, enterprise, security or intelligence domains often contain sensitive raw data which lead to severe restrictions as far as the public accessibility and distribution of such language resources are concerned. The enforcement of strict mechanisms of data protection consitutes a serious barrier for progress in language technology (products) in such domains, since these data are extremely rare or even unavailable for scientists and developers not directly involved in the creation and maintenance of such resources. In order to by-pass this problem, we here propose to distribute trained language models which were derived from such resources as a substitute for the original confidential raw data which remain hidden to the outside world. As an example, we exploit the access-protected German-language medical FRAMED corpus from which we generate and distribute models for sentence splitting, tokenization and POS tagging based on software taken from OPENNLP, NLTK and JCORE, our own UIMA-based text analytics pipeline.</abstract>
    </paper>
    <paper id="713">
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Ananthakrishnan</first><last>Ramanathan</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Karthik</first><last>Visweswariah</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <title>When Transliteration Met Crowdsourcing : An Empirical Study of Transliteration via Crowdsourcing using Efficient, Non-redundant and Fair Quality Control</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/94_Paper.pdf</url>
      <abstract>Sufficient parallel transliteration pairs are needed for training state of the art transliteration engines. Given the cost involved, it is often infeasible to collect such data using experts. Crowdsourcing could be a cheaper alternative, provided that a good quality control (QC) mechanism can be devised for this task. Most QC mechanisms employed in crowdsourcing are aggressive (unfair to workers) and expensive (unfair to requesters). In contrast, we propose a low-cost QC mechanism which is fair to both workers and requesters. At the heart of our approach, lies a rule based Transliteration Equivalence approach which takes as input a list of vowels in the two languages and a mapping of the consonants in the two languages. We empirically show that our approach outperforms other popular QC mechanisms (\textit{viz.}, consensus and sampling) on two vital parameters : (i) fairness to requesters (lower cost per correct transliteration) and (ii) fairness to workers (lower rate of rejecting correct answers). Further, as an extrinsic evaluation we use the standard NEWS 2010 test set and show that such quality controlled crowdsourced data compares well to expert data when used for training a transliteration engine.</abstract>
    </paper>
    <paper id="714">
      <author><first>Frederik</first><last>Baumgardt</last></author>
      <author><first>Giuseppe</first><last>Celano</last></author>
      <author><first>Gregory R.</first><last>Crane</last></author>
      <author><first>Stella</first><last>Dee</last></author>
      <author><first>Maryam</first><last>Foradi</last></author>
      <author><first>Emily</first><last>Franzini</last></author>
      <author><first>Greta</first><last>Franzini</last></author>
      <author><first>Monica</first><last>Lent</last></author>
      <author><first>Maria</first><last>Moritz</last></author>
      <author><first>Simona</first><last>Stoyanova</last></author>
      <title>Open Philology at the <fixed-case>U</fixed-case>niversity of <fixed-case>L</fixed-case>eipzig</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/940_Paper.pdf</url>
      <abstract>The Open Philology Project at the University of Leipzig aspires to re-assert the value of philology in its broadest sense. Philology signifies the widest possible use of the linguistic record to enable a deep understanding of the complete lived experience of humanity. Pragmatically, we focus on Greek and Latin because (1) substantial collections and services are already available within these languages, (2) substantial user communities exist (c. 35,000 unique users a month at the Perseus Digital Library), and (3) a European-based project is better positioned to process extensive cultural heritage materials in these languages rather than in Chinese or Sanskrit. The Open Philology Project has been designed with the hope that it can contribute to any historical language that survives within the human record. It includes three tasks: (1) the creation of an open, extensible, repurposable collection of machine-readable linguistic sources; (2) the development of dynamic textbooks that use annotated corpora to customize the vocabulary and grammar of texts that learners want to read, and at the same time engage students in collaboratively producing new annotated data; (3) the establishment of new workflows for, and forms of, publication, from individual annotations with argumentation to traditional publications with integrated machine-actionable data.</abstract>
    </paper>
    <paper id="715">
      <author><first>Marco</first><last>Del Tredici</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <title>A Modular System for Rule-based Text Categorisation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/941_Paper.pdf</url>
      <abstract>We introduce a modular rule-based approach to text categorisation which is more flexible and less time consuming to build than a standard rule-based system because it works with a hierarchical structure and allows for re-usability of rules. When compared to currently more wide-spread machine learning models on a case study, our modular system shows competitive results, and it has the advantage of reducing manual effort over time, since only fewer rules must be written when moving to a (partially) new domain, while annotation of training data is always required in the same amount.</abstract>
    </paper>
    <paper id="716">
      <author><first>Najeh</first><last>Hajlaoui</last></author>
      <author><first>David</first><last>Kolovratnik</last></author>
      <author><first>Jaakko</first><last>Väyrynen</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <author><first>Daniel</first><last>Varga</last></author>
      <title><fixed-case>DCEP</fixed-case> -Digital Corpus of the <fixed-case>E</fixed-case>uropean Parliament</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/943_Paper.pdf</url>
      <abstract>We are presenting a new highly multilingual document-aligned parallel corpus called DCEP - Digital Corpus of the European Parliament. It consists of various document types covering a wide range of subject domains. With a total of 1.37 billion words in 23 languages (253 language pairs), gathered in the course of ten years, this is the largest single release of documents by a European Union institution. DCEP contains most of the content of the European Parliament’s official Website. It includes different document types produced between 2001 and 2012, excluding only the documents already exist in the Europarl corpus to avoid overlapping. We are presenting the typical acquisition steps of the DCEP corpus: data access, document alignment, sentence splitting, normalisation and tokenisation, and sentence alignment efforts. The sentence-level alignment is still in progress but based on some first experiments; we showed that DCEP is very useful for NLP applications, in particular for Statistical Machine Translation.</abstract>
    </paper>
    <paper id="717">
      <author><first>Joseph</first><last>Mariani</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Gil</first><last>Francopoulo</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Marine</first><last>Delaborde</last></author>
      <title>Facing the Identification Problem in Language-Related Scientific Data Analysis.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/945_Paper.pdf</url>
      <abstract>This paper describes the problems that must be addressed when studying large amounts of data over time which require entity normalization applied not to the usual genres of news or political speech, but to the genre of academic discourse about language resources, technologies and sciences. It reports on the normalization processes that had to be applied to produce data usable for computing statistics in three past studies on the LRE Map, the ISCA Archive and the LDC Bibliography. It shows the need for human expertise during normalization and the necessity to adapt the work to the study objectives. It investigates possible improvements for reducing the workload necessary to produce comparable results. Through this paper, we show the necessity to define and agree on international persistent and unique identifiers.</abstract>
    </paper>
    <paper id="718">
      <author><first>Mariette</first><last>Soury</last></author>
      <author><first>Laurence</first><last>Devillers</last></author>
      <title>Smile and Laughter in Human-Machine Interaction: a study of engagement</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/947_Paper.pdf</url>
      <abstract>This article presents a corpus featuring adults playing games in interaction with machine trying to induce laugh. This corpus was collected during Interspeech 2013 in Lyon to study behavioral differences correlated to different personalities and cultures. We first present the collection protocol, then the corpus obtained and finally different quantitative and qualitative measures. Smiles and laughs are types of affect bursts which are defined as short emotional non-speech expressions. Here we correlate smile and laugh with personality traits and cultural background. Our final objective is to propose a measure of engagement deduced from those affect bursts.</abstract>
    </paper>
    <paper id="719">
      <author><first>Livio</first><last>Robaldo</last></author>
      <author><first>Guido</first><last>Boella</last></author>
      <author><first>Luigi</first><last>Di Caro</last></author>
      <author><first>Andrea</first><last>Violato</last></author>
      <title>Exploiting networks in Law</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/95_Paper.pdf</url>
      <abstract>In this paper we first introduce the working context related to the understanding of an heterogeneous network of references contained in the Italian regulatory framework. We then present an extended analysis of a large network of laws, providing several types of analytical evaluation that can be used within a legal management system for understanding the data through summarization, visualization, and browsing. In the legal domain, yet several tasks are strictly supervised by humans, with strong consumption of time and energy that would dramatically drop with the help of automatic or semi-automatic supporting tools. We overview different techniques and methodologies explaining how they can be helpful in actual scenarios.</abstract>
    </paper>
    <paper id="720">
      <author><first>Kristín</first><last>Bjarnadóttir</last></author>
      <author><first>Jón</first><last>Daðason</last></author>
      <title>Utilizing constituent structure for compound analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/954_Paper.pdf</url>
      <abstract>Compounding is extremely productive in Icelandic and multi-word compounds are common. The likelihood of finding previously unseen compounds in texts is thus very high, which makes out-of-vocabulary words a problem in the use of NLP tools. The tool de-scribed in this paper splits Icelandic compounds and shows their binary constituent structure. The probability of a constituent in an unknown (or unanalysed) compound forming a combined constituent with either of its neighbours is estimated, with the use of data on the constituent structure of over 240 thousand compounds from the Database of Modern Icelandic Inflection, and word frequencies from Íslenskur orðasjóður, a corpus of approx. 550 million words. Thus, the structure of an unknown compound is derived by com-parison with compounds with partially the same constituents and similar structure in the training data. The granularity of the split re-turned by the decompounder is important in tasks such as semantic analysis or machine translation, where a flat (non-structured) se-quence of constituents is insufficient.</abstract>
    </paper>
    <paper id="721">
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Behrang</first><last>Mohit</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Ossama</first><last>Obeid</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <author><first>Noura</first><last>Farra</last></author>
      <author><first>Sarah</first><last>Alkuhlani</last></author>
      <author><first>Kemal</first><last>Oflazer</last></author>
      <title>Large Scale <fixed-case>A</fixed-case>rabic Error Annotation: Guidelines and Framework</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/956_Paper.pdf</url>
      <abstract>We present annotation guidelines and a web-based annotation framework developed as part of an effort to create a manually annotated Arabic corpus of errors and corrections for various text types. Such a corpus will be invaluable for developing Arabic error correction tools, both for training models and as a gold standard for evaluating error correction algorithms. We summarize the guidelines we created. We also describe issues encountered during the training of the annotators, as well as problems that are specific to the Arabic language that arose during the annotation process. Finally, we present the annotation tool that was developed as part of this project, the annotation pipeline, and the quality of the resulting annotations.</abstract>
    </paper>
    <paper id="722">
      <author><first>Thomas</first><last>Pellegrini</last></author>
      <author><first>Vahid</first><last>Hedayati</last></author>
      <author><first>Angela</first><last>Costa</last></author>
      <title>El-<fixed-case>WOZ</fixed-case>: a client-server wizard-of-oz interface</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/959_Paper.pdf</url>
      <abstract>In this paper, we present a speech recording interface developed in the context of a project on automatic speech recognition for elderly native speakers of European Portuguese. In order to collect spontaneous speech in a situation of interaction with a machine, this interface was designed as a Wizard-of-Oz (WOZ) plateform. In this setup, users interact with a fake automated dialog system controled by a human wizard. It was implemented as a client-server application and the subjects interact with a talking head. The human wizard chooses pre-defined questions or sentences in a graphical user interface, which are then synthesized and spoken aloud by the avatar on the client side. A small spontaneous speech corpus was collected in a daily center. Eight speakers between 75 and 90 years old were recorded. They appreciated the interface and felt at ease with the avatar. Manual orthographic transcriptions were created for the total of about 45 minutes of speech.</abstract>
    </paper>
    <paper id="723">
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <title>Parsing <fixed-case>C</fixed-case>hinese Synthetic Words with a Character-based Dependency Model</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/96_Paper.pdf</url>
      <abstract>Synthetic word analysis is a potentially important but relatively unexplored problem in Chinese natural language processing. Two issues with the conventional pipeline methods involving word segmentation are (1) the lack of a common segmentation standard and (2) the poor segmentation performance on OOV words. These issues may be circumvented if we adopt the view of character-based parsing, providing both internal structures to synthetic words and global structure to sentences in a seamless fashion. However, the accuracy of synthetic word parsing is not yet satisfactory, due to the lack of research. In view of this, we propose and present experiments on several synthetic word parsers. Additionally, we demonstrate the usefulness of incorporating large unlabelled corpora and a dictionary for this task. Our parsers significantly outperform the baseline (a pipeline method).</abstract>
    </paper>
    <paper id="724">
      <author><first>Mohamed</first><last>Ben Jannet</last></author>
      <author><first>Martine</first><last>Adda-Decker</last></author>
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Juliette</first><last>Kahn</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <title><fixed-case>ETER</fixed-case> : a new metric for the evaluation of hierarchical named entity recognition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/960_Paper.pdf</url>
      <abstract>This paper addresses the question of hierarchical named entity evaluation. In particular, we focus on metrics to deal with complex named entity structures as those introduced within the QUAERO project. The intended goal is to propose a smart way of evaluating partially correctly detected complex entities, beyond the scope of traditional metrics. None of the existing metrics are fully adequate to evaluate the proposed QUAERO task involving entity detection, classification and decomposition. We are discussing the strong and weak points of the existing metrics. We then introduce a new metric, the Entity Tree Error Rate (ETER), to evaluate hierarchical and structured named entity detection, classification and decomposition. The ETER metric builds upon the commonly accepted SER metric, but it takes the complex entity structure into account by measuring errors not only at the slot (or complex entity) level but also at a basic (atomic) entity level. We are comparing our new metric to the standard one using first some examples and then a set of real data selected from the ETAPE evaluation results.</abstract>
    </paper>
    <paper id="725">
      <author><first>Jun</first><last>Araki</last></author>
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <title>Detecting Subevent Structure for Event Coreference Resolution</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/963_Paper.pdf</url>
      <abstract>In the task of event coreference resolution, recent work has shown the need to perform not only full coreference but also partial coreference of events. We show that subevents can form a particular hierarchical event structure. This paper examines a novel two-stage approach to finding and improving subevent structures. First, we introduce a multiclass logistic regression model that can detect subevent relations in addition to full coreference. Second, we propose a method to improve subevent structure based on subevent clusters detected by the model. Using a corpus in the Intelligence Community domain, we show that the method achieves over 3.2 BLANC F1 gain in detecting subevent relations against the logistic regression model.</abstract>
    </paper>
    <paper id="726">
      <author><first>Kashif</first><last>Shah</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <title>An efficient and user-friendly tool for machine translation quality estimation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/964_Paper.pdf</url>
      <abstract>We present a new version of QUEST ― an open source framework for machine translation quality estimation ― which brings a number of improvements: (i) it provides a Web interface and functionalities such that non-expert users, e.g. translators or lay-users of machine translations, can get quality predictions (or internal features of the framework) for translations without having to install the toolkit, obtain resources or build prediction models; (ii) it significantly improves over the previous runtime performance by keeping resources (such as language models) in memory; (iii) it provides an option for users to submit the source text only and automatically obtain translations from Bing Translator; (iv) it provides a ranking of multiple translations submitted by users for each source text according to their estimated quality. We exemplify the use of this new version through some experiments with the framework.</abstract>
    </paper>
    <paper id="727">
      <author><first>Alexandra</first><last>Balahur</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <author><first>Jose-Manuel</first><last>Perea-Ortega</last></author>
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <author><first>Dilek</first><last>Küçük</last></author>
      <author><first>Vanni</first><last>Zavarella</last></author>
      <author><first>Adil</first><last>El Ghali</last></author>
      <title>Resource Creation and Evaluation for Multilingual Sentiment Analysis in Social Media Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/965_Paper.pdf</url>
      <abstract>This paper presents an evaluation of the use of machine translation to obtain and employ data for training multilingual sentiment classifiers. We show that the use of machine translated data obtained similar results as the use of native-speaker translations of the same data. Additionally, our evaluations pinpoint to the fact that the use of multilingual data, including that obtained through machine translation, leads to improved results in sentiment classification. Finally, we show that the performance of the sentiment classifiers built on machine translated data can be improved using original data from the target language and that even a small amount of such texts can lead to significant growth in the classification performance.</abstract>
    </paper>
    <paper id="728">
      <author><first>Joachim</first><last>Bingel</last></author>
      <author><first>Thomas</first><last>Haider</last></author>
      <title>Named Entity Tagging a Very Large Unbalanced Corpus: Training and Evaluating <fixed-case>NE</fixed-case> Classifiers</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/967_Paper.pdf</url>
      <abstract>We describe a systematic and application-oriented approach to training and evaluating named entity recognition and classification (NERC) systems, the purpose of which is to identify an optimal system and to train an optimal model for named entity tagging DeReKo, a very large general-purpose corpus of contemporary German (Kupietz et al., 2010). DeReKo ‘s strong dispersion wrt. genre, register and time forces us to base our decision for a specific NERC system on an evaluation performed on a representative sample of DeReKo instead of performance figures that have been reported for the individual NERC systems when evaluated on more uniform and less diverse data. We create and manually annotate such a representative sample as evaluation data for three different NERC systems, for each of which various models are learnt on multiple training data. The proposed sampling method can be viewed as a generally applicable method for sampling evaluation data from an unbalanced target corpus for any sort of natural language processing.</abstract>
    </paper>
    <paper id="729">
      <author><first>Ophélie</first><last>Lacroix</last></author>
      <author><first>Denis</first><last>Béchet</last></author>
      <title>Validation Issues induced by an Automatic Pre-Annotation Mechanism in the Building of Non-projective Dependency Treebanks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/970_Paper.pdf</url>
      <abstract>In order to build large dependency treebanks using the CDG Lab, a grammar-based dependency treebank development tool, an annotator usually has to fill a selection form before parsing. This step is usually necessary because, otherwise, the search space is too big for long sentences and the parser fails to produce at least one solution. With the information given by the annotator on the selection form the parser can produce one or several dependency structures and the annotator can proceed by adding positive or negative annotations on dependencies and launching iteratively the parser until the right dependency structure has been found. However, the selection form is sometimes difficult and long to fill because the annotator must have an idea of the result before parsing. The CDG Lab proposes to replace this form by an automatic pre-annotation mechanism. However, this model introduces some issues during the annotation phase that do not exist when the annotator uses a selection form. The article presents those issues and proposes some modifications of the CDG Lab in order to use effectively the automatic pre-annotation mechanism.</abstract>
    </paper>
    <paper id="730">
      <author><first>Renlong</first><last>Ai</last></author>
      <author><first>Marcela</first><last>Charfuelan</last></author>
      <title><fixed-case>MAT</fixed-case>: a tool for <fixed-case>L</fixed-case>2 pronunciation errors annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/971_Paper.pdf</url>
      <abstract>In the area of Computer Assisted Language Learning(CALL), second language (L2) learners spoken data is an important resource for analysing and annotating typical L2 pronunciation errors. The annotation of L2 pronunciation errors in spoken data is not an easy task though, normally it requires manual annotation from trained linguists or phoneticians. In order to facilitate this task, in this paper, we present the MAT tool, a web-based tool intended to facilitate the annotation of L2 learners’ pronunciation errors at various levels. The tool has been designed taking into account recent studies on error detection in pronunciation training. It also aims at providing an easy and fast annotation process via a comprehensive and friendly user interface. The tool is based on the MARY TTS open source platform, from which it uses the components: text analyser (tokeniser, syllabifier, phonemiser), phonetic aligner and speech signal processor. Annotation results at sentence, word, syllable and phoneme levels are stored in XML format. The tool is currently under evaluation with a L2 learners spoken corpus recorded in the SPRINTER (Language Technology for Interactive, Multi-Media Online Language Learning) project.</abstract>
    </paper>
    <paper id="731">
      <author><first>Kalliopi</first><last>Zervanou</last></author>
      <author><first>Elias</first><last>Iosif</last></author>
      <author><first>Alexandros</first><last>Potamianos</last></author>
      <title>Word Semantic Similarity for Morphologically Rich Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/973_Paper.pdf</url>
      <abstract>In this work, we investigate the role of morphology on the performance of semantic similarity for morphologically rich languages, such as German and Greek. The challenge in processing languages with richer morphology than English, lies in reducing estimation error while addressing the semantic distortion introduced by a stemmer or a lemmatiser. For this purpose, we propose a methodology for selective stemming, based on a semantic distortion metric. The proposed algorithm is tested on the task of similarity estimation between words using two types of corpus-based similarity metrics: co-occurrence-based and context-based. The performance on morphologically rich languages is boosted by stemming with the context-based metric, unlike English, where the best results are obtained by the co-occurrence-based metric. A key finding is that the estimation error reduction is different when a word is used as a feature, rather than when it is used as a target word.</abstract>
    </paper>
    <paper id="732">
      <author><first>Joshua</first><last>Elliot</last></author>
      <author><first>Logan</first><last>Kearsley</last></author>
      <author><first>Jason</first><last>Housley</last></author>
      <author><first>Alan</first><last>Melby</last></author>
      <title><fixed-case>L</fixed-case>ex<fixed-case>T</fixed-case>erm Manager: Design for an Integrated Lexicography and Terminology System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/975_Paper.pdf</url>
      <abstract>We present a design for a multi-modal database system for lexical information that can be accessed in either lexicographical or terminological views. The use of a single merged data model makes it easy to transfer common information between termbases and dictionaries, thus facilitating information sharing and re-use. Our combined model is based on the LMF and TMF metamodels for lexicographical and terminological databases and is compatible with both, thus allowing for the import of information from existing dictionaries and termbases, which may be transferred to the complementary view and re-exported. We also present a new Linguistic Configuration Model, analogous to a TBX XCS file, which can be used to specify multiple language-specific schemata for validating and understanding lexical information in a single database. Linguistic configurations are mutable and can be refined and evolved over time as understanding of documentary needs improves. The system is designed with a client-server architecture using the HTTP protocol, allowing for the independent implementation of multiple clients for specific use cases and easy deployment over the web.</abstract>
    </paper>
    <paper id="733">
      <author><first>Daniel</first><last>Peterson</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Shumin</first><last>Wu</last></author>
      <title>Focusing Annotation for Semantic Role Labeling</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/977_Paper.pdf</url>
      <abstract>Annotation of data is a time-consuming process, but necessary for many state-of-the-art solutions to NLP tasks, including semantic role labeling (SRL). In this paper, we show that language models may be used to select sentences that are more useful to annotate. We simulate a situation where only a portion of the available data can be annotated, and compare language model based selection against a more typical baseline of randomly selected data. The data is ordered using an off-the-shelf language modeling toolkit. We show that the least probable sentences provide dramatic improved system performance over the baseline, especially when only a small portion of the data is annotated. In fact, the lion’s share of the performance can be attained by annotating only 10-20% of the data. This result holds for training a model based on new annotation, as well as when adding domain-specific annotation to a general corpus for domain adaptation.</abstract>
    </paper>
    <paper id="734">
      <author><first>Emanuele</first><last>Lapponi</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Rune Lain</first><last>Knudsen</last></author>
      <title>Off-Road <fixed-case>LAF</fixed-case>: Encoding and Processing Annotations in <fixed-case>NLP</fixed-case> Workflows</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/978_Paper.pdf</url>
      <abstract>The Linguistic Annotation Framework (LAF) provides an abstract data model for specifying interchange representations to ensure interoperability among different annotation formats. This paper describes an ongoing effort to adapt the LAF data model as the interchange representation in complex workflows as used in the Language Analysis Portal (LAP), an on-line and large-scale processing service that is developed as part of the Norwegian branch of the Common Language Resources and Technology Infrastructure (CLARIN) initiative. Unlike several related on-line processing environments, which predominantly instantiate a distributed architecture of web services, LAP achives scalability to potentially very large data volumes through integration with the Norwegian national e-Infrastructure, and in particular job sumission to a capacity compute cluster. This setup leads to tighter integration requirements and also calls for efficient, low-overhead communication of (intermediate) processing results with workflows. We meet these demands by coupling the LAF data model with a lean, non-redundant JSON-based interchange format and integration of an agile and performant NoSQL database, allowing parallel access from cluster nodes, as the central repository of linguistic annotation.</abstract>
    </paper>
    <paper id="735">
      <author><first>Penny</first><last>Labropoulou</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Maria</first><last>Gavrilidou</last></author>
      <title>Developing a Framework for Describing Relations among Language Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/979_Paper.pdf</url>
      <abstract>In this paper, we study relations holding between language resources as implemented in activities concerned with their documentation. We envision the term language resources with an inclusive definition covering datasets (corpora, lexica, ontologies, grammars, etc.), tools (including web services, workflows, platforms etc.), related publications and documentation, specifications and guidelines. However, the scope of the paper is limited to relations holding for datasets and tools. The study fosuses on the META-SHARE infrastructure and the Linguistic Data Consortium and takes into account the ISOcat DCR relations. Based on this study, we propose a taxonomy of relations, discuss their semantics and provide specifications for their use in order to cater for semantic interoperability. Issues of granularity, redundancy in codification, naming conventions and semantics of the relations are presented.</abstract>
    </paper>
    <paper id="736">
      <author><first>Clément</first><last>de Groc</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <title>Evaluating Web-as-corpus Topical Document Retrieval with an Index of the <fixed-case>O</fixed-case>pen<fixed-case>D</fixed-case>irectory</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/980_Paper.pdf</url>
      <abstract>This article introduces a novel protocol and resource to evaluate Web-as-corpus topical document retrieval. To the contrary of previous work, our goal is to provide an automatic, reproducible and robust evaluation for this task. We rely on the OpenDirectory (DMOZ) as a source of topically annotated webpages and index them in a search engine. With this OpenDirectory search engine, we can then easily evaluate the impact of various parameters such as the number of seed terms, queries or documents, or the usefulness of various term selection algorithms. A first fully automatic evaluation is described and provides baseline performances for this task. The article concludes with practical information regarding the availability of the index and resource files.</abstract>
    </paper>
    <paper id="737">
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <title>Word Alignment-Based Reordering of Source Chunks in <fixed-case>PB</fixed-case>-<fixed-case>SMT</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/982_Paper.pdf</url>
      <abstract>Reordering poses a big challenge in statistical machine translation between distant language pairs. The paper presents how reordering between distant language pairs can be handled efficiently in phrase-based statistical machine translation. The problem of reordering between distant languages has been approached with prior reordering of the source text at chunk level to simulate the target language ordering. Prior reordering of the source chunks is performed in the present work by following the target word order suggested by word alignment. The testset is reordered using monolingual MT trained on source and reordered source. This approach of prior reordering of the source chunks was compared with pre-ordering of source words based on word alignments and the traditional approach of prior source reordering based on language-pair specific reordering rules. The effects of these reordering approaches were studied on an English--Bengali translation task, a language pair with different word order. From the experimental results it was found that word alignment based reordering of the source chunks is more effective than the other reordering approaches, and it produces statistically significant improvements over the baseline system on BLEU. On manual inspection we found significant improvements in terms of word alignments.</abstract>
    </paper>
    <paper id="738">
      <author><first>Frank</first><last>Landsbergen</last></author>
      <author><first>Carole</first><last>Tiberius</last></author>
      <author><first>Roderik</first><last>Dernison</last></author>
      <title><fixed-case>T</fixed-case>aalportaal: an online grammar of <fixed-case>D</fixed-case>utch and <fixed-case>F</fixed-case>risian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/983_Paper.pdf</url>
      <abstract>In this paper, we present the Taalportaal project. Taalportaal will create an online portal containing an exhaustive and fully searchable electronic reference of Dutch and Frisian phonology, morphology and syntax. Its content will be in English. The main aim of the project is to serve the scientific community by organizing, integrating and completing the grammatical knowledge of both languages, and to make this data accessible in an innovative way. The project is carried out by a consortium of four universities and research institutions. Content is generated in two ways: (1) by a group of authors who, starting from existing grammatical resources, write text directly in XML, and (2) by integrating the full Syntax of Dutch into the portal, after an automatic conversion from Word to XML. We discuss the projects workflow, content creation and management, the actual web application, and the way in which we plan to enrich the portals content, such as by crosslinking between topics and linking to external resources.</abstract>
    </paper>
    <paper id="739">
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Jon</first><last>Parker</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <author><first>Ophir</first><last>Frieder</last></author>
      <title>A Framework for Public Health Surveillance</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/985_Paper.pdf</url>
      <abstract>With the rapid growth of social media, there is increasing potential to augment traditional public health surveillance methods with data from social media. We describe a framework for performing public health surveillance on Twitter data. Our framework, which is publicly available, consists of three components that work together to detect health-related trends in social media: a concept extraction component for identifying health-related concepts, a concept aggregation component for identifying how the extracted health-related concepts relate to each other, and a trend detection component for determining when the aggregated health-related concepts are trending. We describe the architecture of the framework and several components that have been implemented in the framework, identify other components that could be used with the framework, and evaluate our framework on approximately 1.5 years of tweets. While it is difficult to determine how accurately a Twitter trend reflects a trend in the real world, we discuss the differences in trends detected by several different methods and compare flu trends detected by our framework to data from Google Flu Trends.</abstract>
    </paper>
    <paper id="740">
      <author><first>Zdeňka</first><last>Urešová</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <title>Multilingual Test Sets for Machine Translation of Search Queries for Cross-Lingual Information Retrieval in the Medical Domain</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/99_Paper.pdf</url>
      <abstract>This paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain. The data consists of the total of 1,508 real user queries in English translated to Czech, German, and French. We describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system.</abstract>
    </paper>
    <paper id="741">
      <author><first>Axel-Cyrille</first><last>Ngonga Ngomo</last></author>
      <author><first>Norman</first><last>Heino</last></author>
      <author><first>René</first><last>Speck</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last></author>
      <title>A tool suite for creating question answering benchmarks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/990_Paper.pdf</url>
      <abstract>We introduce the BIOASQ suite, a set of open-source Web tools for the creation, assessment and community-driven improvement of question answering benchmarks. The suite comprises three main tools: (1) the annotation tool supports the creation of benchmarks per se. In particular, this tool allows a team of experts to create questions and answers as well as to annotate the latter with documents, document snippets, RDF triples and ontology concepts. While the creation of questions is supported by different views and contextual information pertaining to the same question, the creation of answers is supported by the integration of several search engines and context information to facilitate the retrieval of the said answers as well as their annotation. (2) The assessment tool allows comparing several answers to the same question. Therewith, it can be used to assess the inter-annotator agreement as well as to manually evaluate automatically generated answers. (3) The third tool in the suite, the social network, aims to ensure the sustainability and iterative improvement of the benchmark by empowering communities of experts to provide insights on the questions in the benchmark. The BIOASQ suite has already been used successfully to create the 311 questions comprised in the BIOASQ question answering benchmark. It has also been evaluated by the experts who used it to create the BIOASQ benchmark.</abstract>
    </paper>
    <paper id="742">
      <author><first>Clément</first><last>de Groc</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Claude</first><last>de Loupy</last></author>
      <title>Thematic Cohesion: measuring terms discriminatory power toward themes</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/991_Paper.pdf</url>
      <abstract>We present a new measure of thematic cohesion. This measure associates each term with a weight representing its discriminatory power toward a theme, this theme being itself expressed by a list of terms (a thematic lexicon). This thematic cohesion criterion can be used in many applications, such as query expansion, computer-assisted translation, or iterative construction of domain-specific lexicons and corpora. The measure is computed in two steps. First, a set of documents related to the terms is gathered from the Web by querying a Web search engine. Then, we produce an oriented co-occurrence graph, where vertices are the terms and edges represent the fact that two terms co-occur in a document. This graph can be interpreted as a recommendation graph, where two terms occurring in a same document means that they recommend each other. This leads to using a random walk algorithm that assigns a global importance value to each vertex of the graph. After observing the impact of various parameters on those importance values, we evaluate their correlation with retrieval effectiveness.</abstract>
    </paper>
    <paper id="743">
      <author><first>Tatiana</first><last>Gornostay</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <title>Terminology Resources and Terminology Work Benefit from Cloud Services</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/992_Paper.pdf</url>
      <abstract>This paper presents the concept of the innovative platform TaaS Terminology as a Service. TaaS brings the benefits of cloud services to the user, in order to foster the creation of terminology resources and to maintain their up-to-datedness by integrating automated data extraction and user-supported clean-up of raw terminological data and sharing user-validated terminology. The platform is based on cutting-edge technologies, provides single-access-point terminology services, and facilitates the establishment of emerging trends beyond conventional praxis and static models in terminology work. A cloud-based, user-oriented, collaborative, portable, interoperable, and multilingual platform offers such terminology services as terminology project creation and sharing, data collection for translation lookup, user document upload and management, terminology extraction customisation and execution, raw terminological data management, validated terminological data export and reuse, and other terminology services.</abstract>
    </paper>
    <paper id="744">
      <author><first>Munshi</first><last>Asadullah</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <title>Bidirectionnal converter between syntactic annotations : from <fixed-case>F</fixed-case>rench Treebank Dependencies to <fixed-case>PASSAGE</fixed-case> annotations, and back</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/995_Paper.pdf</url>
      <abstract>We present here part of a bidirectional converter between the French Tree-bank Dependency (FTB - DEP) annotations into the PASSAGE format. FTB - DEP is the representation used by several freely available parsers and the PASSAGE annotation was used to hand-annotate a relatively large sized corpus, used as gold-standard in the PASSAGE evaluation campaigns. Our converter will give the means to evaluate these parsers on the PASSAGE corpus. We shall illustrate the mapping of important syntactic phenomena using the corpus made of the examples of the FTB - DEP annotation guidelines, which we have hand-annotated with PASSAGE annotations and used to compute quantitative performance measures on the FTB - DEP guidelines.n this paper we will briefly introduce the two annotation formats. Then, we detail the two converters, and the rules which have been written. The last part will detail the results we obtained on the phenomenon we mostly study, the passive forms. We evaluate the converters by a double conversion, from PASSAGE to CoN LL and back to PASSAGE. We will detailed in this paper the linguistic phenomenon we detail here, the passive form.</abstract>
    </paper>
    <paper id="745">
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Binyam</first><last>Gebre</last></author>
      <title><fixed-case>V</fixed-case>ar<fixed-case>C</fixed-case>lass: An Open-source Language Identification Tool for Language Varieties</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/996_Paper.pdf</url>
      <abstract>This paper presents VarClass, an open-source tool for language identification available both to be downloaded as well as through a graphical user-friendly interface. The main difference of VarClass in comparison to other state-of-the-art language identification tools is its focus on language varieties. General purpose language identification tools do not take language varieties into account and our work aims to fill this gap. VarClass currently contains language models for over 27 languages in which 10 of them are language varieties. We report an average performance of over 90.5% accuracy in a challenging dataset. More language models will be included in the upcoming months.</abstract>
    </paper>
    <paper id="746">
      <author><first>Achim</first><last>Rettinger</last></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Daša</first><last>Berović</last></author>
      <author><first>Danijela</first><last>Merkler</last></author>
      <author><first>Matea</first><last>Srebačić</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <title><fixed-case>RECSA</fixed-case>: Resource for Evaluating Cross-lingual Semantic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2014/pdf/998_Paper.pdf</url>
      <abstract>In recent years large repositories of structured knowledge (DBpedia, Freebase, YAGO) have become a valuable resource for language technologies, especially for the automatic aggregation of knowledge from textual data. One essential component of language technologies, which leverage such knowledge bases, is the linking of words or phrases in specific text documents with elements from the knowledge base (KB). We call this semantic annotation. In the same time, initiatives like Wikidata try to make those knowledge bases less language dependent in order to allow cross-lingual or language independent knowledge access. This poses a new challenge to semantic annotation tools which typically are language dependent and link documents in one language to a structured knowledge base grounded in the same language. Ultimately, the goal is to construct cross-lingual semantic annotation tools that can link words or phrases in one language to a structured knowledge database in any other language or to a language independent representation. To support this line of research we developed what we believe could serve as a gold standard Resource for Evaluating Cross-lingual Semantic Annotation (RECSA). We compiled a hand-annotated parallel corpus of 300 news articles in three languages with cross-lingual semantic groundings to the English Wikipedia and DBPedia. We hope that this new language resource, which is freely available, will help to establish a standard test set and methodology to comparatively evaluate cross-lingual semantic annotation technologies.</abstract>
    </paper>
  </volume>
</collection>
