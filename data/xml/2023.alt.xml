<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.alt">
  <volume id="1" ingest-date="2023-10-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of ALT2023: Ancient Language Translation Workshop</booktitle>
      <publisher>Asia-Pacific Association for Machine Translation</publisher>
      <address>Macau SAR, China</address>
      <month>September</month>
      <year>2023</year>
      <url hash="2ac19a64">2023.alt-1</url>
      <venue>alt</venue>
    </meta>
    <frontmatter>
      <url hash="68a7f134">2023.alt-1.0</url>
      <bibkey>alt-2023-alt2023</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an2023: Overview of the First International <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Translation Bakeoff</title>
      <author><first>Dongbo</first><last>Wang</last></author>
      <author><first>Litao</first><last>Lin</last></author>
      <author><first>Zhixiao</first><last>Zhao</last></author>
      <author><first>Wenhao</first><last>Ye</last></author>
      <author><first>Kai</first><last>Meng</last></author>
      <author><first>Wenlong</first><last>Sun</last></author>
      <author><first>Lianzhen</first><last>Zhao</last></author>
      <author><first>Xue</first><last>Zhao</last></author>
      <author><first>Si</first><last>Shen</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Li</last></author>
      <pages>1–14</pages>
      <abstract>This paper present the results of the First International Ancient Chinese Transalation Bakeoff (EvaHan), which is a shared task of the Ancient Language Translation Workshop (ALT2023) and a co-located event of the 19th Edition of the Machine Translation Summit 2023 (MTS 2023). We described the motivation for having an international shared contest, as well as the datasets and tracks. The contest consists of two modalities, closed and open. In the closed modality, the participants are only allowed to use the training data, the partic-ipating teams achieved the highest BLEU scores of 27.3315 and 1.1102 in the tasks of translating Ancient Chinese to Modern Chinese and translating Ancient Chinese to English, respectively. In the open mode, contestants can only use any available data and models. The participating teams achieved the highest BLEU scores of 29.6832 and 6.5493 in the ancient Chinese to modern and ancient Chinese to English tasks, respectively.</abstract>
      <url hash="b1ddbe82">2023.alt-1.1</url>
      <bibkey>wang-etal-2023-evahan2023</bibkey>
    </paper>
    <paper id="2">
      <title>The Ups and Downs of Training <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a-based models on Smaller Datasets for Translation Tasks from Classical <fixed-case>C</fixed-case>hinese into Modern Standard <fixed-case>M</fixed-case>andarin and <fixed-case>M</fixed-case>odern <fixed-case>E</fixed-case>nglish</title>
      <author><first>Stuart Michael</first><last>McManus</last></author>
      <author><first>Roslin</first><last>Liu</last></author>
      <author><first>Yuji</first><last>Li</last></author>
      <author><first>Leo</first><last>Tam</last></author>
      <author><first>Stephanie</first><last>Qiu</last></author>
      <author><first>Letian</first><last>Yu</last></author>
      <pages>15–22</pages>
      <abstract>The paper presents an investigation into the effectiveness of pre-trained language models, Siku-RoBERTa and RoBERTa, for Classical Chinese to Modern Standard Mandarin and Classical Chinese to English translation tasks. The English translation model resulted in unsatisfactory performance due to the small dataset, while the Modern Standard Mandarin model gave reasonable results.</abstract>
      <url hash="855e9bbb">2023.alt-1.2</url>
      <bibkey>mcmanus-etal-2023-ups</bibkey>
    </paper>
    <paper id="3">
      <title>Pre-trained Model In <fixed-case>A</fixed-case>ncient-<fixed-case>C</fixed-case>hinese-to-<fixed-case>M</fixed-case>odern-<fixed-case>C</fixed-case>hinese Machine Translation</title>
      <author><first>Jiahui</first><last>Wang</last></author>
      <author><first>Xuqin</first><last>Zhang</last></author>
      <author><first>Jiahuan</first><last>Li</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <pages>23–28</pages>
      <abstract>This paper presents an analysis of the pre-trained Transformer model Neural Machine Translation (NMT) for the Ancient-Chinese-to-Modern-Chinese machine translation task.</abstract>
      <url hash="9e4f5c9a">2023.alt-1.3</url>
      <bibkey>wang-etal-2023-pre-trained</bibkey>
    </paper>
    <paper id="4">
      <title>Some Trials on Ancient <fixed-case>M</fixed-case>odern <fixed-case>C</fixed-case>hinese Translation</title>
      <author><first>Li</first><last>Lin</last></author>
      <author><first>Xinyu</first><last>Hu</last></author>
      <pages>29–33</pages>
      <abstract>In this study, we explored various neural machine translation techniques for the task of translating ancient Chinese into modern Chinese. Our aim was to find an effective method for achieving accurate and reliable translation results. After experimenting with different approaches, we discovered that the method of concatenating adjacent sentences yielded the best performance among all the methods tested.</abstract>
      <url hash="f01059b6">2023.alt-1.4</url>
      <bibkey>lin-hu-2023-trials</bibkey>
    </paper>
    <paper id="5">
      <title>Istic Neural Machine Translation System for <fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an 2023</title>
      <author><first>Ningyuan</first><last>Deng</last></author>
      <author><first>Shuao</first><last>Guo</last></author>
      <author><first>Yanqing</first><last>He</last></author>
      <pages>34–42</pages>
      <abstract>This paper presents the system architecture and the technique details adopted by Institute of Scientific and Technical Information of China (ISTIC) in the evaluation of First Conference on EvaHan(2023). In this evaluation, ISTIC participated in two tasks of Ancient Chinese Machine Translation: Ancient Chinese to Modern Chinese and Ancient Chinese to English. The paper mainly elaborates the model framework and data processing methods adopted in ISTIC’s system. Finally a comparison and analysis of different machine translation systems are also given.</abstract>
      <url hash="416f48b4">2023.alt-1.5</url>
      <bibkey>deng-etal-2023-istic</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>BIT</fixed-case>-<fixed-case>ACT</fixed-case>: An <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Translation System Using Data Augmentation</title>
      <author><first>Li</first><last>Zeng</last></author>
      <author><first>Yanzhi</first><last>Tian</last></author>
      <author><first>Yingyu</first><last>Shan</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>43–47</pages>
      <abstract>This paper describes a translation model for ancient Chinese to modern Chinese and English for the Evahan 2023 competition, a subtask of the Ancient Language Translation 2023 challenge. During the training of our model, we applied various data augmentation techniques and used SiKu-RoBERTa as part of our model architecture. The results indicate that back translation improves the model’s performance, but double back translation introduces noise and harms the model’s performance. Fine-tuning on the original dataset can be helpful in solving the issue.</abstract>
      <url hash="3ac59c9d">2023.alt-1.6</url>
      <bibkey>zeng-etal-2023-bit</bibkey>
    </paper>
    <paper id="7">
      <title>Technical Report on <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Machine Translation Based on m<fixed-case>RASP</fixed-case> Model</title>
      <author><first>Wenjing</first><last>Liu</last></author>
      <author><first>Jing</first><last>Xie</last></author>
      <pages>48–54</pages>
      <abstract>Abstract: Objective This paper aims to improve the performance of machine translation of ancient Chinese classics, which can better promote the development of ancient books research and the spread of Chinese culture. Methods Based on the multilingual translation machine pre-training model of mRASP, the model was trained by fine-tuning the specific language pairs, namely a2m, and a2e, according to the two downstream tasks of classical Chinese translation into modern Chinese and classical Chinese translation into English, using the parallel corpus of ancient white and white and ancient English parallel corpus of Pre-Qin+ZiZhiTongJian, and the translation performance of the fine-tuning model was evaluated by BIEU evaluation index. Results The BIEU4 results of the three downstream tasks of 24_histories_a2m、Pre-Qin+ZiZhiTongJian_a2m、 Pre-Qin+ZiZhiTongJian_a2e were 17.38, 13.69 and 12.90 respectively.</abstract>
      <url hash="2d00f799">2023.alt-1.7</url>
      <bibkey>liu-xie-2023-technical</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>nchi<fixed-case>L</fixed-case>m: An Effective Classical-to-<fixed-case>M</fixed-case>odern <fixed-case>C</fixed-case>hinese Translation Model Leveraging bpe-drop and <fixed-case>S</fixed-case>iku<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Jiahui</first><last>Zhu</last></author>
      <author><first>Sizhou</first><last>Chen</last></author>
      <pages>55–60</pages>
      <abstract>In this paper, we present our submitted model for translating ancient to modern texts, which ranked sixth in the closed track of ancient Chinese in the 2nd International Review of Automatic Analysis of Ancient Chinese (EvaHan). Specifically, we employed two strategies to improve the translation from ancient to modern texts. First, we used bpe-drop to enhance the parallel corpus. Second, we use SikuRoBERTa to simultaneously initialize the translation model’s codec and reconstruct the bpe word list. In our experiments, we compare the baseline model, rdrop, pre-trained model, and parameter initialization methods. The experimental results show that the parameter initialization method in this paper significantly outperforms the baseline model in terms of performance, and its BLEU score reaches 21.75.</abstract>
      <url hash="245fd02d">2023.alt-1.8</url>
      <bibkey>zhu-chen-2023-anchilm</bibkey>
    </paper>
    <paper id="9">
      <title>Translating <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese to <fixed-case>M</fixed-case>odern <fixed-case>C</fixed-case>hinese at Scale: A Large Language Model-based Approach</title>
      <author><first>Jiahuan</first><last>Cao</last></author>
      <author><first>Dezhi</first><last>Peng</last></author>
      <author><first>Yongxin</first><last>Shi</last></author>
      <author><first>Zongyuan</first><last>Jiang</last></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <pages>61–69</pages>
      <abstract>Recently, the emergence of large language models (LLMs) has provided powerful foundation models for a wide range of natural language processing (NLP) tasks. However, the vast majority of the pre-training corpus for most existing LLMs is in English, resulting in their Chinese proficiency falling far behind that of English. Furthermore, ancient Chinese has a much larger vocabulary and less available corpus than modern Chinese, which significantly challenges the generalization capacity of existing LLMs. In this paper, we investigate the Ancient-Chinese-to-Modern-Chinese (A2M) translation using LLMs including LLaMA and Ziya. Specifically, to improve the understanding of Chinese texts, we explore the vocabulary expansion and incremental pre-training methods based on existing pre-trained LLMs. Subsequently, a large-scale A2M translation dataset with 4M pairs is utilized to finetune the LLMs.Experimental results demonstrate the effectiveness of the proposed method, especially with Ziya-13B, in translating ancient Chinese to modern Chinese. Moreover,we deeply analyze the performance of various LLMs with different strategies, which we believe can benefit further research on LLM-based A2M approaches.</abstract>
      <url hash="5cdd99cd">2023.alt-1.9</url>
      <bibkey>cao-etal-2023-translating</bibkey>
    </paper>
  </volume>
</collection>
