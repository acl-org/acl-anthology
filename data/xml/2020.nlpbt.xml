<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.nlpbt">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First International Workshop on Natural Language Processing Beyond Text</booktitle>
      <editor><first>Giuseppe</first><last>Castellucci</last></editor>
      <editor><first>Simone</first><last>Filice</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <editor><first>Erik</first><last>Cambria</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
      <venue>nlpbt</venue>
    </meta>
    <frontmatter>
      <url hash="54cecc5a">2020.nlpbt-1.0</url>
      <bibkey>nlpbt-2020-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Modulated Fusion using Transformer for Linguistic-Acoustic Emotion Recognition</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last></author>
      <author><first>Noé</first><last>Tits</last></author>
      <author><first>Stéphane</first><last>Dupont</last></author>
      <pages>1–10</pages>
      <abstract>This paper aims to bring a new lightweight yet powerful solution for the task of Emotion Recognition and Sentiment Analysis. Our motivation is to propose two architectures based on Transformers and modulation that combine the linguistic and acoustic inputs from a wide range of datasets to challenge, and sometimes surpass, the state-of-the-art in the field. To demonstrate the efficiency of our models, we carefully evaluate their performances on the IEMOCAP, MOSI, MOSEI and MELD dataset. The experiments can be directly replicated and the code is fully open for future researches.</abstract>
      <url hash="1d153a9c">2020.nlpbt-1.1</url>
      <doi>10.18653/v1/2020.nlpbt-1.1</doi>
      <video href="https://slideslive.com/38939779"/>
      <bibkey>delbrouck-etal-2020-modulated</bibkey>
      <pwccode url="https://github.com/jbdel/modulated_fusion_transformer" additional="false">jbdel/modulated_fusion_transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="2">
      <title>Multimodal Speech Recognition with Unstructured Audio Masking</title>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <author><first>Ramon</first><last>Sanabria</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>11–18</pages>
      <abstract>Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called RandWordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.</abstract>
      <url hash="fe4d2c57">2020.nlpbt-1.2</url>
      <doi>10.18653/v1/2020.nlpbt-1.2</doi>
      <video href="https://slideslive.com/38939780"/>
      <bibkey>srinivasan-etal-2020-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/speech-coco">SPEECH-COCO</pwcdataset>
    </paper>
    <paper id="3">
      <title>Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data</title>
      <author><first>Xinyu</first><last>Wang</last></author>
      <author><first>Xiaowen</first><last>Sun</last></author>
      <author><first>Tan</first><last>Yang</last></author>
      <author><first>Hongbo</first><last>Wang</last></author>
      <pages>19–29</pages>
      <abstract>Sarcasm detection in social media with text and image is becoming more challenging. Previous works of image-text sarcasm detection were mainly to fuse the summaries of text and image: different sub-models read the text and image respectively to get the summaries, and fuses the summaries. Recently, some multi-modal models based on the architecture of BERT are proposed such as ViLBERT. However, they can only be pretrained on the image-text data. In this paper, we propose an image-text model for sarcasm detection using the pretrained BERT and ResNet without any further pretraining. BERT and ResNet have been pretrained on much larger text or image data than image-text data. We connect the vector spaces of BERT and ResNet to utilize more data. We use the pretrained Multi-Head Attention of BERT to model the text and image. Besides, we propose a 2D-Intra-Attention to extract the relationships between words and images. In experiments, our model outperforms the state-of-the-art model.</abstract>
      <url hash="6afc9d0b">2020.nlpbt-1.3</url>
      <doi>10.18653/v1/2020.nlpbt-1.3</doi>
      <bibkey>wang-etal-2020-building</bibkey>
    </paper>
    <paper id="4">
      <title>A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos</title>
      <author><first>Frank F.</first><last>Xu</last></author>
      <author><first>Lei</first><last>Ji</last></author>
      <author><first>Botian</first><last>Shi</last></author>
      <author><first>Junyi</first><last>Du</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>30–40</pages>
      <abstract>Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.</abstract>
      <url hash="704e6eb6">2020.nlpbt-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="bc431023">2020.nlpbt-1.4.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.nlpbt-1.4</doi>
      <bibkey>xu-etal-2020-benchmark</bibkey>
      <pwccode url="https://github.com/frankxu2004/cooking-procedural-extraction" additional="false">frankxu2004/cooking-procedural-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coin">COIN</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/crosstask">CrossTask</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youcook2">YouCook2</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>IESTAC</fixed-case>: <fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>talian Parallel Corpus for End-to-End Speech-to-Text Machine Translation</title>
      <author><first>Giuseppe</first><last>Della Corte</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>41–50</pages>
      <abstract>We discuss a set of methods for the creation of IESTAC: a English-Italian speech and text parallel corpus designed for the training of end-to-end speech-to-text machine translation models and publicly released as part of this work. We first mapped English LibriVox audiobooks and their corresponding English Gutenberg Project e-books to Italian e-books with a set of three complementary methods. Then we aligned the English and the Italian texts using both traditional Gale-Church based alignment methods and a recently proposed tool to perform bilingual sentences alignment computing the cosine similarity of multilingual sentence embeddings. Finally, we forced the alignment between the English audiobooks and the English side of our textual parallel corpus with a text-to-speech and dynamic time warping based forced alignment tool. For each step, we provide the reader with a critical discussion based on detailed evaluation and comparison of the results of the different methods.</abstract>
      <url hash="4df45da5">2020.nlpbt-1.5</url>
      <doi>10.18653/v1/2020.nlpbt-1.5</doi>
      <bibkey>della-corte-stymne-2020-iestac</bibkey>
      <pwccode url="https://github.com/giuseppe-della-corte/iestac" additional="false">giuseppe-della-corte/iestac</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl-st">Europarl-ST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="6">
      <title>Unsupervised Keyword Extraction for Full-Sentence <fixed-case>VQA</fixed-case></title>
      <author><first>Kohei</first><last>Uehara</last></author>
      <author><first>Tatsuya</first><last>Harada</last></author>
      <pages>51–59</pages>
      <abstract>In the majority of the existing Visual Question Answering (VQA) research, the answers consist of short, often single words, as per instructions given to the annotators during dataset construction. This study envisions a VQA task for natural situations, where the answers are more likely to be sentences rather than single words. To bridge the gap between this natural VQA and existing VQA approaches, a novel unsupervised keyword extraction method is proposed. The method is based on the principle that the full-sentence answers can be decomposed into two parts: one that contains new information answering the question (i.e. keywords), and one that contains information already included in the question. Discriminative decoders were designed to achieve such decomposition, and the method was experimentally implemented on VQA datasets containing full-sentence answers. The results show that the proposed model can accurately extract the keywords without being given explicit annotations describing them.</abstract>
      <url hash="d30970d1">2020.nlpbt-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9b7f3175">2020.nlpbt-1.6.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.nlpbt-1.6</doi>
      <bibkey>uehara-harada-2020-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>MAST</fixed-case>: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention</title>
      <author><first>Aman</first><last>Khullar</last></author>
      <author><first>Udit</first><last>Arora</last></author>
      <pages>60–69</pages>
      <abstract>This paper presents MAST, a new model for Multimodal Abstractive Text Summarization that utilizes information from all three modalities – text, audio and video – in a multimodal video. Prior work on multimodal abstractive text summarization only utilized information from the text and video modalities. We examine the usefulness and challenges of deriving information from the audio modality and present a sequence-to-sequence trimodal hierarchical attention-based model that overcomes these challenges by letting the model pay more attention to the text modality. MAST outperforms the current state of the art model (video-text) by 2.51 points in terms of Content F1 score and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal language understanding.</abstract>
      <url hash="a8bf05bd">2020.nlpbt-1.7</url>
      <doi>10.18653/v1/2020.nlpbt-1.7</doi>
      <video href="https://slideslive.com/38939781"/>
      <bibkey>khullar-arora-2020-mast</bibkey>
      <pwccode url="https://github.com/amankhullar/mast" additional="false">amankhullar/mast</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="8">
      <title>Towards End-to-End In-Image Neural Machine Translation</title>
      <author><first>Elman</first><last>Mansimov</last></author>
      <author><first>Mitchell</first><last>Stern</last></author>
      <author><first>Mia</first><last>Chen</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Jakob</first><last>Uszkoreit</last></author>
      <author><first>Puneet</first><last>Jain</last></author>
      <pages>70–74</pages>
      <abstract>In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.</abstract>
      <url hash="4a0aba69">2020.nlpbt-1.8</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5e34d2a8">2020.nlpbt-1.8.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.nlpbt-1.8</doi>
      <video href="https://slideslive.com/38939782"/>
      <bibkey>mansimov-etal-2020-towards</bibkey>
    </paper>
    <paper id="9">
      <title>Reasoning Over History: Context Aware Visual Dialog</title>
      <author><first>Muhammad</first><last>Shah</last></author>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <pages>75–83</pages>
      <abstract>While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model’s performance particularly improved on questions that required coreference resolution.</abstract>
      <url hash="6697a871">2020.nlpbt-1.9</url>
      <doi>10.18653/v1/2020.nlpbt-1.9</doi>
      <video href="https://slideslive.com/38939783"/>
      <bibkey>shah-etal-2020-reasoning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr-dialog">CLEVR-Dialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
  </volume>
</collection>
