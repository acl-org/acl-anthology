<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.nodalida">
  <volume id="1" ingest-date="2025-03-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Joint 25th Nordic Conference on Computational Linguistics and 11th Baltic Conference on Human Language Technologies (NoDaLiDa/Baltic-HLT 2025)</booktitle>
      <editor><first>Richard</first><last>Johansson</last></editor>
      <editor><first>Sara</first><last>Stymne</last></editor>
      <publisher>University of Tartu Library</publisher>
      <address>Tallinn, Estonia</address>
      <month>march</month>
      <year>2025</year>
      <url hash="b31b40c7">2025.nodalida-1</url>
      <venue>nodalida</venue>
      <isbn>978-9908-53-109-0</isbn>
    </meta>
    <frontmatter>
      <url hash="32f44e44">2025.nodalida-1.0</url>
      <bibkey>nodalida-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Annotating and Classifying Direct Speech in Historical <fixed-case>Danish</fixed-case> and <fixed-case>Norwegian</fixed-case> Literary Texts</title>
      <author><first>Ali</first><last>Al-Laith</last></author>
      <author><first>Alexander</first><last>Conroy</last></author>
      <author><first>Kirstine Nielsen</first><last>Degn</last></author>
      <author><first>Jens</first><last>Bjerring-Hansen</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <pages>1–7</pages>
      <abstract>Analyzing direct speech in historical literary texts provides insights into character dynamics, narrative style, and discourse patterns. In late 19th century Danish and Norwegian fiction direct speech reflects characters’ social and geographical backgrounds. However, inconsistent typographic conventions in Scandinavian literature complicate computational methods for distinguishing direct speech from other narrative elements. To address this, we introduce an annotated dataset from the MeMo corpus, capturing speech markers and tags in Danish and Norwegian novels. We evaluate pre-trained language models for classifying direct speech, with results showing that a Danish Foundation Model (DFM), trained on extensive Danish data, has the highest performance. Finally, we conduct a classifier-assisted quantitative corpus analysis and find a downward trend in the prevalence of speech over time.</abstract>
      <url hash="f82d5345">2025.nodalida-1.1</url>
      <bibkey>al-laith-etal-2025-annotating</bibkey>
    </paper>
    <paper id="2">
      <title>Diachronic Analysis of Phrasal Verbs in <fixed-case>English</fixed-case> Scientific Writing</title>
      <author><first>Diego</first><last>Alves</last></author>
      <pages>8–16</pages>
      <abstract>Phrasal verbs (PVs) are a specific type of multi-word expressions and a specific feature of the English language. However, their usage in scientific prose is limited. Our study focuses on the analysis of phrasal verbs in the scientific domain using information theory methods to describe diachronic phenomena such as conventionalization and diversification regarding the usage of PVs. Thus, we analysed their developmental trajectory over time from the mid-17th century to the end of the 20th century by measuring the relative entropy (Kullback-Leibler divergence), predictability in context of the phrasal verbs particles (surprisal), and the paradigmatic variability using word embedding spaces. We were able to identify interesting phenomena such as the process of conventionalization over the 20th century and the peaks of diversification throughout the centuries.</abstract>
      <url hash="20ef9e7e">2025.nodalida-1.2</url>
      <bibkey>alves-2025-diachronic</bibkey>
    </paper>
    <paper id="3">
      <title>Applying and Optimising a Multi-Scale Probit Model for Cross-Source Text Complexity Classification and Ranking in <fixed-case>Swedish</fixed-case></title>
      <author><first>Elsa</first><last>Andersson</last></author>
      <author><first>Johan</first><last>Falkenjack</last></author>
      <author><first>Arne</first><last>Jönsson</last></author>
      <pages>17–27</pages>
      <abstract>We present results from using Probit models to classify and rank texts of varying complexity from multiple sources. We use multiple linguistic sources including Swedish easy-to-read books and investigate data augmentation and feature regularisation as optimisation methods for text complexity assessment. Multi-Scale and Single Scale Probit models are implemented using different ratios of training data, and then compared. Overall, the findings suggest that the Multi-Scale Probit model is an effective method for classifying text complexity and ranking new texts and could be used to improve the performance on small datasets as well as normalize datasets labelled using different scales.</abstract>
      <url hash="b6560132">2025.nodalida-1.3</url>
      <bibkey>andersson-etal-2025-applying</bibkey>
    </paper>
    <paper id="4">
      <title>Playing by the Rules: <fixed-case>A</fixed-case> Benchmark Set for Standardized <fixed-case>Icelandic</fixed-case> Orthography</title>
      <author><first>Bjarki</first><last>Ármannsson</last></author>
      <author><first>Hinrik</first><last>Hafsteinsson</last></author>
      <author><first>Jóhannes B.</first><last>Sigtryggsson</last></author>
      <author><first>Atli</first><last>Jasonarson</last></author>
      <author><first>Einar Freyr</first><last>Sigurðsson</last></author>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <pages>28–36</pages>
      <abstract>We present the Icelandic Standardization Benchmark Set: Spelling and Punctuation (IceStaBS:SP), a dataset designed to provide standardized text examples for Icelandic orthography. The dataset includes non-standard orthography examples and their standardized counterparts, along with detailed explanations based on official Icelandic spelling rules. IceStaBS:SP aims to support the development and evaluation of automatic spell and grammar checkers, particularly in educational settings. We evaluate various spell and grammar checkers using IceStaBS:SP, demonstrating its utility as a benchmarking tool and highlighting areas for future improvement.</abstract>
      <url hash="983f2f04">2025.nodalida-1.4</url>
      <bibkey>armannsson-etal-2025-playing</bibkey>
    </paper>
    <paper id="5">
      <title>An <fixed-case>Icelandic</fixed-case> Linguistic Benchmark for Large Language Models</title>
      <author><first>Bjarki</first><last>Ármannsson</last></author>
      <author><first>Finnur Ágúst</first><last>Ingimundarson</last></author>
      <author><first>Einar Freyr</first><last>Sigurðsson</last></author>
      <pages>37–47</pages>
      <abstract>This paper introduces a linguistic benchmark for Icelandic-language LLMs, the first of its kind manually constructed by native speakers. We report on the scores obtained by current state-of-the-art models, which indicate room for improvement, and discuss the theoretical problems involved in creating such a benchmark and scoring a model’s performance.</abstract>
      <url hash="3ead2b0a">2025.nodalida-1.5</url>
      <bibkey>armannsson-etal-2025-icelandic</bibkey>
    </paper>
    <paper id="6">
      <title>Transfer-Learning <fixed-case>German</fixed-case> Metaphors Inspired by Second Language Acquisition</title>
      <author><first>Maria</first><last>Berger</last></author>
      <pages>48–54</pages>
      <abstract>A major part of figurative meaning prediction is based on English-language training corpora. One strategy to apply techniques to languages other than English lies in applying transfer learning techniques to correct this imbalance. However, in previous studies we learned that the bilingual representations of current transformer models are incapable of encoding the deep semantic knowledge necessary for a transfer learning step, especially for metaphor prediction. Hence, inspired by second language acquisition, we attempt to improve German metaphor prediction in transfer learning by modifying the context windows of our input samples to align with lower readability indices achieving up to 13% higher F1 score.</abstract>
      <url hash="998260fe">2025.nodalida-1.6</url>
      <bibkey>berger-2025-transfer</bibkey>
    </paper>
    <paper id="7">
      <title>Comparative Concepts or Descriptive Categories: a <fixed-case>UD</fixed-case> Case study</title>
      <author><first>Matthieu Pierre</first><last>Boyer</last></author>
      <author><first>Mathieu</first><last>Dehouck</last></author>
      <pages>55–65</pages>
      <abstract>In this paper, we present a series of methods used to quantify the soundness of using the same names to annotate cases in different languages. We follow the idea described by Martin Haspelmath that descriptive categories and comparative concepts are different objects and we look at the necessary simplification taken by the Universal Dependencies project. We thus compare cases in closely related languages as belonging to commensurable descriptive categories. Then we look at the corresponding underlying comparative concepts. We finally looked at the possibility of assigning cases to adpositions.</abstract>
      <url hash="60210229">2025.nodalida-1.7</url>
      <bibkey>boyer-dehouck-2025-comparative</bibkey>
    </paper>
    <paper id="8">
      <title>Investigating the effectiveness of Data Augmentation and Contrastive Learning for Named Entity Recognition</title>
      <author><first>Noel</first><last>Chia</last></author>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>66–79</pages>
      <abstract>Data Augmentation (DA) and Contrastive Learning (CL) are widely used in NLP, but their potential for NER has not yet been investigated in detail. Existing work is mostly limited to zero- and few-shot scenarios where improvements over the baseline are easy to obtain. In this paper, we address this research gap by presenting a systematic evaluation of DA for NER on small, medium-sized and large datasets with coarse and fine-grained labels. We report results for a) DA only, b) DA in combination with supervised contrastive learning, and c) DA with transfer learning. Our results show that DA on its own fails to improve results over the baseline and that supervised CL works better on larger datasets while transfer learning is beneficial if the target dataset is very small. Finally, we investigate how contrastive learning affects the learned representations, based on dimensionality reduction and visualisation techniques, and show that CL mostly helps to separate named entities from non-entities.</abstract>
      <url hash="8008c5c8">2025.nodalida-1.8</url>
      <bibkey>chia-etal-2025-investigating</bibkey>
    </paper>
    <paper id="9">
      <title>Comparing Human and Machine Translations of Generative Language Model Evaluation Datasets</title>
      <author><first>Sander Bijl</first><last>de Vroe</last></author>
      <author><first>George</first><last>Stampoulidis</last></author>
      <author><first>Kai</first><last>Hakala</last></author>
      <author><first>Aku</first><last>Rouhe</last></author>
      <author><first>Mark</first><last>van Heeswijk</last></author>
      <author><first>Jussi</first><last>Karlgren</last></author>
      <pages>80–85</pages>
      <abstract>The evaluation of Large Language Models (LLMs) is one of the crucial current challenges in the field of Natural Language Processing (NLP) and becomes even more challenging in the multilingual setting. Since the majority of the community’s benchmarks exist only in English, test sets are now being machine translated at scale into dozens of languages. This work explores the feasibility of that approach, comparing a Finnish machine translation (MT) of ARC-Challenge with a new human translated version. Our findings suggest that since absolute scores are fairly close and model size rankings are preserved, machine translation is adequate in this case. Surprisingly, however, the datasets reverse the order of base models compared to their chat-finetuned counterparts.</abstract>
      <url hash="de4df007">2025.nodalida-1.9</url>
      <bibkey>de-vroe-etal-2025-comparing</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>GliLem</fixed-case>: <fixed-case>Leveraging</fixed-case> <fixed-case>GliNER</fixed-case> for Contextualized Lemmatization in <fixed-case>Estonian</fixed-case></title>
      <author><first>Aleksei</first><last>Dorkin</last></author>
      <author><first>Kairit</first><last>Sirts</last></author>
      <pages>86–97</pages>
      <abstract>We present GliLem—a novel hybrid lemmatization system for Estonian that enhances the highly accurate rule-based morphological analyzer Vabamorf with an external disambiguation module based on GliNER—an open vocabulary NER model that is able to match text spans with text labels in natural language. We leverage the flexibility of a pre-trained GliNER model to improve the lemmatization accuracy of Vabamorf by 10% compared to its original disambiguation module and achieve an improvement over the token classification-based baseline. To measure the impact of improvements in lemmatization accuracy on the information retrieval downstream task, we first created an information retrieval dataset for Estonian by automatically translating the DBpedia-Entity dataset from English. We benchmark several token normalization approaches, including lemmatization, on the created dataset using the BM25 algorithm. We observe a substantial improvement in IR metrics when using lemmatization over simplistic stemming. The benefits of improving lemma disambiguation accuracy manifest in small but consistent improvement in the IR recall measure, especially in the setting of high k.</abstract>
      <url hash="15efc03c">2025.nodalida-1.10</url>
      <bibkey>dorkin-sirts-2025-glilem</bibkey>
    </paper>
    <paper id="11">
      <title>Comparative analysis of optical character recognition methods for <fixed-case>Sámi</fixed-case> texts from the National Library of <fixed-case>Norway</fixed-case></title>
      <author><first>Tita</first><last>Enstad</last></author>
      <author><first>Trond</first><last>Trosterud</last></author>
      <author><first>Marie Iversdatter</first><last>Røsok</last></author>
      <author><first>Yngvil</first><last>Beyer</last></author>
      <author><first>Marie</first><last>Roald</last></author>
      <pages>98–108</pages>
      <abstract>Optical Character Recognition (OCR) is crucial to the National Library of Norway’s (NLN) digitisation process as it converts scanned documents into machine-readable text. However, for the Sámi documents in NLN’s collection, the OCR accuracy is insufficient. Given that OCR quality affects downstream processes, evaluating and improving OCR for text written in Sámi languages is necessary to make these resources accessible. To address this need, this work fine-tunes and evaluates three established OCR approaches, Transkribus, Tesseract and TrOCR, for transcribing Sámi texts from NLN’s collection. Our results show that Transkribus and TrOCR outperform Tesseract on this task, while Tesseract achieves superior performance on an out-of-domain dataset. Furthermore, we show that fine-tuning pre-trained models and supplementing manual annotations with machine annotations and synthetic text images can yield accurate OCR for Sámi languages, even with a moderate amount of manually annotated data.</abstract>
      <url hash="9e79a8cd">2025.nodalida-1.11</url>
      <bibkey>enstad-etal-2025-comparative</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>LAG-MMLU:</fixed-case> Benchmarking Frontier <fixed-case>LLM</fixed-case> Understanding in <fixed-case>Latvian</fixed-case> and <fixed-case>Giriama</fixed-case></title>
      <author><first>Naome A.</first><last>Etori</last></author>
      <author><first>Arturs</first><last>Kanepajs</last></author>
      <author><first>Kevin</first><last>Lu</last></author>
      <author><first>Randu</first><last>Karisa</last></author>
      <pages>109–120</pages>
      <abstract>This paper evaluates the language understanding capabilities of various large language models (LLMs) through an analysis of 112 translated and human-edited questions from the Multitask Language Understanding (MMLU) dataset, focusing specifically on two underrepresented languages: Latvian and Giriama. The study compares the performance of six state-of-the-art (SOTA) models, with OpenAI’s o1-preview model demonstrating superior performance across all languages, significantly outperforming non-proprietary models in Latvian and all other models in Giriama. Human editing of automated translations from English to Latvian yielded only a small, statistically insignificant improvement in performance estimates, suggesting that machine-translated benchmarks may be sufficient for comparing model performance in languages with established digital resources like Latvian. However, automated translation to Giriama proved infeasible, and model performance in Giriama remained poor, highlighting the persistent challenges LLMs face with low-resource languages. These findings underscore the need for more comprehensive datasets and improved machine translation capabilities for underrepresented languages, while emphasizing the importance of localized benchmarks and human evaluation in addressing cultural and contextual limitations in AI models.</abstract>
      <url hash="6242d14d">2025.nodalida-1.12</url>
      <bibkey>etori-etal-2025-lag</bibkey>
    </paper>
    <paper id="13">
      <title>Better Benchmarking <fixed-case>LLM</fixed-case>s for Zero-Shot Dependency Parsing</title>
      <author><first>Ana</first><last>Ezquerro</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <pages>121–135</pages>
      <abstract>While LLMs excel in zero-shot tasks, their performance in linguistic challenges like syntactic parsing has been less scrutinized. This paper studies state-of-the-art open-weight LLMs on the task by comparing them to baselines that do not have access to the input sentence, including baselines that have not been used in this context such as random projective trees or optimal linear arrangements. The results show that most of the tested LLMs cannot outperform the best uninformed baselines, with only the newest and largest versions of LLaMA doing so for most languages, and still achieving rather low performance. Thus, accurate zero-shot syntactic parsing is not forthcoming with open LLMs.</abstract>
      <url hash="216a6f7f">2025.nodalida-1.13</url>
      <bibkey>ezquerro-etal-2025-better</bibkey>
    </paper>
    <paper id="14">
      <title>Optimizing <fixed-case>Estonian</fixed-case> <fixed-case>TV</fixed-case> Subtitles with Semi-supervised Learning and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Artem</first><last>Fedorchenko</last></author>
      <author><first>Tanel</first><last>Alumäe</last></author>
      <pages>136–141</pages>
      <abstract>This paper presents an approach for generating high-quality, same-language subtitles for Estonian TV content. We finetune the Whisper model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling and large language model (LLM) based post-editing. Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. We find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. This approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications.</abstract>
      <url hash="589d4788">2025.nodalida-1.14</url>
      <bibkey>fedorchenko-alumae-2025-optimizing</bibkey>
    </paper>
    <paper id="15">
      <title>Modeling Multilayered Complexity in Literary Texts</title>
      <author><first>Pascale</first><last>Feldkamp</last></author>
      <author><first>Márton</first><last>Kardos</last></author>
      <author><first>Kristoffer</first><last>Nielbo</last></author>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <pages>142–158</pages>
      <abstract>We explore the relationship between stylistic and sentimental complexity in literary texts, analyzing how they interact and affect overall complexity. Using a dataset of over 9,000 English novels (19th-20th century), we find that complexity at the stylistic/syntactic and sentiment levels tend to show a linear association. Finally, using dedicated datasets, we show that both stylistic/syntactic features – particularly those relating to information density – as well as sentiment features are related to text difficulty rank as well as average processing time.</abstract>
      <url hash="d57762b2">2025.nodalida-1.15</url>
      <bibkey>feldkamp-etal-2025-modeling</bibkey>
    </paper>
    <paper id="16">
      <title>Does Preprocessing Matter? <fixed-case>An</fixed-case> Analysis of Acoustic Feature Importance in Deep Learning for Dialect Classification</title>
      <author><first>Lea</first><last>Fischbach</last></author>
      <author><first>Caroline</first><last>Kleen</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <author><first>Alfred</first><last>Lameli</last></author>
      <pages>159–169</pages>
      <abstract>This paper examines the effect of preprocessing techniques on spoken dialect classification using raw audio data. We focus on modifying Root Mean Square (RMS) amplitude, DC-offset, articulation rate (AR), pitch, and Harmonics-to-Noise Ratio (HNR) to assess their impact on model performance. Our analysis determines whether these features are important, irrelevant, or misleading for the classification task. To evaluate these effects, we use a pipeline that tests the significance of each acoustic feature through distortion and normalization techniques. While preprocessing did not directly improve classification accuracy, our findings reveal three key insights: deep learning models for dialect classification are generally robust to variations in the tested audio features, suggesting that normalization may not be necessary. We identify articulation rate as a critical factor, directly affecting the amount of information in audio chunks. Additionally, we demonstrate that intonation, specifically the pitch range, plays a vital role in dialect recognition.</abstract>
      <url hash="f8a9a322">2025.nodalida-1.16</url>
      <bibkey>fischbach-etal-2025-preprocessing</bibkey>
    </paper>
    <paper id="17">
      <title>Language of the <fixed-case>Swedish</fixed-case> Manosphere with <fixed-case>Swedish</fixed-case> <fixed-case>FrameNet</fixed-case></title>
      <author><first>Emilie Marie Carreau</first><last>Francis</last></author>
      <pages>170–180</pages>
      <abstract>The manosphere is a loose group of online communities centralised around the themes of anti-feminism, misogyny, and hetero-masculinity. It has gained a reputation for violent extremism, particularly from members of the incel community. Sweden sees one of the highest volumes of online traffic to well-known incel forums in all of Europe. In spite of this, there is little information on manosphere/incel culutre in Swedish. This paper uses posts from Flashback’s manosphere subforum automatically annotated with Swedish FrameNet to analyse the language community in a Swedish context. To do so, a lexicon for the Swedish manosphere was created and terms of interest were identified in the Swedish discourse. Analysis of prominent semantic frames linked to these terms of interest presents a detailed look into the language of the Swedish manosphere.</abstract>
      <url hash="0694c205">2025.nodalida-1.17</url>
      <bibkey>francis-2025-language</bibkey>
    </paper>
    <paper id="18">
      <title>Hotter and Colder: <fixed-case>A</fixed-case> New Approach to Annotating Sentiment, Emotions, and Bias in <fixed-case>Icelandic</fixed-case> Blog Comments</title>
      <author><first>Steinunn Rut</first><last>Friðriksdóttir</last></author>
      <author><first>Dan</first><last>Saattrup Nielsen</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>181–191</pages>
      <abstract>This paper presents Hotter and Colder, a dataset designed to analyze various types of online behavior in Icelandic blog comments. Building on previous work, we used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks, including sentiment analysis, emotion detection, hate speech, and group generalizations. Each comment was automatically labeled on a 5-point Likert scale. In a second annotation stage, comments with high or low probabilities of containing each examined behavior were subjected to manual revision. By leveraging crowdworkers to refine these automatically labeled comments, we ensure the quality and accuracy of our dataset resulting in 12,232 uniquely annotated comments and 19,301 annotations. Hotter and Colder provides an essential resource for advancing research in content moderation and automatically detectiong harmful online behaviors in Icelandic. We release both the dataset and annotation interface.</abstract>
      <url hash="ecfd2b94">2025.nodalida-1.18</url>
      <bibkey>fridriksdottir-etal-2025-hotter</bibkey>
    </paper>
    <paper id="19">
      <title>Towards large-scale speech foundation models for a low-resource minority language</title>
      <author><first>Yaroslav</first><last>Getman</last></author>
      <author><first>Tamás</first><last>Grósz</last></author>
      <author><first>Katri</first><last>Hiovain-Asikainen</last></author>
      <author><first>Tommi</first><last>Lehtonen</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>192–200</pages>
      <abstract>Modern ASR systems require massive amounts of training data. While ASR training data for most languages are scarce and expensive to transcribe, a practical solution is to collect huge amounts of raw untranscribed speech and pre-train the ASR model in a self-supervised manner. Unfortunately, for many low-resource minority languages, even untranscribed speech data are scarce. In this paper, we propose a solution for the Northern Sámi language with 22,400 hours of speech extracted from the Finnish radio and television archives. We evaluated the model performance with different decoding algorithms and examined the models’ internal behavior with interpretation-based techniques.</abstract>
      <url hash="34d70f7f">2025.nodalida-1.19</url>
      <bibkey>getman-etal-2025-towards</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>OpusDistillery</fixed-case>: <fixed-case>A</fixed-case> Configurable End-to-End Pipeline for Systematic Multilingual Distillation of Open <fixed-case>NMT</fixed-case> Models</title>
      <author><first>Ona de</first><last>Gibert</last></author>
      <author><first>Tommi</first><last>Nieminen</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>201–208</pages>
      <abstract>In this work, we introduce OpusDistillery, a novel framework to streamline the Knowledge Distillation (KD) process of multilingual NMT models. OpusDistillery’s main features are the integration of openly available teacher models from OPUS-MT and Hugging Face, comprehensive multilingual support and robust GPU utilization tracking. We describe the tool in detail and discuss the individual contributions of its pipeline components, demonstrating its flexibility for different use cases. OpusDistillery is open-source and released under a permissive license, aiming to facilitate further research and development in the field of multilingual KD for any sequence-to-sequence task. Our code is available at https://github.com/Helsinki-NLP/OpusDistillery.</abstract>
      <url hash="c1b0ffc9">2025.nodalida-1.20</url>
      <bibkey>gibert-etal-2025-opusdistillery</bibkey>
    </paper>
    <paper id="21">
      <title>Mind the Gap: <fixed-case>Diverse</fixed-case> <fixed-case>NMT</fixed-case> Models for Resource-Constrained Environments</title>
      <author><first>Ona de</first><last>Gibert</last></author>
      <author><first>Dayyán</first><last>O’Brien</last></author>
      <author><first>Dušan</first><last>Variš</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>209–216</pages>
      <abstract>We present fast Neural Machine Translation models for 17 diverse languages, developed using Sequence-level Knowledge Distillation. Our selected languages span multiple language families and scripts, including low-resource languages. The distilled models achieve comparable performance while being 10x times faster than transformer-base and 35x times faster than transformer-big architectures. Our experiments reveal that teacher model quality and capacity strongly influence the distillation success, as well as the language script. We also explore the effectiveness of multilingual students. We release publicly our code and models in our Github repository: anonymised.</abstract>
      <url hash="5426b449">2025.nodalida-1.21</url>
      <bibkey>gibert-etal-2025-mind</bibkey>
    </paper>
    <paper id="22">
      <title>Testing relevant linguistic features in automatic <fixed-case>CEFR</fixed-case> skill level classification for <fixed-case>Icelandic</fixed-case></title>
      <author><first>Isidora</first><last>Glišić</last></author>
      <author><first>Caitlin Laura</first><last>Richter</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>217–222</pages>
      <abstract>This paper explores the use of various linguistic features to develop models for automatic classification of language proficiency on the CEFR scale for Icelandic, a low-resourced and morphologically complex language. We train two classifiers to assess skill level of learner texts. One is used as a baseline and takes in the original unaltered text written by a learner and uses predominantly surface features to assess the level. The other uses both surface and other morphological and lexical features, as well as context vectors from transformer (IceBERT). It takes in both the original and corrected versions of the text and takes into account errors/deviation of the original texts compared to the corrected versions. Both classifiers show promising results, with baseline models achieving between 62.2-67.1% accuracy and dual-version between 75-80.3%.</abstract>
      <url hash="e246d7da">2025.nodalida-1.22</url>
      <bibkey>glisic-etal-2025-testing</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>MorSeD</fixed-case>: <fixed-case>Morphological</fixed-case> Segmentation of <fixed-case>Danish</fixed-case> and its Effect on Language Modeling</title>
      <author><first>Rob van der</first><last>Goot</last></author>
      <author><first>Anette</first><last>Jensen</last></author>
      <author><first>Emil Allerslev</first><last>Schledermann</last></author>
      <author><first>Mikkel Wildner</first><last>Kildeberg</last></author>
      <author><first>Nicolaj</first><last>Larsen</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <pages>223–229</pages>
      <abstract>Current language models (LMs) mostly exploit subwords as input units based on statistical co-occurrences of characters. Adjacently, previous work has shown that modeling morphemes can aid performance for Natural Language Processing (NLP) models. However, morphemes are challenging to obtain as there is no annotated data in most languages. In this work, we release a wide-coverage Danish morphological segmentation evaluation set. We evaluate a range of unsupervised token segmenters and evaluate the downstream effect of using morphemes as input units for transformer-based LMs. Our results show that popular subword algorithms perform poorly on this task, scoring at most an F1 of 57.6 compared to 68.0 for an unsupervised morphological segmenter (Morfessor). Furthermore, evaluate a range of segmenters on the task of language modeling.</abstract>
      <url hash="1f3604bf">2025.nodalida-1.23</url>
      <bibkey>goot-etal-2025-morsed</bibkey>
    </paper>
    <paper id="24">
      <title>Opinion Units: <fixed-case>Concise</fixed-case> and Contextualized Representations for Aspect-Based Sentiment Analysis</title>
      <author><first>Emil</first><last>Häglund</last></author>
      <author><first>Johanna</first><last>Björklund</last></author>
      <pages>230–240</pages>
      <abstract>We introduce opinion units, a contribution to the field Aspect-Based Sentiment Analysis (ABSA) that extends aspect- sentiment pairs by including substantiating excerpts, derived through hybrid abstractive-extractive summarisation. The goal is to provide fine-grained information without sacrificing succinctness and abstraction. Evaluations on review datasets demonstrate that large language models (LLMs) can accurately extract opinion units through few-shot learning. The main types of errors are providing incomplete contexts for opinions and and mischaracterising objective statements as opinions. The method reduces the need for labelled data and allows the LLM to dynamically define aspect types. As a practical evaluation, we present a case study on similarity search across academic datasets and public review data. The results indicate that searches leveraging opinion units are more successful than those relying on traditional data-segmentation strategies, showing robustness across datasets and embeddings.</abstract>
      <url hash="15b6d1cd">2025.nodalida-1.24</url>
      <bibkey>haglund-bjorklund-2025-opinion</bibkey>
    </paper>
    <paper id="25">
      <title>Aligning Language Models for <fixed-case>Icelandic</fixed-case> Legal Text Summarization</title>
      <author><first>Þórir Hrafn</first><last>Harðarson</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <author><first>Stefán</first><last>Ólafsson</last></author>
      <pages>241–251</pages>
      <abstract>The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models’ performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain.</abstract>
      <url hash="7b0ad0f6">2025.nodalida-1.25</url>
      <bibkey>hardarson-etal-2025-aligning</bibkey>
    </paper>
    <paper id="26">
      <title>Question-parsing with <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation enhanced by adding small datasets</title>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Maria</first><last>Boritchev</last></author>
      <author><first>Frédéric</first><last>Herledan</last></author>
      <pages>252–257</pages>
      <abstract>Abstract Meaning Representation (AMR) is a graph-based formalism for representing meaning in sentences. As the annotation is quite complex, few annotated corpora exist. The most well-known and widely-used corpora are LDC’s AMR 3.0 and the datasets available on the new AMR website. Models trained on the LDC corpora work fine on texts with similar genre and style: sentences extracted from news articles, Wikipedia articles. However, other types of texts, in particular questions, are less well processed by models trained on this data. We analyse how adding few sentence-type specific annotations can steer the model to improve parsing in the case of questions in English.</abstract>
      <url hash="bb194e88">2025.nodalida-1.26</url>
      <bibkey>heinecke-etal-2025-question</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>FinerWeb-10BT</fixed-case>: <fixed-case>Refining</fixed-case> Web Data with <fixed-case>LLM</fixed-case>-Based Line-Level Filtering</title>
      <author><first>Erik</first><last>Henriksson</last></author>
      <author><first>Otto</first><last>Tarkka</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>258–268</pages>
      <abstract>Data quality is crucial for training Large Language Models (LLMs). Traditional heuristic filters often miss low-quality text or mistakenly remove valuable content. In this paper, we introduce an LLM-based line-level filtering method to enhance training data quality. We use GPT-4o mini to label a 20,000-document sample from FineWeb at the line level, allowing the model to create descriptive labels for low-quality lines. These labels are grouped into nine main categories, and we train a DeBERTa-v3 classifier to scale the filtering to a 10B-token subset of FineWeb. To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets. The results show that models trained on the filtered data achieve higher accuracy on the HellaSwag benchmark and reach their performance targets faster, even with up to 25% less data. This demonstrates that LLM-based line-level filtering can significantly improve data quality and training efficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT, and the codebase to support further work in this area.</abstract>
      <url hash="c091c61d">2025.nodalida-1.27</url>
      <bibkey>henriksson-etal-2025-finerweb</bibkey>
    </paper>
    <paper id="28">
      <title>Margins in Contrastive Learning: <fixed-case>Evaluating</fixed-case> Multi-task Retrieval for Sentence Embeddings</title>
      <author><first>Tollef Emil</first><last>Jørgensen</last></author>
      <author><first>Jens</first><last>Breitung</last></author>
      <pages>269–278</pages>
      <abstract>This paper explores retrieval with sentence embeddings by fine-tuning sentence-transformer models for classification while preserving their ability to capture semantic similarity. To evaluate this balance, we introduce two opposing metrics – polarity score and semantic similarity score – that measure the model’s capacity to separate classes and retain semantic relationships between sentences. We propose a system that augments supervised datasets with contrastive pairs and triplets, training models under various configurations and evaluating their performance on top-<tex-math>k</tex-math> sentence retrieval. Experiments on two binary classification tasks demonstrate that reducing the margin parameter of loss functions greatly mitigates the trade-off between the metrics. These findings suggest that a single fine-tuned model can effectively handle joint classification and retrieval tasks, particularly in low-resource settings, without relying on multiple specialized models.</abstract>
      <url hash="973db67b">2025.nodalida-1.28</url>
      <bibkey>jorgensen-breitung-2025-margins</bibkey>
    </paper>
    <paper id="29">
      <title>Database of <fixed-case>Latvian</fixed-case> Morphemes and Derivational Models: ideas and expected results</title>
      <author><first>Andra</first><last>Kalnača</last></author>
      <author><first>Tatjana</first><last>Pakalne</last></author>
      <author><first>Kristīne</first><last>Levāne-Petrova</last></author>
      <pages>279–286</pages>
      <abstract>In this paper, we describe “The Database of Latvian Morphemes and Derivational Models” – a large-scale corpus-based and manually validated database of Latvian derivational morphology currently in development at the University of Latvia. The database contains morpheme-level data – morphemes, incl. morpheme variants (allomorphs), morpheme types, morpheme homonymy/ homography resolu- tion, hierarchical relations between root morphemes, links to word families, and lemma-level data – incl. base form, morphemic segmentation, POS, grammatical features, derivational motivation (incl. compounding), word-family membership. The focus of the database is on providing linguistically accurate comprehensive data as a reliable basis for future work in different fields.</abstract>
      <url hash="f2a5c99b">2025.nodalida-1.29</url>
      <bibkey>kalnaca-etal-2025-database</bibkey>
    </paper>
    <paper id="30">
      <title>Localizing <fixed-case>AI:</fixed-case> Evaluating Open-Weight Language Models for Languages of <fixed-case>B</fixed-case>altic States</title>
      <author><first>Jurgita</first><last>Kapočiūtė-Dzikienė</last></author>
      <author><first>Toms</first><last>Bergmanis</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <pages>287–295</pages>
      <abstract>Although large language models (LLMs) have transformed our expectations of modern language technologies, concerns over data privacy often restrict the use of commercially available LLMs hosted outside of EU jurisdictions. This limits their application in governmental, defense, and other data-sensitive sectors. In this work, we evaluate the extent to which locally deployable open-weight large language models support lesser-spoken languages such as Lithuanian, Latvian, and Estonian. We examine various size and precision variants of the top-performing multilingual open-weight models, Llama 3, Gemma 2, Phi, and NeMo, on machine translation, multiple-choice question answering, and free-form text generation. The results indicate that while certain models like Gemma 2 perform close to the top commercially available models, many LLMs struggle with these languages. Most surprisingly, however, we find that these models, while showing close to state-of-the-art translation performance, are still prone to lexical hallucinations with errors in at least 1 in 20 words for all open-weight multilingual LLMs.</abstract>
      <url hash="6d398edb">2025.nodalida-1.30</url>
      <bibkey>kapociute-dzikiene-etal-2025-localizing</bibkey>
    </paper>
    <paper id="31">
      <title>How Aunt-Like Are You? <fixed-case>Exploring</fixed-case> Gender Bias in the Genderless <fixed-case>Estonian</fixed-case> Language: <fixed-case>A</fixed-case> Case Study</title>
      <author><first>Elisabeth</first><last>Kaukonen</last></author>
      <author><first>Ahmed</first><last>Sabir</last></author>
      <author><first>Rajesh</first><last>Sharma</last></author>
      <pages>296–301</pages>
      <abstract>This paper examines gender bias in Estonian, a grammatically genderless Finno-Ugric language, which doesn’t have gendered noun system nor any gendered pronouns, but expresses gender through vocabulary. In this work, we focus on the male-female compound words ending with -tädi ‘aunt’ and -onu ‘uncle’, aiming to pinpoint the occupations these words signify for women and men, and to examine whether they reveal occupational differentiation and gender stereotypes. The findings indicate that these compounds go beyond occupational titles and highlight prevalent gender bias.</abstract>
      <url hash="58a7b9b4">2025.nodalida-1.31</url>
      <bibkey>kaukonen-etal-2025-aunt</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>Estonian</fixed-case> isolated-word text-to-speech synthesiser</title>
      <author><first>Indrek</first><last>Kiissel</last></author>
      <author><first>Liisi</first><last>Piits</last></author>
      <author><first>Heete</first><last>Sahkai</last></author>
      <author><first>Indrek</first><last>Hein</last></author>
      <author><first>Liis</first><last>Ermus</last></author>
      <author><first>Meelis</first><last>Mihkla</last></author>
      <pages>302–306</pages>
      <abstract>This paper presents the development and evaluation of an Estonian isolated-word text-to-speech (TTS) synthesiser. Unlike conventional TTS systems that convert continuous text into speech, this system focuses on the synthesis of isolated words, which is crucial for applications such as pronunciation training, speech therapy, and (learners’) dictionaries. The system addresses two key challenges: generating natural prosody for isolated words and context-free disambiguation of homographs. We conducted a perception test to evaluate the performance of the TTS system in terms of pronunciation accuracy. We used 16 pairs of homographs that differ in palatalisation and 16 pairs of homographs that differ in quantity. Given that all the test items were correctly recognised by a majority of the evaluators, the performance of the synthesiser can be considered very good.</abstract>
      <url hash="fd040335">2025.nodalida-1.32</url>
      <bibkey>kiissel-etal-2025-estonian</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>BiaSWE</fixed-case>: <fixed-case>An</fixed-case> Expert Annotated Dataset for Misogyny Detection in <fixed-case>Swedish</fixed-case></title>
      <author><first>Kätriin</first><last>Kukk</last></author>
      <author><first>Danila</first><last>Petrelli</last></author>
      <author><first>Judit</first><last>Casademont</last></author>
      <author><first>Eric J. W.</first><last>Orlowski</last></author>
      <author><first>Michal</first><last>Dzielinski</last></author>
      <author><first>Maria</first><last>Jacobson</last></author>
      <pages>307–312</pages>
      <abstract>In this study, we introduce the process for creating BiaSWE, an expert-annotated dataset tailored for misogyny detection in the Swedish language. To address the cultural and linguistic specificity of misogyny in Swedish, we collaborated with experts from the social sciences and humanities. Our interdisciplinary team developed a rigorous annotation process, incorporating both domain knowledge and language expertise, to capture the nuances of misogyny in a Swedish context. This methodology ensures that the dataset is not only culturally relevant but also aligned with broader efforts in bias detection for low-resource languages. The dataset, along with the annotation guidelines, is publicly available for further research.</abstract>
      <url hash="b443f5e4">2025.nodalida-1.33</url>
      <bibkey>kukk-etal-2025-biaswe</bibkey>
    </paper>
    <paper id="34">
      <title>Predictability of Microsyntactic Units across <fixed-case>Slavic</fixed-case> Languages: <fixed-case>A</fixed-case> translation-based Study</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Iuliia</first><last>Zaitova</last></author>
      <author><first>Wei</first><last>Xue</last></author>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>313–322</pages>
      <abstract>The paper presents the results of a free translation experiment, which was set up to explore Slavic cross-language intelligibility. In the experiment, native speakers of Russian were asked to read a sentence in one of the five Slavic languages and return a Russian translation of a highlighted item. The experiment is focused on microsyntactic units because they offer an increased intercomprehension difficulty due to opaque semantics. Each language is represented by at least 50 stimuli, and each stimulus has generated at least 20 responses. The levels of intercomprehension are captured by categorising participants’ responses into seven types of translation solutions (paraphrase, correct, fluent_literal, awkward_literal, fantasy, noise, and empty), generally reflecting the level of the cross-linguistic intelligibility of the stimuli. The study aims to reveal linguistic factors that favour intercomprehension across Slavic languages. We use regression and correlation analysis to identify the most important intercomprehension predictors and statistical analysis to bring up the most typical cases and outliers. We explore several feature types that reflect the properties of the translation tasks and their outcomes, including point-wise phonological and orthographic distances, cosine similarities, surprisals, translation quality scores and translation solution entropy indices. The experimental data confirms the expected gradual increase of intelligibility from West-Slavic to East-Slavic languages for the speakers of Russian. We show that intelligibility is highly contingent on the ability of speakers to recognise and interpret formal similarities between languages as well as on the size of these similarities. For several Slavic languages, the context sentence complexity was a significant predictor of intelligibility.</abstract>
      <url hash="da1f735d">2025.nodalida-1.34</url>
      <bibkey>kunilovskaya-etal-2025-predictability</bibkey>
    </paper>
    <paper id="35">
      <title>Train More Parameters But Mind Their Placement: <fixed-case>Insights</fixed-case> into Language Adaptation with <fixed-case>PEFT</fixed-case></title>
      <author><first>Jenny</first><last>Kunz</last></author>
      <pages>323–330</pages>
      <abstract>Smaller LLMs still face significant challenges even in medium-resourced languages, particularly when it comes to language-specific knowledge – a problem not easily resolved with machine-translated data. In this case study on Icelandic, we aim to enhance the generation performance of an LLM by specialising it using unstructured text corpora. A key focus is on preventing interference with the models’ capabilities of handling longer context during this adaptation. Through ablation studies using various parameter-efficient fine-tuning (PEFT) methods and setups, we find that increasing the number of trainable parameters leads to better and more robust language adaptation. LoRAs placed in the feed-forward layers and bottleneck adapters show promising results with sufficient parameters, while prefix tuning and (IA)<tex-math>^3</tex-math> are not suitable. Although improvements are consistent in 0-shot summarisation, some adapted models struggle with longer context lengths, an issue that can be mitigated by adapting only the final layers.</abstract>
      <url hash="e25e5484">2025.nodalida-1.35</url>
      <bibkey>kunz-2025-train</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>SweSAT</fixed-case>-1.0: <fixed-case>The</fixed-case> <fixed-case>Swedish</fixed-case> <fixed-case>U</fixed-case>niversity Entrance Exam as a Benchmark for Large Language Models</title>
      <author><first>Murathan</first><last>Kurfalı</last></author>
      <author><first>Shorouq</first><last>Zahra</last></author>
      <author><first>Evangelia</first><last>Gogoulou</last></author>
      <author><first>Luise</first><last>Dürlich</last></author>
      <author><first>Fredrik</first><last>Carlsson</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>331–339</pages>
      <abstract>This introduces SweSAT-1.0, a new benchmark dataset created from the Swedish university entrance exam (Högskoleprovet) to assess large language models in Swedish. The current version of the benchmark includes 867 questions across six different tasks, including reading comprehension, mathematical problem solving, and logical reasoning. We find that some widely used open-source and commercial models excel in verbal tasks, but we also see that all models, even the commercial ones, struggle with reasoning tasks in Swedish. We hope that SweSAT-1.0 will facilitate research on large language models for Swedish by enriching the breadth of available tasks, offering a challenging evaluation benchmark that is free from any translation biases.</abstract>
      <url hash="ea6d9c5b">2025.nodalida-1.36</url>
      <bibkey>kurfali-etal-2025-swesat</bibkey>
    </paper>
    <paper id="37">
      <title>How Well do <fixed-case>LLM</fixed-case>s know <fixed-case>Finno-Ugric</fixed-case> Languages? <fixed-case>A</fixed-case> Systematic Assessment</title>
      <author><first>Hele-Andra</first><last>Kuulmets</last></author>
      <author><first>Taido</first><last>Purason</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>340–353</pages>
      <abstract>We present a systematic evaluation of multilingual capabilities of open large language models (LLMs), specifically focusing on five Finno-Ugric (FiU) languages. Our investigation covers multiple prompting strategies across several benchmarks and reveals that Llama-2 7B and Llama-2 13B perform weakly on most FiU languages. In contrast, Llama 3.1 models show impressive improvements, even for extremely low-resource languages such as Võro and Komi, indicating successful cross-lingual knowledge transfer inside the models. Finally, we show that stronger base models outperform weaker, language-adapted models, thus emphasizing the importance of base model in successful language adaptation.</abstract>
      <url hash="997c3cc5">2025.nodalida-1.37</url>
      <bibkey>kuulmets-etal-2025-well</bibkey>
    </paper>
    <paper id="38">
      <title>Mapping <fixed-case>Faroese</fixed-case> in the Multilingual Representation Space: <fixed-case>Insights</fixed-case> for <fixed-case>ASR</fixed-case> Model Optimization</title>
      <author><first>Dávid í</first><last>Lág</last></author>
      <author><first>Barbara</first><last>Scalvini</last></author>
      <author><first>Jon</first><last>Gudnason</last></author>
      <pages>354–358</pages>
      <abstract>ASR development for low-resource languages like Faroese faces significant challenges due to the scarcity of large, diverse datasets. While fine-tuning multilingual models using related languages is a common practice, there is no standardized method for selecting these auxiliary languages, leading to a computationally expensive trial-and-error process. By analyzing Faroese’s positioning among other languages in wav2vec2’s multilingual representation space, we find that Faroese’s closest neighbors are influenced not only by linguistic similarity but also by historical, phonetic, and cultural factors. These findings open new avenues for auxiliary language selection to improve Faroese ASR and underscore the potential value of data-driven factors in ASR fine-tuning.</abstract>
      <url hash="03333b6a">2025.nodalida-1.38</url>
      <bibkey>lag-etal-2025-mapping</bibkey>
    </paper>
    <paper id="39">
      <title>Towards a Derivational Semantics Resource for <fixed-case>Latvian</fixed-case></title>
      <author><first>Ilze</first><last>Lokmane</last></author>
      <author><first>Mikus</first><last>Grasmanis</last></author>
      <author><first>Agute</first><last>Klints</last></author>
      <author><first>Gunta</first><last>Nešpore-Bērzkalne</last></author>
      <author><first>Pēteris</first><last>Paikens</last></author>
      <author><first>Lauma</first><last>Pretkalniņa</last></author>
      <author><first>Laura</first><last>Rituma</last></author>
      <author><first>Madara</first><last>Stāde</last></author>
      <author><first>Evelīna</first><last>Tauriņa</last></author>
      <pages>359–366</pages>
      <abstract>In this paper we describe the implementation of the first structured resource of semantic derivational links for Latvian, basing it on the largest online dictionary Tēzaurs.lv and linking it to the Latvian WordNet. We separate two kinds of derivational links: semantic derivation links between senses and morphological derivation links between lexemes. The semantic links between senses are defined as a pair of semantic labels assigned to both ends of the link. The process of semantic linking involves revising the sense inventory of both the base word and the derivative, defining semantic labels for lexemes of four basic word classes – nouns, verbs, adjectives and adverbs, and adding the appropriate labels to the corresponding senses. We exemplify our findings with a detailed representation of sense relations between a base verb and its nominal derivatives.</abstract>
      <url hash="8c917c5d">2025.nodalida-1.39</url>
      <bibkey>lokmane-etal-2025-towards</bibkey>
    </paper>
    <paper id="40">
      <title>Poro <fixed-case>34B</fixed-case> and the Blessing of Multilinguality</title>
      <author><first>Risto</first><last>Luukkonen</last></author>
      <author><first>Jonathan</first><last>Burdge</last></author>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Ville</first><last>Komulainen</last></author>
      <author><first>Väinö</first><last>Hatanpää</last></author>
      <author><first>Peter</first><last>Sarlin</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <pages>367–382</pages>
      <abstract>The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing: when the lack of training data is a constraint for effectively training larger models for a target language, augmenting the dataset with other languages can offer a way to improve over the capabilities of monolingual models for that language. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that substantially advances over the capabilities of existing models for Finnish and excels in translation, while also achieving competitive performance in its class for English and programming languages. We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B.</abstract>
      <url hash="4a737e84">2025.nodalida-1.40</url>
      <bibkey>luukkonen-etal-2025-poro</bibkey>
    </paper>
    <paper id="41">
      <title>Can summarization approximate simplification? <fixed-case>A</fixed-case> gold standard comparison</title>
      <author><first>Giacomo</first><last>Magnifico</last></author>
      <author><first>Eduard</first><last>Barbu</last></author>
      <pages>383–389</pages>
      <abstract>This study explores the overlap between text summarization and simplification outputs. While summarization evaluation methods are streamlined, simplification lacks cohesion, prompting the question: how closely can abstractive summarization resemble gold-standard simplification? We address this by applying two BART-based BRIO summarization methods to the Newsela corpus, comparing outputs with manually annotated simplifications and achieving a top ROUGE-L score of 0.654. This provides insight into where summarization and simplification outputs converge and differ.</abstract>
      <url hash="077a7d23">2025.nodalida-1.41</url>
      <bibkey>magnifico-barbu-2025-summarization</bibkey>
    </paper>
    <paper id="42">
      <title>A Comparative Study of <fixed-case>PEFT</fixed-case> Methods for Python Code Generation</title>
      <author><first>Johanna</first><last>Männistö</last></author>
      <author><first>Joseph</first><last>Attieh</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>390–396</pages>
      <abstract>Fine-tuning language models incurs high costs in training, inference and storage. Parameter-efficient fine-tuning (PEFT) methods have emerged as a more cost-effective alternative to full fine-tuning. However, limited work has compared different PEFT approaches for tasks like code generation. In this study, we examine the effect of various PEFT training methods on model performance in the task of Python code generation. We fine-tune four model families, ranging from 124M to 7B parameters, using three PEFT approaches alongside standard full fine-tuning. Our findings reveal that the effectiveness of each PEFT method varies with the model size and the corpus used.</abstract>
      <url hash="aef4555d">2025.nodalida-1.42</url>
      <bibkey>mannisto-etal-2025-comparative</bibkey>
    </paper>
    <paper id="43">
      <title>A Collection of Question Answering Datasets for <fixed-case>Norwegian</fixed-case></title>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Victoria Ovedie Chruickshank</first><last>Langø</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>397–407</pages>
      <abstract>This paper introduces a new suite of question answering datasets for Norwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The data covers a wide range of skills and knowledge domains, including world knowledge, commonsense reasoning, truthfulness, and knowledge about Norway. Covering both of the written standards of Norwegian – Bokmål and Nynorsk – our datasets comprise over 10k question-answer pairs, created by native speakers. We detail our dataset creation approach and present the results of evaluating 11 language models (LMs) in zero- and few-shot regimes. Most LMs perform better in Bokmål than Nynorsk, struggle most with commonsense reasoning, and are often untruthful in generating answers to questions. All our datasets and annotation materials are publicly available.</abstract>
      <url hash="b5e973bc">2025.nodalida-1.43</url>
      <bibkey>mikhailov-etal-2025-collection</bibkey>
    </paper>
    <paper id="44">
      <title>Incorporating Target Fuzzy Matches into Neural Fuzzy Repair</title>
      <author><first>Tommi</first><last>Nieminen</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <pages>408–418</pages>
      <abstract>Neural fuzzy repair (NFR) is a simple implementation of retrieval-augmented translation (RAT), based on data augmentation. In NFR, a translation database is searched for translation examples where the source sentence is similar to the sentence being translated, and the target side of the example is concatenated with the source sentences. We experiment with introducing retrieval that is based on target similarity to NFR during training. The results of our experiments confirm that including target similarity matches during training supplements source similarity matches and leads to better translations at translation time.</abstract>
      <url hash="c466afc4">2025.nodalida-1.44</url>
      <bibkey>nieminen-etal-2025-incorporating</bibkey>
    </paper>
    <paper id="45">
      <title>Constructions and Strategies in <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>419–423</pages>
      <abstract>Is the framework of Universal Dependencies (UD) compatible with findings from linguistic typology? One way to find out is to investigate whether UD can adequately represent constructions of the world’s languages, as described in William Croft’s recent book Morphosyntax. This paper discusses how such an investigation could be carried out and why it would be useful.</abstract>
      <url hash="b91f3814">2025.nodalida-1.45</url>
      <bibkey>nivre-2025-constructions</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>Finnish</fixed-case> <fixed-case>SQuAD</fixed-case>: <fixed-case>A</fixed-case> Simple Approach to Machine Translation of Span Annotations</title>
      <author><first>Emil</first><last>Nuutinen</last></author>
      <author><first>Iiro</first><last>Rastas</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>424–432</pages>
      <abstract>We apply a simple method to machine translate datasets with span-level annotation using the DeepL MT service and its ability to translate formatted documents. Using this method, we produce a Finnish version of the SQuAD2.0 question answering dataset and train QA retriever models on this new dataset. We evaluate the quality of the dataset and more generally the MT method through direct evaluation, indirect comparison to other similar datasets, a backtranslation experiment, as well as through the performance of downstream trained QA models. In all these evaluations, we find that the method of transfer is not only simple to use but produces consistently better translated data. Given its good performance on the SQuAD dataset, it is likely the method can be used to translate other similar span-annotated datasets for other tasks and languages as well. All code and data is available under an open license: data at HuggingFace TurkuNLP/squad_v2_fi, code on GitHub TurkuNLP/squad2-fi, and model at HuggingFace TurkuNLP/bert-base-finnish-cased-squad2.</abstract>
      <url hash="e4f04fb2">2025.nodalida-1.46</url>
      <bibkey>nuutinen-etal-2025-finnish</bibkey>
    </paper>
    <paper id="47">
      <title>How to Tune a Multilingual Encoder Model for <fixed-case>German</fixed-case>ic Languages: <fixed-case>A</fixed-case> Study of <fixed-case>PEFT,</fixed-case> Full Fine-Tuning, and Language Adapters</title>
      <author><first>Romina</first><last>Oji</last></author>
      <author><first>Jenny</first><last>Kunz</last></author>
      <pages>433–439</pages>
      <abstract>This paper investigates the optimal use of the multilingual encoder model mDeBERTa for tasks in three Germanic languages – German, Swedish, and Icelandic – representing varying levels of presence and likely data quality in mDeBERTas pre-training data. We compare full fine-tuning with the parameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck adapters, finding that PEFT is more effective for the higher-resource language, German. However, results for Swedish and Icelandic are less consistent. We also observe differences between tasks: While PEFT tends to work better for question answering, full fine-tuning is preferable for named entity recognition. Inspired by previous research on modular approaches that combine task and language adapters, we evaluate the impact of adding PEFT modules trained on unstructured text, finding that this approach is not beneficial.</abstract>
      <url hash="7a76d458">2025.nodalida-1.47</url>
      <bibkey>oji-kunz-2025-tune</bibkey>
    </paper>
    <paper id="48">
      <title>Match ‘em: <fixed-case>Multi-Tiered</fixed-case> Alignment for Error Analysis in <fixed-case>ASR</fixed-case></title>
      <author><first>Phoebe</first><last>Parsons</last></author>
      <author><first>Knut</first><last>Kvale</last></author>
      <author><first>Torbjørn</first><last>Svendsen</last></author>
      <author><first>Giampiero</first><last>Salvi</last></author>
      <pages>440–447</pages>
      <abstract>We introduce “Match ‘em”: a new framework for aligning output from automatic speech recognition (ASR) with reference transcriptions. This allows a more detailed analysis of errors produced by end-to-end ASR systems compared to word error rate (WER). Match ‘em performs the alignment on both the word and character level; each relying on information from the other to provide the most meaningful global alignment. At the character level, we define a speech production motivated character similarity metric. At the word level, we rely on character similarities to define word similarity and, additionally, we reconcile compounding (insertion or deletion of spaces). We evaluated Match ‘em on transcripts of three European languages produced by wav2vec2 and Whisper. We show that Match ‘em results in more similar word substitution pairs and that compound reconciling can capture a broad range of spacing errors. We believe Match ‘em to be a valuable tool for ASR error analysis across many languages.</abstract>
      <url hash="35eeb21a">2025.nodalida-1.48</url>
      <bibkey>parsons-etal-2025-match</bibkey>
    </paper>
    <paper id="49">
      <title>Adding Metadata to Existing Parliamentary Speech Corpus</title>
      <author><first>Phoebe</first><last>Parsons</last></author>
      <author><first>Per Erik</first><last>Solberg</last></author>
      <author><first>Knut</first><last>Kvale</last></author>
      <author><first>Torbjørn</first><last>Svendsen</last></author>
      <author><first>Giampiero</first><last>Salvi</last></author>
      <pages>448–457</pages>
      <abstract>Parliamentary proceedings are convenient data sources for creating corpora for speech technology. Given its public nature, there is an abundance of extra information about the speakers that can be legally and ethically harvested to enrich this kind of corpora. This paper describes the methods we have used to add speaker metadata to the Stortinget Speech Corpus (SSC) containing over 5,000 hours of Norwegian speech with non-verbatim transcripts but without speaker metadata. The additional metadata for each speech segment includes speaker ID, gender, date of birth, municipality of birth, and counties represented. We also infer speaker dialect from their municipality of birth using a manually designed mapping between municipalities and Norwegian dialects. We provide observations on the SSC data and give suggestions for how it may be used for tasks other than speech recognition. Finally, we demonstrate the utility of this new metadata through a dialect identification task. The described methods can be adapted to add metadata information to parliamentary corpora in other languages.</abstract>
      <url hash="938a9900">2025.nodalida-1.49</url>
      <bibkey>parsons-etal-2025-adding</bibkey>
    </paper>
    <paper id="50">
      <title>Paragraph-Level Machine Translation for Low-Resource <fixed-case>Finno-Ugric</fixed-case> Languages</title>
      <author><first>Dmytro</first><last>Pashchenko</last></author>
      <author><first>Lisa</first><last>Yankovskaya</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>458–469</pages>
      <abstract>We develop paragraph-level machine translation for four low-resource Finno-Ugric languages: Proper Karelian, Livvi, Ludian, and Veps. The approach is based on sentence-level pre-trained translation models, which are fine-tuned with paragraph-parallel data. This allows the resulting model to develop a native ability to handle discource-level phenomena correctly, in particular translating from grammatically gender-neutral input in Finno-Ugric languages. We collect monolingual and parallel paragraph-level corpora for these languages. Our experiments show that paragraph-level translation models can translate sentences no worse than sentence-level systems, while handling discourse-level phenomena better. For evaluation, we manually translate part of FLORES-200 into these four languages. All our results, data, and models are released openly.</abstract>
      <url hash="60e1893c">2025.nodalida-1.50</url>
      <bibkey>pashchenko-etal-2025-paragraph</bibkey>
    </paper>
    <paper id="51">
      <title>Evaluating <fixed-case>LLM</fixed-case>-Generated Explanations of Metaphors – A Culture-Sensitive Study of <fixed-case>Danish</fixed-case></title>
      <author><first>Bolette S.</first><last>Pedersen</last></author>
      <author><first>Nathalie</first><last>Sørensen</last></author>
      <author><first>Sanni</first><last>Nimb</last></author>
      <author><first>Dorte Haltrup</first><last>Hansen</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <author><first>Ali</first><last>Al-Laith</last></author>
      <pages>470–479</pages>
      <abstract>In this study, we examine how well Danish culture-specific metaphors are explained by two of the best performing language models for Danish, namely ChatGPT and Llama. For comparison, the explana- tions are measured against how well cross- lingual (or ’universal’) metaphors are ex- plained by the models; referring here to metaphors that exist in Danish as well as across cultures and languages and in par- ticular in English. To perform our study, we compile a pilot dataset of 150 Danish metaphors and idioms divided tentatively by culture specificity. We prompt the two models and perform a careful qualitative evaluation of the explanations against a four-graded scale. Our studies show that both models are heavily biased towards English since they have much more suc- cess in explaining the metaphors that also exist in English than the culture-specific ones, relying presumably on erroneous transfer from English when dealing with the latter. In particular, the sentiment of the culture-specific metaphors seems to be often ’lost in translation’. We further claim that this strong colouring towards English poses a serious problem in the era of LLMs with regards to developing and maintaining cultural and linguistic diver- sity in other languages.</abstract>
      <url hash="c7458116">2025.nodalida-1.51</url>
      <bibkey>pedersen-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="52">
      <title>Tokenization on Trial: <fixed-case>The</fixed-case> Case of <fixed-case>Kalaallisut</fixed-case>–<fixed-case>Danish</fixed-case> Legal Machine Translation</title>
      <author><first>Esther</first><last>Ploeger</last></author>
      <author><first>Paola</first><last>Saucedo</last></author>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Ross Deans</first><last>Kristensen-McLachlan</last></author>
      <author><first>Heather</first><last>Lent</last></author>
      <pages>480–491</pages>
      <abstract>The strengths of subword tokenization have been widely demonstrated when applied to higher-resourced, morphologically simple languages. However, it is not self-evident that these results transfer to lower-resourced, morphologically complex languages. In this work, we investigate the influence of different subword segmentation techniques on machine translation between Danish and Kalaallisut, the official language of Greenland. We present the first semi-manually aligned parallel corpus for this language pair, and use it to compare subwords from unsupervised tokenizers and morphological segmenters. We find that Unigram-based segmentation both preserves morphological boundaries and handles out-of-vocabulary words adequately, but that this does not directly correspond to superior translation quality. We hope that our findings lay further groundwork for future efforts in neural machine translation for Kalaallisut.</abstract>
      <url hash="04eee1df">2025.nodalida-1.52</url>
      <bibkey>ploeger-etal-2025-tokenization</bibkey>
    </paper>
    <paper id="53">
      <title>The Roles of <fixed-case>English</fixed-case> in Evaluating Multilingual Language Models</title>
      <author><first>Wessel</first><last>Poelman</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <pages>492–498</pages>
      <abstract>Multilingual natural language processing is getting increased attention, with numerous models, benchmarks, and methods being released for many languages. English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages. In this position paper, we lay out two roles of English in multilingual LM evaluations: as an interface and as a natural language. We argue that these roles have different goals: task performance versus language understanding. This discrepancy is highlighted with examples from datasets and evaluation setups. Numerous works explicitly use English as an interface to boost task performance. We recommend to move away from these imprecise methods and instead focus on language understanding.</abstract>
      <url hash="2ba1c337">2025.nodalida-1.53</url>
      <bibkey>poelman-lhoneux-2025-roles</bibkey>
    </paper>
    <paper id="54">
      <title>Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity Recognition in Low-Resource Languages</title>
      <author><first>Andrei</first><last>Politov</last></author>
      <author><first>Oleh</first><last>Shkalikov</last></author>
      <author><first>Rene</first><last>Jäkel</last></author>
      <author><first>Michael</first><last>Färber</last></author>
      <pages>499–507</pages>
      <abstract>Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer between languages to identify and classify named entities, making it particularly useful for low-resource languages. We show that the data-based cross-lingual transfer method is an effective technique for cross-lingual NER and can outperform multi-lingual language models for low-resource languages. This paper introduces two key enhancements to the annotation projection step in cross-lingual NER for low-resource languages. First, we explore refining word alignments using back-translation to improve accuracy. Second, we present a novel formalized projection approach of matching source entities with extracted target candidates. Through extensive experiments on two datasets spanning 57 languages, we demonstrated that our approach surpasses existing projection-based methods in low-resource settings. These findings highlight the robustness of projection-based data transfer as an alternative to model-based methods for cross-lingual named entity recognition in low-resource languages.</abstract>
      <url hash="eb703569">2025.nodalida-1.54</url>
      <bibkey>politov-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="55">
      <title>Empathy vs Neutrality: <fixed-case>Designing</fixed-case> and Evaluating a Natural Chatbot for the Healthcare Domain</title>
      <author><first>Cristina</first><last>Reguera-Gómez</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <author><first>Maaike H. T.</first><last>de Boer</last></author>
      <pages>508–517</pages>
      <abstract>As lifestyle-related diseases rise due to unhealthy habits such as smoking, poor diet, lack of exercise, and alcohol consumption, the role of Conversational AI in healthcare is increasingly significant. This study provides an empirical study on the design and evaluation of a natural and intuitive healthcare chatbot, specifically focusing on the impact of empathetic responses on user experience regarding lifestyle changes. Findings reveal a strong preference for the empathetic chatbot, with results showing statistical significance (p &lt;0.001), highlighting the importance of empathy in enhancing user interaction with healthcare chatbots.</abstract>
      <url hash="a97575b9">2025.nodalida-1.55</url>
      <bibkey>reguera-gomez-etal-2025-empathy</bibkey>
    </paper>
    <paper id="56">
      <title>Assessed and Annotated Vowel Lengths in Spoken <fixed-case>Icelandic</fixed-case> Sentences for <fixed-case>L1</fixed-case> and <fixed-case>L2</fixed-case> Speakers: <fixed-case>A</fixed-case> Resource for Pronunciation Training</title>
      <author><first>Caitlin Laura</first><last>Richter</last></author>
      <author><first>Kolbrún</first><last>Friðriksdóttir</last></author>
      <author><first>Kormákur Logi</first><last>Bergsson</last></author>
      <author><first>Erik Anders</first><last>Maher</last></author>
      <author><first>Ragnheiður María</first><last>Benediktsdóttir</last></author>
      <author><first>Jon</first><last>Gudnason</last></author>
      <pages>518–524</pages>
      <abstract>We introduce a dataset of time-aligned phonetic transcriptions focusing on vowel length (quantity) in Icelandic. Ultimately, this aims to support computer assisted pronunciation training (CAPT) software, to automatically assess length and possible errors in Icelandic learners’ pronunciations. The dataset contains a range of long and short vowel targets, including the first acoustic description of quantity in non-native Icelandic. Evaluations assess how manual annotations and automatic forced alignment characterise quantity contrasts. Initial analyses also imply partial acquisition of phonologically conditioned quantity alternations by non-native speakers.</abstract>
      <url hash="e0366666">2025.nodalida-1.56</url>
      <bibkey>richter-etal-2025-assessed</bibkey>
    </paper>
    <paper id="57">
      <title>The <fixed-case>BRAGE</fixed-case> Benchmark: <fixed-case>Evaluating</fixed-case> Zero-shot Learning Capabilities of Large Language Models for <fixed-case>Norwegian</fixed-case> Customer Service Dialogues</title>
      <author><first>Mike</first><last>Riess</last></author>
      <author><first>Tollef Emil</first><last>Jørgensen</last></author>
      <pages>525–536</pages>
      <abstract>This study explores the capabilities of open-weight Large Language Models in a zero-shot learning setting, testing their ability to classify the content of customer service dialogues in Norwegian from a single instruction, named the BRAGE benchmark. By comparing results against widely used downstream tasks such as question-answering and named entity recognition, we find that (1) specific instruction models greatly exceed base models on the benchmark, (2) both English and multilingual instruction models outperform the tested Norwegian models of similar sizes, and (3) the difference between base and instruction models is less pronounced than in other generative tasks, suggesting that BRAGE is a challenging benchmark, requiring precise and generalizable instruction-tuning.</abstract>
      <url hash="2f7358e3">2025.nodalida-1.57</url>
      <bibkey>riess-jorgensen-2025-brage</bibkey>
    </paper>
    <paper id="58">
      <title>Mixed Feelings: <fixed-case>Cross-Domain</fixed-case> Sentiment Classification of Patient Feedback</title>
      <author><first>Egil</first><last>Rønningstad</last></author>
      <author><first>Lilja Charlotte</first><last>Storset</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>537–543</pages>
      <abstract>Sentiment analysis of patient feedback from the public health domain can aid decision makers in evaluating the provided services. The current paper focuses on free-text comments in patient surveys about general practitioners and psychiatric healthcare, annotated with four sentence-level polarity classes - positive, negative, mixed and neutral - while also attempting to alleviate data scarcity by leveraging general-domain sources in the form of reviews. For several different architectures, we compare in-domain and out-of-domain effects, as well as the effects of training joint multi-domain models.</abstract>
      <url hash="0b63e655">2025.nodalida-1.58</url>
      <bibkey>ronningstad-etal-2025-mixed</bibkey>
    </paper>
    <paper id="59">
      <title>The Impact of Copyrighted Material on Large Language Models: <fixed-case>A</fixed-case> <fixed-case>Norwegian</fixed-case> Perspective</title>
      <author><first>Javier de la</first><last>Rosa</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Lemei</first><last>Zhang</last></author>
      <author><first>Freddy</first><last>Wetjen</last></author>
      <author><first>David</first><last>Samuel</last></author>
      <author><first>Peng</first><last>Liu</last></author>
      <author><first>Rolv-Arild</first><last>Braaten</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Magnus Breder</first><last>Birkenes</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Tita</first><last>Enstad</last></author>
      <author><first>Hans Christian</first><last>Farsethås</last></author>
      <author><first>Svein Arne</first><last>Brygfjeld</last></author>
      <author><first>Jon Atle</first><last>Gulla</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Wilfred</first><last>Østgulen</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Aslak Sira</first><last>Myhre</last></author>
      <pages>544–560</pages>
      <abstract>The use of copyrighted materials in training language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of publisher-controlled copyrighted corpora on the performance of generative large language models (LLMs) for Norwegian. When evaluated on a diverse set of tasks, we found that adding both books and newspapers to the data mixture of LLMs tend to improve their performance, while the addition of fiction works seems to be detrimental. Our experiments could inform the creation of a compensation scheme for authors whose works contribute to AI development.</abstract>
      <url hash="d9da1e22">2025.nodalida-1.59</url>
      <bibkey>rosa-etal-2025-impact</bibkey>
    </paper>
    <paper id="60">
      <title>Encoder vs Decoder: <fixed-case>Comparative</fixed-case> Analysis of Encoder and Decoder Language Models on Multilingual <fixed-case>NLU</fixed-case> Tasks</title>
      <author><first>Dan</first><last>Saattrup Nielsen</last></author>
      <author><first>Kenneth</first><last>Enevoldsen</last></author>
      <author><first>Peter</first><last>Schneider-Kamp</last></author>
      <pages>561–572</pages>
      <abstract>This paper explores the performance of encoder and decoder language models on multilingual Natural Language Understanding (NLU) tasks, with a broad focus on Germanic languages. Building upon the ScandEval benchmark, initially restricted to evaluating encoder models, we extend the evaluation framework to include decoder models. We introduce a method for evaluating decoder models on NLU tasks and apply it to the languages Danish, Swedish, Norwegian, Icelandic, Faroese, German, Dutch, and English. Through a series of experiments and analyses, we also address research questions regarding the comparative performance of encoder and decoder models, the impact of NLU task types, and the variation across language resources. Our findings reveal that encoder models can achieve significantly better NLU performance than decoder models despite having orders of magnitude fewer parameters. Additionally, we investigate the correlation between decoders and task performance via a UMAP analysis, shedding light on the unique capabilities of decoder and encoder models. This study contributes to a deeper understanding of language model paradigms in NLU tasks and provides valuable insights for model selection and evaluation in multilingual settings.</abstract>
      <url hash="1743fc39">2025.nodalida-1.60</url>
      <bibkey>saattrup-nielsen-etal-2025-encoder</bibkey>
    </paper>
    <paper id="61">
      <title>Small Languages, Big Models: <fixed-case>A</fixed-case> Study of Continual Training on Languages of <fixed-case>Norway</fixed-case></title>
      <author><first>David</first><last>Samuel</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Lucas Georges Gabriel</first><last>Charpentier</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <pages>573–608</pages>
      <abstract>Training large language models requires vast amounts of data, posing a challenge for less widely spoken languages like Norwegian and even more so for truly low-resource languages like Northern Sámi. To address this issue, we present a novel three-stage continual training approach that substantially improves the downstream performance together with the inference efficiency for the target languages. Based on our findings, we train, evaluate, and openly release a new generative language model for Norwegian Bokmål, Nynorsk, and Northern Sámi with 11.4 billion parameters: NorMistral-11B.</abstract>
      <url hash="3d6124df">2025.nodalida-1.61</url>
      <bibkey>samuel-etal-2025-small</bibkey>
    </paper>
    <paper id="62">
      <title>Rethinking Low-Resource <fixed-case>MT:</fixed-case> The Surprising Effectiveness of Fine-Tuned Multilingual Models in the <fixed-case>LLM</fixed-case> Age</title>
      <author><first>Barbara</first><last>Scalvini</last></author>
      <author><first>Iben Nyholm</first><last>Debess</last></author>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>609–621</pages>
      <abstract>This study challenges the current paradigm shift in machine translation, where large language models (LLMs) are gaining prominence over traditional neural machine translation models, with a focus on English-to-Faroese translation. We compare the performance of various models, including fine-tuned multilingual models, LLMs (GPT-SW3, Llama 3.1), and closed-source models (Claude 3.5, GPT-4). Our findings show that a fine-tuned NLLB model outperforms most LLMs, including some larger models, in both automatic and human evaluations. We also demonstrate the effectiveness of using LLM-generated synthetic data for fine-tuning. While closed-source models like Claude 3.5 perform best overall, the competitive performance of smaller, fine-tuned models suggests a more nuanced approach to low-resource machine translation. Our results highlight the potential of specialized multilingual models and the importance of language-specific knowledge. We discuss implications for resource allocation in low-resource settings and suggest future directions for improving low-resource machine translation, including targeted data creation and more comprehensive evaluation methodologies.</abstract>
      <url hash="41ae521d">2025.nodalida-1.62</url>
      <bibkey>scalvini-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="63">
      <title>Prompt Engineering Enhances <fixed-case>Faroese</fixed-case> <fixed-case>MT,</fixed-case> but Only Humans Can Tell</title>
      <author><first>Barbara</first><last>Scalvini</last></author>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Iben Nyholm</first><last>Debess</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>622–633</pages>
      <abstract>This study evaluates GPT-4’s English-to-Faroese translation capabilities, comparing it with multilingual models on FLORES-200 and Sprotin datasets. We propose a prompt optimization strategy using Semantic Textual Similarity (STS) to improve translation quality. Human evaluation confirms the effectiveness of STS-based few-shot example selection, though automated metrics fail to capture these improvements. Our findings advance LLM applications for low-resource language translation while highlighting the need for better evaluation methods in this context.</abstract>
      <url hash="866e1c50">2025.nodalida-1.63</url>
      <bibkey>scalvini-etal-2025-prompt</bibkey>
    </paper>
    <paper id="64">
      <title>Interactive maps for corpus-based dialectology</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Olli</first><last>Kuparinen</last></author>
      <pages>634–638</pages>
      <abstract>Traditional data collection methods in dialectology rely on structured surveys, whose results can be easily presented on printed or digital maps. But in recent years, corpora of transcribed dialect speech have become a precious alternative data source for data-driven linguistic analysis. For example, topic models can be advantageously used to discover both general dialectal variation patterns and specific linguistic features that are most characteristic for certain dialects. Multilingual (or rather, multilectal) language modeling tasks can also be used to learn speaker-specific embeddings. In connection with this paper, we introduce a website that presents the results of two recent studies in the form of interactive maps, allowing visitors to explore the effects of various parameter settings. The website covers two tasks (topic models and speaker embeddings) and three language areas (Finland, Norway, and German-speaking Switzerland). It is available at https://www.corcodial.net/ .</abstract>
      <url hash="3c53ce58">2025.nodalida-1.64</url>
      <bibkey>scherrer-kuparinen-2025-interactive</bibkey>
    </paper>
    <paper id="65">
      <title>Profiling Bias in <fixed-case>LLM</fixed-case>s: <fixed-case>Stereotype</fixed-case> Dimensions in Contextual Word Embeddings</title>
      <author><first>Carolin M.</first><last>Schuster</last></author>
      <author><first>Maria-Alexandra</first><last>Roman</last></author>
      <author><first>Shashwat</first><last>Ghatiwala</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>639–650</pages>
      <abstract>Large language models (LLMs) are the foundation of the current successes of artificial intelligence (AI), however, they are unavoidably biased. To effectively communicate the risks and encourage mitigation efforts these models need adequate and intuitive descriptions of their discriminatory properties, appropriate for all audiences of AI. We suggest bias profiles with respect to stereotype dimensions based on dictionaries from social psychology research. Along these dimensions we investigate gender bias in contextual embeddings, across contexts and layers, and generate stereotype profiles for twelve different LLMs, demonstrating their intuition and use case for exposing and visualizing bias.</abstract>
      <url hash="7e7fabbd">2025.nodalida-1.65</url>
      <bibkey>schuster-etal-2025-profiling</bibkey>
    </paper>
    <paper id="66">
      <title>Entailment Progressions: <fixed-case>A</fixed-case> Robust Approach to Evaluating Reasoning Within Larger Discourse</title>
      <author><first>Rishabh</first><last>Shastry</last></author>
      <author><first>Patricia</first><last>Chiril</last></author>
      <author><first>Joshua</first><last>Charney</last></author>
      <author><first>David</first><last>Uminsky</last></author>
      <pages>651–660</pages>
      <abstract>Textual entailment, or the ability to deduce whether a proposed hypothesis is logically supported by a given premise, has historically been applied to the evaluation of language modelling efficiency in tasks like question answering and text summarization. However, we hypothesize that these zero-shot entailment evaluations can be extended to the task of evaluating discourse within larger textual narratives. In this paper, we propose a simple but effective method that sequentially evaluates changes in textual entailment between sentences within a larger text, in an approach we denote as “Entailment Progressions”. These entailment progressions aim to capture the inference relations between sentences as an underlying component capable of distinguishing texts generated from various models and procedures. Our results suggest that entailment progressions can be used to effectively distinguish between machine-generated and human-authored texts across multiple established benchmark corpora and our own EP4MGT dataset. Additionally, our method displays robustness in performance when evaluated on paraphrased texts a technique that has historically affected the performance of well-established metrics when distinguishing between machine generated and human authored texts.</abstract>
      <url hash="d415745e">2025.nodalida-1.66</url>
      <bibkey>shastry-etal-2025-entailment</bibkey>
    </paper>
    <paper id="67">
      <title>Generative <fixed-case>AI</fixed-case> for Technical Writing: <fixed-case>Comparing</fixed-case> Human and <fixed-case>LLM</fixed-case> Assessments of Generated Content</title>
      <author><first>Karen de</first><last>Souza</last></author>
      <author><first>Alexandre</first><last>Nikolaev</last></author>
      <author><first>Maarit</first><last>Koponen</last></author>
      <pages>661–679</pages>
      <abstract>Large language models (LLMs) have recently gained significant attention for their capabilities in natural language processing (NLP), particularly generative artificial intelligence (AI). LLMs can also be useful tools for software documentation technical writers. We present an assessment of technical documentation content generated by three different LLMs using retrieval-augmented technology (RAG) with product documentation as a knowledge base. The LLM-generated responses were analyzed in three ways: 1) manual error analysis by a technical writer, 2) automatic assessment using deterministic metrics (BLEU, ROUGE, token overlap), and 3) evaluation of correctness by LLM as a judge. The results of these assessments were compared using a Network Analysis and linear regression models to investigate statistical relationships, model preferences, and the distribution of human and LLM scores. The analyses concluded that human quality evaluation is more related to the LLM correctness judgment than deterministic metrics, even when using different analysis frameworks.</abstract>
      <url hash="6d29c3d1">2025.nodalida-1.67</url>
      <bibkey>souza-etal-2025-generative</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>MC-19:</fixed-case> A Corpus of 19th Century <fixed-case>Icelandic</fixed-case> Texts</title>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <author><first>Einar Freyr</first><last>Sigurðsson</last></author>
      <author><first>Atli</first><last>Jasonarson</last></author>
      <pages>680–687</pages>
      <abstract>We present MC-19, a new Icelandic historical corpus containing texts from the period 1800-1920. We describe approaches for enhancing a corpus of historical texts, by preparing the texts so that they can be processed using state-of-the-art NLP tools. We train encoder-decoder models to reduce the number of OCR errors while leaving other orthographical variation be. We generate a separate modern spelling layer by normalizing the spelling to comply with modern spelling rules, using a statistical modernization ruleset as well as a dictionary of the most common words. This allows for the texts to be PoS-tagged and lemmatized using available tools, facilitating usage of the corpus for researchers and language technologists. The published version of the corpus contains over 270 million tokens.</abstract>
      <url hash="dbbd8779">2025.nodalida-1.68</url>
      <bibkey>steingrimsson-etal-2025-mc</bibkey>
    </paper>
    <paper id="69">
      <title>Surface-Level Morphological Segmentation of Low-resource <fixed-case>Inuktitut</fixed-case> Using Pre-trained Large Language Models</title>
      <author><first>Mathias</first><last>Stenlund</last></author>
      <author><first>Hemanadhan</first><last>Myneni</last></author>
      <author><first>Morris</first><last>Riedel</last></author>
      <pages>688–696</pages>
      <abstract>Segmenting languages based on morpheme boundaries instead of relying on language independent segmenting algorithms like Byte-Pair Encoding (BPE) has shown to benefit downstream Natural Language Processing (NLP) task performance. This can however be tricky for polysynthetic languages like Inuktitut due to a high morpheme-to-word ratio and the lack of appropriately sized annotated datasets. Through our work, we display the potential of using pre-trained Large Language Models (LLMs) for surface-level morphological segmentation of Inuktitut by treating it as a binary classification task. We fine-tune on tasks derived from automatically annotated Inuktitut words written in Inuktitut syllabics. Our approach shows good potential when compared to previous neural approaches. We share our best model to encourage further studies on down stream NLP tasks for Inuktitut written in syllabics.</abstract>
      <url hash="6e1758ce">2025.nodalida-1.69</url>
      <bibkey>stenlund-etal-2025-surface</bibkey>
    </paper>
    <paper id="70">
      <title>The Devil’s in the Details: the Detailedness of Classes Influences Personal Information Detection and Labeling</title>
      <author><first>Maria Irena</first><last>Szawerna</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>Ricardo</first><last>Muñoz Sánchez</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <pages>697–708</pages>
      <abstract>In this paper, we experiment with the effect of different levels of detailedness or granularity—understood as i) the number of classes, and ii) the classes’ semantic depth in the sense of hypernym and hyponym relations — of the annotation of Personally Identifiable Information (PII) on automatic detection and labeling of such information. We fine-tune a Swedish BERT model on a corpus of Swedish learner essays annotated with a total of six PII tagsets at varying levels of granularity. We also investigate whether the presence of grammatical and lexical correction annotation in the tokens and class prevalence have an effect on predictions. We observe that the fewer total categories there are, the better the overall results are, but having a more diverse annotation facilitates fewer misclassifications for tokens containing correction annotation. We also note that the classes’ internal diversity has an effect on labeling. We conclude from the results that while labeling based on the detailed annotation is difficult because of the number of classes, it is likely that models trained on such annotation rely more on the semantic content captured by contextual word embeddings rather than just the form of the tokens, making them more robust against nonstandard language.</abstract>
      <url hash="7c10e12a">2025.nodalida-1.70</url>
      <bibkey>szawerna-etal-2025-devils</bibkey>
    </paper>
    <paper id="71">
      <title>Braxen 1.0</title>
      <author><first>Christina</first><last>Tånnander</last></author>
      <author><first>Jens</first><last>Edlund</last></author>
      <pages>709–713</pages>
      <abstract>With this paper, we release a Swedish pronunciation lexicon resource, Braxen 1.0, which is the result of almost 20 years development carried out at the Swedish Agency for Accessible Media (MTM). The lexicon originated with a basic word list, but has continuously been exanded with new entries, mainly acquired from university textbooks and news text. Braxen consists of around 850 000 entries, of which around 150 000 are proper names. The lexicon is released under the CC BY 4.0 license and is accessible for public use.</abstract>
      <url hash="6e766e5b">2025.nodalida-1.71</url>
      <bibkey>tannander-edlund-2025-braxen</bibkey>
    </paper>
    <paper id="72">
      <title>Temporal Relation Classification: <fixed-case>An</fixed-case> <fixed-case>XAI</fixed-case> Perspective</title>
      <author><first>Sofia Elena</first><last>Terenziani</last></author>
      <pages>714–728</pages>
      <abstract>Temporal annotations are used to identify and mark up temporal information, offering definition into how it is expressed through linguistic properties in text. This study investigates various discriminative pre-trained language models of differing sizes on a temporal relation classification task. We define valid reasoning strategies based on the linguistic principles that guide commonly used temporal annotations. Using a combination of saliency-based and counterfactual explanations, we examine if the models’ decisions are in line with these strategies. Our findings suggest that the selected models do not rely on the expected linguistic cues for processing temporal information effectively.</abstract>
      <url hash="975afbb9">2025.nodalida-1.72</url>
      <bibkey>terenziani-2025-temporal</bibkey>
    </paper>
    <paper id="73">
      <title>Benchmarking Abstractive Summarisation: <fixed-case>A</fixed-case> Dataset of Human-authored Summaries of <fixed-case>Norwegian</fixed-case> News Articles</title>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Marie Ingeborg</first><last>Kroka</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>729–738</pages>
      <abstract>We introduce a dataset of high-quality human-authored summaries of news articles in Norwegian. The dataset is intended for benchmarking of the abstractive summarisation capabilities of generative language models. Each document in the dataset is provided with three different candidate gold-standard summaries written by native Norwegian speakers and all summaries are provided in both of the written variants of Norwegian – Bokmål and Nynorsk. The paper describes details on the data creation effort as well as an evaluation of existing open LLMs for Norwegian on the dataset. We also provide insights from a manual human evaluation, comparing human-authored to model generated summaries. Our results indicate that the dataset provides a challenging LLM benchmark for Norwegian summarisation capabilities.</abstract>
      <url hash="9a57d4f5">2025.nodalida-1.73</url>
      <bibkey>touileb-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="74">
      <title>Efficient Elicitation of Fictitious Nursing Notes from Volunteer Healthcare Professionals</title>
      <author><first>Jesper</first><last>Vaaben Bornerup</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>739–754</pages>
      <abstract>Reliable automatic solutions to extract structured information from free-text nursing notes could bring important efficiency gains in healthcare, but their development is hampered by the sensitivity and limited availability of example data. We describe a method for eliciting fictitious nursing documentation and associated structured documentation from volunteers and a resulting dataset of 397 Danish notes collected and annotated through a custom web application from 98 participating nurses. After some manual refinement, we obtained a high-quality dataset containing nurse notes with relevant entities identified. We describe the implementation and limitations of our approach as well as initial experiments in a named entity tagging setup.</abstract>
      <url hash="c4cbfa5f">2025.nodalida-1.74</url>
      <bibkey>vaaben-bornerup-hardmeier-2025-efficient</bibkey>
    </paper>
    <paper id="75">
      <title>Analyzing the Effect of Linguistic Instructions on Paraphrase Generation</title>
      <author><first>Teemu</first><last>Vahtola</last></author>
      <author><first>Songbo</first><last>Hu</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>755–766</pages>
      <abstract>Recent work has demonstrated that large language models can often generate fluent and linguistically correct text, adhering to given instructions. However, to what extent can they execute complex instructions requiring knowledge of fundamental linguistic concepts and elaborate semantic reasoning? Our study connects an established linguistic theory of paraphrasing with LLM-based practice to analyze which specific types of paraphrases LLMs can accurately produce and where they still struggle. To this end, we investigate a method of analyzing paraphrases generated by LLMs prompted with a comprehensive set of systematic linguistic instructions. We conduct a case study using GPT-4, which has shown strong performance across various language generation tasks, and we believe that other LLMs may face similar challenges in comparable scenarios. We examine GPT-4 from a linguistic perspective to explore its potential contributions to linguistic research regarding paraphrasing, systematically assessing how accurately the model generates paraphrases that adhere to specified transformation rules. Our results suggest that GPT-4 frequently prioritizes simple lexical or syntactic alternations, often disregarding the transformation guidelines if they overly complicate the primary task.</abstract>
      <url hash="a074d96f">2025.nodalida-1.75</url>
      <bibkey>vahtola-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="76">
      <title><fixed-case>SweClinEval</fixed-case>: <fixed-case>A</fixed-case> Benchmark for <fixed-case>Swedish</fixed-case> Clinical Natural Language Processing</title>
      <author><first>Thomas</first><last>Vakili</last></author>
      <author><first>Martin</first><last>Hansson</last></author>
      <author><first>Aron</first><last>Henriksson</last></author>
      <pages>767–775</pages>
      <abstract>The lack of benchmarks in certain domains and for certain languages makes it difficult to track progress regarding the state-of-the-art of NLP in those areas, potentially impeding progress in important, specialized domains. Here, we introduce the first Swedish benchmark for clinical NLP: _SweClinEval_. The first iteration of the benchmark consists of six clinical NLP tasks, encompassing both document-level classification and named entity recognition tasks, with real clinical data. We evaluate nine different encoder models, both Swedish and multilingual. The results show that domain-adapted models outperform generic models on sequence-level classification tasks, while certain larger generic models outperform the clinical models on named entity recognition tasks. We describe how the benchmark can be managed despite limited possibilities to share sensitive clinical data, and discuss plans for extending the benchmark in future iterations.</abstract>
      <url hash="6ec80777">2025.nodalida-1.76</url>
      <bibkey>vakili-etal-2025-sweclineval</bibkey>
    </paper>
    <paper id="77">
      <title>Dialectal treebanks and their relation with the standard variety: <fixed-case>The</fixed-case> case of <fixed-case>East Cretan</fixed-case> and <fixed-case>Standard Modern Greek</fixed-case></title>
      <author><first>Socrates</first><last>Vakirtzian</last></author>
      <author><first>Vivian</first><last>Stamou</last></author>
      <author><first>Yannis</first><last>Kazos</last></author>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <pages>776–784</pages>
      <abstract>We report on the development of the first treebank and parser for Eastern Cretan in the framework of Universal Dependencies (UD). Eastern Cretan is a living but under-resourced dialect of Modern Greek. We have worked on the transcription of oral material and relied on active annotation and knowledge transfer from GUD, a treebank of Standard Modern Greek. Along with its other phonological and morphosyntactic differences from Standard Modern Greek, Eastern Cretan (and other varieties of Modern Greek) makes heavy use of euphonics and voicing that have not been included in the UD annotation guidelines so far. We have provided annotation guidelines for East Cretan euphonics and voicing and included them in the models. Knowledge transfer from the treebank of Standard Modern Greek to the dialectal models helped to initiate annotation via an active annotation procedure</abstract>
      <url hash="1ea12b4f">2025.nodalida-1.77</url>
      <bibkey>vakirtzian-etal-2025-dialectal</bibkey>
    </paper>
    <paper id="78">
      <title>Danoliteracy of Generative Large Language Models</title>
      <author><first>Søren</first><last>Vejlgaard Holm</last></author>
      <author><first>Lars Kai</first><last>Hansen</last></author>
      <author><first>Martin Carsten</first><last>Nielsen</last></author>
      <pages>785–800</pages>
      <abstract>The language technology moonshot moment of Generative Large Language Models (GLLMs) was not limited to English: These models brought a surge of technological applications, investments, and hype to low-resource languages as well. However, the capabilities of these models in languages such as Danish were, until recently, difficult to verify beyond qualitative demonstrations due to a lack of applicable evaluation corpora. We present a GLLM benchmark to evaluate Danoliteracy, a measure of Danish language and cultural competency across eight diverse scenarios such as Danish citizenship tests and abstractive social media question answering. This limited-size benchmark was found to produce a robust ranking that correlates to human feedback at <tex-math>\rho \sim 0.8</tex-math> with GPT-4 and Claude Opus models achieving the highest rankings. Analyzing these model results across scenarios, we find one strong underlying factor explaining 95% of scenario performance variance for GLLMs in Danish, suggesting a <tex-math>g</tex-math> factor of model consistency in language adaptation.</abstract>
      <url hash="29d8179f">2025.nodalida-1.78</url>
      <bibkey>vejlgaard-holm-etal-2025-danoliteracy</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>NorEventGen</fixed-case>: generative event extraction from <fixed-case>Norwegian</fixed-case> news</title>
      <author><first>Huiling</first><last>You</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>801–811</pages>
      <abstract>In this work, we approach event extraction from Norwegian news text using a generation-based approach which formulates the task as text-to-structure generation. We present experiments assessing the effect of different modeling configurations and provide an analysis of the model predictions and typical system errors. Finally, we apply our system to a large corpus of raw news texts and analyze the resulting distribution of event structures in a fairly representative snap-shot of the Norwegian news landscape.</abstract>
      <url hash="7d0182e4">2025.nodalida-1.79</url>
      <bibkey>you-etal-2025-noreventgen</bibkey>
    </paper>
    <paper id="80">
      <title><fixed-case>SnakModel</fixed-case>: <fixed-case>Lessons</fixed-case> Learned from Training an Open <fixed-case>Danish</fixed-case> Large Language Model</title>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Rob van der</first><last>Goot</last></author>
      <pages>812–825</pages>
      <abstract>We present SnakModel, a Danish large language model (LLM) based on Llama2-7B, which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M Danish instructions. As best practices for creating LLMs for smaller language communities have yet to be established, we examine the effects of early modeling and training decisions on downstream performance throughout the entire training pipeline, including (1) the creation of a strictly curated corpus of Danish text from diverse sources; (2) the language modeling and instruction-tuning training process itself, including the analysis of intermediate training dynamics, and ablations across different hyperparameters; (3) an evaluation on eight language and culturally-specific tasks. Across these experiments SnakModel achieves the highest overall performance, outperforming multiple contemporary Llama2-7B-based models. By making SnakModel, the majority of our pre-training corpus, and the associated code available under open licenses, we hope to foster further research and development in Danish Natural Language Processing, and establish training guidelines for languages with similar resource constraints.</abstract>
      <url hash="cfc4a35a">2025.nodalida-1.80</url>
      <bibkey>zhang-etal-2025-snakmodel</bibkey>
    </paper>
    <paper id="81">
      <title>Got Compute, but No Data: <fixed-case>Lessons</fixed-case> From Post-training a <fixed-case>Finnish</fixed-case> <fixed-case>LLM</fixed-case></title>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Ville</first><last>Komulainen</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <pages>826–832</pages>
      <abstract>As LLMs gain more popularity as chatbots and general assistants, methods have been developed to enable LLMs to follow instructions and align with human preferences. These methods have found success in the field, but their effectiveness has not been demonstrated outside of high-resource languages. In this work, we discuss our experiences in post-training an LLM for instruction-following for English and Finnish. We use a multilingual LLM to translate instruction and preference datasets from English to Finnish. We perform instruction tuning and preference optimization in English and Finnish and evaluate the instruction-following capabilities of the model in both languages. Our results show that with a few hundred Finnish instruction samples we can obtain competitive performance in Finnish instruction-following. We also found that although preference optimization in English offers some cross-lingual benefits, we obtain our best results by using preference data from both languages. We release our model, datasets, and recipes under open licenses at https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant.</abstract>
      <url hash="1f2ca929">2025.nodalida-1.81</url>
      <bibkey>zosa-etal-2025-got</bibkey>
    </paper>
  </volume>
  <event id="nodalida-2025">
    <meta>
      <title>The Joint 25th Nordic Conference on Computational Linguistics and 11th Baltic Conference on Human Language Technologies (NoDaLiDa/Baltic-HLT 2025)</title>
      <location>Tallinn, Estonia</location>
      <dates>March 2-5, 2025</dates>
    </meta>
    <colocated>
      <volume-id>2025.aaas-1</volume-id>
      <volume-id>2025.cgmta-1</volume-id>
      <volume-id>2025.nbreal-1</volume-id>
      <volume-id>2025.nlp4call-1</volume-id>
      <volume-id>2025.nlp4ecology-1</volume-id>
      <volume-id>2025.resourceful-1</volume-id>
    </colocated>
  </event>
</collection>
