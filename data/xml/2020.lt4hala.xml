<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.lt4hala">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages</booktitle>
      <editor><first>Rachele</first><last>Sprugnoli</last></editor>
      <editor><first>Marco</first><last>Passarotti</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-53-5</isbn>
      <venue>lt4hala</venue>
    </meta>
    <frontmatter>
      <url hash="cbe27ee8">2020.lt4hala-1.0</url>
      <bibkey>lt4hala-2020-lt4hala</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Dating and Stratifying a Historical Corpus with a <fixed-case>B</fixed-case>ayesian Mixture Model</title>
      <author><first>Oliver</first><last>Hellwig</last></author>
      <pages>1–9</pages>
      <abstract>This paper introduces and evaluates a Bayesian mixture model that is designed for dating texts based on the distributions of linguistic features. The model is applied to the corpus of Vedic Sanskrit the historical structure of which is still unclear in many details. The evaluation concentrates on the interaction between time, genre and linguistic features, detecting those whose distributions are clearly coupled with the historical time. The evaluation also highlights the problems that arise when quantitative results need to be reconciled with philological insights.</abstract>
      <url hash="ca7639ff">2020.lt4hala-1.1</url>
      <language>eng</language>
      <bibkey>hellwig-2020-dating</bibkey>
    </paper>
    <paper id="2">
      <title>Automatic Construction of <fixed-case>A</fixed-case>ramaic-<fixed-case>H</fixed-case>ebrew Translation Lexicon</title>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Shmuel</first><last>Liebeskind</last></author>
      <pages>10–16</pages>
      <abstract>Aramaic is an ancient Semitic language with a 3,000 year history. However, since the number of Aramaic speakers in the world hasdeclined, Aramaic is in danger of extinction. In this paper, we suggest a methodology for automatic construction of Aramaic-Hebrew translation Lexicon. First, we generate an initial translation lexicon by a state-of-the-art word alignment translation model. Then,we filter the initial lexicon using string similarity measures of three types: similarity between terms in the target language, similarity between a source and a target term, and similarity between terms in the source language. In our experiments, we use a parallel corporaof Biblical Aramaic-Hebrew sentence pairs and evaluate various string similarity measures for each type of similarity. We illustratethe empirical benefit of our methodology and its effect on precision and F1. In particular, we demonstrate that our filtering methodsignificantly exceeds a filtering approach based on the probability scores given by a state-of-the-art word alignment translation model.</abstract>
      <url hash="eb8c39ec">2020.lt4hala-1.2</url>
      <language>eng</language>
      <bibkey>liebeskind-liebeskind-2020-automatic</bibkey>
    </paper>
    <paper id="3">
      <title>Dating Ancient texts: an Approach for Noisy <fixed-case>F</fixed-case>rench Documents</title>
      <author><first>Anaëlle</first><last>Baledent</last></author>
      <author><first>Nicolas</first><last>Hiebel</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>17–21</pages>
      <abstract>Automatic dating of ancient documents is a very important area of research for digital humanities applications. Many documents available via digital libraries do not have any dating or dating that is uncertain. Document dating is not only useful by itself but it also helps to choose the appropriate NLP tools (lemmatizer, POS tagger ) for subsequent analysis. This paper provides a dataset with thousands of ancient documents in French and present methods and evaluation metrics for this task. We compare character-level methods with token-level methods on two different datasets of two different time periods and two different text genres. Our results show that character-level models are more robust to noise than classical token-level models. The experiments presented in this article focused on documents written in French but we believe that the ability of character-level models to handle noise properly would help to achieve comparable results on other languages and more ancient languages in particular.</abstract>
      <url hash="7b2ec31f">2020.lt4hala-1.3</url>
      <language>eng</language>
      <bibkey>baledent-etal-2020-dating</bibkey>
    </paper>
    <paper id="4">
      <title>Lemmatization and <fixed-case>POS</fixed-case>-tagging process by using joint learning approach. Experimental results on <fixed-case>C</fixed-case>lassical <fixed-case>A</fixed-case>rmenian, <fixed-case>O</fixed-case>ld <fixed-case>G</fixed-case>eorgian, and <fixed-case>S</fixed-case>yriac</title>
      <author><first>Chahan</first><last>Vidal-Gorène</last></author>
      <author><first>Bastien</first><last>Kindt</last></author>
      <pages>22–27</pages>
      <abstract>Classical Armenian, Old Georgian and Syriac are under-resourced digital languages. Even though a lot of printed critical editions or dictionaries are available, there is currently a lack of fully tagged corpora that could be reused for automatic text analysis. In this paper, we introduce an ongoing project of lemmatization and POS-tagging for these languages, relying on a recurrent neural network (RNN), specific morphological tags and dedicated datasets. For this paper, we have combine different corpora previously processed by automatic out-of-context lemmatization and POS-tagging, and manual proofreading by the collaborators of the GREgORI Project (UCLouvain, Louvain-la-Neuve, Belgium). We intend to compare a rule based approach and a RNN approach by using PIE specialized by Calfa (Paris, France). We introduce here first results. We reach a mean accuracy of 91,63% in lemmatization and of 92,56% in POS-tagging. The datasets, which were constituted and used for this project, are not yet representative of the different variations of these languages through centuries, but they are homogenous and allow reaching tangible results, paving the way for further analysis of wider corpora.</abstract>
      <url hash="44fa9ca5">2020.lt4hala-1.4</url>
      <language>eng</language>
      <bibkey>vidal-gorene-kindt-2020-lemmatization</bibkey>
    </paper>
    <paper id="5">
      <title>Computerized Forward Reconstruction for Analysis in Diachronic Phonology, and <fixed-case>L</fixed-case>atin to <fixed-case>F</fixed-case>rench Reflex Prediction</title>
      <author><first>Clayton</first><last>Marr</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <pages>28–36</pages>
      <abstract>Traditionally, historical phonologists have relied on tedious manual derivations to calibrate the sequences of sound changes that shaped the phonological evolution of languages. However, humans are prone to errors, and cannot track thousands of parallel word derivations in any efficient manner. We propose to instead automatically derive each lexical item in parallel, and we demonstrate forward reconstruction as both a computational task with metrics to optimize, and as an empirical tool for inquiry. For this end we present DiaSim, a user-facing application that simulates “cascades” of diachronic developments over a language’s lexicon and provides diagnostics for “debugging” those cascades. We test our methodology on a Latin-to-French reflex prediction task, using a newly compiled dataset FLLex with 1368 paired Latin/French forms. We also present, FLLAPS, which maps 310 Latin reflexes through five stages until Modern French, derived from Pope (1934)’s sound tables. Our publicly available rule cascades include the baselines BaseCLEF and BaseCLEF*, representing the received view of Latin to French development, and DiaCLEF, build by incremental corrections to BaseCLEF aided by DiaSim’s diagnostics. DiaCLEF vastly outperforms the baselines, improving final accuracy on FLLex from 3.2%to 84.9%, and similar improvements across FLLAPS’ stages.</abstract>
      <url hash="cb49a06c">2020.lt4hala-1.5</url>
      <language>eng</language>
      <bibkey>marr-mortensen-2020-computerized</bibkey>
    </paper>
    <paper id="6">
      <title>Using <fixed-case>L</fixed-case>at<fixed-case>I</fixed-case>nf<fixed-case>L</fixed-case>exi for an Entropy-Based Assessment of Predictability in <fixed-case>L</fixed-case>atin Inflection</title>
      <author><first>Matteo</first><last>Pellegrini</last></author>
      <pages>37–46</pages>
      <abstract>This paper presents LatInfLexi, a large inflected lexicon of Latin providing information on all the inflected wordforms of 3,348 verbs and 1,038 nouns. After a description of the structure of the resource and some data on its size, the procedure followed to obtain the lexicon from the database of the Lemlat 3.0 morphological analyzer is detailed, as well as the choices made regarding overabundant and defective cells. The way in which the data of LatInfLexi can be exploited in order to perform a quantitative assessment of predictability in Latin verb inflection is then illustrated: results obtained by computing the conditional entropy of guessing the content of a paradigm cell assuming knowledge of one wordform or multiple wordforms are presented in turn, highlighting the descriptive and theoretical relevance of the analysis. Lastly, the paper envisages the advantages of an inclusion of LatInfLexi into the LiLa knowledge base, both for the presented resource and for the knowledge base itself.</abstract>
      <url hash="04497ef1">2020.lt4hala-1.6</url>
      <language>eng</language>
      <bibkey>pellegrini-2020-using</bibkey>
    </paper>
    <paper id="7">
      <title>A Tool for Facilitating <fixed-case>OCR</fixed-case> Postediting in Historical Documents</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Mohammad</first><last>Aboomar</last></author>
      <author><first>Jan</first><last>Buts</last></author>
      <author><first>James</first><last>Hadley</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>47–51</pages>
      <abstract>Optical character recognition (OCR) for historical documents is a complex procedure subject to a unique set of material issues, including inconsistencies in typefaces and low quality scanning. Consequently, even the most sophisticated OCR engines produce errors. This paper reports on a tool built for postediting the output of Tesseract, more specifically for correcting common errors in digitized historical documents. The proposed tool suggests alternatives for word forms not found in a specified vocabulary. The assumed error is replaced by a presumably correct alternative in the post-edition based on the scores of a Language Model (LM). The tool is tested on a chapter of the book An Essay Towards Regulating the Trade and Employing the Poor of this Kingdom (Cary, 1719). As demonstrated below, the tool is successful in correcting a number of common errors. If sometimes unreliable, it is also transparent and subject to human intervention.</abstract>
      <url hash="c046e2f0">2020.lt4hala-1.7</url>
      <language>eng</language>
      <bibkey>poncelas-etal-2020-tool</bibkey>
      <pwccode url="https://github.com/alberto-poncelas/tesseract_postprocess" additional="false">alberto-poncelas/tesseract_postprocess</pwccode>
    </paper>
    <paper id="8">
      <title>Integration of Automatic Sentence Segmentation and Lexical Analysis of <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese based on <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model</title>
      <author><first>Ning</first><last>Cheng</last></author>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Liming</first><last>Xiao</last></author>
      <author><first>Changwei</first><last>Xu</last></author>
      <author><first>Sijia</first><last>Ge</last></author>
      <author><first>Xingyue</first><last>Hao</last></author>
      <author><first>Minxuan</first><last>Feng</last></author>
      <pages>52–58</pages>
      <abstract>The basic tasks of ancient Chinese information processing include automatic sentence segmentation, word segmentation, part-of-speech tagging and named entity recognition. Tasks such as lexical analysis need to be based on sentence segmentation because of the reason that a plenty of ancient books are not punctuated. However, step-by-step processing is prone to cause multi-level diffusion of errors. This paper designs and implements an integrated annotation system of sentence segmentation and lexical analysis. The BiLSTM-CRF neural network model is used to verify the generalization ability and the effect of sentence segmentation and lexical analysis on different label levels on four cross-age test sets. Research shows that the integration method adopted in ancient Chinese improves the F1-score of sentence segmentation, word segmentation and part of speech tagging. Based on the experimental results of each test set, the F1-score of sentence segmentation reached 78.95, with an average increase of 3.5%; the F1-score of word segmentation reached 85.73%, with an average increase of 0.18%; and the F1-score of part-of-speech tagging reached 72.65, with an average increase of 0.35%.</abstract>
      <url hash="ede924b9">2020.lt4hala-1.8</url>
      <language>eng</language>
      <bibkey>cheng-etal-2020-integration</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic semantic role labeling in <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek using distributional semantic modeling</title>
      <author><first>Alek</first><last>Keersmaekers</last></author>
      <pages>59–67</pages>
      <abstract>This paper describes a first attempt to automatic semantic role labeling in Ancient Greek, using a supervised machine learning approach. A Random Forest classifier is trained on a small semantically annotated corpus of Ancient Greek, annotated with a large amount of linguistic features, including form of the construction, morphology, part-of-speech, lemmas, animacy, syntax and distributional vectors of Greek words. These vectors turned out to be more important in the model than any other features, likely because they are well suited to handle a low amount of training examples. Overall labeling accuracy was 0.757, with large differences with respect to the specific role that was labeled and with respect to text genre. Some ways to further improve these results include expanding the amount of training examples, improving the quality of the distributional vectors and increasing the consistency of the syntactic annotation.</abstract>
      <url hash="76584691">2020.lt4hala-1.9</url>
      <language>eng</language>
      <bibkey>keersmaekers-2020-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>A Thesaurus for Biblical <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Miriam</first><last>Azar</last></author>
      <author><first>Aliza</first><last>Pahmer</last></author>
      <author><first>Joshua</first><last>Waxman</last></author>
      <pages>68–73</pages>
      <abstract>We built a thesaurus for Biblical Hebrew, with connections between roots based on phonetic, semantic, and distributional similarity. To this end, we apply established algorithms to find connections between headwords based on existing lexicons and other digital resources. For semantic similarity, we utilize the cosine-similarity of tf-idf vectors of English gloss text of Hebrew headwords from Ernest Klein’s A Comprehensive Etymological Dictionary of the Hebrew Language for Readers of English as well as to Brown-Driver-Brigg’s Hebrew Lexicon. For phonetic similarity, we digitize part of Matityahu Clark’s Etymological Dictionary of Biblical Hebrew, grouping Hebrew roots into phonemic classes, and establish phonetic relationships between headwords in Klein’s Dictionary. For distributional similarity, we consider the cosine similarity of PPMI vectors of Hebrew roots and also, in a somewhat novel approach, apply Word2Vec to a Biblical corpus reduced to its lexemes. The resulting resource is helpful to those trying to understand Biblical Hebrew, and also stands as a good basis for programs trying to process the Biblical text.</abstract>
      <url hash="06b60503">2020.lt4hala-1.10</url>
      <language>eng</language>
      <bibkey>azar-etal-2020-thesaurus</bibkey>
    </paper>
    <paper id="11">
      <title>Word Probability Findings in the Voynich Manuscript</title>
      <author><first>Colin</first><last>Layfield</last></author>
      <author><first>Lonneke</first><last>van der Plas</last></author>
      <author><first>Michael</first><last>Rosner</last></author>
      <author><first>John</first><last>Abela</last></author>
      <pages>74–78</pages>
      <abstract>The Voynich Manuscript has baffled scholars for centuries. Some believe the elaborate 15th century codex to be a hoax whilst others believe it is a real medieval manuscript whose contents are as yet unknown. In this paper, we provide additional evidence that the text of the manuscript displays the hallmarks of a proper natural language with respect to the relationship between word probabilities and (i) average information per subword segment and (ii) the relative positioning of consecutive subword segments necessary to uniquely identify words of different probabilities.</abstract>
      <url hash="4e6d206e">2020.lt4hala-1.11</url>
      <language>eng</language>
      <bibkey>layfield-etal-2020-word</bibkey>
    </paper>
    <paper id="12">
      <title>Comparing Statistical and Neural Models for Learning Sound Correspondences</title>
      <author><first>Clémentine</first><last>Fourrier</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>79–83</pages>
      <abstract>Cognate prediction and proto-form reconstruction are key tasks in computational historical linguistics that rely on the study of sound change regularity. Solving these tasks appears to be very similar to machine translation, though methods from that field have barely been applied to historical linguistics. Therefore, in this paper, we investigate the learnability of sound correspondences between a proto-language and daughter languages for two machine-translation-inspired models, one statistical, the other neural. We first carry out our experiments on plausible artificial languages, without noise, in order to study the role of each parameter on the algorithms respective performance under almost perfect conditions. We then study real languages, namely Latin, Italian and Spanish, to see if those performances generalise well. We show that both model types manage to learn sound changes despite data scarcity, although the best performing model type depends on several parameters such as the size of the training data, the ambiguity, and the prediction direction.</abstract>
      <url hash="50e462e3">2020.lt4hala-1.12</url>
      <language>eng</language>
      <bibkey>fourrier-sagot-2020-comparing</bibkey>
      <pwccode url="https://github.com/clefourrier/PLexGen" additional="false">clefourrier/PLexGen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/etymdb-2-0">EtymDB 2.0</pwcdataset>
    </paper>
    <paper id="13">
      <title>Distributional Semantics for Neo-<fixed-case>L</fixed-case>atin</title>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Maria Chiara</first><last>Parisi</last></author>
      <author><first>Martin</first><last>Reynaert</last></author>
      <author><first>Yvette</first><last>Oortwijn</last></author>
      <author><first>Arianna</first><last>Betti</last></author>
      <pages>84–93</pages>
      <abstract>We address the problem of creating and evaluating quality Neo-Latin word embeddings for the purpose of philosophical research, adapting the Nonce2Vec tool to learn embeddings from Neo-Latin sentences. This distributional semantic modeling tool can learn from tiny data incrementally, using a larger background corpus for initialization. We conduct two evaluation tasks: definitional learning of Latin Wikipedia terms, and learning consistent embeddings from 18th century Neo-Latin sentences pertaining to the concept of mathematical method. Our results show that consistent Neo-Latin word embeddings can be learned from this type of data. While our evaluation results are promising, they do not reveal to what extent the learned models match domain expert knowledge of our Neo-Latin texts. Therefore, we propose an additional evaluation method, grounded in expert-annotated data, that would assess whether learned representations are conceptually sound in relation to the domain of study.</abstract>
      <url hash="316b606c">2020.lt4hala-1.13</url>
      <language>eng</language>
      <bibkey>bloem-etal-2020-distributional</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>L</fixed-case>atin-<fixed-case>S</fixed-case>panish Neural Machine Translation: from the <fixed-case>B</fixed-case>ible to Saint Augustine</title>
      <author><first>Eva</first><last>Martínez Garcia</last></author>
      <author><first>Álvaro</first><last>García Tejedor</last></author>
      <pages>94–99</pages>
      <abstract>Although there are several sources where to find historical texts, they usually are available in the original language that makes them generally inaccessible. This paper presents the development of state-of-the-art Neural Machine Systems for the low-resourced Latin-Spanish language pair. First, we build a Transformer-based Machine Translation system on the Bible parallel corpus. Then, we build a comparable corpus from Saint Augustine texts and their translations. We use this corpus to study the domain adaptation case from the Bible texts to Saint Augustine’s works. Results show the difficulties of handling a low-resourced language as Latin. First, we noticed the importance of having enough data, since the systems do not achieve high BLEU scores. Regarding domain adaptation, results show how using in-domain data helps systems to achieve a better quality translation. Also, we observed that it is needed a higher amount of data to perform an effective vocabulary extension that includes in-domain vocabulary.</abstract>
      <url hash="662ea4ca">2020.lt4hala-1.14</url>
      <language>eng</language>
      <bibkey>martinez-garcia-garcia-tejedor-2020-latin</bibkey>
    </paper>
    <paper id="15">
      <title>Detecting Direct Speech in Multilingual Collection of 19th-century Novels</title>
      <author><first>Joanna</first><last>Byszuk</last></author>
      <author><first>Michał</first><last>Woźniak</last></author>
      <author><first>Mike</first><last>Kestemont</last></author>
      <author><first>Albert</first><last>Leśniak</last></author>
      <author><first>Wojciech</first><last>Łukasik</last></author>
      <author><first>Artjoms</first><last>Šeļa</last></author>
      <author><first>Maciej</first><last>Eder</last></author>
      <pages>100–104</pages>
      <abstract>Fictional prose can be broadly divided into narrative and discursive forms with direct speech being central to any discourse representation (alongside indirect reported speech and free indirect discourse). This distinction is crucial in digital literary studies and enables interesting forms of narratological or stylistic analysis. The difficulty of automatically detecting direct speech, however, is currently under-estimated. Rule-based systems that work reasonably well for modern languages struggle with (the lack of) typographical conventions in 19th-century literature. While machine learning approaches to sequence modeling can be applied to solve the task, they typically face a severed skewness in the availability of training material, especially for lesser resourced languages. In this paper, we report the result of a multilingual approach to direct speech detection in a diverse corpus of 19th-century fiction in 9 European languages. The proposed method finetunes a transformer architecture with multilingual sentence embedder on a minimal amount of annotated training in each language, and improves performance across languages with ambiguous direct speech marking, in comparison to a carefully constructed regular expression baseline.</abstract>
      <url hash="1c732a30">2020.lt4hala-1.15</url>
      <language>eng</language>
      <bibkey>byszuk-etal-2020-detecting</bibkey>
    </paper>
    <paper id="16">
      <title>Overview of the <fixed-case>E</fixed-case>va<fixed-case>L</fixed-case>atin 2020 Evaluation Campaign</title>
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Flavio Massimiliano</first><last>Cecchini</last></author>
      <author><first>Matteo</first><last>Pellegrini</last></author>
      <pages>105–110</pages>
      <abstract>This paper describes the first edition of EvaLatin, a campaign totally devoted to the evaluation of NLP tools for Latin. The two shared tasks proposed in EvaLatin 2020, i. e. Lemmatization and Part-of-Speech tagging, are aimed at fostering research in the field of language technologies for Classical languages. The shared dataset consists of texts taken from the Perseus Digital Library, processed with UDPipe models and then manually corrected by Latin experts. The training set includes only prose texts by Classical authors. The test set, alongside with prose texts by the same authors represented in the training set, also includes data relative to poetry and to the Medieval period. This also allows us to propose the Cross-genre and Cross-time subtasks for each task, in order to evaluate the portability of NLP tools for Latin across different genres and time periods. The results obtained by the participants for each task and subtask are presented and discussed.</abstract>
      <url hash="69f3b558">2020.lt4hala-1.16</url>
      <language>eng</language>
      <bibkey>sprugnoli-etal-2020-overview</bibkey>
    </paper>
    <paper id="17">
      <title>Data-driven Choices in Neural Part-of-Speech Tagging for <fixed-case>L</fixed-case>atin</title>
      <author><first>Geoff</first><last>Bacon</last></author>
      <pages>111–113</pages>
      <abstract>Textual data in ancient and historical languages such as Latin is increasingly available in machine readable forms, yet computational tools to analyze and process this data are still lacking. We describe our system for part-of-speech tagging in Latin, an entry in the EvaLatin 2020 shared task. Based on a detailed analysis of the training data, we make targeted preprocessing decisions and design our model. We leverage existing large unlabelled resources to pre-train representations at both the grapheme and word level, which serve as the inputs to our LSTM-based models. We perform an extensive cross-validated hyperparameter search, achieving an accuracy score of up to 93 on in-domain texts. We publicly release all our code and trained models in the hope that our system will be of use to social scientists and digital humanists alike. The insights we draw from our inital analysis can also inform future NLP work modeling syntactic information in Latin.</abstract>
      <url hash="2e586785">2020.lt4hala-1.17</url>
      <language>eng</language>
      <bibkey>bacon-2020-data</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>JHUBC</fixed-case>’s Submission to <fixed-case>LT</fixed-case>4<fixed-case>HALA</fixed-case> <fixed-case>E</fixed-case>va<fixed-case>L</fixed-case>atin 2020</title>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <pages>114–118</pages>
      <abstract>We describe the JHUBC submission to the EvaLatin Shared task on lemmatization and part-of-speech tagging for Latin. We modify a hard-attentional character-based encoder-decoder to produce lemmas and POS tags with separate decoders, and to incorporate contextual tagging cues. While our results show that the dual decoder approach fails to encode data as successfully as the single encoder, our simple context incorporation method does lead to modest improvements.</abstract>
      <url hash="ffe71091">2020.lt4hala-1.18</url>
      <language>eng</language>
      <bibkey>wu-nicolai-2020-jhubcs</bibkey>
    </paper>
    <paper id="19">
      <title>A Gradient Boosting-<fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq System for <fixed-case>L</fixed-case>atin <fixed-case>POS</fixed-case> Tagging and Lemmatization</title>
      <author><first>Giuseppe G. A.</first><last>Celano</last></author>
      <pages>119–123</pages>
      <abstract>The paper presents the system used in the EvaLatin shared task to POS tag and lemmatize Latin. It consists of two components. A gradient boosting machine (LightGBM) is used for POS tagging, mainly fed with pre-computed word embeddings of a window of seven contiguous tokens—the token at hand plus the three preceding and following ones—per target feature value. Word embeddings are trained on the texts of the Perseus Digital Library, Patrologia Latina, and Biblioteca Digitale di Testi Tardo Antichi, which together comprise a high number of texts of different genres from the Classical Age to Late Antiquity. Word forms plus the outputted POS labels are used to feed a seq2seq algorithm implemented in Keras to predict lemmas. The final shared-task accuracies measured for Classical Latin texts are in line with state-of-the-art POS taggers (∼0.96) and lemmatizers (∼0.95).</abstract>
      <url hash="179587e2">2020.lt4hala-1.19</url>
      <language>eng</language>
      <bibkey>celano-2020-gradient</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>UDP</fixed-case>ipe at <fixed-case>E</fixed-case>va<fixed-case>L</fixed-case>atin 2020: Contextualized Embeddings and Treebank Embeddings</title>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <pages>124–129</pages>
      <abstract>We present our contribution to the EvaLatin shared task, which is the first evaluation campaign devoted to the evaluation of NLP tools for Latin. We submitted a system based on UDPipe 2.0, one of the winners of the CoNLL 2018 Shared Task, The 2018 Shared Task on Extrinsic Parser Evaluation and SIGMORPHON 2019 Shared Task. Our system places first by a wide margin both in lemmatization and POS tagging in the open modality, where additional supervised data is allowed, in which case we utilize all Universal Dependency Latin treebanks. In the closed modality, where only the EvaLatin training data is allowed, our system achieves the best performance in lemmatization and in classical subtask of POS tagging, while reaching second place in cross-genre and cross-time settings. In the ablation experiments, we also evaluate the influence of BERT and XLM-RoBERTa contextualized embeddings, and the treebank encodings of the different flavors of Latin treebanks.</abstract>
      <url hash="f6c1099d">2020.lt4hala-1.20</url>
      <language>eng</language>
      <bibkey>straka-strakova-2020-udpipe</bibkey>
    </paper>
    <paper id="21">
      <title>Voting for <fixed-case>POS</fixed-case> tagging of <fixed-case>L</fixed-case>atin texts: Using the flair of <fixed-case>FLAIR</fixed-case> to better Ensemble Classifiers by Example of <fixed-case>L</fixed-case>atin</title>
      <author><first>Manuel</first><last>Stoeckel</last></author>
      <author><first>Alexander</first><last>Henlein</last></author>
      <author><first>Wahed</first><last>Hemati</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>130–135</pages>
      <abstract>Despite the great importance of the Latin language in the past, there are relatively few resources available today to develop modern NLP tools for this language. Therefore, the EvaLatin Shared Task for Lemmatization and Part-of-Speech (POS) tagging was published in the LT4HALA workshop. In our work, we dealt with the second EvaLatin task, that is, POS tagging. Since most of the available Latin word embeddings were trained on either few or inaccurate data, we trained several embeddings on better data in the first step. Based on these embeddings, we trained several state-of-the-art taggers and used them as input for an ensemble classifier called LSTMVoter. We were able to achieve the best results for both the cross-genre and the cross-time task (90.64% and 87.00%) without using additional annotated data (closed modality). In the meantime, we further improved the system and achieved even better results (96.91% on classical, 90.87% on cross-genre and 87.35% on cross-time).</abstract>
      <url hash="2e5901d3">2020.lt4hala-1.21</url>
      <language>eng</language>
      <bibkey>stoeckel-etal-2020-voting</bibkey>
    </paper>
  </volume>
</collection>
