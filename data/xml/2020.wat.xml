<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.wat">
  <volume id="1" ingest-date="2020-12-02" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Asian Translation</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Hideki</first><last>Nakayama</last></editor>
      <editor><first>Chenchen</first><last>Ding</last></editor>
      <editor><first>Raj</first><last>Dabre</last></editor>
      <editor><first>Anoop</first><last>Kunchukuttan</last></editor>
      <editor><first>Win Pa</first><last>Pa</last></editor>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Shantipriya</first><last>Parida</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <editor><first>Hidaya</first><last>Mino</last></editor>
      <editor><first>Hiroshi</first><last>Manabe</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Sadao</first><last>Kurohashi</last></editor>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
      <venue>wat</venue>
    </meta>
    <frontmatter>
      <url hash="e4d5969c">2020.wat-1.0</url>
      <bibkey>wat-2020-asian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Overview of the 7th Workshop on <fixed-case>A</fixed-case>sian Translation</title>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <author><first>Chenchen</first><last>Ding</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Isao</first><last>Goto</last></author>
      <author><first>Win</first><last>Pa Pa</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1–44</pages>
      <abstract>This paper presents the results of the shared tasks from the 7th workshop on Asian translation (WAT2020). For the WAT2020, 20 teams participated in the shared tasks and 14 teams submitted their translation results for the human evaluation. We also received 12 research paper submissions out of which 7 were accepted. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.</abstract>
      <url hash="9183d081">2020.wat-1.1</url>
      <bibkey>nakazawa-etal-2020-overview</bibkey>
    </paper>
    <paper id="2">
      <title>An Effective Optimization Method for Neural Machine Translation: The Case of <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>ersian Bilingually Low-Resource Scenario</title>
      <author><first>Benyamin</first><last>Ahmadnia</last></author>
      <author><first>Raul</first><last>Aranovich</last></author>
      <pages>45–49</pages>
      <abstract>In this paper, we propose a useful optimization method for low-resource Neural Machine Translation (NMT) by investigating the effectiveness of multiple neural network optimization algorithms. Our results confirm that applying the proposed optimization method on English-Persian translation can exceed translation quality compared to the English-Persian Statistical Machine Translation (SMT) paradigm.</abstract>
      <url hash="b4cec6ee">2020.wat-1.2</url>
      <bibkey>ahmadnia-aranovich-2020-effective</bibkey>
    </paper>
    <paper id="3">
      <title>Transformer-based Double-token Bidirectional Autoregressive Decoding in Neural Machine Translation</title>
      <author><first>Kenji</first><last>Imamura</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>50–57</pages>
      <abstract>This paper presents a simple method that extends a standard Transformer-based autoregressive decoder, to speed up decoding. The proposed method generates a token from the head and tail of a sentence (two tokens in total) in each step. By simultaneously generating multiple tokens that rarely depend on each other, the decoding speed is increased while the degradation in translation quality is minimized. In our experiments, the proposed method increased the translation speed by around 113%-155% in comparison with a standard autoregressive decoder, while degrading the BLEU scores by no more than 1.03. It was faster than an iterative non-autoregressive decoder in many conditions.</abstract>
      <url hash="488b23ba">2020.wat-1.3</url>
      <bibkey>imamura-sumita-2020-transformer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="4">
      <title>Translation of New Named Entities from <fixed-case>E</fixed-case>nglish to <fixed-case>C</fixed-case>hinese</title>
      <author><first>Zizheng</first><last>Zhang</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Wei</first><last>Houjing</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>58–63</pages>
      <abstract>New things are being created and new words are constantly being added to languages worldwide. However, it is not practical to translate them all manually into a new foreign language. When translating from an alphabetic language such as English to Chinese, appropriate Chinese characters must be assigned, which is particularly costly compared to other language pairs. Therefore, we propose a task of generating and evaluating new translations from English to Chinese focusing on named entities. We defined three criteria for human evaluation—fluency, adequacy of pronunciation, and adequacy of meaning—and constructed evaluation data based on these definitions. In addition, we built a baseline system and analyzed the output of the system.</abstract>
      <url hash="024f27ff">2020.wat-1.4</url>
      <bibkey>zhang-etal-2020-translation</bibkey>
      <pwccode url="https://github.com/toshohirasawa/enzh-named-entity-translation" additional="false">toshohirasawa/enzh-named-entity-translation</pwccode>
    </paper>
    <paper id="5">
      <title>Meta Ensemble for <fixed-case>J</fixed-case>apanese-<fixed-case>C</fixed-case>hinese Neural Machine Translation: <fixed-case>K</fixed-case>yoto-<fixed-case>U</fixed-case>+<fixed-case>ECNU</fixed-case> Participation to <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Zhuoyuan</first><last>Mao</last></author>
      <author><first>Yibin</first><last>Shen</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <author><first>Cheqing</first><last>Jin</last></author>
      <pages>64–71</pages>
      <abstract>This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.</abstract>
      <url hash="e7252315">2020.wat-1.5</url>
      <bibkey>mao-etal-2020-meta</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="6">
      <title>Neural Machine Translation Using Extracted Context Based on Deep Analysis for the <fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Newswire Task at <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Isao</first><last>Goto</last></author>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Hitoshi</first><last>Ito</last></author>
      <author><first>Kazutaka</first><last>Kinugawa</last></author>
      <author><first>Ichiro</first><last>Yamada</last></author>
      <author><first>Hideki</first><last>Tanaka</last></author>
      <pages>72–79</pages>
      <abstract>This paper describes the system of the NHK-NES team for the WAT 2020 Japanese–English newswire task. There are two main problems in Japanese-English news translation: translation of dropped subjects and compatibility between equivalent translations and English news-style outputs. We address these problems by extracting subjects from the context based on predicate-argument structures and using them as additional inputs, and constructing parallel Japanese-English news sentences equivalently translated from English news sentences. The evaluation results confirm the effectiveness of our context-utilization method.</abstract>
      <url hash="ceec65c0">2020.wat-1.6</url>
      <bibkey>goto-etal-2020-neural</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>TMU</fixed-case> <fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Multimodal Machine Translation System for <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Hiroto</first><last>Tamura</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>80–91</pages>
      <abstract>We introduce our TMU system submitted to the Japanese&lt;-&gt;English Multimodal Task (constrained) for WAT 2020 (Nakazawa et al., 2020). This task aims to improve translation performance with the help of another modality (images) associated with the input sentences. In a multimodal translation task, the dataset is, by its nature, a low-resource one. Our method used herein augments the data by generating noisy translations and adding noise to existing training images. Subsequently, we pretrain a translation model on the augmented noisy data, and then fine-tune it on the clean data. We also examine the probabilistic dropping of either the textual or visual context vector in the decoder. This aims to regularize the network to make use of both features while training. The experimental results indicate that translation performance can be improved using our method of textual data augmentation with noising on the target side and probabilistic dropping of either context vector.</abstract>
      <url hash="3b96aa65">2020.wat-1.7</url>
      <bibkey>tamura-etal-2020-tmu</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WAT</fixed-case> 2020 Indic Languages Multilingual Task</title>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>92–97</pages>
      <abstract>This paper describes our work in the WAT 2020 Indic Multilingual Translation Task. We participated in all 7 language pairs (En&lt;-&gt;Bn/Hi/Gu/Ml/Mr/Ta/Te) in both directions under the constrained condition—using only the officially provided data. Using transformer as a baseline, our Multi-&gt;En and En-&gt;Multi translation systems achieve the best performances. Detailed data filtering and data domain selection are the keys to performance enhancement in our experiment, with an average improvement of 2.6 BLEU scores for each language pair in the En-&gt;Multi system and an average improvement of 4.6 BLEU scores regarding the Multi-&gt;En. In addition, we employed language independent adapter to further improve the system performances. Our submission obtains competitive results in the final evaluation.</abstract>
      <url hash="3d3cf08c">2020.wat-1.8</url>
      <bibkey>yu-etal-2020-hw</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>NICT</fixed-case>‘s Submission To <fixed-case>WAT</fixed-case> 2020: How Effective Are Simple Many-To-Many Neural Machine Translation Models?</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <pages>98–102</pages>
      <abstract>In this paper we describe our team‘s (NICT-5) Neural Machine Translation (NMT) models whose translations were submitted to shared tasks of the 7th Workshop on Asian Translation. We participated in the Indic language multilingual sub-task as well as the NICT-SAP multilingual multi-domain sub-task. We focused on naive many-to-many NMT models which gave reasonable translation quality despite their simplicity. Our observations are twofold: (a.) Many-to-many models suffer from a lack of consistency where the translation quality for some language pairs is very good but for some others it is terrible when compared against one-to-many and many-to-one baselines. (b.) Oversampling smaller corpora does not necessarily give the best translation quality for the language pair associated with that pair.</abstract>
      <url hash="59e82f53">2020.wat-1.9</url>
      <bibkey>dabre-chakrabarty-2020-nicts</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>ODIANLP</fixed-case>’s Participation in <fixed-case>WAT</fixed-case>2020</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <author><first>Amulya Ratna</first><last>Dash</last></author>
      <author><first>Satya Ranjan</first><last>Dash</last></author>
      <author><first>Debasish Kumar</first><last>Mallick</last></author>
      <author><first>Satya Prakash</first><last>Biswal</last></author>
      <author><first>Priyanka</first><last>Pattnaik</last></author>
      <author><first>Biranchi Narayan</first><last>Nayak</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>103–108</pages>
      <abstract>This paper describes the ODIANLP submission to WAT 2020. We have participated in the English-Hindi Multimodal task and Indic task. We have used the state-of-the-art Transformer model for the translation task and InceptionResNetV2 for the Hindi Image Captioning task. Our submission tops in English-&gt;Hindi Multimodal task in its track and Odia&lt;-&gt;English translation tasks. Also, our submissions performed well in the Indic Multilingual tasks.</abstract>
      <url hash="10b76e22">2020.wat-1.10</url>
      <bibkey>parida-etal-2020-odianlps</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pmindia">PMIndia</pwcdataset>
    </paper>
    <paper id="11">
      <title>Multimodal Neural Machine Translation for <fixed-case>E</fixed-case>nglish to <fixed-case>H</fixed-case>indi</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Abdullah Faiz Ur Rahman</first><last>Khilji</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>109–113</pages>
      <abstract>Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for low resource languages. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the system for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in English to Hindi. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves Bilingual Evaluation Understudy (BLEU) score of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.</abstract>
      <url hash="aa3f66e3">2020.wat-1.11</url>
      <bibkey>laskar-etal-2020-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hindi-visual-genome">Hindi Visual Genome</pwcdataset>
    </paper>
    <paper id="12">
      <title>The <fixed-case>ADAPT</fixed-case> Centre’s Participation in <fixed-case>WAT</fixed-case> 2020 <fixed-case>E</fixed-case>nglish-to-<fixed-case>O</fixed-case>dia Translation Task</title>
      <author><first>Prashanth</first><last>Nayak</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>114–117</pages>
      <abstract>This paper describes the ADAPT Centre sub-missions to WAT 2020 for the English-to-Odia translation task. We present the approaches that we followed to try to build competitive machine translation (MT) systems for English-to-Odia. Our approaches include monolingual data selection for creating synthetic data and identifying optimal sets of hyperparameters for the Transformer in a low-resource scenario. Our best MT system produces 4.96BLEU points on the evaluation test set in the English-to-Odia translation task.</abstract>
      <url hash="ca606622">2020.wat-1.12</url>
      <bibkey>nayak-etal-2020-adapt</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>NLPRL</fixed-case> <fixed-case>O</fixed-case>dia-<fixed-case>E</fixed-case>nglish: Indic Language Neural Machine Translation System</title>
      <author><first>Rupjyoti</first><last>Baruah</last></author>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <pages>118–121</pages>
      <abstract>In this manuscript, we (team name is NLPRL) describe systems description that was submitted to the translation shared tasks at WAT 2020. We describe our model as transformer based NMT by using byte-level based BPE (BBPE). We used the OdiEnCorp 2.0 parallel corpus provided by the shared task organizer where the training, validation, and test data contain 69370, 13544, and 14344 lines of parallel sentences, respectively. The evaluation results show the BLEU score of English-to-Oria below the Organizer (1.34) and Oria-to-English direction shows above the Organizer (11.33).</abstract>
      <url hash="d923db7f">2020.wat-1.13</url>
      <bibkey>baruah-mundotiya-2020-nlprl</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>WT</fixed-case>: Wipro <fixed-case>AI</fixed-case> Submissions to the <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <pages>122–126</pages>
      <abstract>In this paper we present an English–Hindi and Hindi–English neural machine translation (NMT) system, submitted to the Translation shared Task organized at WAT 2020. We trained a multilingual NMT system based on transformer architecture. In this paper we show: (i) how effective pre-processing helps to improve performance, (ii) how synthetic data through back-translation from available monolingual data can help in overall translation performance, (iii) how language similarity can aid more onto it. Our submissions ranked 1st in both English to Hindi and Hindi to English translation achieving BLEU 20.80 and 29.59 respectively.</abstract>
      <url hash="d5c0308a">2020.wat-1.14</url>
      <bibkey>pal-2020-wt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="15">
      <title><fixed-case>K</fixed-case>orean-to-<fixed-case>J</fixed-case>apanese Neural Machine Translation System using Hanja Information</title>
      <author><first>Hwichan</first><last>Kim</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>127–134</pages>
      <abstract>In this paper, we describe our TMU neural machine translation (NMT) system submitted for the Patent task (Korean→Japanese) of the 7th Workshop on Asian Translation (WAT 2020, Nakazawa et al., 2020). We propose a novel method to train a Korean-to-Japanese translation model. Specifically, we focus on the vocabulary overlap of Korean Hanja words and Japanese Kanji words, and propose strategies to leverage Hanja information. Our experiment shows that Hanja information is effective within a specific domain, leading to an improvement in the BLEU scores by +1.09 points compared to the baseline.</abstract>
      <url hash="9e624212">2020.wat-1.15</url>
      <bibkey>kim-etal-2020-korean</bibkey>
    </paper>
    <paper id="16">
      <title>Goku’s Participation in <fixed-case>WAT</fixed-case> 2020</title>
      <author><first>Dongzhe</first><last>Wang</last></author>
      <author><first>Ohnmar</first><last>Htun</last></author>
      <pages>135–141</pages>
      <abstract>This paper introduces our neural machine translation systems’ participation in the WAT 2020 (team ID: goku20). We participated in the (i) Patent, (ii) Business Scene Dialogue (BSD) document-level translation, (iii) Mixed-domain tasks. Regardless of simplicity, standard Transformer models have been proven to be very effective in many machine translation systems. Recently, some advanced pre-training generative models have been proposed on the basis of encoder-decoder framework. Our main focus of this work is to explore how robust Transformer models perform in translation from sentence-level to document-level, from resource-rich to low-resource languages. Additionally, we also investigated the improvement that fine-tuning on the top of pre-trained transformer-based models can achieve on various tasks.</abstract>
      <url hash="214f2f6b">2020.wat-1.16</url>
      <bibkey>wang-htun-2020-gokus</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
    </paper>
    <paper id="17">
      <title>The <fixed-case>ADAPT</fixed-case> Centre’s Neural <fixed-case>MT</fixed-case> Systems for the <fixed-case>WAT</fixed-case> 2020 Document-Level Translation Task</title>
      <author><first>Wandri</first><last>Jooste</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>142–146</pages>
      <abstract>In this paper we describe the ADAPT Centre’s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from Japanese to English for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality, we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems, as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our systems and report the accuracy achieved through these various experiments.</abstract>
      <url hash="5ac897bd">2020.wat-1.17</url>
      <bibkey>jooste-etal-2020-adapt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="18">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>T</fixed-case>okyo’s Submissions to the <fixed-case>WAT</fixed-case> 2020 Shared Task</title>
      <author><first>Matīss</first><last>Rikters</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Ryokan</first><last>Ri</last></author>
      <pages>147–153</pages>
      <abstract>The paper describes the development process of the The University of Tokyo’s NMT systems that were submitted to the WAT 2020 Document-level Business Scene Dialogue Translation sub-task. We describe the data processing workflow, NMT system training architectures, and automatic evaluation results. For the WAT 2020 shared task, we submitted 12 systems (both constrained and unconstrained) for English-Japanese and Japanese-English translation directions. The submitted systems were trained using Transformer models and one was a SMT baseline.</abstract>
      <url hash="c6cde467">2020.wat-1.18</url>
      <bibkey>rikters-etal-2020-university</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/business-scene-dialogue">Business Scene Dialogue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jparacrawl">JParaCrawl</pwcdataset>
    </paper>
    <paper id="19">
      <title>Improving <fixed-case>NMT</fixed-case> via Filtered Back Translation</title>
      <author><first>Nikhil</first><last>Jaiswal</last></author>
      <author><first>Mayur</first><last>Patidar</last></author>
      <author><first>Surabhi</first><last>Kumari</last></author>
      <author><first>Manasi</first><last>Patwardhan</last></author>
      <author><first>Shirish</first><last>Karande</last></author>
      <author><first>Puneet</first><last>Agarwal</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <pages>154–159</pages>
      <abstract>Document-Level Machine Translation (MT) has become an active research area among the NLP community in recent years. Unlike sentence-level MT, which translates the sentences independently, document-level MT aims to utilize contextual information while translating a given source sentence. This paper demonstrates our submission (Team ID - DEEPNLP) to the Document-Level Translation task organized by WAT 2020. This task focuses on translating texts from a business dialog corpus while optionally utilizing the context present in the dialog. In our proposed approach, we utilize publicly available parallel corpus from different domains to train an open domain base NMT model. We then use monolingual target data to create filtered pseudo parallel data and employ Back-Translation to fine-tune the base model. This is further followed by fine-tuning on the domain-specific corpus. We also ensemble various models to improvise the translation performance. Our best models achieve a BLEU score of 26.59 and 22.83 in an unconstrained setting and 15.10 and 10.91 in the constrained settings for En-&gt;Ja &amp; Ja-&gt;En direction, respectively.</abstract>
      <url hash="29250156">2020.wat-1.19</url>
      <bibkey>jaiswal-etal-2020-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="20">
      <title>A Parallel Evaluation Data Set of Software Documentation with Document Structure Annotation</title>
      <author><first>Bianka</first><last>Buschbeck</last></author>
      <author><first>Miriam</first><last>Exel</last></author>
      <pages>160–169</pages>
      <abstract>This paper accompanies the software documentation data set for machine translation, a parallel evaluation data set of data originating from the SAP Help Portal, that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate machine translation systems in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi, Indonesian, Malay and Thai, and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text, the segments in this data set come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation, the particularities and characteristics of the data set as well as machine translation results.</abstract>
      <url hash="9f2ed0b7">2020.wat-1.20</url>
      <bibkey>buschbeck-exel-2020-parallel</bibkey>
      <pwccode url="https://github.com/SAP/software-documentation-data-set-for-machine-translation" additional="false">SAP/software-documentation-data-set-for-machine-translation</pwccode>
    </paper>
    <paper id="21">
      <title>Inference-only sub-character decomposition improves translation of unseen logographic characters</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Weston</first><last>Feely</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>170–177</pages>
      <abstract>Neural Machine Translation (NMT) on logographic source languages struggles when translating ‘unseen’ characters, which never appear in the training data. One possible approach to this problem uses sub-character decomposition for training and test sentences. However, this approach involves complete retraining, and its effectiveness for unseen character translation to non-logographic languages has not been fully explored. We investigate existing ideograph-based sub-character decomposition approaches for Chinese-to-English and Japanese-to-English NMT, for both high-resource and low-resource domains. For each language pair and domain we construct a test set where all source sentences contain at least one unseen logographic character. We find that complete sub-character decomposition often harms unseen character translation, and gives inconsistent results generally. We offer a simple alternative based on decomposition before inference for unseen characters only. Our approach allows flexible application, achieving translation adequacy improvements and requiring no additional models or training.</abstract>
      <url hash="fe877af1">2020.wat-1.21</url>
      <bibkey>saunders-etal-2020-inference</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
    </paper>
    <paper id="22">
      <title>An Error-based Investigation of Statistical and Neural Machine Translation Performance on <fixed-case>H</fixed-case>indi-to-<fixed-case>T</fixed-case>amil and <fixed-case>E</fixed-case>nglish-to-<fixed-case>T</fixed-case>amil</title>
      <author><first>Akshai</first><last>Ramesh</last></author>
      <author><first>Venkatesh</first><last>Balavadhani Parthasa</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>178–188</pages>
      <abstract>Statistical machine translation (SMT) was the state-of-the-art in machine translation (MT) research for more than two decades, but has since been superseded by neural MT (NMT). Despite producing state-of-the-art results in many translation tasks, neural models underperform in resource-poor scenarios. Despite some success, none of the present-day benchmarks that have tried to overcome this problem can be regarded as a universal solution to the problem of translation of many low-resource languages. In this work, we investigate the performance of phrase-based SMT (PB-SMT) and NMT on two rarely-tested low-resource language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation.</abstract>
      <url hash="922964b7">2020.wat-1.22</url>
      <bibkey>ramesh-etal-2020-error</bibkey>
    </paper>
  </volume>
</collection>
