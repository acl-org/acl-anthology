<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlperspectives">
  <volume id="1" ingest-date="2024-05-16" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024</booktitle>
      <editor><first>Gavin</first><last>Abercrombie</last></editor>
      <editor><first>Valerio</first><last>Basile</last></editor>
      <editor><first>Davide</first><last>Bernadi</last></editor>
      <editor><first>Shiran</first><last>Dudy</last></editor>
      <editor><first>Simona</first><last>Frenda</last></editor>
      <editor><first>Lucy</first><last>Havens</last></editor>
      <editor><first>Sara</first><last>Tonelli</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="deb5f2db">2024.nlperspectives-1</url>
      <venue>nlperspectives</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="abe37c2b">2024.nlperspectives-1.0</url>
      <bibkey>nlperspectives-2024-perspectivist</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Is a picture of a bird a bird? A mixed-methods approach to understanding diverse human perspectives and ambiguity in machine vision models</title>
      <author><first>Alicia</first><last>Parrish</last></author>
      <author><first>Susan</first><last>Hao</last></author>
      <author><first>Sarah</first><last>Laszlo</last></author>
      <author id="lora-aroyo"><first>Lora</first><last>Aroyo</last></author>
      <pages>1–18</pages>
      <abstract>Human experiences are complex and subjective. This subjectivity is reflected in the way people label images for machine vision models. While annotation tasks are often assumed to deliver objective results, this assumption does not allow for the subjectivity of human experience. This paper examines the implications of subjective human judgments in the behavioral task of labeling images used to train machine vision models. We identify three primary sources of ambiguity: (1) depictions of labels in the images can be simply ambiguous, (2) raters’ backgrounds and experiences can influence their judgments and (3) the way the labeling task is defined can also influence raters’ judgments. By taking steps to address these sources of ambiguity, we can create more robust and reliable machine vision models.</abstract>
      <url hash="9f6fd9c0">2024.nlperspectives-1.1</url>
      <bibkey>parrish-etal-2024-picture</bibkey>
    </paper>
    <paper id="2">
      <title>Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>19–30</pages>
      <abstract>Large Language Models (LLMs) exhibit remarkable text classification capabilities, excelling in zero- and few-shot learning (ZSL and FSL) scenarios. However, since they are trained on different datasets, performance varies widely across tasks between those models. Recent studies emphasize the importance of considering human label variation in data annotation. However, how this human label variation also applies to LLMs remains unexplored. Given this likely model specialization, we ask: Do aggregate LLM labels improve over individual models (as for human annotators)? We evaluate four recent instruction-tuned LLMs as “annotators” on five subjective tasks across four languages. We use ZSL and FSL setups and label aggregation from human annotation. Aggregations are indeed substantially better than any individual model, benefiting from specialization in diverse tasks or languages. Surprisingly, FSL does not surpass ZSL, as it depends on the quality of the selected examples. However, there seems to be no good information-theoretical strategy to select those. We find that no LLM method rivals even simple supervised models. We also discuss the tradeoffs in accuracy, cost, and moral/ethical considerations between LLM and human annotation.</abstract>
      <url hash="82b08a13">2024.nlperspectives-1.2</url>
      <bibkey>plaza-del-arco-etal-2024-wisdom</bibkey>
    </paper>
    <paper id="3">
      <title>Revisiting Annotation of Online Gender-Based Violence</title>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>Nikolas</first><last>Vitsakis</last></author>
      <author><first>Aiqi</first><last>Jiang</last></author>
      <author><first>Ioannis</first><last>Konstas</last></author>
      <pages>31–41</pages>
      <abstract>Online Gender-Based Violence is an increasing problem, but existing datasets fail to capture the plurality of possible annotator perspectives or ensure representation of affected groups. In a pilot study, we revisit the annotation of a widely used dataset to investigate the relationship between annotator identities and underlying attitudes and the responses they give to a sexism labelling task. We collect demographic and attitudinal information about crowd-sourced annotators using two validated surveys from Social Psychology. While we do not find any correlation between underlying attitudes and annotation behaviour, ethnicity does appear to be related to annotator responses for this pool of crowd-workers. We also conduct initial classification experiments using Large Language Models, finding that a state-of-the-art model trained with human feedback benefits from our broad data collection to perform better on the new labels. This study represents the initial stages of a wider data collection project, in which we aim to develop a taxonomy of GBV in partnership with affected stakeholders.</abstract>
      <url hash="cc8ea37c">2024.nlperspectives-1.3</url>
      <bibkey>abercrombie-etal-2024-revisiting</bibkey>
    </paper>
    <paper id="4">
      <title>A Perspectivist Corpus of Numbers in Social Judgements</title>
      <author><first>Marlon</first><last>May</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <pages>42–48</pages>
      <abstract>With growing interest in the use of large language models, it is becoming increasingly important to understand whose views they express. These models tend to generate output that conforms to majority opinion and are not representative of diverse views. As a step toward building models that can take differing views into consideration, we build a novel corpus of social judgements. We crowdsourced annotations of a subset of the Commonsense Norm Bank that contained numbers in the situation descriptions and asked annotators to replace the number with a range defined by a start and end value that, in their view, correspond to the given verdict. Our corpus contains unaggregated annotations and annotator demographics. We describe our annotation process for social judgements and will release our dataset to support future work on numerical reasoning and perspectivist approaches to natural language processing.</abstract>
      <url hash="76271c4c">2024.nlperspectives-1.4</url>
      <bibkey>may-etal-2024-perspectivist</bibkey>
    </paper>
    <paper id="5">
      <title>An Overview of Recent Approaches to Enable Diversity in Large Language Models through Aligning with Human Perspectives</title>
      <author><first>Benedetta</first><last>Muscato</last></author>
      <author><first>Chandana Sree</first><last>Mala</last></author>
      <author><first>Marta</first><last>Marchiori Manerba</last></author>
      <author><first>Gizem</first><last>Gezici</last></author>
      <author><first>Fosca</first><last>Giannotti</last></author>
      <pages>49–55</pages>
      <abstract>The varied backgrounds and experiences of human annotators inject different opinions and potential biases into the data, inevitably leading to disagreements. Yet, traditional aggregation methods fail to capture individual judgments since they rely on the notion of a single ground truth. Our aim is to review prior contributions to pinpoint the shortcomings that might cause stereotypical content generation. As a preliminary study, our purpose is to investigate state-of-the-art approaches, primarily focusing on the following two research directions. First, we investigate how adding subjectivity aspects to LLMs might guarantee diversity. We then look into the alignment between humans and LLMs and discuss how to measure it. Considering existing gaps, our review explores possible methods to mitigate the perpetuation of biases targeting specific communities. However, we recognize the potential risk of disseminating sensitive information due to the utilization of socio-demographic data in the training process. These considerations underscore the inclusion of diverse perspectives while taking into account the critical importance of implementing robust safeguards to protect individuals’ privacy and prevent the inadvertent propagation of sensitive information.</abstract>
      <url hash="27d2330b">2024.nlperspectives-1.5</url>
      <bibkey>muscato-etal-2024-overview</bibkey>
    </paper>
    <paper id="6">
      <title>Disagreement in Argumentation Annotation</title>
      <author><first>Anna</first><last>Lindahl</last></author>
      <pages>56–66</pages>
      <abstract>Disagreement, perspective or error? There is a growing discussion against the idea of a unified ground truth in annotated data, as well as the usefulness of such a ground truth and resulting gold standard. In data perspectivism, this issue is exemplified with tasks such as hate speech or sentiment classification in which annotators’ different perspectives are important to include. In this paper we turn to argumentation, a related field which has had less focus from this point of view. Argumentation is difficult to annotate for several reasons, from the more practical parts of deciding where the argumentation begins and ends to questions of how argumentation is defined and what it consists of. Learning more about disagreement is therefore important in order to improve argument annotation and to better utilize argument annotated data. Because of this, we examine disagreement in two corpora annotated with argumentation both manually and computationally. We find that disagreement is often not because of annotation errors or mistakes but due to the possibility of multiple possible interpretations. More specifically, these interpretations can be over boundaries, label or existence of argumentation. These results emphasize the need for more thorough analysis of disagreement in data, outside of the more common inter-annotator agreement measures.</abstract>
      <url hash="b583e11b">2024.nlperspectives-1.6</url>
      <bibkey>lindahl-2024-disagreement</bibkey>
    </paper>
    <paper id="7">
      <title>Moral Disagreement over Serious Matters: Discovering the Knowledge Hidden in the Perspectives</title>
      <author><first>Anny D.</first><last>Alvarez Nogales</last></author>
      <author><first>Oscar</first><last>Araque</last></author>
      <pages>67–77</pages>
      <abstract>Moral values significantly define decision-making processes, notably on contentious issues like global warming. The Moral Foundations Theory (MFT) delineates morality and aims to reconcile moral expressions across cultures, yet different interpretations arise, posing challenges for computational modeling. This paper addresses the need to incorporate diverse moral perspectives into the learning systems used to estimate morality in text. To do so, it explores how training language models with varied annotator perspectives affects the performance of the learners. Building on top if this, this work also proposes an ensemble method that exploits the diverse perspectives of annotators to construct a more robust moral estimation model. Additionally, we investigate the automated identification of texts that pose annotation challenges, enhancing the understanding of linguistic cues towards annotator disagreement. To evaluate the proposed models we use the Moral Foundations Twitter Corpus (MFTC), a resource that is currently the reference for modeling moral values in computational social sciences. We observe that incorporating the diverse perspectives of annotators into an ensemble model benefits the learning process, showing large improvements in the classification performance. Finally, the results also indicate that instances that convey strong moral meaning are more challenging to annotate.</abstract>
      <url hash="a5c5a110">2024.nlperspectives-1.7</url>
      <bibkey>alvarez-nogales-araque-2024-moral</bibkey>
    </paper>
    <paper id="8">
      <title>Perspectives on Hate: General vs. Domain-Specific Models</title>
      <author><first>Giulia</first><last>Rizzi</last></author>
      <author><first>Michele</first><last>Fontana</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <pages>78–83</pages>
      <abstract>The rise of online hostility, combined with broad social media use, leads to the necessity of the comprehension of its human impact. However, the process of hate identification is challenging because, on the one hand, the line between healthy disagreement and poisonous speech is not well defined, and, on the other hand, multiple socio-cultural factors or prior beliefs shape people’s perceptions of potentially harmful text. To address disagreements in hate speech identification, Natural Language Processing (NLP) models must capture several perspectives. This paper introduces a strategy based on the Contrastive Learning paradigm for detecting disagreements in hate speech using pre-trained language models. Two approaches are proposed: the General Model, a comprehensive framework, and the Domain-Specific Model, which focuses on more specific hate-related tasks. The source code is available at ://anonymous.4open.science/r/Disagreement-530C.</abstract>
      <url hash="df67be09">2024.nlperspectives-1.8</url>
      <bibkey>rizzi-etal-2024-perspectives</bibkey>
    </paper>
    <paper id="9">
      <title>Soft metrics for evaluation with disagreements: an assessment</title>
      <author><first>Giulia</first><last>Rizzi</last></author>
      <author><first>Elisa</first><last>Leonardelli</last></author>
      <author id="massimo-poesio"><first>Massimo</first><last>Poesio</last></author>
      <author><first>Alexandra</first><last>Uma</last></author>
      <author><first>Maja</first><last>Pavlovic</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <pages>84–94</pages>
      <abstract>The move towards preserving judgement disagreements in NLP requires the identification of adequate evaluation metrics. We identify a set of key properties that such metrics should have, and assess the extent to which natural candidates for soft evaluation such as Cross Entropy satisfy such properties. We employ a theoretical framework, supported by a visual approach, by practical examples, and by the analysis of a real case scenario. Our results indicate that Cross Entropy can result in fairly paradoxical results in some cases, whereas other measures Manhattan distance and Euclidean distance exhibit a more intuitive behavior, at least for the case of binary classification.</abstract>
      <url hash="024dde3b">2024.nlperspectives-1.9</url>
      <bibkey>rizzi-etal-2024-soft</bibkey>
    </paper>
    <paper id="10">
      <title>Designing <fixed-case>NLP</fixed-case> Systems That Adapt to Diverse Worldviews</title>
      <author><first>Claudiu</first><last>Creanga</last></author>
      <author id="liviu-p-dinu"><first>Liviu P.</first><last>Dinu</last></author>
      <pages>95–99</pages>
      <abstract>Natural Language Inference (NLI) is foundational for evaluating language understanding in AI. However, progress has plateaued, with models failing on ambiguous examples and exhibiting poor generalization. We argue that this stems from disregarding the subjective nature of meaning, which is intrinsically tied to an individual’s <i>weltanschauung</i> (which roughly translates to worldview). Existing NLP datasets often obscure this by aggregating labels or filtering out disagreement. We propose a perspectivist approach: building datasets that capture annotator demographics, values, and justifications for their labels. Such datasets would explicitly model diverse worldviews. Our initial experiments with a subset of the SBIC dataset demonstrate that even limited annotator metadata can improve model performance.</abstract>
      <url hash="e34ec67e">2024.nlperspectives-1.10</url>
      <bibkey>creanga-dinu-2024-designing</bibkey>
    </paper>
    <paper id="11">
      <title>The Effectiveness of <fixed-case>LLM</fixed-case>s as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation</title>
      <author><first>Maja</first><last>Pavlovic</last></author>
      <author id="massimo-poesio"><first>Massimo</first><last>Poesio</last></author>
      <pages>100–110</pages>
      <abstract>Recent studies focus on exploring the capability of Large Language Models (LLMs) for data annotation. Our work, firstly, offers a comparative overview of twelve such studies that investigate labelling with LLMs, particularly focusing on classification tasks. Secondly, we present an empirical analysis that examines the degree of alignment between the opinion distributions returned by GPT and those provided by human annotators across four subjective datasets. Our analysis supports a minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.</abstract>
      <url hash="5f2fcebe">2024.nlperspectives-1.11</url>
      <bibkey>pavlovic-poesio-2024-effectiveness</bibkey>
    </paper>
    <paper id="12">
      <title>What Does Perspectivism Mean? An Ethical and Methodological Countercriticism</title>
      <author><first>Mathieu</first><last>Valette</last></author>
      <pages>111–115</pages>
      <abstract>In this paper, we address the epistemological and ethical break of perspectivism in NLP. First, we propose to consider data annotation from the point of view of the scientific management of annotation work - which is part of the automation process inherent in NLP, in order to ideologically situate the perspectivist paradigm. We then analyze some of the concepts of perspectivism (in particular, truth). Finally, based on this analysis, we formulate a set of proposals aimed at overcoming the observed limitations of corpus annotation in general and perspectivism in particular.</abstract>
      <url hash="6ae58086">2024.nlperspectives-1.12</url>
      <bibkey>valette-2024-perspectivism</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>O</fixed-case>rigam<fixed-case>IM</fixed-case>: A Dataset of Ambiguous Sentence Interpretations for Social Grounding and Implicit Language Understanding</title>
      <author><first>Liesbeth</first><last>Allein</last></author>
      <author id="marie-francine-moens"><first>Marie-Francine</first><last>Moens</last></author>
      <pages>116–122</pages>
      <abstract>Sentences elicit different interpretations and reactions among readers, especially when there is ambiguity in their implicit layers. We present a first-of-its kind dataset of sentences from Reddit, where each sentence is annotated with multiple interpretations of its meanings, understandings of implicit moral judgments about mentioned people, and reader impressions of its author. Scrutiny of the dataset proves the evoked variability and polarity in reactions. It further shows that readers strongly disagree on both the presence of implied judgments and the social acceptability of the behaviors they evaluate. In all, the dataset offers a valuable resource for socially grounding language and modeling the intricacies of implicit language understanding from multiple reader perspectives.</abstract>
      <url hash="c8650afa">2024.nlperspectives-1.13</url>
      <bibkey>allein-moens-2024-origamim</bibkey>
    </paper>
    <paper id="14">
      <title>Linguistic Fingerprint in Transformer Models: How Language Variation Influences Parameter Selection in Irony Detection</title>
      <author><first>Michele</first><last>Mastromattei</last></author>
      <author id="fabio-massimo-zanzotto"><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <pages>123–130</pages>
      <abstract>This paper explores the correlation between linguistic diversity, sentiment analysis and transformer model architectures. We aim to investigate how different English variations impact transformer-based models for irony detection. To conduct our study, we used the EPIC corpus to extract five diverse English variation-specific datasets and applied the KEN pruning algorithm on five different architectures. Our results reveal several similarities between optimal subnetworks, which provide insights into the linguistic variations that share strong resemblances and those that exhibit greater dissimilarities. We discovered that optimal subnetworks across models share at least 60% of their parameters, emphasizing the significance of parameter values in capturing and interpreting linguistic variations. This study highlights the inherent structural similarities between models trained on different variants of the same language and also the critical role of parameter values in capturing these nuances.</abstract>
      <url hash="e72bb4ec">2024.nlperspectives-1.14</url>
      <bibkey>mastromattei-zanzotto-2024-linguistic</bibkey>
    </paper>
    <paper id="15">
      <title>Intersectionality in <fixed-case>AI</fixed-case> Safety: Using Multilevel Models to Understand Diverse Perceptions of Safety in Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Christopher</first><last>Homan</last></author>
      <author><first>Gregory</first><last>Serapio-Garcia</last></author>
      <author id="lora-aroyo"><first>Lora</first><last>Aroyo</last></author>
      <author><first>Mark</first><last>Diaz</last></author>
      <author><first>Alicia</first><last>Parrish</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Alex</first><last>Taylor</last></author>
      <author><first>Ding</first><last>Wang</last></author>
      <pages>131–141</pages>
      <abstract>State-of-the-art conversational AI exhibits a level of sophistication that promises to have profound impacts on many aspects of daily life, including how people seek information, create content, and find emotional support. It has also shown a propensity for bias, offensive language, and false information. Consequently, understanding and moderating safety risks posed by interacting with AI chatbots is a critical technical and social challenge. Safety annotation is an intrinsically subjective task, where many factors—often intersecting—determine why people may express different opinions on whether a conversation is safe. We apply Bayesian multilevel models to surface factors that best predict rater behavior to a dataset of 101,286 annotations of conversations between humans and an AI chatbot, stratified by rater gender, age, race/ethnicity, and education level. We show that intersectional effects involving these factors play significant roles in validating safety in conversational AI data. For example, race/ethnicity and gender show strong intersectional effects, particularly among South Asian and East Asian women. We also find that conversational degree of harm impacts raters of all race/ethnicity groups, but that Indigenous and South Asian raters are particularly sensitive. Finally, we discover that the effect of education is uniquely intersectional for Indigenous raters. Our results underscore the utility of multilevel frameworks for uncovering underrepresented social perspectives.</abstract>
      <url hash="04a1b56c">2024.nlperspectives-1.15</url>
      <bibkey>homan-etal-2024-intersectionality</bibkey>
    </paper>
    <paper id="16">
      <title>A Dataset for Multi-Scale Film Rating Inference from Reviews</title>
      <author><first>Frankie</first><last>Robertson</last></author>
      <author><first>Stefano</first><last>Leone</last></author>
      <pages>142–150</pages>
      <abstract>This resource paper introduces a dataset for multi-scale rating inference of film review scores based upon review summaries. The dataset and task are unique in pairing a text regression problem with ratings given on multiple scales, e.g. the A-F letter scale and the 4-point star scale. It retains entity identifiers such as film and reviewer names. The paper describes the construction of the dataset before exploring potential baseline architectures for the task, and evaluating their performance. Baselines based on classifier-per-scale, affine-per-scale, and ordinal regression models are presented and evaluated with the BERT-base backbone. Additional experiments are used to ground a discussion of the different architectures’ merits and drawbacks with regards to explainability and model interpretation.</abstract>
      <url hash="dea51f7a">2024.nlperspectives-1.16</url>
      <bibkey>robertson-leone-2024-dataset</bibkey>
    </paper>
  </volume>
</collection>
