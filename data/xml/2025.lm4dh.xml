<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.lm4dh">
  <volume id="1" ingest-date="2026-01-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First on Natural Language Processing and Language Models for Digital Humanities</booktitle>
      <editor><first>Isuri Nanomi</first><last>Arachchige</last></editor>
      <editor><first>Francesca</first><last>Frontini</last></editor>
      <editor id="ruslan-mitkov"><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Paul</first><last>Rayson</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2025</year>
      <url hash="0774fbcb">2025.lm4dh-1</url>
      <venue>lm4dh</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="b88bcdbc">2025.lm4dh-1.0</url>
      <bibkey>lm4dh-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>H</fixed-case>am<fixed-case>R</fixed-case>az: A Culture-Based <fixed-case>P</fixed-case>ersian Conversation Dataset for Person-Centered Therapy Using <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Mohammad Amin</first><last>Abbasi</last></author>
      <author><first>Farnaz Sadat</first><last>Mirnezami</last></author>
      <author><first>Ali</first><last>Neshati</last></author>
      <author><first>Hassan</first><last>Naderi</last></author>
      <pages>1–24</pages>
      <abstract>We present HamRaz, a culturally adapted Persian-language dataset for AI-assisted mental health support, grounded in Person-Centered Therapy (PCT). To reflect real-world therapeutic challenges, we combine script-based dialogue with adaptive large language models (LLM) role-playing, capturing the ambiguity and emotional nuance of Persian-speaking clients. We introduce HamRazEval, a dual-framework for assessing conversational and therapeutic quality using General Metrics and specialized psychological relationship measures. Human evaluations show HamRaz outperforms existing baselines in empathy, coherence, and real-ism. This resource contributes to the Digital Humanities by bridging language, culture, and mental health in underrepresented communities.</abstract>
      <url hash="c1caa9a5">2025.lm4dh-1.1</url>
      <bibkey>abbasi-etal-2025-hamraz</bibkey>
    </paper>
    <paper id="2">
      <title>Simulating Complex Immediate Textual Variation with Large Language Models</title>
      <author><first>Fernando</first><last>Aguilar-Canto</last></author>
      <author><first>Alberto</first><last>Espinosa-Juarez</last></author>
      <author><first>Hiram</first><last>Calvo</last></author>
      <pages>25–31</pages>
      <abstract>Immediate Textual Variation (ITV) is defined as the process of introducing changes during text transmission from one node to another. One-step variation can be useful for testing specific philological hypotheses. In this paper, we propose using Large Language Models (LLMs) as text-modifying agents. We analyze three scenarios: (1) simple variations (omissions), (2) paraphrasing, and (3) paraphrasing with bias injection (polarity). We generate simulated news items using a predefined scheme. We hypothesize that central tendency measures—such as the mean and median vectors in the feature space of sentence transformers—can effectively approximate the original text representation. Our findings indicate that the median vector is a more accurate estimator of the original vector than most alternatives. However, in cases involving substantial rephrasing, the agent that produces the least semantic drift provides the best estimation, aligning with the principles of Bédierian textual criticism.</abstract>
      <url hash="4bbf6392">2025.lm4dh-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="89ee95e2">2025.lm4dh-1.2.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>aguilar-canto-etal-2025-simulating</bibkey>
    </paper>
    <paper id="3">
      <title>Versus: an automatic text comparison tool for the digital humanities</title>
      <author><first>Motasem</first><last>Alrahabi</last></author>
      <author><first>Tom</first><last>Wainstain</last></author>
      <pages>32–37</pages>
      <abstract>Digital humanities (DH) have been exploring large-scale textual reuse for several decades: quotation, allusion, paraphrase, translation, rephrasing. Automatic comparison, made possible by the increasing digitization of corpora, opens new perspectives in philology and intertextual studies. This article presents a state of the art of existing methods (formal, vector-based, statistical, graph-based) and introduces an open-source tool, Versus, which combines multigranular vector alignment, interactive visualization, and critical traceability. This framework aims to provide a reproducible and accessible solution for DH researchers, with support for text comparison in multiple languages.</abstract>
      <url hash="5181abda">2025.lm4dh-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2882dd36">2025.lm4dh-1.3.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>alrahabi-wainstain-2025-versus</bibkey>
    </paper>
    <paper id="4">
      <title>Like a Human? A Linguistic Analysis of Human-written and Machine-generated Scientific Texts</title>
      <author><first>Sergei</first><last>Bagdasarov</last></author>
      <author><first>Diego</first><last>Alves</last></author>
      <pages>38–47</pages>
      <abstract>The purpose of this study is to analyze lexical and syntactic features in human-written texts and machine-generated texts produced by three state-of-the-art large language models: GPT-4o, Llama 3.1 and Qwen 2.5. We use Kullback-Leibler divergence to quantify the dissimilarity between humans and LLMs as well as to identify relevant features for comparison. We test the predictive power of our features using binary and multi-label random forest classifiers. The classifiers achieve robust performance of above 80% for multi-label classification and above 90% for binary classification. Our results point to substantial differences between human- and machine-generated texts. Human writers show higher variability in the use of syntactic resources, while LLMs score higher in lexical variability.</abstract>
      <url hash="e1f564e6">2025.lm4dh-1.4</url>
      <bibkey>bagdasarov-alves-2025-like</bibkey>
    </paper>
    <paper id="5">
      <title>A State-of-the-Art Morphosyntactic Parser and Lemmatizer for <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek</title>
      <author><first>Giuseppe G. A.</first><last>Celano</last></author>
      <pages>48–65</pages>
      <abstract>This paper presents an experiment comparing six models to identify state-of-the-art models for Ancient Greek: a morphosyntactic parser and a lemmatizer that are capable of annotating in accordance with the Ancient Greek Dependency Treebank annotation scheme. A normalized version of the major collections of annotated texts was used to (i) train the baseline model Dithrax with randomly initialized character embeddings and (ii) fine-tune Trankit and four recent models pretrained on Ancient Greek texts, namely GreBERTa and PhilBERTa for morphosyntactic annotation and GreTA and PhilTa for lemmatization. A Bayesian analysis shows that Dithrax and Trankit are practically equivalent in morphological annotation, while syntax is best annotated by Trankit and lemmata by GreTa. The results of the experiment suggest that token embeddings are not sufficient to achieve high UAS and LAS scores unless they are coupled with a modeling strategy specifically designed to capture syntactic relationships. The dataset and best-performing models are made available online for reuse</abstract>
      <url hash="1c265c26">2025.lm4dh-1.5</url>
      <bibkey>celano-2025-state</bibkey>
    </paper>
    <paper id="6">
      <title>It takes a village to grammaticalize</title>
      <author><first>Joseph E.</first><last>Larson</last></author>
      <author><first>Patricia</first><last>Amaral</last></author>
      <pages>66–74</pages>
      <abstract>This paper investigates the grammaticalization of the noun caleta ‘cove, village’ to an inten- sifier, as part of the system of degree words in Chilean Spanish. We use word embeddings trained on a corpus of tweets to show the on- going syntactic and semantic change of caleta, while also revealing how high degree is ex- pressed in colloquial Chilean Spanish.</abstract>
      <url hash="b113499b">2025.lm4dh-1.6</url>
      <bibkey>larson-amaral-2025-takes</bibkey>
    </paper>
    <paper id="7">
      <title>Evaluating <fixed-case>LLM</fixed-case>s for Historical Document <fixed-case>OCR</fixed-case>: A Methodological Framework for Digital Humanities</title>
      <author><first>Maria A.</first><last>Levchenko</last></author>
      <pages>75–85</pages>
      <abstract>Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting “over-historicization”—inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.</abstract>
      <url hash="d8d1cef3">2025.lm4dh-1.7</url>
      <bibkey>levchenko-2025-evaluating</bibkey>
    </paper>
    <paper id="8">
      <title>Finding the Plea: Evaluating the Ability of <fixed-case>LLM</fixed-case>s to Identify Rhetorical Structure in <fixed-case>S</fixed-case>wedish and <fixed-case>E</fixed-case>nglish Historical Petitions</title>
      <author><first>Ellinor</first><last>Lindqvist</last></author>
      <author><first>Eva</first><last>Pettersson</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>86–101</pages>
      <abstract>Large language models (LLMs) have shown impressive capabilities across many NLP tasks, but their effectiveness on fine-grained content annotation, especially for historical texts, remains underexplored. This study investigates how well GPT-4, Gemini, Mixtral, Mistral, and LLaMA can identify rhetorical sections (Salutatio, Petitio, and Conclusio) in 100 English and 100 Swedish petitions using few-shot prompting with varying levels of detail. Most models perform very well, achieving F1 scores in the high 90s for Salutatio, though Petitio and Conclusio prove more challenging, particularly for smaller models and Swedish data. Cross-lingual prompting yields mixed results, and models generally underestimate document difficulty. These findings demonstrate the strong potential of LLMs for assisting with nuanced historical annotation while highlighting areas for further investigation.</abstract>
      <url hash="999ea9bb">2025.lm4dh-1.8</url>
      <bibkey>lindqvist-etal-2025-finding</bibkey>
    </paper>
    <paper id="9">
      <title>Leveraging <fixed-case>RAG</fixed-case> for a Low-Resource Audio-Aware Diachronic Analysis of Gendered Toy Marketing</title>
      <author><first>Luca</first><last>Marinelli</last></author>
      <author><first>Iacopo</first><last>Ghinassi</last></author>
      <author><first>Charalampos</first><last>Saitis</last></author>
      <pages>102–111</pages>
      <abstract>We performed a diachronic analysis of sound and language in toy commercials, leveraging retrieval-augmented generation (RAG) and open-weight language models in low-resource settings. A pool of 2508 UK toy advertisements spanning 14 years was semi-automatically annotated, integrating thematic coding of transcripts with audio annotation. With our RAG pipeline, we thematically coded and classified commercials by gender-target audience (feminine, masculine, or mixed) achieving substantial inter-coder reliability. In parallel, a music-focused multitask model was applied to annotate affective and mid-level musical perceptual attributes, enabling multimodal discourse analysis. Our findings reveal significant diachronic shifts and enduring patterns. Soundtracks classified as energizing registered an overall increase across distinct themes and audiences, but such increase was steeper for masculine-adjacent commercials. Moreover, themes stereotypically associated with masculinity paired more frequently with louder, distorted, and aggressive music, while stereotypically feminine themes with softer, calmer, and more harmonious soundtracks. Code and data to reproduce the results are available on github.com/marinelliluca/low-resource-RAG.</abstract>
      <url hash="ab57dd90">2025.lm4dh-1.9</url>
      <bibkey>marinelli-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="10">
      <title>Quantifying Societal Stress: Forecasting Historical <fixed-case>L</fixed-case>ondon Mortality using Hardship Sentiment and Crime Data with Natural Language Processing and Time-Series</title>
      <author><first>Sebastian</first><last>Olsen</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <pages>112–119</pages>
      <abstract>We study links between societal stress - quantified from 18th–19th century Old Bailey trial records - and weekly mortality in historical London. Using MacBERTh-based hardship sentiment and time-series analyses (CCF, VAR/IRF, and a Temporal Fusion Transformer, TFT), we find robust lead–lag associations. Hardship sentiment shows its strongest predictive contribution at a 5–6 week lead for mortality in the TFT, while mortality increases precede higher conviction rates in the courts. Results align with Epidemic Psychology and suggest that text-derived stress markers can improve forecasting of public-health relevant mortality fluctuations.</abstract>
      <url hash="4a239549">2025.lm4dh-1.10</url>
      <bibkey>olsen-bloem-2025-quantifying</bibkey>
    </paper>
    <paper id="11">
      <title>Exploring Language in Different Daily Time Segments Through Text Prediction and Language Modeling</title>
      <author><first>Kennedy</first><last>Roland</last></author>
      <author><first>Milton</first><last>King</last></author>
      <pages>120–125</pages>
      <abstract>Temporal-aware language models have proved to be effective over longer time periods as language and its use changes, but little research has looked at how language use can change at different times of the day. We hypothesize that a person’s usage of language varies at different times of day. We explore this concept by evaluating if models for language modeling and next word prediction improve their performance when considering the time of day. Specifically, we explore personalized temporal-aware models for next-word prediction and language modeling and compare them against baseline models, including non-temporal-aware personalized models. Specifically, our proposed model considers which of the 8, 3-hr daily time segments that a text snippet was written during for a given author. We found that our temporal-aware models tend to outperform temporal-agnostic models with respect to accuracy and perplexity.</abstract>
      <url hash="cbf8e886">2025.lm4dh-1.11</url>
      <bibkey>roland-king-2025-exploring</bibkey>
    </paper>
    <paper id="12">
      <title>Identifying Severity of Depression in Forum Posts using Zero-Shot Classifier and <fixed-case>D</fixed-case>istil<fixed-case>BERT</fixed-case> Model</title>
      <author><first>Zafar</first><last>Sarif</last></author>
      <author><first>Sannidhya</first><last>Das</last></author>
      <author><first>Dr. Abhishek</first><last>Das</last></author>
      <author><first>Md Fahin</first><last>Parvej</last></author>
      <author><first>Dipankar</first><last>Das</last></author>
      <pages>126–132</pages>
      <abstract>This paper presents our approach to the RANLP 2025 Shared Task on “Identification of the Severity of Depression in Forum Posts.” The objective of the task is to classify user-generated posts into one of four severity levels of depression: subthreshold, mild, moderate, or severe. A key challenge in the task was the absence of annotated training data. To address this, we employed a two-stage pipeline: first, we used zero-shot classification with facebook/bart-large-mnli to generate pseudo-labels for the unlabeled training set. Next, we fine-tuned a DistilBERT model on the pseudo-labeled data for multi-class classification. Our system achieved an internal accuracy of 0.92 on the pseudo-labeled test set and an accuracy of 0.289 on the official blind evaluation set. These results demonstrate the feasibility of leveraging zero-shot learning and weak supervision for mental health classification tasks, even in the absence of gold-standard annotations.</abstract>
      <url hash="05056765">2025.lm4dh-1.12</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e14703b3">2025.lm4dh-1.12.OptionalSupplementaryMaterial.png</attachment>
      <bibkey>sarif-etal-2025-identifying</bibkey>
    </paper>
    <paper id="13">
      <title>Recall Them All: Long List Generation from Long Novels</title>
      <author><first>Sneha</first><last>Singhania</last></author>
      <author><first>Simon</first><last>Razniewski</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <pages>133–142</pages>
      <abstract>Language models can generate lists of salient literary characters for specific relations but struggle with long, complete lists spanning entire novels. This paper studies the non-standard setting of extracting complete entity lists from full-length books, such as identifying all 50+ friends of Harry Potter across the 7-volume book series. We construct a benchmark dataset with meticulously compiled ground-truth, posing it as a challenge for the research community. We present a first-cut method to tackle this task, based on RAG with LLMs. Our method introduces the novel contribution of harnessing IR-style pseudo-relevance feedback for effective passage retrieval from literary texts. Experimental results show that our approach clearly outperforms both LLM-only and standard RAG baselines, achieving higher recall while maintaining acceptable precision.</abstract>
      <url hash="792a53a6">2025.lm4dh-1.13</url>
      <bibkey>singhania-etal-2025-recall</bibkey>
    </paper>
    <paper id="14">
      <title>Exploring the Limits of Prompting <fixed-case>LLM</fixed-case>s with Speaker-Specific Rhetorical Fingerprints</title>
      <author><first>Wassiliki</first><last>Siskou</last></author>
      <author id="annette-hautli"><first>Annette</first><last>Hautli-Janisz</last></author>
      <pages>143–154</pages>
      <abstract>The capabilities of Large Language Models (LLMs) to mimic written content are being tested on a wide range of tasks and settings, from persuasive essays to programming code. However, the question to what extent they are capable of mimicking human conversational monologue is less well-researched. In this study, we explore the limits of popular LLMs in impersonating content in a high-stakes legal setting, namely for the generation of the decision statement in parole suitability hearings: We distill a linguistically well-motivated rhetorical fingerprint from individual presiding commissioners, based on patterns observed in verbatim transcripts and then enhance the model prompts with those characteristics. When comparing this enhanced prompt with an underspecified prompt we show that LLMs can approximate certain rhetorical features when prompted accordingly, but are not able to fully replicate the linguistic profile of the original speakers as their own fingerprint dominates.</abstract>
      <url hash="e87d452f">2025.lm4dh-1.14</url>
      <bibkey>siskou-hautli-janisz-2025-exploring</bibkey>
    </paper>
    <paper id="15">
      <title>Annotating Personal Information in <fixed-case>S</fixed-case>wedish Texts with <fixed-case>SPARV</fixed-case></title>
      <author><first>Maria Irena</first><last>Szawerna</last></author>
      <author><first>David</first><last>Alfter</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <pages>155–163</pages>
      <abstract>Digital Humanities (DH) research, among many others, relies on data, a subset of which comes in the form of language data that contains personal information (PI). Working with and sharing such data has ethical and legal implications. The process of removing (anonymization) or replacing (pseudonymization) of personal information in texts may be used to address these issues, and often begins with a PI detection and labeling stage. We present a new tool for personal information detection and labeling for Swedish, SBX-PI-DETECTION (henceforth SBX-PI), alongside a visualization interface, (IM)PERSONAL DATA, which allows for the comparison of outputs from different tools. A valuable feature of SBX-PI is that it enables the users to run the annotation locally. It is also integrated into the text annotation pipeline SPARV, allowing for other types of annotation to be performed simultaneously and contributing to the privacy by design requirement set by the GDPR. A novel feature of (IM)PERSONAL DATA is that it allows researchers to assess the extent of detected PI in a text and how much of it will be manipulated once anonymization or pseudonymization are applied. The tools are primarily aimed at researchers within Digital Humanities and Natural Language Processing and are linked to CLARIN’s Virtual Language Observatory.</abstract>
      <url hash="98d903d3">2025.lm4dh-1.15</url>
      <bibkey>szawerna-etal-2025-annotating</bibkey>
    </paper>
    <paper id="16">
      <title>Can <fixed-case>LLM</fixed-case>s Help Sun Wukong in his Journey to the West? A Case Study of Language Models in Video Game Localization</title>
      <author id="xiaojing-zhao"><first>Xiaojing</first><last>Zhao</last></author>
      <author><first>Han</first><last>Xu</last></author>
      <author><first>Huacheng</first><last>Song</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>164–173</pages>
      <abstract>Large language models (LLMs) have demonstrated increasing proficiency in general-purpose translation, yet their effectiveness in creative domains such as game localization remains underexplored. This study focuses on the role of LLMs in game localization from both linguistic quality and sociocultural adequacy through a case study of the video game Black Myth: Wukong. Results indicate that LLMs demonstrate adequate competence in accuracy and fluency, achieving performance comparable to human translators. However, limitations remain in the literal translation of culture-specific terms and offensive language. Human oversight is required to ensure nuanced cultural authenticity and sensitivity. Insights from human evaluations also suggest that current automatic metrics and the Multidimensional Quality Metrics framework may be inadequate for evaluating creative translation. Finally, varying human preferences in localization pose a learning ambiguity for LLMs to perform optimal translation strategies. The findings highlight the potential and shortcomings of LLMs to serve as collaborative tools in game localization workflows. Data are available at https://github.com/zcocozz/wukong-localization.</abstract>
      <url hash="5ef38950">2025.lm4dh-1.16</url>
      <bibkey>zhao-etal-2025-llms-help</bibkey>
    </paper>
  </volume>
</collection>
