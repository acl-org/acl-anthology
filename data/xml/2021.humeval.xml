<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.humeval">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)</booktitle>
      <editor><first>Anya</first><last>Belz</last></editor>
      <editor><first>Shubham</first><last>Agarwal</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Ehud</first><last>Reiter</last></editor>
      <editor><first>Anastasia</first><last>Shimorina</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
      <url hash="49abb657">2021.humeval-1</url>
      <venue>humeval</venue>
    </meta>
    <frontmatter>
      <url hash="2207b5d3">2021.humeval-1.0</url>
      <bibkey>humeval-2021-human</bibkey>
    </frontmatter>
    <paper id="1">
      <title>It’s Commonsense, isn’t it? Demystifying Human Evaluations in Commonsense-Enhanced <fixed-case>NLG</fixed-case> Systems</title>
      <author><first>Miruna-Adriana</first><last>Clinciu</last></author>
      <author><first>Dimitra</first><last>Gkatzia</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <pages>1–12</pages>
      <abstract>Common sense is an integral part of human cognition which allows us to make sound decisions, communicate effectively with others and interpret situations and utterances. Endowing AI systems with commonsense knowledge capabilities will help us get closer to creating systems that exhibit human intelligence. Recent efforts in Natural Language Generation (NLG) have focused on incorporating commonsense knowledge through large-scale pre-trained language models or by incorporating external knowledge bases. Such systems exhibit reasoning capabilities without common sense being explicitly encoded in the training set. These systems require careful evaluation, as they incorporate additional resources during training which adds additional sources of errors. Additionally, human evaluation of such systems can have significant variation, making it impossible to compare different systems and define baselines. This paper aims to demystify human evaluations of commonsense-enhanced NLG systems by proposing the Commonsense Evaluation Card (CEC), a set of recommendations for evaluation reporting of commonsense-enhanced NLG systems, underpinned by an extensive analysis of human evaluations reported in the recent literature.</abstract>
      <url hash="9d3a126b">2021.humeval-1.1</url>
      <video href="https://www.youtube.com/watch?v=LlrsKZOKIoo"/>
      <bibkey>clinciu-etal-2021-commonsense</bibkey>
      <video href="2021.humeval-1.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="2">
      <title>Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation</title>
      <author><first>Jakob</first><last>Nyberg</last></author>
      <author><first>Maike</first><last>Paetzel</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <pages>13–24</pages>
      <abstract>Human ratings are one of the most prevalent methods to evaluate the performance of NLP (natural language processing) algorithms. Similarly, it is common to measure the quality of sentences generated by a natural language generation model using human raters. In this paper we argue for exploring the use of subjective evaluations within the process of training language generation models in a multi-task learning setting. As a case study, we use a crowd-authored dialogue corpus to fine-tune six different language generation models. Two of these models incorporate multi-task learning and use subjective ratings of lines as part of an explicit learning goal. A human evaluation of the generated dialogue lines reveals that utterances generated by the multi-tasking models were subjectively rated as the most typical, most moving the conversation forward, and least offensive. Based on these promising first results, we discuss future research directions for incorporating subjective human evaluations into language model training and to hence keep the human user in the loop during the development process.</abstract>
      <url hash="3ae444bc">2021.humeval-1.2</url>
      <video href="https://www.youtube.com/watch?v=SE-y2PLX2wE"/>
      <bibkey>nyberg-etal-2021-estimating</bibkey>
      <video href="2021.humeval-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Trading Off Diversity and Quality in Natural Language Generation</title>
      <author><first>Hugh</first><last>Zhang</last></author>
      <author><first>Daniel</first><last>Duckworth</last></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Arvind</first><last>Neelakantan</last></author>
      <pages>25–33</pages>
      <abstract>For open-ended language generation tasks such as storytelling or dialogue, choosing the right decoding algorithm is vital for controlling the tradeoff between generation <i>quality</i> and <i>diversity</i>. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. In this paper, we cast decoding as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap: the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms.</abstract>
      <url hash="318e99f6">2021.humeval-1.3</url>
      <video href="https://www.youtube.com/watch?v=P0SWVm30MFM"/>
      <bibkey>zhang-etal-2021-trading</bibkey>
      <video href="2021.humeval-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Towards Document-Level Human <fixed-case>MT</fixed-case> Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation</title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <pages>34–45</pages>
      <abstract>Document-level human evaluation of machine translation (MT) has been raising interest in the community. However, little is known about the issues of using document-level methodologies to assess MT quality. In this article, we compare the inter-annotator agreement (IAA) scores, the effort to assess the quality in different document-level methodologies, and the issue of misevaluation when sentences are evaluated out of context.</abstract>
      <url hash="bd13eb58">2021.humeval-1.4</url>
      <video href="https://www.youtube.com/watch?v=djkFwF2RJ74"/>
      <bibkey>castilho-2021-towards</bibkey>
      <video href="2021.humeval-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors</title>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Kosuke</first><last>Takahashi</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>46–55</pages>
      <abstract>This paper discusses a classification-based approach to machine translation evaluation, as opposed to a common regression-based approach in the WMT Metrics task. Recent machine translation usually works well but sometimes makes critical errors due to just a few wrong word choices. Our classification-based approach focuses on such errors using several error type labels, for practical machine translation evaluation in an age of neural machine translation. We made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available upon publication.</abstract>
      <url hash="6d5b87b3">2021.humeval-1.5</url>
      <attachment type="Dataset" hash="3b09bc8d">2021.humeval-1.5.Dataset.zip</attachment>
      <video href="https://www.youtube.com/watch?v=myG72lA2hpo"/>
      <bibkey>sudoh-etal-2021-translation</bibkey>
      <video href="2021.humeval-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Towards Objectively Evaluating the Quality of Generated Medical Summaries</title>
      <author><first>Francesco</first><last>Moramarco</last></author>
      <author><first>Damir</first><last>Juric</last></author>
      <author><first>Aleksandar</first><last>Savkov</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>56–61</pages>
      <abstract>We propose a method for evaluating the quality of generated text by asking evaluators to count facts, and computing precision, recall, f-score, and accuracy from the raw counts. We believe this approach leads to a more objective and easier to reproduce evaluation. We apply this to the task of medical report summarisation, where measuring objective quality and accuracy is of paramount importance.</abstract>
      <url hash="2fa2ed24">2021.humeval-1.6</url>
      <bibkey>moramarco-etal-2021-towards</bibkey>
    </paper>
    <paper id="7">
      <title>A Preliminary Study on Evaluating Consultation Notes With Post-Editing</title>
      <author><first>Francesco</first><last>Moramarco</last></author>
      <author><first>Alex</first><last>Papadopoulos Korfiatis</last></author>
      <author><first>Aleksandar</first><last>Savkov</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>62–68</pages>
      <abstract>Automatic summarisation has the potential to aid physicians in streamlining clerical tasks such as note taking. But it is notoriously difficult to evaluate these systems and demonstrate that they are safe to be used in a clinical setting. To circumvent this issue, we propose a semi-automatic approach whereby physicians post-edit generated notes before submitting them. We conduct a preliminary study on the time saving of automatically generated consultation notes with post-editing. Our evaluators are asked to listen to mock consultations and to post-edit three generated notes. We time this and find that it is faster than writing the note from scratch. We present insights and lessons learnt from this experiment.</abstract>
      <url hash="e328a95b">2021.humeval-1.7</url>
      <bibkey>moramarco-etal-2021-preliminary</bibkey>
    </paper>
    <paper id="8">
      <title>The Great Misalignment Problem in Human Evaluation of <fixed-case>NLP</fixed-case> Methods</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <pages>69–74</pages>
      <abstract>We outline the Great Misalignment Problem in natural language processing research, this means simply that the problem definition is not in line with the method proposed and the human evaluation is not in line with the definition nor the method. We study this misalignment problem by surveying 10 randomly sampled papers published in ACL 2020 that report results with human evaluation. Our results show that only one paper was fully in line in terms of problem definition, method and evaluation. Only two papers presented a human evaluation that was in line with what was modeled in the method. These results highlight that the Great Misalignment Problem is a major one and it affects the validity and reproducibility of results obtained by a human evaluation.</abstract>
      <url hash="450cc3d6">2021.humeval-1.8</url>
      <bibkey>hamalainen-alnajjar-2021-great</bibkey>
    </paper>
    <paper id="9">
      <title>A View From the Crowd: Evaluation Challenges for Time-Offset Interaction Applications</title>
      <author><first>Alberto</first><last>Chierici</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>75–85</pages>
      <abstract>Dialogue systems like chatbots, and tasks like question-answering (QA) have gained traction in recent years; yet evaluating such systems remains difficult. Reasons include the great variety in contexts and use cases for these systems as well as the high cost of human evaluation. In this paper, we focus on a specific type of dialogue systems: Time-Offset Interaction Applications (TOIAs) are intelligent, conversational software that simulates face-to-face conversations between humans and pre-recorded human avatars. Under the constraint that a TOIA is a single output system interacting with users with different expectations, we identify two challenges: first, how do we define a ‘good’ answer? and second, what’s an appropriate metric to use? We explore both challenges through the creation of a novel dataset that identifies multiple good answers to specific TOIA questions through the help of Amazon Mechanical Turk workers. This ‘view from the crowd’ allows us to study the variations of how TOIA interrogators perceive its answers. Our contributions include the annotated dataset that we make publicly available and the proposal of Success Rate @k as an evaluation metric that is more appropriate than the traditional QA’s and information retrieval’s metrics.</abstract>
      <url hash="3ba1b463">2021.humeval-1.9</url>
      <bibkey>chierici-habash-2021-view</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/douban">Douban</pwcdataset>
    </paper>
    <paper id="10">
      <title>Reliability of Human Evaluation for Text Summarization: Lessons Learned and Challenges Ahead</title>
      <author><first>Neslihan</first><last>Iskender</last></author>
      <author><first>Tim</first><last>Polzehl</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>86–96</pages>
      <abstract>Only a small portion of research papers with human evaluation for text summarization provide information about the participant demographics, task design, and experiment protocol. Additionally, many researchers use human evaluation as gold standard without questioning the reliability or investigating the factors that might affect the reliability of the human evaluation. As a result, there is a lack of best practices for reliable human summarization evaluation grounded by empirical evidence. To investigate human evaluation reliability, we conduct a series of human evaluation experiments, provide an overview of participant demographics, task design, experimental set-up and compare the results from different experiments. Based on our empirical analysis, we provide guidelines to ensure the reliability of expert and non-expert evaluations, and we determine the factors that might affect the reliability of the human evaluation.</abstract>
      <url hash="b9f00b08">2021.humeval-1.10</url>
      <bibkey>iskender-etal-2021-reliability</bibkey>
      <pwccode url="https://github.com/nesliskender/reliability_humeval_summarization" additional="false">nesliskender/reliability_humeval_summarization</pwccode>
    </paper>
    <paper id="11">
      <title>On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs</title>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <pages>97–106</pages>
      <abstract>Recent studies emphasize the need of document context in human evaluation of machine translations, but little research has been done on the impact of user interfaces on annotator productivity and the reliability of assessments. In this work, we compare human assessment data from the last two WMT evaluation campaigns collected via two different methods for document-level evaluation. Our analysis shows that a document-centric approach to evaluation where the annotator is presented with the entire document context on a screen leads to higher quality segment and document level assessments. It improves the correlation between segment and document scores and increases inter-annotator agreement for document scores but is considerably more time consuming for annotators.</abstract>
      <url hash="c245e82e">2021.humeval-1.11</url>
      <bibkey>grundkiewicz-etal-2021-user</bibkey>
    </paper>
    <paper id="12">
      <title>Eliciting Explicit Knowledge From Domain Experts in Direct Intrinsic Evaluation of Word Embeddings for Specialized Domains</title>
      <author><first>Goya</first><last>van Boven</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <pages>107–113</pages>
      <abstract>We evaluate the use of direct intrinsic word embedding evaluation tasks for specialized language. Our case study is philosophical text: human expert judgements on the relatedness of philosophical terms are elicited using a synonym detection task and a coherence task. Uniquely for our task, experts must rely on explicit knowledge and cannot use their linguistic intuition, which may differ from that of the philosopher. We find that inter-rater agreement rates are similar to those of more conventional semantic annotation tasks, suggesting that these tasks can be used to evaluate word embeddings of text types for which implicit knowledge may not suffice.</abstract>
      <url hash="ef439fcf">2021.humeval-1.12</url>
      <attachment type="Dataset" hash="1e9db1c7">2021.humeval-1.12.Dataset.zip</attachment>
      <bibkey>van-boven-bloem-2021-eliciting</bibkey>
      <pwccode url="https://github.com/gvanboven/direct-intrinsic-evaluation" additional="false">gvanboven/direct-intrinsic-evaluation</pwccode>
    </paper>
    <paper id="13">
      <title>Detecting Post-Edited References and Their Effect on Human Evaluation</title>
      <author><first>Věra</first><last>Kloudová</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>114–119</pages>
      <abstract>This paper provides a quick overview of possible methods how to detect that reference translations were actually created by post-editing an MT system. Two methods based on automatic metrics are presented: BLEU difference between the suspected MT and some other good MT and BLEU difference using additional references. These two methods revealed a suspicion that the WMT 2020 Czech reference is based on MT. The suspicion was confirmed in a manual analysis by finding concrete proofs of the post-editing procedure in particular sentences. Finally, a typology of post-editing changes is presented where typical errors or changes made by the post-editor or errors adopted from the MT are classified.</abstract>
      <url hash="275e28f4">2021.humeval-1.13</url>
      <bibkey>kloudova-etal-2021-detecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="14">
      <title>A Case Study of Efficacy and Challenges in Practical Human-in-Loop Evaluation of <fixed-case>NLP</fixed-case> Systems Using Checklist</title>
      <author><first>Shaily</first><last>Bhatt</last></author>
      <author><first>Rahul</first><last>Jain</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <pages>120–130</pages>
      <abstract>Despite state-of-the-art performance, NLP systems can be fragile in real-world situations. This is often due to insufficient understanding of the capabilities and limitations of models and the heavy reliance on standard evaluation benchmarks. Research into non-standard evaluation to mitigate this brittleness is gaining increasing attention. Notably, the behavioral testing principle ‘Checklist’, which decouples testing from implementation revealed significant failures in state-of-the-art models for multiple tasks. In this paper, we present a case study of using Checklist in a practical scenario. We conduct experiments for evaluating an offensive content detection system and use a data augmentation technique for improving the model using insights from Checklist. We lay out the challenges and open questions based on our observations of using Checklist for human-in-loop evaluation and improvement of NLP systems. Disclaimer: The paper contains examples of content with offensive language. The examples do not represent the views of the authors or their employers towards any person(s), group(s), practice(s), or entity/entities.</abstract>
      <url hash="e16ad0fe">2021.humeval-1.14</url>
      <video href="https://www.youtube.com/watch?v=fjkKVUZHJRQ"/>
      <bibkey>bhatt-etal-2021-case</bibkey>
      <video href="2021.humeval-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Interrater Disagreement Resolution: A Systematic Procedure to Reach Consensus in Annotation Tasks</title>
      <author><first>Yvette</first><last>Oortwijn</last></author>
      <author><first>Thijs</first><last>Ossenkoppele</last></author>
      <author><first>Arianna</first><last>Betti</last></author>
      <pages>131–141</pages>
      <abstract>We present a systematic procedure for interrater disagreement resolution. The procedure is general, but of particular use in multiple-annotator tasks geared towards ground truth construction. We motivate our proposal by arguing that, barring cases in which the researchers’ goal is to elicit different viewpoints, interrater disagreement is a sign of poor quality in the design or the description of a task. Consensus among annotators, we maintain, should be striven for, through a systematic procedure for disagreement resolution such as the one we describe.</abstract>
      <url hash="858f30b5">2021.humeval-1.15</url>
      <video href="https://www.youtube.com/watch?v=z-O6zZJDxOY"/>
      <bibkey>oortwijn-etal-2021-interrater</bibkey>
      <video href="2021.humeval-1.15.mp4"/>
      <pwccode url="https://github.com/yoortwijn/humevaldisres" additional="false">yoortwijn/humevaldisres</pwccode>
    </paper>
  </volume>
</collection>
