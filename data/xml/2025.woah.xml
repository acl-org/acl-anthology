<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.woah">
  <volume id="1" ingest-date="2025-07-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the The 9th Workshop on Online Abuse and Harms (WOAH)</booktitle>
      <editor><first>Agostina</first><last>Calabrese</last></editor>
      <editor><first>Christine</first><last>de Kock</last></editor>
      <editor><first>Debora</first><last>Nozza</last></editor>
      <editor><first>Flor Miriam</first><last>Plaza-del-Arco</last></editor>
      <editor><first>Zeerak</first><last>Talat</last></editor>
      <editor><first>Francielle</first><last>Vargas</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="a0f0cc69">2025.woah-1</url>
      <venue>woah</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-105-6</isbn>
    </meta>
    <frontmatter>
      <url hash="34c6bf19">2025.woah-1.0</url>
      <bibkey>woah-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Comprehensive Taxonomy of Bias Mitigation Methods for Hate Speech Detection</title>
      <author><first>Jan</first><last>Fillies</last><affiliation>Freie Universität Berlin</affiliation></author>
      <author><first>Marius</first><last>Wawerek</last><affiliation>Freie Universität Berlin</affiliation></author>
      <author><first>Adrian</first><last>Paschke</last><affiliation>Freie Universität Berlin</affiliation></author>
      <pages>1-16</pages>
      <abstract>Algorithmic hate speech detection is widely used today. However, biases within these systems can lead to discrimination. This research presents an overview of bias mitigation strategies in the field of hate speech detection. The identified principles are grouped into four categories, based on their operation principles. A novel taxonomy of bias mitigation methods is proposed. The mitigation strategies are characterized based on their key concepts and analyzed in terms of their application stage and their need for knowledge of protected attributes. Additionally, the paper discusses potential combinations of these strategies. This research shifts the focus from identifying present biases to examining the similarities and differences between mitigation strategies, thereby facilitating the exchange, stacking, and ensembling of these strategies in future research.</abstract>
      <url hash="3cedb1cc">2025.woah-1.1</url>
      <attachment type="SupplementaryMaterial" hash="a4038ab2">2025.woah-1.1.SupplementaryMaterial.zip</attachment>
      <bibkey>fillies-etal-2025-comprehensive</bibkey>
    </paper>
    <paper id="2">
      <title>Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation</title>
      <author><first>Dimosthenis</first><last>Antypas</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Indira</first><last>Sen</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Carla</first><last>Perez Almendros</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Francesco</first><last>Barbieri</last><affiliation>Meta</affiliation></author>
      <pages>17-31</pages>
      <abstract>The detection of sensitive content in large datasets is crucial for ensuring that shared and analysed data is free from harmful material. However, current moderation tools, such as external APIs, suffer from limitations in customisation, accuracy across diverse sensitive categories, and privacy concerns. Additionally, existing datasets and open-source models focus predominantly on toxic language, leaving gaps in detecting other sensitive categories such as substance abuse or self-harm. In this paper, we put forward a unified dataset tailored for social media content moderation across six sensitive categories: conflictual language, profanity,sexually explicit material, drug-related content, self-harm, and spam. By collecting and annotating data with consistent retrieval strategies and guidelines, we address the shortcomings of previous focalised research. Our analysis demonstrates that fine-tuning large language models (LLMs) on this novel dataset yields significant improvements in detection performance compared to open off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which underperform by 10-15% overall. This limitation is even more pronounced on popular moderation APIs, which cannot be easily tailored to specific sensitive content categories, among others.</abstract>
      <url hash="64a75cee">2025.woah-1.2</url>
      <bibkey>antypas-etal-2025-sensitive</bibkey>
    </paper>
    <paper id="3">
      <title>From civility to parity: Marxist-feminist ethics for context-aware algorithmic content moderation</title>
      <author><first>Dayei</first><last>Oh</last><affiliation>University of Helsinki</affiliation></author>
      <pages>32-40</pages>
      <abstract>Algorithmic content moderation governs online speech on large-scale commercial platforms, often under the guise of neutrality. Yet, it routinely reproduces white, middle-class norms of civility and penalizes marginalized voices for unruly and resistant speech. This paper critiques the prevailing ‘pathological’ approach to moderation that prioritizes sanitization over justice. Drawing on Marxist-feminist ethics, this paper advances three theses for the future of context-aware algorithmic moderation: (1) prioritizing participatory parity over civility, (2) incorporating identity- and context-aware analysis of speech; and (3) replacing purely numerical evaluations with justice-oriented, community-sensitive metrics. While acknowledging the structural limitations posed by platform capitalism, this paper positions the proposed framework as both critique and provocation, guiding regulatory reform, civil advocacy, and visions for mission-driven online content moderation serving digital commons.</abstract>
      <url hash="3710c96d">2025.woah-1.3</url>
      <bibkey>oh-2025-civility</bibkey>
    </paper>
    <paper id="4">
      <title>A Novel Dataset for Classifying <fixed-case>G</fixed-case>erman Hate Speech Comments with Criminal Relevance</title>
      <author><first>Vincent</first><last>Kums</last><affiliation>Hochschule Mittweida</affiliation></author>
      <author><first>Florian</first><last>Meyer</last><affiliation>Hochschule Mittweida</affiliation></author>
      <author><first>Luisa</first><last>Pivit</last><affiliation>Hochschule Darmstadt</affiliation></author>
      <author><first>Uliana</first><last>Vedenina</last><affiliation>Hochschule Darmstadt</affiliation></author>
      <author><first>Jonas</first><last>Wortmann</last><affiliation>Hochschule Darmstadt</affiliation></author>
      <author><first>Melanie</first><last>Siegel</last><affiliation>Hochschule Darmstadt</affiliation></author>
      <author><first>Dirk</first><last>Labudde</last><affiliation>Hochschule Mittweida</affiliation></author>
      <pages>41-52</pages>
      <abstract>The consistently high prevalence of hate speech on the Internet continues to pose significant social and individual challenges. Given the centrality of social networks in public discourse, automating the identification of criminally relevant content is a pressing challenge. This study addresses the challenge of developing an automated system that is capable of classifying online comments in a criminal justice context and categorising them into relevant sections of the criminal code. Not only technical, but also ethical and legal requirements must be considered. To this end, 351 comments were annotated by public prosecutors from the Central Office for Combating Internet and Computer Crime (ZIT) according to previously formed paragraph classes. These groupings consist of several German criminal law statutes that most hate comments violate. In the subsequent phase of the research, a further 839 records were assigned to the classes by student annotators who had been trained previously.</abstract>
      <url hash="a9e8231f">2025.woah-1.4</url>
      <attachment type="SupplementaryMaterial" hash="e10e450b">2025.woah-1.4.SupplementaryMaterial.zip</attachment>
      <bibkey>kums-etal-2025-novel</bibkey>
    </paper>
    <paper id="5">
      <title>Learning from Disagreement: Entropy-Guided Few-Shot Selection for Toxic Language Detection</title>
      <author><first>Tommaso</first><last>Caselli</last><affiliation>Rijksuniversiteit Groningen</affiliation></author>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last><affiliation>Leiden University</affiliation></author>
      <pages>53-66</pages>
      <abstract>In-context learning (ICL) has shown significant benefits, particularly in scenarios where large amounts of labeled data are unavailable. However, its effectiveness for highly subjective tasks, such as toxic language detection, remains an open question. A key challenge in this setting is to select shots to maximize performance. Although previous work has focused on enhancing variety and representativeness, the role of annotator disagreement in shot selection has received less attention. In this paper, we conduct an in-depth analysis of ICL using two families of open-source LLMs (Llama-3* and Qwen2.5) of varying sizes, evaluating their performance in five prominent English datasets covering multiple toxic language phenomena. We use disaggregated annotations and categorize different types of training examples to assess their impact on model predictions. We specifically investigate whether selecting shots based on annotators’ entropy – focusing on ambiguous or difficult examples – can improve generalization in LLMs. Additionally, we examine the extent to which the order of examples in prompts influences model behavior.Our results show that selecting shots based on entropy from annotator disagreement can enhance ICL performance. Specifically, ambiguous shots with a median entropy value generally lead to the best results for our selected LLMs in the few-shot setting. However, ICL often underperforms when compared to fine-tuned encoders.</abstract>
      <url hash="b69f91a4">2025.woah-1.5</url>
      <bibkey>caselli-plaza-del-arco-2025-learning</bibkey>
    </paper>
    <paper id="8">
      <title>Debiasing Static Embeddings for Hate Speech Detection</title>
      <author><first>Ling</first><last>Sun</last><affiliation>Indiana University</affiliation></author>
      <author><first>Soyoung</first><last>Kim</last><affiliation>Indiana University</affiliation></author>
      <author><first>Xiao</first><last>Dong</last><affiliation>Indiana University</affiliation></author>
      <author><first>Sandra</first><last>Kübler</last><affiliation>Indiana University</affiliation></author>
      <pages>67-76</pages>
      <abstract>We examine how embedding bias affects hate speech detection by evaluating two debiasing methods—hard-debiasing and soft-debiasing. We analyze stereotype and sentiment associations within the embedding space and assess whether debiased models reduce censorship of marginalized authors while improving detection of hate speech targeting these groups. Our findings highlight how embedding bias propagates into downstream tasks and demonstrates how well different embedding bias metrics can predict bias in hate speech detection.</abstract>
      <url hash="0ed60ba9">2025.woah-1.8</url>
      <bibkey>sun-etal-2025-debiasing</bibkey>
    </paper>
    <paper id="9">
      <title>Web(er) of Hate: A Survey on How Hate Speech Is Typed</title>
      <author><first>Luna</first><last>Wang</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andrew</first><last>Caines</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Alice</first><last>Hutchings</last><affiliation>University of Cambridge</affiliation></author>
      <pages>77-103</pages>
      <abstract>The curation of hate speech datasets involves complex design decisions that balance competing priorities. This paper critically examines these methodological choices in a diverse range of datasets, highlighting common themes and practices, and their implications for dataset reliability. Drawing on Max Weber’s notion of ideal types, we argue for a reflexive approach in dataset creation, urging researchers to acknowledge their own value judgments during dataset construction, fostering transparency and methodological rigour.</abstract>
      <url hash="d356d4f2">2025.woah-1.9</url>
      <attachment type="SupplementaryMaterial" hash="1b76f586">2025.woah-1.9.SupplementaryMaterial.zip</attachment>
      <bibkey>wang-etal-2025-web</bibkey>
    </paper>
    <paper id="10">
      <title>Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided <fixed-case>LLM</fixed-case>s for Countering Hate Speech.</title>
      <author><first>Mikel</first><last>Ngueajio</last><affiliation>Howard University</affiliation></author>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last><affiliation>Leiden University</affiliation></author>
      <author><first>Yi-Ling</first><last>Chung</last><affiliation>Genaios</affiliation></author>
      <author><first>Danda</first><last>Rawat</last><affiliation>Howard University</affiliation></author>
      <author><first>Amanda</first><last>Cercas Curry</last><affiliation>Bocconi University</affiliation></author>
      <pages>104-123</pages>
      <abstract>Automated counter-narratives (CN) offer a promising strategy for mitigating online hate speech, yet concerns about their affective tone, accessibility, and ethical risks remain. We propose a framework for evaluating Large Language Model (LLM)-generated CNs across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Using GPT-4o-Mini, Cohere’s CommandR-7B, and Meta’s LLaMA 3.1-70B, we assess three prompting strategies on the MT-Conan and HatEval datasets.Our findings reveal that LLM-generated CNs are often verbose and adapted for people with college-level literacy, limiting their accessibility. While emotionally guided prompts yield more empathetic and readable responses, there remain concerns surrounding safety and effectiveness.</abstract>
      <url hash="d0a6b261">2025.woah-1.10</url>
      <attachment type="SupplementaryMaterial" hash="21a46320">2025.woah-1.10.SupplementaryMaterial.zip</attachment>
      <bibkey>ngueajio-etal-2025-think</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>HODIAT</fixed-case>: A Dataset for Detecting Homotransphobic Hate Speech in <fixed-case>I</fixed-case>talian with Aggressiveness and Target Annotation</title>
      <author><first>Greta</first><last>Damo</last><affiliation>UniversitÃ© CÃ ́te d’Azur</affiliation></author>
      <author><first>Alessandra Teresa</first><last>Cignarella</last><affiliation>LT3, Ghent University</affiliation></author>
      <author><first>Tommaso</first><last>Caselli</last><affiliation>Rijksuniversiteit Groningen</affiliation></author>
      <author><first>Viviana</first><last>Patti</last><affiliation>University of Turin, Dipartimento di Informatica</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>124-135</pages>
      <abstract>The escalating spread of homophobic and transphobic rhetoric in both online and offline spaces has become a growing global concern, with Italy standing out as one of the countries where acts of violence against LGBTQIA+ individuals persist and increase year after year. This short paper study analyzes hateful language against LGBTQIA+ individuals in Italian using novel annotation labels for aggressiveness and target. We assess a range of multilingual and Italian language models on this newannotation layers across zero-shot, few-shot, and fine-tuning settings. The results reveal significant performance gaps across models and settings, highlighting the limitations of zero- and few-shot approaches and the importance of fine-tuning on labelled data, when available, to achieve high prediction performance.</abstract>
      <url hash="3dbcf97e">2025.woah-1.11</url>
      <bibkey>damo-etal-2025-hodiat</bibkey>
    </paper>
    <paper id="12">
      <title>Beyond the Binary: Analysing Transphobic Hate and Harassment Online</title>
      <author><first>Anna</first><last>Talas</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Alice</first><last>Hutchings</last><affiliation>University of Cambridge</affiliation></author>
      <pages>136-152</pages>
      <abstract>Online communities provide support and help to individuals transitioning gender. However, this point of transition also increases vulnerability, coupled with increased exposure to online harms. In this research, we analyse a popular hate and harassment site known for targeting minority groups, including transgender people. We analyse 17 million posts dating back to 2012 to gain insights into the types of information collected about targets. We find users commonly link to social media sites such as Twitter/X and meticulously archive links related to their targets. We scrape over 150,000 relevant links posted to Twitter/X and their archived versions and analyse the profiles and posts. We find targets often tweet about harassment, popculture, and queer and gender-related discussions. We develop and evaluate classifiers to detect calls for harassment, doxxing, mention of transgender individuals, and toxic/abusive speech within the forum posts. The results of our classifiers show that forum posts about transgender individuals are significantly more likely to contain other harmful content.</abstract>
      <url hash="7b8257ac">2025.woah-1.12</url>
      <attachment type="SupplementaryMaterial" hash="53fa5c7d">2025.woah-1.12.SupplementaryMaterial.zip</attachment>
      <bibkey>talas-hutchings-2025-beyond</bibkey>
    </paper>
    <paper id="13">
      <title>Evading Toxicity Detection with <fixed-case>ASCII</fixed-case>-art: A Benchmark of Spatial Attacks on Moderation Systems</title>
      <author><first>Sergey</first><last>Berezin</last><affiliation>Telecom SudParis</affiliation></author>
      <author><first>Reza</first><last>Farahbakhsh</last><affiliation>Adjunct Associate Professor</affiliation></author>
      <author><first>Noel</first><last>Crespi</last><affiliation>Telecom SudParis</affiliation></author>
      <pages>153-162</pages>
      <abstract>We introduce a novel class of adversarial attacks on toxicity detection models that exploit language models’ failure to interpret spatially structured text in the form of ASCII art. To evaluate the effectiveness of these attacks, we propose ToxASCII, a benchmark designed to assess the robustness of toxicity detection systems against visually obfuscated inputs. Our attacks achieve a perfect Attack Success Rate (ASR) across a diverse set of state-of-the-art large language models and dedicated moderation tools, revealing a significant vulnerability in current text-only moderation systems.</abstract>
      <url hash="bd8ccdd4">2025.woah-1.13</url>
      <bibkey>berezin-etal-2025-evading</bibkey>
    </paper>
    <paper id="15">
      <title>Debunking with Dialogue? Exploring <fixed-case>AI</fixed-case>-Generated Counterspeech to Challenge Conspiracy Theories</title>
      <author><first>Mareike</first><last>Lisker</last><affiliation>Hochschule für Technik und Wirtschaft (HTW) Berlin</affiliation></author>
      <author><first>Christina</first><last>Gottschalk</last><affiliation>Hochschule für Technik und Wirtschaft (HTW) Berlin</affiliation></author>
      <author><first>Helena</first><last>Mihaljević</last><affiliation>Hochschule für Technik und Wirtschaft (HTW) Berlin</affiliation></author>
      <pages>163-178</pages>
      <abstract>Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.</abstract>
      <url hash="0d6a17e7">2025.woah-1.15</url>
      <attachment type="SupplementaryMaterial" hash="9eb68f2e">2025.woah-1.15.SupplementaryMaterial.zip</attachment>
      <bibkey>lisker-etal-2025-debunking</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>M</fixed-case>isinfo<fixed-case>T</fixed-case>ele<fixed-case>G</fixed-case>raph: Network-driven Misinformation Detection for <fixed-case>G</fixed-case>erman Telegram Messages</title>
      <author><first>Lu</first><last>Kalkbrenner</last><affiliation>CeMAS›</affiliation></author>
      <author><first>Veronika</first><last>Solopova</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Steffen</first><last>Zeiler</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Robert</first><last>Nickel</last><affiliation>Bucknell University</affiliation></author>
      <author><first>Dorothea</first><last>Kolossa</last><affiliation>Technische Universität Berlin</affiliation></author>
      <pages>179-191</pages>
      <abstract>Connectivity and message propagation are central, yet often underutilised, sources of information in misinformation detection—especially on poorly moderated platforms such as Telegram, which has become a critical channel for misinformation dissemination, namely in the German electoral context. In this paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based graph dataset for misinformation detection. It includes over 5 million messages from public channels, enriched with metadata, channel relationships, and both weak and strong labels. These labels are derived via semantic similarity to fact-checks and news articles using M3-embeddings, as well as manual annotation. To establish reproducible baselines, we evaluate both text-only models and graph neural networks (GNNs) that incorporate message forwarding as a network structure. Our results show that GraphSAGE with LSTM aggregation significantly outperforms text-only baselines in terms of Matthews Correlation Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers, view counts, and automatically versus human-created labels on performance, and highlight both the potential and challenges of weak supervision in this domain. This work provides a reproducible benchmark and open dataset for future research on misinformation detection in German-language Telegram networks and other low-moderation social platforms.</abstract>
      <url hash="65867f1b">2025.woah-1.16</url>
      <attachment type="SupplementaryMaterial" hash="ddbbffaf">2025.woah-1.16.SupplementaryMaterial.zip</attachment>
      <bibkey>kalkbrenner-etal-2025-misinfotelegraph</bibkey>
    </paper>
    <paper id="17">
      <title>Catching Stray Balls: Football, fandom, and the impact on digital discourse</title>
      <author><first>Mark</first><last>Hill</last><affiliation>King’s College London</affiliation></author>
      <pages>192-205</pages>
      <abstract>This paper examines how emotional responses to football matches influence online discourse across digital spaces on Reddit. By analysing millions of posts from dozens of subreddits, it demonstrates that real-world events trigger sentiment shifts that move across communities. It shows that negative sentiment correlates with problematic language; match outcomes directly influence sentiment and posting habits; sentiment can transfer to unrelated communities; and offers insights into the content of this shifting discourse. These findings reveal how digital spaces function not as isolated environments, but as interconnected emotional ecosystems vulnerable to cross-domain contagion triggered by real-world events, contributing to our understanding of the propagation of online toxicity. While football is used as a case-study to computationally measure affective causes and movements, these patterns have implications for understanding online communities broadly.</abstract>
      <url hash="6e54df6e">2025.woah-1.17</url>
      <attachment type="SupplementaryMaterial" hash="8ea68bc4">2025.woah-1.17.SupplementaryMaterial.zip</attachment>
      <bibkey>hill-2025-catching</bibkey>
    </paper>
    <paper id="18">
      <title>Exploring Hate Speech Detection Models for <fixed-case>L</fixed-case>ithuanian Language</title>
      <author><first>Justina</first><last>Mandravickaitė</last><affiliation>Vytautas Magnus University</affiliation></author>
      <author><first>Eglė</first><last>Rimkienė</last><affiliation>Vytautas Magnus University</affiliation></author>
      <author><first>Mindaugas</first><last>Petkevičius</last><affiliation>Vytautas Magnus University</affiliation></author>
      <author><first>Milita</first><last>Songailaitė</last><affiliation>Vytautas Magnus University</affiliation></author>
      <author><first>Eimantas</first><last>Zaranka</last><affiliation>Vytautas Magnus University</affiliation></author>
      <author><first>Tomas</first><last>Krilavičius</last><affiliation>Vytautas Magnus University</affiliation></author>
      <pages>206-218</pages>
      <abstract>Online hate speech poses a significant challenge, as it can incite violence and contribute to social polarization. This study evaluates traditional machine learning, deep learning and large language models (LLMs) for Lithuanian hate speech detection, addressing class imbalance issue via data augmentation and resampling techniques. Our dataset included 27,358 user-generated comments, annotated into Neutral language (56%), Offensive language (29%) and Hate speech (15%). We trained BiLSTM, LSTM, CNN, SVM, and Random Forest models and fine-tuned Multilingual BERT, LitLat BERT, Electra, RWKV, ChatGPT, LT-Llama-2, and Gemma-2 models. Additionally, we pre-trained Electra for Lithuanian. Models were evaluated using accuracy and weighted F1-score. On the imbalanced dataset, LitLat BERT (0.76 weighted F1-score) and Multilingual BERT (0.73 weighted F1-score) performed best. Over-sampling further boosted weighted F1-scores, with Multilingual BERT (0.85) and LitLat BERT (0.84) outperforming other models. Over-sampling combined with augmentation provided the best overall results. Under-sampling led to performance declines and was less effective. Finally, fine-tuning LLMs improved their accuracy which highlighted the importance of fine-tuning for more specialized NLP tasks.</abstract>
      <url hash="987dd3e8">2025.woah-1.18</url>
      <attachment type="SupplementaryMaterial" hash="2fc78ad0">2025.woah-1.18.SupplementaryMaterial.zip</attachment>
      <bibkey>mandravickaite-etal-2025-exploring</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>RAG</fixed-case> and Recall: Multilingual Hate Speech Detection with Semantic Memory</title>
      <author><first>Khouloud</first><last>Mnassri</last><affiliation>Samovar, Telecom SudParis, Institut Polytechnique de Paris</affiliation></author>
      <author><first>Reza</first><last>Farahbakhsh</last><affiliation>Samovar, Telecom SudParis, Institut Polytechnique de Paris</affiliation></author>
      <author><first>Noel</first><last>Crespi</last><affiliation>Samovar, Telecom SudParis, Institut Polytechnique de Paris</affiliation></author>
      <pages>219-227</pages>
      <abstract>Multilingual hate speech detection presents a challenging task, particularly in limited-resource contexts when performance is affected by cultural nuances and data scarcity. Fine-tuned models are often unable to generalize beyond their training, which limits their efficiency, especially for low-resource languages. In this paper, we introduce HS-RAG, a retrieval-augmented generation (RAG) system that directly leverages knowledge, in English, French, and Arabic, from Hate Speech Superset (publicly available dataset) and Wikipedia to Large Language Models (LLMs). To further enhance robustness, we introduce HS-MemRAG, a memory-augmented extension that integrates a semantic cache. This model reduces redundant retrieval while improving contextual relevance and hate speech detection among the three languages.</abstract>
      <url hash="69d9014b">2025.woah-1.20</url>
      <attachment type="SupplementaryMaterial" hash="2e2ad39f">2025.woah-1.20.SupplementaryMaterial.zip</attachment>
      <bibkey>mnassri-etal-2025-rag</bibkey>
    </paper>
    <paper id="21">
      <title>Implicit Hate Target Span Detection in Zero- and Few-Shot Settings with Selective Sub-Billion Parameter Models</title>
      <author><first>Hossam</first><last>Boudraa</last><affiliation>Aix-Marseille University</affiliation></author>
      <author><first>Benoit</first><last>Favre</last><affiliation>LIS, AMU Marseille, France</affiliation></author>
      <author><first>Raquel</first><last>Urena</last><affiliation>SESSTIM, AMU Marseille, France</affiliation></author>
      <pages>228-240</pages>
      <abstract>This work investigates the effectiveness of masked language models (MLMs) and autoregressive language models (LLMs) with fewer than one billion parameters in the detection of implicit hate speech through fine-grained span identification. The evaluation spans zero-shot, few-shot, and full supervision settings across two core benchmarks—SBIC and IHC—and an auxiliary testbed, OffensiveLang.RoBERTa-Large-355M emerges as the strongest zero-shot model, achieving the highest F1 scores of 75.8 (SBIC) and 72.5 (IHC), outperforming larger models like LLaMA 3.2-1B. ModernBERT-125M closely matches this performance with scores of 75.1 and 72.2, demonstrating the advantage of architectural efficiency. Among instruction-tuned models, SmolLM2-135M Instruct and LLaMA 3.2 1B Instruct consistently outperform their non-instructed counterparts, with up to +2.3 F1 gain on SBIC and +1.7 on IHC. Interestingly, the larger SmolLM2-360M Instruct does not outperform the 135M variant, highlighting that model scale does not always correlate with performance in implicit hate detection tasks.Few-shot fine-tuning with SmolLM2-135M Instruct achieves F1 scores of 68.2 (SBIC) and 64.0 (IHC), trailing full-data fine-tuning by only 1.6 and 2.0 points, respectively, with accuracy drops under 0.5 points. This illustrates the promise of compact, instruction-aligned models in data-scarce settings, particularly when optimized with Low-Rank Adaptation (LoRA).Topic-guided error analysis using Latent Dirichlet Allocation (LDA) reveals recurring model failures in ideologically charged or euphemistic discourse. Misclassifications often involve neutral references to identity, politics, or advocacy language, underscoring current limitations in discourse-level inference and sociopragmatic understanding.</abstract>
      <url hash="e5ca169a">2025.woah-1.21</url>
      <bibkey>boudraa-etal-2025-implicit</bibkey>
    </paper>
    <paper id="22">
      <title>Hate Speech in Times of Crises: a Cross-Disciplinary Analysis of Online Xenophobia in <fixed-case>G</fixed-case>reece</title>
      <author><first>Maria</first><last>Pontiki</last><affiliation>Institute for Language and Speech Processing (ILSP), Athena R.C., Greece</affiliation></author>
      <author><first>Vasiliki</first><last>Georgiadou</last><affiliation>Panteion University of Social and Political Sciences</affiliation></author>
      <author><first>Lamprini</first><last>Rori</last><affiliation>National and Kapodistrian University of Athens</affiliation></author>
      <author><first>Maria</first><last>Gavriilidou</last><affiliation>ILSP / Athena RC</affiliation></author>
      <pages>241-253</pages>
      <abstract>Bridging NLP with political science, this paper examines both the potential and the limitations of a computational hate speech detection method in addressing real-world questions. Using Greece as a case study, we analyze over 4 million tweets from 2015 to 2022—a period marked by economic, refugee, foreign policy, and pandemic crises. The analysis of false positives highlights the challenges of accurately detecting different types of verbal attacks across various targets and timeframes. In addition, the analysis of true positives reveals distinct linguistic patterns that reinforce populist narratives, polarization and hostility. By situating these findings within their socio-political context, we provide insights into how hate speech manifests online in response to real-world crises.</abstract>
      <url hash="b9a03145">2025.woah-1.22</url>
      <bibkey>pontiki-etal-2025-hate</bibkey>
    </paper>
    <paper id="23">
      <title>Hostility Detection in <fixed-case>UK</fixed-case> Politics: A Dataset on Online Abuse Targeting <fixed-case>MP</fixed-case>s</title>
      <author><first>Mugdha</first><last>Pandya</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Mali</first><last>Jin</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Kalina</first><last>Bontcheva</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Diana</first><last>Maynard</last><affiliation>University of Sheffield</affiliation></author>
      <pages>254-266</pages>
      <abstract>Social media platforms, particularly X, enable direct interaction between politicians and constituents but also expose politicians to hostile responses targetting both their governmental role and personal identity. This online hostility can undermine public trust and potentially incite offline violence. While general hostility detection models exist, they lack the specificity needed for political contexts and country-specific issues. We address this gap by creating a dataset of 3,320 English tweets directed at UK Members of Parliament (MPs) over two years, annotated for hostility and targeted identity characteristics (race, gender, religion). Through linguistic and topical analyses, we examine the unique features of UK political discourse and evaluate pre-trained language models and large language models on binary hostility detection and multi-class targeted identity type classification tasks. Our work provides essential data and insights for studying politics-related hostility in the UK.</abstract>
      <url hash="e42ff8a8">2025.woah-1.23</url>
      <attachment type="SupplementaryMaterial" hash="ca4391bb">2025.woah-1.23.SupplementaryMaterial.zip</attachment>
      <bibkey>pandya-etal-2025-hostility</bibkey>
    </paper>
    <paper id="24">
      <title>Detoxify-<fixed-case>IT</fixed-case>: An <fixed-case>I</fixed-case>talian Parallel Dataset for Text Detoxification</title>
      <author><first>Viola</first><last>De Ruvo</last><affiliation>Bocconi</affiliation></author>
      <author><first>Arianna</first><last>Muti</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Daryna</first><last>Dementieva</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>267-275</pages>
      <abstract>Toxic language online poses growing challenges for content moderation. Detoxification, which rewrites toxic content into neutral form, offers a promising alternative but remains underexplored beyond English. We present Detoxify-IT, the first Italian dataset for this task, featuring toxic comments and their human-written neutral rewrites. Our experiments show that even limited fine-tuning on Italian data leads to notable improvements in content preservation and fluency compared to both multilingual models and LLMs used in zero-shot settings, underlining the need for language-specific resources. This work enables detoxification research in Italian and supports broader efforts toward safer, more inclusive online communication.</abstract>
      <url hash="d88e9609">2025.woah-1.24</url>
      <attachment type="SupplementaryMaterial" hash="26b98a3d">2025.woah-1.24.SupplementaryMaterial.zip</attachment>
      <bibkey>de-ruvo-etal-2025-detoxify</bibkey>
    </paper>
    <paper id="25">
      <title>Pathways to Radicalisation: On Research for Online Radicalisation in Natural Language Processing and Machine Learning</title>
      <author><first>Zeerak</first><last>Talat</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Michael Sejr</first><last>Schlichtkrull</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Pranava</first><last>Madhyastha</last><affiliation>City, University of London</affiliation></author>
      <author><first>Christine</first><last>De Kock</last><affiliation>University of Melbourne</affiliation></author>
      <pages>276-283</pages>
      <abstract>Online communities play an integral part in communication for communication across the globe. Online communities that are known for extremist content. As a field of surveillance technologies, NLP and other ML fields hold particular promise for monitoring extremist communities that may turn violent.Such communities make use of a wide variety of modalities of communication, including textual posts on specialised fora, memes, videos, and podcasts. Furthermore, such communities undergo rapid linguistic evolution, thus presenting a challenge to machine learning technologies that quickly diverge from the data that are used. In this position, we argue that radicalisation is a nascent area for which machine learning is particularly apt. However, in addressing radicalisation research it is important that avoids falling into the temptation of focusing on prediction. We argue that such communities present a particular avenue for addressing key concerns with machine learning technologies: (1) temporal misalignment of models and (2) aligning and linking content across modalities.</abstract>
      <url hash="4991750d">2025.woah-1.25</url>
      <bibkey>talat-etal-2025-pathways</bibkey>
    </paper>
    <paper id="26">
      <title>Social Hatred: Efficient Multimodal Detection of Hatemongers</title>
      <author><first>Tom</first><last>Marzea</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Abraham</first><last>Israeli</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Oren</first><last>Tsur</last><affiliation>Ben Gurion University</affiliation></author>
      <pages>284-298</pages>
      <abstract>Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon.While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network.Evaluating our method on three unique datasets X (Twitter), Gab, and Parler we show that processing a user’s texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. We offer comprehensive set of results obtained in different experimental settings as well as qualitative analysis of illustrative cases.Our method can be used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as to inform intervention measures. Moreover, we demonstrate that our multimodal approach performs well across very different content platforms and over large datasets and networks.</abstract>
      <url hash="d3af5e48">2025.woah-1.26</url>
      <bibkey>marzea-etal-2025-social</bibkey>
    </paper>
    <paper id="27">
      <title>Blue-haired, misandriche, rabiata: Tracing the Connotation of ‘Feminist(s)’ Across Time, Languages and Domains</title>
      <author><first>Arianna</first><last>Muti</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Sara</first><last>Gemelli</last><affiliation>University of Pavia, University of Bergamo</affiliation></author>
      <author><first>Emanuele</first><last>Moscato</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Emilie</first><last>Francis</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Amanda</first><last>Cercas Curry</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last><affiliation>Leiden University</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>299-311</pages>
      <abstract>Understanding how words shift in meaning is crucial for analyzing societal attitudes.In this study, we investigate the contextual variations of the terms feminist, feminists along three axes: time, language, and domain.To this aim, we collect and release FEMME, a dataset comprising the occurrences of such terms from 2014 to 2023 in English, Italian and Swedish in Twitter, Reddit and Incel domains.Our methodology leverages frame analysis, as well as fine-tuning and LLMs. We find that the connotation of the plural form feminists is consistently more negative than feminist, indicating more hostility towards feminists as a collective, which often triggers greater societal pushback, reflecting broader patterns of group-based hostility and stigma. Across languages, we observe similar stereotypes towards feminists that often include body shaming, as well as accusations of hypocrisy and irrational behavior. In terms of time, we identify events that trigger a peak in terms of negative or positive connotation.As expected, the Incel spheres show predominantly negative connotations, while the general domains show mixed connotations.</abstract>
      <url hash="0cfcf34c">2025.woah-1.27</url>
      <attachment type="SupplementaryMaterial" hash="9af8ee80">2025.woah-1.27.SupplementaryMaterial.zip</attachment>
      <bibkey>muti-etal-2025-blue</bibkey>
    </paper>
    <paper id="28">
      <title>Towards Fairness Assessment of <fixed-case>D</fixed-case>utch Hate Speech Detection</title>
      <author><first>Julie</first><last>Bauer</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Rishabh</first><last>Kaushal</last><affiliation>Indira Gandhi Delhi Technical University for Women and Maastricht University</affiliation></author>
      <author><first>Thales</first><last>Bertaglia</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Adriana</first><last>Iamnitchi</last><affiliation>Maastricht University</affiliation></author>
      <pages>312-324</pages>
      <abstract>Numerous studies have proposed computational methods to detect hate speech online, yet most focus on the English language and emphasize model development. In this study, we evaluate the counterfactual fairness of hate speech detection models in the Dutch language, specifically examining the performance and fairness of transformer-based models.We make the following key contributions. First, we curate a list of Dutch Social Group Terms that reflect social context. Second, we generate counterfactual data for Dutch hate speech using LLMs and established strategies like Manual Group Substitution (MGS) and Sentence Log-Likelihood (SLL). Through qualitative evaluation, we highlight the challenges of generating realistic counterfactuals, particularly with Dutch grammar and contextual coherence. Third, we fine-tune baseline transformer-based models with counterfactual data and evaluate their performance in detecting hate speech. Fourth, we assess the fairness of these models using Counterfactual Token Fairness (CTF) and group fairness metrics, including equality of odds and demographic parity. Our analysis shows that models perform better in terms of hate speech detection, average counterfactual fairness and group fairness. This work addresses a significant gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides practical insights and recommendations for improving both model performance and fairness.</abstract>
      <url hash="aff0cba4">2025.woah-1.28</url>
      <attachment type="SupplementaryMaterial" hash="b9a3df54">2025.woah-1.28.SupplementaryMaterial.zip</attachment>
      <bibkey>bauer-etal-2025-towards</bibkey>
    </paper>
    <paper id="29">
      <title>Between Hetero-Fatalism and Dark Femininity: Discussions of Relationships, Sex, and Men in the Femosphere</title>
      <author><first>Emilie</first><last>Francis</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>325-341</pages>
      <abstract>The ‘femosphere’ is a term coined to describe a group of online ideological spaces for women characterised by toxicity, reactionary feminism, and hetero-pessimism. It is often portrayed as a mirror of a similar group of communities for men, called the ‘manosphere’. Although there have been several studies investigating the ideologies and language of the manosphere, the femosphere has been largely overlooked - especially in NLP. This paper presents a study of two communities in the femosphere: Female Dating Strategy and Femcels. It presents an exploration of the language of these communities on topics related to relationships, sex, and men from the perspective of hetero-pessimism using topic modelling and semantic analysis. It reveals dissatisfaction with heterosexual courtship and frustration with the patriarchal society through which members attempt to navigate.</abstract>
      <url hash="4349cf86">2025.woah-1.29</url>
      <attachment type="SupplementaryMaterial" hash="ed73d866">2025.woah-1.29.SupplementaryMaterial.tex</attachment>
      <bibkey>francis-2025-hetero</bibkey>
    </paper>
    <paper id="30">
      <title>Can <fixed-case>LLM</fixed-case>s Rank the Harmfulness of Smaller <fixed-case>LLM</fixed-case>s? We are Not There Yet</title>
      <author><first>Berk</first><last>Atil</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Vipul</first><last>Gupta</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Sarkar Snigdha Sarathi</first><last>Das</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Rebecca</first><last>Passonneau</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>342-354</pages>
      <abstract>Large language models (LLMs) have become ubiquitous, thus it is important to understand their risks and limitations, such as their propensity to generate harmful output. This includes smaller LLMs, which are important for settings with constrained compute resources, such as edge devices. Detection of LLM harm typically requires human annotation, which is expensive to collect. This work studies two questions: How do smaller LLMs rank regarding generation of harmful content? How well can larger LLMs annotate harmfulness? We prompt three small LLMs to elicit harmful content of various types, such as discriminatory language, offensive content, privacy invasion, or negative influence, and collect human rankings of their outputs. Then, we compare harm annotation from three state-of-the-art large LLMs with each other and with humans. We find that the smaller models differ with respect to harmfulness. We also find that large LLMs show low to moderate agreement with humans.</abstract>
      <url hash="7469732c">2025.woah-1.30</url>
      <bibkey>atil-etal-2025-llms</bibkey>
    </paper>
    <paper id="31">
      <title>Are You Trying to Convince Me or Are You Trying to Deceive Me? Using Argumentation Types to Identify Deceptive News</title>
      <author><first>Ricardo</first><last>Muñoz Sánchez</last><affiliation>Språkbanken Text, Göteborgs Universitet</affiliation></author>
      <author><first>Emilie</first><last>Francis</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Anna</first><last>Lindahl</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>355-372</pages>
      <abstract>The way we relay factual information and the way we present deceptive information as truth differs from the perspective of argumentation. In this paper, we explore whether these differences can be exploited to detect deceptive political news in English. We do this by training a model to detect different kinds of argumentation in online news text. We use sentence embeddings extracted from an argumentation type classification model as features for a deceptive news classifier. This deceptive news classification model leverages the sequence of argumentation types within an article to determine whether it is credible or deceptive. Our approach outperforms other state-of-the-art models while having lower variance. Finally, we use the output of our argumentation model to analyze the differences between credible and deceptive news based on the distribution of argumentation types across the articles. Results of this analysis indicate that credible political news presents statements supported by a variety of argumentation types, while deceptive news relies on anecdotes and testimonial.</abstract>
      <url hash="7843d4d9">2025.woah-1.31</url>
      <attachment type="SupplementaryMaterial" hash="49e0bfc8">2025.woah-1.31.SupplementaryMaterial.zip</attachment>
      <bibkey>munoz-sanchez-etal-2025-trying</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>QG</fixed-case>uard:Question-based Zero-shot Guard for Multi-modal <fixed-case>LLM</fixed-case> Safety</title>
      <author><first>Taegyeong</first><last>Lee</last><affiliation>MODULABS</affiliation></author>
      <author><first>Jeonghwa</first><last>Yoo</last><affiliation>MODULABS</affiliation></author>
      <author><first>Hyoungseo</first><last>Cho</last><affiliation>MODULABS</affiliation></author>
      <author><first>Soo Yong</first><last>Kim</last><affiliation>AI Matics</affiliation></author>
      <author><first>Yunho</first><last>Maeng</last><affiliation>Ewha Womans University</affiliation></author>
      <pages>373-382</pages>
      <abstract>The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.</abstract>
      <url hash="bb49f0d8">2025.woah-1.33</url>
      <attachment type="SupplementaryMaterial" hash="7f52fb7b">2025.woah-1.33.SupplementaryMaterial.zip</attachment>
      <bibkey>lee-etal-2025-qguard</bibkey>
    </paper>
    <paper id="34">
      <title>Who leads? Who follows? Temporal dynamics of political dogwhistles in <fixed-case>S</fixed-case>wedish online communities</title>
      <author><first>Max</first><last>Boholm</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Gregor</first><last>Rettenegger</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Ellen</first><last>Breitholtz</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Robin</first><last>Cooper</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Elina</first><last>Lindgren</last><affiliation>Karlstad University</affiliation></author>
      <author><first>Björn</first><last>Rönnerstrand</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Asad</first><last>Sayeed</last><affiliation>University of Gothenburg</affiliation></author>
      <pages>383-395</pages>
      <abstract>A dogwhistle is a communicative act intended to broadcast a message only understood by a select in-group while going unnoticed by others (out-group). We illustrate that political dogwhistle behavior in a more radical community precedes the occurrence of the dogwhistles in a less radical community, but the reverse does not hold. We study two Swedish online communities – Flashback and Familjeliv – which both contain discussions of life and society, with the former having a stronger anti-immigrant subtext. Expressions associated with dogwhistles are substantially more frequent in Flashback than in Familjeliv. We analyze the time series of changes in intensity of three dogwhistle expressions (DWEs), i.e., the strength of association of a DWE and its in-group meaning modeled by Swedish Sentence-BERT, and model the dynamic temporal relationship of intensity in the two communities for the three DWEs using Vector Autoregression (VAR). We show that changes in intensity in Familjeliv are explained by the changes of intensity observed at previous lags in Flashback but not the other way around. This suggests a direction of travel for dogwhistles associated with radical ideologies to less radical contexts.</abstract>
      <url hash="ef9ba3bc">2025.woah-1.34</url>
      <attachment type="SupplementaryMaterial" hash="f4d5e1ca">2025.woah-1.34.SupplementaryMaterial.zip</attachment>
      <bibkey>boholm-etal-2025-leads</bibkey>
    </paper>
    <paper id="36">
      <title>Detecting Child Objectification on Social Media: Challenges in Language Modeling</title>
      <author><first>Miriam</first><last>Schirmer</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Angelina</first><last>Voggenreiter</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Juergen</first><last>Pfeffer</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Agnes</first><last>Horvat</last><affiliation>Northwestern University</affiliation></author>
      <pages>396-412</pages>
      <abstract>Online objectification of children can harm their self-image and influence how others perceive them. Objectifying comments may start with a focus on appearance but also include language that treats children as passive, decorative, or lacking agency. On TikTok, algorithm-driven visibility amplifies this focus on looks. Drawing on objectification theory, we introduce a Child Objectification Language Typology to automatically classify objectifying comments. Our dataset consists of 562,508 comments from 9,090 videos across 482 TikTok accounts. We compare language models of different complexity, including an n-gram-based model, RoBERTa, GPT-4, LlaMA, and Mistral. On our training dataset of 6,000 manually labeled comments, we found that RoBERTa performed best overall in detecting appearance- and objectification-related language. 10.35% of comments contained appearance-related language, while 2.90% included objectifying language. Videos with school-aged girls received more appearance-related comments compared to boys in that age group, while videos with toddlers show a slight increase in objectification-related comments compared to other age groups. Neither gender alone nor engagement metrics showed significant effects.The findings raise concerns about children’s digital exposure, emphasizing the need for stricter policies to protect minors.</abstract>
      <url hash="e7200f9b">2025.woah-1.36</url>
      <attachment type="SupplementaryMaterial" hash="b95ddbd1">2025.woah-1.36.SupplementaryMaterial.zip</attachment>
      <bibkey>schirmer-etal-2025-detecting</bibkey>
    </paper>
    <paper id="39">
      <title>Can Prompting <fixed-case>LLM</fixed-case>s Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study</title>
      <author><first>Faeze</first><last>Ghorbanpour</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Daryna</first><last>Dementieva</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Alexandar</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>413-425</pages>
      <abstract>Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.</abstract>
      <url hash="035434e8">2025.woah-1.39</url>
      <attachment type="SupplementaryMaterial" hash="3db61eeb">2025.woah-1.39.SupplementaryMaterial.zip</attachment>
      <bibkey>ghorbanpour-etal-2025-prompting</bibkey>
    </paper>
    <paper id="41">
      <title>Multilingual Analysis of Narrative Properties in Conspiracist vs Mainstream Telegram Channels</title>
      <author><first>Katarina</first><last>Laken</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Matteo</first><last>Melis</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Sara</first><last>Tonelli</last><affiliation>FBK</affiliation></author>
      <author><first>Marcos</first><last>Garcia</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <pages>426-457</pages>
      <abstract>Conspiracist narratives posit an omnipotent, evil group causing harm throughout domains. However, modern-day online conspiracism is often more erratic, consisting of loosely connected posts displaying a general anti-establishment attitude pervaded by negative emotions. We gather a dataset of 300 conspiracist and mainstream, Telegram channels in Italian and English and use the automatic extraction of entities and emotion detection to compare structural characteristics of both types of channels. We create a co-occurrence network of entities to analyze how the different types of channels introduce and use them across posts and topics. We find that conspiracist channels are characterized by anger. Moreover, co-occurrence networks of entities appearing in conspiracist channels are more dense. We theorize that this reflects a narrative structure where all actants are pushed into a single domain. Conspiracist channels disproportionately associate the most central group of entities with anger and fear. We do not find evidence that entities in conspiracist narratives occur across more topics. This could indicate an erratic type of online conspiracism where everything can be connected to everything and that is characterized by a high number of entities and high levels of anger.</abstract>
      <url hash="631bae93">2025.woah-1.41</url>
      <attachment type="SupplementaryMaterial" hash="01da7e56">2025.woah-1.41.SupplementaryMaterial.zip</attachment>
      <bibkey>laken-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="42">
      <title>Hate Explained: Evaluating <fixed-case>NER</fixed-case>-Enriched Text in Human and Machine Moderation of Hate Speech</title>
      <author><first>Andres</first><last>Carvallo</last><affiliation>CENIA</affiliation></author>
      <author><first>Marcelo</first><last>Mendoza</last><affiliation>Pontificia Universidad Católica de Chile</affiliation></author>
      <author><first>Miguel</first><last>Fernandez</last><affiliation>Pontificia Universidad Católica de Chile</affiliation></author>
      <author><first>Maximiliano</first><last>Ojeda</last><affiliation>Pontificia Universidad Catolica de Chile</affiliation></author>
      <author><first>Lilly</first><last>Guevara</last><affiliation>Universidad Tecnica Federico Santa Maria</affiliation></author>
      <author><first>Diego</first><last>Varela</last><affiliation>Universidad Tecnica Federico Santa Maria</affiliation></author>
      <author><first>Martin</first><last>Borquez</last><affiliation>Pontificia Universidad Católica de Chile</affiliation></author>
      <author><first>Nicolas</first><last>Buzeta</last><affiliation>Pontificia Universidad Católica de Chile</affiliation></author>
      <author><first>Felipe</first><last>Ayala</last><affiliation>Universidad Tecnica Federico Santa Maria</affiliation></author>
      <pages>458-467</pages>
      <abstract>Hate speech detection is vital for creating safe online environments, as harmful content can drive social polarization. This study explores the impact of enriching text with intent and group tags on machine performance and human moderation workflows. For machine performance, we enriched text with intent and group tags to train hate speech classifiers. Intent tags were the most effective, achieving state-of-the-art F1-score improvements on the IHC, SBIC, and DH datasets, respectively. Cross-dataset evaluations further demonstrated the superior generalization of intent-tagged models compared to other pre-trained approaches. Then, through a user study (N=100), we evaluated seven moderation settings, including intent tags, group tags, model probabilities, and randomized counterparts. Intent annotations significantly improved the accuracy of the moderators, allowing them to outperform machine classifiers by 12.9%. Moderators also rated intent tags as the most useful explanation tool, with a 41% increase in perceived helpfulness over the control group. Our findings demonstrate that intent-based annotations enhance both machine classification performance and human moderation workflows.</abstract>
      <url hash="64b5e6a9">2025.woah-1.42</url>
      <bibkey>carvallo-etal-2025-hate</bibkey>
    </paper>
    <paper id="43">
      <title>Personas with Attitudes: Controlling <fixed-case>LLM</fixed-case>s for Diverse Data Annotation</title>
      <author><first>Leon</first><last>Fröhling</last><affiliation/></author>
      <author><first>Gianluca</first><last>Demartini</last><affiliation/></author>
      <author><first>Dennis</first><last>Assenmacher</last><affiliation/></author>
      <pages>468-481</pages>
      <abstract>We present a novel approach for enhancing diversity and control in data annotation tasks by personalizing large language models (LLMs). We investigate the impact of injecting diverse persona descriptions into LLM prompts across two studies, exploring whether personas increase annotation diversity and whether the impacts of individual personas on the resulting annotations are consistent and controllable. Our results indicate that persona-prompted LLMs generate more diverse annotations than LLMs prompted without personas, and that the effects of personas on LLM annotations align with subjective differences in human annotations. These effects are both controllable and repeatable, making our approach a valuable tool for enhancing data annotation in subjective NLP tasks such as toxicity detection.</abstract>
      <url hash="33bd52c8">2025.woah-1.43</url>
      <bibkey>frohling-etal-2025-personas</bibkey>
    </paper>
    <paper id="44">
      <title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt. Generation for Enhanced <fixed-case>LLM</fixed-case> Content Moderation</title>
      <author><first>Daniel</first><last>Schwarz</last><affiliation/></author>
      <author><first>Dmitriy</first><last>Bespalov</last><affiliation/></author>
      <author><first>Zhe</first><last>Wang</last><affiliation/></author>
      <author><first>Ninad</first><last>Kulkarni</last><affiliation/></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation/></author>
      <pages>482-489</pages>
      <abstract>As large language models (LLMs) become increasingly prevalent, ensuring their robustness against adversarial misuse is crucial. This paper introduces the GAP (Graph of Attacks with Pruning) framework, an advanced approach for generating stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP addresses limitations in existing tree-based methods by implementing an interconnected graph structure that enables knowledge sharing across attack paths. Our experimental evaluation demonstrates GAP’s superiority over existing techniques, achieving a 20.8% increase in attack success rates while reducing query costs by 62.7%. GAP consistently outperforms state-of-the-art methods across various open and closed LLMs, with attack success rates of 96%. Additionally, we present specialized variants like GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks. GAP-generated prompts prove highly effective in improving content moderation systems, increasing true positive detection rates by 108.5% and accuracy by 183.6% when used for fine-tuning.</abstract>
      <url hash="5be80a12">2025.woah-1.44</url>
      <bibkey>schwarz-etal-2025-graph</bibkey>
    </paper>
    <paper id="45">
      <title>A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot <fixed-case>LLM</fixed-case> Classification Performance</title>
      <author><first>Matteo</first><last>Melis</last><affiliation/></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation/></author>
      <author><first>Dennis</first><last>Assenmacher</last><affiliation/></author>
      <pages>490-521</pages>
      <abstract>Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 conceptual elements—building blocks that capture different aspects of hate speech definitions, such as references to the target of hate. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.</abstract>
      <url hash="a17879a7">2025.woah-1.45</url>
      <bibkey>melis-etal-2025-modular</bibkey>
    </paper>
    <paper id="46">
      <title>Red-Teaming for Uncovering Societal Bias in Large Language Models</title>
      <author><first>Chu Fei</first><last>Luo</last><affiliation/></author>
      <author><first>Ahmad</first><last>Ghawanmeh</last><affiliation/></author>
      <author><first>Kashyap</first><last>Coimbatore Murali</last><affiliation/></author>
      <author><first>Bhimshetty Bharat</first><last>Kumar</last><affiliation/></author>
      <author><first>Murli</first><last>Jadhav</last><affiliation/></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation/></author>
      <author><first>Faiza</first><last>Khan Khattak</last><affiliation/></author>
      <pages>522-537</pages>
      <abstract>Ensuring the safe deployment of AI systems is critical in industry settings where biased outputs can lead to significant operational, reputational, and regulatory risks. Thorough evaluation before deployment is essential to prevent these hazards. Red-teaming addresses this need by employing adversarial attacks to develop guardrails that detect and reject biased or harmful queries, enabling models to be retrained or steered away from harmful outputs. However, red-teaming techniques are often limited, and malicious actors may discover new vulnerabilities that bypass safety fine-tuning, underscoring the need for ongoing research and innovative approaches. Notably, most red-teaming efforts focus on harmful or unethical instructions rather than addressing social bias, leaving this critical area under-explored despite its significant real-world impact, especially in customer-facing AI systems. We propose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and BiasKG, to evaluate how standard safety measures for harmful content mitigate bias. For BiasKG, we refactor natural language stereotypes into a knowledge graph. and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models. We find our method increases bias in all models, even those trained with safety guardrails. Our work emphasizes uncovering societal bias in LLMs through rigorous evaluation, addressing adversarial challenges to ensure AI safety in high-stakes industry deployments.</abstract>
      <url hash="f63fdb17">2025.woah-1.46</url>
      <bibkey>luo-etal-2025-red</bibkey>
    </paper>
    <paper id="47">
      <title>Using <fixed-case>LLM</fixed-case>s and Preference Optimization for Agreement-Aware <fixed-case>H</fixed-case>ate<fixed-case>W</fixed-case>i<fixed-case>C</fixed-case> Classification</title>
      <author><first>Sebastian</first><last>Loftus</last><affiliation/></author>
      <author><first>Adrian</first><last>Mülthaler</last><affiliation/></author>
      <author><first>Sanne</first><last>Hoeken</last><affiliation/></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation/></author>
      <author><first>Ozge</first><last>Alacam</last><affiliation/></author>
      <pages>538-547</pages>
      <abstract>Annotator disagreement poses a significant challenge in subjective tasks like hate speech detection. In this paper, we introduce a novel variant of the HateWiC task that explicitly models annotator agreement by estimating the proportion of annotators who classify the meaning of a term as hateful. To tackle this challenge, we explore the use of Llama 3 models fine-tuned through Direct Preference Optimization (DPO). Our experiments show that while LLMs perform well for majority-based hate classification, they struggle with the more complex agreement-aware task. DPO fine-tuning offers improvements, particularly when applied to instruction-tuned models. Yet, our results emphasize the need for improved modeling of subjectivity in hate classification and this study can serve as foundation for future advancements.</abstract>
      <url hash="1e90e95b">2025.woah-1.47</url>
      <bibkey>loftus-etal-2025-using</bibkey>
    </paper>
  </volume>
</collection>
