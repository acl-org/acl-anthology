<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.sumeval">
  <volume id="1" ingest-date="2022-11-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</booktitle>
      <editor><first>Kabir</first><last>Ahuja</last></editor>
      <editor><first>Antonios</first><last>Anastasopoulos</last></editor>
      <editor><first>Barun</first><last>Patra</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Monojit</first><last>Choudhury</last></editor>
      <editor><first>Sandipan</first><last>Dandapat</last></editor>
      <editor><first>Sunayana</first><last>Sitaram</last></editor>
      <editor><first>Vishrav</first><last>Chaudhary</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2022</year>
      <url hash="83a5b945">2022.sumeval-1</url>
      <venue>sumeval</venue>
    </meta>
    <frontmatter>
      <url hash="f1d2663a">2022.sumeval-1.0</url>
      <bibkey>sumeval-2022-scaling</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The <fixed-case>SUME</fixed-case>val 2022 Shared Task on Performance Prediction of Multilingual Pre-trained Language Models</title>
      <author><first>Kabir</first><last>Ahuja</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <pages>1–7</pages>
      <url hash="99aea200">2022.sumeval-1.1</url>
      <bibkey>ahuja-etal-2022-sumeval</bibkey>
    </paper>
    <paper id="2">
      <title>To Train or Not to Train: Predicting the Performance of Massively Multilingual Models</title>
      <author><first>Shantanu</first><last>Patankar</last></author>
      <author><first>Omkar</first><last>Gokhale</last></author>
      <author><first>Onkar</first><last>Litake</last></author>
      <author><first>Aditya</first><last>Mandke</last></author>
      <author><first>Dipali</first><last>Kadam</last></author>
      <pages>8–12</pages>
      <url hash="dc7933c7">2022.sumeval-1.2</url>
      <bibkey>patankar-etal-2022-train</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>GMU</fixed-case> System Submission for the <fixed-case>SUME</fixed-case>val 2022 Shared Task</title>
      <author><first>Syeda Sabrina</first><last>Akter</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>13–20</pages>
      <url hash="21023201">2022.sumeval-1.3</url>
      <bibkey>akter-anastasopoulos-2022-gmu</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>NTREX</fixed-case>-128 – News Test References for <fixed-case>MT</fixed-case> Evaluation of 128 Languages</title>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Ying</first><last>Xin</last></author>
      <pages>21–24</pages>
      <url hash="162d5a44">2022.sumeval-1.4</url>
      <bibkey>federmann-etal-2022-ntrex</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>R</fixed-case>obusta: Towards Robustness Against Diverse Code-Mixed <fixed-case>I</fixed-case>ndonesian Local Languages</title>
      <author><first>Muhammad Farid</first><last>Adilazuarda</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <pages>25–34</pages>
      <url hash="3c8258ba">2022.sumeval-1.5</url>
      <bibkey>adilazuarda-etal-2022-indorobusta</bibkey>
    </paper>
  </volume>
</collection>
