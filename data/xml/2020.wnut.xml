<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.wnut">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</booktitle>
      <editor><first>Wei</first><last>Xu</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Tim</first><last>Baldwin</last></editor>
      <editor><first>Afshin</first><last>Rahimi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="626d85d3">2020.wnut-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>May <fixed-case>I</fixed-case> Ask Who’s Calling? Named Entity Recognition on Call Center Transcripts for Privacy Law Compliance</title>
      <author><first>Micaela</first><last>Kaplan</last></author>
      <pages>1–6</pages>
      <abstract>We investigate using Named Entity Recognition on a new type of user-generated text: a call center conversation. These conversations combine problems from spontaneous speech with problems novel to conversational Automated Speech Recognition, including incorrect recognition, alongside other common problems from noisy user-generated text. Using our own corpus with new annotations, training custom contextual string embeddings, and applying a BiLSTM-CRF, we match state-of- the-art results on our novel task.</abstract>
      <url hash="e2a50533">2020.wnut-1.1</url>
      <doi>10.18653/v1/2020.wnut-1.1</doi>
    </paper>
    <paper id="2">
      <title>“Did you really mean what you said?” : Sarcasm Detection in <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Code-Mixed Data using Bilingual Word Embeddings</title>
      <author><first>Akshita</first><last>Aggarwal</last></author>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <author><first>Anshima</first><last>Chaudhary</last></author>
      <author><first>Kavita</first><last>Maurya</last></author>
      <pages>7–15</pages>
      <abstract>With the increased use of social media platforms by people across the world, many new interesting NLP problems have come into existence. One such being the detection of sarcasm in the social media texts. We present a corpus of tweets for training custom word embeddings and a Hinglish dataset labelled for sarcasm detection. We propose a deep learning based approach to address the issue of sarcasm detection in Hindi-English code mixed tweets using bilingual word embeddings derived from FastText and Word2Vec approaches. We experimented with various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with and without attention). We were able to outperform all state-of-the-art performances with our deep learning models, with attention based Bi-directional LSTMs giving the best performance exhibiting an accuracy of 78.49%.</abstract>
      <url hash="a9163609">2020.wnut-1.2</url>
      <doi>10.18653/v1/2020.wnut-1.2</doi>
    </paper>
    <paper id="3">
      <title>Noisy Text Data: Achilles’ Heel of <fixed-case>BERT</fixed-case></title>
      <author><first>Ankit</first><last>Kumar</last></author>
      <author><first>Piyush</first><last>Makhija</last></author>
      <author><first>Anuj</first><last>Gupta</last></author>
      <pages>16–21</pages>
      <abstract>Owing to the phenomenal success of BERT on various NLP tasks and benchmark datasets, industry practitioners are actively experimenting with fine-tuning BERT to build NLP applications for solving industry use cases. For most datasets that are used by practitioners to build industrial NLP applications, it is hard to guarantee absence of any noise in the data. While BERT has performed exceedingly well for transferring the learnings from one use case to another, it remains unclear how BERT performs when fine-tuned on noisy text. In this work, we explore the sensitivity of BERT to noise in the data. We work with most commonly occurring noise (spelling mistakes, typos) and show that this results in significant degradation in the performance of BERT. We present experimental results to show that BERT’s performance on fundamental NLP tasks like sentiment analysis and textual similarity drops significantly in the presence of (simulated) noise on benchmark datasets viz. IMDB Movie Review, STS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline that are responsible for this drop in performance. Our findings suggest that practitioners need to be vary of presence of noise in their datasets while fine-tuning BERT to solve industry use cases.</abstract>
      <url hash="d52f5b44">2020.wnut-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c4149e53">2020.wnut-1.3.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.3</doi>
    </paper>
    <paper id="4">
      <title>Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning</title>
      <author><first>Rachel</first><last>Gardner</last></author>
      <author><first>Maya</first><last>Varma</last></author>
      <author><first>Clare</first><last>Zhu</last></author>
      <author><first>Ranjay</first><last>Krishna</last></author>
      <pages>22–27</pages>
      <abstract>Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer (QA) plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response. We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers (Question Plausibility AUROC=0.75, Response Plausibility AUROC=0.78, Answer Extraction F1=0.665).</abstract>
      <url hash="5d1f0c53">2020.wnut-1.4</url>
      <doi>10.18653/v1/2020.wnut-1.4</doi>
    </paper>
    <paper id="5">
      <title>Combining <fixed-case>BERT</fixed-case> with Static Word Embeddings for Categorizing Social Media</title>
      <author><first>Israa</first><last>Alghanmi</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>28–33</pages>
      <abstract>Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the social media genre, despite the fact that social media often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the Arabic language, which is successful in categorizing social media posts in Arabic dialects, despite only having been trained on Modern Standard Arabic. Our hypothesis in this paper is that the performance of LMs for social media can nonetheless be improved by incorporating static word vectors that have been specifically trained on social media. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).</abstract>
      <url hash="87599cd4">2020.wnut-1.5</url>
      <doi>10.18653/v1/2020.wnut-1.5</doi>
    </paper>
    <paper id="6">
      <title>Enhanced Sentence Alignment Network for Efficient Short Text Matching</title>
      <author><first>Zhe</first><last>Hu</last></author>
      <author><first>Zuohui</first><last>Fu</last></author>
      <author><first>Cheng</first><last>Peng</last></author>
      <author><first>Weiwei</first><last>Wang</last></author>
      <pages>34–40</pages>
      <abstract>Cross-sentence attention has been widely applied in text matching, in which model learns the aligned information between two intermediate sequence representations to capture their semantic relationship. However, commonly the intermediate representations are generated solely based on the preceding layers and the models may suffer from error propagation and unstable matching, especially when multiple attention layers are used. In this paper, we pro-pose an enhanced sentence alignment network with simple gated feature augmentation, where the model is able to flexibly integrate both original word and contextual features to improve the cross-sentence attention. Moreover, our model is less complex with fewer parameters compared to many state-of-the-art structures.Experiments on three benchmark datasets validate our model capacity for text matching.</abstract>
      <url hash="8d42f4ec">2020.wnut-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="90867caf">2020.wnut-1.6.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>PHINC</fixed-case>: A Parallel <fixed-case>H</fixed-case>inglish Social Media Code-Mixed Corpus for Machine Translation</title>
      <author><first>Vivek</first><last>Srivastava</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>41–49</pages>
      <abstract>Code-mixing is the phenomenon of using more than one language in a sentence. In the multilingual communities, it is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding natural language to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as Google Translate fail at times to translate code-mixed text effectively. To address this challenge, we present a parallel corpus of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in English. In addition, we also propose a translation pipeline build on top of Google Translate. The evaluation of the proposed pipeline on <tex-math>PHINC</tex-math> demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.</abstract>
      <url hash="7a0cbddf">2020.wnut-1.7</url>
      <doi>10.18653/v1/2020.wnut-1.7</doi>
    </paper>
    <paper id="8">
      <title>Cross-lingual sentiment classification in low-resource <fixed-case>B</fixed-case>engali language</title>
      <author><first>Salim</first><last>Sazzed</last></author>
      <pages>50–60</pages>
      <abstract>Sentiment analysis research in low-resource languages such as Bengali is still unexplored due to the scarcity of annotated data and the lack of text processing tools. Therefore, in this work, we focus on generating resources and showing the applicability of the cross-lingual sentiment analysis approach in Bengali. For benchmarking, we created and annotated a comprehensive corpus of around 12000 Bengali reviews. To address the lack of standard text-processing tools in Bengali, we leverage resources from English utilizing machine translation. We determine the performance of supervised machine learning (ML) classifiers in machine-translated English corpus and compare it with the original Bengali corpus. Besides, we examine sentiment preservation in the machine-translated corpus utilizing Cohen’s Kappa and Gwet’s AC1. To circumvent the laborious data labeling process, we explore lexicon-based methods and study the applicability of utilizing cross-domain labeled data from the resource-rich language. We find that supervised ML classifiers show comparable performances in Bengali and machine-translated English corpus. By utilizing labeled data, they achieve 15%-20% higher F1 scores compared to both lexicon-based and transfer learning-based methods. Besides, we observe that machine translation does not alter the sentiment polarity of the review for most of the cases. Our experimental results demonstrate that the machine translation based cross-lingual approach can be an effective way for sentiment classification in Bengali.</abstract>
      <url hash="89894694">2020.wnut-1.8</url>
      <doi>10.18653/v1/2020.wnut-1.8</doi>
    </paper>
    <paper id="9">
      <title>The Non-native Speaker Aspect: <fixed-case>I</fixed-case>ndian <fixed-case>E</fixed-case>nglish in Social Media</title>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Sayantan</first><last>Mahinder</last></author>
      <author><first>Ashiqur</first><last>KhudaBukhsh</last></author>
      <pages>61–70</pages>
      <abstract>As the largest institutionalized second language variety of English, Indian English has received a sustained focus from linguists for decades. However, to the best of our knowledge, no prior study has contrasted web-expressions of Indian English in noisy social media with English generated by a social media user base that are predominantly native speakers. In this paper, we address this gap in the literature through conducting a comprehensive analysis considering multiple structural and semantic aspects. In addition, we propose a novel application of language models to perform automatic linguistic quality assessment.</abstract>
      <url hash="bcbf3d2e">2020.wnut-1.9</url>
      <doi>10.18653/v1/2020.wnut-1.9</doi>
    </paper>
    <paper id="10">
      <title>Sentence Boundary Detection on Line Breaks in <fixed-case>J</fixed-case>apanese</title>
      <author><first>Yuta</first><last>Hayashibe</last></author>
      <author><first>Kensuke</first><last>Mitsuzawa</last></author>
      <pages>71–75</pages>
      <abstract>For NLP, sentence boundary detection (SBD) is an essential task to decompose a text into sentences. Most of the previous studies have used a simple rule that uses only typical characters as sentence boundaries. However, some characters may or may not be sentence boundaries depending on the context. We focused on line breaks in them. We newly constructed annotated corpora, implemented sentence boundary detectors, and analyzed performance of SBD in several settings.</abstract>
      <url hash="916264fa">2020.wnut-1.10</url>
      <revision id="1" href="2020.wnut-1.10v1" hash="e7d15380"/>
      <revision id="2" href="2020.wnut-1.10v2" hash="916264fa" date="2020-11-29">All modifications are fixes of typos in tables and their captions.</revision>
      <doi>10.18653/v1/2020.wnut-1.10</doi>
    </paper>
    <paper id="11">
      <title>Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach</title>
      <author><first>Yasuhiro</first><last>Yamaguchi</last></author>
      <author><first>Shintaro</first><last>Inuzuka</last></author>
      <author><first>Makoto</first><last>Hiramatsu</last></author>
      <author><first>Jun</first><last>Harashima</last></author>
      <pages>76–80</pages>
      <abstract>Recently, the number of user-generated recipes on the Internet has increased. In such recipes, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a user-generated recipe are not actually edible ingredients. For example, headings, comments, and kitchenware sometimes appear in an ingredient list because users can freely write the list in their recipes. Such noise makes it difficult for computers to use recipes for a variety of tasks, such as calorie estimation. To address this issue, we propose a non-ingredient detection method inspired by a neural sequence tagging model. In our experiment, we annotated 6,675 ingredients in 600 user-generated recipes and showed that our proposed method achieved a 93.3 F1 score.</abstract>
      <url hash="b13595b3">2020.wnut-1.11</url>
      <doi>10.18653/v1/2020.wnut-1.11</doi>
    </paper>
    <paper id="12">
      <title>Generating Fact Checking Summaries for Web Claims</title>
      <author><first>Rahul</first><last>Mishra</last></author>
      <author><first>Dhruv</first><last>Gupta</last></author>
      <author><first>Markus</first><last>Leippold</last></author>
      <pages>81–90</pages>
      <abstract>We present SUMO, a neural attention-based approach that learns to establish correctness of textual claims based on evidence in the form of text documents (e.g., news articles or web documents). SUMO further generates an extractive summary by presenting a diversified set of sentences from the documents that explain its decision on the correctness of the textual claim. Prior approaches to address the problem of fact checking and evidence extraction have relied on simple concatenation of claim and document word embeddings as an input to claim driven attention weight computation. This is done so as to extract salient words and sentences from the documents that help establish the correctness of the claim. However this design of claim-driven attention fails to capture the contextual information in documents properly. We improve on the prior art by using improved claim and title guided hierarchical attention to model effective contextual cues. We show the efficacy of our approach on political, healthcare, and environmental datasets.</abstract>
      <url hash="fe294e99">2020.wnut-1.12</url>
      <doi>10.18653/v1/2020.wnut-1.12</doi>
    </paper>
    <paper id="13">
      <title>Intelligent Analyses on Storytelling for Impact Measurement</title>
      <author><first>Koen</first><last>Kicken</last></author>
      <author><first>Tessa</first><last>De Maesschalck</last></author>
      <author><first>Bart</first><last>Vanrumste</last></author>
      <author><first>Tom</first><last>De Keyser</last></author>
      <author><first>Hee Reen</first><last>Shim</last></author>
      <pages>91–100</pages>
      <abstract>This paper explores how Dutch diary fragments, written by family coaches in the social sector, can be analysed automatically using machine learning techniques to quantitatively measure the impact of social coaching. The focus lays on two tasks: determining which sentiment a fragment contains (sentiment analysis) and investigating which fundamental social rights (education, employment, legal aid, etc.) are addressed in the fragment. To train and test the new algorithms, a dataset consisting of 1715 Dutch diary fragments is used. These fragments are manually labelled on sentiment and on the applicable fundamental social rights. The sentiment analysis models were trained to classify the fragments into three classes: negative, neutral or positive. Fine-tuning the Dutch pre-trained Bidirectional Encoder Representations from Transformers (BERTje) (de Vries et al., 2019) language model surpassed the more classic algorithms by correctly classifying 79.6% of the fragments on the sentiment analysis, which is considered as a good result. This technique also achieved the best results in the identification of the fundamental rights, where for every fragment the three most likely fundamental rights were given as output. In this way, 93% of the present fundamental rights were correctly recognised. To our knowledge, we are the first to try to extract social rights from written text with the help of Natural Language Processing techniques.</abstract>
      <url hash="5bcb8388">2020.wnut-1.13</url>
      <doi>10.18653/v1/2020.wnut-1.13</doi>
    </paper>
    <paper id="14">
      <title>An Empirical Analysis of Human-Bot Interaction on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Ming-Cheng</first><last>Ma</last></author>
      <author><first>John P.</first><last>Lalor</last></author>
      <pages>101–106</pages>
      <abstract>Automated agents (“bots”) have emerged as an ubiquitous and influential presence on social media. Bots engage on social media platforms by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of the activity of a single bot on Reddit. Our goal is to determine whether bot activity (in the form of posted comments on the website) has an effect on how humans engage on Reddit. We find that (1) the sentiment of a bot comment has a significant, positive effect on the subsequent human reply, and (2) human Reddit users modify their comment behaviors to overlap with the text of the bot, similar to how humans modify their text to mimic other humans in conversation. Understanding human-bot interactions on social media with relatively simple bots is important for preparing for more advanced bots in the future.</abstract>
      <url hash="2142851b">2020.wnut-1.14</url>
      <doi>10.18653/v1/2020.wnut-1.14</doi>
    </paper>
    <paper id="15">
      <title>Detecting Trending Terms in Cybersecurity Forum Discussions</title>
      <author><first>Jack</first><last>Hughes</last></author>
      <author><first>Seth</first><last>Aycock</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <author><first>Alice</first><last>Hutchings</last></author>
      <pages>107–115</pages>
      <abstract>We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground hacking forum, spanning over ten years of activity, with posts containing misspellings, orthographic variation, acronyms, and slang. Our statistical approach supports analysis of linguistic change and discussion topics over time, without a requirement to train a topic model for each time interval for analysis. We evaluate the approach by comparing the results to TF-IDF using the discounted cumulative gain metric with human annotations, finding our method outperforms TF-IDF on information retrieval.</abstract>
      <url hash="1f5b8b49">2020.wnut-1.15</url>
      <doi>10.18653/v1/2020.wnut-1.15</doi>
    </paper>
    <paper id="16">
      <title>Service registration chatbot: collecting and comparing dialogues from <fixed-case>AMT</fixed-case> workers and service’s users</title>
      <author><first>Luca</first><last>Molteni</last></author>
      <author><first>Mittul</first><last>Singh</last></author>
      <author><first>Juho</first><last>Leinonen</last></author>
      <author><first>Katri</first><last>Leino</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <author><first>Emanuele</first><last>Della Valle</last></author>
      <pages>116–121</pages>
      <abstract>Crowdsourcing is the go-to solution for data collection and annotation in the context of NLP tasks. Nevertheless, crowdsourced data is noisy by nature; the source is often unknown and additional validation work is performed to guarantee the dataset’s quality. In this article, we compare two crowdsourcing sources on a dialogue paraphrasing task revolving around a chatbot service. We observe that workers hired on crowdsourcing platforms produce lexically poorer and less diverse rewrites than service users engaged voluntarily. Notably enough, on dialogue clarity and optimality, the two paraphrase sources’ human-perceived quality does not differ significantly. Furthermore, for the chatbot service, the combined crowdsourced data is enough to train a transformer-based Natural Language Generation (NLG) system. To enable similar services, we also release tools for collecting data and training the dialogue-act-based transformer-based NLG module.</abstract>
      <url hash="e003fca2">2020.wnut-1.16</url>
      <doi>10.18653/v1/2020.wnut-1.16</doi>
    </paper>
    <paper id="17">
      <title>Automated Assessment of Noisy Crowdsourced Free-text Answers for <fixed-case>H</fixed-case>indi in Low Resource Setting</title>
      <author><first>Dolly</first><last>Agarwal</last></author>
      <author><first>Somya</first><last>Gupta</last></author>
      <author><first>Nishant</first><last>Baghel</last></author>
      <pages>122–131</pages>
      <abstract>The requirement of performing assessments continually on a larger scale necessitates the implementation of automated systems for evaluation of the learners’ responses to free-text questions. We target children of age group 8-14 years and use an ASR integrated assessment app to crowdsource learners’ responses to free text questions in Hindi. The app helped collect 39641 user answers to 35 different questions of Science topics. Since the users are young children from rural India and may not be well-equipped with technology, it brings in various noise types in the answers. We describe these noise types and propose a preprocessing pipeline to denoise user’s answers. We showcase the performance of different similarity metrics on the noisy and denoised versions of user and model answers. Our findings have large-scale applications for automated answer assessment for school children in India in low resource settings.</abstract>
      <url hash="aef8ee80">2020.wnut-1.17</url>
      <doi>10.18653/v1/2020.wnut-1.17</doi>
    </paper>
    <paper id="18">
      <title>Punctuation Restoration using Transformer Models for High-and Low-Resource Languages</title>
      <author><first>Tanvirul</first><last>Alam</last></author>
      <author><first>Akib</first><last>Khan</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <pages>132–142</pages>
      <abstract>Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this problem using different deep learning models. Recently, transformer models have proven their success in downstream NLP tasks, and these models have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For English, we obtain comparable state-of-the-art results, while for Bangla, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.</abstract>
      <url hash="267337be">2020.wnut-1.18</url>
      <doi>10.18653/v1/2020.wnut-1.18</doi>
    </paper>
    <paper id="19">
      <title>Truecasing <fixed-case>G</fixed-case>erman user-generated conversational text</title>
      <author><first>Yulia</first><last>Grishina</last></author>
      <author><first>Thomas</first><last>Gueudre</last></author>
      <author><first>Ralf</first><last>Winkler</last></author>
      <pages>143–148</pages>
      <abstract>True-casing, the task of restoring proper case to (generally) lower case input, is important in downstream tasks and for screen display. In this paper, we investigate truecasing as an in- trinsic task and present several experiments on noisy user queries to a voice-controlled dia- log system. In particular, we compare a rule- based, an n-gram language model (LM) and a recurrent neural network (RNN) approaches, evaluating the results on a German Q&amp;A cor- pus and reporting accuracy for different case categories. We show that while RNNs reach higher accuracy especially on large datasets, character n-gram models with interpolation are still competitive, in particular on mixed- case words where their fall-back mechanisms come into play.</abstract>
      <url hash="bb8cfda0">2020.wnut-1.19</url>
      <doi>10.18653/v1/2020.wnut-1.19</doi>
    </paper>
    <paper id="20">
      <title>Fine-Tuning <fixed-case>MT</fixed-case> systems for Robustness to Second-Language Speaker Variations</title>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>149–158</pages>
      <abstract>The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate this issue. We show that fine-tuning using naturally occurring noise along with pseudo-references (i.e. “corrected” non-native inputs translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from English to Spanish, Italian, French, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new state-of-the-art on the JFLEG-ES dataset. All datasets and code are publicly available here: https://github.com/mahfuzibnalam/finetuning_for_robustness .</abstract>
      <url hash="f665096a">2020.wnut-1.20</url>
      <doi>10.18653/v1/2020.wnut-1.20</doi>
    </paper>
    <paper id="21">
      <title>Impact of <fixed-case>ASR</fixed-case> on <fixed-case>A</fixed-case>lzheimer’s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others</title>
      <author><first>Aparna</first><last>Balagopalan</last></author>
      <author><first>Ksenia</first><last>Shkaruta</last></author>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>159–164</pages>
      <abstract>Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect detection performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for deletion errors in order to improve dementia detection performance.</abstract>
      <url hash="0914b233">2020.wnut-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="84821a87">2020.wnut-1.21.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.21</doi>
    </paper>
    <paper id="22">
      <title>Detecting Entailment in Code-Mixed <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Conversations</title>
      <author><first>Sharanya</first><last>Chakravarthy</last></author>
      <author><first>Anjana</first><last>Umapathy</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>165–170</pages>
      <abstract>The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among polyglots. Given the rising importance of dialogue agents, it is imperative that they understand code-mixing, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The dataset by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of language modeling, data augmentation, translation, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this dataset. We obtain an 8.09% increase in test set accuracy over the current state of the art.</abstract>
      <url hash="c3b2840b">2020.wnut-1.22</url>
      <doi>10.18653/v1/2020.wnut-1.22</doi>
    </paper>
    <paper id="23">
      <title>Detecting Objectifying Language in Online Professor Reviews</title>
      <author><first>Angie</first><last>Waller</last></author>
      <author><first>Kyle</first><last>Gorman</last></author>
      <pages>171–180</pages>
      <abstract>Student reviews often make reference to professors’ physical appearances. Until recently RateMyProfessors.com, the website of this study’s focus, used a design feature to encourage a “hot or not” rating of college professors. In the wake of recent #MeToo and #TimesUp movements, social awareness of the inappropriateness of these reviews has grown; however, objectifying comments remain and continue to be posted in this online context. We describe two supervised text classifiers for detecting objectifying commentary in professor reviews. We then ensemble these classifiers and use the resulting model to track objectifying commentary at scale. We measure correlations between objectifying commentary, changes to the review website interface, and teacher gender across a ten-year period.</abstract>
      <url hash="74aa2929">2020.wnut-1.23</url>
      <attachment type="OptionalSupplementaryMaterial" hash="60452039">2020.wnut-1.23.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.wnut-1.23</doi>
    </paper>
    <paper id="24">
      <title>Annotation Efficient Language Identification from Weak Labels</title>
      <author><first>Shriphani</first><last>Palakodety</last></author>
      <author><first>Ashiqur</first><last>KhudaBukhsh</last></author>
      <pages>181–192</pages>
      <abstract>India is home to several languages with more than 30m speakers. These languages exhibit significant presence on social media platforms. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) models and resources. User generated social media content in these languages is also typically authored in the Roman script as opposed to the traditional native script further contributing to resource scarcity. In this paper, we leverage a minimally supervised NLP technique to obtain weak language labels from a large-scale Indian social media corpus leading to a robust and annotation-efficient language-identification technique spanning nine Romanized Indian languages. In fast-spreading pandemic situations such as the current COVID-19 situation, information processing objectives might be heavily tilted towards under-served languages in densely populated regions. We release our models to facilitate downstream analyses in these low-resource languages. Experiments across multiple social media corpora demonstrate the model’s robustness and provide several interesting insights on Indian language usage patterns on social media. We release an annotated data set of 1,000 comments in ten Romanized languages as a social media evaluation benchmark.</abstract>
      <url hash="a803b951">2020.wnut-1.24</url>
      <doi>10.18653/v1/2020.wnut-1.24</doi>
    </paper>
    <paper id="25">
      <title>Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach</title>
      <author><first>Ben</first><last>Eyre</last></author>
      <author><first>Aparna</first><last>Balagopalan</last></author>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>193–199</pages>
      <abstract>Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from noisy text is time and resource consuming, and can potentially result in features that do not enhance model performance. To combat this, we describe a new approach to feature engineering that leverages sequential machine learning models and domain knowledge to predict which features help enhance performance. We provide a concrete example of this method on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3% over a strong baseline when using features produced by this method. This demonstration provides an example of how this method can be used to assist classification in fields where interpretability is important, such as health care.</abstract>
      <url hash="fb9c2b7d">2020.wnut-1.25</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8564b332">2020.wnut-1.25.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.25</doi>
    </paper>
    <paper id="26">
      <title>Quantifying the Evaluation of Heuristic Methods for Textual Data Augmentation</title>
      <author><first>Omid</first><last>Kashefi</last></author>
      <author><first>Rebecca</first><last>Hwa</last></author>
      <pages>200–208</pages>
      <abstract>Data augmentation has been shown to be effective in providing more training data for machine learning and resulting in more robust classifiers. However, for some problems, there may be multiple augmentation heuristics, and the choices of which one to use may significantly impact the success of the training. In this work, we propose a metric for evaluating augmentation heuristics; specifically, we quantify the extent to which an example is “hard to distinguish” by considering the difference between the distribution of the augmented samples of different classes. Experimenting with multiple heuristics in two prediction tasks (positive/negative sentiment and verbosity/conciseness) validates our claims by revealing the connection between the distribution difference of different classes and the classification accuracy.</abstract>
      <url hash="96e178ea">2020.wnut-1.26</url>
      <doi>10.18653/v1/2020.wnut-1.26</doi>
    </paper>
    <paper id="27">
      <title>An Empirical Survey of Unsupervised Text Representation Methods on <fixed-case>T</fixed-case>witter Data</title>
      <author><first>Lili</first><last>Wang</last></author>
      <author><first>Chongyang</first><last>Gao</last></author>
      <author><first>Jason</first><last>Wei</last></author>
      <author><first>Weicheng</first><last>Ma</last></author>
      <author><first>Ruibo</first><last>Liu</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>209–214</pages>
      <abstract>The field of NLP has seen unprecedented achievements in recent years. Most notably, with the advent of large-scale pre-trained Transformer-based language models, such as BERT, there has been a noticeable improvement in text representation. It is, however, unclear whether these improvements translate to noisy user-generated text, such as tweets. In this paper, we present an experimental survey of a wide range of well-known text representation techniques for the task of text clustering on noisy Twitter data. Our results indicate that the more advanced models do not necessarily work best on tweets and that more exploration in this area is needed.</abstract>
      <url hash="0907f6cd">2020.wnut-1.27</url>
      <doi>10.18653/v1/2020.wnut-1.27</doi>
    </paper>
    <paper id="28">
      <title>Civil Unrest on <fixed-case>T</fixed-case>witter (<fixed-case>CUT</fixed-case>): A Dataset of Tweets to Support Research on Civil Unrest</title>
      <author><first>Justin</first><last>Sech</last></author>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Anna L.</first><last>Buczak</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>215–221</pages>
      <abstract>We present CUT, a dataset for studying Civil Unrest on Twitter. Our dataset includes 4,381 tweets related to civil unrest, hand-annotated with information related to the study of civil unrest discussion and events. Our dataset is drawn from 42 countries from 2014 to 2019. We present baseline systems trained on this data for the identification of tweets related to civil unrest. We include a discussion of ethical issues related to research on this topic.</abstract>
      <url hash="49aba556">2020.wnut-1.28</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c311282c">2020.wnut-1.28.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.wnut-1.28</doi>
    </paper>
    <paper id="29">
      <title>Tweeki: Linking Named Entities on <fixed-case>T</fixed-case>witter to a Knowledge Graph</title>
      <author><first>Bahareh</first><last>Harandizadeh</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>222–231</pages>
      <abstract>To identify what entities are being talked about in tweets, we need to automatically link named entities that appear in tweets to structured KBs like WikiData. Existing approaches often struggle with such short, noisy texts, or their complex design and reliance on supervision make them brittle, difficult to use and maintain, and lose significance over time. Further, there is a lack of a large, linked corpus of tweets to aid researchers, along with lack of gold dataset to evaluate the accuracy of entity linking. In this paper, we introduce (1) Tweeki, an unsupervised, modular entity linking system for Twitter, (2) TweekiData, a large, automatically-annotated corpus of Tweets linked to entities in WikiData, and (3) TweekiGold, a gold dataset for entity linking evaluation. Through comprehensive analysis, we show that Tweeki is comparable to the performance of recent state-of-the-art entity linkers models, the dataset is of high quality, and a use case of how the dataset can be used to improve downstream tasks in social media analysis (geolocation prediction).</abstract>
      <url hash="089fc25a">2020.wnut-1.29</url>
      <doi>10.18653/v1/2020.wnut-1.29</doi>
    </paper>
    <paper id="30">
      <title>Representation learning of writing style</title>
      <author><first>Julien</first><last>Hay</last></author>
      <author><first>Bich-Lien</first><last>Doan</last></author>
      <author><first>Fabrice</first><last>Popineau</last></author>
      <author><first>Ouassim</first><last>Ait Elhara</last></author>
      <pages>232–243</pages>
      <abstract>In this paper, we introduce a new method of representation learning that aims to embed documents in a stylometric space. Previous studies in the field of authorship analysis focused on feature engineering techniques in order to represent document styles and to enhance model performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of writing style. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but it relies only on the metadata of the articles which are available in huge amounts. We evaluate the model on multiple datasets, on both the authorship clustering and the authorship attribution tasks.</abstract>
      <url hash="9682d7d0">2020.wnut-1.30</url>
      <doi>10.18653/v1/2020.wnut-1.30</doi>
    </paper>
    <paper id="31">
      <title>“A Little Birdie Told Me ... ” - Inductive Biases for Rumour Stance Detection on Social Media</title>
      <author><first>Karthik</first><last>Radhakrishnan</last></author>
      <author><first>Tushar</first><last>Kanakagiri</last></author>
      <author><first>Sharanya</first><last>Chakravarthy</last></author>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <pages>244–248</pages>
      <abstract>The rise in the usage of social media has placed it in a central position for news dissemination and consumption. This greatly increases the potential for proliferation of rumours and misinformation. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a social media post. Unlike previous works, we impose inductive biases that capture platform specific user behavior. These biases, coupled with social media fine-tuning of BERT allow for better language understanding, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.</abstract>
      <url hash="07e94742">2020.wnut-1.31</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b721a636">2020.wnut-1.31.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.wnut-1.31</doi>
    </paper>
    <paper id="32">
      <title>Paraphrase Generation via Adversarial Penalizations</title>
      <author><first>Gerson</first><last>Vizcarra</last></author>
      <author><first>Jose</first><last>Ochoa-Luna</last></author>
      <pages>249–259</pages>
      <abstract>Paraphrase generation is an important problem in Natural Language Processing that has been addressed with neural network-based approaches recently. This paper presents an adversarial framework to address the paraphrase generation problem in English. Unlike previous methods, we employ the discriminator output as penalization instead of using policy gradients, and we propose a global discriminator to avoid the Monte-Carlo search. In addition, this work use and compare different settings of input representation. We compare our methods to some baselines in the Quora question pairs dataset. The results show that our framework is competitive against the previous benchmarks.</abstract>
      <url hash="c8ef53a1">2020.wnut-1.32</url>
      <doi>10.18653/v1/2020.wnut-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>WNUT</fixed-case>-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols</title>
      <author><first>Jeniya</first><last>Tabassum</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <pages>260–267</pages>
      <abstract>This paper presents the results of the wet labinformation extraction task at WNUT 2020.This task consisted of two sub tasks- (1) anamed entity recognition task with 13 partic-ipants; and (2) a relation extraction task with2 participants. We outline the task, data an-notation process, corpus statistics, and providea high-level overview of the participating sys-tems for each sub task.</abstract>
      <url hash="b805d0ff">2020.wnut-1.33</url>
      <doi>10.18653/v1/2020.wnut-1.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>IITKGP</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-1: Domain specific <fixed-case>BERT</fixed-case> representation for Named Entity Recognition of lab protocol</title>
      <author><first>Tejas</first><last>Vaidhya</last></author>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <pages>268–272</pages>
      <abstract>Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks.For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistictasks. But the vocabulary used in the medical field contains a lot of different tokens used only in the medical industry such as the name of different diseases, devices, organisms,medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the System for Named Entity Tagging based on Bio-Bert. Experimental results show that our model gives substantial improvements over the baseline and stood the fourth runner up in terms of F1 score, and first runner up in terms of Recall with just 2.21 F1 score behind the best one.</abstract>
      <url hash="60747a41">2020.wnut-1.34</url>
      <doi>10.18653/v1/2020.wnut-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>P</fixed-case>ublish<fixed-case>I</fixed-case>n<fixed-case>C</fixed-case>ovid19 at <fixed-case>WNUT</fixed-case> 2020 Shared Task-1: Entity Recognition in Wet Lab Protocols using Structured Learning Ensemble and Contextualised Embeddings</title>
      <author><first>Janvijay</first><last>Singh</last></author>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <pages>273–280</pages>
      <abstract>In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings (like Flair, BERT-based) and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling (SLE). Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively.</abstract>
      <url hash="b51d7de1">2020.wnut-1.35</url>
      <doi>10.18653/v1/2020.wnut-1.35</doi>
    </paper>
    <paper id="36">
      <title>Big Green at <fixed-case>WNUT</fixed-case> 2020 Shared Task-1: Relation Extraction as Contextualized Sequence Classification</title>
      <author><first>Chris</first><last>Miller</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>281–285</pages>
      <abstract>Relation and event extraction is an important task in natural language processing. We introduce a system which uses contextualized knowledge graph completion to classify relations and events between known entities in a noisy text environment. We report results which show that our system is able to effectively extract relations and events from a dataset of wet lab protocols.</abstract>
      <url hash="897e2669">2020.wnut-1.36</url>
      <doi>10.18653/v1/2020.wnut-1.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>WNUT</fixed-case> 2020 Shared Task-1: Conditional Random Field(<fixed-case>CRF</fixed-case>) based Named Entity Recognition(<fixed-case>NER</fixed-case>) for Wet Lab Protocols</title>
      <author><first>Kaushik</first><last>Acharya</last></author>
      <pages>286–289</pages>
      <abstract>The paper describes how classifier model built using Conditional Random Field detects named entities in wet lab protocols.</abstract>
      <url hash="823785dd">2020.wnut-1.37</url>
      <attachment type="OptionalSupplementaryMaterial" hash="12fdd120">2020.wnut-1.37.OptionalSupplementaryMaterial.txt</attachment>
      <doi>10.18653/v1/2020.wnut-1.37</doi>
    </paper>
    <paper id="38">
      <title>mgsohrab at <fixed-case>WNUT</fixed-case> 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols</title>
      <author><first>Mohammad Golam</first><last>Sohrab</last></author>
      <author><first>Anh-Khoa</first><last>Duong Nguyen</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>290–298</pages>
      <abstract>We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60% in terms of F-score as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46% in terms of F-score as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.</abstract>
      <url hash="5e97f082">2020.wnut-1.38</url>
      <doi>10.18653/v1/2020.wnut-1.38</doi>
    </paper>
    <paper id="39">
      <title>Fancy Man Launches Zippo at <fixed-case>WNUT</fixed-case> 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction</title>
      <author><first>Qingcheng</first><last>Zeng</last></author>
      <author><first>Xiaoyang</first><last>Fang</last></author>
      <author><first>Zhexin</first><last>Liang</last></author>
      <author><first>Haoding</first><last>Meng</last></author>
      <pages>299–304</pages>
      <abstract>Automatic or semi-automatic conversion of protocols specifying steps in performing a lab procedure into machine-readable format benefits biological research a lot. These noisy, dense, and domain-specific lab protocols processing draws more and more interests with the development of deep learning. This paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity extract, that we conducted studies in several models, including a BiLSTM CRF model and a Bert case model which can be used to complete wet lab entity extraction. And we mainly discussed the performance differences of Bert case under different situations such as transformers versions, case sensitivity that may don’t get enough attention before.</abstract>
      <url hash="da976ca2">2020.wnut-1.39</url>
      <doi>10.18653/v1/2020.wnut-1.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>B</fixed-case>i<fixed-case>T</fixed-case>e<fixed-case>M</fixed-case> at <fixed-case>WNUT</fixed-case> 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models</title>
      <author><first>Julien</first><last>Knafou</last></author>
      <author><first>Nona</first><last>Naderi</last></author>
      <author><first>Jenny</first><last>Copara</last></author>
      <author><first>Douglas</first><last>Teodoro</last></author>
      <author><first>Patrick</first><last>Ruch</last></author>
      <pages>305–313</pages>
      <abstract>Recent improvements in machine-reading technologies attracted much attention to automation problems and their possibilities. In this context, WNUT 2020 introduces a Name Entity Recognition (NER) task based on wet laboratory procedures. In this paper, we present a 3-step method based on deep neural language models that reported the best overall exact match F1-score (77.99%) of the competition. By fine-tuning 10 times, 10 different pretrained language models, this work shows the advantage of having more models in an ensemble based on a majority of votes strategy. On top of that, having 100 different models allowed us to analyse the combinations of ensemble that demonstrated the impact of having multiple pretrained models versus fine-tuning a pretrained model multiple times.</abstract>
      <url hash="d237e0d9">2020.wnut-1.40</url>
      <doi>10.18653/v1/2020.wnut-1.40</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>WNUT</fixed-case>-2020 Task 2: Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Thanh</first><last>Vu</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Mai Hoang</first><last>Dao</last></author>
      <author><first>Linh The</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Doan</last></author>
      <pages>314–318</pages>
      <abstract>In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.</abstract>
      <url hash="fc2a48b6">2020.wnut-1.41</url>
      <doi>10.18653/v1/2020.wnut-1.41</doi>
    </paper>
    <paper id="42">
      <title><fixed-case>TATL</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: A Transformer-based Baseline System for Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Anh</first><last>Tuan Nguyen</last></author>
      <pages>319–323</pages>
      <abstract>As the COVID-19 outbreak continues to spread throughout the world, more and more information about the pandemic has been shared publicly on social media. For example, there are a huge number of COVID-19 English Tweets daily on Twitter. However, the majority of those Tweets are uninformative, and hence it is important to be able to automatically select only the informative ones for downstream applications. In this short paper, we present our participation in the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Inspired by the recent advances in pretrained Transformer language models, we propose a simple yet effective baseline for the task. Despite its simplicity, our proposed approach shows very competitive results in the leaderboard as we ranked 8 over 56 teams participated in total.</abstract>
      <url hash="413922ef">2020.wnut-1.42</url>
      <doi>10.18653/v1/2020.wnut-1.42</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>NHK</fixed-case>_<fixed-case>STRL</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: <fixed-case>GAT</fixed-case>s with Syntactic Dependencies as Edges and <fixed-case>CTC</fixed-case>-based Loss for Text Classification</title>
      <author><first>Yuki</first><last>Yasuda</last></author>
      <author><first>Taichi</first><last>Ishiwatari</last></author>
      <author><first>Taro</first><last>Miyazaki</last></author>
      <author><first>Jun</first><last>Goto</last></author>
      <pages>324–330</pages>
      <abstract>The outbreak of COVID-19 has greatly impacted our daily lives. In these circumstances, it is important to grasp the latest information to avoid causing too much fear and panic. To help grasp new information, extracting information from social networking sites is one of the effective ways. In this paper, we describe a method to identify whether a tweet related to COVID-19 is informative or not, which can help to grasp new information. The key features of our method are its use of graph attention networks to encode syntactic dependencies and word positions in the sentence, and a loss function based on connectionist temporal classification (CTC) that can learn a label for each token without reference data for each token. Experimental results show that the proposed method achieved an F1 score of 0.9175, out- performing baseline methods.</abstract>
      <url hash="21c7ae2f">2020.wnut-1.43</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c9c0bfd5">2020.wnut-1.43.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.wnut-1.43</doi>
    </paper>
    <paper id="44">
      <title><fixed-case>NLP</fixed-case> North at <fixed-case>WNUT</fixed-case>-2020 Task 2: Pre-training versus Ensembling for Detection of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Anders</first><last>Giovanni Møller</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>331–336</pages>
      <abstract>With the COVID-19 pandemic raging world-wide since the beginning of the 2020 decade, the need for monitoring systems to track relevant information on social media is vitally important. This paper describes our submission to the WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. We investigate the effectiveness for a variety of classification models, and found that domain-specific pre-trained BERT models lead to the best performance. On top of this, we attempt a variety of ensembling strategies, but these attempts did not lead to further improvements. Our final best model, the standalone CT-BERT model, proved to be highly competitive, leading to a shared first place in the shared task. Our results emphasize the importance of domain and task-related pre-training.</abstract>
      <url hash="c0b8bee9">2020.wnut-1.44</url>
      <doi>10.18653/v1/2020.wnut-1.44</doi>
    </paper>
    <paper id="45">
      <title>Siva at <fixed-case>WNUT</fixed-case>-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets</title>
      <author><first>Siva</first><last>Sai</last></author>
      <pages>337–341</pages>
      <abstract>Social media witnessed vast amounts of misinformation being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as “infodemic.” The ill-effects of such misinformation are multifarious. Thus, identifying and eliminating the sources of misinformation becomes very crucial, especially when mass panic can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on social media. WNUT-2020 Task 2 aims at building systems for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3% in the final evaluation.</abstract>
      <url hash="40268765">2020.wnut-1.45</url>
      <doi>10.18653/v1/2020.wnut-1.45</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>IIITBH</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Exploiting the best of both worlds</title>
      <author><first>Saichethan</first><last>Reddy</last></author>
      <author><first>Pradeep</first><last>Biswal</last></author>
      <pages>342–346</pages>
      <abstract>In this paper, we present IIITBH team’s effort to solve the second shared task of the 6th Workshop on Noisy User-generated Text (W-NUT)i.e Identification of informative COVID-19 English Tweets. The central theme of the task is to develop a system that automatically identify whether an English Tweet related to the novel coronavirus (COVID-19) is Informative or not. Our approach is based on exploiting semantic information from both max pooling and average pooling, to this end we propose two models.</abstract>
      <url hash="e31c1001">2020.wnut-1.46</url>
      <doi>10.18653/v1/2020.wnut-1.46</doi>
    </paper>
    <paper id="47">
      <title>Phonemer at <fixed-case>WNUT</fixed-case>-2020 Task 2: Sequence Classification Using <fixed-case>COVID</fixed-case> <fixed-case>T</fixed-case>witter <fixed-case>BERT</fixed-case> and Bagging Ensemble Technique based on Plurality Voting</title>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <pages>347–351</pages>
      <abstract>This paper presents the approach that we employed to tackle the EMNLP WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English Tweets. The task is to develop a system that automatically identifies whether an English Tweet related to the novel coronavirus (COVID-19) is informative or not. We solve the task in three stages. The first stage involves pre-processing the dataset by filtering only relevant information. This is followed by experimenting with multiple deep learning models like CNNs, RNNs and Transformer based models. In the last stage, we propose an ensemble of the best model trained on different subsets of the provided dataset. Our final approach achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score as the evaluation criteria.</abstract>
      <url hash="45412cfd">2020.wnut-1.47</url>
      <doi>10.18653/v1/2020.wnut-1.47</doi>
    </paper>
    <paper id="48">
      <title><fixed-case>CXP</fixed-case>949 at <fixed-case>WNUT</fixed-case>-2020 Task 2: Extracting Informative <fixed-case>COVID</fixed-case>-19 Tweets - <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Ensembles and The Continued Relevance of Handcrafted Features</title>
      <author><first>Calum</first><last>Perrio</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <pages>352–358</pages>
      <abstract>This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned features in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional features can improve classification results and achieve a score within 2 points of the top performing team.</abstract>
      <url hash="2b799cf4">2020.wnut-1.48</url>
      <doi>10.18653/v1/2020.wnut-1.48</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>I</fixed-case>nfo<fixed-case>M</fixed-case>iner at <fixed-case>WNUT</fixed-case>-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction</title>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>359–365</pages>
      <abstract>Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves 10th place in the final rankings scoring 0.9004 F1 score for the test set.</abstract>
      <url hash="2fb1910b">2020.wnut-1.49</url>
      <doi>10.18653/v1/2020.wnut-1.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>BANANA</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Identifying <fixed-case>COVID</fixed-case>-19 Information on <fixed-case>T</fixed-case>witter by Combining Deep Learning and Transfer Learning Models</title>
      <author><first>Tin</first><last>Huynh</last></author>
      <author><first>Luan</first><last>Thanh Luan</last></author>
      <author><first>Son T.</first><last>Luu</last></author>
      <pages>366–370</pages>
      <abstract>The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction system for WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets. The dataset for this task contains size 10,000 tweets in English labeled by humans. The ensemble model from our three transformer and deep learning models is used for the final prediction. The experimental result indicates that we have achieved F1 for the INFORMATIVE label on our systems at 88.81% on the test set.</abstract>
      <url hash="06cd6db0">2020.wnut-1.50</url>
      <doi>10.18653/v1/2020.wnut-1.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>DATAMAFIA</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 <fixed-case>T</fixed-case>ask 2: <fixed-case>A</fixed-case> <fixed-case>S</fixed-case>tudy of <fixed-case>P</fixed-case>re-trained <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels along with <fixed-case>R</fixed-case>egularization <fixed-case>T</fixed-case>echniques for <fixed-case>D</fixed-case>ownstream <fixed-case>T</fixed-case>asks</title>
      <author><first>Ayan</first><last>Sengupta</last></author>
      <pages>371–377</pages>
      <abstract>This document describes the system description developed by team datamafia at WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. This paper contains a thorough study of pre-trained language models on downstream binary classification task over noisy user generated Twitter data. The solution submitted to final test leaderboard is a fine tuned RoBERTa model which achieves F1 score of 90.8% and 89.4% on the dev and test data respectively. In the later part, we explore several techniques for injecting regularization explicitly into language models to generalize predictions over noisy data. Our experiments show that adding regularizations to RoBERTa pre-trained model can be very robust to data and annotation noises and can improve overall performance by more than 1.2%.</abstract>
      <url hash="5e8bf63b">2020.wnut-1.51</url>
      <attachment type="OptionalSupplementaryMaterial" hash="6215b453">2020.wnut-1.51.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.wnut-1.51</doi>
    </paper>
    <paper id="52">
      <title><fixed-case>UP</fixed-case>enn<fixed-case>HLP</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2 : Transformer models for classification of <fixed-case>COVID</fixed-case>19 posts on <fixed-case>T</fixed-case>witter</title>
      <author><first>Arjun</first><last>Magge</last></author>
      <author><first>Varad</first><last>Pimpalkhute</last></author>
      <author><first>Divya</first><last>Rallapalli</last></author>
      <author><first>David</first><last>Siguenza</last></author>
      <author><first>Graciela</first><last>Gonzalez-Hernandez</last></author>
      <pages>378–382</pages>
      <abstract>Increasing usage of social media presents new non-traditional avenues for monitoring disease outbreaks, virus transmissions and disease progressions through user posts describing test results or disease symptoms. However, the discussions on the topic of infectious diseases that are informative in nature also span various topics such as news, politics and humor which makes the data mining challenging. We present a system to identify tweets about the COVID19 disease outbreak that are deemed to be informative on Twitter for use in downstream applications. The system scored a F1-score of 0.8941, Precision of 0.9028, Recall of 0.8856 and Accuracy of 0.9010. In the shared task organized as part of the 6th Workshop of Noisy User-generated Text (WNUT), the system was ranked 18th by F1-score and 13th by Accuracy.</abstract>
      <url hash="8f897d7b">2020.wnut-1.52</url>
      <doi>10.18653/v1/2020.wnut-1.52</doi>
    </paper>
    <paper id="53">
      <title><fixed-case>UIT</fixed-case>-<fixed-case>HSE</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Exploiting <fixed-case>CT</fixed-case>-<fixed-case>BERT</fixed-case> for Identifying <fixed-case>COVID</fixed-case>-19 Information on the <fixed-case>T</fixed-case>witter Social Network</title>
      <author><first>Khiem</first><last>Tran</last></author>
      <author><first>Hao</first><last>Phan</last></author>
      <author><first>Kiet</first><last>Nguyen</last></author>
      <author><first>Ngan Luu</first><last>Thuy Nguyen</last></author>
      <pages>383–387</pages>
      <abstract>Recently, COVID-19 has affected a variety of real-life aspects of the world and led to dreadful consequences. More and more tweets about COVID-19 has been shared publicly on Twitter. However, the plurality of those Tweets are uninformative, which is challenging to build automatic systems to detect the informative ones for useful AI applications. In this paper, we present our results at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. In particular, we propose our simple but effective approach using the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with different fine-tuning techniques. As a result, we achieve the F1-Score of 90.94% with the third place on the leaderboard of this task which attracted 56 submitted teams in total.</abstract>
      <url hash="28fa549b">2020.wnut-1.53</url>
      <doi>10.18653/v1/2020.wnut-1.53</doi>
    </paper>
    <paper id="54">
      <title>Emory at <fixed-case>WNUT</fixed-case>-2020 Task 2: Combining Pretrained Deep Learning Models and Feature Enrichment for Informative Tweet Identification</title>
      <author><first>Yuting</first><last>Guo</last></author>
      <author><first>Mohammed</first><last>Ali Al-Garadi</last></author>
      <author><first>Abeed</first><last>Sarker</last></author>
      <pages>388–393</pages>
      <abstract>This paper describes the system developed by the Emory team for the WNUT-2020 Task 2: “Identifi- cation of Informative COVID-19 English Tweet”. Our system explores three recent Transformer- based deep learning models pretrained on large- scale data to encode documents. Moreover, we developed two feature enrichment methods to en- hance document embeddings by integrating emoji embeddings and syntactic features into deep learn- ing models. Our system achieved F1-score of 0.897 and accuracy of 90.1% on the test set, and ranked in the top-third of all 55 teams.</abstract>
      <url hash="9051ee96">2020.wnut-1.54</url>
      <doi>10.18653/v1/2020.wnut-1.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>CSECU</fixed-case>-<fixed-case>DSG</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Fareen</first><last>Tasneem</last></author>
      <author><first>Jannatun</first><last>Naim</last></author>
      <author><first>Radiathun</first><last>Tasnia</last></author>
      <author><first>Tashin</first><last>Hossain</last></author>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <pages>394–398</pages>
      <abstract>COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the situation as well as beneficial for public safety personnel for decision making. However, the informal nature of twitter makes it challenging to refine the informative tweets from the huge tweet streams. To address these challenges WNUT-2020 introduced a shared task focusing on COVID-19 related informative tweet identification. In this paper, we describe our participation in this task. We propose a neural model that adopts the strength of transfer learning and hand-crafted features in a unified architecture. To extract the transfer learning features, we utilize the state-of-the-art pre-trained sentence embedding model BERT, RoBERTa, and InferSent, whereas various twitter characteristics are exploited to extract the hand-crafted features. Next, various feature combinations are utilized to train a set of multilayer perceptron (MLP) as the base-classifier. Finally, a majority voting based fusion approach is employed to determine the informative tweets. Our approach achieved competitive performance and outperformed the baseline by 7% (approx.).</abstract>
      <url hash="f5edb100">2020.wnut-1.55</url>
      <doi>10.18653/v1/2020.wnut-1.55</doi>
    </paper>
    <paper id="56">
      <title><fixed-case>IRL</fixed-case>ab@<fixed-case>IITBHU</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Identification of informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets using <fixed-case>BERT</fixed-case></title>
      <author><first>Supriya</first><last>Chanda</last></author>
      <author><first>Eshita</first><last>Nandy</last></author>
      <author><first>Sukomal</first><last>Pal</last></author>
      <pages>399–403</pages>
      <abstract>This paper reports our submission to the shared Task 2: Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks: DistilBERT and FastText. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.</abstract>
      <url hash="813d202a">2020.wnut-1.56</url>
      <doi>10.18653/v1/2020.wnut-1.56</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>N</fixed-case>ut<fixed-case>C</fixed-case>racker at <fixed-case>WNUT</fixed-case>-2020 Task 2: Robustly Identifying Informative <fixed-case>COVID</fixed-case>-19 Tweets using Ensembling and Adversarial Training</title>
      <author><first>Priyanshu</first><last>Kumar</last></author>
      <author><first>Aadarsh</first><last>Singh</last></author>
      <pages>404–408</pages>
      <abstract>We experiment with COVID-Twitter-BERT and RoBERTa models to identify informative COVID-19 tweets. We further experiment with adversarial training to make our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains a F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task 2 and ranks 1st on the leaderboard. The ensemble of the models trained using adversarial training also produces similar result.</abstract>
      <url hash="6532f337">2020.wnut-1.57</url>
      <doi>10.18653/v1/2020.wnut-1.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>DSC</fixed-case>-<fixed-case>IIT</fixed-case> <fixed-case>ISM</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Detection of <fixed-case>COVID</fixed-case>-19 informative tweets using <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Sirigireddy</first><last>Dhana Laxmi</last></author>
      <author><first>Rohit</first><last>Agarwal</last></author>
      <author><first>Aman</first><last>Sinha</last></author>
      <pages>409–413</pages>
      <abstract>Social media such as Twitter is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on social media which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our model on a public dataset with an F1-score of 0.89 on the validation dataset and 0.87 on the leaderboard.</abstract>
      <url hash="3853e316">2020.wnut-1.58</url>
      <doi>10.18653/v1/2020.wnut-1.58</doi>
    </paper>
    <paper id="59">
      <title>Linguist Geeks on <fixed-case>WNUT</fixed-case>-2020 Task 2: <fixed-case>COVID</fixed-case>-19 Informative Tweet Identification using Progressive Trained Language Models and Data Augmentation</title>
      <author><first>Vasudev</first><last>Awatramani</last></author>
      <author><first>Anupam</first><last>Kumar</last></author>
      <pages>414–418</pages>
      <abstract>Since the outbreak of COVID-19, there has been a surge of digital content on social media. The content ranges from news articles, academic reports, tweets, videos, and even memes. Among such an overabundance of data, it is crucial to distinguish which information is actually informative or merely sensational, redundant or false. This work focuses on developing such a language system that can differentiate between Informative or Uninformative tweets associated with COVID-19 for WNUT-2020 Shared Task 2. For this purpose, we employ deep transfer learning models such as BERT along other techniques such as Noisy Data Augmentation and Progress Training. The approach achieves a competitive F1-score of 0.8715 on the final testing dataset.</abstract>
      <url hash="b27b6b6b">2020.wnut-1.59</url>
      <doi>10.18653/v1/2020.wnut-1.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>NLPRL</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: <fixed-case>ELM</fixed-case>o-based System for Identification of <fixed-case>COVID</fixed-case>-19 Tweets</title>
      <author><first>Rajesh Kumar</first><last>Mundotiya</last></author>
      <author><first>Rupjyoti</first><last>Baruah</last></author>
      <author><first>Bhavana</first><last>Srivastava</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>419–422</pages>
      <abstract>The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on social media, such as Twitter, can help in prevention and taking precautions. This is an example of using noisy text processing for disaster management. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with accuracy as 80.85% and 78.54% as F1-score on the provided test dataset. The experimental code is available online.</abstract>
      <url hash="9dfb4503">2020.wnut-1.60</url>
      <doi>10.18653/v1/2020.wnut-1.60</doi>
    </paper>
    <paper id="61">
      <title><fixed-case>SU</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: The Ensemble Models</title>
      <author><first>Kenan</first><last>Fayoumi</last></author>
      <author><first>Reyyan</first><last>Yeniterzi</last></author>
      <pages>423–427</pages>
      <abstract>In this paper, we address the problem of identifying informative tweets related to COVID-19 in the form of a binary classification task as part of our submission for W-NUT 2020 Task 2. Specifically, we focus on ensembling methods to boost the classification performance of classification models such as BERT and CNN. We show that ensembling can reduce the variance in performance, specifically for BERT base models.</abstract>
      <url hash="01137cac">2020.wnut-1.61</url>
      <doi>10.18653/v1/2020.wnut-1.61</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>IDSOU</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Sora</first><last>Ohashi</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Noriko</first><last>Takemura</last></author>
      <author><first>Yuta</first><last>Nakashima</last></author>
      <author><first>Hajime</first><last>Nagahara</last></author>
      <pages>428–433</pages>
      <abstract>We introduce the IDSOU submission for the WNUT-2020 task 2: identification of informative COVID-19 English Tweets. Our system is an ensemble of pre-trained language models such as BERT. We ranked 16th in the F1 score.</abstract>
      <url hash="51814970">2020.wnut-1.62</url>
      <doi>10.18653/v1/2020.wnut-1.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>C</fixed-case>omplex<fixed-case>D</fixed-case>ata<fixed-case>L</fixed-case>ab at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Task 2: Detecting Informative <fixed-case>COVID</fixed-case>-19 Tweets by Attending over Linked Documents</title>
      <author><first>Kellin</first><last>Pelrine</last></author>
      <author><first>Jacob</first><last>Danovitch</last></author>
      <author><first>Albert Orozco</first><last>Camacho</last></author>
      <author><first>Reihaneh</first><last>Rabbany</last></author>
      <pages>434–439</pages>
      <abstract>Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as graph classification, drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.</abstract>
      <url hash="c1693783">2020.wnut-1.63</url>
      <doi>10.18653/v1/2020.wnut-1.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>NEU</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Data Augmentation To Tell <fixed-case>BERT</fixed-case> That Death Is Not Necessarily Informative</title>
      <author><first>Kumud</first><last>Chauhan</last></author>
      <pages>440–443</pages>
      <abstract>Millions of people around the world are sharing COVID-19 related information on social media platforms. Since not all the information shared on the social media is useful, a machine learning system to identify informative posts can help users in finding relevant information. In this paper, we present a BERT classifier system for W-NUT2020 Shared Task 2: Identification of Informative COVID-19 English Tweets. Further, we show that BERT exploits some easy signals to identify informative tweets, and adding simple patterns to uninformative tweets drastically degrades BERT performance. In particular, simply adding “10 deaths” to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also propose a simple data augmentation technique that helps in improving the robustness and generalization ability of the BERT classifier.</abstract>
      <url hash="91e8aa4c">2020.wnut-1.64</url>
      <doi>10.18653/v1/2020.wnut-1.64</doi>
    </paper>
    <paper id="65">
      <title><fixed-case>L</fixed-case>ynyrd<fixed-case>S</fixed-case>kynyrd at <fixed-case>WNUT</fixed-case>-2020 Task 2: Semi-Supervised Learning for Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <author><first>Kushal</first><last>Chawla</last></author>
      <author><first>Gaurav</first><last>Verma</last></author>
      <pages>444–449</pages>
      <abstract>In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.</abstract>
      <url hash="29e1d30b">2020.wnut-1.65</url>
      <doi>10.18653/v1/2020.wnut-1.65</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>NIT</fixed-case>_<fixed-case>COVID</fixed-case>-19 at <fixed-case>WNUT</fixed-case>-2020 Task 2: Deep Learning Model <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for Identify Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Jagadeesh</first><last>M S</last></author>
      <author><first>Alphonse</first><last>P J A</last></author>
      <pages>450–454</pages>
      <abstract>This paper presents the model submitted by NIT COVID-19 team for identified informative COVID-19 English tweets at WNUT-2020 Task2. This shared task addresses the problem of automatically identifying whether an English tweet related to informative (novel coronavirus) or not. These informative tweets provide information about recovered, confirmed, suspected, and death cases as well as location or travel history of the cases. The proposed approach includes pre-processing techniques and pre-trained RoBERTa with suitable hyperparameters for English coronavirus tweet classification. The performance achieved by the proposed model for shared task WNUT 2020 Task2 is 89.14% in the F1-score metric.</abstract>
      <url hash="0bddeb6a">2020.wnut-1.66</url>
      <doi>10.18653/v1/2020.wnut-1.66</doi>
    </paper>
    <paper id="67">
      <title><fixed-case>E</fixed-case>dinburgh<fixed-case>NLP</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Leveraging Transformers with Generalized Augmentation for Identifying Informativeness in <fixed-case>COVID</fixed-case>-19 Tweets</title>
      <author><first>Nickil</first><last>Maveli</last></author>
      <pages>455–461</pages>
      <abstract>Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (disaster relief organizations and news agencies) and therefore recognizing the informativeness of a tweet can help filter noise from large volumes of data. In this paper, we present our submission for WNUT-2020 Task 2: Identification of informative COVID-19 English Tweets. Our most successful model is an ensemble of transformers including RoBERTa, XLNet, and BERTweet trained in a Semi-Supervised Learning (SSL) setting. The proposed system achieves a F1 score of 0.9011 on the test set (ranking 7th on the leaderboard), and shows significant gains in performance compared to a baseline system using fasttext embeddings.</abstract>
      <url hash="1f941810">2020.wnut-1.67</url>
      <doi>10.18653/v1/2020.wnut-1.67</doi>
    </paper>
    <paper id="68">
      <title>#<fixed-case>GCDH</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: <fixed-case>BERT</fixed-case>-Based Models for the Detection of Informativeness in <fixed-case>E</fixed-case>nglish <fixed-case>COVID</fixed-case>-19 Related Tweets</title>
      <author><first>Hanna</first><last>Varachkina</last></author>
      <author><first>Stefan</first><last>Ziehe</last></author>
      <author><first>Tillmann</first><last>Dönicke</last></author>
      <author><first>Franziska</first><last>Pannach</last></author>
      <pages>462–465</pages>
      <abstract>In this system paper, we present a transformer-based approach to the detection of informativeness in English tweets on the topic of the current COVID-19 pandemic. Our models distinguish informative tweets, i.e. tweets containing statistics on recovery, suspected and confirmed cases and COVID-19 related deaths, from uninformative tweets. We present two transformer-based approaches as well as a Naive Bayes classifier and a support vector machine as baseline systems. The transformer models outperform the baselines by more than 0.1 in F1-score, with F1-scores of 0.9091 and 0.9036. Our models were submitted to the shared task Identification of informative COVID-19 English tweets WNUT-2020 Task 2.</abstract>
      <url hash="b14f4e79">2020.wnut-1.68</url>
      <doi>10.18653/v1/2020.wnut-1.68</doi>
    </paper>
    <paper id="69">
      <title>Not-<fixed-case>NUT</fixed-case>s at <fixed-case>WNUT</fixed-case>-2020 Task 2: A <fixed-case>BERT</fixed-case>-based System in Identifying Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets</title>
      <author><first>Thai</first><last>Hoang</last></author>
      <author><first>Phuong</first><last>Vu</last></author>
      <pages>466–470</pages>
      <abstract>As of 2020 when the COVID-19 pandemic is full-blown on a global scale, people’s need to have access to legitimate information regarding COVID-19 is more urgent than ever, especially via online media where the abundance of irrelevant information overshadows the more informative ones. In response to such, we proposed a model that, given an English tweet, automatically identifies whether that tweet bears informative content regarding COVID-19 or not. By ensembling different BERTweet model configurations, we have achieved competitive results that are only shy of those by top performing teams by roughly 1% in terms of F1 score on the informative class. In the post-competition period, we have also experimented with various other approaches that potentially boost generalization to a new dataset.</abstract>
      <url hash="1e710352">2020.wnut-1.69</url>
      <doi>10.18653/v1/2020.wnut-1.69</doi>
    </paper>
    <paper id="70">
      <title><fixed-case>CIA</fixed-case>_<fixed-case>NITT</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Classification of <fixed-case>COVID</fixed-case>-19 Tweets Using Pre-trained Language Models</title>
      <author><first>Yandrapati</first><last>Prakash Babu</last></author>
      <author><first>Rajagopal</first><last>Eswari</last></author>
      <pages>471–474</pages>
      <abstract>This paper presents our models for WNUT2020 shared task2. The shared task2 involves identification of COVID-19 related informative tweets. We treat this as binary text clas-sification problem and experiment with pre-trained language models. Our first model which is based on CT-BERT achieves F1-scoreof 88.7% and second model which is an ensemble of CT-BERT, RoBERTa and SVM achieves F1-score of 88.52%.</abstract>
      <url hash="7b4455d5">2020.wnut-1.70</url>
      <doi>10.18653/v1/2020.wnut-1.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>UET</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: A Study of Combining Transfer Learning Methods for Text Classification with <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Huy</first><last>Dao Quang</last></author>
      <author><first>Tam</first><last>Nguyen Minh</last></author>
      <pages>475–479</pages>
      <abstract>This paper reports our approach and the results of our experiments for W-NUT task 2: Identification of Informative COVID-19 English Tweets. In this paper, we test out the effectiveness of transfer learning method with state of the art language models as RoBERTa on this text classification task. Moreover, we examine the benefit of applying additional fine-tuning and training techniques including fine-tuning discrimination, gradual unfreezing as well as our custom head for the classifier. Our best model results in a high F1-score of 89.89 on the task’s private test dataset and that of 90.96 on public test set without ensembling multiple models and additional data.</abstract>
      <url hash="361e59e7">2020.wnut-1.71</url>
      <doi>10.18653/v1/2020.wnut-1.71</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>D</fixed-case>artmouth <fixed-case>CS</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Informative <fixed-case>COVID</fixed-case>-19 Tweet Classification Using <fixed-case>BERT</fixed-case></title>
      <author><first>Dylan</first><last>Whang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>480–484</pages>
      <abstract>We describe the systems developed for the WNUT-2020 shared task 2, identification of informative COVID-19 English Tweets. BERT is a highly performant model for Natural Language Processing tasks. We increased BERT’s performance in this classification task by fine-tuning BERT and concatenating its embeddings with Tweet-specific features and training a Support Vector Machine (SVM) for classification (henceforth called BERT+). We compared its performance to a suite of machine learning models. We used a Twitter specific data cleaning pipeline and word-level TF-IDF to extract features for the non-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.</abstract>
      <url hash="d4327b14">2020.wnut-1.72</url>
      <doi>10.18653/v1/2020.wnut-1.72</doi>
    </paper>
    <paper id="73">
      <title><fixed-case>S</fixed-case>un<fixed-case>B</fixed-case>ear at <fixed-case>WNUT</fixed-case>-2020 Task 2: Improving <fixed-case>BERT</fixed-case>-Based Noisy Text Classification with Knowledge of the Data domain</title>
      <author><first>Linh</first><last>Doan Bao</last></author>
      <author><first>Viet Anh</first><last>Nguyen</last></author>
      <author><first>Quang</first><last>Pham Huu</last></author>
      <pages>485–490</pages>
      <abstract>This paper proposes an improved custom model for WNUT task 2: Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of fine-tuning methodologies for state-of-the-art language model RoBERTa. We make a preliminary instantiation of this formal model for the text classification approaches. With appropriate training techniques, our model is able to achieve 0.9218 F1-score on public validation set and the ensemble version settles at top 9 F1-score (0.9005) and top 2 Recall (0.9301) on private test set.</abstract>
      <url hash="59778d2f">2020.wnut-1.73</url>
      <doi>10.18653/v1/2020.wnut-1.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>ISWARA</fixed-case> at <fixed-case>WNUT</fixed-case>-2020 Task 2: Identification of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>E</fixed-case>nglish Tweets using <fixed-case>BERT</fixed-case> and <fixed-case>F</fixed-case>ast<fixed-case>T</fixed-case>ext Embeddings</title>
      <author><first>Wava Carissa</first><last>Putri</last></author>
      <author><first>Rani Aulia</first><last>Hidayat</last></author>
      <author><first>Isnaini Nurul</first><last>Khasanah</last></author>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <pages>491–494</pages>
      <abstract>This paper presents Iswara’s participation in the WNUT-2020 Task 2 “Identification of Informative COVID-19 English Tweets using BERT and FastText Embeddings”,which tries to classify whether a certain tweet is considered informative or not. We proposed a method that utilizes word embeddings and using word occurrence related to the topic for this task. We compare several models to get the best performance. Results show that pairing BERT with word occurrences outperforms fastText with F1-Score, precision, recall, and accuracy on test data of 76%, 81%, 72%, and 79%, respectively</abstract>
      <url hash="eab0ea25">2020.wnut-1.74</url>
      <doi>10.18653/v1/2020.wnut-1.74</doi>
    </paper>
    <paper id="75">
      <title><fixed-case>COVCOR</fixed-case>20 at <fixed-case>WNUT</fixed-case>-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules</title>
      <author><first>Ali</first><last>Hürriyetoğlu</last></author>
      <author><first>Ali</first><last>Safaya</last></author>
      <author><first>Osman</first><last>Mutlu</last></author>
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Erdem</first><last>Yörük</last></author>
      <pages>495–498</pages>
      <abstract>In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of (the output of) the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.</abstract>
      <url hash="5ba041ce">2020.wnut-1.75</url>
      <doi>10.18653/v1/2020.wnut-1.75</doi>
    </paper>
    <paper id="76">
      <title><fixed-case>TEST</fixed-case>_<fixed-case>POSITIVE</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: Cross-task modeling</title>
      <author><first>Chacha</first><last>Chen</last></author>
      <author><first>Chieh-Yang</first><last>Huang</last></author>
      <author><first>Yaqi</first><last>Hou</last></author>
      <author><first>Yang</first><last>Shi</last></author>
      <author><first>Enyan</first><last>Dai</last></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <pages>499–504</pages>
      <abstract>The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions (e.g., Who is tested positive? What is the age of the person? Where is he/she?). To tackle these challenges, we propose the Joint Event Multi-task Learning (JOELIN) model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model. Moreover, we implement a type-aware post-processing procedure using named entity recognition (NER) to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2% in micro F1.</abstract>
      <url hash="54d58088">2020.wnut-1.76</url>
      <doi>10.18653/v1/2020.wnut-1.76</doi>
    </paper>
    <paper id="77">
      <title>imec-<fixed-case>ETRO</fixed-case>-<fixed-case>VUB</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: A multilabel <fixed-case>BERT</fixed-case>-based system for predicting <fixed-case>COVID</fixed-case>-19 events</title>
      <author><first>Xiangyu</first><last>Yang</last></author>
      <author><first>Giannis</first><last>Bekoulis</last></author>
      <author><first>Nikos</first><last>Deligiannis</last></author>
      <pages>505–513</pages>
      <abstract>In this paper, we present our system designed to address the W-NUT 2020 shared task for COVID-19 Event Extraction from Twitter. To mitigate the noisy nature of the Twitter stream, our system makes use of the COVID-Twitter-BERT (CT-BERT), which is a language model pre-trained on a large corpus of COVID-19 related Twitter messages. Our system is trained on the COVID-19 Twitter Event Corpus and is able to identify relevant text spans that answer pre-defined questions (i.e., slot types) for five COVID-19 related events (i.e., TESTED POSITIVE, TESTED NEGATIVE, CAN-NOT-TEST, DEATH and CURE &amp; PREVENTION). We have experimented with different architectures; our best performing model relies on a multilabel classifier on top of the CT-BERT model that jointly trains all the slot types for a single event. Our experimental results indicate that our Multilabel-CT-BERT system outperforms the baseline methods by 7 percentage points in terms of micro average F1 score. Our model ranked as 4th in the shared task leaderboard.</abstract>
      <url hash="485a153a">2020.wnut-1.77</url>
      <doi>10.18653/v1/2020.wnut-1.77</doi>
    </paper>
    <paper id="78">
      <title><fixed-case>UCD</fixed-case>-<fixed-case>CS</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: A Text to Text Approach for <fixed-case>COVID</fixed-case>-19 Event Extraction on Social Media</title>
      <author><first>Congcong</first><last>Wang</last></author>
      <author><first>David</first><last>Lillis</last></author>
      <pages>514–521</pages>
      <abstract>In this paper, we describe our approach in the shared task: COVID-19 event extraction from Twitter. The objective of this task is to extract answers from COVID-related tweets to a set of predefined slot-filling questions. Our approach treats the event extraction task as a question answering task by leveraging the transformer-based T5 text-to-text model. According to the official evaluation scores returned, namely F1, our submitted run achieves competitive performance compared to other participating runs (Top 3). However, we argue that this evaluation may underestimate the actual performance of runs based on text-generation. Although some such runs may answer the slot questions well, they may not be an exact string match for the gold standard answers. To measure the extent of this underestimation, we adopt a simple exact-answer transformation method aiming at converting the well-answered predictions to exactly-matched predictions. The results show that after this transformation our run overall reaches the same level of performance as the best participating run and state-of-the-art F1 scores in three of five COVID-related events. Our code is publicly available to aid reproducibility</abstract>
      <url hash="4435ff85">2020.wnut-1.78</url>
      <doi>10.18653/v1/2020.wnut-1.78</doi>
    </paper>
    <paper id="79">
      <title>Winners at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: Leveraging Event Specific and Chunk Span information for Extracting <fixed-case>COVID</fixed-case> Entities from Tweets</title>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <author><first>Tejas</first><last>Vaidhya</last></author>
      <pages>522–529</pages>
      <abstract>Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction of a variety of COVID-19 related events from Twitter, such as individuals who recently contracted the virus, someone with symptoms who were denied testing and believed remedies against the infection. The system consists of separate multi-task models for slot-filling subtasks and sentence-classification subtasks, while leveraging the useful sentence-level information for the corresponding event. The system uses COVID-Twitter-BERT with attention-weighted pooling of candidate slot-chunk features to capture the useful information chunks. The system ranks 1st at the leaderboard with F1 of 0.6598, without using any ensembles or additional datasets.</abstract>
      <url hash="96a3ef05">2020.wnut-1.79</url>
      <doi>10.18653/v1/2020.wnut-1.79</doi>
    </paper>
    <paper id="80">
      <title><fixed-case>HLTRI</fixed-case> at <fixed-case>W</fixed-case>-<fixed-case>NUT</fixed-case> 2020 Shared Task-3: <fixed-case>COVID</fixed-case>-19 Event Extraction from <fixed-case>T</fixed-case>witter Using Multi-Task Hopfield Pooling</title>
      <author><first>Maxwell</first><last>Weinzierl</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <pages>530–538</pages>
      <abstract>Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from Twitter has the potential to inform surveillance systems that play a critical role in public health. The event extraction challenge presented by the W-NUT 2020 Shared Task 3 focused on the identification of five types of events relevant to the COVID-19 pandemic and their respective set of pre-defined slots encoding demographic, epidemiological, clinical as well as spatial, temporal or subjective knowledge. Our participation in the challenge led to the design of a neural architecture for jointly identifying all Event Slots expressed in a tweet relevant to an event of interest. This architecture uses COVID-Twitter-BERT as the pre-trained language model. In addition, to learn text span embeddings for each Event Slot, we relied on a special case of Hopfield Networks, namely Hopfield pooling. The results of the shared task evaluation indicate that our system performs best when it is trained on a larger dataset, while it remains competitive when training on smaller datasets.</abstract>
      <url hash="86654687">2020.wnut-1.80</url>
      <doi>10.18653/v1/2020.wnut-1.80</doi>
    </paper>
  </volume>
</collection>
