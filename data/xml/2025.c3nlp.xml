<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.c3nlp">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Cross-Cultural Considerations in NLP (C3NLP 2025)</booktitle>
      <editor><first>Vinodkumar</first><last>Prabhakaran</last></editor>
      <editor><first>Sunipa</first><last>Dev</last></editor>
      <editor><first>Luciana</first><last>Benotti</last></editor>
      <editor><first>Daniel</first><last>Hershcovich</last></editor>
      <editor><first>Yong</first><last>Cao</last></editor>
      <editor><first>Li</first><last>Zhou</last></editor>
      <editor><first>Laura</first><last>Cabello</last></editor>
      <editor><first>Ife</first><last>Adebara</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="3ce2b569">2025.c3nlp-1</url>
      <venue>c3nlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-237-4</isbn>
    </meta>
    <frontmatter>
      <url hash="08aeb069">2025.c3nlp-1.0</url>
      <bibkey>c3nlp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>LLM</fixed-case> Alignment for the <fixed-case>A</fixed-case>rabs: A Homogenous Culture or Diverse Ones</title>
      <author><first>Amr</first><last>Keleg</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>1-9</pages>
      <abstract>Large Language Models (LLMs) have the potential of being a useful tool that can automate tasks, and assist humans. However, these models are more fluent in English and more aligned with Western cultures, norms, and values. Arabic-specific LLMs are being developed to better capture the nuances of the Arabic language, and the views of the Arabs. However, Arabs are sometimes assumed to share the same culture. In this position paper, we discuss the limitations of this assumption and provide our recommendations for how to curate better alignment data that models the cultural diversity within the Arab world.</abstract>
      <url hash="7c7aa7a8">2025.c3nlp-1.1</url>
      <bibkey>keleg-2025-llm</bibkey>
    </paper>
    <paper id="2">
      <title>Multi-Step Reasoning in <fixed-case>K</fixed-case>orean and the Emergent Mirage</title>
      <author><first>Guijin</first><last>Son</last></author>
      <author><first>Hyunwoo</first><last>Ko</last><affiliation>haerae.com and OnelineAI</affiliation></author>
      <author><first>Dasol</first><last>Choi</last><affiliation>Yonsei University</affiliation></author>
      <pages>10-21</pages>
      <url hash="fa4be485">2025.c3nlp-1.2</url>
      <bibkey>son-etal-2025-multi</bibkey>
    </paper>
    <paper id="3">
      <title>Fair Summarization: Bridging Quality and Diversity in Extractive Summaries</title>
      <author><first>Sina</first><last>Bagheri Nezhad</last><affiliation>Portland State University</affiliation></author>
      <author><first>Sayan</first><last>Bandyapadhyay</last><affiliation>Portland State University</affiliation></author>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <pages>22-34</pages>
      <abstract>Fairness in multi-document summarization of user-generated content remains a critical challenge in natural language processing (NLP). Existing summarization methods often fail to ensure equitable representation across different social groups, leading to biased outputs. In this paper, we introduce two novel methods for fair extractive summarization: FairExtract, a clustering-based approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints. We evaluate these methods using Divsumm summarization dataset of White-aligned, Hispanic, and African-American dialect tweets and compare them against relevant baselines. The results obtained using a comprehensive set of summarization quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well as a fairness metric F, demonstrate that FairExtract and FairGPT achieve superior fairness while maintaining competitive summarization quality. Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that integrate quality and fairness into a single evaluation framework, offering a more nuanced understanding of the trade-offs between these objectives. Our code is available online.</abstract>
      <url hash="5e35ed30">2025.c3nlp-1.3</url>
      <bibkey>bagheri-nezhad-etal-2025-fair</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>I</fixed-case>nsp<fixed-case>AI</fixed-case>red: Cross-cultural Inspiration Detection and Analysis in Real and <fixed-case>LLM</fixed-case>-generated Social Media Data</title>
      <author><first>Oana</first><last>Ignat</last><affiliation>Santa Clara University</affiliation></author>
      <author><first>Gayathri Ganesh</first><last>Lakshmy</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>35-49</pages>
      <abstract>Inspiration is linked to various positive outcomes, such as increased creativity, productivity, and happiness. Although inspiration has great potential, there has been limited effort toward identifying content that is inspiring, as opposed to just engaging or positive. Additionally, most research has concentrated on Western data, with little attention paid to other cultures. This work is the first to study cross-cultural inspiration through machine learning methods. We aim to identify and analyze real and AI-generated cross-cultural inspiring posts. To this end, we compile and make publicly available the InspAIred dataset, which consists of 2,000 real inspiring posts, 2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly distributed across India and the UK. The real posts are sourced from Reddit, while the generated posts are created using the GPT-4 model. Using this dataset, we conduct extensive computational linguistic analyses to (1) compare inspiring content across cultures, (2) compare AI-generated inspiring posts to real inspiring posts, and (3) determine if detection models can accurately distinguish between inspiring content across cultures and data sources.</abstract>
      <url hash="1bd7f580">2025.c3nlp-1.4</url>
      <bibkey>ignat-etal-2025-inspaired</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>D</fixed-case>a<fixed-case>K</fixed-case>ultur: Evaluating the Cultural Awareness of Language Models for <fixed-case>D</fixed-case>anish with Native Speakers</title>
      <author><first>Max</first><last>MÃ¼ller-Eberstein</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Peter Brunsgaard</first><last>Trolle</last></author>
      <author><first>Rob Van Der</first><last>Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>50-58</pages>
      <abstract>Large Language Models (LLMs) have seen widespread societal adoption. However, while they are able to interact with users in languages beyond English, they have been shown to lack cultural awareness, providing anglocentric or inappropriate responses for underrepresented language communities. To investigate this gap and disentangle linguistic versus cultural proficiency, we conduct the first cultural evaluation study for the mid-resource language of Danish, in which native speakers prompt different models to solve tasks requiring cultural awareness. Our analysis of the resulting 1,038 interactions from 63 demographically diverse participants highlights open challenges to cultural adaptation: Particularly, how currently employed automatically translated data are insufficient to train or measure cultural adaptation, and how training on native-speaker data can more than double response acceptance rates. We release our study data as DaKultur - the first native Danish cultural awareness dataset.</abstract>
      <url hash="445c579e">2025.c3nlp-1.5</url>
      <bibkey>muller-eberstein-etal-2025-dakultur</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>K</fixed-case>orean Stereotype Content Model: Translating Stereotypes Across Cultures</title>
      <author><first>Michelle YoungJin</first><last>Kim</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Kristen</first><last>Johnson</last><affiliation>Michigan State University</affiliation></author>
      <pages>59-70</pages>
      <abstract>To address bias in language models, researchers are leveraging established social psychology research on stereotyping. This interdisciplinary approach uses frameworks like the Stereotype Content Model (SCM) to understand how stereotypes about social groups are formed and perpetuated. The SCM posits that stereotypes are based on two dimensions: warmth (intent to harm) and competence (ability to harm). This framework has been applied in NLP for various tasks, including stereotype identification, bias mitigation, and hate speech detection. While the SCM has been extensively studied in English language models and Western cultural contexts, its applicability as a cross-cultural measure of stereotypes remains an open research question. This paper explores the cross-cultural validity of the SCM by developing a Korean Stereotype Content Model (KoSCM). We create a Korean warmth-competence lexicon through machine translation of existing English lexicons, validated by an expert translator, and utilize this lexicon to develop a labeled training dataset of Korean sentences. This work presents the first extension of SCM lexicons to a non-English language (Korean), aiming to broaden understanding of stereotypes and cultural dynamics.</abstract>
      <url hash="061af696">2025.c3nlp-1.6</url>
      <bibkey>kim-johnson-2025-korean</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>LLM</fixed-case>-<fixed-case>C</fixed-case>3<fixed-case>MOD</fixed-case>: A Human-<fixed-case>LLM</fixed-case> Collaborative System for Cross-Cultural Hate Speech Moderation</title>
      <author><first>Junyeong</first><last>Park</last></author>
      <author><first>Seogyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seyoung</first><last>Song</last><affiliation>KAIST</affiliation></author>
      <author><first>Yohan</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>71-88</pages>
      <abstract>Content moderation platforms concentrate resources on English content despite serving predominantly non-English speaking users.Also, given the scarcity of native moderators for low-resource languages, non-native moderators must bridge this gap in moderation tasks such as hate speech moderation.Through a user study, we identify that non-native moderators struggle with understanding culturally-specific knowledge, sentiment, and internet culture in the hate speech.To assist non-native moderators, we present LLM-C3MOD, a human-LLM collaborative pipeline with three steps: (1) RAG-enhanced cultural context annotations; (2) initial LLM-based moderation; and (3) targeted human moderation for cases lacking LLM consensus.Evaluated on Korean hate speech dataset with Indonesian and German participants, our system achieves 78% accuracy (surpassing GPT-4oâs 71% baseline) while reducing human workload by 83.6%.In addition, cultural context annotations improved non-native moderator accuracy from 22% to 61%, with humans notably excelling at nuanced tasks where LLMs struggle.Our findings demonstrate that non-native moderators, when properly supported by LLMs, can effectively contribute to cross-cultural hate speech moderation.</abstract>
      <url hash="39f57874">2025.c3nlp-1.7</url>
      <bibkey>park-etal-2025-llm</bibkey>
    </paper>
    <paper id="8">
      <title>One world, one opinion? The superstar effect in <fixed-case>LLM</fixed-case> responses</title>
      <author><first>Sofie</first><last>Goethals</last></author>
      <author><first>Lauren</first><last>Rhue</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>89-107</pages>
      <abstract>As large language models (LLMs) are shaping the way information is shared and accessed online, their opinions have the potential to influence a wide audience. This study examines who is predicted by the studied LLMs as the most prominent figures across various fields, while using prompts in ten different languages to explore the influence of linguistic diversity. Our findings reveal low diversity in responses, with a small number of figures dominating recognition across languages (also known as the âsuperstar effectâ). These results highlight the risk of narrowing global knowledge representation when LLMs are used to retrieve subjective information.</abstract>
      <url hash="dd7c5290">2025.c3nlp-1.8</url>
      <bibkey>goethals-rhue-2025-one</bibkey>
    </paper>
    <paper id="9">
      <title>Towards Region-aware Bias Evaluation Metrics</title>
      <author><first>Angana</first><last>Borah</last></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>108-131</pages>
      <abstract>When exposed to human-generated data, language models are known to learn and amplify societal biases. While previous works introduced metrics that can be used to assess the bias in these models, they rely on assumptions that may not be universally true. For instance, a gender bias dimension commonly used by these metrics is that of familyâcareer, but this may not be the only common bias in certain regions of the world. In this paper, we identify topical differences in gender bias across different regions and propose a region-aware bottom-up approach for bias assessment. Several of our proposed region-aware gender bias dimensions are found to be aligned with the human perception of gender biases in these regions.</abstract>
      <url hash="9918b89a">2025.c3nlp-1.9</url>
      <bibkey>borah-etal-2025-towards</bibkey>
    </paper>
    <paper id="10">
      <title>Cross-Cultural Differences in Mental Health Expressions on Social Media</title>
      <author><first>Sunny</first><last>Rai</last><affiliation>School of Engineering and Applied Science, University of Pennsylvania</affiliation></author>
      <author><first>Khushi</first><last>Shelat</last></author>
      <author><first>Devansh</first><last>Jain</last></author>
      <author><first>Ashwin</first><last>Kishen</last><affiliation>NA</affiliation></author>
      <author><first>Young Min</first><last>Cho</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Maitreyi</first><last>Redkar</last></author>
      <author><first>Samindara</first><last>Hardikar-Sawant</last><affiliation>Shri Jagdishprasad Jhabarmal Tibrewala University</affiliation></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>132-142</pages>
      <abstract>Culture moderates the way individuals perceive and express mental distress. Current understandings of mental health expressions on social media, however, are predominantly derived from WEIRD (Western, Educated, Industrialized, Rich, and Democratic) contexts. To address this gap, we examine mental health posts on Reddit made by individuals geolocated in India, to identify variations in social media language specific to the Indian context compared to users from Western nations. Our experiments reveal significant psychosocial variations in emotions and temporal orientation. This study demonstrates the potential of social media platforms for identifying cross-cultural differences in mental health expressions (e.g. seeking advice in India vs seeking support by Western users). Significant linguistic variations in online mental health-related language emphasize the importance of developing precision-targeted interventions that are culturally appropriate.</abstract>
      <url hash="5a019f51">2025.c3nlp-1.10</url>
      <bibkey>rai-etal-2025-cross</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>WHEN</fixed-case> <fixed-case>TOM</fixed-case> <fixed-case>EATS</fixed-case> <fixed-case>KIMCHI</fixed-case>: Evaluating Cultural Awareness of Multimodal Large Language Models in Cultural Mixture Contexts</title>
      <author><first>Jun Seong</first><last>Kim</last></author>
      <author><first>Kyaw Ye</first><last>Thu</last></author>
      <author><first>Javad</first><last>Ismayilzada</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Junyeong</first><last>Park</last></author>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Huzama</first><last>Ahmad</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Na Min</first><last>An</last><affiliation>KAIST</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>143-154</pages>
      <abstract>In a highly globalized world, it is important for multi-modal large language models (MLLMs) to recognize and respond correctly to mixed-cultural inputs.For example, a model should correctly identify kimchi (Korean food) in an image both when an Asian woman is eating it, as well as an African man is eating it.However, current MLLMs show an over-reliance on the visual features of the person, leading to misclassification of the entities. To examine the robustness of MLLMs to different ethnicity, we introduce MIXCUBE, a cross-cultural bias benchmark, and study elements from five countries and four ethnicities. Our findings reveal that MLLMs achieve both higher accuracy and lower sensitivity to such perturbation for high-resource cultures, but not for low-resource cultures. GPT-4o, the best-performing model overall, shows up to 58% difference in accuracy between the original and perturbed cultural settings in low-resource cultures</abstract>
      <url hash="d78640b2">2025.c3nlp-1.11</url>
      <bibkey>kim-etal-2025-tom</bibkey>
    </paper>
  </volume>
</collection>
