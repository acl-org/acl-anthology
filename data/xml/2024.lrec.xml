<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.lrec">
  <volume id="main" ingest-date="2024-05-17" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</booktitle>
      <editor><first>Nicoletta</first><last>Calzolari</last></editor>
      <editor><first>Min-Yen</first><last>Kan</last></editor>
      <editor><first>Veronique</first><last>Hoste</last></editor>
      <editor><first>Alessandro</first><last>Lenci</last></editor>
      <editor><first>Sakriani</first><last>Sakti</last></editor>
      <editor><first>Nianwen</first><last>Xue</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="bf2544a5">2024.lrec-main</url>
      <venue>lrec</venue>
      <venue>coling</venue>
    </meta>
    <frontmatter>
      <url hash="e3949820">2024.lrec-main.0</url>
      <bibkey>lrec-2024-2024</bibkey>
    </frontmatter>
    <paper id="1">
      <title>3<fixed-case>AM</fixed-case>: An Ambiguity-Aware Multi-Modal Machine Translation Dataset</title>
      <author><first>Xinyu</first><last>Ma</last></author>
      <author><first>Xuebo</first><last>Liu</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Jun</first><last>Rao</last></author>
      <author><first>Bei</first><last>Li</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>1–13</pages>
      <abstract>Multimodal machine translation (MMT) is a challenging task that seeks to improve translation quality by incorporating visual information. However, recent studies have indicated that the visual information provided by existing MMT datasets is insufficient, causing models to disregard it and overestimate their capabilities. This issue presents a significant obstacle to the development of MMT research. This paper presents a novel solution to this issue by introducing 3AM, an ambiguity-aware MMT dataset comprising 26,000 parallel sentence pairs in English and Chinese, each with corresponding images. Our dataset is specifically designed to include more ambiguity and a greater variety of both captions and images than other MMT datasets. We utilize a word sense disambiguation model to select ambiguous data from vision-and-language datasets, resulting in a more challenging dataset. We further benchmark several state-of-the-art MMT models on our proposed dataset. Experimental results show that MMT models trained on our dataset exhibit a greater ability to exploit visual information than those trained on other MMT datasets. Our work provides a valuable resource for researchers in the field of multimodal learning and encourages further exploration in this area. The data, code and scripts are freely available at https://github.com/MaxyLee/3AM.</abstract>
      <url hash="580ff9a3">2024.lrec-main.1</url>
      <bibkey>ma-etal-2024-3am-ambiguity</bibkey>
    </paper>
    <paper id="2">
      <title>A Benchmark Evaluation of Clinical Named Entity Recognition in <fixed-case>F</fixed-case>rench</title>
      <author><first>Nesrine</first><last>Bannour</last></author>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <pages>14–21</pages>
      <abstract>Background: Transformer-based language models have shown strong performance on many Natural Language Processing (NLP) tasks. Masked Language Models (MLMs) attract sustained interest because they can be adapted to different languages and sub-domains through training or fine-tuning on specific corpora while remaining lighter than modern Large Language Models (MLMs). Recently, several MLMs have been released for the biomedical domain in French, and experiments suggest that they outperform standard French counterparts. However, no systematic evaluation comparing all models on the same corpora is available. Objective: This paper presents an evaluation of masked language models for biomedical French on the task of clinical named entity recognition. Material and methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare them to standard French models CamemBERT, FlauBERT and FrAlBERT as well as multilingual mBERT using three publically available corpora for clinical named entity recognition in French. The evaluation set-up relies on gold-standard corpora as released by the corpus developers. Results: Results suggest that CamemBERT-bio outperforms DrBERT consistently while FlauBERT offers competitive performance and FrAlBERT achieves the lowest carbon footprint. Conclusion: This is the first benchmark evaluation of biomedical masked language models for French clinical entity recognition that compares model performance consistently on nested entity recognition using metrics covering performance and environmental impact.</abstract>
      <url hash="f4d884d7">2024.lrec-main.2</url>
      <bibkey>bannour-etal-2024-benchmark-evaluation</bibkey>
    </paper>
    <paper id="3">
      <title>A Benchmark for Recipe Understanding in Artificial Agents</title>
      <author><first>Jens</first><last>Nevens</last></author>
      <author><first>Robin</first><last>de Haes</last></author>
      <author><first>Rachel</first><last>Ringe</last></author>
      <author><first>Mihai</first><last>Pomarlan</last></author>
      <author><first>Robert</first><last>Porzel</last></author>
      <author><first>Katrien</first><last>Beuls</last></author>
      <author><first>Paul</first><last>van Eecke</last></author>
      <pages>22–42</pages>
      <abstract>This paper introduces a novel benchmark that has been designed as a test bed for evaluating whether artificial agents are able to understand how to perform everyday activities, with a focus on the cooking domain. Understanding how to cook recipes is a highly challenging endeavour due to the underspecified and grounded nature of recipe texts, combined with the fact that recipe execution is a knowledge-intensive and precise activity. The benchmark comprises a corpus of recipes, a procedural semantic representation language of cooking actions, qualitative and quantitative kitchen simulators, and a standardised evaluation procedure. Concretely, the benchmark task consists in mapping a recipe formulated in natural language to a set of cooking actions that is precise enough to be executed in the simulated kitchen and yields the desired dish. To overcome the challenges inherent to recipe execution, this mapping process needs to incorporate reasoning over the recipe text, the state of the simulated kitchen environment, common-sense knowledge, knowledge of the cooking domain, and the action space of a virtual or robotic chef. This benchmark thereby addresses the growing interest in human-centric systems that combine natural language processing and situated reasoning to perform everyday activities.</abstract>
      <url hash="fb767de8">2024.lrec-main.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="76b573a8">2024.lrec-main.3.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>nevens-etal-2024-benchmark-recipe</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>ABLE</fixed-case>: Agency-<fixed-case>B</fixed-case>e<fixed-case>L</fixed-case>iefs Embedding to Address Stereotypical Bias through Awareness Instead of Obliviousness</title>
      <author><first>Michelle YoungJin</first><last>Kim</last></author>
      <author><first>Junghwan</first><last>Kim</last></author>
      <author><first>Kristen</first><last>Johnson</last></author>
      <pages>43–56</pages>
      <abstract>Natural Language Processing (NLP) models tend to inherit and amplify stereotypical biases present in their training data, leading to harmful societal consequences. Current efforts to rectify these biases typically revolve around making models oblivious to bias, which is at odds with the idea that humans require increased awareness to tackle these biases better. This prompts a fundamental research question: are bias-oblivious models the only viable solution to combat stereotypical biases? This paper answers this question by proposing the Agency-BeLiefs Embedding (ABLE) model, a novel approach that actively encodes stereotypical biases into the embedding space. ABLE draws upon social psychological theory to acquire and represent stereotypical biases in the form of agency and belief scores rather than directly representing stereotyped groups. Our experimental results showcase ABLE’s effectiveness in learning agency and belief stereotypes while preserving the language model’s proficiency. Furthermore, we underscore the practical significance of incorporating stereotypes within the ABLE model by demonstrating its utility in various downstream tasks. Our approach exemplifies the potential benefits of addressing bias through awareness, as opposed to the prevailing approach of mitigating bias through obliviousness.</abstract>
      <url hash="c457a9ea">2024.lrec-main.4</url>
      <bibkey>kim-etal-2024-able-agency</bibkey>
    </paper>
    <paper id="5">
      <title>Abstractive Multi-Video Captioning: Benchmark Dataset Construction and Extensive Evaluation</title>
      <author><first>Rikito</first><last>Takahashi</last></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>57–69</pages>
      <abstract>This paper introduces a new task, abstractive multi-video captioning, which focuses on abstracting multiple videos with natural language. Unlike conventional video captioning tasks generating a specific caption for a video, our task generates an abstract caption of the shared content in a video group containing multiple videos. To address our task, models must learn to understand each video in detail and have strong abstraction abilities to find commonalities among videos. We construct a benchmark dataset for abstractive multi-video captioning named AbstrActs. AbstrActs contains 13.5k video groups and corresponding abstract captions. As abstractive multi-video captioning models, we explore two approaches: end-to-end and cascade. For evaluation, we proposed a new metric, CocoA, which can evaluate the model performance based on the abstractness of the generated captions. In experiments, we report the impact of the way of combining multiple video features, the overall model architecture, and the number of input videos.</abstract>
      <url hash="ceeaee36">2024.lrec-main.5</url>
      <bibkey>takahashi-etal-2024-abstractive-multi</bibkey>
    </paper>
    <paper id="6">
      <title>Abstract-level Deductive Reasoning for Pre-trained Language Models</title>
      <author><first>Xin</first><last>Wu</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <author><first>Ho-fung</first><last>Leung</last></author>
      <pages>70–76</pages>
      <abstract>Pre-trained Language Models have been shown to be able to emulate deductive reasoning in natural language. However, PLMs are easily affected by irrelevant information (e.g., entity) in instance-level proofs when learning deductive reasoning. To address this limitation, we propose an Abstract-level Deductive Reasoner (ADR). ADR is trained to predict the abstract reasoning proof of each sample, which guides PLMs to learn general reasoning patterns rather than instance-level knowledge. Experimental results demonstrate that ADR significantly reduces the impact of PLMs learning instance-level knowledge (over 70%).</abstract>
      <url hash="ec44ce5c">2024.lrec-main.6</url>
      <bibkey>wu-etal-2024-abstract-level</bibkey>
    </paper>
    <paper id="7">
      <title>A Call for Clarity in Beam Search: How It Works and When It Stops</title>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>77–90</pages>
      <abstract>Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any implementation. Further, we find that different versions of beam decoding result in large performance differences in summarization, demonstrating the need for clarity in specifying the beam search implementation in research work. Our code will be available upon publication.</abstract>
      <url hash="33231929">2024.lrec-main.7</url>
      <bibkey>kasai-etal-2024-call-clarity</bibkey>
    </paper>
    <paper id="8">
      <title>A Canonical Form for Flexible Multiword Expressions</title>
      <author><first>Jan</first><last>Odijk</last></author>
      <author><first>Martin</first><last>Kroon</last></author>
      <pages>91–101</pages>
      <abstract>This paper proposes a canonical form for Multiword Expressions (MWEs), in particular for the Dutch language. The canonical form can be enriched with all kinds of annotations that can be used to describe the properties of the MWE and its components. It also introduces the DUCAME (DUtch CAnonical Multiword Expressions) lexical resource with more than 11k MWEs in canonical form. DUCAME is used in MWE-Finder to automatically generate queries for searching for flexible MWEs in large text corpora.</abstract>
      <url hash="ba3cc982">2024.lrec-main.8</url>
      <bibkey>odijk-2024-canonical-form</bibkey>
    </paper>
    <paper id="9">
      <title>A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation</title>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Xiaohan</first><last>Zhang</last></author>
      <author><first>Yifan</first><last>Xu</last></author>
      <author><first>Xuanyu</first><last>Lei</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>102–112</pages>
      <abstract>Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the &lt;b&gt;hallucination&lt;/b&gt; problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.</abstract>
      <url hash="85fcd9d0">2024.lrec-main.9</url>
      <bibkey>yu-etal-2024-cause-effect</bibkey>
    </paper>
    <paper id="10">
      <title>Access Control Framework for Language Collections</title>
      <author><first>Ben</first><last>Foley</last></author>
      <author><first>Peter</first><last>Sefton</last></author>
      <author><first>Simon</first><last>Musgrave</last></author>
      <author><first>Moises</first><last>Sacal Bonequi</last></author>
      <pages>113–121</pages>
      <abstract>This paper introduces the licence-based access control framework developed by the Language Data Commons of Australia (LDaCA) for a range of language collections, with examples given of implementation for significant Indigenous and Australian English collections. Language collections may be curated for many reasons, such as documentation for language revival, for research, security or commercial purposes. Some language collections are created with the intention of being “Open Access”; publicly available with no restriction. Other collections require that access be limited to individuals or groups of people, either at the collection level or at the level of individual items, such as a recording. To facilitate access, while respecting the intended access conditions for a collection, or collection items, some form of user identification and authorisation process is typically required. The access control framework described in this paper is based upon descriptions of access conditions in easy-to-read licences which are stored alongside data files in the collections; and is implemented using identity-based authentication and authorisation systems where required. The framework accommodates accessibility needs from unrestricted to extremely limited access, is dynamic, and able to be modified in response to changes in access needs. Storing licences with the data is a significant development in separating language data and access requirements from access infrastructure.</abstract>
      <url hash="ffb28216">2024.lrec-main.10</url>
      <bibkey>foley-etal-2024-access-control</bibkey>
    </paper>
    <paper id="11">
      <title>A Challenge Dataset and Effective Models for Conversational Stance Detection</title>
      <author><first>Fuqiang</first><last>Niu</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Baoquan</first><last>Zhang</last></author>
      <author><first>Xiaojiang</first><last>Peng</last></author>
      <author><first>Bowen</first><last>Zhang</last></author>
      <pages>122–132</pages>
      <abstract>Previous stance detection studies typically concentrate on evaluating stances within individual instances, thereby exhibiting limitations in effectively modeling multi-party discussions concerning the same specific topic, as naturally transpire in authentic social media interactions. This constraint arises primarily due to the scarcity of datasets that authentically replicate real social media contexts, hindering the research progress of conversational stance detection. In this paper, we introduce a new multi-turn conversation stance detection dataset (called <b>MT-CSD</b>), which encompasses multiple targets for conversational stance detection. To derive stances from this challenging dataset, we propose a global-local attention network (<b>GLAN</b>) to address both long and short-range dependencies inherent in conversational data. Notably, even state-of-the-art stance detection methods, exemplified by GLAN, exhibit an accuracy of only 50.47%, highlighting the persistent challenges in conversational stance detection. Furthermore, our MT-CSD dataset serves as a valuable resource to catalyze advancements in cross-domain stance detection, where a classifier is adapted from a different yet related target. We believe that MT-CSD will contribute to advancing real-world applications of stance detection research. Our source code, data, and models are available at <url>https://github.com/nfq729/MT-CSD</url>.</abstract>
      <url hash="f1a93712">2024.lrec-main.11</url>
      <bibkey>niu-etal-2024-challenge-dataset</bibkey>
    </paper>
    <paper id="12">
      <title>A Closer Look at Clustering Bilingual Comparable Corpora</title>
      <author><first>Anna</first><last>Laskina</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <author><first>Gaelle</first><last>Calvary</last></author>
      <pages>133–142</pages>
      <abstract>We study in this paper the problem of clustering comparable corpora, building upon the observation that different types of clusters can be present in such corpora: monolingual clusters comprising documents in a single language, and bilingual or multilingual clusters comprising documents written in different languages. Based on a state-of-the-art deep variant of Kmeans, we propose new clustering models fully adapted to comparable corpora and illustrate their behavior on several bilingual collections (in English, French, German and Russian) created from Wikipedia.</abstract>
      <url hash="4023447f">2024.lrec-main.12</url>
      <bibkey>laskina-etal-2024-closer-look</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>A</fixed-case>cn<fixed-case>E</fixed-case>mpathize: A Dataset for Understanding Empathy in Dermatology Conversations</title>
      <author><first>Gyeongeun</first><last>Lee</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>143–153</pages>
      <abstract>Empathy is critical for effective communication and mental health support, and in many online health communities people anonymously engage in conversations to seek and provide empathetic support. The ability to automatically recognize and detect empathy contributes to the understanding of human emotions expressed in text, therefore advancing natural language understanding across various domains. Existing empathy and mental health-related corpora focus on broader contexts and lack domain specificity, but similarly to other tasks (e.g., learning distinct patterns associated with COVID-19 versus skin allergies in clinical notes), observing empathy within different domains is crucial to providing tailored support. To address this need, we introduce AcnEmpathize, a dataset that captures empathy expressed in acne-related discussions from forum posts focused on its emotional and psychological effects. We find that transformer-based models trained on our dataset demonstrate excellent performance at empathy classification. Our dataset is publicly released to facilitate analysis of domain-specific empathy in online conversations and advance research in this challenging and intriguing domain.</abstract>
      <url hash="4bd30853">2024.lrec-main.13</url>
      <bibkey>lee-parde-2024-acnempathize-dataset</bibkey>
    </paper>
    <paper id="14">
      <title>A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances</title>
      <author><first>Nigel</first><last>Ward</last></author>
      <author><first>Divette</first><last>Marco</last></author>
      <pages>154–163</pages>
      <abstract>Automatic measures of similarity between sentences or utterances are invaluable for training speech synthesizers, evaluating machine translation, and assessing learner productions. While there exist measures for semantic similarity and prosodic similarity, there are as yet none for pragmatic similarity. To enable the training of such measures, we developed the first collection of human judgments of pragmatic similarity between utterance pairs. 9 judges listened to 220 utterance pairs, each consisting of an utterance extracted from a recorded dialog and a re-enactment of that utterance under various conditions designed to create various degrees of similarity. Each pair was rated on a continuous scale. The average inter-judge correlation was 0.45. We make this data available at https://github.com/divettemarco/PragSim .</abstract>
      <url hash="59749af2">2024.lrec-main.14</url>
      <bibkey>ward-marco-2024-collection-pragmatic</bibkey>
    </paper>
    <paper id="15">
      <title>A Community-Driven Data-to-Text Platform for Football Match Summaries</title>
      <author><first>Pedro</first><last>Fernandes</last></author>
      <author><first>Sérgio</first><last>Nunes</last></author>
      <author><first>Luís</first><last>Santos</last></author>
      <pages>164–173</pages>
      <abstract>Data-to-text systems offer a transformative approach to generating textual content in data-rich environments. This paper describes the architecture and deployment of Prosebot, a community-driven data-to-text platform tailored for generating textual summaries of football matches derived from match statistics. The system enhances the visibility of lower-tier matches, traditionally accessible only through data tables. Prosebot uses a template-based Natural Language Generation (NLG) module to generate initial drafts, which are subsequently refined by the reading community. Comprehensive evaluations, encompassing both human-mediated and automated assessments, were conducted to assess the system’s efficacy. Analysis of the community-edited texts reveals that significant segments of the initial automated drafts are retained, suggesting their high quality and acceptance by the collaborators. Preliminary surveys conducted among platform users highlight a predominantly positive reception within the community.</abstract>
      <url hash="6b8b6f25">2024.lrec-main.15</url>
      <bibkey>fernandes-etal-2024-community-driven</bibkey>
    </paper>
    <paper id="16">
      <title>A Comparative Analysis of Word-Level Metric Differential Privacy: Benchmarking the Privacy-Utility Trade-off</title>
      <author><first>Stephen</first><last>Meisenbacher</last></author>
      <author><first>Nihildev</first><last>Nandakumar</last></author>
      <author><first>Alexandra</first><last>Klymenko</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>174–185</pages>
      <abstract>The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the *word-level*, where calibrated noise is added to word embedding vectors to achieve “noisy” representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy. Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the *epsilon* parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field.</abstract>
      <url hash="5c424678">2024.lrec-main.16</url>
      <bibkey>meisenbacher-etal-2024-comparative-analysis</bibkey>
    </paper>
    <paper id="17">
      <title>A Comparative Study of Explicit and Implicit Gender Biases in Large Language Models via Self-evaluation</title>
      <author><first>Yachao</first><last>Zhao</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Dongming</first><last>Zhao</last></author>
      <author><first>Xiaojia</first><last>Jin</last></author>
      <author><first>Jijun</first><last>Zhang</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <pages>186–198</pages>
      <abstract>While extensive work has examined the explicit and implicit biases in large language models (LLMs), little research explores the relation between these two types of biases. This paper presents a comparative study of the explicit and implicit biases in LLMs grounded in social psychology. Social psychology distinguishes between explicit and implicit biases by whether the bias can be self-recognized by individuals. Aligning with this conceptualization, we propose a self-evaluation-based two-stage measurement of explicit and implicit biases within LLMs. First, the LLM is prompted to automatically fill templates with social targets to measure implicit bias toward these targets, where the bias is less likely to be self-recognized by the LLM. Then, the LLM is prompted to self-evaluate the templates filled by itself to measure explicit bias toward the same targets, where the bias is more likely to be self-recognized by the LLM. Experiments conducted on state-of-the-art LLMs reveal human-like inconsistency between explicit and implicit occupational gender biases. This work bridges a critical gap where prior studies concentrate solely on either explicit or implicit bias. We advocate that future work highlight the relation between explicit and implicit biases in LLMs.</abstract>
      <url hash="074996ef">2024.lrec-main.17</url>
      <bibkey>zhao-etal-2024-comparative-study</bibkey>
    </paper>
    <paper id="18">
      <title>A Computational Analysis of the Dehumanisation of Migrants from Syria and <fixed-case>U</fixed-case>kraine in <fixed-case>S</fixed-case>lovene News Media</title>
      <author><first>Jaya</first><last>Caporusso</last></author>
      <author><first>Damar</first><last>Hoogland</last></author>
      <author><first>Mojca</first><last>Brglez</last></author>
      <author><first>Boshko</first><last>Koloski</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>199–210</pages>
      <abstract>Dehumanisation involves the perception and/or treatment of a social group’s members as less than human. This phenomenon is rarely addressed with computational linguistic techniques. We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of zero-shot cross-lingual valence and arousal detection, and a new method for statistical significance testing. We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine. We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others.</abstract>
      <url hash="1e289a1e">2024.lrec-main.18</url>
      <bibkey>caporusso-etal-2024-computational-analysis</bibkey>
    </paper>
    <paper id="19">
      <title>A Computational Approach to Quantifying Grammaticization of <fixed-case>E</fixed-case>nglish Deverbal Prepositions</title>
      <author><first>Ryo</first><last>Nagata</last></author>
      <author><first>Yoshifumi</first><last>Kawasaki</last></author>
      <author><first>Naoki</first><last>Otani</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>211–220</pages>
      <abstract>This paper explores grammaticization of deverbal prepositions by a computational approach based on corpus data. Deverbal prepositions are words or phrases that are derived from a verb and that behave as a preposition such as “regarding” and “according to”. Linguistic studies have revealed important aspects of grammaticization of deverbal prepositions. This paper augments them by methods for measuring the degree of grammaticization of deverbal prepositions based on non-contextualized or contextualized word vectors. Experiments show that the methods correlate well with human judgements (as high as 0.69 in Spearman’s rank correlation coefficient). Using the best-performing method, this paper further shows that the methods support previous findings in linguistics including (i) Deverbal prepositions are marginal in terms of prepositionality; and (ii) The process where verbs are grammaticized into prepositions is gradual. As a pilot study, it also conducts a diachronic analysis of grammaticization of deverbal preposition.</abstract>
      <url hash="019cf8e4">2024.lrec-main.19</url>
      <bibkey>nagata-etal-2024-computational-approach</bibkey>
    </paper>
    <paper id="20">
      <title>A Computational Model of <fixed-case>L</fixed-case>atvian Morphology</title>
      <author><first>Peteris</first><last>Paikens</last></author>
      <author><first>Lauma</first><last>Pretkalniņa</last></author>
      <author><first>Laura</first><last>Rituma</last></author>
      <pages>221–232</pages>
      <abstract>In this paper we describe a computational model of Latvian morphology that provides a formal structure for Latvian word form inflection and has been implemented in software for generation, analysis and lemmatization of Latvian word forms. The work was motivated by the need for a NLP inflection model that can cover all the complexity of Latvian language and explicitly enumerate and handle the many exceptions to the general Latvian inflection principles. This is an evolution of earlier work, extending the initial proof of concept model to properly cover Latvian language. We provide a set of morphological paradigms that differ from current linguistic tradition, a set of systematic stem changes and combine it with an extensive lexicon that includes paradigm information and structured morphological attributes for 118 000 lexemes. This model has been applied on both dictionary and corpora data, demonstrating that it provides a good coverage for modern Latvian literary language. We also consider that there is a good potential to extend this also to the related Latgalian language.</abstract>
      <url hash="7b43e3d9">2024.lrec-main.20</url>
      <bibkey>paikens-etal-2024-computational-model</bibkey>
    </paper>
    <paper id="21">
      <title>A Concept Based Approach for Translation of Medical Dialogues into Pictographs</title>
      <author><first>Johanna</first><last>Gerlach</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Jonathan</first><last>Mutal</last></author>
      <author><first>Hervé</first><last>Spechbach</last></author>
      <pages>233–242</pages>
      <abstract>Pictographs have been found to improve patient comprehension of medical information or instructions. However, tools to produce pictograph representations from natural language are still scarce. In this contribution we describe a system that automatically translates French speech into pictographs to enable diagnostic interviews in emergency settings, thereby providing a tool to overcome the language barrier or provide support in Augmentative and Alternative Communication (AAC) contexts. Our approach is based on a semantic gloss that serves as pivot between spontaneous language and pictographs, with medical concepts represented using the UMLS ontology. In this study we evaluate different available pre-trained models fine-tuned on artificial data to translate French into this semantic gloss. On unseen data collected in real settings, consisting of questions and instructions by physicians, the best model achieves an F0.5 score of 86.7. A complementary human evaluation of the semantic glosses differing from the reference shows that 71% of these would be usable to transmit the intended meaning. Finally, a human evaluation of the pictograph sequences derived from the gloss reveals very few additions, omissions or order issues (&lt;3%), suggesting that the gloss as designed is well suited as a pivot for translation into pictographs.</abstract>
      <url hash="a5087e51">2024.lrec-main.21</url>
      <bibkey>gerlach-etal-2024-concept-based</bibkey>
    </paper>
    <paper id="22">
      <title>A Construction Grammar Corpus of Varying Schematicity: A Dataset for the Evaluation of Abstractions in Language Models</title>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <pages>243–255</pages>
      <abstract>Large Language Models (LLMs) have been developed without a theoretical framework, yet we posit that evaluating and improving LLMs will benefit from the development of theoretical frameworks that enable comparison of the structures of human language and the model of language built up by LLMs through the processing of text. In service of this goal, we develop the Construction Grammar Schematicity (“CoGS”) corpus of 10 distinct English constructions, where the constructions vary with respect to schematicity, or in other words the level to which constructional slots require specific, fixed lexical items, or can be filled with a variety of elements that fulfill a particular semantic role of the slot. Our corpus constructions are carefully curated to range from substantive, frozen constructions (e.g., Let-alone) to entirely schematic constructions (e.g., Resultative). The corpus was collected to allow us to probe LLMs for constructional information at varying levels of abstraction. We present our own probing experiments using this corpus, which clearly demonstrate that even the largest LLMs are limited to more substantive constructions and do not exhibit recognition of the similarity of purely schematic constructions. We publicly release our dataset, prompts, and associated model responses.</abstract>
      <url hash="1ec8314d">2024.lrec-main.22</url>
      <bibkey>bonial-tayyar-madabushi-2024-construction-grammar</bibkey>
    </paper>
    <paper id="23">
      <title>A Controlled Reevaluation of Coreference Resolution Models</title>
      <author><first>Ian</first><last>Porada</last></author>
      <author><first>Xiyuan</first><last>Zou</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>256–263</pages>
      <abstract>All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of the increase in F1 score reported in the past five years.</abstract>
      <url hash="30a4d8d3">2024.lrec-main.23</url>
      <bibkey>porada-etal-2024-controlled-reevaluation</bibkey>
    </paper>
    <paper id="24">
      <title>A Corpus and Method for <fixed-case>C</fixed-case>hinese Named Entity Recognition in Manufacturing</title>
      <author><first>Ruiting</first><last>Li</last></author>
      <author><first>Peiyan</first><last>Wang</last></author>
      <author><first>Libang</first><last>Wang</last></author>
      <author><first>Danqingxin</first><last>Yang</last></author>
      <author><first>Dongfeng</first><last>Cai</last></author>
      <pages>264–272</pages>
      <abstract>Manufacturing specifications are documents entailing different techniques, processes, and components involved in manufacturing. There is a growing demand for named entity recognition (NER) resources and techniques for manufacturing-specific named entities, with the development of smart manufacturing. In this paper, we introduce a corpus of Chinese manufacturing specifications, named MS-NERC, including 4,424 sentences and 16,383 entities. We also propose an entity recognizer named Trainable State Transducer (TST), which is initialized with a finite state transducer describing the morphological patterns of entities. It can directly recognize entities based on prior morphological knowledge without training. Experimental results show that TST achieves an overall 82.05% F1 score for morphological-specific entities in zero-shot. TST can be improved through training, the result of which outperforms neural methods in few-shot and rich-resource. We believe that our corpus and model will be valuable resources for NER research not only in manufacturing but also in other low-resource domains.</abstract>
      <url hash="8b2e0f88">2024.lrec-main.24</url>
      <bibkey>li-etal-2024-corpus-method</bibkey>
    </paper>
    <paper id="25">
      <title>A Corpus for Sentence-Level Subjectivity Detection on <fixed-case>E</fixed-case>nglish News Articles</title>
      <author><first>Francesco</first><last>Antici</last></author>
      <author><first>Federico</first><last>Ruggeri</last></author>
      <author><first>Andrea</first><last>Galassi</last></author>
      <author><first>Katerina</first><last>Korre</last></author>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Alessandra</first><last>Bardi</last></author>
      <author><first>Alice</first><last>Fedotova</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>273–285</pages>
      <abstract>We develop novel annotation guidelines for sentence-level subjectivity detection, which are not limited to language-specific cues. We use our guidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjective sentences extracted from English news articles on controversial topics. Our corpus paves the way for subjectivity detection in English and across other languages without relying on language-specific tools, such as lexicons or machine translation. We evaluate state-of-the-art multilingual transformer-based models on the task in mono-, multi-, and cross-language settings. For this purpose, we re-annotate an existing Italian corpus. We observe that models trained in the multilingual setting achieve the best performance on the task.</abstract>
      <url hash="af9d652e">2024.lrec-main.25</url>
      <bibkey>antici-etal-2024-corpus-sentence</bibkey>
    </paper>
    <paper id="26">
      <title>A Corpus of <fixed-case>G</fixed-case>erman <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation (<fixed-case>D</fixed-case>e<fixed-case>AMR</fixed-case>)</title>
      <author><first>Christoph</first><last>Otto</last></author>
      <author><first>Jonas</first><last>Groschwitz</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <author><first>Xiulin</first><last>Yang</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <pages>286–292</pages>
      <abstract>We present the first comprehensive set of guidelines for German Abstract Meaning Representation (Deutsche AMR, DeAMR) along with an annotated corpus of 400 DeAMR. Taking English AMR (EnAMR) as our starting point, we propose significant adaptations to faithfully represent the structure and semantics of German, focusing particularly on verb frames, compound words, and modality. We validate our annotation through inter-annotator agreement and further evaluate our corpus with a comparison of structural divergences between EnAMR and DeAMR on parallel sentences, replicating previous work that finds both cases of cross-lingual structural alignment and cases of meaningful linguistic divergence. Finally, we fine-tune state-of-the-art multi-lingual and cross-lingual AMR parsers on our corpus and find that, while our small corpus is insufficient to produce quality output, there is a need to continue develop and evaluate against gold non-English AMR data.</abstract>
      <url hash="fc946212">2024.lrec-main.26</url>
      <bibkey>otto-etal-2024-corpus-german</bibkey>
    </paper>
    <paper id="27">
      <title>A Corpus of Spontaneous <fixed-case>L</fixed-case>2 <fixed-case>E</fixed-case>nglish Speech for Real-situation Speaking Assessment</title>
      <author><first>Sylvain</first><last>Coulange</last></author>
      <author><first>Marie-Hélène</first><last>Fries</last></author>
      <author><first>Monica</first><last>Masperi</last></author>
      <author><first>Solange</first><last>Rossato</last></author>
      <pages>293–297</pages>
      <abstract>When assessing second language proficiency (L2), evaluation of spontaneous speech performance is crucial. This paper presents a corpus of spontaneous L2 English speech, focusing on the speech performance of B1 and B2 proficiency speakers. Two hundred and sixty university students were recorded during a speaking task as part of a French national certificate in English. This task entailed a 10-minute role-play among 2 or 3 candidates, arguing about a controversial topic, in order to reach a negotiated compromise. Each student’s performance was evaluated by two experts, categorizing them into B2, B1 or below B1 speaking proficiency levels. Automatic diarization, transcription, and alignment at the word level were performed on the recorded conversations, in order to analyse lexical stress realisation in polysyllabic plain words of B1 and B2 proficiency students. Results showed that only 35.4% of the 6,350 targeted words had stress detected on the expected syllable, revealing a common stress shift to the final syllable. Besides a substantial inter-speaker variability (0% to 68.4%), B2 speakers demonstrated a slightly higher stress accuracy (36%) compared to B1 speakers (29.6%). Those with accurate stress placement utilized F0 and intensity to make syllable prominence, while speakers with lower accuracy tended to lengthen words on their last syllables, with minimal changes in other dimensions.</abstract>
      <url hash="4fa1385c">2024.lrec-main.27</url>
      <bibkey>coulange-etal-2024-corpus-spontaneous</bibkey>
    </paper>
    <paper id="28">
      <title>Action and Reaction Go Hand in Hand! a Multi-modal Dialogue Act Aided Sarcasm Identification</title>
      <author><first>Mohit Singh</first><last>Tomar</last></author>
      <author><first>Tulika</first><last>Saha</last></author>
      <author><first>Abhisek</first><last>Tiwari</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <pages>298–309</pages>
      <abstract>Sarcasm primarily involves saying something but “meaning the opposite” or “meaning something completely different” in order to convey a particular tone or mood. In both the above cases, the “meaning” is reflected by the communicative intention of the speaker, known as dialogue acts. In this paper, we seek to investigate a novel phenomenon of analyzing sarcasm in the context of dialogue acts with the hypothesis that the latter helps to understand the former better. Toward this aim, we extend the multi-modal MUStARD dataset to enclose dialogue acts for each dialogue. To demonstrate the utility of our hypothesis, we develop a dialogue act-aided multi-modal transformer network for sarcasm identification (MM-SARDAC), leveraging interrelation between these tasks. In addition, we introduce an order-infused, multi-modal infusion mechanism into our proposed model, which allows for a more intuitive combined modality representation by selectively focusing on relevant modalities in an ordered manner. Extensive empirical results indicate that dialogue act-aided sarcasm identification achieved better performance compared to performing sarcasm identification alone. The dataset and code are available at https://github.com/mohit2b/MM-SARDAC.</abstract>
      <url hash="8b4a79cb">2024.lrec-main.28</url>
      <bibkey>tomar-etal-2024-action-reaction</bibkey>
    </paper>
    <paper id="29">
      <title>Action-Concentrated Embedding Framework: This Is Your Captain Sign-tokening</title>
      <author><first>Hyunwook</first><last>Yu</last></author>
      <author><first>Suhyeon</first><last>Shin</last></author>
      <author><first>Junku</first><last>Heo</last></author>
      <author><first>Hyuntaek</first><last>Shin</last></author>
      <author><first>Hyosu</first><last>Kim</last></author>
      <author><first>Mucheol</first><last>Kim</last></author>
      <pages>310–320</pages>
      <abstract>Sign language is the primary communication medium for people who are deaf or have hearing loss. However, given the divergent range of sensory abilities of these individuals, there is a communication gap that needs to be addressed. In this paper, we present action-concentrated embedding (ACE), which is a novel sign token embedding framework. Additionally, to provide a more structured foundation for sign language analysis, we introduce a dedicated notation system tailored for sign language that endeavors to encapsulate the nuanced gestures and movements that are integral with sign communication. The proposed ACE approach tracks a signer’s actions based on human posture estimation. Tokenizing these actions and capturing the token embedding using a short-time Fourier transform encapsulates the time-based behavioral changes. Hence, ACE offers input embedding to translate sign language into natural language sentences. When tested against a disaster sign language dataset using automated machine translation measures, ACE notably surpasses prior research in terms of translation capabilities, improving the performance by up to 5.79% for BLEU-4 and 5.46% for ROUGE-L metric.</abstract>
      <url hash="80e4d1d8">2024.lrec-main.29</url>
      <bibkey>yu-etal-2024-action-concentrated</bibkey>
    </paper>
    <paper id="30">
      <title>Active Learning Design Choices for <fixed-case>NER</fixed-case> with Transformers</title>
      <author><first>Robert</first><last>Vacareanu</last></author>
      <author><first>Enrique</first><last>Noriega-Atala</last></author>
      <author><first>Gus</first><last>Hahn-Powell</last></author>
      <author><first>Marco A.</first><last>Valenzuela-Escarcega</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>321–334</pages>
      <abstract>We explore multiple important choices that have not been analyzed in conjunction regarding active learning for token classification using transformer networks. These choices are: (i) how to select what to annotate, (ii) decide whether to annotate entire sentences or smaller sentence fragments, (iii) how to train with incomplete annotations at token-level, and (iv) how to select the initial seed dataset. We explore whether annotating at sub-sentence level can translate to an improved downstream performance by considering two different sub-sentence annotation strategies: (i) entity-level, and (ii) token-level. These approaches result in some sentences being only partially annotated. To address this issue, we introduce and evaluate multiple strategies to deal with partially-annotated sentences during the training process. We show that annotating at the sub-sentence level achieves comparable or better performance than sentence-level annotations with a smaller number of annotated tokens. We then explore the extent to which the performance gap remains once accounting for the annotation time and found that both annotation schemes perform similarly.</abstract>
      <url hash="cf373e9c">2024.lrec-main.30</url>
      <bibkey>vacareanu-etal-2024-active-learning</bibkey>
    </paper>
    <paper id="31">
      <title>A <fixed-case>CURATE</fixed-case>d <fixed-case>CAT</fixed-case>alog: Rethinking the Extraction of Pretraining Corpora for Mid-Resourced Languages</title>
      <author><first>Jorge</first><last>Palomar-Giner</last></author>
      <author><first>Jose Javier</first><last>Saiz</last></author>
      <author><first>Ferran</first><last>Espuña</last></author>
      <author><first>Mario</first><last>Mina</last></author>
      <author><first>Severino</first><last>Da Dalt</last></author>
      <author><first>Joan</first><last>Llop</last></author>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Aitor</first><last>Gonzalez-Agirre</last></author>
      <author><first>Marta</first><last>Villegas</last></author>
      <pages>335–349</pages>
      <abstract>We present and describe two language resources in this paper: CATalog 1.0, the largest text corpus in Catalan to date, and CURATE (Corpus Utility for RAting TExt), a modular, parallelizable pipeline used for processing and scoring documents based on text quality that we have optimised to run in High Performance Cluster (HPC) environments. In the coming sections we describe our data preprocessing pipeline at length; traditional pipelines usually implement a set of binary filters such that a given document is either in or out. In our experience with Catalan, in lower-resource settings it is more practical to instead assign a document a soft score to allow for more flexible decision-making. We describe how the document score is calculated and highlight its interpretability by showing that it is significantly correlated with human judgements as obtained from a comparative judgement experiment. We additionally describe the different subcorpora that make up CATalog 1.0.</abstract>
      <url hash="150f5b16">2024.lrec-main.31</url>
      <bibkey>palomar-giner-etal-2024-curated-catalog</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>A</fixed-case>da<fixed-case>K</fixed-case>ron: An Adapter-based Parameter Efficient Model Tuning with Kronecker Product</title>
      <author><first>Marco</first><last>Braga</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Gabriella</first><last>Pasi</last></author>
      <pages>350–357</pages>
      <abstract>The fine-tuning paradigm has been widely adopted to train neural models tailored for specific tasks. However, the recent upsurge of Large Language Models (LLMs), characterized by billions of parameters, has introduced profound computational challenges to the fine-tuning process. This has fueled intensive research on Parameter-Efficient Fine-Tuning (PEFT) techniques, usually involving the training of a selective subset of the original model parameters. One of the most used approaches is Adapters, which add trainable lightweight layers to the existing pretrained weights. Within this context, we propose AdaKron, an Adapter-based fine-tuning with the Kronecker product. In particular, we leverage the Kronecker product to combine the output of two small networks, resulting in a final vector whose dimension is the product of the dimensions of the individual outputs, allowing us to train only 0.55% of the model’s original parameters. We evaluate AdaKron performing a series of experiments on the General Language Understanding Evaluation (GLUE) benchmark, achieving results in the same ballpark as recent state-of-the-art PEFT methods, despite training fewer parameters.</abstract>
      <url hash="907686ca">2024.lrec-main.32</url>
      <bibkey>braga-etal-2024-adakron-adapter</bibkey>
    </paper>
    <paper id="33">
      <title>Adaptive Reinforcement Tuning Language Models as Hard Data Generators for Sentence Representation</title>
      <author><first>Bo</first><last>Xu</last></author>
      <author><first>Yifei</first><last>Wu</last></author>
      <author><first>Shouang</first><last>Wei</last></author>
      <author><first>Ming</first><last>Du</last></author>
      <author><first>Hongya</first><last>Wang</last></author>
      <pages>358–371</pages>
      <abstract>Sentence representation learning is a fundamental task in NLP. Existing methods use contrastive learning (CL) to learn effective sentence representations, which benefit from high-quality contrastive data but require extensive human annotation. Large language models (LLMs) like ChatGPT and GPT4 can automatically generate such data. However, this alternative strategy also encounters challenges: 1) obtaining high-quality generated data from small-parameter LLMs is difficult, and 2) inefficient utilization of the generated data. To address these challenges, we propose a novel adaptive reinforcement tuning (ART) framework. Specifically, to address the first challenge, we introduce a reinforcement learning approach for fine-tuning small-parameter LLMs, enabling the generation of high-quality hard contrastive data without human feedback. To address the second challenge, we propose an adaptive iterative framework to guide the small-parameter LLMs to generate progressively harder samples through multiple iterations, thereby maximizing the utility of generated data. Experiments conducted on seven semantic text similarity tasks demonstrate that the sentence representation models trained using the synthetic data generated by our proposed method achieve state-of-the-art performance. Our code is available at https://github.com/WuNein/AdaptCL.</abstract>
      <url hash="8b208e15">2024.lrec-main.33</url>
      <bibkey>xu-etal-2024-adaptive-reinforcement</bibkey>
    </paper>
    <paper id="34">
      <title>Adaptive Simultaneous Sign Language Translation with Confident Translation Length Estimation</title>
      <author><first>Tong</first><last>Sun</last></author>
      <author><first>Biao</first><last>Fu</last></author>
      <author><first>Cong</first><last>Hu</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Ruiquan</first><last>Zhang</last></author>
      <author><first>Xiaodong</first><last>Shi</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <pages>372–384</pages>
      <abstract>Traditional non-simultaneous Sign Language Translation (SLT) methods, while effective for pre-recorded videos, face challenges in real-time scenarios due to inherent inference delays. The emerging field of simultaneous SLT aims to address this issue by progressively translating incrementally received sign video. However, the sole existing work in simultaneous SLT adopts a fixed gloss-based policy, which suffer from limitations in boundary prediction and contextual comprehension. In this paper, we delve deeper into this area and propose an adaptive policy for simultaneous SLT. Our approach introduces the concept of “confident translation length”, denoting maximum accurate translation achievable from current input. An estimator measures this length for streaming sign video, enabling the model to make informed decisions on whether to wait for more input or proceed with translation. To train the estimator, we construct a training data of confident translation length based on the longest common prefix between translations of partial and complete inputs. Furthermore, we incorporate adaptive training, utilizing pseudo prefix pairs, to refine the offline translation model for optimal performance in simultaneous scenarios. Experimental results on PHOENIX2014T and CSL-Daily demonstrate the superiority of our adaptive policy over existing methods, particularly excelling in situations requiring extremely low latency.</abstract>
      <url hash="802b9c36">2024.lrec-main.34</url>
      <bibkey>sun-etal-2024-adaptive-simultaneous</bibkey>
    </paper>
    <paper id="35">
      <title>A Dataset for Named Entity Recognition and Entity Linking in <fixed-case>C</fixed-case>hinese Historical Newspapers</title>
      <author><first>Baptiste</first><last>Blouin</last></author>
      <author><first>Cécile</first><last>Armand</last></author>
      <author><first>Christian</first><last>Henriot</last></author>
      <pages>385–394</pages>
      <abstract>In this study, we present a novel historical Chinese dataset for named entity recognition, entity linking, coreference and entity relations. We use data from Chinese newspapers from 1872 to 1949 and multilingual bibliographic resources from the same period. The period and the language are the main strength of the present work, offering a resource which covers different styles and language uses, as well as the largest historical Chinese NER dataset with manual annotations from this transitional period. After detailing the selection and annotation process, we present the very first results that can be obtained from this dataset. Texts and annotations are freely downloadable from the GitHub repository.</abstract>
      <url hash="19405e90">2024.lrec-main.35</url>
      <bibkey>blouin-etal-2024-dataset-named</bibkey>
    </paper>
    <paper id="36">
      <title>A Dataset for Pharmacovigilance in <fixed-case>G</fixed-case>erman, <fixed-case>F</fixed-case>rench, and <fixed-case>J</fixed-case>apanese: Annotating Adverse Drug Reactions across Languages</title>
      <author><first>Lisa</first><last>Raithel</last></author>
      <author><first>Hui-Syuan</first><last>Yeh</last></author>
      <author><first>Shuntaro</first><last>Yada</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Philippe</first><last>Thomas</last></author>
      <author><first>Tomohiro</first><last>Nishiyama</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>395–414</pages>
      <abstract>User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.</abstract>
      <url hash="edaaab95">2024.lrec-main.36</url>
      <bibkey>raithel-etal-2024-dataset-pharmacovigilance</bibkey>
    </paper>
    <paper id="37">
      <title>Adding <fixed-case>SPICE</fixed-case> to Life: Speaker Profiling in Multiparty Conversations</title>
      <author><first>Shivani</first><last>Kumar</last></author>
      <author><first>Rishabh</first><last>Gupta</last></author>
      <author><first>Md. Shad</first><last>Akhtar</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>415–425</pages>
      <abstract>In the realm of conversational dynamics, individual idiosyncrasies challenge the suitability of a one-size-fits-all approach for dialogue agent responses. Prior studies often assumed the speaker’s persona’s immediate availability, a premise not universally applicable. To address this gap, we explore the Speaker Profiling in Conversations (SPC) task, aiming to synthesize persona attributes for each dialogue participant. SPC comprises three core subtasks: persona discovery, persona-type identification, and persona-value extraction. The first subtask identifies persona-related utterances, the second classifies specific attributes, and the third extracts precise values for the persona. To confront this multifaceted challenge, we’ve diligently compiled SPICE, an annotated dataset, underpinning our thorough evaluation of diverse baseline models. Additionally, we benchmark these findings against our innovative neural model, SPOT, presenting an exhaustive analysis encompassing a nuanced assessment of quantitative and qualitative merits and limitations.</abstract>
      <url hash="4093f36b">2024.lrec-main.37</url>
      <bibkey>kumar-etal-2024-adding-spice</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>ADEA</fixed-case>: An Argumentative Dialogue Dataset on Ethical Issues Concerning Future <fixed-case>A</fixed-case>.<fixed-case>I</fixed-case>. Applications</title>
      <author><first>Christian</first><last>Hauptmann</last></author>
      <author><first>Adrian</first><last>Krenzer</last></author>
      <author><first>Antonia</first><last>Krause</last></author>
      <author><first>Frank</first><last>Puppe</last></author>
      <pages>426–437</pages>
      <abstract>Introducing ADEA: a German dataset that captures online dialogues and focuses on ethical issues related to future AI applications. This dataset, which includes over 2800 labeled user utterances on four different topics, is specifically designed for the training of chatbots that can navigate the complexities of real-world ethical AI conversations. The creation of these dialogues is the result of two carefully conducted studies in which university students interacted with an argumentative dialogue system. A fundamental part of our methodology is the use of German argument graphs. These graphs not only form the knowledge base of the dialogue system but also serve as an effective annotation scheme for the dialogues. Apart from the introduction of the dataset and the argument graphs, we provide a preliminary benchmark using GPT-4 via the OpenAI API. This provides researchers with a concrete reference point while demonstrating the potential of our dataset. We make our dataset and argument graphs available at https://github.com/HaupChris/ADEA-Dialogue-Dataset.</abstract>
      <url hash="4fa127fe">2024.lrec-main.38</url>
      <bibkey>hauptmann-etal-2024-adea-argumentative</bibkey>
    </paper>
    <paper id="39">
      <title>A Decade of Scholarly Research on Open Knowledge Graphs</title>
      <author><first>Houcemeddine</first><last>Turki</last></author>
      <author><first>Abraham Toluwase</first><last>Owodunni</last></author>
      <author><first>Mohamed Ali</first><last>Hadj Taieb</last></author>
      <author><first>René Fabrice</first><last>Bile</last></author>
      <author><first>Mohamed</first><last>Ben Aouicha</last></author>
      <pages>438–448</pages>
      <abstract>The proliferation of open knowledge graphs has led to a surge in scholarly research on the topic over the past decade. This paper presents a bibliometric analysis of the scholarly literature on open knowledge graphs published between 2013 and 2023. The study aims to identify the trends, patterns, and impact of research in this field, as well as the key topics and research questions that have emerged. The work uses bibliometric techniques to analyze a sample of 4445 scholarly articles retrieved from Scopus. The findings reveal an ever-increasing number of publications on open knowledge graphs published every year, particularly in developed countries (+50 per year). These outputs are published in highly-referred scholarly journals and conferences. The study identifies three main research themes: (1) knowledge graph construction and enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into NLP systems. Within these themes, the study identifies specific tasks that have received considerable attention, including entity linking, knowledge graph embedding, and graph neural networks.</abstract>
      <url hash="04a8a5cc">2024.lrec-main.39</url>
      <bibkey>turki-etal-2024-decade-scholarly</bibkey>
    </paper>
    <paper id="40">
      <title>A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference</title>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>449–458</pages>
      <abstract>Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders. Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent NLI in complex domains.</abstract>
      <url hash="579f3241">2024.lrec-main.40</url>
      <bibkey>thayaparan-etal-2024-differentiable-integer</bibkey>
    </paper>
    <paper id="41">
      <title>A Document-Level Text Simplification Dataset for <fixed-case>J</fixed-case>apanese</title>
      <author><first>Yoshinari</first><last>Nagai</last></author>
      <author><first>Teruaki</first><last>Oka</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>459–476</pages>
      <abstract>Document-level text simplification, a task that combines single-document summarization and intra-sentence simplification, has garnered significant attention. However, studies have primarily focused on languages such as English and German, leaving Japanese and similar languages underexplored because of a scarcity of linguistic resources. In this study, we devised JADOS, the first Japanese document-level text simplification dataset based on newspaper articles and Wikipedia. Our dataset focuses on simplification, to enhance readability by reducing the number of sentences and tokens in a document. We conducted investigations using our dataset. Firstly, we analyzed the characteristics of Japanese simplification by comparing it across different domains and with English counterparts. Moreover, we experimentally evaluated the performances of text summarization methods, transformer-based text simplification models, and large language models. In terms of D-SARI scores, the transformer-based models performed best across all domains. Finally, we manually evaluated several model outputs and target articles, demonstrating the need for document-level text simplification models in Japanese.</abstract>
      <url hash="cbeacd59">2024.lrec-main.41</url>
      <bibkey>nagai-etal-2024-document-level</bibkey>
    </paper>
    <paper id="42">
      <title>A Dual-View Approach to Classifying Radiology Reports by Co-Training</title>
      <author><first>Yutong</first><last>Han</last></author>
      <author><first>Yan</first><last>Yuan</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <pages>477–483</pages>
      <abstract>Radiology report analysis provides valuable information that can aid with public health initiatives, and has been attracting increasing attention from the research community. In this work, we present a novel insight that the structure of a radiology report (namely, the Findings and Impression sections) offers different views of a radiology scan. Based on this intuition, we further propose a co-training approach, where two machine learning models are built upon the Findings and Impression sections, respectively, and use each other’s information to boost performance with massive unlabeled data in a semi-supervised manner. We conducted experiments in a public health surveillance study, and results show that our co-training approach is able to improve performance using the dual views and surpass competing supervised and semi-supervised methods.</abstract>
      <url hash="d5158758">2024.lrec-main.42</url>
      <bibkey>han-etal-2024-dual-view</bibkey>
    </paper>
    <paper id="43">
      <title>Advancing Semi-Supervised Learning for Automatic Post-Editing: Data-Synthesis by Mask-Infilling with Erroneous Terms</title>
      <author><first>Wonkee</first><last>Lee</last></author>
      <author><first>Seong-Hwan</first><last>Heo</last></author>
      <author><first>Jong-Hyeok</first><last>Lee</last></author>
      <pages>484–494</pages>
      <abstract>Semi-supervised learning that leverages synthetic data for training has been widely adopted for developing automatic post-editing (APE) models due to the lack of training data. With this aim, we focus on data-synthesis methods to create high-quality synthetic data. Given that APE takes as input a machine-translation result that might include errors, we present a data-synthesis method by which the resulting synthetic data mimic the translation errors found in actual data. We introduce a noising-based data-synthesis method by adapting the masked language model approach, generating a noisy text from a clean text by infilling masked tokens with erroneous tokens. Moreover, we propose selective corpus interleaving that combines two separate synthetic datasets by taking only the advantageous samples to enhance the quality of the synthetic data further. Experimental results show that using the synthetic data created by our approach results in significantly better APE performance than other synthetic data created by existing methods.</abstract>
      <url hash="cfc07f3e">2024.lrec-main.43</url>
      <bibkey>lee-etal-2024-advancing-semi</bibkey>
    </paper>
    <paper id="44">
      <title>Advancing Topic Segmentation and Outline Generation in <fixed-case>C</fixed-case>hinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark</title>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Weihao</first><last>Liu</last></author>
      <author><first>Xiaomin</first><last>Chu</last></author>
      <author><first>Peifeng</first><last>Li</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>495–506</pages>
      <abstract>Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings, unveiling the discourse topic structure of a document. Compared with sentence-level topic structure, the paragraph-level topic structure can quickly grasp and understand the overall context of the document from a higher level, benefitting many downstream tasks such as summarization, discourse parsing, and information retrieval. However, the lack of large-scale, high-quality Chinese paragraph-level topic structure corpora restrained relative research and applications. To fill this gap, we build the Chinese paragraph-level topic representation, corpus, and benchmark in this paper. Firstly, we propose a hierarchical paragraph-level topic structure representation with three layers to guide the corpus construction. Then, we employ a two-stage man-machine collaborative annotation method to construct the largest Chinese Paragraph-level Topic Structure corpus (CPTS), achieving high quality. We also build several strong baselines, including ChatGPT, to validate the computability of CPTS on two fundamental tasks (topic segmentation and outline generation) and preliminarily verified its usefulness for the downstream task (discourse parsing).</abstract>
      <url hash="dc7857a9">2024.lrec-main.44</url>
      <bibkey>jiang-etal-2024-advancing-topic</bibkey>
    </paper>
    <paper id="45">
      <title>A Family of Pretrained Transformer Language Models for <fixed-case>R</fixed-case>ussian</title>
      <author><first>Dmitry</first><last>Zmitrovich</last></author>
      <author><first>Aleksandr</first><last>Abramov</last></author>
      <author><first>Andrey</first><last>Kalmykov</last></author>
      <author><first>Vitaly</first><last>Kadulin</last></author>
      <author><first>Maria</first><last>Tikhonova</last></author>
      <author><first>Ekaterina</first><last>Taktasheva</last></author>
      <author><first>Danil</first><last>Astafurov</last></author>
      <author><first>Mark</first><last>Baushenko</last></author>
      <author><first>Artem</first><last>Snegirev</last></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <author><first>Sergei S.</first><last>Markov</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <pages>507–524</pages>
      <abstract>Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However, developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs, which spans encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. We provide a report on the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.</abstract>
      <url hash="dcdcb07b">2024.lrec-main.45</url>
      <bibkey>zmitrovich-etal-2024-family-pretrained</bibkey>
    </paper>
    <paper id="46">
      <title>A Fast and High-quality Text-to-Speech Method with Compressed Auxiliary Corpus and Limited Target Speaker Corpus</title>
      <author><first>Ye</first><last>Tao</last></author>
      <author><first>Chaofeng</first><last>Lu</last></author>
      <author><first>Meng</first><last>Liu</last></author>
      <author><first>Kai</first><last>Xu</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Yunlong</first><last>Tian</last></author>
      <author><first>Yongjie</first><last>Du</last></author>
      <pages>525–535</pages>
      <abstract>With an auxiliary corpus (non-target speaker corpus) for model pre-training, Text-to-Speech (TTS) methods can generate high-quality speech with a limited target speaker corpus. However, this approach comes with expensive training costs. To overcome the challenge, a high-quality TTS method is proposed, significantly reducing training costs while maintaining the naturalness of synthesized speech. In this paper, we propose an auxiliary corpus compression algorithm that reduces the training cost while the naturalness of the synthesized speech is not significantly degraded. We then use the compressed corpus to pre-train the proposed TTS model CMDTTS, which fuses phoneme and word multi-level prosody modeling components and denoises the generated mel-spectrograms using denoising diffusion probabilistic models (DDPMs). In addition, a fine-tuning step that the conditional generative adversarial network (cGAN) is introduced to embed the target speaker feature and improve speech quality using the target speaker corpus. Experiments are conducted on Chinese and English single speaker’s corpora, and the results show that the method effectively balances the model training speed and the synthesized speech quality and outperforms the current models.</abstract>
      <url hash="a6c3aee8">2024.lrec-main.46</url>
      <bibkey>tao-etal-2024-fast-high</bibkey>
    </paper>
    <paper id="47">
      <title>A Frustratingly Simple Decoding Method for Neural Text Generation</title>
      <author><first>Haoran</first><last>Yang</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>536–557</pages>
      <abstract>We introduce a frustratingly simple, highly efficient, and surprisingly effective decoding method, termed Frustratingly Simple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: We construct an anti-language model (anti-LM) based on previously generated text, which is employed to penalize the future generation of repetitive content. The anti-LM can be implemented as simple as an n-gram language model or a vectorized variant. In this way, FSD incurs no additional model parameters and negligible computational overhead (FSD can be as fast as greedy search). Despite its simplicity, FSD is surprisingly effective and generalizes across different datasets, models, and languages. Extensive experiments show that FSD outperforms established strong baselines in terms of generation quality, decoding speed, and universality.</abstract>
      <url hash="faa187c6">2024.lrec-main.47</url>
      <bibkey>yang-etal-2024-frustratingly-simple</bibkey>
    </paper>
    <paper id="48">
      <title>A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous <fixed-case>J</fixed-case>apanese Questions</title>
      <author><first>Shun</first><last>Inadumi</last></author>
      <author><first>Seiya</first><last>Kawano</last></author>
      <author><first>Akishige</first><last>Yuguchi</last></author>
      <author><first>Yasutomo</first><last>Kawanishi</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <pages>558–571</pages>
      <abstract>Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved.</abstract>
      <url hash="8804c37f">2024.lrec-main.48</url>
      <bibkey>inadumi-etal-2024-gaze-grounded</bibkey>
    </paper>
    <paper id="49">
      <title>Agenda-Driven Question Generation: A Case Study in the Courtroom Domain</title>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Anoop</first><last>Kumar</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Prem</first><last>Natarajan</last></author>
      <pages>572–583</pages>
      <abstract>This paper introduces a novel problem of automated question generation for courtroom examinations, CourtQG. While question generation has been studied in domains such as educational testing and product description, CourtQG poses several unique challenges owing to its non-cooperative and agenda-driven nature. Specifically, not only the generated questions need to be relevant to the case and underlying context, they also have to achieve certain objectives such as challenging the opponent’s arguments and/or revealing potential inconsistencies in their answers. We propose to leverage large language models (LLM) for CourtQG by fine-tuning them on two auxiliary tasks, agenda explanation (i.e., uncovering the underlying intents) and question type prediction. We additionally propose cold-start generation of questions from background documents without relying on examination history. We construct a dataset to evaluate our proposed method and show that it generates better questions according to standard metrics when compared to several baselines.</abstract>
      <url hash="c823d7b6">2024.lrec-main.49</url>
      <bibkey>fung-etal-2024-agenda-driven</bibkey>
    </paper>
    <paper id="50">
      <title>A Generative Model for <fixed-case>L</fixed-case>ambek Categorial Sequents</title>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author><first>Gerald</first><last>Penn</last></author>
      <pages>584–593</pages>
      <abstract>In this work, we introduce a generative model, PLC+, for generating Lambek Categorial Grammar(LCG) sequents. We also introduce a simple method to numerically estimate the model’s parameters from an annotated corpus. Then we compare our model with probabilistic context-free grammars (PCFGs) and show that PLC+ simultaneously assigns a higher probability to a common corpus, and has greater coverage.</abstract>
      <url hash="d5b06542">2024.lrec-main.50</url>
      <bibkey>zhao-penn-2024-generative-model</bibkey>
    </paper>
    <paper id="51">
      <title>Agent-based Modeling of Language Change in a Small-world Network</title>
      <author><first>Dalmo</first><last>Buzato</last></author>
      <author><first>Evandro</first><last>Cunha</last></author>
      <pages>594–599</pages>
      <abstract>Language change has been the subject of numerous studies in linguistics. However, due to the dynamic and complex nature of this phenomenon, and to the difficulty of obtaining extensive real data of language in use, some of its aspects remain obscure. In recent years, nonetheless, research has used computational modeling to simulate features related to variation, change, propagation, and evolution of languages in speech communities, finding compelling results. In this article, agent-based modeling and simulation is used to study language change. Drawing on previous studies, a speech community was modeled using Zachary’s karate club network, a well-established small-world network model in the field of complex systems. Idiolects were assigned through numerical values for each agent. The results demonstrate that the centrality of each agent in the network, interpreted as social prestige, appears to be a factor influencing change. Additionally, the nature of idiolects also seems to impact the spread of linguistic variants in the language change process. These findings complement the theoretical understanding of the language change phenomenon with new simulation data and provide new avenues for research.</abstract>
      <url hash="c1349819">2024.lrec-main.51</url>
      <bibkey>buzato-cunha-2024-agent-based</bibkey>
    </paper>
    <paper id="52">
      <title>Agettivu, Aggitivu o Aghjettivu? <fixed-case>POS</fixed-case> Tagging <fixed-case>C</fixed-case>orsican Dialects</title>
      <author><first>Alice</first><last>Millour</last></author>
      <author><first>Lorenza</first><last>Brasile</last></author>
      <author><first>Alberto</first><last>Ghia</last></author>
      <author><first>Laurent</first><last>Kevers</last></author>
      <pages>600–608</pages>
      <abstract>In this paper we present a series of experiments towards POS tagging Corsican, a less-resourced language spoken in Corsica and linguistically related to Italian. The first contribution is Corsican-POS, the first gold standard POS-tagged corpus for Corsica, composed of 500 sentences manually annotated with the Universal POS tagset. Our second contribution is a set of experiments and evaluation of POS tagging models which starts with a baseline model for Italian and is aimed at finding the best training configuration, namely in terms of the size and combination strategy of the existing raw and annotated resources. These experiments result in (i) the first POS tagger for Corsican, reaching an accuracy of 93.38%, (ii) a quantification of the gain provided by the use of each available resource. We find that the optimal configuration uses Italian word embeddings further specialized with Corsican embeddings and trained on the largest gold corpus for Corsican available so far.</abstract>
      <url hash="c1788310">2024.lrec-main.52</url>
      <bibkey>millour-etal-2024-agettivu-aggitivu</bibkey>
    </paper>
    <paper id="53">
      <title>Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models</title>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Qiushi</first><last>Sun</last></author>
      <author><first>Qipeng</first><last>Guo</last></author>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Xiaonan</first><last>Li</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Cheng</first><last>Chang</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Ding</first><last>Wang</last></author>
      <author><first>Xiaofeng</first><last>Mou</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>609–625</pages>
      <abstract>Recent advancements in Chain-of-Thought prompting have facilitated significant breakthroughs for Large Language Models (LLMs) in complex reasoning tasks. Current research enhances the reasoning performance of LLMs by sampling multiple reasoning chains and ensembling based on the answer frequency. However, this approach fails in scenarios where the correct answers are in the minority. We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers. To address this shortcoming, we introduce a hierarchical reasoning aggregation framework AoR (Aggregation of Reasoning), which selects answers based on the evaluation of reasoning chains. Additionally, AoR incorporates dynamic sampling, adjusting the number of reasoning chains in accordance with the complexity of the task. Experimental results on a series of complex reasoning tasks show that AoR outperforms prominent ensemble methods. Further analysis reveals that AoR not only adapts various LLMs but also achieves a superior performance ceiling when compared to current methods.</abstract>
      <url hash="17cb78aa">2024.lrec-main.53</url>
      <bibkey>yin-etal-2024-aggregation-reasoning</bibkey>
    </paper>
    <paper id="54">
      <title>A Hierarchical Sequence-to-Set Model with Coverage Mechanism for Aspect Category Sentiment Analysis</title>
      <author><first>Siyu</first><last>Wang</last></author>
      <author><first>Jianhui</first><last>Jiang</last></author>
      <author><first>Shengran</first><last>Dai</last></author>
      <author><first>Jiangtao</first><last>Qiu</last></author>
      <pages>626–635</pages>
      <abstract>Aspect category sentiment analysis (ACSA) aims to simultaneously detect aspect categories and their corresponding sentiment polarities (category-sentiment pairs). Some recent studies have used pre-trained generative models to complete ACSA and achieved good results. However, for ACSA, generative models still face three challenges. First, addressing the missing predictions in ACSA is crucial, which involves accurately predicting all category-sentiment pairs within a sentence. Second, category-sentiment pairs are inherently a disordered set. Consequently, the model incurs a penalty even when its predictions are correct, but the predicted order is inconsistent with the ground truths. Third, different aspect categories should focus on relevant sentiment words, and the polarity of the aspect category should be the aggregation of the polarities of these sentiment words. This paper proposes a hierarchical generative model with a coverage mechanism using sequence-to-set learning to tackle all three challenges simultaneously. Our model’s superior performance is demonstrated through extensive experiments conducted on several datasets.</abstract>
      <url hash="fd56a13b">2024.lrec-main.54</url>
      <bibkey>wang-etal-2024-hierarchical-sequence</bibkey>
    </paper>
    <paper id="55">
      <title>A <fixed-case>H</fixed-case>ong <fixed-case>K</fixed-case>ong <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Corpus Collected from Sign-interpreted <fixed-case>TV</fixed-case> News</title>
      <author><first>Zhe</first><last>Niu</last></author>
      <author><first>Ronglai</first><last>Zuo</last></author>
      <author><first>Brian</first><last>Mak</last></author>
      <author><first>Fangyun</first><last>Wei</last></author>
      <pages>636–646</pages>
      <abstract>This paper introduces TVB-HKSL-News, a new Hong Kong sign language (HKSL) dataset collected from a TV news program over a period of 7 months. The dataset is collected to enrich resources for HKSL and support research in large-vocabulary continuous sign language recognition (SLR) and translation (SLT). It consists of 16.07 hours of sign videos of two signers with a vocabulary of 6,515 glosses (for SLR) and 2,850 Chinese characters or 18K Chinese words (for SLT). One signer has 11.66 hours of sign videos and the other has 4.41 hours. One objective in building the dataset is to support the investigation of how well large-vocabulary continuous sign language recognition/translation can be done for a single signer given a (relatively) large amount of his/her training data, which could potentially lead to the development of new modeling methods. Besides, most parts of the data collection pipeline are automated with little human intervention; we believe that our collection method can be scaled up to collect more sign language data easily for SLT in the future for any sign languages if such sign-interpreted videos are available. We also run a SOTA SLR/SLT model on the dataset and get a baseline SLR word error rate of 34.08% and a baseline SLT BLEU-4 score of 23.58 for benchmarking future research on the dataset.</abstract>
      <url hash="7da03d7f">2024.lrec-main.55</url>
      <bibkey>niu-etal-2024-hong-kong</bibkey>
    </paper>
    <paper id="56">
      <title>A Hybrid Approach to Aspect Based Sentiment Analysis Using Transfer Learning</title>
      <author><first>Gaurav</first><last>Negi</last></author>
      <author><first>Rajdeep</first><last>Sarkar</last></author>
      <author><first>Omnia</first><last>Zayed</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>647–658</pages>
      <abstract>Aspect-Based Sentiment Analysis ( ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification.</abstract>
      <url hash="e7983025">2024.lrec-main.56</url>
      <bibkey>negi-etal-2024-hybrid-approach</bibkey>
    </paper>
    <paper id="57">
      <title>A <fixed-case>J</fixed-case>apanese News Simplification Corpus with Faithfulness</title>
      <author><first>Toru</first><last>Urakawa</last></author>
      <author><first>Yuya</first><last>Taguchi</last></author>
      <author><first>Takuro</first><last>Niitsuma</last></author>
      <author><first>Hideaki</first><last>Tamori</last></author>
      <pages>659–665</pages>
      <abstract>Text Simplification enhances the readability of texts for specific audiences. However, automated models may introduce unwanted content or omit essential details, necessitating a focus on maintaining faithfulness to the original input. Furthermore, existing simplified corpora contain instances of low faithfulness. Motivated by this issue, we present a new Japanese simplification corpus designed to prioritize faithfulness. Our collection comprises 7,075 paired sentences simplified from newspaper articles. This process involved collaboration with language education experts who followed guidelines balancing readability and faithfulness. Through corpus analysis, we confirmed that our dataset preserves the content of the original text, including personal names, dates, and city names. Manual evaluation showed that our corpus robustly maintains faithfulness to the original text, surpassing other existing corpora. Furthermore, evaluation by non-native readers confirmed its readability to the target audience. Through the experiment of fine-tuning and in-context learning, we demonstrated that our corpus enhances faithful sentence simplification.</abstract>
      <url hash="32bef275">2024.lrec-main.57</url>
      <bibkey>urakawa-etal-2024-japanese-news</bibkey>
    </paper>
    <paper id="58">
      <title>A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation</title>
      <author><first>Xiangci</first><last>Li</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Haitao</first><last>Mi</last></author>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>666–676</pages>
      <abstract>Knowledge-based, open-domain dialogue generation aims to build chit-chat systems that talk to humans using mined support knowledge. Many types and sources of knowledge have previously been shown to be useful as support knowledge. Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach. While prior work using single-source knowledge has shown a clear positive correlation between the performances of knowledge selection and response generation, there are no existing multi-source datasets for evaluating support knowledge retrieval. Further, prior work has assumed that the knowledge sources available at test time are the same as during training. This unrealistic assumption unnecessarily handicaps models, as new knowledge sources can become available after a model is trained. In this paper, we present a high-quality benchmark named multi-source Wizard of Wikipedia (Ms.WoW) for evaluating multi-source dialogue knowledge selection and response generation. Unlike existing datasets, it contains clean support knowledge, grounded at the utterance level and partitioned into multiple knowledge sources. We further propose a new challenge, dialogue knowledge plug-and-play, which aims to test an already trained dialogue model on using new support knowledge from previously unseen sources in a zero-shot fashion.</abstract>
      <url hash="0d7c0833">2024.lrec-main.58</url>
      <bibkey>li-etal-2024-knowledge-plug</bibkey>
    </paper>
    <paper id="59">
      <title>A Large Annotated Reference Corpus of <fixed-case>N</fixed-case>ew <fixed-case>H</fixed-case>igh <fixed-case>G</fixed-case>erman Poetry</title>
      <author><first>Thomas</first><last>Haider</last></author>
      <pages>677–683</pages>
      <abstract>This paper introduces a large annotated corpus of public domain German poetry, covering the time period from 1600 to the 1920s with 65k poems. We describe how the corpus was compiled, how it was cleaned (including duplicate detection), and how it looks now in terms of size, format, temporal distribution, and automatic annotation. Besides metadata, the corpus contains reliable annotation of tokens, syllables, part-of-speech, and meter and verse measure. Finally, we give some statistics on the annotation and an overview of other poetry corpora.</abstract>
      <url hash="64396cd7">2024.lrec-main.59</url>
      <bibkey>haider-2024-large-annotated</bibkey>
    </paper>
    <paper id="60">
      <title>A Lifelong Multilingual Multi-granularity Semantic Alignment Approach via Maximum Co-occurrence Probability</title>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Hongwei</first><last>Sun</last></author>
      <author><first>Shaojie</first><last>Dai</last></author>
      <author><first>Bo</first><last>Lv</last></author>
      <author><first>Youcheng</first><last>Pan</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <pages>684–694</pages>
      <abstract>Cross-lingual pre-training methods mask and predict tokens in multilingual text to generalize diverse multilingual information. However, due to the lack of sufficient aligned multilingual resources in the pre-training process, these methods may not fully explore the multilingual correlation of masked tokens, resulting in the limitation of multilingual information interaction. In this paper, we propose a lifelong multilingual multi-granularity semantic alignment approach, which continuously extracts massive aligned linguistic units from noisy data via a maximum co-occurrence probability algorithm. Then, the approach releases a version of the multilingual multi-granularity semantic alignment resource, supporting seven languages, namely English, Czech, German, Russian, Romanian, Hindi and Turkish. Finally, we propose how to use this resource to improve the translation performance on WMT14 18 benchmarks in twelve directions. Experimental results show an average of 0.3 1.1 BLEU improvements in all translation benchmarks. The analysis and discussion also demonstrate the superiority and potential of the proposed approach. The resource used in this work will be publicly available.</abstract>
      <url hash="4c2f81e4">2024.lrec-main.60</url>
      <bibkey>liu-etal-2024-lifelong-multilingual</bibkey>
    </paper>
    <paper id="61">
      <title>A Lightweight Approach to a Giga-Corpus of Historical Periodicals: The Story of a <fixed-case>S</fixed-case>lovenian Historical Newspaper Collection</title>
      <author><first>Filip</first><last>Dobranić</last></author>
      <author><first>Bojan</first><last>Evkoski</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>695–703</pages>
      <abstract>Preparing historical newspaper collections is a complicated endeavour, consisting of multiple steps that have to be carefully adapted to the specific content in question, including imaging, layout prediction, optical character recognition, and linguistic annotation. To address the high costs associated with the process, we present a lightweight approach to producing high-quality corpora and apply it to a massive collection of Slovenian historical newspapers from the 18th, 19th and 20th century resulting in a billion-word giga-corpus. We start with noisy OCR-ed data produced by different technologies in varying periods by the National and University Library of Slovenia. To address the inherent variability in the quality of textual data, a challenge commonly encountered in digital libraries globally, we perform a targeted post-digitisation correction procedure, coupled with a robust curation mechanism for noisy texts via language model inference. Subsequently, we subject the corrected and filtered output to comprehensive linguistic annotation, enriching the corpus with part-of-speech tags, lemmas, and named entity labels. Finally, we perform an analysis through topic modeling at the noun lemma level, along with a frequency analysis of the named entities, to confirm the viability of our corpus preparation method.</abstract>
      <url hash="c5b960ef">2024.lrec-main.61</url>
      <bibkey>dobranic-etal-2024-lightweight-approach</bibkey>
    </paper>
    <paper id="62">
      <title>Aligning the <fixed-case>N</fixed-case>orwegian <fixed-case>UD</fixed-case> Treebank with Entity and Coreference Information</title>
      <author><first>Tollef Emil</first><last>Jørgensen</last></author>
      <author><first>Andre</first><last>Kåsen</last></author>
      <pages>704–710</pages>
      <abstract>This paper presents a merged collection of entity and coreference annotated data grounded in the Universal Dependencies (UD) treebanks for the two written forms of Norwegian: Bokmål and Nynorsk. The aligned and converted corpora are the Norwegian Named Entities (NorNE) and Norwegian Anaphora Resolution Corpus (NARC). While NorNE is aligned with an older version of the treebank, NARC is misaligned and requires extensive transformation from the original annotations to the UD structure and CoNLL-U format. Here, we demonstrate the conversion and alignment processes, along with an analysis of discovered issues and errors in the data, some of which include data split overlaps in the original treebank. These procedures and the developed system may prove helpful for future work on processing and aligning data from universal dependencies. The merged corpora comprise the first Norwegian UD treebank enriched with named entities and coreference information, supporting the standardized format for the CorefUD initiative.</abstract>
      <url hash="5afa3e29">2024.lrec-main.62</url>
      <bibkey>jorgensen-kasen-2024-aligning-norwegian</bibkey>
    </paper>
    <paper id="63">
      <title>Alignment before Awareness: Towards Visual Question Localized-Answering in Robotic Surgery via Optimal Transport and Answer Semantics</title>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Yunyan</first><last>Zhang</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Zhiqi</first><last>Huang</last></author>
      <author><first>Derong</first><last>Xu</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>711–721</pages>
      <abstract>The visual question localized-answering (VQLA) system has garnered increasing attention due to its potential as a knowledgeable assistant in surgical education. Apart from providing text-based answers, VQLA can also pinpoint the specific region of interest for better surgical scene understanding. Although recent Transformer-based models for VQLA have obtained promising results, they (1) conduct vanilla text-to-image cross attention, leading to unidirectional and coarse-grained alignment; (2) ignore exploiting the semantics of answers to further boost performance. In this paper, we propose a novel model termed OTAS, which first introduces optimal transport to achieve bidirectional and fine-grained alignment between images and questions, enabling more precise localization. Besides, OTAS incorporates a set of learnable candidate answer embeddings to query the probability of each answer class for a given image-question pair. Through Transformer attention, the candidate answer embeddings interact with the fused features of the image-question pair to make the answer decision. Extensive experiments on two widely-used benchmark datasets demonstrate the superiority of our model over state-of-the-art methods.</abstract>
      <url hash="5f952f08">2024.lrec-main.63</url>
      <bibkey>zhu-etal-2024-alignment-awareness</bibkey>
    </paper>
    <paper id="64">
      <title>Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation</title>
      <author><first>Heegon</first><last>Jin</last></author>
      <author><first>Seonil</first><last>Son</last></author>
      <author><first>Jemin</first><last>Park</last></author>
      <author><first>Youngseok</first><last>Kim</last></author>
      <author><first>Hyungjong</first><last>Noh</last></author>
      <author><first>Yeonsoo</first><last>Lee</last></author>
      <pages>722–732</pages>
      <abstract>The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation (NMT). Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the “Align-to-Distill” (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module (AAM) in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De→Dsb and WMT-2014 En→De, respectively, compared to Transformer baselines.The code and data are available at https://github.com/ncsoft/Align-to-Distill.</abstract>
      <url hash="4b7af016">2024.lrec-main.64</url>
      <bibkey>jin-etal-2024-align-distill</bibkey>
    </paper>
    <paper id="65">
      <title>A Linguistically-Informed Annotation Strategy for <fixed-case>K</fixed-case>orean Semantic Role Labeling</title>
      <author><first>Yige</first><last>Chen</last></author>
      <author><first>KyungTae</first><last>Lim</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>733–738</pages>
      <abstract>Semantic role labeling is an essential component of semantic and syntactic processing of natural languages, which reveals the predicate-argument structure of the language. Despite its importance, semantic role labeling for the Korean language has not been studied extensively. One notable issue is the lack of uniformity among data annotation strategies across different datasets, which often lack thorough rationales. In this study, we suggest an annotation strategy for Korean semantic role labeling that is in line with the previously proposed linguistic theories as well as the distinct properties of the Korean language. We further propose a simple yet viable conversion strategy from the Sejong verb dictionary to a CoNLL-style dataset for Korean semantic role labeling. Experiment results using a transformer-based sequence labeling model demonstrate the reliability and trainability of the converted dataset.</abstract>
      <url hash="4c35dc69">2024.lrec-main.65</url>
      <bibkey>chen-etal-2024-linguistically-informed</bibkey>
    </paper>
    <paper id="66">
      <title>Alleviating Exposure Bias in Abstractive Summarization via Sequentially Generating and Revising</title>
      <author><first>Jiaxin</first><last>Duan</last></author>
      <author><first>Fengyu</first><last>Lu</last></author>
      <author><first>Junfei</first><last>Liu</last></author>
      <pages>739–750</pages>
      <abstract>Abstractive summarization commonly suffers from exposure bias caused by supervised teacher-force learning, that a model predicts the next token conditioned on the accurate pre-context during training while on its preceding outputs at inference. Existing solutions bridge this gap through un- or semi-supervised holistic learning yet still leave the risk of error accumulation while generating a summary. In this paper, we attribute this problem to the limitation of unidirectional autoregressive text generation and introduce post-processing steps to alleviate it. Specifically, we reformat abstractive summarization to sequential generation and revision (SeGRe), i.e., a model in the revision phase re-inputs the generated summary and refines it by contrasting it with the source document. This provides the model additional opportunities to assess the flawed summary from a global view and thereby modify inappropriate expressions. Moreover, we train the SeGRe model with a regularized minimum-risk policy to ensure effective generation and revision. A lot of comparative experiments are implemented on two well-known datasets, exhibiting the new or matched state-of-the-art performance of SeGRe.</abstract>
      <url hash="55697581">2024.lrec-main.66</url>
      <bibkey>duan-etal-2024-alleviating-exposure</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>ALLIES</fixed-case>: A Speech Corpus for Segmentation, Speaker Diarization, Speech Recognition and Speaker Change Detection</title>
      <author><first>Marie</first><last>Tahon</last></author>
      <author><first>Anthony</first><last>Larcher</last></author>
      <author><first>Martin</first><last>Lebourdais</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Anna</first><last>Silnova</last></author>
      <author><first>Pablo</first><last>Gimeno</last></author>
      <pages>751–758</pages>
      <abstract>This paper presents ALLIES, a meta corpus which gathers and extends existing French corpora collected from radio and TV shows. The corpus contains 1048 audio files for about 500 hours of speech. Agglomeration of data is always a difficult issue, as the guidelines used to collect, annotate and transcribe speech are generally different from one corpus to another. ALLIES intends to homogenize and correct speaker labels among the different files by integrated human feedback within a speaker verification system. The main contribution of this article is the design of a protocol in order to evaluate properly speech segmentation (including music and overlap detection), speaker diarization, speech transcription and speaker change detection. As part of it, a test partition has been carefully manually 1) segmented and annotated according to speech, music, noise, speaker labels with specific guidelines for overlap speech, 2) orthographically transcribed. This article also provides as a second contribution baseline results for several speech processing tasks.</abstract>
      <url hash="54cdab6f">2024.lrec-main.67</url>
      <bibkey>tahon-etal-2024-allies-speech</bibkey>
    </paper>
    <paper id="68">
      <title>A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation</title>
      <author><first>Li</first><last>Yuan</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <author><first>Haopeng</first><last>Ren</last></author>
      <author><first>Jiexin</first><last>Wang</last></author>
      <pages>759–772</pages>
      <abstract>Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach to construct the dataset for pre-training LMPM. The experimental results highlight the effectiveness of our approach in improving the quality of entailment tree generation. By leveraging logical entailment patterns, our model produces more coherent and reasonable conclusions that closely align with the underlying premises.</abstract>
      <url hash="6753a86c">2024.lrec-main.68</url>
      <bibkey>yuan-etal-2024-logical-pattern</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>A</fixed-case>lpha<fixed-case>F</fixed-case>in: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework</title>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Chen</first><last>Shi</last></author>
      <author><first>Yong</first><last>Xu</last></author>
      <author><first>Qing</first><last>Du</last></author>
      <author><first>Mingkui</first><last>Tan</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <pages>773–783</pages>
      <abstract>The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering. Currently, machine learning and deep learning algorithms (ML&amp;DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, large language models (LLM) have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLM still suffer from hallucinations and unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data. It has positive impact on training LLM for completing financial analysis. We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques. Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis.</abstract>
      <url hash="c554bb7a">2024.lrec-main.69</url>
      <bibkey>li-etal-2024-alphafin-benchmarking</bibkey>
    </paper>
    <paper id="70">
      <title>A <fixed-case>L</fixed-case>uxembourgish Corpus as a Gender Bias Evaluation Testset</title>
      <author><first>Dimitra</first><last>Anastasiou</last></author>
      <author><first>Carole</first><last>Blond-Hanten</last></author>
      <author><first>Marie</first><last>Gallais</last></author>
      <pages>784–788</pages>
      <abstract>According to the United Nations Development Programme, gender inequality is a metric that is composed of three dimensions: reproductive health, empowerment, and the labour market. Gender inequality is an obstacle to equal opportunities in society as a whole. In this paper we present our work-in-progress of designing and playing a physical game with digital elements. We currently conduct Conversation Analysis of transcribed speech of 58567 words and documenting bias. We also test OpenAI’s ChatGPT for bias in quiz-like gender-related questions.</abstract>
      <url hash="e2d509ad">2024.lrec-main.70</url>
      <bibkey>anastasiou-etal-2024-luxembourgish-corpus</bibkey>
    </paper>
    <paper id="71">
      <title>A Matter of Perspective: Building a Multi-Perspective Annotated Dataset for the Study of Literary Quality</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Pascale Feldkamp</first><last>Moreira</last></author>
      <author><first>Ida Marie S.</first><last>Lassen</last></author>
      <author><first>Mads Rosendahl</first><last>Thomsen</last></author>
      <author><first>Kristoffer</first><last>Nielbo</last></author>
      <pages>789–800</pages>
      <abstract>Studies on literary quality have constantly stimulated the interest of critics, both in theoretical and empirical fields. To examine the perceived quality of literary works, some approaches have focused on data annotated through crowd-sourcing platforms, and others relied on available expert annotated data. In this work, we contribute to the debate by presenting a dataset collecting quality judgments on 9,000 19th and 20th century English-language literary novels by 3,150 predominantly Anglophone authors. We incorporate expert opinions and crowd-sourced annotations to allow comparative analyses between different literary quality evaluations. We also provide several textual metrics chosen for their potential connection with literary reception and engagement. While a large part of the texts is subjected to copyright, we release quality and reception measures together with stylometric and sentiment data for each of the 9,000 novels to promote future research and comparison.</abstract>
      <url hash="fbaa760f">2024.lrec-main.71</url>
      <bibkey>bizzoni-etal-2024-matter-perspective</bibkey>
    </paper>
    <paper id="72">
      <title><fixed-case>AM</fixed-case>en<fixed-case>D</fixed-case>e<fixed-case>D</fixed-case>: Modelling Concepts by Aligning Mentions, Definitions and Decontextualised Embeddings</title>
      <author><first>Amit</first><last>Gajbhiye</last></author>
      <author><first>Zied</first><last>Bouraoui</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>801–811</pages>
      <abstract>Contextualised Language Models (LM) improve on traditional word embeddings by encoding the meaning of words in context. However, such models have also made it possible to learn high-quality decontextualised concept embeddings. Three main strategies for learning such embeddings have thus far been considered: (i) fine-tuning the LM to directly predict concept embeddings from the name of the concept itself, (ii) averaging contextualised representations of mentions of the concept in a corpus, and (iii) encoding definitions of the concept. As these strategies have complementary strengths and weaknesses, we propose to learn a unified embedding space in which all three types of representations can be integrated. We show that this allows us to outperform existing approaches in tasks such as ontology completion, which heavily depends on access to high-quality concept embeddings. We furthermore find that mentions and definitions are well-aligned in the resulting space, enabling tasks such as target sense verification, even without the need for any fine-tuning.</abstract>
      <url hash="2779f699">2024.lrec-main.72</url>
      <bibkey>gajbhiye-etal-2024-amended-modelling</bibkey>
    </paper>
    <paper id="73">
      <title>A Multi-Label Dataset of <fixed-case>F</fixed-case>rench Fake News: Human and Machine Insights</title>
      <author><first>Benjamin</first><last>Icard</last></author>
      <author><first>François</first><last>Maine</last></author>
      <author><first>Morgane</first><last>Casanova</last></author>
      <author><first>Géraud</first><last>Faye</last></author>
      <author><first>Julien</first><last>Chanson</last></author>
      <author><first>Guillaume</first><last>Gadek</last></author>
      <author><first>Ghislain</first><last>Atemezing</last></author>
      <author><first>François</first><last>Bancilhon</last></author>
      <author><first>Paul</first><last>Égré</last></author>
      <pages>812–818</pages>
      <abstract>We present a corpus of 100 documents, named OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press</abstract>
      <url hash="29617c98">2024.lrec-main.73</url>
      <bibkey>icard-etal-2024-multi-label</bibkey>
    </paper>
    <paper id="74">
      <title>A Multi-layered Approach to Physical Commonsense Understanding: Creation and Evaluation of an <fixed-case>I</fixed-case>talian Dataset</title>
      <author><first>Giulia</first><last>Pensa</last></author>
      <author><first>Begoña</first><last>Altuna</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
      <pages>819–831</pages>
      <abstract>In this paper, we explore physical commonsense reasoning of large language models (LLMs) and propose a specific methodology to evaluate low-level understanding of the physical world. Specifically, the goal is to create a test set to analyze physical commonsense reasoning in large language models for Italian and focus on a trustworthy analysis of the results. To that end, we present a tiered Italian dataset, called Graded Italian Annotated dataset (GITA), written and thoroughly annotated by a professional linguist, which allows us to concentrate on three different levels of commonsense understanding. Moreover, we create a semi-automated system to complete the accurate annotation of the dataset. We also validate our dataset by carrying out three tasks with a multilingual model (XLM-RoBERTa) and propose a qualitative analysis of the results. We found out that, although the model may perform at high-level classification tasks, its easoning is inconsistent and unverifiable, since it does not capture intermediate evidence.</abstract>
      <url hash="4c9122c9">2024.lrec-main.74</url>
      <bibkey>pensa-etal-2024-multi-layered</bibkey>
    </paper>
    <paper id="75">
      <title>A Multilingual Parallel Corpus for <fixed-case>A</fixed-case>romanian</title>
      <author><first>Iulia</first><last>Petrariu</last></author>
      <author><first>Sergiu</first><last>Nisioi</last></author>
      <pages>832–838</pages>
      <abstract>We report the creation of the first high-quality corpus of Aromanian - an endangered Romance language spoken in the Balkans - and the equivalent sentence-aligned translations into Romanian, English, and French. The corpus is released publicly using several orthographic standards and consists in short stories collected in the ‘70s in Romania. Additionally, we provide an corpus-based analysis of Aromanian linguistic particularities and the overall demographic and political context which impacts the contemporary development of the language.</abstract>
      <url hash="9471a457">2024.lrec-main.75</url>
      <bibkey>petrariu-nisioi-2024-multilingual-parallel</bibkey>
    </paper>
    <paper id="76">
      <title>A Multimodal <fixed-case>F</fixed-case>rench Corpus of Aligned Speech, Text, and Pictogram Sequences for Speech-to-Pictogram Machine Translation</title>
      <author><first>Cécile</first><last>Macaire</last></author>
      <author><first>Chloé</first><last>Dion</last></author>
      <author><first>Jordan</first><last>Arrigo</last></author>
      <author><first>Claire</first><last>Lemaire</last></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>839–849</pages>
      <abstract>The automatic translation of spoken language into pictogram units can facilitate communication involving individuals with language impairments. However, there is no established translation formalism or publicly available datasets for training end-to-end speech translation systems. This paper introduces the first aligned speech, text, and pictogram translation dataset ever created in any language. We provide a French dataset that contains 230 hours of speech resources. We create a rule-based pictogram grammar with a restricted vocabulary and include a discussion of the strategic decisions involved. It takes advantage of an in-depth linguistic study of resources taken from the ARASAAC website. We validate these rules through multiple post-editing phases by expert annotators. The constructed dataset is then used to experiment with a Speech-to-Pictogram cascade model, which employs state-of-the-art Automatic Speech Recognition models. The dataset is freely available under a non-commercial licence. This marks a starting point to conduct research into the automatic translation of speech into pictogram units.</abstract>
      <url hash="64a1b41e">2024.lrec-main.76</url>
      <bibkey>macaire-etal-2024-multimodal-french</bibkey>
    </paper>
    <paper id="77">
      <title>A Multimodal In-Context Tuning Approach for <fixed-case>E</fixed-case>-Commerce Product Description Generation</title>
      <author><first>Yunxin</first><last>Li</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <author><first>Wenhan</first><last>Luo</last></author>
      <author><first>Lin</first><last>Ma</last></author>
      <author><first>Yuxin</first><last>Ding</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>850–861</pages>
      <abstract>In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing the automatic generation of product descriptions in a wide range of applications. Data and code are at https://github.com/HITsz-TMG/Multimodal-In-Context-Tuning</abstract>
      <url hash="b4e92c03">2024.lrec-main.77</url>
      <bibkey>li-etal-2024-multimodal-context</bibkey>
    </paper>
    <paper id="78">
      <title>A Multi-Task Transformer Model for Fine-grained Labelling of Chest <fixed-case>X</fixed-case>-Ray Reports</title>
      <author><first>Yuanyi</first><last>Zhu</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <author><first>Giovanni</first><last>Montana</last></author>
      <pages>862–875</pages>
      <abstract>Precise understanding of free-text radiology reports through localised extraction of clinical findings can enhance medical imaging applications like computer-aided diagnosis. We present a new task, that of segmenting radiology reports into topically meaningful passages (segments) and a transformer-based model that both segments reports into semantically coherent segments and classifies each segment using a set of 37 radiological abnormalities, thus enabling fine-grained analysis. This contrasts with prior work that performs classification on full reports without localisation. Trained on over 2.7 million unlabelled chest X-ray reports and over 28k segmented and labelled reports, our model achieves state-of-the-art performance on report segmentation (0.0442 WinDiff) and multi-label classification (0.84 report-level macro F1) over 37 radiological labels and 8 NLP-specific labels. This work establishes new benchmarks for fine-grained understanding of free-text radiology reports, with precise localisation of semantics unlocking new opportunities to improve computer vision model training and clinical decision support. We open-source our annotation tool, model code and pretrained weights to encourage future research.</abstract>
      <url hash="b5312bb3">2024.lrec-main.78</url>
      <attachment type="OptionalSupplementaryMaterial" hash="74502224">2024.lrec-main.78.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>zhu-etal-2024-multi-task</bibkey>
    </paper>
    <paper id="79">
      <title>Analysis of Sensation-transfer Dialogues in Motorsports</title>
      <author><first>Takeru</first><last>Isaka</last></author>
      <author><first>Atsushi</first><last>Otsuka</last></author>
      <author><first>Iwaki</first><last>Toshima</last></author>
      <pages>876–886</pages>
      <abstract>Clarifying the effects of subjective ideas on group performance is essential for future dialogue systems to improve mutual understanding among humans and group creativity. However, there has been little focus on dialogue research on quantitatively analyzing the effects of the quality and quantity of subjective information contained in dialogues on group performance. We hypothesize that the more subjective information interlocutors exchange, the better the group performance in collaborative work. We collected dialogues between drivers and engineers in motorsports when deciding how the car should be tuned as a suitable case to verify this hypothesis. Our analysis suggests that the greater the amount of subjective information (which we defined as “sensation”) in the driver’s utterances, the greater the race performance and driver satisfaction with the car’s tuning. The results indicate that it is essential for the development of dialogue research to create a corpus of situations that require high performance through collaboration among experts with different backgrounds but who have mastered their respective fields.</abstract>
      <url hash="3995b9a9">2024.lrec-main.79</url>
      <bibkey>isaka-etal-2024-analysis-sensation</bibkey>
    </paper>
    <paper id="80">
      <title>Analysis on Unsupervised Acquisition Process of Bilingual Vocabulary through Iterative Back-Translation</title>
      <author><first>Takuma</first><last>Tanigawa</last></author>
      <author><first>Tomoyosi</first><last>Akiba</last></author>
      <author><first>Hajime</first><last>Tsukada</last></author>
      <pages>887–892</pages>
      <abstract>In this paper, we investigate how new bilingual vocabulary is acquired through Iterative Back-Translation (IBT), which is known as a data augmentation method for machine translation from monolingual data of both source and target languages. To reveal the acquisition process, we first identify the word translation pairs in test data that do not exist in a bilingual data but do only in two monolingual data, then observe how many pairs are successfully translated by the translation model trained through IBT. We experimented on it with domain adaptation settings on two language pairs. Our experimental evaluation showed that more than 60% of the new bilingual vocabulary is successfully acquired through IBT along with the improvement in the translation quality in terms of BLEU. It also revealed that new bilingual vocabulary was gradually acquired by repeating IBT iterations. From the results, we present our hypothesis on the process of new bilingual vocabulary acquisition where the context of the words plays a critical role in the success of the acquisition.</abstract>
      <url hash="ecc2567d">2024.lrec-main.80</url>
      <bibkey>tanigawa-etal-2024-analysis-unsupervised</bibkey>
    </paper>
    <paper id="81">
      <title>Analyzing Chain-of-thought Prompting in Black-Box Large Language Models via Estimated <fixed-case>V</fixed-case>-information</title>
      <author><first>Zecheng</first><last>Wang</last></author>
      <author><first>Chunshan</first><last>Li</last></author>
      <author><first>Zhao</first><last>Yang</last></author>
      <author><first>Qingbin</first><last>Liu</last></author>
      <author><first>Yanchao</first><last>Hao</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Dianhui</first><last>Chu</last></author>
      <author><first>Dianbo</first><last>Sui</last></author>
      <pages>893–903</pages>
      <abstract>Chain-of-Thought (CoT) prompting combined with large language models (LLM) has shown great potential in improving performance on challenging reasoning tasks. While understanding why CoT prompting is effective is crucial for the application and improvement of CoT prompting, few studies have addressed this issue. Besides, almost no prior work has conducted theoretical analysis on CoT prompting in the context of black-box models. In this paper, we approach the analysis of CoT prompting in black-box LLMs from an information-theoretic perspective. Specifically, we propose a new metric, EPVI (Estimated Pointwise V-Information), which extends the concept of pointwise V-information to black-box models, quantifying the label-relevant new information introduced by CoT prompting beyond the pre-existing information in the input. Based on this, we conduct a series of experiments at both the task and instance levels to analyze CoT prompting, demonstrating that the effectiveness of CoT prompting can be attributed to its capacity to influence the difficulty of model inference by augmenting or reducing the model-usable information. Furthermore, we show that selecting high-quality demonstrations of CoT reasoning based on EPVI can improve the downstream performance of reasoning tasks.</abstract>
      <url hash="5300b4a0">2024.lrec-main.81</url>
      <bibkey>wang-etal-2024-analyzing-chain</bibkey>
    </paper>
    <paper id="82">
      <title>Analyzing Effects of Learning Downstream Tasks on Moral Bias in Large Language Models</title>
      <author><first>Niklas</first><last>Kiehne</last></author>
      <author><first>Alexander</first><last>Ljapunov</last></author>
      <author><first>Marc</first><last>Bätje</last></author>
      <author><first>Wolf-Tilo</first><last>Balke</last></author>
      <pages>904–923</pages>
      <abstract>Pre-training and fine-tuning large language models (LMs) is currently the state-of-the-art methodology for enabling data-scarce downstream tasks. However, the derived models still tend to replicate and perpetuate social biases. To understand this process in more detail, this paper investigates the actual effects of learning downstream tasks on moral bias in LMs. We develop methods to assess the agreement of LMs to explicitly codified norms in both pre-training and fine-tuning stages. Even if a pre-trained foundation model exhibits consistent norms, we find that introducing downstream tasks may indeed lead to unexpected inconsistencies in norm representation. Specifically, we observe two phenomena during fine-tuning across both masked and causal LMs: (1) pre-existing moral bias may be mitigated or amplified even when presented with opposing views and (2) prompt sensitivity may be negatively impacted. We provide empirical evidence of models deteriorating into conflicting states, where contradictory answers can easily be triggered by slight modifications in the input sequence. Our findings thus raise concerns about the general ability of LMs to mitigate moral biases effectively.</abstract>
      <url hash="019d79b3">2024.lrec-main.82</url>
      <bibkey>kiehne-etal-2024-analyzing-effects</bibkey>
    </paper>
    <paper id="83">
      <title>Analyzing Homonymy Disambiguation Capabilities of Pretrained Language Models</title>
      <author><first>Lorenzo</first><last>Proietti</last></author>
      <author><first>Stefano</first><last>Perrella</last></author>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Giulia</first><last>Vulpis</last></author>
      <author><first>Leonardo</first><last>Lavalle</last></author>
      <author><first>Andrea</first><last>Sanchietti</last></author>
      <author><first>Andrea</first><last>Ferrari</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>924–938</pages>
      <abstract>Word Sense Disambiguation (WSD) is a key task in Natural Language Processing (NLP), aiming to assign the correct meaning (sense) to a word in context. However, traditional WSD systems rely on WordNet as the underlying sense inventory, often differentiating meticulously between subtle nuances of word meanings, which may lead to excessive complexity and reduced practicality of WSD systems in today’s NLP. Indeed, current Pretrained Language Models (PLMs) do seem to be able to perform disambiguation, but it is not clear to what extent, or to what level of granularity, they actually operate. In this paper, we address these points and, firstly, introduce a new large-scale resource that leverages homonymy relations to systematically cluster WordNet senses, effectively reducing the granularity of word senses to a very coarse-grained level; secondly, we use this resource to train Homonymy Disambiguation systems and investigate whether PLMs are inherently able to differentiate coarse-grained word senses. Our findings demonstrate that, while state-of-the-art models still struggle to choose the correct fine-grained meaning of a word in context, Homonymy Disambiguation systems are able to differentiate homonyms with up to 95% accuracy scores even without fine-tuning the underlying PLM. We release our data and code at https://github.com/SapienzaNLP/homonymy-wsd.</abstract>
      <url hash="f68f3d37">2024.lrec-main.83</url>
      <bibkey>proietti-etal-2024-analyzing-homonymy</bibkey>
    </paper>
    <paper id="84">
      <title>Analyzing Interpretability of Summarization Model with Eye-gaze Information</title>
      <author><first>Fariz</first><last>Ikhwantri</last></author>
      <author><first>Hiroaki</first><last>Yamada</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <pages>939–950</pages>
      <abstract>Interpretation methods provide saliency scores indicating the importance of input words for neural summarization models. Prior work has analyzed models by comparing them to human behavior, often using eye-gaze as a proxy for human attention in reading tasks such as classification. This paper presents a framework to analyze the model behavior in summarization by comparing it to human summarization behavior using eye-gaze data. We examine two research questions: RQ1) whether model saliency conforms to human gaze during summarization and RQ2) how model saliency and human gaze affect summarization performance. For RQ1, we measure conformity by calculating the correlation between model saliency and human fixation counts. For RQ2, we conduct ablation experiments removing words/sentences considered important by models or humans. Experiments on two datasets with human eye-gaze during summarization partially confirm that model saliency aligns with human gaze (RQ1). However, ablation experiments show that removing highly-attended words/sentences from the human gaze does not significantly degrade performance compared with the removal by the model saliency (RQ2).</abstract>
      <url hash="1dc05382">2024.lrec-main.84</url>
      <bibkey>ikhwantri-etal-2024-analyzing-interpretability</bibkey>
    </paper>
    <paper id="85">
      <title>Analyzing Large Language Models’ Capability in Location Prediction</title>
      <author><first>Zhaomin</first><last>Xiao</last></author>
      <author><first>Yan</first><last>Huang</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>951–958</pages>
      <abstract>In this paper, we investigate and evaluate large language models’ capability in location prediction. We present experimental results with four models—FLAN-T5, FLAN-UL2, FLAN-Alpaca, and ChatGPT—in various instruction finetuning and exemplar settings. We analyze whether taking into account the context—tweets published before and after the tweet mentioning a location—is beneficial. Additionally, we conduct an ablation study to explore whether instruction modification is beneficial. Lastly, our qualitative analysis sheds light on the errors made by the best-performing model.</abstract>
      <url hash="c857c024">2024.lrec-main.85</url>
      <bibkey>xiao-etal-2024-analyzing-large</bibkey>
    </paper>
    <paper id="86">
      <title>Analyzing Occupational Distribution Representation in <fixed-case>J</fixed-case>apanese Language Models</title>
      <author><first>Katsumi</first><last>Ibaraki</last></author>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>959–973</pages>
      <abstract>Recent advances in large language models (LLMs) have enabled users to generate fluent and seemingly convincing text. However, these models have uneven performance in different languages, which is also associated with undesirable societal biases toward marginalized populations. Specifically, there is relatively little work on Japanese models, despite it being the thirteenth most widely spoken language. In this work, we first develop three Japanese language prompts to probe LLMs’ understanding of Japanese names and their association between gender and occupations. We then evaluate a variety of English, multilingual, and Japanese models, correlating the models’ outputs with occupation statistics from the Japanese Census Bureau from the last 100 years. Our findings indicate that models can associate Japanese names with the correct gendered occupations when using constrained decoding. However, with sampling or greedy decoding, Japanese language models have a preference for a small set of stereotypically gendered occupations, and multilingual models, though trained on Japanese, are not always able to understand Japanese prompts.</abstract>
      <url hash="15364c47">2024.lrec-main.86</url>
      <bibkey>ibaraki-etal-2024-analyzing-occupational</bibkey>
    </paper>
    <paper id="87">
      <title>Analyzing Symptom-based Depression Level Estimation through the Prism of Psychiatric Expertise</title>
      <author><first>Navneet</first><last>Agarwal</last></author>
      <author><first>Kirill</first><last>Milintsevich</last></author>
      <author><first>Lucie</first><last>Metivier</last></author>
      <author><first>Maud</first><last>Rotharmel</last></author>
      <author><first>Gaël</first><last>Dias</last></author>
      <author><first>Sonia</first><last>Dollfus</last></author>
      <pages>974–983</pages>
      <abstract>The ever-growing number of people suffering from mental distress has motivated significant research initiatives towards automated depression estimation. Despite the multidisciplinary nature of the task, very few of these approaches include medical professionals in their research process, thus ignoring a vital source of domain knowledge. In this paper, we propose to bring the domain experts back into the loop and incorporate their knowledge within the gold-standard DAIC-WOZ dataset. In particular, we define a novel transformer-based architecture and analyse its performance in light of our expert annotations. Overall findings demonstrate a strong correlation between the psychological tendencies of medical professionals and the behavior of the proposed model, which additionally provides new state-of-the-art results.</abstract>
      <url hash="aa744777">2024.lrec-main.87</url>
      <bibkey>agarwal-etal-2024-analyzing-symptom</bibkey>
    </paper>
    <paper id="88">
      <title>Analyzing the Dynamics of Climate Change Discourse on <fixed-case>T</fixed-case>witter: A New Annotated Corpus and Multi-Aspect Classification</title>
      <author><first>Shuvam</first><last>Shiwakoti</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Kritesh</first><last>Rauniyar</last></author>
      <author><first>Akshyat</first><last>Shah</last></author>
      <author><first>Aashish</first><last>Bhandari</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>984–994</pages>
      <abstract>The discourse surrounding climate change on social media platforms has emerged as a significant avenue for understanding public sentiments, perspectives, and engagement with this critical global issue. The unavailability of publicly available datasets, coupled with ignoring the multi-aspect analysis of climate discourse on social media platforms, has underscored the necessity for further advancement in this area. To address this gap, in this paper, we present an extensive exploration of the intricate realm of climate change discourse on Twitter, leveraging a meticulously annotated <i>ClimaConvo</i> dataset comprising 15,309 tweets. Our annotations encompass a rich spectrum, including aspects like relevance, stance, hate speech, the direction of hate, and humor, offering a nuanced understanding of the discourse dynamics. We address the challenges inherent in dissecting online climate discussions and detail our comprehensive annotation methodology. In addition to annotations, we conduct benchmarking assessments across various algorithms for six tasks: relevance detection, stance detection, hate speech identification, direction and target, and humor analysis. This assessment enhances our grasp of sentiment fluctuations and linguistic subtleties within the discourse. Our analysis extends to exploratory data examination, unveiling tweet distribution patterns, stance prevalence, and hate speech trends. Employing sophisticated topic modeling techniques uncovers underlying thematic clusters, providing insights into the diverse narrative threads woven within the discourse. The findings present a valuable resource for researchers, policymakers, and communicators seeking to navigate the intricacies of climate change discussions. The dataset and resources for this paper are available at https://github.com/shucoll/ClimaConvo.</abstract>
      <url hash="5395bdd3">2024.lrec-main.88</url>
      <bibkey>shiwakoti-etal-2024-analyzing-dynamics</bibkey>
    </paper>
    <paper id="89">
      <title>Analyzing the Performance of Large Language Models on Code Summarization</title>
      <author><first>Rajarshi</first><last>Haldar</last></author>
      <author><first>Julia</first><last>Hockenmaier</last></author>
      <pages>995–1008</pages>
      <abstract>Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance of these models on individual examples often depends on the amount of (subword) token overlap between the code and the corresponding reference natural language descriptions in the dataset. This token overlap arises because the reference descriptions in standard datasets (corresponding to docstrings in large code bases) are often highly similar to the names of the functions they describe. We also show that this token overlap occurs largely in the function names of the code and compare the relative performance of these models after removing function names versus removing code structure. We also show that using multiple evaluation metrics like BLEU and BERTScore gives us very little additional insight since these metrics are highly correlated with each other.</abstract>
      <url hash="c1778137">2024.lrec-main.89</url>
      <bibkey>haldar-hockenmaier-2024-analyzing-performance</bibkey>
    </paper>
    <paper id="90">
      <title>Analyzing the Understanding of Morphologically Complex Words in Large Language Models</title>
      <author><first>Marion</first><last>Weller-Di Marco</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>1009–1020</pages>
      <abstract>We empirically study the ability of a Large Language Model (gpt-3.5-turbo-instruct) to understand morphologically complex words. In our experiments, we looked at a variety of tasks to analyse German compounds with regard to compositional word formation and derivation, such as identifying the head noun of existing and novel compounds, identifying the shared verb stem between two words, or recognizing words constructed with inappropriately used derivation morphemes as invalid. Our results show that the language model is generally capable of solving most tasks, except for the task of identifying ill-formed word forms. While the model demonstrated a good overall understanding of complex words and their word-internal structure, the results also suggest that there is no formal knowledge of derivational rules, but rather an interpretation of the observed word parts to derive the meaning of a word.</abstract>
      <url hash="d15da0a6">2024.lrec-main.90</url>
      <bibkey>weller-di-marco-fraser-2024-analyzing-understanding</bibkey>
    </paper>
    <paper id="91">
      <title>An Argument for Symmetric Coordination from Dependency Length Minimization: A Replication Study</title>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <author><first>Magdalena</first><last>Borysiak</last></author>
      <author><first>Adam</first><last>Głowacki</last></author>
      <pages>1021–1033</pages>
      <abstract>It is well known that left conjuncts tend to be shorter in English coordinate structures. On the basis of Penn Treebank, Przepiórkowski and Woźniak 2023 (in ACL 2023 proceedings) show that this tendency depends on the difference between lengths of conjuncts: the larger the difference, the stronger the tendency for the shorter conjunct to occur on the left. However, this dynamics is observed only when the governor of the coordinate structure is on the left of the coordination (e.g., “Bring apples and oranges!”) or when it is absent (e.g., “Come and sing!”), and not when it is on the right (e.g., “Apples and oranges fell”). Given the principle of Dependency Length Minimization, this turns out to provide an argument for the symmetric structure of coordination. We replicate and sharpen this result on the basis of a much larger dataset: parts of the COCA corpus parsed with Stanza. We also investigate the dependence of this result on the assumed unit of length (word vs. character) and on genre.</abstract>
      <url hash="f7b637f6">2024.lrec-main.91</url>
      <bibkey>przepiorkowski-etal-2024-argument-symmetric</bibkey>
    </paper>
    <paper id="92">
      <title>A Natural Approach for Synthetic Short-Form Text Analysis</title>
      <author><first>Ruiting</first><last>Shao</last></author>
      <author><first>Ryan</first><last>Schwarz</last></author>
      <author><first>Christopher</first><last>Clifton</last></author>
      <author><first>Edward</first><last>Delp</last></author>
      <pages>1034–1042</pages>
      <abstract>Detecting synthetically generated text in the wild has become increasingly difficult with advances in Natural Language Generation techniques and the proliferation of freely available Large Language Models (LLMs). Social media and news sites can be flooded with synthetically generated misinformation via tweets and posts while authentic users can inadvertently spread this text via shares and retweets. Most modern natural language processing techniques designed to detect synthetically generated text focus primarily on long-form content, such as news articles, or incorporate stylometric characteristics and metadata during their analysis. Unfortunately, for short form text like tweets, this information is often unavailable, usually detached from its original source, displayed out of context, and is often too short or informal to yield significant information from stylometry. This paper proposes a method of detecting synthetically generated tweets via a Transformer architecture and incorporating unique style-based features. Additionally, we have created a new dataset consisting of human-generated and Large Language Model generated tweets for 4 topics and another dataset consisting of tweets paraphrased by 3 different paraphrase models.</abstract>
      <url hash="615484e8">2024.lrec-main.92</url>
      <bibkey>shao-etal-2024-natural-approach</bibkey>
    </paper>
    <paper id="93">
      <title>An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation</title>
      <author><first>Ahmet</first><last>Gunduz</last></author>
      <author><first>Kamer Ali</first><last>Yuksel</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Golara</first><last>Javadi</last></author>
      <author><first>Fabio</first><last>Minazzi</last></author>
      <author><first>Nicola</first><last>Sobieski</last></author>
      <author><first>Sébastien</first><last>Bratières</last></author>
      <pages>1043–1051</pages>
      <abstract>Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application aims to streamline the dataset creation process for TTS models through these features, thereby facilitating advancements in voice-based technologies.</abstract>
      <url hash="ac3e45d6">2024.lrec-main.93</url>
      <bibkey>gunduz-etal-2024-automated-end</bibkey>
    </paper>
    <paper id="94">
      <title>Anchor and Broadcast: An Efficient Concept Alignment Approach for Evaluation of Semantic Graphs</title>
      <author><first>Haibo</first><last>Sun</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>1052–1062</pages>
      <abstract>In this paper, we present AnCast, an intuitive and efficient tool for evaluating graph-based meaning representations (MR). AnCast implements evaluation metrics that are well understood in the NLP community, and they include concept F1, unlabeled relation F1, labeled relation F1, and weighted relation F1. The efficiency of the tool comes from a novel anchor broadcast alignment algorithm that is not subject to the trappings of local maxima. We show through experimental results that the AnCast score is highly correlated with the widely used Smatch score, but its computation takes only about 40% the time.</abstract>
      <url hash="47b4b6e9">2024.lrec-main.94</url>
      <bibkey>sun-xue-2024-anchor-broadcast</bibkey>
    </paper>
    <paper id="95">
      <title>An Effective Span-based Multimodal Named Entity Recognition with Consistent Cross-Modal Alignment</title>
      <author><first>Yongxiu</first><last>Xu</last></author>
      <author><first>Hao</first><last>Xu</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Shiyao</first><last>Cui</last></author>
      <author><first>Minghao</first><last>Tang</last></author>
      <author><first>Longzheng</first><last>Wang</last></author>
      <author><first>Hongbo</first><last>Xu</last></author>
      <pages>1063–1072</pages>
      <abstract>With the increasing availability of multimodal content on social media, consisting primarily of text and images, multimodal named entity recognition (MNER) has gained a wide-spread attention. A fundamental challenge of MNER lies in effectively aligning different modalities. However, the majority of current approaches rely on word-based sequence labeling framework and align the image and text at inconsistent semantic levels (whole image-words or regions-words). This misalignment may lead to inferior entity recognition performance. To address this issue, we propose an effective span-based method, named SMNER, which achieves a more consistent multimodal alignment from the perspectives of information-theoretic and cross-modal interaction, respectively. Specifically, we first introduce a cross-modal information bottleneck module for the global-level multimodal alignment (whole image-whole text). This module aims to encourage the semantic distribution of the image to be closer to the semantic distribution of the text, which can enable the filtering out of visual noise. Next, we introduce a cross-modal attention module for the local-level multimodal alignment (regions-spans), which captures the correlations between regions in the image and spans in the text, enabling a more precise alignment of the two modalities. Extensive ex- periments conducted on two benchmark datasets demonstrate that SMNER outperforms the state-of-the-art baselines.</abstract>
      <url hash="13a5e34f">2024.lrec-main.95</url>
      <bibkey>xu-etal-2024-effective-span</bibkey>
    </paper>
    <paper id="96">
      <title>An Empirical Study of Synthetic Data Generation for Implicit Discourse Relation Recognition</title>
      <author><first>Kazumasa</first><last>Omura</last></author>
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>1073–1085</pages>
      <abstract>Implicit Discourse Relation Recognition (IDRR), which is the task of recognizing the semantic relation between given text spans that do not contain overt clues, is a long-standing and challenging problem. In particular, the paucity of training data for some error-prone discourse relations makes the problem even more challenging. To address this issue, we propose a method of generating synthetic data for IDRR using a large language model. The proposed method is summarized as two folds: extraction of confusing discourse relation pairs based on false negative rate and synthesis of data focused on the confusion. The key points of our proposed method are utilizing a confusion matrix and adopting two-stage prompting to obtain effective synthetic data. According to the proposed method, we generated synthetic data several times larger than training examples for some error-prone discourse relations and incorporated it into training. As a result of experiments, we achieved state-of-the-art macro-F1 performance thanks to the synthetic data without sacrificing micro-F1 performance and demonstrated its positive effects especially on recognizing some infrequent discourse relations.</abstract>
      <url hash="d0aa0823">2024.lrec-main.96</url>
      <bibkey>omura-etal-2024-empirical-study</bibkey>
    </paper>
    <paper id="97">
      <title>An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation</title>
      <author><first>Supryadi</first><last>Supryadi</last></author>
      <author><first>Leiyu</first><last>Pan</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>1086–1097</pages>
      <abstract>Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages. In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise. To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation. This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes. We conduct both automatic and human evaluations. Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators. The dataset is publicly available at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval.</abstract>
      <url hash="ac523217">2024.lrec-main.97</url>
      <bibkey>supryadi-etal-2024-empirical-study</bibkey>
    </paper>
    <paper id="98">
      <title>An Evaluation of <fixed-case>C</fixed-case>roatian <fixed-case>ASR</fixed-case> Models for Čakavian Transcription</title>
      <author><first>Shulin</first><last>Zhang</last></author>
      <author><first>John</first><last>Hale</last></author>
      <author><first>Margaret</first><last>Renwick</last></author>
      <author><first>Zvjezdana</first><last>Vrzić</last></author>
      <author><first>Keith</first><last>Langston</last></author>
      <pages>1098–1104</pages>
      <abstract>To assist in the documentation of Čakavian, an endangered language variety closely related to Croatian, we test four currently available ASR models that are trained with Croatian data and assess their performance in the transcription of Čakavian audio data. We compare the models’ word error rates, analyze the word-level error types, and showcase the most frequent Deletion and Substitution errors. The evaluation results indicate that the best-performing system for transcribing Čakavian was a CTC-based variant of the Conformer model.</abstract>
      <url hash="008545db">2024.lrec-main.98</url>
      <bibkey>zhang-etal-2024-evaluation-croatian</bibkey>
    </paper>
    <paper id="99">
      <title>An Event-based Abductive Learning for Hard Time-sensitive Question Answering</title>
      <author><first>Shaojuan</first><last>Wu</last></author>
      <author><first>Jitong</first><last>Li</last></author>
      <author><first>Xiaowang</first><last>Zhang</last></author>
      <author><first>Zhiyong</first><last>Feng</last></author>
      <pages>1105–1115</pages>
      <abstract>Time-Sensitive Question Answering (TSQA) is to answer questions qualified for a certain timestamp based on the given document. It is split into easy and hard modes depending on whether the document contain time qualifiers mentioned in the question. While existing models have performed well on easy mode, their performance is significant reduced for answering hard time-sensitive questions, whose time qualifiers are implicit in the document. An intuitive idea is to match temporal events in the given document by treating time-sensitive question as a temporal event of missing objects. However, not all temporal events extracted from the document have explicit time qualifiers. In this paper, we propose an Event-AL framework, in which a graph pruning model is designed to locate the timespan of implicit temporal events by capturing temporal relation between events. Moreover, we present an abductive reasoning module to determine proper objects while providing explanations. Besides, as the same relation may be scattered throughout the document in diverse expressions, a relation-based prompt is introduced to instructs LLMs in extracting candidate temporal events. We conduct extensive experiment and results show that Event-AL outperforms strong baselines for hard time-sensitive questions, with a 12.7% improvement in EM scores. In addition, it also exhibits great superiority for multi-answer and beyond hard time-sensitive questions.</abstract>
      <url hash="c4cf1ba2">2024.lrec-main.99</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8b8372e3">2024.lrec-main.99.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>wu-etal-2024-event-based</bibkey>
    </paper>
    <paper id="100">
      <title>A New Massive Multilingual Dataset for High-Performance Language Technologies</title>
      <author><first>Ona</first><last>de Gibert</last></author>
      <author><first>Graeme</first><last>Nail</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Marta</first><last>Bañón</last></author>
      <author><first>Jelmer</first><last>van der Linde</last></author>
      <author><first>Shaoxiong</first><last>Ji</last></author>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last></author>
      <author><first>Mikko</first><last>Aulamo</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>1116–1128</pages>
      <abstract>We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive. We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing. Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ≈ 5.6 trillion word tokens de-duplicated on the document level. Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens. The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and machine translation training. We publicly release the corpora, the software, and the tools used in this work.</abstract>
      <url hash="755abc1c">2024.lrec-main.100</url>
      <bibkey>de-gibert-etal-2024-new-massive</bibkey>
    </paper>
    <paper id="101">
      <title>An <fixed-case>LCF</fixed-case>-<fixed-case>IDF</fixed-case> Document Representation Model Applied to Long Document Classification</title>
      <author><first>Renzo Arturo</first><last>Alva Principe</last></author>
      <author><first>Nicola</first><last>Chiarini</last></author>
      <author><first>Marco</first><last>Viviani</last></author>
      <pages>1129–1135</pages>
      <abstract>A document representation model that has been used for years in NLP and Text Mining tasks is TF-IDF (Term Frequency-Inverse Document Frequency). This model is indeed effective for various tasks like Information Retrieval and Document Classification. However, it may fall short when it comes to capturing the deeper semantic and contextual meaning of a text, which is where Transformer-based Pre-trained Language Models (PLMs) such as BERT have been gaining significant traction in recent years. Despite this, these models also face specific challenges related to Transformers and their attention mechanism limits, especially when dealing with long documents. Therefore, this paper proposes a novel approach to exploit the advantages of the TF-IDF representation while incorporating semantic context, by introducing a Latent Concept Frequency-Inverse Document Frequency (LCF-IDF) document representation model. Its effectiveness is tested with respect to the Long Document Classification task. The results obtained show promising performance of the proposed solution compared to TF-IDF and BERT-like representation models, including those specifically for long documents such as Longformer as well as those designed for particular domains, especially when it comes to Single Label Multi-Class (SLMC) classification.</abstract>
      <url hash="f84c2b36">2024.lrec-main.101</url>
      <bibkey>alva-principe-etal-2024-lcf-idf</bibkey>
    </paper>
    <paper id="102">
      <title>An <fixed-case>LLM</fixed-case>-Enhanced Adversarial Editing System for Lexical Simplification</title>
      <author><first>Keren</first><last>Tan</last></author>
      <author><first>Kangyang</first><last>Luo</last></author>
      <author><first>Yunshi</first><last>Lan</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Jinlong</first><last>Shu</last></author>
      <pages>1136–1146</pages>
      <abstract>Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="83850e34">2024.lrec-main.102</url>
      <bibkey>tan-etal-2024-llm-enhanced</bibkey>
    </paper>
    <paper id="103">
      <title><fixed-case>A</fixed-case>nno<fixed-case>CTR</fixed-case>: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports</title>
      <author><first>Lukas</first><last>Lange</last></author>
      <author><first>Marc</first><last>Müller</last></author>
      <author><first>Ghazaleh</first><last>Haratinezhad Torbati</last></author>
      <author><first>Dragan</first><last>Milchevski</last></author>
      <author><first>Patrick</first><last>Grau</last></author>
      <author><first>Subhash Chandra</first><last>Pujari</last></author>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <pages>1147–1160</pages>
      <abstract>Monitoring the threat landscape to be aware of actual or potential attacks is of utmost importance to cybersecurity professionals. Information about cyber threats is typically distributed using natural language reports. Natural language processing can help with managing this large amount of unstructured information, yet to date, the topic has received little attention. With this paper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat reports. The reports have been annotated by a domain expert with named entities, temporal expressions, and cybersecurity-specific concepts including implicitly mentioned techniques and tactics. Entities and concepts are linked to Wikipedia and the MITRE ATT&amp;CK knowledge base, the most widely-used taxonomy for classifying types of attacks. Prior datasets linking to MITRE ATT&amp;CK either provide a single label per document or annotate sentences out-of-context; our dataset annotates entire documents in a much finer-grained way. In an experimental study, we model the annotations of our dataset using state-of-the-art neural models. In our few-shot scenario, we find that for identifying the MITRE ATT&amp;CK concepts that are mentioned explicitly or implicitly in a text, concept descriptions from MITRE ATT&amp;CK are an effective source for training data augmentation.</abstract>
      <url hash="d8827969">2024.lrec-main.103</url>
      <bibkey>lange-etal-2024-annoctr-dataset</bibkey>
    </paper>
    <paper id="104">
      <title>Annotate <fixed-case>C</fixed-case>hinese Aspect with <fixed-case>UMR</fixed-case>——a Case Study on the Liitle Prince</title>
      <author><first>Sijia</first><last>Ge</last></author>
      <author><first>Zilong</first><last>Li</last></author>
      <author><first>Alvin Po-Chun</first><last>Chen</last></author>
      <author><first>Guanchao</first><last>Wang</last></author>
      <pages>1161–1172</pages>
      <abstract>Aspect is a valuable tool for determining the perspective from which an event is observed, allowing for viewing both at the situation and viewpoint level. Uniform Meaning Representation (UMR) seeks to provide a standard, typologically-informed representation of aspects across languages. It employs an aspectual lattice to adapt to different languages and design values that encompass both viewpoint aspect and situation aspects. In the context of annotating the Chinese version of The Little Prince, we paid particular attention to the interactions between aspect values and aspect markers and we also want to know the annotation effectiveness and challenges under the UMR aspectual lattice. During our annotation process, we identified the relationships between aspectual markers and labels. We further categorized and analyzed complex examples that led to low inter-annotator agreement. The factors contributing to disagreement among annotators included the interpretations of lexical semantics, implications, and the influence of aspectual markers, which is related to the inclination of the situation aspect and the exclusivity between the two aspects’ perspectives. Overall, our work sheds light on the challenges of aspect annotation in Chinese and highlights the need for more comprehensive guidelines.</abstract>
      <url hash="f5339593">2024.lrec-main.104</url>
      <bibkey>ge-etal-2024-annotate-chinese</bibkey>
    </paper>
    <paper id="105">
      <title>Annotate the Way You Think: An Incremental Note Generation Framework for the Summarization of Medical Conversations</title>
      <author><first>Longxiang</first><last>Zhang</last></author>
      <author><first>Caleb D.</first><last>Hart</last></author>
      <author><first>Susanne</first><last>Burger</last></author>
      <author><first>Thomas</first><last>Schaaf</last></author>
      <pages>1173–1186</pages>
      <abstract>The scarcity of public datasets for the summarization of medical conversations has been a limiting factor for advancing NLP research in the healthcare domain, and the structure of the existing data is largely limited to the simple format of conversation-summary pairs. We therefore propose a novel Incremental Note Generation (ING) annotation framework capable of greatly enriching summarization datasets in the healthcare domain and beyond. Our framework is designed to capture the human summarization process via an annotation task by instructing the annotators to first incrementally create a draft note as they accumulate information through a conversation transcript (Generation) and then polish the draft note into a reference note (Rewriting). The annotation results include both the reference note and a comprehensive editing history of the draft note in tabular format. Our pilot study on the task of SOAP note generation showed reasonable consistency between four expert annotators, established a solid baseline for quantitative targets of inter-rater agreement, and demonstrated the ING framework as an improvement over the traditional annotation process for future modeling of summarization.</abstract>
      <url hash="30f259ec">2024.lrec-main.105</url>
      <bibkey>zhang-etal-2024-annotate-way</bibkey>
    </paper>
    <paper id="106">
      <title>Annotating <fixed-case>C</fixed-case>hinese Word Senses with <fixed-case>E</fixed-case>nglish <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et: A Practice on <fixed-case>O</fixed-case>nto<fixed-case>N</fixed-case>otes <fixed-case>C</fixed-case>hinese Sense Inventories</title>
      <author><first>Hongzhi</first><last>Xu</last></author>
      <author><first>Jingxia</first><last>Lin</last></author>
      <author><first>Sameer</first><last>Pradhan</last></author>
      <author><first>Mitchell</first><last>Marcus</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <pages>1187–1196</pages>
      <abstract>In this paper, we present our exploration of annotating Chinese word senses using English WordNet synsets, with examples extracted from OntoNotes Chinese sense inventories. Given a target word along with the example that contains it, the annotators select a WordNet synset that best describes the meaning of the target word in the context. The result demonstrates an inter-annotator agreement of 38% between two annotators. We delve into the instances of disagreement by comparing the two annotated synsets, including their positions within the WordNet hierarchy. The examination reveals intriguing patterns among closely related synsets, shedding light on similar concepts represented within the WordNet structure. The data offers as an indirect linking of Chinese word senses defined in OntoNotes Chinese sense inventories to WordNet sysnets, and thus promotes the value of the OntoNotes corpus. Compared to a direct linking of Chinese word senses to WordNet synsets, the example-based annotation has the merit of not being affected by inaccurate sense definitions and thus offers a new way of mapping WordNets of different languages. At the same time, the annotated data also serves as a valuable linguistic resource for exploring potential lexical differences between English and Chinese, with potential contributions to the broader understanding of cross-linguistic semantic mapping</abstract>
      <url hash="588673b8">2024.lrec-main.106</url>
      <bibkey>xu-etal-2024-annotating-chinese</bibkey>
    </paper>
    <paper id="107">
      <title>Annotating Customer-Oriented Behaviour in Call Centre Sales Dialogues</title>
      <author><first>Jutta</first><last>Stock</last></author>
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>1197–1208</pages>
      <abstract>Customer-oriented behaviour (COB) plays an important role in call centre interactions, particularly in the context of successful sales negotiation. However, the evaluation of COB in customer-agent conversations often lacks clarity in its definition and robust computational assessment methods. This paper addresses these challenges by presenting a comprehensive conceptual and empirical framework. We conducted multidimensional dialogue act annotations on authentic call centre interactions using the ISO 24617-2 taxonomy, capturing the multifaceted nature of these interactions. This process led to the identification of relevant dialogue act categories, proposed extensions concerning relationship-building aspects, and derived corpus statistics. The findings highlight specific facets of COB that positively impact on Customer Satisfaction (CS), as determined through correlation analysis. Additionally, we delved into the dependencies between COB and feedback acts, leveraging the hierarchical structure of the DIT++ model. This framework improves our understanding of the dynamics shaping sales strategies in call centres and holds promise for practical applications in optimising customer-agent interactions.</abstract>
      <url hash="cf4ef0e3">2024.lrec-main.107</url>
      <bibkey>stock-etal-2024-annotating-customer</bibkey>
    </paper>
    <paper id="108">
      <title>Annotation and Classification of Relevant Clauses in Terms-and-Conditions Contracts</title>
      <author><first>Pietro Giovanni</first><last>Bizzaro</last></author>
      <author><first>Elena</first><last>Della Valentina</last></author>
      <author><first>Maurizio</first><last>Napolitano</last></author>
      <author><first>Nadia</first><last>Mana</last></author>
      <author><first>Massimo</first><last>Zancanaro</last></author>
      <pages>1209–1214</pages>
      <abstract>In this paper, we propose a new annotation scheme to classify different types of clauses in Terms-and-Conditions contracts with the ultimate goal of supporting legal experts to quickly identify and assess problematic issues in this type of legal documents. To this end, we built a small corpus of Terms-and-Conditions contracts and finalized an annotation scheme of 14 categories, eventually reaching an inter-annotator agreement of 0.92. Then, for 11 of them, we experimented with binary classification tasks using few-shot prompting with a multilingual T5 and two fine-tuned versions of two BERT-based LLMs for Italian. Our experiments showed the feasibility of automatic classification of our categories by reaching accuracies ranging from .79 to .95 on validation tasks.</abstract>
      <url hash="91dcfcb6">2024.lrec-main.108</url>
      <bibkey>bizzaro-etal-2024-annotation-classification</bibkey>
    </paper>
    <paper id="109">
      <title>Annotation of <fixed-case>J</fixed-case>apanese Discourse Relations Focusing on Concessive Inferences</title>
      <author><first>Ai</first><last>Kubota</last></author>
      <author><first>Takuma</first><last>Sato</last></author>
      <author><first>Takayuki</first><last>Amamoto</last></author>
      <author><first>Ryota</first><last>Akiyoshi</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <pages>1215–1224</pages>
      <abstract>In this study, we focus on the inference presupposed in the concessive discourse relation and present the discourse relation annotation for the Japanese connectives ‘nagara’ and ‘tsutsu’, both of which have two usages: Synchronous and Concession, just like English while. We also present the annotation for ‘tokorode’, which is ambiguous in three ways: Temporal, Location, and Concession. While corpora containing concessive discourse relations already exist, the distinctive feature of our study is that it aims to identify the concessive inferential relations by writing out the implicit presupposed inferences. In this paper, we report on the annotation methodology and its results, as well as the characteristics of concession that became apparent during annotation.</abstract>
      <url hash="4fa790df">2024.lrec-main.109</url>
      <bibkey>kubota-etal-2024-annotation-japanese</bibkey>
    </paper>
    <paper id="110">
      <title>Annotation of Transition-Relevance Places and Interruptions for the Description of Turn-Taking in Conversations in <fixed-case>F</fixed-case>rench Media Content</title>
      <author><first>Rémi</first><last>Uro</last></author>
      <author><first>Marie</first><last>Tahon</last></author>
      <author><first>Jane</first><last>Wottawa</last></author>
      <author><first>David</first><last>Doukhan</last></author>
      <author><first>Albert</first><last>Rilliard</last></author>
      <author><first>Antoine</first><last>Laurent</last></author>
      <pages>1225–1232</pages>
      <abstract>Few speech resources describe interruption phenomena, especially for TV and media content. The description of these phenomena may vary across authors: it thus leaves room for improved annotation protocols. We present an annotation of Transition-Relevance Places (TRP) and Floor-Taking event types on an existing French TV and Radio broadcast corpus to facilitate studies of interruptions and turn-taking. Each speaker change is annotated with the presence or absence of a TRP, and a classification of the next-speaker floor-taking as Smooth, Backchannel or different types of turn violations (cooperative or competitive, successful or attempted interruption). An inter-rater agreement analysis shows such annotations’ moderate to substantial reliability. The inter-annotator agreement for TRP annotation reaches κ=0.75, κ=0.56 for Backchannel and κ=0.5 for the Interruption/non-interruption distinction. More precise differences linked to cooperative or competitive behaviors lead to lower agreements. These results underline the importance of low-level features like TRP to derive a classification of turn changes that would be less subject to interpretation. The analysis of the presence of overlapping speech highlights the existence of interruptions without overlaps and smooth transitions with overlaps. These annotations are available at https://lium.univ-lemans.fr/corpus-allies/.</abstract>
      <url hash="c5ba6a55">2024.lrec-main.110</url>
      <bibkey>uro-etal-2024-annotation-transition</bibkey>
    </paper>
    <paper id="111">
      <title>Annotations for Exploring Food Tweets from Multiple Aspects</title>
      <author><first>Matiss</first><last>Rikters</last></author>
      <author><first>Rinalds</first><last>Vīksna</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <pages>1233–1238</pages>
      <abstract>This research builds upon the Latvian Twitter Eater Corpus (LTEC), which is focused on the narrow domain of tweets related to food, drinks, eating and drinking. LTEC has been collected for more than 12 years and reaching almost 3 million tweets with the basic information as well as extended automatically and manually annotated metadata. In this paper we supplement the LTEC with manually annotated subsets of evaluation data for machine translation, named entity recognition, timeline-balanced sentiment analysis, and text-image relation classification. We experiment with each of the data sets using baseline models and highlight future challenges for various modelling approaches.</abstract>
      <url hash="d75de917">2024.lrec-main.111</url>
      <attachment type="OptionalSupplementaryMaterial" hash="66b4a8a4">2024.lrec-main.111.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>rikters-etal-2024-annotations-exploring</bibkey>
    </paper>
    <paper id="112">
      <title>Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost</title>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Longju</first><last>Bai</last></author>
      <author><first>Joan C.</first><last>Nwatu</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>1239–1259</pages>
      <abstract>Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget.</abstract>
      <url hash="58329182">2024.lrec-main.112</url>
      <bibkey>ignat-etal-2024-annotations-budget</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>A</fixed-case>nno<fixed-case>T</fixed-case>heia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies</title>
      <author><first>José-M.</first><last>Acosta-Triana</last></author>
      <author><first>David</first><last>Gimeno-Gómez</last></author>
      <author><first>Carlos-D.</first><last>Martínez-Hinarejos</last></author>
      <pages>1260–1269</pages>
      <abstract>More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. Prior evaluations show that the toolkit is able to speed up to four times the annotation process. The AnnoTheia toolkit, tutorials, and pre-trained models are available at https://github.com/joactr/AnnoTheia/.</abstract>
      <url hash="f3705195">2024.lrec-main.113</url>
      <bibkey>acosta-triana-etal-2024-annotheia-semi</bibkey>
    </paper>
    <paper id="114">
      <title>Announcing the <fixed-case>P</fixed-case>rague Discourse Treebank 3.0</title>
      <author><first>Pavlína</first><last>Synková</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Lucie</first><last>Poláková</last></author>
      <author><first>Magdaléna</first><last>Rysová</last></author>
      <pages>1270–1279</pages>
      <abstract>We present the Prague Discourse Treebank 3.0 – a new version of the annotation of discourse relations marked by primary and secondary discourse connectives in the data of the Prague Dependency Treebank. Compared to the previous version (PDiT 2.0), the version 3.0 comes with three types of major updates: (i) it brings a largely revised annotation of discourse relations: pragmatic relations have been thoroughly reworked, many inconsistencies across all discourse types have been fixed and previously unclear cases marked in annotators’ comments have been resolved, (ii) it achieves consistency with a Lexicon of Czech Discourse Connectives (CzeDLex), and (iii) it provides the data not only in its native format (Prague Markup Language, discourse relations annotated at the top of the dependency trees), but also in the Penn Discourse Treebank 3.0 format (plain text plus a stand-off discourse annotation) and sense taxonomy. PDiT 3.0 contains 21,662 discourse relations (plus 445 list relations) in 49 thousand sentences.</abstract>
      <url hash="c8e8e307">2024.lrec-main.114</url>
      <bibkey>synkova-etal-2024-announcing-prague</bibkey>
    </paper>
    <paper id="115">
      <title>A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using <fixed-case>BERT</fixed-case>-based Language Models</title>
      <author><first>Namu</first><last>Park</last></author>
      <author><first>Kevin</first><last>Lybarger</last></author>
      <author><first>Giridhar Kaushik</first><last>Ramachandran</last></author>
      <author><first>Spencer</first><last>Lewis</last></author>
      <author><first>Aashka</first><last>Damani</last></author>
      <author><first>Özlem</first><last>Uzuner</last></author>
      <author><first>Martin</first><last>Gunn</last></author>
      <author><first>Meliha</first><last>Yetisgen</last></author>
      <pages>1280–1292</pages>
      <abstract>Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex, unstructured images and articulate their assessments through narrative reports that remain largely unstructured. This unstructured narrative must be converted into a structured semantic representation to facilitate secondary applications such as retrospective analyses or clinical decision support. Here, we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which includes 609 annotated radiology reports from three imaging modality types: Computed Tomography, Magnetic Resonance Imaging, and Positron Emission Tomography-Computed Tomography. Reports were annotated using an event-based schema that captures clinical indications, lesions, and medical problems. Each event consists of a trigger and multiple arguments, and a majority of the argument types, including anatomy, normalize the spans to pre-defined concepts to facilitate secondary use. CAMIR uniquely combines a granular event structure and concept normalization. To extract CAMIR events, we explored two BERT (Bi-directional Encoder Representation from Transformers)-based architectures, including an existing architecture (mSpERT) that jointly extracts all event information and a multi-step approach (PL-Marker++) that we augmented for the CAMIR schema.</abstract>
      <url hash="a8fdcd19">2024.lrec-main.115</url>
      <bibkey>park-etal-2024-novel-corpus</bibkey>
    </paper>
    <paper id="116">
      <title>A Novel Three-stage Framework for Few-shot Named Entity Recognition</title>
      <author><first>Shengjie</first><last>Ji</last></author>
      <author><first>Fang</first><last>Kong</last></author>
      <pages>1293–1305</pages>
      <abstract>Different from most existing tasks relying on abundant labeled data, Few-shot Named Entity Recognition (NER) aims to develop NER systems that are capable of learning from a small set of labeled samples and then generalizing well to new, unseen data.In this paper, with the intention of obtaining a model that can better adapt to new domains, we design a novel three-stage framework for Few-shot NER, including teacher span recognizer, student span recognizer and entity classifier.We first train a teacher span recognizer which is based on a global boundary matrix to obtain soft boundary labels.Then we leverage the soft boundary labels learned by the teacher model to assist in training the student span recognizer,which can smooth the training process of span recognizer.Finally, we adopt the traditional prototypical network as entity classifier and incorporate the idea of prompt learning to construct a more generalizable semantic space.Extensive experiments on various benchmarks demonstrate that our approach surpasses prior methods.</abstract>
      <url hash="b5f2b933">2024.lrec-main.116</url>
      <bibkey>ji-kong-2024-novel-three</bibkey>
    </paper>
    <paper id="117">
      <title><fixed-case>A</fixed-case>nt<fixed-case>C</fixed-case>ritic: Argument Mining for Free-Form and Visually-Rich Financial Comments</title>
      <author><first>Huadai</first><last>Liu</last></author>
      <author><first>Xu</first><last>Wenqiang</last></author>
      <author><first>Xuan</first><last>Lin</last></author>
      <author><first>Jingjing</first><last>Huo</last></author>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Zhou</first><last>Zhao</last></author>
      <pages>1306–1317</pages>
      <abstract>Argument mining aims to detect all possible argumentative components and identify their relationships automatically. As a thriving task in natural language processing, there has been a large amount of corpus for academic study and application development in this field. However, the research in this area is still constrained by the inherent limitations of existing datasets. Specifically, all the publicly available datasets are relatively small in scale, and few of them provide information from other modalities to facilitate the learning process. Moreover, the statements and expressions in these corpora are usually in a <i>compact</i> form, which restricts the generalization ability of models. To this end, we collect a novel dataset <i>AntCritic</i> to serve as a helpful complement to this area, which consists of about 10k free-form and visually-rich financial comments and supports both argument component detection and argument relation prediction tasks. Besides, to cope with the challenges brought by scenario expansion, we thoroughly explore the fine-grained relation prediction and structure reconstruction scheme and discuss the encoding mechanism for visual styles and layouts. On this basis, we design two simple but effective model architectures and conduct various experiments on this dataset to provide benchmark performances as a reference and verify the practicability of our proposed architecture. We release our data and code in this <i>link</i>, and this dataset follows CC BY-NC-ND 4.0 license.</abstract>
      <url hash="e1e9e534">2024.lrec-main.117</url>
      <attachment type="OptionalSupplementaryMaterial" hash="16b8d61c">2024.lrec-main.117.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>liu-etal-2024-antcritic-argument</bibkey>
    </paper>
    <paper id="118">
      <title>An Unsupervised Framework for Adaptive Context-aware Simplified-Traditional <fixed-case>C</fixed-case>hinese Character Conversion</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Shutan</first><last>Huang</last></author>
      <author><first>Yanqiu</first><last>Shao</last></author>
      <pages>1318–1326</pages>
      <abstract>Traditional Chinese character is an important carrier of Chinese culture, and is still actively used in many areas. Automatic conversion between traditional and simplified Chinese characters can help modern people understand traditional culture and facilitate communication among different regions. Previous conversion methods rely on rule-based mapping or shallow feature-based machine learning models, which struggle to convert simplified characters with different origins and constructing training data is costly. In this study, we propose an unsupervised adaptive context-aware conversion model that learns to convert between simplified and traditional Chinese characters under a denoising auto-encoder framework requiring no labeled data. Our model includes a Latent Generative Adversarial Encoder that transforms vectors to a latent space with generative adversarial network, which adds noise as an inevitable side effect, Based on which a Context-aware Semantic Reconstruction Decoder restores the original input while considering a broader range of context with a pretrained language model. Additionally, we propose to apply early exit mechanism during inference to reduce the computation complexity and improve the generalization ability. To test the effectiveness of our model, we construct a high quality test dataset with simplified-traditional Chinese character text pairs. Experiment results and extensive analysis demonstrate that our model outperforms strong unsupervised baselines and yields better conversion result for one-to-many cases.</abstract>
      <url hash="4821a5ff">2024.lrec-main.118</url>
      <bibkey>li-etal-2024-unsupervised-framework</bibkey>
    </paper>
    <paper id="119">
      <title>An Untold Story of Preprocessing Task Evaluation: An Alignment-based Joint Evaluation Approach</title>
      <author><first>Eunkyul Leah</first><last>Jo</last></author>
      <author><first>Angela Yoonseo</first><last>Park</last></author>
      <author><first>Grace Tianjiao</first><last>Zhang</last></author>
      <author><first>Izia Xiaoxiao</first><last>Wang</last></author>
      <author><first>Junrui</first><last>Wang</last></author>
      <author><first>MingJia</first><last>Mao</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>1327–1338</pages>
      <abstract>A preprocessing task such as tokenization and sentence boundary detection (SBD) has commonly been considered as NLP challenges that have already been solved. This perception is due to their generally good performance and the presence of pre-tokenized data. However, it’s important to note that the low error rates of current methods are mainly specific to certain tasks, and rule-based tokenization can be difficult to use across different systems. Despite being subtle, these limitations are significant in the context of the NLP pipeline. In this paper, we introduce a novel evaluation algorithm for the preprocessing task, including both tokenization and SBD results. This algorithm aims to enhance the reliability of evaluations by reevaluating the counts of true positive cases for F1 measures in both preprocessing tasks jointly. It achieves this through an alignment-based approach inspired by sentence and word alignments used in machine translation. Our evaluation algorithm not only allows for precise counting of true positive tokens and sentence boundaries but also combines these two evaluation tasks into a single organized pipeline. To illustrate and clarify the intricacies of this calculation and integration, we provide detailed pseudo-code configurations for implementation. Additionally, we offer empirical evidence demonstrating how sentence and word alignment can improve evaluation reliability and present case studies to further support our approach.</abstract>
      <url hash="849f2fc0">2024.lrec-main.119</url>
      <bibkey>jo-etal-2024-untold-story</bibkey>
    </paper>
    <paper id="120">
      <title>A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models</title>
      <author><first>Chenyang</first><last>Lyu</last></author>
      <author><first>Zefeng</first><last>Du</last></author>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>Yitao</first><last>Duan</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Teresa</first><last>Lynn</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <pages>1339–1352</pages>
      <abstract>Machine Translation (MT) has greatly advanced over the years due to the developments in deep neural networks. However, the emergence of Large Language Models (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT domain. In this context, we believe that the future of MT is intricately tied to the capabilities of LLMs. These models not only offer vast linguistic understandings but also bring innovative methodologies, such as prompt-based techniques, that have the potential to further elevate MT. In this paper, we provide an overview of the significant enhancements in MT that are influenced by LLMs and advocate for their pivotal role in upcoming MT research and implementations. We highlight several new MT directions, emphasizing the benefits of LLMs in scenarios such as Long-Document Translation, Stylized Translation, and Interactive Translation. Additionally, we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies. By showcasing practical instances, we aim to demonstrate the advantages that LLMs offer, particularly in tasks like translating extended documents. We conclude by emphasizing the critical role of LLMs in guiding the future evolution of MT and offer a roadmap for future exploration in the sector.</abstract>
      <url hash="37330da6">2024.lrec-main.120</url>
      <bibkey>lyu-etal-2024-paradigm-shift</bibkey>
    </paper>
    <paper id="121">
      <title>A Persona-Based Corpus in the Diabetes Self-Care Domain - Applying a Human-Centered Approach to a Low-Resource Context</title>
      <author><first>Rossana</first><last>Cunha</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Adriana</first><last>Pagano</last></author>
      <author><first>Fabio</first><last>Alves</last></author>
      <pages>1353–1369</pages>
      <abstract>While Natural Language Processing (NLP) models have gained substantial attention, only in recent years has research opened new paths for tackling Human-Computer Design (HCD) from the perspective of natural language. We focus on developing a human-centered corpus, more specifically, a persona-based corpus in a particular healthcare domain (diabetes mellitus self-care). In order to follow an HCD approach, we created personas to model interpersonal interaction (expert and non-expert users) in that specific domain. We show that an HCD approach benefits language generation from different perspectives, from machines to humans - contributing with new directions for low-resource contexts (languages other than English and sensitive domains) where the need to promote effective communication is essential.</abstract>
      <url hash="e8790ab3">2024.lrec-main.121</url>
      <attachment type="OptionalSupplementaryMaterial" hash="410517e1">2024.lrec-main.121.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>cunha-etal-2024-persona-based</bibkey>
    </paper>
    <paper id="122">
      <title><fixed-case>APOLLO</fixed-case>: An Optimized Training Approach for Long-form Numerical Reasoning</title>
      <author><first>Jiashuo</first><last>Sun</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Xiangdong</first><last>Su</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Jian</first><last>Guo</last></author>
      <pages>1370–1382</pages>
      <abstract>Long-form numerical reasoning aims to generate a reasoning program to calculate the answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on the retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numerical information. Furthermore, they ignored program consistency, leading to the wrong punishment of programs that differed from the ground truth. In order to address these issues, we proposed APOLLO (An optimized training aPproach fOr Long-form numericaL reasOning), to improve long-form numerical reasoning. APOLLO includes a number-aware negative sampling strategy for the retriever to discriminate key numerical facts, and a consistency-based reinforcement learning with target program augmentation for the generator to ultimately increase the execution accuracy. Experimental results on the FinQA and ConvFinQA leaderboards verify the effectiveness of our proposed methods, achieving the new state-of-the-art.</abstract>
      <url hash="11f320a7">2024.lrec-main.122</url>
      <attachment type="OptionalSupplementaryMaterial" hash="798a2678">2024.lrec-main.122.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>sun-etal-2024-apollo-optimized</bibkey>
    </paper>
    <paper id="123">
      <title>Applying Transfer Learning to <fixed-case>G</fixed-case>erman Metaphor Prediction</title>
      <author><first>Maria</first><last>Berger</last></author>
      <author><first>Sebastian Michael</first><last>Reimann</last></author>
      <author><first>Nieke Marie</first><last>Kiwitt</last></author>
      <pages>1383–1392</pages>
      <abstract>This paper presents results in transfer-learning metaphor recognition in German. Starting from an English language corpus annotated for metaphor at the sentence level, and its machine-translation to German, we annotate 1000 sentences of the German part to use it as a Gold standard for two different metaphor prediction setups: i) a sequence labeling set-up (on the token-level), and ii) a classification (based on sentences) setup. We test two transfer leaning approaches: i) a group of transformer models, and ii) a technique that utilizes bilingual embeddings together with an RNN classifier. We find out that the transformer models do moderately in a zero-shot scenario (up to 61% F1 for classification) and the embeddings approaches do not even beat the guessing baseline (36% F1 for classification). We use our Gold data to fine-tune the classification tasks on target-language data achieving up to 90% F1 with both, the multilingual BERT and the bilingual embeddings. We also publish the annotated bilingual corpus.</abstract>
      <url hash="c50dc219">2024.lrec-main.123</url>
      <bibkey>berger-etal-2024-applying-transfer</bibkey>
    </paper>
    <paper id="124">
      <title>Appraisal Framework for Clinical Empathy: A Novel Application to Breaking Bad News Conversations</title>
      <author><first>Allison Claire</first><last>Lahnala</last></author>
      <author><first>Béla</first><last>Neuendorf</last></author>
      <author><first>Alexander</first><last>Thomin</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <author><first>Tina</first><last>Stibane</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>1393–1407</pages>
      <abstract>Empathy is essential in healthcare communication. We introduce an annotation approach that draws on well-established frameworks for <i>clinical empathy</i> and <i>breaking bad news</i> (BBN) conversations for considering the interactive dynamics of discourse relations. We construct Empathy in BBNs, a span-relation task dataset of simulated BBN conversations in German, using our annotation scheme, in collaboration with a large medical school to support research on educational tools for medical didactics. The annotation is based on 1) Pounds (2011)’s appraisal framework for clinical empathy, which is grounded in systemic functional linguistics, and 2) the SPIKES protocol for breaking bad news (Baile et al., 2000), commonly taught in medical didactics training. This approach presents novel opportunities to study clinical empathic behavior and enables the training of models to detect causal relations involving empathy, a highly desirable feature of systems that can provide feedback to medical professionals in training. We present illustrative examples, discuss applications of the annotation scheme, and insights we can draw from the framework.</abstract>
      <url hash="11398354">2024.lrec-main.124</url>
      <bibkey>lahnala-etal-2024-appraisal-framework</bibkey>
    </paper>
    <paper id="125">
      <title>Approaches and Challenges for Resolving Different Representations of Fictional Characters for <fixed-case>C</fixed-case>hinese Novels</title>
      <author><first>Li</first><last>Song</last></author>
      <author><first>Ying</first><last>Liu</last></author>
      <pages>1408–1421</pages>
      <abstract>Due to the huge scale of literary works, automatic text analysis technologies are urgently needed for literary studies such as Digital Humanities. However, the domain-generality of existing NLP technologies limits their effectiveness on in-depth literary studies. It is valuable to explore how to adapt NLP technologies to the literary-specific tasks. Fictional characters are the most essential elements of a novel, and thus crucial to understanding the content of novels. The prerequisite of collecting a character’s information is to resolve its different representations. It is a specific problem of anaphora resolution which is a classical and open-domain NLP task. We adapt a state-of-the-art anaphora resolution model to resolve character representations in Chinese novels by making some modifications, and train a widely used BERT fine-tuned model for speaker extraction as assistance. We also analyze the challenges and potential solutions for character-resolution in Chinese novels according to the resolution results on a specific Chinese novel.</abstract>
      <url hash="bf43ca4d">2024.lrec-main.125</url>
      <bibkey>song-liu-2024-approaches-challenges</bibkey>
    </paper>
    <paper id="126">
      <title>A Preliminary Study of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for <fixed-case>S</fixed-case>panish <fixed-case>E</fixed-case>2<fixed-case>R</fixed-case> Text Adaptation</title>
      <author><first>Margot</first><last>Madina</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
      <author><first>Melanie</first><last>Siegel</last></author>
      <pages>1422–1434</pages>
      <abstract>The process of adapting and creating Easy-to-Read (E2R) texts is very expensive and time-consuming. Due to the success of Large Language Models (LLMs) such as ChatGPT and their ability to generate written language, it is likely to think that such models can help in the adaptation or creation of text in E2R. In this paper, we explore the concept of E2R, its underlying principles and applications, and provides a preliminary study on the usefulness of ChatGPT-4 for E2R text adaptation. We focus on the Spanish language and its E2R variant, Lectura Fácil (LF). We consider a range of prompts that can be used and the differences in output that this produces. We then carry out a three-folded evaluation on 10 texts adapted by ChatGPT-4: (1) an automated evaluation to check values related to the readability of texts, (2) a checklist-based manual evaluation (for which we also propose three new capabilities) and (3) a users’ evaluation with people with cognitive disabilities. We show that it is difficult to choose the best prompt to make ChatGPT-4 adapt texts to LF. Furthermore, the generated output does not follow the E2R text rules, so it is often not suitable for the target audience.</abstract>
      <url hash="909e761b">2024.lrec-main.126</url>
      <bibkey>madina-etal-2024-preliminary-study</bibkey>
    </paper>
    <paper id="127">
      <title>A Quantum-Inspired Matching Network with Linguistic Theories for Metaphor Detection</title>
      <author><first>Wenbo</first><last>Qiao</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>ZengLai</first><last>Ma</last></author>
      <pages>1435–1445</pages>
      <abstract>Enabling machines with the capability to recognize and comprehend metaphors is a crucial step toward achieving artificial intelligence. In linguistic theories, metaphor can be identified through Metaphor Identification Procedure (MIP) or Selectional Preference Violation (SPV), both of which are typically considered as matching tasks in the field of natural language processing. However, the implementation of MIP poses a challenge due to the semantic uncertainty and ambiguity of literal meanings of words. Simultaneously, SPV often struggles to recognize conventional metaphors. Inspired by Quantum Language Model (QLM) for modeling semantic uncertainty and fine-grained feature matching, we propose a quantum-inspired matching network for metaphor detection. Specifically, we use the density matrix to explicitly characterize the literal meanings of the target word for MIP, in order to model the uncertainty and ambiguity of the literal meanings of words. This can make SPV effective even in the face of conventional metaphors. MIP and SPV are then achieved by fine-grained feature matching. The results of the experiment finally demonstrated our approach has strong competitiveness.</abstract>
      <url hash="db53f89f">2024.lrec-main.127</url>
      <bibkey>qiao-etal-2024-quantum-inspired</bibkey>
    </paper>
    <paper id="128">
      <title><fixed-case>A</fixed-case>rabic Diacritization Using Morphologically Informed Character-Level Model</title>
      <author><first>Muhammad Morsy</first><last>Elmallah</last></author>
      <author><first>Mahmoud</first><last>Reda</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Abdelrahman</first><last>El-Sheikh</last></author>
      <author><first>Ashraf Hatim</first><last>Elneima</last></author>
      <author><first>Murtadha</first><last>Aljubran</last></author>
      <author><first>Nouf</first><last>Alsaeed</last></author>
      <author><first>Reem</first><last>Mohammed</last></author>
      <author><first>Mohamed</first><last>Al-Badrashiny</last></author>
      <pages>1446–1454</pages>
      <abstract>Arabic diacritic recovery i.e. diacritization is necessary for proper vocalization and an enabler for downstream applications such as language learning and text to speech. Diacritics come in two varieties, namely: core-word diacritics and case endings. In this paper we introduce a highly effective morphologically informed character-level model that can recover both types of diacritics simultaneously. The model uses a Recurrent Neural Network (RNN) based architecture that takes in text as a sequence of characters, with markers for morphological segmentation, and outputs a sequence of diacritics. We also introduce a character-based morphological segmentation model that we train for Modern Standard Arabic (MSA) and dialectal Arabic. We demonstrate the efficacy of our diacritization model on Classical Arabic, MSA, and two dialectal (Moroccan and Tunisian) texts. We achieve the lowest reported word-level diacritization error rate for MSA (3.4%), match the best results for Classical Arabic (5.4%), and report competitive results for dialectal Arabic.</abstract>
      <url hash="fae77dec">2024.lrec-main.128</url>
      <bibkey>elmallah-etal-2024-arabic-diacritization</bibkey>
    </paper>
    <paper id="129">
      <title>Arbitrary Time Information Modeling via Polynomial Approximation for Temporal Knowledge Graph Embedding</title>
      <author><first>Zhiyu</first><last>Fang</last></author>
      <author><first>Jingyan</first><last>Qin</last></author>
      <author><first>Xiaobin</first><last>Zhu</last></author>
      <author><first>Chun</first><last>Yang</last></author>
      <author><first>Xu-Cheng</first><last>Yin</last></author>
      <pages>1455–1465</pages>
      <abstract>Distinguished from traditional knowledge graphs (KGs), temporal knowledge graphs (TKGs) must explore and reason over temporally evolving facts adequately. However, existing TKG approaches still face two main challenges, i.e., the limited capability to model arbitrary timestamps continuously and the lack of rich inference patterns under temporal constraints. In this paper, we propose an innovative TKGE method (PTBox) via polynomial decomposition-based temporal representation and box embedding-based entity representation to tackle the above-mentioned problems. Specifically, we decompose time information by polynomials and then enhance the model’s capability to represent arbitrary timestamps flexibly by incorporating the learnable temporal basis tensor. In addition, we model every entity as a hyperrectangle box and define each relation as a transformation on the head and tail entity boxes. The entity boxes can capture complex geometric structures and learn robust representations, improving the model’s inductive capability for rich inference patterns. Theoretically, our PTBox can encode arbitrary time information or even unseen timestamps while capturing rich inference patterns and higher-arity relations of the knowledge base. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="9f882752">2024.lrec-main.129</url>
      <bibkey>fang-etal-2024-arbitrary-time</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>ARBRES</fixed-case> Kenstur: A <fixed-case>B</fixed-case>reton-<fixed-case>F</fixed-case>rench Parallel Corpus Rooted in Field Linguistics</title>
      <author><first>Loïc</first><last>Grobol</last></author>
      <author><first>Mélanie</first><last>Jouitteau</last></author>
      <pages>1466–1471</pages>
      <abstract>ARBRES is an ongoing project of open science implemented as a platform (“wikigrammar”) documenting both the Breton language itself and the state of research and engineering work in linguistics and NLP. Along its nearly 15 years of operation, it has aggregated a wealth of linguistic data in the form of interlinear glosses with translations illustrating lexical items, grammatical features, dialectal variations... While these glosses were primarily meant for human consumption, their volume and the regular format imposed by the wiki engine used for the website also make them suitable for machine processing. ARBRES Kenstur is a new parallel corpus derived from the glosses in ARBRES, including about 5k phrases and sentences in Breton along with translations in standard French. The nature of the original data — sourced from field linguistic inquiries meant to document the structure of Breton — leads to a resource that is mechanically more concerned with the internal variations of the language and rare phenomena than typical parallel corpora. Preliminaries experiments in using this corpus show that it can help improve machine translation for Breton, demonstrating that sourcing data from field linguistic documentation can be a way to help provide NLP tools for minority and low-resource languages.</abstract>
      <url hash="98cf7833">2024.lrec-main.130</url>
      <bibkey>grobol-jouitteau-2024-arbres-kenstur</bibkey>
    </paper>
    <paper id="131">
      <title>A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder</title>
      <author><first>Kedi</first><last>Chen</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Qin</first><last>Chen</last></author>
      <author><first>Shunyu</first><last>Liu</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>1472–1485</pages>
      <abstract>Information extraction (IE) aims to extract complex structured information from the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. In this study, we propose a regularization-based transfer learning method for IE (TIE) via an instructed graph decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. In this way, the common knowledge shared with existing datasets can be learned and transferred to a new dataset with new labels. Furthermore, to alleviate the label inconsistency problem among various IE tasks, we introduce a task-specific regularization strategy, which does not update the gradients of two tasks with ‘opposite direction’. We conduct extensive experiments on 12 datasets spanning four IE tasks, and the results demonstrate the great advantages of our proposed method.</abstract>
      <url hash="92736935">2024.lrec-main.131</url>
      <bibkey>chen-etal-2024-regularization-based</bibkey>
    </paper>
    <paper id="132">
      <title>A Reinforcement Learning Approach to Improve Low-Resource Machine Translation Leveraging Domain Monolingual Data</title>
      <author><first>Hongxiao</first><last>Zhang</last></author>
      <author><first>Mingtong</first><last>Liu</last></author>
      <author><first>Chunyou</first><last>Li</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>1486–1497</pages>
      <abstract>Due to the lack of parallel data, the mainstream fine-tuning-based domain adaptation methods have the overfitting problem in the translation of low-resource domains, and it is difficult for the model to learn the in-domain generalization knowledge. To address the above issue, in this work, we propose a novel Reinforcement Learning Domain Adaptation method for Neural Machine Translation (RLDA-NMT) in the low-resource domain. RLDA-NMT utilizes in-domain source monolingual data to make up for the lack of parallel data, and reinforces domain features learning to make the translation model learn the domain-specific knowledge more fully. Specifically, we first train a ranking-based model with a small-scale in-domain parallel corpus, and then adopt it as the reward model to select higher-quality generated translations for reinforcement when fine-tuning pre-trained NMT model using in-domain source monolingual data. We conduct experiments on Education, Laws, Thesis, and Patent domains of Chinese⇔English translation tasks. Experimental results demonstrate that RLDA-NMT can alleviate overfitting and reinforce the NMT model to learn domain-specific knowledge. Additionally, the results also show that RLDA-NMT and back-translation (BT) are nicely complementary to each other, where combining RLDA-NMT with BT can further improve translation quality.</abstract>
      <url hash="8bb621be">2024.lrec-main.132</url>
      <bibkey>zhang-etal-2024-reinforcement-learning</bibkey>
    </paper>
    <paper id="133">
      <title>Are Large Language Models Good at Lexical Semantics? A Case of Taxonomy Learning</title>
      <author><first>Viktor</first><last>Moskvoretskii</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <pages>1498–1510</pages>
      <abstract>Recent studies on LLMs do not pay enough attention to linguistic and lexical semantic tasks, such as taxonomy learning. In this paper, we explore the capacities of Large Language Models featuring LLaMA-2 and Mistral for several Taxonomy-related tasks. We introduce a new methodology and algorithm for data collection via stochastic graph traversal leading to controllable data collection. Collected cases provide the ability to form nearly any type of graph operation. We test the collected dataset for learning taxonomy structure based on English WordNet and compare different input templates for fine-tuning LLMs. Moreover, we apply the fine-tuned models on such datasets on the downstream tasks achieving state-of-the-art results on the TexEval-2 dataset.</abstract>
      <url hash="367e31d5">2024.lrec-main.133</url>
      <bibkey>moskvoretskii-etal-2024-large-language</bibkey>
    </paper>
    <paper id="134">
      <title>Are Text Classifiers Xenophobic? A Country-Oriented Bias Detection Method with Least Confounding Variables</title>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>Sebastian</first><last>Cifuentes</last></author>
      <pages>1511–1518</pages>
      <abstract>Classical bias detection methods used in Machine Learning are themselves biased because of the different confounding variables implied in the assessment of the initial biases. First they are using templates that are syntactically simple and distant from the target data on which the model will deployed. Second, current methods are assessing biases in pre-trained language models or in dataset, but not directly on the fine-tuned classifier that can actually produce harms. We propose a simple method to detect the biases of a specific fine-tuned classifier on any type of unlabeled data. The idea is to study the classifier behavior by creating counterfactual examples directly on the target data distribution and quantify the amount of changes. In this work, we focus on named entity perturbations by applying a Named Entity Recognition on target-domain data and modifying them accordingly to most common names or location of a target group (gender and country), and this for several morphosynctactically different languages spoken in relation with the countries of the target groups. We used our method on two models available open-source that are likely to be deployed by industry, and on two tasks and domains. We first assess the bias of a multilingual sentiment analysis model trained over multiple-languages tweets and available open-source, and then a multilingual stance recognition model trained over several languages and assessed over English language. Finally we propose to link the perplexity of each example with the bias of the model, by looking at the change in label distribution with respect to the language of the target group. Our work offers a fine-grained analysis of the interactions between names and languages, revealing significant biases in multilingual models.</abstract>
      <url hash="d0ed9724">2024.lrec-main.134</url>
      <bibkey>barriere-cifuentes-2024-text-classifiers</bibkey>
    </paper>
    <paper id="135">
      <title>Argument Quality Assessment in the Age of Instruction-Following Large Language Models</title>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Joonsuk</first><last>Park</last></author>
      <author><first>Eva Maria</first><last>Vecchi</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <author><first>Timon</first><last>Ziegenbein</last></author>
      <pages>1519–1538</pages>
      <abstract>The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument’s quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby.</abstract>
      <url hash="717147b2">2024.lrec-main.135</url>
      <bibkey>wachsmuth-etal-2024-argument-quality</bibkey>
    </paper>
    <paper id="136">
      <title>Article Classification with Graph Neural Networks and Multigraphs</title>
      <author><first>Khang</first><last>Ly</last></author>
      <author><first>Yury</first><last>Kashnitsky</last></author>
      <author><first>Savvas</first><last>Chamezopoulos</last></author>
      <author><first>Valeria</first><last>Krzhizhanovskaya</last></author>
      <pages>1539–1547</pages>
      <abstract>Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Network (GNN) pipelines with multi-graph representations that simultaneously encode multiple signals of article relatedness, e.g. references, co-authorship, shared publication source, shared subject headings, as distinct edge types. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark OGBN-arXiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph and PubMed Central, respectively. The results demonstrate that multi-graphs consistently improve the performance of a variety of GNN models compared to the default graphs. When deployed with SOTA textual node embedding methods, the transformed multi-graphs enable simple and shallow 2-layer GNN pipelines to achieve results on par with more complex architectures.</abstract>
      <url hash="203212a5">2024.lrec-main.136</url>
      <bibkey>ly-etal-2024-article-classification</bibkey>
    </paper>
    <paper id="137">
      <title><fixed-case>ART</fixed-case>: The Alternating Reading Task Corpus for Speech Entrainment and Imitation</title>
      <author><first>Zheng Byron</first><last>Yuan</last></author>
      <author><first>Dorina</first><last>de Jong</last></author>
      <author><first>Ruitao</first><last>Feng</last></author>
      <author><first>Štefan</first><last>Beňuš</last></author>
      <author><first>Noël</first><last>Nguyen</last></author>
      <author><first>Róbert</first><last>Sabo</last></author>
      <author><first>Luciano</first><last>Fadiga</last></author>
      <author><first>Alessandro</first><last>D’Ausilio</last></author>
      <pages>1548–1562</pages>
      <abstract>We introduce the Alternating Reading Task (ART) Corpus, a collection of dyadic sentence reading for studying the entrainment and imitation behaviour in speech communication. The ART corpus features three experimental conditions - solo reading, alternating reading, and deliberate imitation - as well as three subcorpora encompassing French-, Italian-, and Slovak-accented English. This design allows systematic investigation of speech entrainment in a controlled and less spontaneous setting. Alongside detailed transcriptions, it includes English proficiency scores, demographics, and in-experiment questionnaires for probing linguistic, personal and interpersonal influences on entrainment. Our presentation covers its design, collection, annotation processes, initial analysis, and future research prospects.</abstract>
      <url hash="c3c09e19">2024.lrec-main.137</url>
      <bibkey>yuan-etal-2024-art-alternating</bibkey>
    </paper>
    <paper id="138">
      <title>A Self-verified Method for Exploring Simile Knowledge from Pre-trained Language Models</title>
      <author><first>Longxuan</first><last>Ma</last></author>
      <author><first>Changxin</first><last>Ke</last></author>
      <author><first>Shuhan</first><last>Zhou</last></author>
      <author><first>Churui</first><last>Sun</last></author>
      <author><first>Wei-Nan</first><last>Zhang</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>1563–1576</pages>
      <abstract>Simile tasks are challenging in natural language processing (NLP) because models require adequate world knowledge to produce predictions. In recent years, pre-trained language models (PLMs) have succeeded in NLP since they learn generic knowledge from a large corpus. The knowledge embedded in PLMs can be used for different kinds of Simile tasks. However, previous work usually explored one type of simile knowledge for a specific simile task, how to fully utilize different types of knowledge embedded in the PLMs requires further exploration. This paper proposes a self-verified method for exploring simile knowledge from PLMs, which allows the PLMs to leverage one type of simile knowledge to self-validate another. To this end, we first enhance PLMs with a novel multi-level simile recognition (MLSR) task that trains PLMs to evaluate the quality of similes. Then the PLMs leverage this evaluation score to assist the simile interpretation and generation tasks. In this way, we connect different types of simile knowledge in PLMs and make better use of them. Experiments on different pre-trained models and multiple publicly available datasets show that our method works for different kinds of PLMs and can explore more accurate simile knowledge for PLMs. Our code/data will be released on GitHub.</abstract>
      <url hash="59b6c90c">2024.lrec-main.138</url>
      <bibkey>ma-etal-2024-self-verified</bibkey>
    </paper>
    <paper id="139">
      <title>A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction</title>
      <author><first>Jian</first><last>Zhang</last></author>
      <author><first>Changlin</first><last>Yang</last></author>
      <author><first>Haiping</first><last>Zhu</last></author>
      <author><first>Qika</first><last>Lin</last></author>
      <author><first>Fangzhi</first><last>Xu</last></author>
      <author><first>Jun</first><last>Liu</last></author>
      <pages>1577–1587</pages>
      <abstract>Document-level Event Argument Extraction (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize prompt-based methods to guide pre-trained language models (PLMs) in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention Graph Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention graph that captures relations within and between documents and prompts, encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensemble graph transformer module to address mentions and their three semantic relations effectively. Later, the graph-augmented encoder-decoder module incorporates the relation-specific graph into the input embedding of PLMs and optimizes the encoder section with topology information, enhancing the relations comprehensively. Extensive experiments on the RAMS and WikiEvents datasets demonstrate the effectiveness of our approach, surpassing baseline methods and achieving a new state-of-the-art performance.</abstract>
      <url hash="b84e8037">2024.lrec-main.139</url>
      <bibkey>zhang-etal-2024-semantic-mention</bibkey>
    </paper>
    <paper id="140">
      <title><fixed-case>ASEM</fixed-case>: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling</title>
      <author><first>Omama</first><last>Hamad</last></author>
      <author><first>Khaled</first><last>Shaban</last></author>
      <author><first>Ali</first><last>Hamdi</last></author>
      <pages>1588–1601</pages>
      <abstract>Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text. In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user’s utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely zeroes in on sentiment and emotion nuances within the user’s utterance. This ensures the generation of context-rich representations tailored to the underlying emotional tone and sentiment intricacies of the text. Our approach outperforms existing methods for generating empathetic embeddings, providing empathetic and diverse responses. The performance of our proposed model significantly exceeds that of existing models, enhancing emotion detection accuracy by 6.2% and lexical diversity by 1.4%. ASEM code is released at https://github.com/MIRAH-Official/Empathetic-Chatbot-ASEM.git</abstract>
      <url hash="62ab1ea2">2024.lrec-main.140</url>
      <bibkey>hamad-etal-2024-asem-enhancing</bibkey>
    </paper>
    <paper id="141">
      <title>A Single Linear Layer Yields Task-Adapted Low-Rank Matrices</title>
      <author><first>Hwichan</first><last>Kim</last></author>
      <author><first>Shota</first><last>Sasaki</last></author>
      <author><first>Sho</first><last>Hoshino</last></author>
      <author><first>Ukyo</first><last>Honda</last></author>
      <pages>1602–1608</pages>
      <abstract>Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix <tex-math>W_0</tex-math> with a delta matrix <tex-math>\Delta W</tex-math> consisted by two low-rank matrices <tex-math>A</tex-math> and <tex-math>B</tex-math>. A previous study suggested that there is correlation between <tex-math>W_0</tex-math> and <tex-math>\Delta W</tex-math>. In this study, we aim to delve deeper into relationships between <tex-math>W_0</tex-math> and low-rank matrices <tex-math>A</tex-math> and <tex-math>B</tex-math> to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform <tex-math>W_0</tex-math> into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer’s <tex-math>W_0</tex-math> as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weight matrices with low-rank matrices derived from a single linear layer. Our empirical results show that CondLoRA maintains a performance on par with LoRA, despite the fact that the trainable parameters of CondLoRA are fewer than those of LoRA. Therefore, we conclude that “a single linear layer yields task-adapted low-rank matrices.” The code used in our experiments is available at <url>https://github.com/CyberAgentAILab/CondLoRA</url>.</abstract>
      <url hash="c653bb62">2024.lrec-main.141</url>
      <bibkey>kim-etal-2024-single-linear</bibkey>
    </paper>
    <paper id="142">
      <title>Asking and Answering Questions to Extract Event-Argument Structures</title>
      <author><first>Md Nayem</first><last>Uddin</last></author>
      <author><first>Enfa Rose</first><last>George</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <author><first>Steven R.</first><last>Corman</last></author>
      <pages>1609–1626</pages>
      <abstract>This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model.</abstract>
      <url hash="61539d92">2024.lrec-main.142</url>
      <bibkey>uddin-etal-2024-asking-answering</bibkey>
    </paper>
    <paper id="143">
      <title><fixed-case>A</fixed-case>ssamese<fixed-case>B</fixed-case>ack<fixed-case>T</fixed-case>ranslit: Back Transliteration of <fixed-case>R</fixed-case>omanized <fixed-case>A</fixed-case>ssamese Social Media Text</title>
      <author><first>Hemanta</first><last>Baruah</last></author>
      <author><first>Sanasam Ranbir</first><last>Singh</last></author>
      <author><first>Priyankoo</first><last>Sarmah</last></author>
      <pages>1627–1637</pages>
      <abstract>This paper presents a novel back transliteration dataset capturing native language text originally composed in the Roman/Latin script, harvested from popular social media platforms, along with its corresponding representation in the native Assamese script. Assamese, categorized as a low-resource language within the Indo-Aryan language family, predominantly spoken in the north-east Indian state of Assam, faces a scarcity of linguistic resources. The dataset comprises a total of 60,312 Roman-native parallel transliterated sentences. This paper diverges from conventional forward transliteration datasets consisting mainly of named entities and technical terms, instead presenting a novel transliteration dataset cultivated from three prominent social media platforms, Facebook, Twitter(currently X), and YouTube, in the backward transliteration direction. The paper offers a comprehensive examination of ten state-of-the-art word-level transliteration models within the context of this dataset, encompassing transliteration evaluation benchmarks, extensive performance assessments, and a discussion of the unique chal- lenges encountered during the processing of transliterated social media content. Our approach involves the initial use of two statistical transliteration models, followed by the training of two state-of-the-art neural network-based transliteration models, evaluation of three publicly available pre-trained models, and ultimately fine-tuning one existing state-of-the-art multilingual transliteration model along with two pre-trained large language models using the collected datasets. Notably, the Neural Transformer model outperforms all other baseline transliteration models, achieving the lowest Word Error Rate (WER) and Character Error Rate (CER), and the highest BLEU (up to 4 gram) score of 55.05, 19.44, and 69.15, respectively.</abstract>
      <url hash="2445b763">2024.lrec-main.143</url>
      <bibkey>baruah-etal-2024-assamesebacktranslit-back</bibkey>
    </paper>
    <paper id="144">
      <title>Assessing Online Writing Feedback Resources: Generative <fixed-case>AI</fixed-case> vs. Good Samaritans</title>
      <author><first>Shabnam</first><last>Behzad</last></author>
      <author><first>Omid</first><last>Kashefi</last></author>
      <author><first>Swapna</first><last>Somasundaran</last></author>
      <pages>1638–1644</pages>
      <abstract>Providing constructive feedback on student essays is a critical factor in improving educational results; however, it presents notable difficulties and may demand substantial time investments, especially when aiming to deliver individualized and informative guidance. This study undertakes a comparative analysis of two readily available online resources for students seeking to hone their skills in essay writing for English proficiency tests: 1) essayforum.com, a widely used platform where students can submit their essays and receive feedback from volunteer educators at no cost, and 2) Large Language Models (LLMs) such as ChatGPT. By contrasting the feedback obtained from these two resources, we posit that they can mutually reinforce each other and are more helpful if employed in conjunction when seeking no-cost online assistance. The findings of this research shed light on the challenges of providing personalized feedback and highlight the potential of AI in advancing the field of automated essay evaluation.</abstract>
      <url hash="ade7b069">2024.lrec-main.144</url>
      <bibkey>behzad-etal-2024-assessing-online</bibkey>
    </paper>
    <paper id="145">
      <title>Assessing the Capabilities of Large Language Models in Coreference: An Evaluation</title>
      <author><first>Yujian</first><last>Gan</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Juntao</first><last>Yu</last></author>
      <pages>1645–1665</pages>
      <abstract>This paper offers a nuanced examination of the role Large Language Models (LLMs) play in coreference resolution, aimed at guiding the future direction in the era of LLMs. We carried out both manual and automatic analyses of different LLMs’ abilities, employing different prompts to examine the performance of different LLMs, obtaining a comprehensive view of their strengths and weaknesses. We found that LLMs show exceptional ability in understanding coreference. However, harnessing this ability to achieve state of the art results on traditional datasets and benchmarks isn’t straightforward. Given these findings, we propose that future efforts should: (1) Improve the scope, data, and evaluation methods of traditional coreference research to adapt to the development of LLMs. (2) Enhance the fine-grained language understanding capabilities of LLMs.</abstract>
      <url hash="a5ba85fe">2024.lrec-main.145</url>
      <bibkey>gan-etal-2024-assessing-capabilities</bibkey>
    </paper>
    <paper id="146">
      <title>Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the <fixed-case>J</fixed-case>apanese Context</title>
      <author><first>Qiao</first><last>Wang</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <pages>1666–1672</pages>
      <abstract>In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students’ writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger’s performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger’s performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model’s high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant. Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures. Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students’ first language (L1), remained particularly challenging.</abstract>
      <url hash="662bc90f">2024.lrec-main.146</url>
      <bibkey>wang-yuan-2024-assessing-efficacy</bibkey>
    </paper>
    <paper id="147">
      <title>A Streamlined Span-based Factorization Method for Few Shot Named Entity Recognition</title>
      <author><first>Wenjie</first><last>Xu</last></author>
      <author><first>Yidan</first><last>Chen</last></author>
      <author><first>Jianquan</first><last>Ouyang</last></author>
      <pages>1673–1683</pages>
      <abstract>Few-shot named entity recognition (NER) is a challenging task that aims to recognize new named entities with only a limited amount of labeled examples. In this paper, we introduce SSF, which is a streamlined span-based factorization method that addresses the problem of few-shot NER. Our approach formulates few-shot NER as a span-level alignment problem between query and support instances. To achieve this goal, SSF decomposes the span-level alignment problem into several refined span-level procedures. The proposed approach encompasses several key modules such as the Span Boosting Module, Span Prototypical Module, Span Alignment Module, and Span Optimization Module. Our experimental results demonstrate a significant improvement over the previous state-of-the-art performance. Specifically, compared to previous methods, our proposed approach achieves an average F1 score improvement of 12 points on the FewNERD dataset and 10 points on the SNIPS dataset. Moreover, our approach has surpassed the latest state-of-the-art performance on both datasets.</abstract>
      <url hash="ecbdf4fc">2024.lrec-main.147</url>
      <bibkey>xu-etal-2024-streamlined-span</bibkey>
    </paper>
    <paper id="148">
      <title>A Study on How Attention Scores in the <fixed-case>BERT</fixed-case> Model Are Aware of Lexical Categories in Syntactic and Semantic Tasks on the <fixed-case>GLUE</fixed-case> Benchmark</title>
      <author><first>Dongjun</first><last>Jang</last></author>
      <author><first>Sungjoo</first><last>Byun</last></author>
      <author><first>Hyopil</first><last>Shin</last></author>
      <pages>1684–1689</pages>
      <abstract>This study examines whether the attention scores between tokens in the BERT model significantly vary based on lexical categories during the fine-tuning process for downstream tasks. Drawing inspiration from the notion that in human language processing, syntactic and semantic information is parsed differently, we categorize tokens in sentences according to their lexical categories and focus on changes in attention scores among these categories. Our hypothesis posits that in downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases emphasizing syntactic information, attention scores centered on function words are intensified. Through experimentation conducted on six tasks from the GLUE benchmark dataset, we substantiate our hypothesis regarding the fine-tuning process. Furthermore, our additional investigations reveal the presence of BERT layers that consistently assign more bias to specific lexical categories, irrespective of the task, highlighting the existence of task-agnostic lexical category preferences.</abstract>
      <url hash="8c7f9cba">2024.lrec-main.148</url>
      <bibkey>jang-etal-2024-study-attention</bibkey>
    </paper>
    <paper id="149">
      <title>A Survey on Natural Language Processing for Programming</title>
      <author><first>Qingfu</first><last>Zhu</last></author>
      <author><first>Xianzhen</first><last>Luo</last></author>
      <author><first>Fang</first><last>Liu</last></author>
      <author><first>Cuiyun</first><last>Gao</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>1690–1704</pages>
      <abstract>Natural language processing for programming aims to use NLP techniques to assist programming. It is increasingly prevalent for its effectiveness in improving productivity. Distinct from natural language, a programming language is highly structured and functional. Constructing a structure-based representation and a functionality-oriented algorithm is at the heart of program understanding and generation. In this paper, we conduct a systematic review covering tasks, datasets, evaluation methods, techniques, and models from the perspective of the structure-based and functionality-oriented property, aiming to understand the role of the two properties in each component. Based on the analysis, we illustrate unexplored areas and suggest potential directions for future work.</abstract>
      <url hash="f55b64e0">2024.lrec-main.149</url>
      <bibkey>zhu-etal-2024-survey-natural</bibkey>
    </paper>
    <paper id="150">
      <title>A Tool for Determining Distances and Overlaps between Multimodal Annotations</title>
      <author><first>Camila Antonio</first><last>Barros</last></author>
      <author><first>Jorge Francisco</first><last>Ciprián-Sánchez</last></author>
      <author><first>Saulo Mendes</first><last>Santos</last></author>
      <pages>1705–1714</pages>
      <abstract>Comparing annotations is a constant and necessary step in corpus analysis. Although the nature of these annotations is normally research-specific, the tools used for this purpose do not have to be. Here, we present a tool for extracting and comparing annotations from ELAN, despite their idiosyncrasies. The intention behind this tool is to provide a handy way to analyze ELAN annotated files, by comparing tiers to a reference unit. Using the presented tool, it is possible to see how tiers overlap (even if they are of symbolic type), to which ratio, and the displacement regarding a reference unit. We present an example of multimodal corpus analysis, regarding the coordination between speech and gesture units based on a pragmatic reference. We argue that looking into overlap ratios can be more informative of the association between speech and gestures, and that considering a time buffer between speech and gestural events can be misleading.</abstract>
      <url hash="bd5c8b1a">2024.lrec-main.150</url>
      <bibkey>barros-etal-2024-tool-determining</bibkey>
    </paper>
    <paper id="151">
      <title>A Treebank of <fixed-case>A</fixed-case>sia Minor <fixed-case>G</fixed-case>reek</title>
      <author><first>Eleni</first><last>Vligouridou</last></author>
      <author><first>Inessa</first><last>Iliadou</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>1715–1721</pages>
      <abstract>Asia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and cultAsia Minor Greek (AMG) dialects are endangered dialects rich in history and culture that face a dire struggle for preservation due to declining speaker base and scarce linguistic resources. To address this need, we introduce a Universal Dependencies treebank of Pharasiot Greek, one of the severly endangerd AMG dialects. The present treebank is fully manually annotated and currently consists of 350 sentences from six fairy tales in Pharasiot dialect. Besides describing the treebank and the annotation process, we provide and discuss interesting phenomena we observed in the treebank. Most phenomena we discuss are related to contact-induced linguistic changes that these dialects are well known for. Beyond linguistic inquiry, like other treebanks for truly low-resource languages, the AMG treebank we present offers potentials for diverse applications, such as language preservation and revitalization, as well as NLP tools that have to be developed with scarce resources.</abstract>
      <url hash="4862afe9">2024.lrec-main.151</url>
      <bibkey>vligouridou-etal-2024-treebank-asia</bibkey>
    </paper>
    <paper id="152">
      <title>A Trusted Multi-View Evidential Fusion Framework for Commonsense Reasoning</title>
      <author><first>Shuo</first><last>Yang</last></author>
      <pages>1722–1733</pages>
      <abstract>While deep learning models are powerful, they have limitations in tasks that require commonsense reasoning, as these tasks often involve interpreting information that may not be directly available in the input. Providing evidence has been proven to significantly enhance performance in commonsense reasoning tasks. However, there are various perspectives on evidence, including natural language explanations generated by pre-trained language models, facts derived from world knowledge like text corpora and knowledge bases, and rationales extracted from the input context. Hence, it is crucial to determine how to estimate the confidence degree of different evidence and how to combine them reliably. To address these challenges, this study proposes a trusted multi-view evidential fusion framework for reliable commonsense reasoning tasks that dynamically assesses the confidence of evidence and combines different views of evidence in a trustworthy manner. The proposed method is applied to three commonsense question-answering benchmarks, demonstrating that this approach can effectively reason with multi-view evidence and can compete with state-of-the-art performance.</abstract>
      <url hash="ea8fe48c">2024.lrec-main.152</url>
      <bibkey>yang-2024-trusted-multi</bibkey>
    </paper>
    <paper id="153">
      <title>Attack Named Entity Recognition by Entity Boundary Interference</title>
      <author><first>Yifei</first><last>Yang</last></author>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>1734–1744</pages>
      <abstract>Named Entity Recognition (NER) is a cornerstone natural language processing task while its robustness has been given little attention. This paper rethinks the principles of the conventional text attack, as they can easily violate the label consistency between the original and adversarial NER samples. This is due to the fine-grained nature of NER, as even minor word changes in the sentence can result in the emergence or mutation of any entity, producing invalid adversarial samples. To this end, we propose a novel one-word modification NER attack based on a key insight, NER models are always vulnerable to the boundary position of an entity to make their decision. We thus strategically insert a new boundary into the sentence and trigger the victim model to make a wrong recognition either on this boundary word or on other words in the sentence. We call this attack <i>Virtual Boundary Attack (ViBA)</i>, which is shown to be remarkably effective when attacking both English and Chinese models with a 70%-90% attack success rate on state-of-the-art language models, and also significantly faster than previous methods.</abstract>
      <url hash="37b83e92">2024.lrec-main.153</url>
      <bibkey>yang-etal-2024-attack-named</bibkey>
    </paper>
    <paper id="154">
      <title>At the Crossroad of Cuneiform and <fixed-case>NLP</fixed-case>: Challenges for Fine-grained Part-of-speech Tagging</title>
      <author><first>Gustav Ryberg</first><last>Smidt</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <author><first>Katrien</first><last>de Graef</last></author>
      <pages>1745–1755</pages>
      <abstract>The study of ancient Middle Eastern cultures is dominated by the vast number of cuneiform texts. Multiple languages and language families were expressed in cuneiform. The most dominant language written in cuneiform is the Semitic Akkadian, which is the focus of this paper. We are specifically focusing on letters written in the dialect used in modern-day Baghdad and south towards the Persian Gulf during the Old Babylonian period (c. 2000-1600 B.C.E.). The Akkadian language was rediscovered in the 19th century and is now being scrutinised by Natural Language Processing (NLP) methods. However, existing Akkadian text publications are not always suitable for digital editions. We therefore risk applying NLP methods onto renderings of Akkadian unfit for the purpose. In this paper we want to investigate the input material and try to initiate a discussion about best-practices in the crossroad where NLP meets cuneiform studies. Specifically, we want to question the use of pre-trained embeddings, sentence segmentation and the type of cuneiform input used to fine-tune language models for the task of fine-grained part-of-speech tagging. We examine the issues by theoretical and practical approaches in a way that we hope spurs discussions that are relevant for automatic processing of other ancient languages.</abstract>
      <url hash="6ded8b94">2024.lrec-main.154</url>
      <bibkey>smidt-etal-2024-crossroad-cuneiform</bibkey>
    </paper>
    <paper id="155">
      <title>A <fixed-case>T</fixed-case>ulu Resource for Machine Translation</title>
      <author><first>Manu</first><last>Narayanan</last></author>
      <author><first>Noëmi</first><last>Aepli</last></author>
      <pages>1756–1767</pages>
      <abstract>We present the first parallel dataset for English–Tulu translation. Tulu, classified within the South Dravidian linguistic family branch, is predominantly spoken by approximately 2.5 million individuals in southwestern India. Our dataset is constructed by integrating human translations into the multilingual machine translation resource FLORES-200. Furthermore, we use this dataset for evaluation purposes in developing our English–Tulu machine translation model. For the model’s training, we leverage resources available for related South Dravidian languages. We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages. This method enables the training of a machine translation system even in the absence of parallel data between the source and target language, thereby overcoming a significant obstacle in machine translation development for low-resource languages. Our English–Tulu system, trained without using parallel English–Tulu data, outperforms Google Translate by 19 BLEU points (in September 2023). The dataset and code are available here: https://github.com/manunarayanan/Tulu-NMT.</abstract>
      <url hash="c7ee98ff">2024.lrec-main.155</url>
      <bibkey>narayanan-aepli-2024-tulu-resource</bibkey>
    </paper>
    <paper id="156">
      <title>A Two-Stage Framework with Self-Supervised Distillation for Cross-Domain Text Classification</title>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Bohan</first><last>Li</last></author>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Xiao</first><last>Xu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>1768–1777</pages>
      <abstract>Cross-domain text classification is a crucial task as it enables models to adapt to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with <i>self-supervised distillation</i> (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptations (94.17% +1.03%) and multi-source domain adaptations (95.09% +1.34%).</abstract>
      <url hash="6862d475">2024.lrec-main.156</url>
      <bibkey>feng-etal-2024-two-stage</bibkey>
    </paper>
    <paper id="157">
      <title>A Two-Stage Prediction-Aware Contrastive Learning Framework for Multi-Intent <fixed-case>NLU</fixed-case></title>
      <author><first>Guanhua</first><last>Chen</last></author>
      <author><first>Yutong</first><last>Yao</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <pages>1778–1788</pages>
      <abstract>Multi-intent natural language understanding (NLU) presents a formidable challenge due to the model confusion arising from multiple intents within a single utterance. While previous works train the model contrastively to increase the margin between different multi-intent labels, they are less suited to the nuances of multi-intent NLU. They ignore the rich information between the shared intents, which is beneficial to constructing a better embedding space, especially in low-data scenarios. We introduce a two-stage Prediction-Aware Contrastive Learning (PACL) framework for multi-intent NLU to harness this valuable knowledge. Our approach capitalizes on shared intent information by integrating word-level pre-training and prediction-aware contrastive fine-tuning. We construct a pre-training dataset using a word-level data augmentation strategy. Subsequently, our framework dynamically assigns roles to instances during contrastive fine-tuning while introducing a prediction-aware contrastive loss to maximize the impact of contrastive learning. We present experimental results and empirical analysis conducted on three widely used datasets, demonstrating that our method surpasses the performance of three prominent baselines on both low-data and full-data scenarios.</abstract>
      <url hash="1b6d3cf6">2024.lrec-main.157</url>
      <bibkey>chen-etal-2024-two-stage</bibkey>
    </paper>
    <paper id="158">
      <title>A Typology of Errors for User Utterances in Chatbots</title>
      <author><first>Anu</first><last>Singh</last></author>
      <author><first>Esme</first><last>Manandise</last></author>
      <pages>1789–1794</pages>
      <abstract>This paper discusses the challenges non-prescriptive language uses in chatbot communication create for Semantic Parsing (SP). To help SP developers improve their systems, we propose a flexible error typology based on an analysis of a sample of non-prescriptive language uses mined from a domain-specific chatbot logs. This typology is not tied to any specific language model. We also present a framework for automatically mapping these errors to the typology. Finally, we show how our framework can help evaluate SP systems from a linguistic robustness perspective. Our framework can be expanded to include new classes of errors across different domains and user demographics.</abstract>
      <url hash="8a60fde9">2024.lrec-main.158</url>
      <bibkey>singh-manandise-2024-typology-errors</bibkey>
    </paper>
    <paper id="159">
      <title>Audiocite.net : A Large Spoken Read Dataset in <fixed-case>F</fixed-case>rench</title>
      <author><first>Soline</first><last>Felice</last></author>
      <author><first>Solene Virginie</first><last>Evain</last></author>
      <author><first>Solange</first><last>Rossato</last></author>
      <author><first>François</first><last>Portet</last></author>
      <pages>1795–1800</pages>
      <abstract>The advent of self-supervised learning (SSL) in speech processing has allowed the use of large unlabeled datasets to learn pre-trained models, serving as powerful encoders for various downstream tasks. However, the application of these SSL methods to languages such as French has proved difficult due to the scarcity of large French speech datasets. To advance the emergence of pre-trained models for French speech, we present the Audiocite.net corpus composed of 6,682 hours of recordings from 130 readers. This corpus is composed of audiobooks from the audiocite.net website, shared by 130 readers. In addition to describing the creation process and final statistics, we also show how this dataset impacted the models of LeBenchmark project in its 14k version for speech processing downstream tasks.</abstract>
      <url hash="773d1016">2024.lrec-main.159</url>
      <bibkey>felice-etal-2024-audiocite-net</bibkey>
    </paper>
    <paper id="160">
      <title><fixed-case>A</fixed-case>u<fixed-case>R</fixed-case>o<fixed-case>RA</fixed-case>: A One-for-all Platform for Augmented Reasoning and Refining with Task-Adaptive Chain-of-Thought Prompting</title>
      <author><first>Anni</first><last>Zou</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>1801–1807</pages>
      <abstract>Large language models (LLMs) empowered by chain-of-thought (CoT) prompting have yielded remarkable prowess in reasoning tasks. Nevertheless, current methods predominantly lean on handcrafted or task-specific demonstrations, lack reliable knowledge basis and thus struggle for trustworthy responses in an automated pattern. While recent works endeavor to improve upon one certain aspect, they ignore the importance and necessity of establishing an integrated and interpretable reasoning system. To address these drawbacks and provide a universal solution, we propose AuRoRA: a one-for-all platform for augmented reasoning and refining based on CoT prompting that excels in adaptability, reliability, integrity, and interpretability. The system exhibits superior performances across six reasoning tasks and offers real-time visual analysis, which has pivotal academic and application value in the era of LLMs. The AuRoRA platform is available at https://huggingface.co/spaces/Anni123/AuRoRA.</abstract>
      <url hash="44d9f53e">2024.lrec-main.160</url>
      <bibkey>zou-etal-2024-aurora-one</bibkey>
    </paper>
    <paper id="161">
      <title>Automated Extraction of Prosodic Structure from Unannotated Sign Language Video</title>
      <author><first>Antonio F. G.</first><last>Sevilla</last></author>
      <author><first>José María</first><last>Lahoz-Bengoechea</last></author>
      <author><first>Alberto</first><last>Diaz</last></author>
      <pages>1808–1816</pages>
      <abstract>As in oral phonology, prosody is an important carrier of linguistic information in sign languages. One of the most prominent ways this reveals itself is in the time structure of signs: their rhythm and intensity of articulation. To be able to empirically see these effects, the velocity of the hands can be computed throughout the execution of a sign. In this article, we propose a method for extracting this information from unlabeled videos of sign language, exploiting CoTracker, a recent advancement in computer vision which can track every point in a video without the need of any calibration or fine-tuning. The dominant hand is identified via clustering of the computed point velocities, and its dynamic profile plotted to make apparent the prosodic structure of signing. We apply our method to different datasets and sign languages, and perform a preliminary visual exploration of results. This exploration supports the usefulness of our methodology for linguistic analysis, though issues to be tackled remain, such as bi-manual signs and a formal and numerical evaluation of accuracy. Nonetheless, the absence of any preprocessing requirements may make it useful for other researchers and datasets.</abstract>
      <url hash="f6c6cad4">2024.lrec-main.161</url>
      <bibkey>sevilla-etal-2024-automated-extraction</bibkey>
    </paper>
    <paper id="162">
      <title>Automatically Estimating Textual and Phonemic Complexity for Cued Speech: How to See the Sounds from <fixed-case>F</fixed-case>rench Texts</title>
      <author><first>Núria</first><last>Gala</last></author>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Marie</first><last>Bauer</last></author>
      <pages>1817–1824</pages>
      <abstract>In this position paper we present a methodology to automatically annotate French text for Cued Speech (CS), a communication system developed for people with hearing loss to complement speech reading at the phonetic level. This visual communication mode uses handshapes in different placements near the face in combination with the mouth movements (called ‘cues’ or ‘keys’) to make the phonemes of spoken language look different from each other. CS is used to acquire skills in lip reading, in oral communication and for reading. Despite many studies demonstrating its benefits, there are few resources available for learning and practicing it, especially in French. We thus propose a methodology to phonemize written corpora so that each word is aligned with the corresponding CS key(s). This methodology is proposed as part of a wider project aimed at creating an augmented reality system displaying a virtual coding hand where the user will be able to choose a text upon its complexity for cueing.</abstract>
      <url hash="e8fc29c9">2024.lrec-main.162</url>
      <bibkey>gala-etal-2024-automatically-estimating</bibkey>
    </paper>
    <paper id="163">
      <title>Automatic <fixed-case>A</fixed-case>nimacy Classification for <fixed-case>R</fixed-case>omanian Nouns</title>
      <author><first>Maria</first><last>Tepei</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <pages>1825–1831</pages>
      <abstract>We introduce the first Romanian animacy classifier, specifically a type-based binary classifier of Romanian nouns into the classes human/non-human, using pre-trained word embeddings and animacy information derived from Romanian WordNet. By obtaining a seed set of labeled nouns and their embeddings, we are able to train classifiers that generalize to unseen nouns. We compare three different architectures and observe good performance on classifying word types. In addition, we manually annotate a small corpus for animacy to perform a token-based evaluation of Romanian animacy classification in a naturalistic setting, which reveals limitations of the type-based classification approach.</abstract>
      <url hash="d49a477a">2024.lrec-main.163</url>
      <bibkey>tepei-bloem-2024-automatic-animacy</bibkey>
    </paper>
    <paper id="164">
      <title>Automatic Annotation of Grammaticality in Child-Caregiver Conversations</title>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>Abhishek</first><last>Agrawal</last></author>
      <author><first>Petros</first><last>Kaklamanis</last></author>
      <author><first>Alex</first><last>Warstadt</last></author>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <pages>1832–1844</pages>
      <abstract>The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that fine-tuned Transformer-based models perform best, achieving human inter-annotation agreement levels. As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children’s grammaticality shows a steady increase with age. This work contributes to the growing literature on applying state-of-the-art NLP methods to help study child language acquisition at scale.</abstract>
      <url hash="68bdcdfd">2024.lrec-main.164</url>
      <bibkey>nikolaus-etal-2024-automatic-annotation</bibkey>
    </paper>
    <paper id="165">
      <title>Automatic Authorship Analysis in Human-<fixed-case>AI</fixed-case> Collaborative Writing</title>
      <author><first>Aquia</first><last>Richburg</last></author>
      <author><first>Calvin</first><last>Bao</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>1845–1855</pages>
      <abstract>As the quality of AI-generated text increases with the development of new Large Language Models, people use them to write in a variety of contexts. Human-AI collaborative writing poses a potential challenge for existing AI analysis techniques, which have been primarily tested either on human-written text only, or on samples independently generated by humans and AI. In this work, we investigate the extent to which existing AI detection and authorship analysis models can perform classification on data generated in human-AI collaborative writing sessions. Results show that, for AI text detection in the cowriting setting, classifiers based on authorship embeddings (Rivera-Soto et al., 2021) outperform classifiers used in prior work distinguishing AI vs. human text generated independently. However, these embeddings are not optimal for finer-grained authorship identification tasks: for authorship verification, n-gram based models are more robust to human-AI co-written text, and authorship attribution performance degrades compared to baselines that use human-written text only. Taken together, this suggests that the rise of human-AI co-written text will require adapting AI detection tools and authorship analysis techniques in the near future. We release our code at https://github.com/AARichburg/Human-AI_Authorship_Analysis.</abstract>
      <url hash="82b9c16e">2024.lrec-main.165</url>
      <bibkey>richburg-etal-2024-automatic-authorship</bibkey>
    </paper>
    <paper id="166">
      <title>Automatic Coding of Contingency in Child-Caregiver Conversations</title>
      <author><first>Abhishek</first><last>Agrawal</last></author>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <pages>1856–1870</pages>
      <abstract>One of the most important communicative skills children have to learn is to engage in meaningful conversations with people around them. At the heart of this learning lies the mastery of contingency, i.e., the ability to contribute to an ongoing exchange in a relevant fashion (e.g., by staying on topic). Current research on this question relies on the manual annotation of a small sample of children, which limits our ability to draw general conclusions about development. Here, we propose to mitigate the limitations of manual labor by relying on automatic tools for contingency judgment in children’s early natural interactions with caregivers. Drawing inspiration from the field of dialogue systems evaluation, we built and compared several automatic classifiers. We found that a Transformer-based pre-trained language model – when fine-tuned on a relatively small set of data we annotated manually (around 3,500 turns) – provided the best predictions. We used this model to automatically annotate, new and large-scale data, almost two orders of magnitude larger than our fine-tuning set. It was able to replicate existing results and generate new data-driven hypotheses. The broad impact of the work is to provide resources that can help the language development community study communicative development at scale, leading to more robust theories.</abstract>
      <url hash="0bf147c1">2024.lrec-main.166</url>
      <bibkey>agrawal-etal-2024-automatic-coding</bibkey>
    </paper>
    <paper id="167">
      <title>Automatic Construction of a <fixed-case>C</fixed-case>hinese Review Dataset for Aspect Sentiment Triplet Extraction via Iterative Weak Supervision</title>
      <author><first>Chia-Wen</first><last>Lu</last></author>
      <author><first>Ching-Wen</first><last>Yang</last></author>
      <author><first>Wei-Yun</first><last>Ma</last></author>
      <pages>1871–1882</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE), introduced in 2020, is a task that involves the extraction of three key elements: target aspects, descriptive opinion spans, and their corresponding sentiment polarity. This process, however, faces a significant hurdle, particularly when applied to Chinese languages, due to the lack of sufficient datasets for model training, largely attributable to the arduous manual labeling process. To address this issue, we present an innovative framework that facilitates the automatic construction of ASTE via Iterative Weak Supervision, negating the need for manual labeling, aided by a discriminator to weed out subpar samples. The objective is to successively improve the quality of this raw data and generate supplementary data. The effectiveness of our approach is underscored by our results, which include the creation of a substantial Chinese review dataset. This dataset encompasses over 60,000 Google restaurant reviews in Chinese and features more than 200,000 extracted triplets. Moreover, we have also established a robust baseline model by leveraging a novel method of weak supervision. Both our dataset and model are openly accessible to the public.</abstract>
      <url hash="dedd35b0">2024.lrec-main.167</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9ff71dc7">2024.lrec-main.167.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lu-etal-2024-automatic-construction</bibkey>
    </paper>
    <paper id="168">
      <title>Automatic Construction of a Large-Scale Corpus for Geoparsing Using <fixed-case>W</fixed-case>ikipedia Hyperlinks</title>
      <author><first>Keyaki</first><last>Ohno</last></author>
      <author><first>Hirotaka</first><last>Kameko</last></author>
      <author><first>Keisuke</first><last>Shirai</last></author>
      <author><first>Taichi</first><last>Nishimura</last></author>
      <author><first>Shinsuke</first><last>Mori</last></author>
      <pages>1883–1888</pages>
      <abstract>Geoparsing is the task of estimating the latitude and longitude (coordinates) of location expressions in texts. Geoparsing must deal with the ambiguity of the expressions that indicate multiple locations with the same notation. For evaluating geoparsing systems, several corpora have been proposed in previous work. However, these corpora are small-scale and suffer from the coverage of location expressions on general domains. In this paper, we propose Wikipedia Hyperlink-based Location Linking (WHLL), a novel method to construct a large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages hyperlinks in Wikipedia to annotate multiple location expressions with coordinates. With this method, we constructed the WHLL corpus, a new large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles, each containing about 7.8 unique location expressions. 45.6% of location expressions are ambiguous and refer to more than one location with the same notation. In each article, location expressions of the article title and those hyperlinks to other articles are assigned with coordinates. By utilizing hyperlinks, we can accurately assign location expressions with coordinates even with ambiguous location expressions in the texts. Experimental results show that there remains room for improvement by disambiguating location expressions.</abstract>
      <url hash="39e21518">2024.lrec-main.168</url>
      <bibkey>ohno-etal-2024-automatic-construction</bibkey>
    </paper>
    <paper id="169">
      <title>Automatic Data Visualization Generation from <fixed-case>C</fixed-case>hinese Natural Language Questions</title>
      <author><first>Yan</first><last>Ge</last></author>
      <author><first>Victor Junqiu</first><last>Wei</last></author>
      <author><first>Yuanfeng</first><last>Song</last></author>
      <author><first>Jason Chen</first><last>Zhang</last></author>
      <author><first>Raymond Chi-Wing</first><last>Wong</last></author>
      <pages>1889–1898</pages>
      <abstract>Data visualization has emerged as an effective tool for getting insights from massive datasets. Due to the hardness of manipulating the programming languages of data visualization, automatic data visualization generation from natural languages (Text-to-Vis) is becoming increasingly popular. Despite the plethora of research effort on the English Text-to-Vis, studies have yet to be conducted on data visualization generation from questions in Chinese. Motivated by this, we propose a Chinese Text-to-Vis dataset in the paper and demonstrate our first attempt to tackle this problem. Our model integrates multilingual BERT as the encoder, boosts the cross-lingual ability, and infuses the n-gram information into our word representation learning. Our experimental results show that our dataset is challenging and deserves further research.</abstract>
      <url hash="a0229c5b">2024.lrec-main.169</url>
      <bibkey>ge-etal-2024-automatic-data</bibkey>
    </paper>
    <paper id="170">
      <title>Automatic Decomposition of Text Editing Examples into Primitive Edit Operations: Toward Analytic Evaluation of Editing Systems</title>
      <author><first>Daichi</first><last>Yamaguchi</last></author>
      <author><first>Rei</first><last>Miyata</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Satoshi</first><last>Sato</last></author>
      <pages>1899–1914</pages>
      <abstract>This paper presents our work on a task of automatic decomposition of text editing examples into primitive edit operations. Toward a detailed analysis of the behavior of text editing systems, identification of fine-grained edit operations performed by the systems is essential. Given a pair of source and edited sentences, the goal of our task is to generate a non-redundant sequence of primitive edit operations, i.e., the semantically minimal edit operations preserving grammaticality, that iteratively converts the source sentence to the edited sentence. First, we formalize this task, explaining its significant features and specifying the constraints that primitive edit operations should satisfy. Then, we propose a method to automate this task, which consists of two steps: generation of an edit operation lattice and selection of an optimal path. To obtain a wide range of edit operation candidates in the first step, we combine a phrase aligner and a large language model. Experimental results show that our method perfectly decomposes 44% and 64% of editing examples in the text simplification and machine translation post-editing datasets, respectively. Detailed analyses also provide insights into the difficulties of this task, suggesting directions for improvement.</abstract>
      <url hash="b14dacfc">2024.lrec-main.170</url>
      <bibkey>yamaguchi-etal-2024-automatic-decomposition</bibkey>
    </paper>
    <paper id="171">
      <title>Automatic Extraction of Language-Specific Biomarkers of Healthy Aging in <fixed-case>I</fixed-case>celandic</title>
      <author><first>Elena</first><last>Callegari</last></author>
      <author><first>Iris Edda</first><last>Nowenstein</last></author>
      <author><first>Ingunn Jóhanna</first><last>Kristjánsdóttir</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>1915–1924</pages>
      <abstract>This study examines the influence of task type and healthy aging on various automatically extracted part-of-speech features in Icelandic. We administered three language tasks to participants aged 60–80: picture description, trip planning, and description of one’s childhood home. Our findings reveal significant task effects on 11 out of 14 linguistic variables studied, highlighting the substantial influence of sampling methods on language production. Among the variables showing statistically significant task effects, we find the rate of the genitive and subjunctive, variables which can only be studied in morphologically richer languages like Icelandic. On the other hand, rates of pronouns, adverbs, and prepositions remained stable across task types. Aging effects were more subtle, being evident in 3 of the 14 variables, including an interaction with task type for dative case marking. These findings underscore the significance of task selection in studies targeting linguistic features but also emphasize the need to examine languages other than English to fully understand the effects of aging on language production. Additionally, the results have clinical implications: understanding healthy aging’s impact on language can help us better identify and study changes caused by Alzheimer’s Disease in older adults’ speech.</abstract>
      <url hash="35436156">2024.lrec-main.171</url>
      <bibkey>callegari-etal-2024-automatic-extraction</bibkey>
    </paper>
    <paper id="172">
      <title>Automatic Extraction of Nominal Phrases from <fixed-case>G</fixed-case>erman Learner Texts of Different Proficiency Levels</title>
      <author><first>Ronja</first><last>Laarmann-Quante</last></author>
      <author><first>Marco</first><last>Müller</last></author>
      <author><first>Eva</first><last>Belke</last></author>
      <pages>1925–1931</pages>
      <abstract>Correctly inflecting determiners and adjectives so that they agree with the noun in nominal phrases (NPs) is a big challenge for learners of German. Given the increasing number of available learner corpora, a large-scale corpus-based study on the acquisition of this aspect of German morphosyntax would be desirable. In this paper, we present a pilot study in which we investigate how well nouns, their grammatical heads and the dependents that have to agree with the noun can be extracted automatically via dependency parsing. For six samples of the German learner corpus MERLIN (one per proficiency level), we found that in spite of many ungrammatical sentences in texts of low proficiency levels, human annotators find only few true ambiguities that would make the extraction of NPs and their heads infeasible. The automatic parsers, however, perform rather poorly on extracting the relevant elements for texts on CEFR levels A1-B1 (&lt; 70%) but quite well from level B2 onwards ( 90%). We discuss the sources of errors and how performance could potentially be increased in the future.</abstract>
      <url hash="f290d95d">2024.lrec-main.172</url>
      <bibkey>laarmann-quante-etal-2024-automatic-extraction</bibkey>
    </paper>
    <paper id="173">
      <title>Automatic Identification of <fixed-case>COVID</fixed-case>-19-Related Conspiracy Narratives in <fixed-case>G</fixed-case>erman Telegram Channels and Chats</title>
      <author><first>Philipp</first><last>Heinrich</last></author>
      <author><first>Andreas</first><last>Blombach</last></author>
      <author><first>Bao Minh</first><last>Doan Dang</last></author>
      <author><first>Leonardo</first><last>Zilio</last></author>
      <author><first>Linda</first><last>Havenstein</last></author>
      <author><first>Nathan</first><last>Dykes</last></author>
      <author><first>Stephanie</first><last>Evert</last></author>
      <author><first>Fabian</first><last>Schäfer</last></author>
      <pages>1932–1943</pages>
      <abstract>We are concerned with mapping the discursive landscape of conspiracy narratives surrounding the COVID-19 pandemic. In the present study, we analyse a corpus of more than 1,000 German Telegram posts tagged with 14 fine-grained conspiracy narrative labels by three independent annotators. Since emerging narratives on social media are short-lived and notoriously hard to track, we experiment with different state-of-the-art approaches to few-shot and zero-shot text classification. We report performance in terms of ROC-AUC and in terms of optimal F1, and compare fine-tuned methods with off-the-shelf approaches and human performance.</abstract>
      <url hash="c1a98848">2024.lrec-main.173</url>
      <bibkey>heinrich-etal-2024-automatic-identification</bibkey>
    </paper>
    <paper id="174">
      <title>Automatic Partitioning of a Code-Switched Speech Corpus Using Mixed-Integer Programming</title>
      <author><first>Joshua Miles</first><last>Jansen van Vüren</last></author>
      <author><first>Febe</first><last>de Wet</last></author>
      <author><first>Thomas</first><last>Niesler</last></author>
      <pages>1944–1952</pages>
      <abstract>Defining training, development and test set partitions for speech corpora is usually accomplished by hand. However, for the dataset under investigation, which contains a large number of speakers, eight different languages and code-switching between all the languages, this style of partitioning is not feasible. Therefore, we view the partitioning task as a resource allocation problem and propose to solve it automatically and optimally by the application of mixed-integer linear programming. Using this approach, we are able to partition a new 41.6-hour multilingual corpus of code-switched speech into training, development and testing partitions while maintaining a fixed number of speakers and a specific amount of code-switched speech in the development and test partitions. For this newly partitioned corpus, we present baseline speech recognition results using a state-of-the-art multilingual transformer model (Wav2Vec2-XLS-R) and show that the exclusion of very short utterances (&lt;1s) results in substantially improved speech recognition performance.</abstract>
      <url hash="d4792276">2024.lrec-main.174</url>
      <bibkey>jansen-van-vuren-etal-2024-automatic-partitioning</bibkey>
    </paper>
    <paper id="175">
      <title>Automatic Punctuation Model for <fixed-case>S</fixed-case>panish Live Transcriptions</title>
      <author><first>Mario</first><last>Perez-Enriquez</last></author>
      <author><first>Jose Manuel</first><last>Masiello-Ruiz</last></author>
      <author><first>Jose Luis</first><last>Lopez-Cuadrado</last></author>
      <author><first>Israel</first><last>Gonzalez-Carrasco</last></author>
      <author><first>Paloma</first><last>Martinez-Fernandez</last></author>
      <author><first>Belen</first><last>Ruiz-Mezcua</last></author>
      <pages>1953–1958</pages>
      <abstract>With the widespread adoption of automatic transcription tools, acquiring speech transcriptions within seconds has become a reality. Nonetheless, many of these tools yield unpunctuated outputs, potentially incurring additional costs. This paper presents a novel approach to integrating punctuation into the transcriptions generated by such automatic tools, specifically focusing on Spanish-speaking contexts. Leveraging the RoBERTa-bne model pre-trained with data from the Spanish National Library, our training proposal is augmented with additional corpora to enhance performance on less common punctuation marks, such as question marks. Also, the proposed model has been trained through fine-tuning pre-trained models, involving adjustments for token classification and using SoftMax to identify the highest probability token. The proposed model obtains promising results when compared with other Spanish reference paper models. Ultimately, this model aims to facilitate punctuation on live transcriptions seamlessly and accurately. The proposed model will be applied to a real-case education project to improve the readability of the transcriptions.</abstract>
      <url hash="55c1d7bc">2024.lrec-main.175</url>
      <bibkey>perez-enriquez-etal-2024-automatic-punctuation</bibkey>
    </paper>
    <paper id="176">
      <title>Automatic Speech Interruption Detection: Analysis, Corpus, and System</title>
      <author><first>Martin</first><last>Lebourdais</last></author>
      <author><first>Marie</first><last>Tahon</last></author>
      <author><first>Antoine</first><last>Laurent</last></author>
      <author><first>Sylvain</first><last>Meignier</last></author>
      <pages>1959–1968</pages>
      <abstract>Interruption detection is a new yet challenging task in the field of speech processing. This article presents a comprehensive study on automatic speech interruption detection, from the definition of this task, the assembly of a specialized corpus, and the development of an initial baseline system. We provide three main contributions: Firstly, we define the task, taking into account the nuanced nature of interruptions within spontaneous conversations. Secondly, we introduce a new corpus of conversational data, annotated for interruptions, to facilitate research in this domain. This corpus serves as a valuable resource for evaluating and advancing interruption detection techniques. Lastly, we present a first baseline system, which use speech processing methods to automatically identify interruptions in speech with promising results. In this article, we derivate from theoretical notions of interruption to build a simplification of this notion based on overlapped speech detection. Our findings can not only serve as a foundation for further research in the field but also provide a benchmark for assessing future advancements in automatic speech interruption detection.</abstract>
      <url hash="e3b8564b">2024.lrec-main.176</url>
      <bibkey>lebourdais-etal-2024-automatic-speech</bibkey>
    </paper>
    <paper id="177">
      <title>Automatic Speech Recognition for <fixed-case>G</fixed-case>ascon and Languedocian Variants of <fixed-case>O</fixed-case>ccitan</title>
      <author><first>Iñigo</first><last>Morcillo</last></author>
      <author><first>Igor</first><last>Leturia</last></author>
      <author><first>Ander</first><last>Corral</last></author>
      <author><first>Xabier</first><last>Sarasola</last></author>
      <author><first>Michaël</first><last>Barret</last></author>
      <author><first>Aure</first><last>Séguier</last></author>
      <author><first>Benaset</first><last>Dazéas</last></author>
      <pages>1969–1978</pages>
      <abstract>This paper describes different approaches for developing, for the first time, an automatic speech recognition system for two of the main dialects of Occitan, namely Gascon and Languedocian, and the results obtained in them. The difficulty of the task lies in the fact that Occitan is a less-resourced language. Although a great effort has been made to collect or create corpora of each variant (transcribed speech recordings for the acoustic models and two text corpora for the language models), the sizes of the corpora obtained are far from those of successful systems reported in the literature, and thus we have tested different techniques to compensate for the lack of resources. We have developed classical systems using Kaldi, creating an acoustic model for each variant and also creating language models from the collected corpora and from machine translated texts. We have also tried fine-tuning a Whisper model with our speech corpora. We report word error rates of 20.86 for Gascon and 13.52 for Languedocian with the Kaldi systems and 16.37 for Gascon and 11.74 for Languedocian with Whisper.</abstract>
      <url hash="0be5beb1">2024.lrec-main.177</url>
      <bibkey>morcillo-etal-2024-automatic-speech</bibkey>
    </paper>
    <paper id="178">
      <title>Automatic Speech Recognition System-Independent Word Error Rate Estimation</title>
      <author><first>Chanho</first><last>Park</last></author>
      <author><first>Mingjie</first><last>Chen</last></author>
      <author><first>Thomas</first><last>Hain</last></author>
      <pages>1979–1987</pages>
      <abstract>Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.</abstract>
      <url hash="bb84a68e">2024.lrec-main.178</url>
      <bibkey>park-etal-2024-automatic-speech</bibkey>
    </paper>
    <paper id="179">
      <title>Automating Dataset Production Using Generative Text and Image Models</title>
      <author><first>Christopher</first><last>Thierauf</last></author>
      <author><first>Mitchell</first><last>Abrams</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <pages>1988–1995</pages>
      <abstract>Practical and ethical dataset collection remains a challenge blocking many empirical methods in natural language processing, resulting in a lack of benchmarks or data on which to test hypotheses. We propose a solution to some of these areas by presenting a pipeline to reduce the research burden of producing image and text datasets when datasets may not exist. Our approach, with accompanying software tools, involves (1) generating text with LLMs; (2) creating accompanying image vignettes with text–to–image transformers; and (3) low-cost human validation. Based on existing literature that has struggled with quantitative evaluation (due to difficulty of data collection), we present the creation of 3 relevant datasets, and conduct a user study that demonstrates this approach is able to aid researchers in obtaining previously-challenging datasets. We provide sample data generated with this technique, the source code used to produce it, and discuss applicability and limitations.</abstract>
      <url hash="98d01bc9">2024.lrec-main.179</url>
      <bibkey>thierauf-etal-2024-automating-dataset</bibkey>
    </paper>
    <paper id="180">
      <title>Autonomous Aspect-Image Instruction a2<fixed-case>II</fixed-case>: <fixed-case>Q</fixed-case>-Former Guided Multimodal Sentiment Classification</title>
      <author><first>Junjia</first><last>Feng</last></author>
      <author><first>Mingqian</first><last>Lin</last></author>
      <author><first>Lin</first><last>Shang</last></author>
      <author><first>Xiaoying</first><last>Gao</last></author>
      <pages>1996–2005</pages>
      <abstract>Multimodal aspect-oriented sentiment classification (MABSC) task has garnered significant attention, which aims to identify the sentiment polarities of aspects by combining both language and vision information. However, the limited multimodal data in this task has become a big gap for the vision-language multimodal fusion. While large-scale vision-language pretrained models have been adapted to multiple tasks, their use for MABSC task is still in a nascent stage. In this work, we present an attempt to use the instruction tuning paradigm to MABSC task and leverage the ability of large vision-language models to alleviate the limitation in the fusion of textual and image modalities. To tackle the problem of potential irrelevance between aspects and images, we propose a plug-and-play selector to autonomously choose the most appropriate instruction from the instruction pool, thereby reducing the impact of irrelevant image noise on the final sentiment classification results. We conduct extensive experiments in various scenarios and our model achieves state-of-the-art performance on benchmark datasets, as well as in few-shot settings.</abstract>
      <url hash="648da1eb">2024.lrec-main.180</url>
      <bibkey>feng-etal-2024-autonomous-aspect</bibkey>
    </paper>
    <paper id="181">
      <title>Auxiliary Knowledge-Induced Learning for Automatic Multi-Label Medical Document Classification</title>
      <author><first>Xindi</first><last>Wang</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>2006–2016</pages>
      <abstract>The International Classification of Diseases (ICD) is an authoritative medical classification system of different diseases and conditions for clinical and management purposes. ICD indexing aims to assign a subset of ICD codes to a medical record. Since human coding is labour-intensive and error-prone, many studies employ machine learning techniques to automate the coding process. ICD coding is a challenging task, as it needs to assign multiple codes to each medical document from an extremely large hierarchically organized collection. In this paper, we propose a novel approach for ICD indexing that adopts three ideas: (1) we use a multi-level deep dilated residual convolution encoder to aggregate the information from the clinical notes and learn document representations across different lengths of the texts; (2) we formalize the task of ICD classification with auxiliary knowledge of the medical records, which incorporates not only the clinical texts but also different clinical code terminologies and drug prescriptions for better inferring the ICD codes; and (3) we introduce a graph convolutional network to leverage the co-occurrence patterns among ICD codes, aiming to enhance the quality of label representations. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures.</abstract>
      <url hash="4231f680">2024.lrec-main.181</url>
      <bibkey>wang-etal-2024-auxiliary-knowledge</bibkey>
    </paper>
    <paper id="182">
      <title>A Virtual Patient Dialogue System Based on Question-Answering on Clinical Records</title>
      <author><first>Janire</first><last>Arana</last></author>
      <author><first>Mikel</first><last>Idoyaga</last></author>
      <author><first>Maitane</first><last>Urruela</last></author>
      <author><first>Elisa</first><last>Espina</last></author>
      <author><first>Aitziber</first><last>Atutxa Salazar</last></author>
      <author><first>Koldo</first><last>Gojenola</last></author>
      <pages>2017–2027</pages>
      <abstract>In this work we present two datasets for the development of virtual patients and the first evaluation results. We firstly introduce a Spanish corpus of medical dialogue questions annotated with intents, built upon prior research in French. We also propose a second dataset of dialogues using a novel annotation approach that involves doctor questions, patient answers, and corresponding clinical records, organized as triples of the form (clinical report, question, patient answer). This way, the doctor-patient conversation is modeled as a question-answering system that tries to find responses to questions taking a clinical record as input. This approach can help to eliminate the need for manually structured patient records, as commonly used in previous studies, thereby expanding the pool of diverse virtual patients available. Leveraging these annotated corpora, we develop and assess an automatic system designed to answer medical dialogue questions posed by medical students to simulated patients in medical exams. Our approach demonstrates robust generalization, relying solely on medical records to generate new patient cases. The two datasets and the code will be freely available for the research community.</abstract>
      <url hash="1f1c9049">2024.lrec-main.182</url>
      <bibkey>arana-etal-2024-virtual-patient</bibkey>
    </paper>
    <paper id="183">
      <title>A Web Portal about the State of the Art of <fixed-case>NLP</fixed-case> Tasks in <fixed-case>S</fixed-case>panish</title>
      <author><first>Enrique</first><last>Amigó</last></author>
      <author><first>Jorge</first><last>Carrillo-de-Albornoz</last></author>
      <author><first>Andrés</first><last>Fernández</last></author>
      <author><first>Julio</first><last>Gonzalo</last></author>
      <author><first>Guillermo</first><last>Marco</last></author>
      <author><first>Roser</first><last>Morante</last></author>
      <author><first>Laura</first><last>Plaza</last></author>
      <author><first>Jacobo</first><last>Pedrosa</last></author>
      <pages>2028–2038</pages>
      <abstract>This paper presents a new web portal with information about the state of the art of natural language processing tasks in Spanish. It provides information about forums, competitions, tasks and datasets in Spanish, that would otherwise be spread in multiple articles and web sites. The portal consists of overview pages where information can be searched for and filtered by several criteria and individual pages with detailed information and hyperlinks to facilitate navigation. Information has been manually curated from publications that describe competitions and NLP tasks from 2013 until 2023 and will be updated as new tasks appear. A total of 185 tasks and 128 datasets from 94 competitions have been introduced.</abstract>
      <url hash="ea5cc1fb">2024.lrec-main.183</url>
      <bibkey>amigo-etal-2024-web-portal</bibkey>
    </paper>
    <paper id="184">
      <title>A Workflow for <fixed-case>HTR</fixed-case>-Postprocessing, Labeling and Classifying Diachronic and Regional Variation in Pre-<fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>lavic Texts</title>
      <author><first>Piroska</first><last>Lendvai</last></author>
      <author><first>Maarten</first><last>van Gompel</last></author>
      <author><first>Anna</first><last>Jouravel</last></author>
      <author><first>Elena</first><last>Renje</last></author>
      <author><first>Uwe</first><last>Reichel</last></author>
      <author><first>Achim</first><last>Rabus</last></author>
      <author><first>Eckhart</first><last>Arnold</last></author>
      <pages>2039–2048</pages>
      <abstract>We describe ongoing work for developing a workflow for the applied use case of classifying diachronic and regional language variation in Pre-Modern Slavic texts. The data were obtained via handwritten text recognition (HTR) on medieval manuscripts and printings and partly by manual transcription. Our goal is to develop a workflow for such historical language data, covering HTR-postprocessing, annotating and classifying the digitized texts. We test and adapt existing language resources to fit the pipeline with low-barrier tooling, accessible for Humanists with limited experience in research data infrastructures, computational analysis or advanced methods of natural language processing (NLP). The workflow starts by addressing ground truth (GT) data creation for diagnosing and correcting HTR errors via string metrics and data-driven methods. On GT and on HTR data, we subsequently show classification results using transfer learning on sentence-level text snippets. Next, we report on our token-level data labeling efforts. Each step of the workflow is complemented with describing current limitations and our corresponding work in progress.</abstract>
      <url hash="71c6698b">2024.lrec-main.184</url>
      <bibkey>lendvai-etal-2024-workflow-htr</bibkey>
    </paper>
    <paper id="185">
      <title>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</title>
      <author><first>Yanis</first><last>Labrak</last></author>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <pages>2049–2066</pages>
      <abstract>The recent emergence of Large Language Models (LLMs) has enabled significant advances in the field of Natural Language Processing (NLP). While these new models have demonstrated superior performance on various tasks, their application and potential are still underexplored, both in terms of the diversity of tasks they can handle and their domain of application. In this context, we evaluate four state-of-the-art instruction-tuned LLMs (ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca) on a set of 13 real-world clinical and biomedical NLP tasks in English, including named-entity recognition (NER), question-answering (QA), relation extraction (RE), and more. Our overall results show that these evaluated LLMs approach the performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, particularly excelling in the QA task, even though they have never encountered examples from these tasks before. However, we also observe that the classification and RE tasks fall short of the performance achievable with specifically trained models designed for the medical field, such as PubMedBERT. Finally, we note that no single LLM outperforms all others across all studied tasks, with some models proving more suitable for certain tasks than others.</abstract>
      <url hash="59c13a1e">2024.lrec-main.185</url>
      <bibkey>labrak-etal-2024-zero-shot</bibkey>
    </paper>
    <paper id="186">
      <title>Backdoor <fixed-case>NLP</fixed-case> Models via <fixed-case>AI</fixed-case>-Generated Text</title>
      <author><first>Wei</first><last>Du</last></author>
      <author><first>Tianjie</first><last>Ju</last></author>
      <author><first>Ge</first><last>Ren</last></author>
      <author><first>GaoLei</first><last>Li</last></author>
      <author><first>Gongshen</first><last>Liu</last></author>
      <pages>2067–2079</pages>
      <abstract>Backdoor attacks pose a critical security threat to natural language processing (NLP) models by establishing covert associations between trigger patterns and target labels without affecting normal accuracy. Existing attacks usually disregard fluency and semantic fidelity of poisoned text, rendering the malicious data easily detectable. However, text generation models can produce coherent and content-relevant text given prompts. Moreover, potential differences between human-written and AI-generated text may be captured by NLP models while being imperceptible to humans. More insidious threats could arise if attackers leverage latent features of AI-generated text as trigger patterns. We comprehensively investigate backdoor attacks on NLP models using AI-generated poisoned text obtained via continued writing or paraphrasing, exploring three attack scenarios: data, model and pre-training. For data poisoning, we fine-tune generators with attribute control to enhance the attack performance. For model poisoning, we leverage downstream tasks to derive specialized generators. For pre-training poisoning, we train multiple attribute-based generators and align their generated text with pre-defined vectors, enabling task-agnostic migration attacks. Experiments demonstrate that our method achieves effective attacks while maintaining fluency and semantic similarity across all scenarios. We hope this work can raise awareness of the security risks hidden in AI-generated text.</abstract>
      <url hash="a3a3e8de">2024.lrec-main.186</url>
      <bibkey>du-etal-2024-backdoor-nlp</bibkey>
    </paper>
    <paper id="187">
      <title><fixed-case>B</fixed-case>alsu<fixed-case>T</fixed-case>alka.lv - Boosting the Common Voice Corpus for Low-Resource Languages</title>
      <author><first>Roberts</first><last>Dargis</last></author>
      <author><first>Arturs</first><last>Znotins</last></author>
      <author><first>Ilze</first><last>Auzina</last></author>
      <author><first>Baiba</first><last>Saulite</last></author>
      <author><first>Sanita</first><last>Reinsone</last></author>
      <author><first>Raivis</first><last>Dejus</last></author>
      <author><first>Antra</first><last>Klavinska</last></author>
      <author><first>Normunds</first><last>Gruzitis</last></author>
      <pages>2080–2085</pages>
      <abstract>Open speech corpora of substantial size are seldom available for less-spoken languages, and this was recently the case also for Latvian with its 1.5M native speakers. While there exist several closed Latvian speech corpora of 100+ hours, used to train competitive models for automatic speech recognition (ASR), there were only a few tiny open datasets available at the beginning of 2023, the 18-hour Latvian Common Voice 13.0 dataset being the largest one. In the result of a successful national crowdsourcing initiative, organised jointly by several institutions, the size and speaker diversity of the Latvian Common Voice 17.0 release have increased more than tenfold in less than a year. A successful follow-up initiative was also launched for Latgalian, which has been recognized as an endangered historic variant of Latvian with 150k speakers. The goal of these initiatives is not only to enlarge the datasets but also to make them more diverse in terms of speakers and accents, text genres and styles, intonations, grammar and lexicon. They have already become considerable language resources for both improving ASR and conducting linguistic research. Since we use the Mozilla Common Voice platform to record and validate speech samples, this paper focuses on (i) the selection of text snippets to enrich the language data and to stimulate various intonations, (ii) an indicative evaluation of the acquired corpus and the first ASR models fine-tuned on this data, (iii) our social campaigns to boost and maintain this initiative.</abstract>
      <url hash="865e846b">2024.lrec-main.187</url>
      <bibkey>dargis-etal-2024-balsutalka-lv</bibkey>
    </paper>
    <paper id="188">
      <title><fixed-case>BAMBOO</fixed-case>: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models</title>
      <author><first>Zican</first><last>Dong</last></author>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>2086–2099</pages>
      <abstract>Large language models (LLMs) have achieved dramatic proficiency over NLP tasks with normal length. Recently, multiple studies have committed to extending the context length and enhancing the long text modeling capabilities of LLMs. To comprehensively evaluate the long context ability of LLMs, we propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed with four principles: comprehensive capacity evaluation, avoidance of data contamination, accurate automatic evaluation, and different length levels. It consists of 10 datasets from 5 different long text understanding tasks, i.e., question answering, hallucination detection, text sorting, language modeling, and code completion, to cover various domains and core capacities of LLMs. We conduct experiments with five widely-used long-context models and further discuss five key questions for long text research. In the end, we discuss problems of current long-context models and point out future directions for enhancing long text modeling capacities. We release our data, prompts, and code at https://anonymous.4open.science/r/BAMBOO/.</abstract>
      <url hash="19022023">2024.lrec-main.188</url>
      <bibkey>dong-etal-2024-bamboo-comprehensive</bibkey>
    </paper>
    <paper id="189">
      <title><fixed-case>B</fixed-case>angla<fixed-case>A</fixed-case>uto<fixed-case>KG</fixed-case>: Automatic <fixed-case>B</fixed-case>angla Knowledge Graph Construction with Semantic Neural Graph Filtering</title>
      <author><first>Azmine Toushik</first><last>Wasi</last></author>
      <author><first>Taki Hasan</first><last>Rafi</last></author>
      <author><first>Raima</first><last>Islam</last></author>
      <author><first>Dong-Kyu</first><last>Chae</last></author>
      <pages>2100–2106</pages>
      <abstract>Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text. Data and code are available here: https://github.com/azminewasi/BanglaAutoKG</abstract>
      <url hash="b50c9c90">2024.lrec-main.189</url>
      <bibkey>wasi-etal-2024-banglaautokg-automatic</bibkey>
    </paper>
    <paper id="190">
      <title><fixed-case>BAN</fixed-case>-<fixed-case>PL</fixed-case>: A <fixed-case>P</fixed-case>olish Dataset of Banned Harmful and Offensive Content from Wykop.pl Web Service</title>
      <author><first>Anna</first><last>Kolos</last></author>
      <author><first>Inez</first><last>Okulska</last></author>
      <author><first>Kinga</first><last>Głąbińska</last></author>
      <author><first>Agnieszka</first><last>Karlinska</last></author>
      <author><first>Emilia</first><last>Wisnios</last></author>
      <author><first>Paweł</first><last>Ellerik</last></author>
      <author><first>Andrzej</first><last>Prałat</last></author>
      <pages>2107–2118</pages>
      <abstract>Since the Internet is flooded with hate, it is one of the main tasks for NLP experts to master automated online content moderation. However, advancements in this field require improved access to publicly available accurate and non-synthetic datasets of social media content. For the Polish language, such resources are very limited. In this paper, we address this gap by presenting a new open dataset of offensive social media content for the Polish language. The dataset comprises content from Wykop.pl, a popular online service often referred to as the Polish Reddit, reported by users and banned in the internal moderation process. It contains a total of 691,662 posts and comments, evenly divided into two categories: harmful and neutral (non-harmful). The anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000 for each class), along with preprocessing scripts have been made publicly available. Furthermore the paper offers valuable insights into real-life content moderation processes and delves into an analysis of linguistic features and content characteristics of the dataset. Moreover, a comprehensive anonymization procedure has been meticulously described and applied. The prevalent biases encountered in similar datasets, including post-moderation and pre-selection biases, are also discussed.</abstract>
      <url hash="83ba23ef">2024.lrec-main.190</url>
      <bibkey>kolos-etal-2024-ban-pl</bibkey>
    </paper>
    <paper id="191">
      <title>“Barking up the Right Tree”, a <fixed-case>GAN</fixed-case>-Based Pun Generation Model through Semantic Pruning</title>
      <author><first>JingJie</first><last>Zeng</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Jiahao</first><last>Kang</last></author>
      <author><first>Yufeng</first><last>Diao</last></author>
      <author><first>Zhihao</first><last>Yang</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>2119–2131</pages>
      <abstract>In the realm of artificial intelligence and linguistics, the automatic generation of humor, particularly puns, remains a complex task. This paper introduces an innovative approach that employs a Generative Adversarial Network (GAN) and semantic pruning techniques to generate humorous puns. We initiate our process by identifying potential pun candidates via semantic pruning. This is followed by the use of contrastive learning to decode the unique characteristics of puns, emphasizing both correct and incorrect interpretations. The learned features from contrastive learning are utilized within our GAN model to better capture the semantic nuances of puns. Specifically, the generator exploits the pruned semantic tree to generate pun texts, while the discriminator evaluates the generated puns, ensuring both linguistic correctness and humor. Evaluation results highlight our model’s capacity to produce semantically coherent and humorous puns, demonstrating an enhancement over prior methods and approach human-level performance. This work contributes significantly to the field of computational humor, advancing the capabilities of automatic pun generation.</abstract>
      <url hash="966cbed2">2024.lrec-main.191</url>
      <bibkey>zeng-etal-2024-barking-right</bibkey>
    </paper>
    <paper id="192">
      <title><fixed-case>B</fixed-case>asque and <fixed-case>S</fixed-case>panish Counter Narrative Generation: Data Creation and Evaluation</title>
      <author><first>Jaione</first><last>Bengoetxea</last></author>
      <author><first>Yi-Ling</first><last>Chung</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <pages>2132–2141</pages>
      <abstract>Counter Narratives (CNs) are non-negative textual responses to Hate Speech (HS) aiming at defusing online hatred and mitigating its spreading across media. Despite the recent increase in HS content posted online, research on automatic CN generation has been relatively scarce and predominantly focused on English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset for CN generation developed by means of Machine Translation (MT) and professional post-edition. Being a parallel corpus, also with respect to the original English CONAN, it allows to perform novel research on multilingual and crosslingual automatic generation of CNs. Our experiments on CN generation with mT5, a multilingual encoder-decoder model, shows that generation greatly benefits from training on post-edited data, as opposed to relying on silver MT data only. These results are confirmed by their correlation with a qualitative manual evaluation, demonstrating that manually revised training data remains crucial for the quality of the generated CNs. Furthermore, multilingual data augmentation improves results over monolingual settings for structurally similar languages such as English and Spanish, while being detrimental for Basque, a language isolate. Similar findings occur in zero-shot crosslingual evaluations, where model transfer (fine-tuning in English and generating in a different target language) outperforms fine-tuning mT5 on machine translated data for Spanish but not for Basque. This provides an interesting insight into the asymmetry in the multilinguality of generative models, a challenging topic which is still open to research. Data and code will be made publicly available upon publication.</abstract>
      <url hash="17c08f44">2024.lrec-main.192</url>
      <attachment type="OptionalSupplementaryMaterial" hash="57d31d7b">2024.lrec-main.192.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>bengoetxea-etal-2024-basque-spanish</bibkey>
    </paper>
    <paper id="193">
      <title>Becoming a High-Resource Language in Speech: The <fixed-case>C</fixed-case>atalan Case in the Common Voice Corpus</title>
      <author><first>Carme</first><last>Armentano-Oller</last></author>
      <author><first>Montserrat</first><last>Marimon</last></author>
      <author><first>Marta</first><last>Villegas</last></author>
      <pages>2142–2148</pages>
      <abstract>Collecting voice resources for speech recognition systems is a multifaceted challenge, involving legal, technical, and diversity considerations. However, it is crucial to ensure fair access to voice-driven technology across diverse linguistic backgrounds. We describe an ongoing effort to create an extensive, high-quality, publicly available voice dataset for future development of speech technologies in Catalan through the Mozilla Common Voice crowd-sourcing platform. We detail the specific approaches used to address the challenges faced in recruiting contributors and managing the collection, validation, and recording of sentences. This detailed overview can serve as a source of guidance for similar initiatives across other projects and linguistic contexts. The success of this project is evident in the latest corpus release, version 16.1, where Catalan ranks as the most prominent language in the corpus, both in terms of recorded hours and when considering validated hours. This establishes Catalan as a language with significant speech resources for language technology development and significantly raises its international visibility.</abstract>
      <url hash="62e9b27f">2024.lrec-main.193</url>
      <bibkey>armentano-oller-etal-2024-becoming-high</bibkey>
    </paper>
    <paper id="194">
      <title><fixed-case>BEIR</fixed-case>-<fixed-case>PL</fixed-case>: Zero Shot Information Retrieval Benchmark for the <fixed-case>P</fixed-case>olish Language</title>
      <author><first>Konrad</first><last>Wojtasik</last></author>
      <author><first>Kacper</first><last>Wołowiec</last></author>
      <author><first>Vadim</first><last>Shishkin</last></author>
      <author><first>Arkadiusz</first><last>Janz</last></author>
      <author><first>Maciej</first><last>Piasecki</last></author>
      <pages>2149–2160</pages>
      <abstract>The BEIR dataset is a large, heterogeneous benchmark for Information Retrieval (IR), garnering considerable attention within the research community. However, BEIR and analogous datasets are predominantly restricted to English language. Our objective is to establish extensive large-scale resources for IR in the Polish language, thereby advancing the research in this NLP area. In this work, inspired by mMARCO and Mr. TyDi datasets, we translated all accessible open IR datasets into Polish, and we introduced the BEIR-PL benchmark – a new benchmark which comprises 13 datasets, facilitating further development, training and evaluation of modern Polish language models for IR tasks. We executed an evaluation and comparison of numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore, we publish pre-trained open IR models for Polish language, marking a pioneering development in this field. The BEIR-PL is included in MTEB Benchmark and also available with trained models at URL <url>https://huggingface.co/clarin-knext</url>.</abstract>
      <url hash="ec2295ef">2024.lrec-main.194</url>
      <bibkey>wojtasik-etal-2024-beir-pl</bibkey>
    </paper>
    <paper id="195">
      <title>Benchmarking <fixed-case>GPT</fixed-case>-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies</title>
      <author><first>Flavio</first><last>Petruzzellis</last></author>
      <author><first>Alberto</first><last>Testolin</last></author>
      <author><first>Alessandro</first><last>Sperduti</last></author>
      <pages>2161–2177</pages>
      <abstract>Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating that state-of-the-art LLMs constitute a very strong baseline also in challenging tasks that require systematic generalization.</abstract>
      <url hash="6fa91f0f">2024.lrec-main.195</url>
      <bibkey>petruzzellis-etal-2024-benchmarking-gpt</bibkey>
    </paper>
    <paper id="196">
      <title>Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem</title>
      <author><first>YuHong</first><last>Sun</last></author>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Qipeng</first><last>Guo</last></author>
      <author><first>Jiawen</first><last>Wu</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Hui</first><last>Zhao</last></author>
      <pages>2178–2188</pages>
      <abstract>Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model’s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.</abstract>
      <url hash="d06b00e2">2024.lrec-main.196</url>
      <bibkey>sun-etal-2024-benchmarking-hallucination</bibkey>
    </paper>
    <paper id="197">
      <title>Benchmarking Large Language Models for <fixed-case>P</fixed-case>ersian: A Preliminary Study Focusing on <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>Amirhossein</first><last>Abaskohi</last></author>
      <author><first>Sara</first><last>Baruni</last></author>
      <author><first>Mostafa</first><last>Masoudi</last></author>
      <author><first>Nesa</first><last>Abbasi</last></author>
      <author><first>Mohammad Hadi</first><last>Babalou</last></author>
      <author><first>Ali</first><last>Edalat</last></author>
      <author><first>Sepehr</first><last>Kamahi</last></author>
      <author><first>Samin</first><last>Mahdizadeh Sani</last></author>
      <author><first>Nikoo</first><last>Naghavian</last></author>
      <author><first>Danial</first><last>Namazifard</last></author>
      <author><first>Pouya</first><last>Sadeghi</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <pages>2189–2203</pages>
      <abstract>This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities and a broad understanding of general knowledge, they often lag behind smaller pretrained models fine-tuned specifically for particular tasks. Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5. These results highlight the significant potential for enhancing LLM performance in the Persian language. This is particularly noteworthy due to the unique attributes of Persian, including its distinct alphabet and writing styles. We have made our codes, prompts, and data available here: https://github.com/Ipouyall/Benchmarking_ChatGPT_for_Persian.</abstract>
      <url hash="5fa3c907">2024.lrec-main.197</url>
      <bibkey>abaskohi-etal-2024-benchmarking-large</bibkey>
    </paper>
    <paper id="198">
      <title>Benchmarking the Performance of Machine Translation Evaluation Metrics with <fixed-case>C</fixed-case>hinese Multiword Expressions</title>
      <author><first>Huacheng</first><last>Song</last></author>
      <author><first>Hongzhi</first><last>Xu</last></author>
      <pages>2204–2216</pages>
      <abstract>To investigate the impact of Multiword Expressions (MWEs) on the fine-grained performance of the state-of-the-art metrics for Machine Translation Evaluation (MTE), we conduct experiments on the WMT22 Metrics Shared Task dataset with a preliminary focus on the Chinese-to-English language pair. We further annotate 28 types of Chinese MWEs on the source texts and then examine the performance of 31 MTE metrics on groups of sentences containing different MWEs. We have 3 interesting findings: 1) Machine Translation (MT) systems tend to perform worse on most Chinese MWE categories, confirming the previous claim that MWEs are a bottleneck of MT; 2) automatic metrics tend to overrate the translation of sentences containing MWEs; 3) most neural-network-based metrics perform better than string-overlap-based metrics. It concludes that both MT systems and MTE metrics still suffer from MWEs, suggesting richer annotation of data to facilitate MWE-aware automatic MTE and MT.</abstract>
      <url hash="772d5a39">2024.lrec-main.198</url>
      <bibkey>song-xu-2024-benchmarking-performance</bibkey>
    </paper>
    <paper id="199">
      <title>Benchmarking the Simplification of <fixed-case>D</fixed-case>utch Municipal Text</title>
      <author><first>Daniel</first><last>Vlantis</last></author>
      <author><first>Iva</first><last>Gornishka</last></author>
      <author><first>Shuai</first><last>Wang</last></author>
      <pages>2217–2226</pages>
      <abstract>Text simplification (TS) makes written information more accessible to all people, especially those with cognitive or language impairments. Despite much progress in TS due to advances in NLP technology, the bottleneck issue of lack of data for low-resource languages persists. Dutch is one of these languages that lack a monolingual simplification corpus. In this paper, we use English as a pivot language for the simplification of Dutch medical and municipal text. We experiment with augmenting training data and corpus choice for this pivot-based approach. We compare the results to a baseline and an end-to-end LLM approach using the GPT 3.5 Turbo model. Our evaluation shows that, while we can substantially improve the results of the pivot pipeline, the zero-shot end-to-end GPT-based simplification performs better on all metrics. Our work shows how an existing pivot-based pipeline can be improved for simplifying Dutch medical text. Moreover, we provide baselines for the comparison in the domain of Dutch municipal text and make our corresponding evaluation dataset publicly available.</abstract>
      <url hash="63163f17">2024.lrec-main.199</url>
      <bibkey>vlantis-etal-2024-benchmarking-simplification</bibkey>
    </paper>
    <paper id="200">
      <title><fixed-case>B</fixed-case>engali<fixed-case>LCP</fixed-case>: A Dataset for Lexical Complexity Prediction in the <fixed-case>B</fixed-case>engali Texts</title>
      <author><first>Nabila</first><last>Ayman</last></author>
      <author><first>Md. Akram</first><last>Hossain</last></author>
      <author><first>Abdul</first><last>Aziz</last></author>
      <author><first>Rokan Uddin</first><last>Faruqui</last></author>
      <author><first>Abu Nowshed</first><last>Chy</last></author>
      <pages>2227–2237</pages>
      <abstract>Encountering intricate or ambiguous terms within a sentence produces distress for the reader during comprehension. Lexical Complexity Prediction (LCP) deals with predicting the complexity score of a word or a phrase considering its context. This task poses several challenges including ambiguity, context sensitivity, and subjectivity in perceiving complexity. Despite having 300 million native speakers and ranking as the seventh most spoken language in the world, Bengali falls behind in the research on lexical complexity when compared to other languages. To bridge this gap, we introduce the first annotated Bengali dataset, that assists in performing the task of LCP in this language. Besides, we propose a transformer-based deep neural approach with a pairwise multi-head attention mechanism and LSTM model to predict the lexical complexity of Bengali tokens. The outcomes demonstrate that the proposed neural approach outperformed the existing state-of-the-art models for the Bengali language.</abstract>
      <url hash="789ee951">2024.lrec-main.200</url>
      <bibkey>ayman-etal-2024-bengalilcp-dataset</bibkey>
    </paper>
    <paper id="201">
      <title><fixed-case>B</fixed-case>en<fixed-case>LLM</fixed-case>-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on <fixed-case>B</fixed-case>engali <fixed-case>NLP</fixed-case></title>
      <author><first>Mohsinul</first><last>Kabir</last></author>
      <author><first>Mohammed Saidul</first><last>Islam</last></author>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last></author>
      <author><first>Mir Tafseer</first><last>Nayeem</last></author>
      <author><first>M Saiful</first><last>Bari</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <pages>2238–2252</pages>
      <abstract>Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). To this end, this paper introduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to benchmark their performance in the low-resourced Bangla language. In this regard, we select various important and diverse Bangla NLP tasks, such as text summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation of popular LLMs, namely, ChatGPT, LLaMA-2, and Claude-2. Our experimental results demonstrate that while in some Bangla NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2 being significantly bad) in comparison to the current SOTA results. Therefore, it calls for further efforts to develop a better understanding of LLMs in low-resource languages like Bangla.</abstract>
      <url hash="83bb571a">2024.lrec-main.201</url>
      <bibkey>kabir-etal-2024-benllm-eval</bibkey>
    </paper>
    <paper id="202">
      <title><fixed-case>BERT</fixed-case>-<fixed-case>BC</fixed-case>: A Unified Alignment and Interaction Model over Hierarchical <fixed-case>BERT</fixed-case> for Response Selection</title>
      <author><first>Zhenfei</first><last>Yang</last></author>
      <author><first>Beiming</first><last>Yu</last></author>
      <author><first>Yuan</first><last>Cui</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>2253–2263</pages>
      <abstract>Recently, we have witnessed a significant performance boosting for dialogue response selection task achieved by Cross-Encoder based models. However, such models directly feed the concatenation of context and response into the pre-trained model for interactive inference, ignoring the comprehensively independent representation modeling of context and response. Moreover, randomly sampling negative responses from other dialogue contexts is simplistic, and the learned models have poor generalization capability in realistic scenarios. In this paper, we propose a response selection model called BERT-BC that combines the representation-based Bi-Encoder and interaction-based Cross-Encoder. Three contrastive learning methods are devised for the Bi-Encoder to align context and response to obtain the better semantic representation. Meanwhile, according to the alignment difficulty of context and response semantics, the harder samples are dynamically selected from the same batch with negligible cost and sent to Cross-Encoder to enhance the model’s interactive reasoning ability. Experimental results show that BERT-BC can achieve state-of-the-art performance on three benchmark datasets for multi-turn response selection.</abstract>
      <url hash="d3721c8f">2024.lrec-main.202</url>
      <bibkey>yang-etal-2024-bert-bc</bibkey>
    </paper>
    <paper id="203">
      <title>Beyond Binary: Towards Embracing Complexities in Cyberbullying Detection and Intervention - a Position Paper</title>
      <author><first>Kanishk</first><last>Verma</last></author>
      <author><first>Kolawole John</first><last>Adebayo</last></author>
      <author><first>Joachim</first><last>Wagner</last></author>
      <author><first>Megan</first><last>Reynolds</last></author>
      <author><first>Rebecca</first><last>Umbach</last></author>
      <author><first>Tijana</first><last>Milosevic</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>2264–2284</pages>
      <abstract>In the digital age, cyberbullying (CB) poses a significant concern, impacting individuals as early as primary school and leading to severe or lasting consequences, including an increased risk of self-harm. CB incidents, are not limited to bullies and victims, but include bystanders with various roles, and usually have numerous sub-categories and variations of online harms. This position paper emphasises the complexity of CB incidents by drawing on insights from psychology, social sciences, and computational linguistics. While awareness of CB complexities is growing, existing computational techniques tend to oversimplify CB as a binary classification task, often relying on training datasets that capture peripheries of CB behaviours. Inconsistent definitions and categories of CB-related online harms across various platforms further complicates the issue. Ethical concerns arise when CB research involves children to role-play CB incidents to curate datasets. Through multi-disciplinary collaboration, we propose strategies for consideration when developing CB detection systems. We present our position on leveraging large language models (LLMs) such as Claude-2 and Llama2-Chat as an alternative approach to generate CB-related role-playing datasets. Our goal is to assist researchers, policymakers, and online platforms in making informed decisions regarding the automation of CB incident detection and intervention. By addressing these complexities, our research contributes to a more nuanced and effective approach to combating CB especially in young people.</abstract>
      <url hash="fd5ca3e0">2024.lrec-main.203</url>
      <bibkey>verma-etal-2024-beyond-binary</bibkey>
    </paper>
    <paper id="204">
      <title>Beyond Canonical Fine-tuning: Leveraging Hybrid Multi-Layer Pooled Representations of <fixed-case>BERT</fixed-case> for Automated Essay Scoring</title>
      <author><first>Eujene Nikka V.</first><last>Boquio</last></author>
      <author><first>Prospero C.</first><last>Naval, Jr.</last></author>
      <pages>2285–2295</pages>
      <abstract>The challenging yet relevant task of automated essay scoring (AES) continuously gains attention from multiple disciplines over the years. With the advent of pre-trained large language models such as BERT, fine-tuning those models has become the dominant technique in various natural language processing (NLP) tasks. Several studies fine-tune BERT for the AES task but only utilize the final pooled output from its last layer. With BERT’s multi-layer architecture that encodes hierarchical linguistic information, we believe we can improve overall essay scoring performance by leveraging information from its intermediate layers. In this study, we diverge from the canonical fine-tuning paradigm by exploring different combinations of model outputs and single- and multi-layer pooling strategies, as well as architecture modifications to the task-specific component of the model. Using a hybrid pooling strategy, experimental results show that our best essay representa- tion combined with a simple architectural modification outperforms the average QWK score of the basic fine-tuned BERT with default output on the ASAP AES dataset, suggesting its effectiveness for the AES task and potentially other long-text tasks.</abstract>
      <url hash="cd2ccce6">2024.lrec-main.204</url>
      <bibkey>boquio-naval-jr-2024-beyond-canonical</bibkey>
    </paper>
    <paper id="205">
      <title>Beyond Code: Evaluate Thought Steps for Complex Code Generation</title>
      <author><first>Liuwen</first><last>Cao</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <author><first>Jiexin</first><last>Wang</last></author>
      <author><first>Hongkui</first><last>He</last></author>
      <author><first>Hailin</first><last>Huang</last></author>
      <pages>2296–2306</pages>
      <abstract>Code generation aims to generate code in a general-purpose programming language, such as C++, based on natural language intents. Existing efforts primarily focus on relatively simple programming problems and fail to evaluate the thought process involved in complex programming scenarios. In this paper, we introduce “steps-guided code generation,” a task that assesses the quality of both thought steps and code implementation to evaluate the overall management of handling a complex programming problem. To support this task, we construct CodeStepsEval, a real-world scenario dataset of complex programming problems in the C++ programming language with varying levels of difficulty. Comprehensive experiments on this dataset demonstrate the importance of high-quality steps in enhancing code generation performance and the challenges faced by the code LLMs in this task.</abstract>
      <url hash="fb3ccac5">2024.lrec-main.205</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ca1deb3e">2024.lrec-main.205.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>cao-etal-2024-beyond-code</bibkey>
    </paper>
    <paper id="206">
      <title>Beyond Full Fine-tuning: Harnessing the Power of <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> for Multi-Task Instruction Tuning</title>
      <author><first>Chunlei</first><last>Xin</last></author>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Shuheng</first><last>Zhou</last></author>
      <author><first>Huijia</first><last>Zhu</last></author>
      <author><first>Weiqiang</first><last>Wang</last></author>
      <author><first>Zhongyi</first><last>Liu</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>2307–2317</pages>
      <abstract>Low-Rank Adaptation (LoRA) is a widespread parameter-efficient fine-tuning algorithm for large-scale language models. It has been commonly accepted that LoRA mostly achieves promising results in single-task, low-resource settings, and struggles to handle multi-task instruction tuning scenarios. In this paper, we conduct a systematic study of LoRA on diverse tasks and rich resources with different learning capacities, examining its performance on seen tasks during training and its cross-task generalization on unseen tasks. Our findings challenge the prevalent assumption that the limited learning capacity will inevitably result in performance decline. In fact, our study reveals that when configured with an appropriate rank, LoRA can achieve remarkable performance in high-resource and multi-task scenarios, even comparable to that achieved through full fine-tuning. It turns out that the constrained learning capacity encourages LoRA to prioritize conforming to instruction requirements rather than memorizing specialized features of particular tasks or instances. This study reveals the underlying connection between learning capacity and generalization capabilities for robust parameter-efficient fine-tuning, highlighting a promising direction for the broader application of LoRA across various tasks and settings.</abstract>
      <url hash="a3fa7ba1">2024.lrec-main.206</url>
      <bibkey>xin-etal-2024-beyond-full</bibkey>
    </paper>
    <paper id="207">
      <title>Beyond Linguistic Cues: Fine-grained Conversational Emotion Recognition via Belief-Desire Modelling</title>
      <author><first>Bo</first><last>Xu</last></author>
      <author><first>Longjiao</first><last>Li</last></author>
      <author><first>Wei</first><last>Luo</last></author>
      <author><first>Mehdi</first><last>Naseriparsa</last></author>
      <author><first>Zhehuan</first><last>Zhao</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <author><first>Feng</first><last>Xia</last></author>
      <pages>2318–2328</pages>
      <abstract>Emotion recognition in conversation (ERC) is essential for dialogue systems to identify the emotions expressed by speakers. Although previous studies have made significant progress, accurate recognition and interpretation of similar fine-grained emotion properly accounting for individual variability remains a challenge. One particular under-explored area is the role of individual beliefs and desires in modelling emotion. Inspired by the Belief-Desire Theory of Emotion, we propose a novel method for conversational emotion recognition that incorporates both belief and desire to accurately identify emotions. We extract emotion-eliciting events from utterances and construct graphs that represent beliefs and desires in conversations. By applying message passing between nodes, our graph effectively models the utterance context, speaker’s global state, and the interaction between emotional beliefs, desires, and utterances. We evaluate our model’s performance by conducting extensive experiments on four popular ERC datasets and comparing it with multiple state-of-the-art models. The experimental results demonstrate the superiority of our proposed model and validate the effectiveness of each module in the model.</abstract>
      <url hash="50c50e49">2024.lrec-main.207</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b419dfab">2024.lrec-main.207.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>xu-etal-2024-beyond-linguistic</bibkey>
    </paper>
    <paper id="208">
      <title>Beyond Model Performance: Can Link Prediction Enrich <fixed-case>F</fixed-case>rench Lexical Graphs?</title>
      <author><first>Hee-Soo</first><last>Choi</last></author>
      <author><first>Priyansh</first><last>Trivedi</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <author><first>Karen</first><last>Fort</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <pages>2329–2341</pages>
      <abstract>This paper presents a resource-centric study of link prediction approaches over French lexical-semantic graphs. Our study incorporates two graphs, RezoJDM16k and RL-fr, and we evaluated seven link prediction models, with CompGCN-ConvE emerging as the best performer. We also conducted a qualitative analysis of the predictions using manual annotations. Based on this, we found that predictions with higher confidence scores were more valid for inclusion. Our findings highlight different benefits for the dense graph compared to the sparser graph RL-fr. While the addition of new triples to RezoJDM16k offers limited advantages, RL-fr can benefit substantially from our approach.</abstract>
      <url hash="11959741">2024.lrec-main.208</url>
      <bibkey>choi-etal-2024-beyond-model</bibkey>
    </paper>
    <paper id="209">
      <title>Beyond Static Evaluation: A Dynamic Approach to Assessing <fixed-case>AI</fixed-case> Assistants’ <fixed-case>API</fixed-case> Invocation Capabilities</title>
      <author><first>Honglin</first><last>Mu</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Xiaofeng</first><last>Han</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Yutai</first><last>Hou</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>2342–2353</pages>
      <abstract>With the rise of Large Language Models (LLMs), AI assistants’ ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants’ API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant’s API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors overlooked by static evaluations, aligning more closely with human assessment. Testing four AI assistants using our crafted benchmark, our method further mirrored human evaluation compared to conventional static evaluations.</abstract>
      <url hash="1e96c38e">2024.lrec-main.209</url>
      <bibkey>mu-etal-2024-beyond-static</bibkey>
    </paper>
    <paper id="210">
      <title>Beyond the Known: Investigating <fixed-case>LLM</fixed-case>s Performance on Out-of-Domain Intent Detection</title>
      <author><first>Pei</first><last>Wang</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yejie</first><last>Wang</last></author>
      <author><first>Xiaoshuai</first><last>Song</last></author>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Yunsen</first><last>Xian</last></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>2354–2364</pages>
      <abstract>Out-of-domain (OOD) intent detection aims to examine whether the user’s query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD, and understanding long instructions.</abstract>
      <url hash="04341387">2024.lrec-main.210</url>
      <bibkey>wang-etal-2024-beyond-known</bibkey>
    </paper>
    <paper id="211">
      <title>Beyond Words: Decoding Facial Expression Dynamics in Motivational Interviewing</title>
      <author><first>Nezih</first><last>Younsi</last></author>
      <author><first>Catherine</first><last>Pelachaud</last></author>
      <author><first>Laurence</first><last>Chaby</last></author>
      <pages>2365–2374</pages>
      <abstract>Authors : Nezih Younsi, Catherine Pelachaud, Laurence Chaby Title : Beyond Words: Decoding Facial Expression Dynamics in Motivational Interviewing Abstract : This paper focuses on studying the facial expressions of both client and therapist in the context of Motivational Interviewing (MI). The annotation system Motivational Interview Skill Code MISC defines three types of talk, namely sustain, change, and neutral for the client and information, question, or reflection for the therapist. Most studies on MI look at the verbal modality. Our research aims to understand the variation and dynamics of facial expressions of both interlocutors over a counseling session. We apply a sequence mining algorithm to identify categories of facial expressions for each type. Using co-occurrence analysis, we derive the correlation between the facial expressions and the different types of talk, as well as the interplay between interlocutors’ expressions.</abstract>
      <url hash="9b050ed7">2024.lrec-main.211</url>
      <bibkey>younsi-etal-2024-beyond-words</bibkey>
    </paper>
    <paper id="212">
      <title><fixed-case>B</fixed-case>ig<fixed-case>NLI</fixed-case>: Native Language Identification with Big Bird Embeddings</title>
      <author><first>Sergey</first><last>Kramp</last></author>
      <author><first>Giovanni</first><last>Cassani</last></author>
      <author><first>Chris</first><last>Emmery</last></author>
      <pages>2375–2382</pages>
      <abstract>Native Language Identification (NLI) intends to classify an author’s native language based on their writing in another language. Historically, the task has heavily relied on time-consuming linguistic feature engineering, and NLI transformer models have thus far failed to offer effective, practical alternatives. The current work shows input size is a limiting factor, and that classifiers trained using Big Bird embeddings outperform linguistic feature engineering models (for which we reproduce previous work) by a large margin on the Reddit-L2 dataset. Additionally, we provide further insight into input length dependencies, show consistent out-of-sample (Europe subreddit) and out-of-domain (TOEFL-11) performance, and qualitatively analyze the embedding space. Given the effectiveness and computational efficiency of this method, we believe it offers a promising avenue for future NLI work.</abstract>
      <url hash="ec6bb971">2024.lrec-main.212</url>
      <bibkey>kramp-etal-2024-bignli-native</bibkey>
    </paper>
    <paper id="213">
      <title>Biomedical Concept Normalization over Nested Entities with Partial <fixed-case>UMLS</fixed-case> Terminology in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Natalia</first><last>Loukachevitch</last></author>
      <author><first>Andrey</first><last>Sakhovskiy</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>2383–2389</pages>
      <abstract>We present a new manually annotated dataset of PubMed abstracts for concept normalization in Russian. It contains over 23,641 entity mentions in 756 documents linked to 4,544 unique concepts from the UMLS ontology. Compared to existing corpora, we explore two novel annotation characteristics: the nestedness of named entities and the incompleteness of the Russian medical terminology in UMLS. 4,424 entity mentions are linked to 1,535 unique English concepts absent in the Russian part of the UMLS ontology. We present several baselines for normalization over nested named entities obtained with state-of-the-art models such as SapBERT. Our experimental results show that models pre-trained on graph structural data from UMLS achieve superior performance in a zero-shot setting on bilingual terminology.</abstract>
      <url hash="e0af9ec7">2024.lrec-main.213</url>
      <bibkey>loukachevitch-etal-2024-biomedical-concept</bibkey>
    </paper>
    <paper id="214">
      <title>Biomedical Entity Linking as Multiple Choice Question Answering</title>
      <author><first>Zhenxi</first><last>Lin</last></author>
      <author><first>Ziheng</first><last>Zhang</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>2390–2396</pages>
      <abstract>Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets.</abstract>
      <url hash="d74c6c75">2024.lrec-main.214</url>
      <bibkey>lin-etal-2024-biomedical-entity</bibkey>
    </paper>
    <paper id="215">
      <title>Bits and Pieces: Investigating the Effects of Subwords in Multi-task Parsing across Languages and Domains</title>
      <author><first>Daniel</first><last>Dakota</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>2397–2409</pages>
      <abstract>Neural parsing is very dependent on the underlying language model. However, very little is known about how choices in the language model affect parsing performance, especially in multi-task learning. We investigate questions on how the choice of subwords affects parsing, how subword sharing is responsible for gains or negative transfer in a multi-task setting where each task is parsing of a specific domain of the same language. More specifically, we investigate these issues across four languages: English, German, Italian, and Turkish. We find a general preference for averaged or last subwords across languages and domains. However, specific POS tags may require different subwords, and the distributional overlap between subwords across domains is perhaps a more influential factor in determining positive or negative transfer than discrepancies in the data sizes.</abstract>
      <url hash="21ee04af">2024.lrec-main.215</url>
      <bibkey>dakota-kubler-2024-bits-pieces</bibkey>
      <revision id="1" href="2024.lrec-main.215v1" hash="c528aed4"/>
      <revision id="2" href="2024.lrec-main.215v2" hash="21ee04af" date="2024-06-02">Table 5 error corrected (9.5 -&gt; 79.50).</revision>
    </paper>
    <paper id="216">
      <title><fixed-case>B</fixed-case>i<fixed-case>V</fixed-case>ert: Bidirectional Vocabulary Evaluation Using Relations for Machine Translation</title>
      <author><first>Carinne</first><last>Cherf</last></author>
      <author><first>Yuval</first><last>Pinter</last></author>
      <pages>2410–2420</pages>
      <abstract>Neural machine translation (NMT) has progressed rapidly in the past few years, promising improvements and quality translations for different languages. Evaluation of this task is crucial to determine the quality of the translation. Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods. We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text. This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet. Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level. Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various machine translation systems for English-German language pair. Finally, our method proposes a new multilingual approach to rank MT systems without the need for parallel corpora.</abstract>
      <url hash="873e643e">2024.lrec-main.216</url>
      <bibkey>cherf-pinter-2024-bivert-bidirectional</bibkey>
    </paper>
    <paper id="217">
      <title><fixed-case>BKEE</fixed-case>: Pioneering Event Extraction in the <fixed-case>V</fixed-case>ietnamese Language</title>
      <author><first>Thi-Nhung</first><last>Nguyen</last></author>
      <author><first>Bang Tien</first><last>Tran</last></author>
      <author><first>Trong-Nghia</first><last>Luu</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <author><first>Kiem-Hieu</first><last>Nguyen</last></author>
      <pages>2421–2427</pages>
      <abstract>Event Extraction (EE) is a fundamental task in information extraction, aimed at identifying events and their associated arguments within textual data. It holds significant importance in various applications and serves as a catalyst for the development of related tasks. Despite the availability of numerous datasets and methods for event extraction in various languages, there has been a notable absence of a dedicated dataset for the Vietnamese language. To address this limitation, we propose BKEE, a novel event extraction dataset for Vietnamese. BKEE encompasses over 33 distinct event types and 28 different event argument roles, providing a labeled dataset for entity mentions, event mentions, and event arguments on 1066 documents. Additionally, we establish robust baselines for potential downstream tasks on this dataset, facilitating the analysis of challenges and future development prospects in the field of Vietnamese event extraction.</abstract>
      <url hash="264e8373">2024.lrec-main.217</url>
      <bibkey>nguyen-etal-2024-bkee-pioneering</bibkey>
    </paper>
    <paper id="218">
      <title><fixed-case>B</fixed-case>lend<fixed-case>X</fixed-case>: Complex Multi-Intent Detection with Blended Patterns</title>
      <author><first>Yejin</first><last>Yoon</last></author>
      <author><first>Jungyeon</first><last>Lee</last></author>
      <author><first>Kangsan</first><last>Kim</last></author>
      <author><first>Chanhee</first><last>Park</last></author>
      <author><first>Taeuk</first><last>Kim</last></author>
      <pages>2428–2439</pages>
      <abstract>Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool—OpenAI’s ChatGPT—which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at <url>https://github.com/HYU-NLP/BlendX</url>.</abstract>
      <url hash="4c8857d0">2024.lrec-main.218</url>
      <bibkey>yoon-etal-2024-blendx-complex</bibkey>
    </paper>
    <paper id="219">
      <title><fixed-case>BLN</fixed-case>600: A Parallel Corpus of Machine/Human Transcribed Nineteenth Century Newspaper Texts</title>
      <author><first>Callum William</first><last>Booth</last></author>
      <author><first>Alan</first><last>Thomas</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <pages>2440–2446</pages>
      <abstract>We present a publicly available corpus of nineteenth-century newspaper text focused on crime in London, derived from the Gale British Library Newspapers corpus parts 1 and 2. The corpus comprises 600 newspaper excerpts and for each excerpt contains the original source image, the machine transcription of that image as found in the BLN and a gold standard manual transcription that we have created. We envisage the corpus will be helpful for the training and development of OCR and post-OCR correction methodologies for historical newspaper machine transcription—for which there is currently a dearth of publicly available resources. In this paper, we discuss the rationale behind gathering such a corpus, the methodology used to select, process, and align the data, and the corpus’ potential utility for historians and digital humanities researchers—particularly within the realms of neural machine translation-based post-OCR correction approaches, and other natural language processing tasks that are critically affected by erroneous OCR.</abstract>
      <url hash="01d18159">2024.lrec-main.219</url>
      <bibkey>booth-etal-2024-bln600-parallel</bibkey>
    </paper>
    <paper id="220">
      <title>Bootstrapping <fixed-case>UMR</fixed-case> Annotations for <fixed-case>A</fixed-case>rapaho from Language Documentation Resources</title>
      <author><first>Matthew J.</first><last>Buchholz</last></author>
      <author><first>Julia</first><last>Bonn</last></author>
      <author><first>Claire Benet</first><last>Post</last></author>
      <author><first>Andrew</first><last>Cowell</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <pages>2447–2457</pages>
      <abstract>Uniform Meaning Representation (UMR) is a semantic labeling system in the AMR family designed to be uniformly applicable to typologically diverse languages. The UMR labeling system is quite thorough and can be time-consuming to execute, especially if annotators are starting from scratch. In this paper, we focus on methods for bootstrapping UMR annotations for a given language from existing resources, and specifically from typical products of language documentation work, such as lexical databases and interlinear glossed text (IGT). Using Arapaho as our test case, we present and evaluate a bootstrapping process that automatically generates UMR subgraphs from IGT. Additionally, we describe and evaluate a method for bootstrapping valency lexicon entries from lexical databases for both the target language and English. We are able to generate enough basic structure in UMR graphs from the existing Arapaho interlinearized texts to automate UMR labeling to a significant extent. Our method thus has the potential to streamline the process of building meaning representations for new languages without existing large-scale computational resources.</abstract>
      <url hash="fdadc281">2024.lrec-main.220</url>
      <bibkey>buchholz-etal-2024-bootstrapping-umr</bibkey>
    </paper>
    <paper id="221">
      <title><fixed-case>B</fixed-case>oot<fixed-case>TOD</fixed-case>: Bootstrap Task-oriented Dialogue Representations by Aligning Diverse Responses</title>
      <author><first>Weihao</first><last>Zeng</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yejie</first><last>Wang</last></author>
      <author><first>Dayuan</first><last>Fu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>2458–2467</pages>
      <abstract>Pre-trained language models have been successful in many scenarios. However, their usefulness in task-oriented dialogues is limited due to the intrinsic linguistic differences between general text and task-oriented dialogues. Current task-oriented dialogue pre-training methods rely on a contrastive framework, which faces challenges such as selecting true positives and hard negatives, as well as lacking diversity. In this paper, we propose a novel dialogue pre-training model called BootTOD. It learns task-oriented dialogue representations via a self-bootstrapping framework. Unlike contrastive counterparts, BootTOD aligns context and context+response representations and dismisses the requirements of contrastive pairs. BootTOD also uses multiple appropriate response targets to model the intrinsic one-to-many diversity of human conversations. Experimental results show that BootTOD outperforms strong TOD baselines on diverse downstream dialogue tasks.</abstract>
      <url hash="6a45fe37">2024.lrec-main.221</url>
      <bibkey>zeng-etal-2024-boottod-bootstrap</bibkey>
    </paper>
    <paper id="222">
      <title>Born a <fixed-case>B</fixed-case>aby<fixed-case>N</fixed-case>et with Hierarchical Parental Supervision for End-to-End Text Image Machine Translation</title>
      <author><first>Cong</first><last>Ma</last></author>
      <author><first>Yaping</first><last>Zhang</last></author>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Yupu</first><last>Liang</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>2468–2479</pages>
      <abstract>Text image machine translation (TIMT) aims at translating source language texts in images into another target language, which has been proven successful by bridging text image recognition encoder and text translation decoder. However, it is still an open question of how to incorporate fine-grained knowledge supervision to make it consistent between recognition and translation modules. In this paper, we propose a novel TIMT method named as BabyNet, which is optimized with hierarchical parental supervision to improve translation performance. Inspired by genetic recombination and variation in the field of genetics, the proposed BabyNet is inherited from the recognition and translation parent models with a variation module of which parameters can be updated when training on the TIMT task. Meanwhile, hierarchical and multi-granularity supervision from parent models is introduced to bridge the gap between inherited modules in BabyNet. Extensive experiments on both synthetic and real-world TIMT tests show that our proposed method significantly outperforms existing methods. Further analyses of various parent model combinations show the good generalization of our method.</abstract>
      <url hash="865c50b0">2024.lrec-main.222</url>
      <bibkey>ma-etal-2024-born-babynet</bibkey>
    </paper>
    <paper id="223">
      <title><fixed-case>BP</fixed-case>4<fixed-case>ER</fixed-case>: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation</title>
      <author><first>Yuhong</first><last>He</last></author>
      <author><first>Yongqi</first><last>Zhang</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Jun</first><last>Wan</last></author>
      <pages>2480–2492</pages>
      <abstract>Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation. To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG’s multi-step reasoning process and iteratively enhance this reasoning process. We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for prompting, which autonomously correct errors and facilitate the LLM’s explicit reasoning. This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate reasoning chain. Experimental results on the two publicly datasets show that BP4ER outperforms state-of-the-art methods across both objective and subjective evaluation.</abstract>
      <url hash="db816f2f">2024.lrec-main.223</url>
      <bibkey>he-etal-2024-bp4er-bootstrap</bibkey>
    </paper>
    <paper id="224">
      <title>Breakthrough from Nuance and Inconsistency: Enhancing Multimodal Sarcasm Detection with Context-Aware Self-Attention Fusion and Word Weight Calculation.</title>
      <author><first>Hongfei</first><last>Xue</last></author>
      <author><first>Linyan</first><last>Xu</last></author>
      <author><first>Yu</first><last>Tong</last></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Jiali</first><last>Lin</last></author>
      <author><first>Dazhi</first><last>Jiang</last></author>
      <pages>2493–2503</pages>
      <abstract>Multimodal sarcasm detection has received considerable attention due to its unique role in social networks. Existing methods often rely on feature concatenation to fuse different modalities or model the inconsistencies among modalities. However, sarcasm is often embodied in local and momentary nuances in a subtle way, which causes difficulty for sarcasm detection. To effectively incorporate these nuances, this paper presents Context-Aware Self-Attention Fusion (CAAF) to integrate local and momentary multimodal information into specific words. Furthermore, due to the instantaneous nature of sarcasm, the connotative meanings of words post-multimodal integration generally deviate from their denotative meanings. Therefore, Word Weight Calculation (WWC) is presented to compute the weight of specific words based on CAAF’s fusion nuances, illustrating the inconsistency between connotation and denotation. We evaluate our method on the MUStARD dataset, achieving an accuracy of 76.9 and an F1 score of 76.1, which surpasses the current state-of-the-art IWAN model by 1.7 and 1.6 respectively.</abstract>
      <url hash="a173d1a7">2024.lrec-main.224</url>
      <bibkey>xue-etal-2024-breakthrough-nuance</bibkey>
    </paper>
    <paper id="225">
      <title>Bridging Computational Lexicography and Corpus Linguistics: A Query Extension for <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-<fixed-case>F</fixed-case>r<fixed-case>AC</fixed-case></title>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Maxim</first><last>Ionov</last></author>
      <author><first>Gilles</first><last>Sérasset</last></author>
      <pages>2504–2514</pages>
      <abstract>OntoLex, the dominant community standard for machine-readable lexical resources in the context of RDF, Linked Data and Semantic Web technologies, is currently extended with a designated module for Frequency, Attestations and Corpus-based Information (OntoLex-FrAC). We propose a novel component for OntoLex-FrAC, addressing the incorporation of corpus queries for (a) linking dictionaries with corpus engines, (b) enabling RDF-based web services to exchange corpus queries and responses data dynamically, and (c) using conventional query languages to formalize the internal structure of collocations, word sketches, and colligations. The primary field of application of the query extension is in digital lexicography and corpus linguistics, and we present a proof-of-principle implementation in backend components of a novel platform designed to support digital lexicography for the Serbian language.</abstract>
      <url hash="441d9c70">2024.lrec-main.225</url>
      <bibkey>chiarcos-etal-2024-bridging-computational</bibkey>
    </paper>
    <paper id="226">
      <title>Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model</title>
      <author><first>Shirin</first><last>Dabbaghi Varnosfaderani</last></author>
      <author><first>Canasai</first><last>Kruengkrai</last></author>
      <author><first>Ramin</first><last>Yahyapour</last></author>
      <author><first>Junichi</first><last>Yamagishi</last></author>
      <pages>2515–2519</pages>
      <abstract>FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence’s context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model’s modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that our approach exhibits competitive performance, aligning itself closely with top-tier models on the FEVEROUS benchmark.</abstract>
      <url hash="45522a82">2024.lrec-main.226</url>
      <bibkey>dabbaghi-varnosfaderani-etal-2024-bridging-textual</bibkey>
    </paper>
    <paper id="227">
      <title>Bridging the Code Gap: A Joint Learning Framework across Medical Coding Systems</title>
      <author><first>Geunyeong</first><last>Jeong</last></author>
      <author><first>Seokwon</first><last>Jeong</last></author>
      <author><first>Juoh</first><last>Sun</last></author>
      <author><first>Harksoo</first><last>Kim</last></author>
      <pages>2520–2525</pages>
      <abstract>Automated Medical Coding (AMC) is the task of automatically converting free-text medical documents into predefined codes according to a specific medical coding system. Although deep learning has significantly advanced AMC, the class imbalance problem remains a significant challenge. To address this issue, most existing methods consider only a single coding system and disregard the potential benefits of reflecting the relevance between different coding systems. To bridge this gap, we introduce a Joint learning framework for Across Medical coding Systems (JAMS), which jointly learns different coding systems through multi-task learning. It learns various representations using a shared encoder and explicitly captures the relationships across these coding systems using the medical code attention network, a modification of the graph attention network. In the experiments on the MIMIC-IV ICD-9 and MIMIC-IV ICD-10 datasets, connected through General Equivalence Mappings, JAMS improved the performance consistently regardless of the backbone models. This result demonstrates its model-agnostic characteristic, which is not constrained by specific model structures. Notably, JAMS significantly improved the performance of low-frequency codes. Our analysis shows that these performance gains are due to the connections between the codes of the different coding systems.</abstract>
      <url hash="0209dd08">2024.lrec-main.227</url>
      <bibkey>jeong-etal-2024-bridging-code</bibkey>
    </paper>
    <paper id="228">
      <title>Bring Invariant to Variant: A Contrastive Prompt-based Framework for Temporal Knowledge Graph Forecasting</title>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Xinying</first><last>Qian</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Baohang</first><last>Zhou</last></author>
      <author><first>Kehui</first><last>Song</last></author>
      <author><first>Xiaojie</first><last>Yuan</last></author>
      <pages>2526–2536</pages>
      <abstract>Temporal knowledge graph forecasting aims to reason over known facts to complete the missing links in the future. Existing methods are highly dependent on the structures of temporal knowledge graphs and commonly utilize recurrent or graph neural networks for forecasting. However, entities that are infrequently observed or have not been seen recently face challenges in learning effective knowledge representations due to insufficient structural contexts. To address the above disadvantages, in this paper, we propose a Contrastive Prompt-based framework with Entity background information for TKG forecasting, which we named CoPET. Specifically, to bring the time-invariant entity background information to time-variant structural information, we employ a dual encoder architecture consisting of a candidate encoder and a query encoder. A contrastive learning framework is used to encourage the query representation to be closer to the candidate representation. We further propose three kinds of trainable time-variant prompts aimed at capturing temporal structural information. Experiments on two datasets demonstrate that our method is effective and stays competitive in inference with limited structural information. Our code is available at https://github.com/qianxinying/CoPET.</abstract>
      <url hash="3d1bbc72">2024.lrec-main.228</url>
      <bibkey>zhang-etal-2024-bring-invariant</bibkey>
    </paper>
    <paper id="229">
      <title>Building a Broad Infrastructure for Uniform Meaning Representations</title>
      <author><first>Julia</first><last>Bonn</last></author>
      <author><first>Matthew J.</first><last>Buchholz</last></author>
      <author><first>Jayeol</first><last>Chun</last></author>
      <author><first>Andrew</first><last>Cowell</last></author>
      <author><first>William</first><last>Croft</last></author>
      <author><first>Lukas</first><last>Denk</last></author>
      <author><first>Sijia</first><last>Ge</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Kenneth</first><last>Lai</last></author>
      <author><first>James H.</first><last>Martin</last></author>
      <author><first>Skatje</first><last>Myers</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Claire Benet</first><last>Post</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Kristine</first><last>Stenzel</last></author>
      <author><first>Haibo</first><last>Sun</last></author>
      <author><first>Zdeňka</first><last>Urešová</last></author>
      <author><first>Rosa</first><last>Vallejos</last></author>
      <author><first>Jens E. L.</first><last>Van Gysel</last></author>
      <author><first>Meagan</first><last>Vigus</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Jin</first><last>Zhao</last></author>
      <pages>2537–2547</pages>
      <abstract>This paper reports the first release of the UMR (Uniform Meaning Representation) data set. UMR is a graph-based meaning representation formalism consisting of a sentence-level graph and a document-level graph. The sentence-level graph represents predicate-argument structures, named entities, word senses, aspectuality of events, as well as person and number information for entities. The document-level graph represents coreferential, temporal, and modal relations that go beyond sentence boundaries. UMR is designed to capture the commonalities and variations across languages and this is done through the use of a common set of abstract concepts, relations, and attributes as well as concrete concepts derived from words from invidual languages. This UMR release includes annotations for six languages (Arapaho, Chinese, English, Kukama, Navajo, Sanapana) that vary greatly in terms of their linguistic properties and resource availability. We also describe on-going efforts to enlarge this data set and extend it to other genres and modalities. We also briefly describe the available infrastructure (UMR annotation guidelines and tools) that others can use to create similar data sets.</abstract>
      <url hash="b3d1399d">2024.lrec-main.229</url>
      <bibkey>bonn-etal-2024-building-broad</bibkey>
    </paper>
    <paper id="230">
      <title>Building a Database of Conversational Routines</title>
      <author><first>Polina</first><last>Bychkova</last></author>
      <author><first>Alyaxey</first><last>Yaskevich</last></author>
      <author><first>Serafima</first><last>Gyulasaryan</last></author>
      <author><first>Ekaterina</first><last>Rakhilina</last></author>
      <pages>2548–2555</pages>
      <abstract>This paper discusses the Routinicon, a new constructicographic resource for the description of conversational routines. Conversational routines are defined as conventional formulaic expressions that language speakers use in standard extralinguistic situations (cf. Bless you! as a reaction to sneezing or Who’s there? as a typical answer to a knock on the door). The Routinicon’s goal is to accumulate the routines that constitute the inventory of conventional expressions in Russian language and systematically describe them in a way that would enable future cross-linguistic comparison and typological research. Conceptually, the Routinicon is a natural extension of such projects as the Russian Constructicon and Pragmaticon. It inherits their approach to the systematization of phraseological units as well as to the data collection. At the same time, the new project focuses on a fundamentally different domain of units and hence offers a radically new structure of linguistic annotation. Its principles and challenges are addressed in the paper.</abstract>
      <url hash="c765d434">2024.lrec-main.230</url>
      <bibkey>bychkova-etal-2024-building-database</bibkey>
    </paper>
    <paper id="231">
      <title>Building a Data Infrastructure for a Mid-Resource Language: The Case of <fixed-case>C</fixed-case>atalan</title>
      <author><first>Aitor</first><last>Gonzalez-Agirre</last></author>
      <author><first>Montserrat</first><last>Marimon</last></author>
      <author><first>Carlos</first><last>Rodriguez-Penagos</last></author>
      <author><first>Javier</first><last>Aula-Blasco</last></author>
      <author><first>Irene</first><last>Baucells</last></author>
      <author><first>Carme</first><last>Armentano-Oller</last></author>
      <author><first>Jorge</first><last>Palomar-Giner</last></author>
      <author><first>Baybars</first><last>Kulebi</last></author>
      <author><first>Marta</first><last>Villegas</last></author>
      <pages>2556–2566</pages>
      <abstract>Current LLM-based applications are becoming steadily available for everyone with a reliable access to technology and the internet. These applications offer benefits to their users that leave those without access to them at a serious disadvantage. Given the vastly large amount of data needed to train LLMs, the gap between languages with access to such quantity of data and those without it is currently larger than ever. Aimed at saving this gap, the Aina Project was created to provide Catalan with the necessary resources to keep being relevant in the context of AI/NLP applications based on LLMs. We thus present a set of strategies to consider when improving technology support for a mid- or low-resource language, specially addressing sustainability of high-quality data acquisition and the challenges involved in the process. We also introduce a large amount of new annotated data for Catalan. Our hope is that those interested in replicating this work for another language can learn from what worked for us, the challenges that we faced, and the sometimes disheartening truth of working with mid- and low-resource languages.</abstract>
      <url hash="5217867e">2024.lrec-main.231</url>
      <bibkey>gonzalez-agirre-etal-2024-building-data</bibkey>
    </paper>
    <paper id="232">
      <title>Building a <fixed-case>J</fixed-case>apanese Document-Level Relation Extraction Dataset Assisted by Cross-Lingual Transfer</title>
      <author><first>Youmi</first><last>Ma</last></author>
      <author><first>An</first><last>Wang</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>2567–2579</pages>
      <abstract>Document-level Relation Extraction (DocRE) is the task of extracting all semantic relationships from a document. While studies have been conducted on English DocRE, limited attention has been given to DocRE in non-English languages. This work delves into effectively utilizing existing English resources to promote DocRE studies in non-English languages, with Japanese as the representative case. As an initial attempt, we construct a dataset by transferring an English dataset to Japanese. However, models trained on such a dataset are observed to suffer from low recalls. We investigate the error cases and attribute the failure to different surface structures and semantics of documents translated from English and those written by native speakers. We thus switch to explore if the transferred dataset can assist human annotation on Japanese documents. In our proposal, annotators edit relation predictions from a model trained on the transferred dataset. Quantitative analysis shows that relation recommendations suggested by the model help reduce approximately 50% of the human edit steps compared with the previous approach. Experiments quantify the performance of existing DocRE models on our collected dataset, portraying the challenges of Japanese and cross-lingual DocRE.</abstract>
      <url hash="4a1aef35">2024.lrec-main.232</url>
      <bibkey>ma-etal-2024-building-japanese</bibkey>
    </paper>
    <paper id="233">
      <title>Building <fixed-case>MUSCLE</fixed-case>, a Dataset for <fixed-case>MU</fixed-case>ltilingual Semantic Classification of Links between Entities</title>
      <author><first>Lucia</first><last>Pitarch</last></author>
      <author><first>Carlos</first><last>Bobed Lisbona</last></author>
      <author><first>David</first><last>Abián</last></author>
      <author><first>Jorge</first><last>Gracia</last></author>
      <author><first>Jordi</first><last>Bernad</last></author>
      <pages>2580–2594</pages>
      <abstract>In this paper we introduce MUSCLE, a dataset for MUltilingual lexico-Semantic Classification of Links between Entities. The MUSCLE dataset was designed to train and evaluate Lexical Relation Classification (LRC) systems with 27K pairs of universal concepts selected from Wikidata, a large and highly multilingual factual Knowledge Graph (KG). Each pair of concepts includes its lexical forms in 25 languages and is labeled with up to five possible lexico-semantic relations between the concepts: hypernymy, hyponymy, meronymy, holonymy, and antonymy. Inspired by Semantic Map theory, the dataset bridges lexical and conceptual semantics, is more challenging and robust than previous datasets for LRC, avoids lexical memorization, is domain-balanced across entities, and enables enrichment and hierarchical information retrieval.</abstract>
      <url hash="d4ba15ca">2024.lrec-main.233</url>
      <bibkey>pitarch-etal-2024-building-muscle</bibkey>
    </paper>
    <paper id="234">
      <title>Building Question-Answer Data Using Web Register Identification</title>
      <author><first>Anni</first><last>Eskelinen</last></author>
      <author><first>Amanda</first><last>Myntti</last></author>
      <author><first>Erik</first><last>Henriksson</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <author><first>Veronika</first><last>Laippala</last></author>
      <pages>2595–2611</pages>
      <abstract>This article introduces a resource-efficient method for developing question-answer (QA) datasets by extracting QA pairs from web-scale data using machine learning (ML). Our method benefits from recent advances in web register (genre) identification and consists of two ML steps with an additional post-processing step. First, using XLM-R and the multilingual CORE web register corpus series with categories such as QA Forum, we train a multilingual classifier to retrieve documents that are likely to contain QA pairs from web-scale data. Second, we develop a NER-style token classifier to identify the QA text spans within these documents. To this end, we experiment with training on a semi-synthetic dataset built on top of the English LFQA, a small set of manually cleaned web QA pairs in English and Finnish, and a Finnish web QA pair dataset cleaned using ChatGPT. The evaluation of our pipeline demonstrates its capability to efficiently retrieve a substantial volume of QA pairs. While the approach is adaptable to any language given the availability of language models and extensive web data, we showcase its efficiency in English and Finnish, developing the first open, non-synthetic and non-machine translated QA dataset for Finnish – Turku WebQA – comprising over 200,000 QA pairs.</abstract>
      <url hash="be81192b">2024.lrec-main.234</url>
      <bibkey>eskelinen-etal-2024-building-question</bibkey>
    </paper>
    <paper id="235">
      <title><fixed-case>CAGK</fixed-case>: Collaborative Aspect Graph Enhanced Knowledge-based Recommendation</title>
      <author><first>Xiaotong</first><last>Song</last></author>
      <author><first>Huiping</first><last>Lin</last></author>
      <author><first>Jiatao</first><last>Zhu</last></author>
      <author><first>Xinyi</first><last>Gong</last></author>
      <pages>2612–2621</pages>
      <abstract>Auxiliary information, such as knowledge graph (KG), has become increasingly crucial in recommender systems. However, the current KG-based recommendation still has some limitations: (1) low link rates between items and KG entities, (2) redundant knowledge in KG. In this paper, we introduce the aspect, which refers to keywords describing item attributes in reviews, to KG-based recommendation, and propose a new model, Collaborative Aspect Graph enhanced Knowledge-based Network (CAGK). Firstly, CAGK builds a Collaborative Aspect Graph (CAG) with user-item interactions, aspects and KG, where aspects can fill most of the sparsity. Secondly, we leverage interactive information and aspect features to generate aspect-aware guidance signals to customize knowledge extraction and eliminate redundant knowledge. Lastly, we utilize low ratings and negative aspect sentiment to capture features of that users dislike to prevent repetitive recommendations of disliked items. Experimental results on two widely used benchmark datasets, Amazon-book and Yelp2018, confirm the superiority of CAGK.</abstract>
      <url hash="382df4e7">2024.lrec-main.235</url>
      <bibkey>song-etal-2024-cagk-collaborative</bibkey>
    </paper>
    <paper id="236">
      <title><fixed-case>CALAMR</fixed-case>: Component <fixed-case>AL</fixed-case>ignment for <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Paul</first><last>Landes</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <pages>2622–2637</pages>
      <abstract>We present Component ALignment for Abstract Meaning Representation (Calamr), a novel method for graph alignment that can support summarization and its evaluation. First, our method produces graphs that explain what is summarized through their alignments, which can be used to train graph based summarization learners. Second, although numerous scoring methods have been proposed for abstract meaning representation (AMR) that evaluate semantic similarity, no AMR based summarization metrics exist despite years of work using AMR for this task. Calamr provides alignments on which new scores can be based. The contributions of this work include a) a novel approach to aligning AMR graphs, b) a new summarization based scoring methods for similarity of AMR subgraphs composed of one or more sentences, and c) the entire reusable source code to reproduce our results.</abstract>
      <url hash="835d1779">2024.lrec-main.236</url>
      <bibkey>landes-di-eugenio-2024-calamr-component</bibkey>
    </paper>
    <paper id="237">
      <title>Calibrating <fixed-case>LLM</fixed-case>-Based Evaluator</title>
      <author><first>Yuxuan</first><last>Liu</last></author>
      <author><first>Tianchi</first><last>Yang</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Haizhen</first><last>Huang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Weiwei</first><last>Deng</last></author>
      <author><first>Feng</first><last>Sun</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>2638–2656</pages>
      <abstract>Recent advancements in large language models (LLMs) and their emergent capabilities make LLM a promising reference-free evaluator on the quality of natural language generation, and a competent alternative to human evaluation. However, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf LLM-based evaluator towards better human alignment. In this work, we propose AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an LLM-based evaluator toward human preference. Instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. Then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. To further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. Our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. Our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.</abstract>
      <url hash="3ede47fc">2024.lrec-main.237</url>
      <bibkey>liu-etal-2024-calibrating-llm</bibkey>
    </paper>
    <paper id="238">
      <title><fixed-case>CAM</fixed-case> 2.0: End-to-End Open Domain Comparative Question Answering System</title>
      <author><first>Ahmad</first><last>Shallouf</last></author>
      <author><first>Hanna</first><last>Herasimchyk</last></author>
      <author><first>Mikhail</first><last>Salnikov</last></author>
      <author><first>Rudy Alexandro</first><last>Garrido Veliz</last></author>
      <author><first>Natia</first><last>Mestvirishvili</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Irina</first><last>Nikishina</last></author>
      <pages>2657–2672</pages>
      <abstract>Comparative Question Answering (CompQA) is a Natural Language Processing task that combines Question Answering and Argument Mining approaches to answer subjective comparative questions in an efficient argumentative manner. In this paper, we present an end-to-end (full pipeline) system for answering comparative questions called CAM 2.0 as well as a public leaderboard called CompUGE that unifies the existing datasets under a single easy-to-use evaluation suite. As compared to previous web-form-based CompQA systems, it features question identification, object and aspect labeling, stance classification, and summarization using up-to-date models. We also select the most time- and memory-effective pipeline by comparing separately fine-tuned Transformer Encoder models which show state-of-the-art performance on the subtasks with Generative LLMs in few-shot and LoRA setups. We also conduct a user study for a whole-system evaluation.</abstract>
      <url hash="5a60e6c4">2024.lrec-main.238</url>
      <bibkey>shallouf-etal-2024-cam-2</bibkey>
    </paper>
    <paper id="239">
      <title><fixed-case>CAMAL</fixed-case>: A Novel Dataset for Multi-label Conversational Argument Move Analysis</title>
      <author><first>Viet Dac</first><last>Lai</last></author>
      <author><first>Duy Ngoc</first><last>Pham</last></author>
      <author><first>Jonathan</first><last>Steinberg</last></author>
      <author><first>Jamie</first><last>Mikeska</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>2673–2682</pages>
      <abstract>Understanding the discussion moves that teachers and students use to engage in classroom discussions is important to support pre-service teacher learning and teacher educators. This work introduces a novel conversational multi-label corpus of teaching transcripts collected from a simulated classroom environment for Conversational Argument Move AnaLysis (CAMAL). The dataset offers various argumentation moves used by pre-service teachers and students in mathematics and science classroom discussions. The dataset includes 165 transcripts from these discussions that pre-service elementary teachers facilitated in a simulated classroom environment of five student avatars. The discussion transcripts were annotated by education assessment experts for nine argumentation moves (aka. intents) used by the pre-service teachers and students during the discussions. In this paper, we describe the dataset, our annotation framework, and the models we employed to detect argumentation moves. Our experiments with state-of-the-art models demonstrate the complexity of the CAMAL task presented in the dataset. The result reveals that models that combined CNN and LSTM structures with speaker ID graphs improved the F1-score of our baseline models to detect speakers’ intents by a large margin. Given the complexity of the CAMAL task, it creates research opportunities for future studies. We share the dataset, the source code, and the annotation framework publicly at http://github.com/uonlp/camal-dataset.</abstract>
      <url hash="dd20fc0a">2024.lrec-main.239</url>
      <bibkey>lai-etal-2024-camal-novel</bibkey>
    </paper>
    <paper id="240">
      <title>Camel Morph <fixed-case>MSA</fixed-case>: A Large-Scale Open-Source Morphological Analyzer for <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic</title>
      <author><first>Christian</first><last>Khairallah</last></author>
      <author><first>Salam</first><last>Khalifa</last></author>
      <author><first>Reham</first><last>Marzouk</last></author>
      <author><first>Mayar</first><last>Nassar</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>2683–2691</pages>
      <abstract>We present Camel Morph MSA, the largest open-source Modern Standard Arabic morphological analyzer and generator. Camel Morph MSA has over 100K lemmas, and includes rarely modeled morphological features of Modern Standard Arabic with Classical Arabic origins. Camel Morph MSA can produce ∼1.45B analyses and ∼535M unique diacritizations, almost an order of magnitude larger than SAMA (Maamouri et al., 2010c), in addition to having ∼36% less OOV rate than SAMA on a 10B word corpus. Furthermore, Camel Morph MSA fills the gaps of many lemma paradigms by modeling linguistic phenomena consistently. Camel Morph MSA seamlessly integrates with the Camel Tools Python toolkit (Obeid et al., 2020), ensuring ease of use and accessibility.</abstract>
      <url hash="5f37b49d">2024.lrec-main.240</url>
      <bibkey>khairallah-etal-2024-camel-morph</bibkey>
    </paper>
    <paper id="241">
      <title><fixed-case>C</fixed-case>amem<fixed-case>BERT</fixed-case>-bio: Leveraging Continual Pre-training for Cost-Effective Models on <fixed-case>F</fixed-case>rench Biomedical Data</title>
      <author><first>Rian</first><last>Touchent</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <pages>2692–2701</pages>
      <abstract>Clinical data in hospitals are increasingly accessible for research through clinical data warehouses. However these documents are unstructured and it is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances for French, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. Addressing this gap, we introduce CamemBERT-bio, a dedicated French biomedical model derived from a new public French biomedical dataset. Through continual pre-training of the original CamemBERT, CamemBERT-bio achieves an improvement of 2.54 points of F1-score on average across various biomedical named entity recognition tasks, reinforcing the potential of continual pre-training as an equally proficient yet less computationally intensive alternative to training from scratch. Additionally, we highlight the importance of using a standard evaluation protocol that provides a clear view of the current state-of-the-art for French biomedical models.</abstract>
      <url hash="892f3524">2024.lrec-main.241</url>
      <bibkey>touchent-de-la-clergerie-2024-camembert-bio</bibkey>
    </paper>
    <paper id="242">
      <title><fixed-case>CAMERA</fixed-case>³: An Evaluation Dataset for Controllable Ad Text Generation in <fixed-case>J</fixed-case>apanese</title>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Akihiko</first><last>Kato</last></author>
      <author><first>Masato</first><last>Mita</last></author>
      <author><first>Ukyo</first><last>Honda</last></author>
      <author><first>Peinan</first><last>Zhang</last></author>
      <pages>2702–2707</pages>
      <abstract>Ad text generation is the task of creating compelling text from an advertising asset that describes products or services, such as a landing page. In advertising, diversity plays an important role in enhancing the effectiveness of an ad text, mitigating a phenomenon called “ad fatigue,” where users become disengaged due to repetitive exposure to the same advertisement. Despite numerous efforts in ad text generation, the aspect of diversifying ad texts has received limited attention, particularly in non-English languages like Japanese. To address this, we present CAMERA³, an evaluation dataset for controllable text generation in the advertising domain in Japanese. Our dataset includes 3,980 ad texts written by expert annotators, taking into account various aspects of ad appeals. We make CAMERA³ publicly available, allowing researchers to examine the capabilities of recent NLG models in controllable text generation in a real-world scenario.</abstract>
      <url hash="f61c5835">2024.lrec-main.242</url>
      <bibkey>inoue-etal-2024-camera3-evaluation</bibkey>
    </paper>
    <paper id="243">
      <title>Can Factual Statements Be Deceptive? The <fixed-case>D</fixed-case>e<fixed-case>F</fixed-case>a<fixed-case>B</fixed-case>el Corpus of Belief-based Deception</title>
      <author><first>Aswathy</first><last>Velutharambath</last></author>
      <author><first>Amelie</first><last>Wührl</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>2708–2723</pages>
      <abstract>If a person firmly believes in a non-factual statement, such as “The Earth is flat”, and argues in its favor, there is no inherent intention to deceive. As the argumentation stems from genuine belief, it may be unlikely to exhibit the linguistic properties associated with deception or lying. This interplay of factuality, personal belief, and intent to deceive remains an understudied area. Disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them. To study the relation between deception and factuality, based on belief, we present the DeFaBel corpus, a crowd-sourced resource of belief-based deception. To create this corpus, we devise a study in which participants are instructed to write arguments supporting statements like “eating watermelon seeds can cause indigestion”, regardless of its factual accuracy or their personal beliefs about the statement. In addition to the generation task, we ask them to disclose their belief about the statement. The collected instances are labelled as deceptive if the arguments are in contradiction to the participants’ personal beliefs. Each instance in the corpus is thus annotated (or implicitly labelled) with personal beliefs of the author, factuality of the statement, and the intended deceptiveness. The DeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive and 388 are non-deceptive. It is the first publicly available corpus for studying deception in German. In our analysis, we find that people are more confident in the persuasiveness of their arguments when the statement is aligned with their belief, but surprisingly less confident when they are generating arguments in favor of facts. The DeFaBel corpus can be obtained from https://www.ims.uni-stuttgart.de/data/defabel .</abstract>
      <url hash="60db2c65">2024.lrec-main.243</url>
      <attachment type="OptionalSupplementaryMaterial" hash="37c30a8e">2024.lrec-main.243.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>velutharambath-etal-2024-factual-statements</bibkey>
    </paper>
    <paper id="244">
      <title>Can <fixed-case>GPT</fixed-case>-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles</title>
      <author><first>Maram</first><last>Hasanain</last></author>
      <author><first>Fatema</first><last>Ahmad</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <pages>2724–2744</pages>
      <abstract>The use of propaganda has spiked on mainstream and social media, aiming to manipulate or mislead users. While efforts to automatically detect propaganda techniques in textual, visual, or multimodal content have increased, most of them primarily focus on English content. The majority of the recent initiatives targeting medium to low-resource languages produced relatively small annotated datasets, with a skewed distribution, posing challenges for the development of sophisticated propaganda detection models. To address this challenge, we carefully develop the largest propaganda dataset to date, ArPro, comprised of 8K paragraphs from newspaper articles, labeled at the text span level following a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the first attempt to understand the performance of large language models (LLMs), using GPT-4, for fine-grained propaganda detection from text. Results showed that GPT-4’s performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a dataset consisting of six other languages for span detection, and results suggest that the model struggles with the task across languages. We made the dataset publicly available for the community.</abstract>
      <url hash="67987f6f">2024.lrec-main.244</url>
      <bibkey>hasanain-etal-2024-gpt-4</bibkey>
    </paper>
    <paper id="245">
      <title>Can Humans Identify Domains?</title>
      <author><first>Maria</first><last>Barrett</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Amalie Brogaard</first><last>Pauli</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>2745–2765</pages>
      <abstract>Textual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in TGeGUM: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task. With a Fleiss’ kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them. By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models.</abstract>
      <url hash="550cb77b">2024.lrec-main.245</url>
      <bibkey>barrett-etal-2024-humans-identify</bibkey>
    </paper>
    <paper id="246">
      <title>Can Language Models Learn Embeddings of Propositional Logic Assertions?</title>
      <author><first>Nurul Fajrin</first><last>Ariyani</last></author>
      <author><first>Zied</first><last>Bouraoui</last></author>
      <author><first>Richard</first><last>Booth</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>2766–2776</pages>
      <abstract>Natural language offers an appealing alternative to formal logics as a vehicle for representing knowledge. However, using natural language means that standard methods for automated reasoning can no longer be used. A popular solution is to use transformer-based language models (LMs) to directly reason about knowledge expressed in natural language, but this has two important limitations. First, the set of premises is often too large to be directly processed by the LM. This means that we need a retrieval strategy which can select the most relevant premises when trying to infer some conclusion. Second, LMs have been found to learn shortcuts and thus lack robustness, putting in doubt to what extent they actually understand the knowledge that is expressed. Given these limitations, we explore the following alternative: rather than using LMs to perform reasoning directly, we use them to learn embeddings of individual assertions. Reasoning is then carried out by manipulating the learned embeddings. We show that this strategy is feasible to some extent, while at the same time also highlighting the limitations of directly fine-tuning LMs to learn the required embeddings.</abstract>
      <url hash="c41ee323">2024.lrec-main.246</url>
      <bibkey>ariyani-etal-2024-language-models</bibkey>
    </paper>
    <paper id="247">
      <title>Can Large Language Models Automatically Score Proficiency of Written Essays?</title>
      <author><first>Watheq Ahmad</first><last>Mansour</last></author>
      <author><first>Salam</first><last>Albatarni</last></author>
      <author><first>Sohaila</first><last>Eltanbouly</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>2777–2786</pages>
      <abstract>Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential on this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depends highly on the model and nature of the task. Second, the two LLMs exhibited comparable average performance in AES, with a slight advantage for ChatGPT. Finally, despite the performance gap between the two LLMs and SOTA models in terms of predictions, they provide feedback to enhance the quality of the essays, which can potentially help both teachers and students.</abstract>
      <url hash="7f72c4bd">2024.lrec-main.247</url>
      <bibkey>mansour-etal-2024-large-language</bibkey>
    </paper>
    <paper id="248">
      <title>Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences</title>
      <author><first>Sai</first><last>Koneru</last></author>
      <author><first>Jian</first><last>Wu</last></author>
      <author><first>Sarah</first><last>Rajtmajer</last></author>
      <pages>2787–2797</pages>
      <abstract>Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state of the art methods and highlight opportunities for future research in this area. Our dataset is shared with the research community: https://github.com/Sai90000/ScientificHypothesisEvidencing.git</abstract>
      <url hash="51ab39a6">2024.lrec-main.248</url>
      <bibkey>koneru-etal-2024-large-language</bibkey>
    </paper>
    <paper id="249">
      <title>Can Large Language Models Learn Translation Robustness from Noisy-Source In-context Demonstrations?</title>
      <author><first>Leiyu</first><last>Pan</last></author>
      <author><first>Yongqi</first><last>Leng</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>2798–2808</pages>
      <abstract>Large language models (LLMs) have been used for machine translation. When provided with prompts and source sentences, LLMs can achieve impressive translation results. However, the robustness of these LLMs remains a significant challenge, as they often struggle to accurately translate sentences in the presence of noise, even when using similarity-based in-context learning methods. This work proposes a research scheme for studying machine translation robustness on LLMs, investigating whether LLMs can learn translation robustness from noisy-source demonstration examples. Through experiments on different models, languages, and noise types, we empirically demonstrate that LLMs can learn how to handle noise and translation methods from noisy-source demonstration examples, thereby improving their translation performance on noisy sentences. Furthermore, we find that increasing the noise ratio appropriately for the noisy-source demonstration examples can enhance the translation robustness of LLMs. Additionally, we also attempt to investigate scenarios where LLMs are more likely to learn translation robustness for mixed and specific types of noise. We find that the model’s performance varies across different noise settings.</abstract>
      <url hash="4a5a54c9">2024.lrec-main.249</url>
      <bibkey>pan-etal-2024-large-language</bibkey>
    </paper>
    <paper id="250">
      <title>Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?</title>
      <author><first>Shaoxiong</first><last>Ji</last></author>
      <author><first>Timothee</first><last>Mickus</last></author>
      <author><first>Vincent</first><last>Segonne</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>2809–2818</pages>
      <abstract>Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language understanding tasks. We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability—which we argue is of use for machine translation but detrimental elsewhere.</abstract>
      <url hash="ef1d437c">2024.lrec-main.250</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9a091592">2024.lrec-main.250.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>ji-etal-2024-machine-translation</bibkey>
    </paper>
    <paper id="251">
      <title>Can Multiple-choice Questions Really Be Useful in Detecting the Abilities of <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Wangyue</first><last>Li</last></author>
      <author><first>Liangzhi</first><last>Li</last></author>
      <author><first>Tong</first><last>Xiang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Wei</first><last>Deng</last></author>
      <author><first>Noa</first><last>Garcia</last></author>
      <pages>2819–2834</pages>
      <abstract>Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM’s capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ’s efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs’ output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.</abstract>
      <url hash="b9fdc351">2024.lrec-main.251</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c3584ee0">2024.lrec-main.251.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2024-multiple-choice</bibkey>
    </paper>
    <paper id="252">
      <title>Can Small Language Models Help Large Language Models Reason Better?: <fixed-case>LM</fixed-case>-Guided Chain-of-Thought</title>
      <author><first>Jooyoung</first><last>Lee</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Thanh</first><last>Tran</last></author>
      <author><first>Qian</first><last>Hu</last></author>
      <author><first>Emre</first><last>Barut</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>2835–2843</pages>
      <abstract>We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., &lt;1B) language model (LM) for guiding a black-box large (i.e., &gt;10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance.</abstract>
      <url hash="25efbf77">2024.lrec-main.252</url>
      <bibkey>lee-etal-2024-small-language</bibkey>
    </paper>
    <paper id="253">
      <title>Can We Identify Stance without Target Arguments? A Study for Rumour Stance Classification</title>
      <author><first>Yue</first><last>Li</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <pages>2844–2851</pages>
      <abstract>Considering a conversation thread, rumour stance classification aims to identify the opinion (e.g. agree or disagree) of replies towards a target (rumour story). Although the target is expected to be an essential component in traditional stance classification, we show that rumour stance classification datasets contain a considerable amount of real-world data whose stance could be naturally inferred directly from the replies, contributing to the strong performance of the supervised models without awareness of the target. We find that current target-aware models underperform in cases where the context of the target is crucial. Finally, we propose a simple yet effective framework to enhance reasoning with the targets, achieving state-of-the-art performance on two benchmark datasets.</abstract>
      <url hash="b73356fa">2024.lrec-main.253</url>
      <bibkey>li-scarton-2024-identify-stance</bibkey>
    </paper>
    <paper id="254">
      <title>Can We Learn Question, Answer, and Distractors All from an Image? A New Task for Multiple-choice Visual Question Answering</title>
      <author><first>Wenjian</first><last>Ding</last></author>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>2852–2863</pages>
      <abstract>Multiple-choice visual question answering (MC VQA) requires an answer picked from a list of distractors, based on a question and an image. This research has attracted wide interest from the fields of visual question answering, visual question generation, and visual distractor generation. However, these fields still stay in their own territories, and how to jointly generate meaningful questions, correct answers, and challenging distractors remains unexplored. In this paper, we introduce a novel task, Visual Question-Answer-Distractors Generation (VQADG), which can bridge this research gap as well as take as a cornerstone to promote existing VQA models. Specific to the VQADG task, we present a novel framework consisting of a vision-and-language model to encode the given image and generate QADs jointly, and contrastive learning to ensure the consistency of the generated question, answer, and distractors. Empirical evaluations on the benchmark dataset validate the performance of our model in the VQADG task.</abstract>
      <url hash="f1fe83c6">2024.lrec-main.254</url>
      <bibkey>ding-etal-2024-learn-question</bibkey>
    </paper>
    <paper id="255">
      <title><fixed-case>CARE</fixed-case>: Co-Attention Network for Joint Entity and Relation Extraction</title>
      <author><first>Wenjun</first><last>Kong</last></author>
      <author><first>Yamei</first><last>Xia</last></author>
      <pages>2864–2870</pages>
      <abstract>Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. However, most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between the two subtasks. Addressing these challenges, in this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach includes adopting a parallel encoding strategy to learn separate representations for each subtask, aiming to avoid feature overlap or confusion. At the core of our approach is the co-attention module that captures two-way interaction between the two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Through extensive experiments on three benchmark datasets for joint entity and relation extraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model outperforms existing baseline models. Our code will be available at https://github.com/kwj0x7f/CARE.</abstract>
      <url hash="f427ffaf">2024.lrec-main.255</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7fbb7199">2024.lrec-main.255.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kong-xia-2024-care-co</bibkey>
    </paper>
    <paper id="256">
      <title><fixed-case>C</fixed-case>are<fixed-case>C</fixed-case>orpus: A Corpus of Real-World Solution-Focused Caregiver Strategies for Personalized Pediatric Rehabilitation Service Design</title>
      <author><first>Mina</first><last>Valizadeh</last></author>
      <author><first>Vera C.</first><last>Kaelin</last></author>
      <author><first>Mary A.</first><last>Khetani</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>2871–2882</pages>
      <abstract>In pediatric rehabilitation services, one intervention approach involves using solution-focused caregiver strategies to support children in their daily life activities. The manual sharing of these strategies is not scalable, warranting need for an automated approach to recognize and select relevant strategies. We introduce CareCorpus, a dataset of 780 real-world strategies written by caregivers. Strategies underwent dual-annotation by three trained annotators according to four established rehabilitation classes (i.e., environment/context, n=325 strategies; a child’s sense of self, n=151 strategies; a child’s preferences, n=104 strategies; and a child’s activity competences, n=62 strategies) and a no-strategy class (n=138 instances) for irrelevant or indeterminate instances. The average percent agreement was 80.18%, with a Cohen’s Kappa of 0.75 across all classes. To validate this dataset, we propose multi-grained classification tasks for detecting and categorizing strategies, and establish new performance benchmarks ranging from F1=0.53-0.79. Our results provide a first step towards a smart option to sort caregiver strategies for use in designing pediatric rehabilitation care plans. This novel, interdisciplinary resource and application is also anticipated to generalize to other pediatric rehabilitation service contexts that target children with developmental need.</abstract>
      <url hash="5c7c1ff7">2024.lrec-main.256</url>
      <bibkey>valizadeh-etal-2024-carecorpus-corpus</bibkey>
    </paper>
    <paper id="257">
      <title><fixed-case>CASIMIR</fixed-case>: A Corpus of Scientific Articles Enhanced with Multiple Author-Integrated Revisions</title>
      <author><first>Léane Isabelle</first><last>Jourdan</last></author>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <pages>2883–2892</pages>
      <abstract>Writing a scientific article is a challenging task as it is a highly codified and specific genre, consequently proficiency in written communication is essential for effectively conveying research findings and ideas. In this article, we propose an original textual resource on the revision step of the writing process of scientific articles. This new dataset, called CASIMIR, contains the multiple revised versions of 15,646 scientific articles from OpenReview, along with their peer reviews. Pairs of consecutive versions of an article are aligned at sentence-level while keeping paragraph location information as metadata for supporting future revision studies at the discourse level. Each pair of revised sentences is enriched with automatically extracted edits and associated revision intention. To assess the initial quality on the dataset, we conducted a qualitative study of several state-of-the-art text revision approaches and compared various evaluation metrics. Our experiments led us to question the relevance of the current evaluation methods for the text revision task.</abstract>
      <url hash="1c64c3e8">2024.lrec-main.257</url>
      <bibkey>jourdan-etal-2024-casimir-corpus</bibkey>
    </paper>
    <paper id="258">
      <title>Categorial Grammar Induction with Stochastic Category Selection</title>
      <author><first>Christian</first><last>Clark</last></author>
      <author><first>William</first><last>Schuler</last></author>
      <pages>2893–2900</pages>
      <abstract>Grammar induction, the task of learning a set of syntactic rules from minimally annotated training data, provides a means of exploring the longstanding question of whether humans rely on innate knowledge to acquire language. Of the various formalisms available for grammar induction, categorial grammars provide an appealing option due to their transparent interface between syntax and semantics. However, to obtain competitive results, previous categorial grammar inducers have relied on shortcuts such as part-of-speech annotations or an ad hoc bias term in the objective function to ensure desirable branching behavior. We present a categorial grammar inducer that eliminates both shortcuts: it learns from raw data, and does not rely on a biased objective function. This improvement is achieved through a novel stochastic process used to select the set of available syntactic categories. On a corpus of English child-directed speech, the model attains a recall-homogeneity of 0.48, a large improvement over previous categorial grammar inducers.</abstract>
      <url hash="826f88c3">2024.lrec-main.258</url>
      <bibkey>clark-schuler-2024-categorial-grammar</bibkey>
    </paper>
    <paper id="259">
      <title>Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: A Case Study on Hateful Memes</title>
      <author><first>Yosuke</first><last>Miyanishi</last></author>
      <author><first>Minh Le</first><last>Nguyen</last></author>
      <pages>2901–2916</pages>
      <abstract>Amidst the rapid expansion of Machine Learning (ML) and Large Language Models (LLMs), understanding the semantics within their mechanisms is vital. Causal analyses define semantics, while gradient-based methods are essential to eXplainable AI (XAI), interpreting the model’s ‘black box’. Integrating these, we investigate how a model’s mechanisms reveal its causal effect on evidence-based decision-making. Research indicates intersectionality - the combined impact of an individual’s demographics - can be framed as an Average Treatment Effect (ATE). This paper demonstrates that hateful meme detection can be viewed as an ATE estimation using intersectionality principles, and summarized gradient-based attention scores highlight distinct behaviors of three Transformer models. We further reveal that LLM Llama-2 can discern the intersectional aspects of the detection through in-context learning and that the learning process could be explained via meta-gradient, a secondary form of gradient. In conclusion, this work furthers the dialogue on Causality and XAI. Our code is available online (see External Resources section).</abstract>
      <url hash="337509f4">2024.lrec-main.259</url>
      <bibkey>miyanishi-nguyen-2024-causal-intersectionality</bibkey>
    </paper>
    <paper id="260">
      <title><fixed-case>CBBQ</fixed-case>: A <fixed-case>C</fixed-case>hinese Bias Benchmark Dataset Curated with Human-<fixed-case>AI</fixed-case> Collaboration for Large Language Models</title>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>2917–2929</pages>
      <abstract>Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification, ambiguous context generation, AI-assisted disambiguous context generation, and manual review and recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in evaluating model bias, with all 12 publicly available Chinese large language models exhibiting strong bias in certain categories. Additionally, we observe from our experiments that fine-tuned models could, to a certain extent, heed instructions and avoid generating harmful outputs, in the way of “moral self-correction”. Our dataset is available at https://anonymous.4open.science/r/CBBQ-B860/.</abstract>
      <url hash="3ff8b696">2024.lrec-main.260</url>
      <bibkey>huang-xiong-2024-cbbq-chinese</bibkey>
    </paper>
    <paper id="261">
      <title><fixed-case>CBT</fixed-case>-<fixed-case>LLM</fixed-case>: A <fixed-case>C</fixed-case>hinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering</title>
      <author><first>Hongbin</first><last>Na</last></author>
      <pages>2930–2940</pages>
      <abstract>The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&amp;A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM.</abstract>
      <url hash="f4eca391">2024.lrec-main.261</url>
      <bibkey>na-2024-cbt-llm</bibkey>
    </paper>
    <paper id="262">
      <title><fixed-case>CB</fixed-case>-Whisper: Contextual Biasing Whisper Using Open-Vocabulary Keyword-Spotting</title>
      <author><first>Yuang</first><last>Li</last></author>
      <author><first>Yinglu</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Jiawei</first><last>Yu</last></author>
      <author><first>Mengyao</first><last>Piao</last></author>
      <author><first>Xiaosong</first><last>Qiao</last></author>
      <author><first>Miaomiao</first><last>Ma</last></author>
      <author><first>Yanqing</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>2941–2946</pages>
      <abstract>End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations and terminologies that are not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI’s Whisper model that can recognize user-defined name entities by performing open-vocabulary keyword-spotting (KWS) before the decoder. The KWS module leverages text-to-speech (TTS) techniques and a convolutional neural network (CNN) classifier to match the features between the entities and the utterances. To integrate the recognized entities into the Whipser decoder and avoid hallucinations, we carefully crafted multiple prompts with spoken form hints. Experiments show that the KWS module based on Whisper encoder’s features can recognize unseen user-defined keywords effectively. More importantly, the proposed CB-Whisper substantially improves the mixed-error-rate (MER) and entity recall compared to the original Whisper model on three internal datasets and two publicly available datasets including Aishell and ACL datasets that cover English-only, Chinese-only, and code-switching scenarios.</abstract>
      <url hash="c33f1c76">2024.lrec-main.262</url>
      <bibkey>li-etal-2024-cb-whisper</bibkey>
    </paper>
    <paper id="263">
      <title><fixed-case>CEPT</fixed-case>: A Contrast-Enhanced Prompt-Tuning Framework for Emotion Recognition in Conversation</title>
      <author><first>Qingqing</first><last>Gao</last></author>
      <author><first>Jiuxin</first><last>Cao</last></author>
      <author><first>Biwei</first><last>Cao</last></author>
      <author><first>Xin</first><last>Guan</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <pages>2947–2957</pages>
      <abstract>Emotion Recognition in Conversation (ERC) has attracted increasing attention due to its wide applications in public opinion analysis, empathetic conversation generation, and so on. However, ERC research suffers from the problems of data imbalance and the presence of similar linguistic expressions for different emotions. These issues can result in limited learning for minority emotions, biased predictions for common emotions, and the misclassification of different emotions with similar linguistic expressions. To alleviate these problems, we propose a Contrast-Enhanced Prompt-Tuning (CEPT) framework for ERC. We transform the ERC task into a Masked Language Modeling (MLM) generation task and generate the emotion for each utterance in the conversation based on the prompt-tuning of the Pre-trained Language Model (PLM), where a novel mixed prompt template and a label mapping strategy are introduced for better context and emotion feature modeling. Moreover, Supervised Contrastive Learning (SCL) is employed to help the PLM mine more information from the labels and learn a more discriminative representation space for utterances with different emotions. We conduct extensive experiments and the results demonstrate that CEPT outperforms the state-of-the-art methods on all three benchmark datasets and excels in recognizing minority emotions.</abstract>
      <url hash="6f759861">2024.lrec-main.263</url>
      <bibkey>gao-etal-2024-cept-contrast</bibkey>
    </paper>
    <paper id="264">
      <title><fixed-case>CE</fixed-case>-<fixed-case>VDG</fixed-case>: Counterfactual Entropy-based Bias Reduction for Video-grounded Dialogue Generation</title>
      <author><first>Hongcheng</first><last>Liu</last></author>
      <author><first>Pingjie</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Zhu</last></author>
      <author><first>Yanfeng</first><last>Wang</last></author>
      <author><first>Yu</first><last>Wang</last></author>
      <pages>2958–2968</pages>
      <abstract>The Video-Grounded Dialogue generation (VDG) is a challenging task requiring a comprehensive understanding of the multi-modal information to produce a pertinent response. However, VDG models may rely on dataset bias as a shortcut and fail to learn the multi-modal knowledge from both video and audio. Counterfactual reasoning is an effective method that can estimate and eliminate bias on some special aspects of classification tasks. However, conventional counterfactual reasoning cannot be applied to VDG tasks directly due to the BPE algorithm. In this paper, we reformulate the counterfactual reasoning from the information entropy perspective and extend it from the classification task to the generative task, which can effectively reduce the question-related bias in the auto-regressive generation task. We design CE-VDG to demonstrate the effectiveness in bias elimination of the reformulated counterfactual reasoning by using the proposed counterfactual entropy as an external loss. Extensive experiment results on two popular VDG datasets show the superiority of CE-VDG over the existing baseline method, demonstrating the effective debiasing capability in our model considering counterfactual entropy.</abstract>
      <url hash="2d633a1b">2024.lrec-main.264</url>
      <bibkey>liu-etal-2024-ce-vdg</bibkey>
    </paper>
    <paper id="265">
      <title><fixed-case>C</fixed-case>hain<fixed-case>LM</fixed-case>: Empowering Large Language Models with Improved Chain-of-Thought Prompting</title>
      <author><first>Xiaoxue</first><last>Cheng</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>2969–2983</pages>
      <abstract>Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify—alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.</abstract>
      <url hash="455cce33">2024.lrec-main.265</url>
      <bibkey>cheng-etal-2024-chainlm-empowering</bibkey>
    </paper>
    <paper id="266">
      <title><fixed-case>C</fixed-case>hain<fixed-case>N</fixed-case>et: Structured Metaphor and Metonymy in <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Rowan Hall</first><last>Maudslay</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>2984–2996</pages>
      <abstract>The senses of a word exhibit rich internal structure. In a typical lexicon, this structure is overlooked: A word’s senses are encoded as a list, without inter-sense relations. We present ChainNet, a lexical resource which for the first time explicitly identifies these structures, by expressing how senses in the Open English Wordnet are derived from one another. In ChainNet, every nominal sense of a word is either connected to another sense by metaphor or metonymy, or is disconnected (in the case of homonymy). Because WordNet senses are linked to resources which capture information about their meaning, ChainNet represents the first dataset of grounded metaphor and metonymy.</abstract>
      <url hash="0306cc8c">2024.lrec-main.266</url>
      <bibkey>maudslay-etal-2024-chainnet-structured</bibkey>
    </paper>
    <paper id="267">
      <title>Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations</title>
      <author><first>Gregor</first><last>Donabauer</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <pages>2997–3004</pages>
      <abstract>Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of fake news were mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form graph-like structures that hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of Graph Neural Networks (GNNs) in the domain of context-based fake news detection. Our experiments provide an evaluation of different pre-training strategies for graph-based misinformation detection and demonstrate that transfer learning does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used for pre-training.</abstract>
      <url hash="6a86453e">2024.lrec-main.267</url>
      <bibkey>donabauer-kruschwitz-2024-challenges-pre</bibkey>
    </paper>
    <paper id="268">
      <title>Challenging Negative Gender Stereotypes: A Study on the Effectiveness of Automated Counter-Stereotypes</title>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Kathleen C.</first><last>Fraser</last></author>
      <author><first>Anna</first><last>Kerkhof</last></author>
      <author><first>Svetlana</first><last>Kiritchenko</last></author>
      <pages>3005–3015</pages>
      <abstract>Gender stereotypes are pervasive beliefs about individuals based on their gender that play a significant role in shaping societal attitudes, behaviours, and even opportunities. Recognizing the negative implications of gender stereotypes, particularly in online communications, this study investigates eleven strategies to automatically counteract and challenge these views. We present AI-generated gender-based counter-stereotypes to (self-identified) male and female study participants and ask them to assess their offensiveness, plausibility, and potential effectiveness. The strategies of counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) emerged as the most robust approaches, while humour, perspective-taking, counter-examples, and empathy for the speaker were perceived as less effective. Also, the differences in ratings were more pronounced for stereotypes about the different targets than between the genders of the raters. Alarmingly, many AI-generated counter-stereotypes were perceived as offensive and/or implausible. Our analysis and the collected dataset offer foundational insight into counter-stereotype generation, guiding future efforts to develop strategies that effectively challenge gender stereotypes in online interactions.</abstract>
      <url hash="8b9ecc1d">2024.lrec-main.268</url>
      <bibkey>nejadgholi-etal-2024-challenging-negative</bibkey>
    </paper>
    <paper id="269">
      <title>Characteristic <fixed-case>AI</fixed-case> Agents via Large Language Models</title>
      <author><first>Xi</first><last>Wang</last></author>
      <author><first>Hongliang</first><last>Dai</last></author>
      <author><first>Shen</first><last>Gao</last></author>
      <author><first>Piji</first><last>Li</last></author>
      <pages>3016–3027</pages>
      <abstract>The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called “Character100” is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents. The benchmark is available at https://github.com/nuaa-nlp/Character100.</abstract>
      <url hash="c2e99185">2024.lrec-main.269</url>
      <bibkey>wang-etal-2024-characteristic-ai</bibkey>
    </paper>
    <paper id="270">
      <title>Character-level Language Models for Abbreviation and Long-form Detection</title>
      <author><first>Leonardo</first><last>Zilio</last></author>
      <author><first>Shenbin</first><last>Qian</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <pages>3028–3037</pages>
      <abstract>Abbreviations and their associated long forms are important textual elements that are present in almost every scientific communication, and having information about these forms can help improve several NLP tasks. In this paper, our aim is to fine-tune language models for automatically identifying abbreviations and long forms. We used existing datasets which are annotated with abbreviations and long forms to train and test several language models, including transformer models, character-level language models, stacking of different embeddings, and ensemble methods. Our experiments showed that it was possible to achieve state-of-the-art results by stacking RoBERTa embeddings with domain-specific embeddings. However, the analysis of our first run showed that one of the datasets had issues in the BIO annotation, which led us to propose a revised dataset. After re-training selected models on the revised dataset, results show that character-level models achieve comparable results, especially when detecting abbreviations, but both RoBERTa large and the stacking of embeddings presented better results on biomedical data. When tested on a different subdomain (segments extracted from computer science texts), an ensemble method proved to yield the best results for the detection of long forms, and a character-level model had the best performance in detecting abbreviations.</abstract>
      <url hash="cceda49a">2024.lrec-main.270</url>
      <bibkey>zilio-etal-2024-character-level</bibkey>
    </paper>
    <paper id="271">
      <title><fixed-case>C</fixed-case>harles Translator: A Machine Translation System between <fixed-case>U</fixed-case>krainian and <fixed-case>C</fixed-case>zech</title>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Lucie</first><last>Polakova</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Jindřich</first><last>Helcl</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Pavel</first><last>Straňák</last></author>
      <author><first>Tomas</first><last>Krabac</last></author>
      <author><first>Jaroslava</first><last>Hlavacova</last></author>
      <author><first>Mariia</first><last>Anisimova</last></author>
      <author><first>Tereza</first><last>Chlanova</last></author>
      <pages>3038–3045</pages>
      <abstract>We present Charles Translator, a machine translation system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society. The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality. The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration. The system translates directly, in comparison to other available systems that use English as a pivot, and thus makes advantage of the typological similarity of the two languages. It uses the block back-translation method which allows for efficient use of monolingual training data. The paper describes the development process including data collection and implementation, evaluation, mentions several use cases and outlines possibilities for further development of the system for educational purposes.</abstract>
      <url hash="c39cf4b7">2024.lrec-main.271</url>
      <bibkey>popel-etal-2024-charles-translator</bibkey>
    </paper>
    <paper id="272">
      <title>Charting the Linguistic Landscape of Developing Writers: An Annotation Scheme for Enhancing Native Language Proficiency</title>
      <author><first>Miguel</first><last>Da Corte</last></author>
      <author><first>Jorge</first><last>Baptista</last></author>
      <pages>3046–3056</pages>
      <abstract>This study describes a pilot annotation task designed to capture orthographic, grammatical, lexical, semantic, and discursive patterns exhibited by college native English speakers participating in developmental education (DevEd) courses. The paper introduces an annotation scheme developed by two linguists aiming at pinpointing linguistic challenges that hinder effective written communication. The scheme builds upon patterns supported by the literature, which are known as predictors of student placement in DevEd courses and English proficiency levels. Other novel, multilayered, linguistic aspects that the literature has not yet explored are also presented. The scheme and its primary categories are succinctly presented and justified. Two trained annotators used this scheme to annotate a sample of 103 text units (3 during the training phase and 100 during the annotation task proper). Texts were randomly selected from a population of 290 community college intending students. An in-depth quality assurance inspection was conducted to assess tagging consistency between annotators and to discern (and address) annotation inaccuracies. Krippendorff’s Alpha (K-alpha) interrater reliability coefficients were calculated, revealing a K-alpha score of k=0.40, which corresponds to a moderate level of agreement, deemed adequate for the complexity and length of the annotation task.</abstract>
      <url hash="9cadf6cb">2024.lrec-main.272</url>
      <bibkey>da-corte-baptista-2024-charting-linguistic</bibkey>
    </paper>
    <paper id="273">
      <title><fixed-case>C</fixed-case>hart<fixed-case>T</fixed-case>hinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization</title>
      <author><first>Mengsha</first><last>Liu</last></author>
      <author><first>Daoyuan</first><last>Chen</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Guian</first><last>Fang</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <pages>3057–3074</pages>
      <abstract>Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datasets, our trained model consistently exhibits superior performance in chart summarization tasks, surpassing 8 state-of-the-art models over 7 evaluation metrics. Our dataset and codes are publicly accessible.</abstract>
      <url hash="5aa22afe">2024.lrec-main.273</url>
      <bibkey>liu-etal-2024-chartthinker-contextual</bibkey>
    </paper>
    <paper id="274">
      <title><fixed-case>C</fixed-case>hat<fixed-case>ASU</fixed-case>: Evoking <fixed-case>LLM</fixed-case>’s Reflexion to Truly Understand Aspect Sentiment in Dialogues</title>
      <author><first>Yiding</first><last>Liu</last></author>
      <author><first>Jingjing</first><last>Wang</last></author>
      <author><first>Jiamin</first><last>Luo</last></author>
      <author><first>Tao</first><last>Zeng</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>3075–3085</pages>
      <abstract>Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs’ ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU. Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU.</abstract>
      <url hash="6b4b48b3">2024.lrec-main.274</url>
      <bibkey>liu-etal-2024-chatasu-evoking</bibkey>
    </paper>
    <paper id="275">
      <title><fixed-case>C</fixed-case>hat<fixed-case>EL</fixed-case>: Entity Linking with Chatbots</title>
      <author><first>Yifan</first><last>Ding</last></author>
      <author><first>Qingkai</first><last>Zeng</last></author>
      <author><first>Tim</first><last>Weninger</last></author>
      <pages>3086–3097</pages>
      <abstract>Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these fine-tuned language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced solution to the problems inherent in EL models, but simply naive prompts to LLMs do not work well. In the present work, we define ChatEL, which is a three-step framework to prompt LLMs to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough error analysis shows many instances with the ground truth labels were actually incorrect, and the labels predicted by ChatEL were actually correct. This indicates that the quantitative results presented in this paper may be a conservative estimate of the actual performance. All data and code are available as an open-source package on GitHub at https://github.com/yifding/In_Context_EL.</abstract>
      <url hash="f2e2e629">2024.lrec-main.275</url>
      <bibkey>ding-etal-2024-chatel-entity</bibkey>
    </paper>
    <paper id="276">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models</title>
      <author><first>Ning</first><last>Bian</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Bin</first><last>Dong</last></author>
      <pages>3098–3110</pages>
      <abstract>Large language models (LLMs) have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point. In this paper, we specifically focus on ChatGPT, a widely used and easily accessible LLM, and ask the following questions: (1) Can ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question? (3) Is ChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage commonsense for answering questions? We conduct a series of experiments on 11 datasets to evaluate ChatGPT’s commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again. Experimental results show that: (1) ChatGPT can achieve good QA accuracies in commonsense tasks, while still struggling with certain domains of datasets. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question. These findings raise the need to explore improved mechanisms for effectively incorporating commonsense into LLMs like ChatGPT, such as better instruction following and commonsense guidance.</abstract>
      <url hash="8bbc92a5">2024.lrec-main.276</url>
      <bibkey>bian-etal-2024-chatgpt-knowledgeable</bibkey>
    </paper>
    <paper id="277">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Rates Natural Language Explanation Quality like Humans: But on Which Scales?</title>
      <author><first>Fan</first><last>Huang</last></author>
      <author><first>Haewoon</first><last>Kwak</last></author>
      <author><first>Kunwoo</first><last>Park</last></author>
      <author><first>Jisun</first><last>An</last></author>
      <pages>3111–3132</pages>
      <abstract>As AI becomes more integral in our lives, the need for transparency and responsibility grows. While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that ChatGPT aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment. This research advances our understanding of large language models’ capabilities to assess the text explanation quality in different configurations for responsible AI development.</abstract>
      <url hash="768cab15">2024.lrec-main.277</url>
      <bibkey>huang-etal-2024-chatgpt-rates</bibkey>
    </paper>
    <paper id="278">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Role-play Dataset: Analysis of User Motives and Model Naturalness</title>
      <author><first>Yufei</first><last>Tao</last></author>
      <author><first>Ameeta</first><last>Agrawal</last></author>
      <author><first>Judit</first><last>Dombi</last></author>
      <author><first>Tetyana</first><last>Sydorenko</last></author>
      <author><first>Jung In</first><last>Lee</last></author>
      <pages>3133–3145</pages>
      <abstract>Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.</abstract>
      <url hash="d8b085c0">2024.lrec-main.278</url>
      <bibkey>tao-etal-2024-chatgpt-role</bibkey>
    </paper>
    <paper id="279">
      <title><fixed-case>C</fixed-case>hat<fixed-case>UIE</fixed-case>: Exploring Chat-based Unified Information Extraction Using Large Language Models</title>
      <author><first>Jun</first><last>Xu</last></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Zhou</last></author>
      <pages>3146–3152</pages>
      <abstract>Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations. Extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. In this paper, we present ChatUIE, an innovative unified information extraction framework built upon ChatGLM. Simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE can significantly improve the performance of information extraction with a slight decrease in chatting ability.</abstract>
      <url hash="05456e74">2024.lrec-main.279</url>
      <bibkey>xu-etal-2024-chatuie-exploring</bibkey>
    </paper>
    <paper id="280">
      <title><fixed-case>CHICA</fixed-case>: A Developmental Corpus of Child-Caregiver’s Face-to-face vs. Video Call Conversations in Middle Childhood</title>
      <author><first>Dhia Elhak</first><last>Goumri</last></author>
      <author><first>Abhishek</first><last>Agrawal</last></author>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>Hong Duc Thang</first><last>Vu</last></author>
      <author><first>Kübra</first><last>Bodur</last></author>
      <author><first>Elias</first><last>Emmar</last></author>
      <author><first>Cassandre</first><last>Armand</last></author>
      <author><first>Chiara</first><last>Mazzocconi</last></author>
      <author><first>Shreejata</first><last>Gupta</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Leonor</first><last>Becerra-Bonache</last></author>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <pages>3153–3164</pages>
      <abstract>Existing studies of naturally occurring language-in-interaction have largely focused on the two ends of the developmental spectrum, i.e., early childhood and adulthood, leaving a gap in our knowledge about how development unfolds, especially across middle childhood. The current work contributes to filling this gap by introducing CHICA (for Child Interpersonal Communication Analysis), a developmental corpus of child-caregiver conversations <i>at home</i>, involving groups of French-speaking children aged 7, 9, and 11 years old. Each dyad was recorded twice: once in a face-to-face setting and once using computer-mediated video calls. For the face-to-face settings, we capitalized on recent advances in mobile, lightweight eye-tracking and head motion detection technology to optimize the naturalness of the recordings, allowing us to obtain both precise and ecologically valid data. Further, we mitigated the challenges of manual annotation by relying – to the extent possible – on automatic tools in speech processing and computer vision. Finally, to demonstrate the richness of this corpus for the study of child communicative development, we provide preliminary analyses comparing several measures of child-caregiver conversational dynamics across developmental age, modality, and communicative medium. We hope the current corpus will allow new discoveries into the properties and mechanisms of multimodal communicative development across middle childhood.</abstract>
      <url hash="ccf40ec0">2024.lrec-main.280</url>
      <bibkey>goumri-etal-2024-chica-developmental</bibkey>
    </paper>
    <paper id="281">
      <title><fixed-case>C</fixed-case>hinese Morpheme-informed Evaluation of Large Language Models</title>
      <author><first>Yaqi</first><last>Yin</last></author>
      <author><first>Yue</first><last>Wang</last></author>
      <author id="yang-liu-pk"><first>Yang</first><last>Liu</last></author>
      <pages>3165–3178</pages>
      <abstract>Previous evaluations of large language models (LLMs) focused on the perspective of various tasks or abilities. In this paper, we propose to evaluate from a linguistic viewpoint and argue that morpheme, a potential linguistic feature that captures both word-formation and lexical semantics, is another suitable component for evaluation that remains largely unexplored. In light of this, we construct MorphEval, a morpheme-informed benchmark, including three datasets following the bottom-up levels of characters, words, and sentences in Chinese, and then evaluate representative LLMs with both zero- and few-shot settings under two metrics. From this perspective, we reveal three aspects of issues LLMs nowadays encounter: dysfunctions in morphology and syntax, challenges with the long-tailed distribution of semantics, and difficulties from cultural implications. In these scenarios, even a smaller Chinese-targeted model may outperform ChatGPT, highlighting the actual challenges LLMs face and the necessity of language-specific improvements when applied to non-English languages. This new approach could also help guide model enhancements as well as get extended to other languages.</abstract>
      <url hash="b36292ea">2024.lrec-main.281</url>
      <bibkey>yin-etal-2024-chinese-morpheme</bibkey>
    </paper>
    <paper id="282">
      <title><fixed-case>C</fixed-case>hinese Sequence Labeling with Semi-Supervised Boundary-Aware Language Model Pre-training</title>
      <author><first>Longhui</first><last>Zhang</last></author>
      <author><first>Dingkun</first><last>Long</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Yanzhao</first><last>Zhang</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3179–3191</pages>
      <abstract>Chinese sequence labeling tasks are sensitive to word boundaries. Although pretrained language models (PLM) have achieved considerable success in these tasks, current PLMs rarely consider boundary information explicitly. An exception to this is BABERT, which incorporates unsupervised statistical boundary information into Chinese BERT’s pre-training objectives. Building upon this approach, we input supervised high-quality boundary information to enhance BABERT’s learning, developing a semi-supervised boundary-aware PLM. To assess PLMs’ ability to encode boundaries, we introduce a novel “Boundary Information Metric” that is both simple and effective. This metric allows comparison of different PLMs without task-specific fine-tuning. Experimental results on Chinese sequence labeling datasets demonstrate that the improved BABERT version outperforms the vanilla version, not only in these tasks but also in broader Chinese natural language understanding tasks. Additionally, our proposed metric offers a convenient and accurate means of evaluating PLMs’ boundary awareness.</abstract>
      <url hash="f6472164">2024.lrec-main.282</url>
      <bibkey>zhang-etal-2024-chinese-sequence</bibkey>
    </paper>
    <paper id="283">
      <title><fixed-case>CH</fixed-case>is<fixed-case>IEC</fixed-case>: An Information Extraction Corpus for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese History</title>
      <author><first>Xuemei</first><last>Tang</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Zekun</first><last>Deng</last></author>
      <pages>3192–3202</pages>
      <abstract>Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the “Chinese Historical Information Extraction Corpus”(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 14,194 entities and 8,609 relations. To establish the robustness and versatility of our dataset, we have undertaken comprehensive experimentation involving models of various sizes and paradigms. Additionally, we have evaluated the capabilities of Large Language Models (LLMs) in the context of tasks related to ancient Chinese history. The dataset and code are available at <url>https://github.com/tangxuemei1995/CHisIEC</url>.</abstract>
      <url hash="f019a611">2024.lrec-main.283</url>
      <bibkey>tang-etal-2024-chisiec-information</bibkey>
    </paper>
    <paper id="284">
      <title>Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues</title>
      <author><first>Armand</first><last>Stricker</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>3203–3214</pages>
      <abstract>During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis demonstrates that our enhanced dataset poses a challenge for these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user’s backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of generating novel chitchat-TOD scenarios to test TOD systems more thoroughly and improve their resilience to natural user interferences.</abstract>
      <url hash="a9dae725">2024.lrec-main.284</url>
      <bibkey>stricker-paroubek-2024-chitchat-interference</bibkey>
    </paper>
    <paper id="285">
      <title>Choice-75: A Dataset on Decision Branching in Script Learning</title>
      <author><first>Zhaoyi</first><last>Hou</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>3215–3223</pages>
      <abstract>Script learning studies how daily events unfold. It enables machines to reason about narratives with implicit information. Previous works mainly consider a script as a linear sequence of events while ignoring the potential branches that arise due to people’s circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to make decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. We also present preliminary results with current large language models (LLM). Although they demonstrate overall decent performances, there is still notable headroom in hard scenarios.</abstract>
      <url hash="8cbd04a8">2024.lrec-main.285</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2cb7a278">2024.lrec-main.285.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>hou-etal-2024-choice-75</bibkey>
    </paper>
    <paper id="286">
      <title><fixed-case>C</fixed-case>-Journal: A Journaling Application for Detecting and Classifying Cognitive Distortions Using Deep-Learning Based on a Crowd-sourced Dataset</title>
      <author><first>Nada</first><last>Elsharawi</last></author>
      <author><first>Alia</first><last>El Bolock</last></author>
      <pages>3224–3234</pages>
      <abstract>Cognitive distortions are negatively biased thinking patterns and erroneous self-statements resulting from and leading to logical errors in one’s own internal reasoning. Cognitive distortions have an adverse effect on mental health and can lead to mental health disorders in extreme cases. This paper belongs to a bigger project which aims to provide an application for detecting and classifying cognitive distortions in texts. As no public data sets were available for the task, the first contribution of the proposed work lies in providing an open-source labeled dataset of 14 cognitive distortions consisting of 34370 entries collected via crowd-sourcing, user questionnaires, and re-purposing emotions dataset from social media. The dataset is collected in cooperation with a licensed psychologist. We implemented a baseline model using Naïve Bayes and Count Vectorizer and different CNN, LSTM, and DNN classifiers to classify cognitive distortions based on the dataset. We investigated the usage of different word embeddings with the best-performing models. The best-performing model relied on a CNN with pre-trained Sentence-BERT embedding with an F1-score of 84 % for classifying cognitive distortions. The best-performing model was built into C-Journal, a free journaling and mood-tracking mobile application that pinpoints potential thinking distortions to the users.</abstract>
      <url hash="985fd785">2024.lrec-main.286</url>
      <bibkey>elsharawi-el-bolock-2024-c-journal</bibkey>
    </paper>
    <paper id="287">
      <title>Claim-Centric and Sentiment Guided Graph Attention Network for Rumour Detection</title>
      <author><first>Sajad</first><last>Ramezani</last></author>
      <author><first>Mauzama</first><last>Firdaus</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <pages>3235–3241</pages>
      <abstract>Automatic rumour detection has gained attention due to the influence of social media on individuals and its pervasiveness. In this work, we construct a representation that takes into account the claim in the source tweet, considering both the propagation graph and the accompanying text alongside tweet sentiment. This is achieved through the implementation of a hierarchical attention mechanism, which not only captures the embedding of documents from individual word vectors but also combines these document representations as nodes within the propagation graph. Furthermore, to address potential overfitting concerns, we employ generative models to augment the existing datasets. This involves rephrasing the claims initially made in the source tweet, thereby creating a more diverse and robust dataset. In addition, we augment the dataset with sentiment labels to improve the performance of the rumour detection task. This holistic and refined approach yields a significant enhancement in the performance of our model across three distinct datasets designed for rumour detection. Quantitative and qualitative analysis proves the effectiveness of our methodology, surpassing the achievements of prior methodologies.</abstract>
      <url hash="f24e2673">2024.lrec-main.287</url>
      <bibkey>ramezani-etal-2024-claim-centric</bibkey>
    </paper>
    <paper id="288">
      <title>Clarifying Completions: Evaluating How <fixed-case>LLM</fixed-case>s Respond to Incomplete Questions</title>
      <author><first>Angus</first><last>Addlesee</last></author>
      <author><first>Oliver</first><last>Lemon</last></author>
      <author><first>Arash</first><last>Eshghi</last></author>
      <pages>3242–3249</pages>
      <abstract>People understand and produce language incrementally on a word by word basis. This gives rise to many characteristic conversational phenomena including long mid-sentence pauses that are followed by incremental clarification requests (iCRs) intended to recover the rest of the truncated turn (see Fig. 1; (A), (B), (C)). The ability to generate iCRs is important in natural conversational AI systems, and crucial to their accessibility to users with memory impairment. In this paper, we collect, release and analyse SLUICE-CR: a large corpus of 3000 human produced iCRs. We then use this corpus to probe the incremental processing capability of a number of state of the art LLMs by evaluating the quality of the model’s generated iCRs in response to incomplete questions. Our evaluations show that the ability to generate contextually appropriate iCRs only emerges at larger LLM sizes, and only when prompted with example iCRs from our corpus. They also indicate that autoregressive LMs are, in principle, able to both understand and generate language incrementally.</abstract>
      <url hash="bcd0a37e">2024.lrec-main.288</url>
      <bibkey>addlesee-etal-2024-clarifying-completions</bibkey>
    </paper>
    <paper id="289">
      <title>Classifying Social Media Users before and after Depression Diagnosis via Their Language Usage: A Dataset and Study</title>
      <author><first>Falwah</first><last>Alhamed</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>3250–3260</pages>
      <abstract>Mental illness can significantly impact individuals’ quality of life. Analysing social media data to uncover potential mental health issues in individuals via their posts is a popular research direction. However, most studies focus on the classification of users suffering from depression versus healthy users, or on the detection of suicidal thoughts. In this paper, we instead aim to understand and model linguistic changes that occur when users transition from a healthy to an unhealthy state. Addressing this gap could lead to better approaches for earlier depression detection when signs are not as obvious as in cases of severe depression or suicidal ideation. In order to achieve this goal, we have collected the first dataset of textual posts by the same users before and after reportedly being diagnosed with depression. We then use this data to build multiple predictive models (based on SVM, Random Forests, BERT, RoBERTa, MentalBERT, GPT-3, GPT-3.5, Bard, and Alpaca) for the task of classifying user posts. Transformer-based models achieved the best performance, while large language models used off-the-shelf proved less effective as they produced random guesses (GPT and Bard) or hallucinations (Alpaca).</abstract>
      <url hash="cdad3963">2024.lrec-main.289</url>
      <bibkey>alhamed-etal-2024-classifying-social</bibkey>
    </paper>
    <paper id="290">
      <title>Class-Incremental Few-Shot Event Detection</title>
      <author><first>Kailin</first><last>Zhao</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>3261–3270</pages>
      <abstract>Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, there are two problems (i.e., old knowledge forgetting and new class overfitting) in this task. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to reduce the forgetting issue about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the state-of-the-art performance of Prompt-KD.</abstract>
      <url hash="35860938">2024.lrec-main.290</url>
      <bibkey>zhao-etal-2024-class-incremental</bibkey>
    </paper>
    <paper id="291">
      <title><fixed-case>CLASSLA</fixed-case>-web: Comparable Web Corpora of <fixed-case>S</fixed-case>outh <fixed-case>S</fixed-case>lavic Languages Enriched with Linguistic and Genre Annotation</title>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Taja</first><last>Kuzman</last></author>
      <pages>3271–3282</pages>
      <abstract>This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community. A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles. Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts.</abstract>
      <url hash="c93174ee">2024.lrec-main.291</url>
      <bibkey>ljubesic-kuzman-2024-classla-web</bibkey>
    </paper>
    <paper id="292">
      <title><fixed-case>CLAUSE</fixed-case>-<fixed-case>ATLAS</fixed-case>: A Corpus of Narrative Information to Scale up Computational Literary Analysis</title>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Piek T.J.M.</first><last>Vossen</last></author>
      <pages>3283–3296</pages>
      <abstract>We introduce CLAUSE-ATLAS, a resource of XIX and XX century English novels annotated automatically. This corpus, which contains 41,715 labeled clauses, allows to study stories as sequences of eventive, subjective and contextual information. We use it to investigate if recent large language models, in particular gpt-3.5-turbo with 16k tokens of context, constitute promising tools to annotate large amounts of data for literary studies (we show that this is the case). Moreover, by analyzing the annotations so collected, we find that our clause-based approach to literature captures structural patterns within books, as well as qualitative differences between them.</abstract>
      <url hash="1b984fcf">2024.lrec-main.292</url>
      <bibkey>troiano-vossen-2024-clause-atlas</bibkey>
    </paper>
    <paper id="293">
      <title><fixed-case>CLEVR</fixed-case>-<fixed-case>POC</fixed-case>: Reasoning-Intensive Visual Question Answering in Partially Observable Environments</title>
      <author><first>Savitha</first><last>Sam Abraham</last></author>
      <author><first>Marjan</first><last>Alirezaie</last></author>
      <author><first>Luc</first><last>de Raedt</last></author>
      <pages>3297–3313</pages>
      <abstract>The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to using existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged in order to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments we observe that the performance of pre-trained vision language models like CLIP (approx. 22%) and a large language model (LLM) like GPT-4 (approx. 46%) on CLEVR-POC are not satisfactory, ascertaining the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC.</abstract>
      <url hash="c3ed5eae">2024.lrec-main.293</url>
      <bibkey>sam-abraham-etal-2024-clevr-poc</bibkey>
    </paper>
    <paper id="294">
      <title><fixed-case>CLFFRD</fixed-case>: Curriculum Learning and Fine-grained Fusion for Multimodal Rumor Detection</title>
      <author><first>Fan</first><last>Xu</last></author>
      <author><first>Lei</first><last>Zeng</last></author>
      <author><first>Bowei</first><last>Zou</last></author>
      <author><first>Ai Ti</first><last>Aw</last></author>
      <author><first>Huan</first><last>Rong</last></author>
      <pages>3314–3324</pages>
      <abstract>In an era where rumors can propagate rapidly across social media platforms such as Twitter and Weibo, automatic rumor detection has garnered considerable attention from both academia and industry. Existing multimodal rumor detection models often overlook the intricacies of sample difficulty, e.g., text-level difficulty, image-level difficulty, and multimodal-level difficulty, as well as their order when training. Inspired by the concept of curriculum learning, we propose the Curriculum Learning and Fine-grained Fusion-driven multimodal Rumor Detection (CLFFRD) framework, which employs curriculum learning to automatically select and train samples according to their difficulty at different training stages. Furthermore, we introduce a fine-grained fusion strategy that unifies entities from text and objects from images, enhancing their semantic cohesion. We also propose a novel data augmentation method that utilizes linear interpolation between textual and visual modalities to generate diverse data. Additionally, our approach incorporates deep fusion for both intra-modality (e.g., text entities and image objects) and inter-modality (e.g., CLIP and social graph) features. Extensive experimental results demonstrate that CLFFRD outperforms state-of-the-art models on both English and Chinese benchmark datasets for rumor detection in social media.</abstract>
      <url hash="390a57d0">2024.lrec-main.294</url>
      <bibkey>xu-etal-2024-clffrd-curriculum</bibkey>
    </paper>
    <paper id="295">
      <title><fixed-case>CLHA</fixed-case>: A Simple Yet Effective Contrastive Learning Framework for Human Alignment</title>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Liang</first><last>Zhu</last></author>
      <author><first>Xi</first><last>Feng</last></author>
      <author><first>Jinchang</first><last>Hou</last></author>
      <author><first>Qixuan</first><last>Zhao</last></author>
      <author><first>Chengming</first><last>Li</last></author>
      <author><first>Xiping</first><last>Hu</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <pages>3325–3334</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, automatic evaluations, and human assessments on the widely used “Helpful and Harmless” dataset.</abstract>
      <url hash="b55d6ef7">2024.lrec-main.295</url>
      <bibkey>fang-etal-2024-clha-simple</bibkey>
    </paper>
    <paper id="296">
      <title><fixed-case>CLI</fixed-case>c<fixed-case>K</fixed-case>: A Benchmark Dataset of Cultural and Linguistic Intelligence in <fixed-case>K</fixed-case>orean</title>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Juyoung</first><last>Suk</last></author>
      <author><first>Philhoon</first><last>Oh</last></author>
      <author><first>Haneul</first><last>Yoo</last></author>
      <author><first>James</first><last>Thorne</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>3335–3346</pages>
      <abstract>Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in click, we provide fine-grained annotation of which cultural and linguistic knowledge is required to correctly answer the question. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs’ proficiency in Korean language and culture.</abstract>
      <url hash="b48fcc9b">2024.lrec-main.296</url>
      <bibkey>kim-etal-2024-click-benchmark</bibkey>
    </paper>
    <paper id="297">
      <title>Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles</title>
      <author><first>Andrea</first><last>Zugarini</last></author>
      <author><first>Kamyar</first><last>Zeinalipour</last></author>
      <author><first>Surya Sai</first><last>Kadali</last></author>
      <author><first>Marco</first><last>Maggini</last></author>
      <author><first>Marco</first><last>Gori</last></author>
      <author><first>Leonardo</first><last>Rigutini</last></author>
      <pages>3347–3356</pages>
      <abstract>Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing. In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct Large Language Models (LLMs). By gathering from Wikipedia pages informative content associated with relevant keywords, we use Large Language Models to automatically generate pedagogical clues related to the given input keyword and its context. With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues. We used clue-instruct to instruct different LLMs to generate educational clues from a given input content and keyword. Both human and automatic evaluations confirmed the quality of the generated clues, thus validating the effectiveness of our approach.</abstract>
      <url hash="44796210">2024.lrec-main.297</url>
      <bibkey>zugarini-etal-2024-clue-instruct</bibkey>
    </paper>
    <paper id="298">
      <title><fixed-case>CMDAG</fixed-case>: A <fixed-case>C</fixed-case>hinese Metaphor Dataset with Annotated Grounds as <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> for Boosting Metaphor Generation</title>
      <author><first>Yujie</first><last>Shao</last></author>
      <author><first>Xinrong</first><last>Yao</last></author>
      <author><first>Xingwei</first><last>Qu</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Shi</first><last>Wang</last></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Fu</last></author>
      <pages>3357–3366</pages>
      <abstract>Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes tenors and their distinct features rather than the conventional combination of tenors and vehicles. By integrating “ground” as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition. We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus. These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research.</abstract>
      <url hash="fe18f62f">2024.lrec-main.298</url>
      <bibkey>shao-etal-2024-cmdag-chinese</bibkey>
    </paper>
    <paper id="299">
      <title><fixed-case>CMNEE</fixed-case>:A Large-Scale Document-Level Event Extraction Dataset Based on Open-Source <fixed-case>C</fixed-case>hinese Military News</title>
      <author><first>Mengna</first><last>Zhu</last></author>
      <author><first>Zijie</first><last>Xu</last></author>
      <author><first>Kaisheng</first><last>Zeng</last></author>
      <author><first>Kaiming</first><last>Xiao</last></author>
      <author><first>Mao</first><last>Wang</last></author>
      <author><first>Wenjun</first><last>Ke</last></author>
      <author><first>Hongbin</first><last>Huang</last></author>
      <pages>3367–3379</pages>
      <abstract>Extracting structured event knowledge, including event triggers and corresponding arguments, from military texts is fundamental to many applications, such as intelligence analysis and decision assistance. However, event extraction in the military field faces the data scarcity problem, which impedes the research of event extraction models in this domain. To alleviate this problem, we propose CMNEE, a large-scale, document-level open-source Chinese Military News Event Extraction dataset. It contains 17,000 documents and 29,223 events, which are all manually annotated based on a pre-defined schema for the military domain including 8 event types and 11 argument role types. We designed a two-stage, multi-turns annotation strategy to ensure the quality of CMNEE and reproduced several state-of-the-art event extraction models with a systematic evaluation. The experimental results on CMNEE fall shorter than those on other domain datasets obviously, which demonstrates that event extraction for military domain poses unique challenges and requires further research efforts. Our code and data can be obtained from https://github.com/Mzzzhu/CMNEE. Keywords: Corpus,Information Extraction, Information Retrieval, Knowledge Discovery/Representation</abstract>
      <url hash="4d8da59e">2024.lrec-main.299</url>
      <bibkey>zhu-etal-2024-cmnee-large</bibkey>
    </paper>
    <paper id="300">
      <title><fixed-case>CM</fixed-case>-Off-Meme: Code-Mixed <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Offensive Meme Detection with Multi-Task Learning by Leveraging Contextual Knowledge</title>
      <author><first>Gitanjali</first><last>Kumari</last></author>
      <author><first>Dibyanayan</first><last>Bandyopadhyay</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Vinutha B.</first><last>NarayanaMurthy</last></author>
      <pages>3380–3393</pages>
      <abstract>Detecting offensive content in internet memes is challenging as it needs additional contextual knowledge. While previous works have only focused on detecting offensive memes, classifying them further into implicit and explicit categories depending on their severity is still a challenging and underexplored area. In this work, we present an end-to-end multitask model for addressing this challenge by empirically investigating two correlated tasks simultaneously: (i) offensive meme detection and (ii) explicit-implicit offensive meme detection by leveraging the two self-supervised pre-trained models. The first pre-trained model, referred to as the “knowledge encoder,” incorporates contextual knowledge of the meme. On the other hand, the second model, referred to as the “fine-grained information encoder”, is trained to understand the obscure psycho-linguistic information of the meme. Our proposed model utilizes contrastive learning to integrate these two pre-trained models, resulting in a more comprehensive understanding of the meme and its potential for offensiveness. To support our approach, we create a large-scale dataset, CM-Off-Meme, as there is no publicly available such dataset for the code-mixed Hindi-English (Hinglish) domain. Empirical evaluation, including both qualitative and quantitative analysis, on the CM-Off-Meme dataset demonstrates the effectiveness of the proposed model in terms of cross-domain generalization.</abstract>
      <url hash="3999f825">2024.lrec-main.300</url>
      <bibkey>kumari-etal-2024-cm-meme</bibkey>
    </paper>
    <paper id="301">
      <title><fixed-case>CO</fixed-case>3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite</title>
      <author><first>Yifei</first><last>Yuan</last></author>
      <author><first>Chen</first><last>Shi</last></author>
      <author><first>Wang</first><last>Runze</last></author>
      <author><first>Liyi</first><last>Chen</last></author>
      <author><first>Renjun</first><last>Hu</last></author>
      <author><first>Zengming</first><last>Zhang</last></author>
      <author><first>Feijun</first><last>Jiang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>3394–3406</pages>
      <abstract>Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, few-shot learning is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited data size. Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study low-resource generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled data to make further improvements via a contrastive co-training paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage contrastive learning with data augmentation, which enables our model pay more attention on the truly valuable information than the noise. Extensive experiments demonstrate the superiority of our model under both few-shot and zero-shot scenarios. We also verify the better generalization ability of our model when encountering language style shift.</abstract>
      <url hash="01565479">2024.lrec-main.301</url>
      <bibkey>yuan-etal-2024-co3-low</bibkey>
    </paper>
    <paper id="302">
      <title><fixed-case>C</fixed-case>o<fixed-case>ANZSE</fixed-case> Audio: Creation of an Online Corpus for Linguistic and Phonetic Analysis of <fixed-case>A</fixed-case>ustralian and <fixed-case>N</fixed-case>ew <fixed-case>Z</fixed-case>ealand Englishes</title>
      <author><first>Steven</first><last>Coats</last></author>
      <pages>3407–3412</pages>
      <abstract>CoANZSE Audio is a searchable online version of the Corpus of Australian and New Zealand Spoken English, a 195-million-word collection of geo-located YouTube transcripts of local government channels. In addition to the part-of-speech-tagged and lemmatized transcript data, CoANZSE Audio provides access to almost all of the underlying audio, as well as to forced alignments of the audio with transcript content, in Praat’s TextGrid format. This paper describes the methods used to create the corpus from open-source tools and the architecture of the CoANZSE Audio website. Two possible linguistic analyses based on CoANZSE Audio data are described: use of double modals, a rare syntactic feature, and raising of the mid front vowel /ɛ/ in New Zealand English. CoANZSE Audio can be considered to be among the first large, free, fully searchable online corpora containing data suitable for acoustic phonetic analyses in addition to lexical, grammatical, and discourse properties of Australian and New Zealand Englishes.</abstract>
      <url hash="8c19e13c">2024.lrec-main.302</url>
      <bibkey>coats-2024-coanzse-audio</bibkey>
    </paper>
    <paper id="303">
      <title>Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models</title>
      <author><first>Atsushi</first><last>Keyaki</last></author>
      <author><first>Ribeka</first><last>Keyaki</last></author>
      <pages>3413–3421</pages>
      <abstract>Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.</abstract>
      <url hash="d1c6ee1b">2024.lrec-main.303</url>
      <bibkey>keyaki-keyaki-2024-coarse-tuning</bibkey>
    </paper>
    <paper id="304">
      <title><fixed-case>C</fixed-case>o<fixed-case>B</fixed-case>a<fixed-case>LD</fixed-case> Annotation: The Enrichment of the Enhanced <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies with the Semantical Pattern</title>
      <author><first>Maria Andreevna</first><last>Petrova</last></author>
      <author><first>Alexandra M.</first><last>Ivoylova</last></author>
      <author><first>Anastasia</first><last>Tishchenkova</last></author>
      <pages>3422–3432</pages>
      <abstract>The paper is devoted to the annotation format aimed at morphological, syntactic and especially semantic markup. The format combines the Enhanced UD morphosyntax and the Compreno semantic pattern, enriching the UD annotation with word meanings and labels for semantic relations between words. To adapt the Compreno semantics for the current purpose, we reduced the number of the semantic fields denoting lexical meanings by using hyperonym fields. Moreover, we used a generalized variant of the semantic relations as the original roles possess rather narrow meanings which makes them too numerous. Creating such a format demands the Compreno-to-UD morphosyntax conversion as well, which, in turn, demands solving the asymmetry problem between the models. The asymmetry concerns tokenization, lemmatization, POS-tagging, sets of grammatical features and dependency heads. To overcome this problem, the Compreno-to-UD converter was created. As an application, the work presents a 150,000 token corpus of English news annotated according to the standard.</abstract>
      <url hash="ed2b94e4">2024.lrec-main.304</url>
      <bibkey>petrova-etal-2024-cobald-annotation</bibkey>
    </paper>
    <paper id="305">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>MIC</fixed-case>: Code Completion by Jointly Modeling In-file and Cross-file Context</title>
      <author><first>Yangruibo</first><last>Ding</last></author>
      <author><first>Zijian</first><last>Wang</last></author>
      <author><first>Wasi</first><last>Ahmad</last></author>
      <author><first>Murali Krishna</first><last>Ramanathan</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Parminder</first><last>Bhatia</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>3433–3445</pages>
      <abstract>While pre-trained language models (LM) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., project-level cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code LMs’ capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we propose CoCoMIC, a novel framework that jointly learns the in-file and cross-file context on top of code LMs. To empower CoCoMIC, we develop CCFinder, a static-analysis-based tool that locates and retrieves the most relevant project-level cross-file context for code completion. CoCoMIC successfully improves the existing code LM with a 33.94% relative increase in exact match and 28.69% in identifier matching for code completion when the cross-file context is provided. Finally, we perform a series of ablation studies and share valuable insights for future research on integrating cross-file context into code LMs.</abstract>
      <url hash="7072696b">2024.lrec-main.305</url>
      <bibkey>ding-etal-2024-cocomic-code</bibkey>
    </paper>
    <paper id="306">
      <title>Code Defect Detection Using Pre-trained Language Models with Encoder-Decoder via Line-Level Defect Localization</title>
      <author><first>Jimin</first><last>An</last></author>
      <author><first>YunSeok</first><last>Choi</last></author>
      <author><first>Jee-Hyong</first><last>Lee</last></author>
      <pages>3446–3456</pages>
      <abstract>Recently, code Pre-trained Language Models (PLMs) trained on large amounts of code and comment, have shown great success in code defect detection tasks. However, most PLMs simply treated the code as a single sequence and only used the encoder of PLMs to determine if there exist defects in the entire code. For a more analyzable and explainable approach, it is crucial to identify which lines contain defects. In this paper, we propose a novel method for code defect detection that integrates line-level defect localization into a unified training process. To identify code defects at the line-level, we convert the code into a sequence separated by lines using a special token. Then, to utilize the characteristic that both the encoder and decoder of PLMs process information differently, we leverage both the encoder and decoder for line-level defect localization. By learning code defect detection and line-level defect localization tasks in a unified manner, our proposed method promotes knowledge sharing between the two tasks. We demonstrate that our proposed method significantly improves performance on four benchmark datasets for code defect detection. Additionally, we show that our method can be easily integrated with ChatGPT.</abstract>
      <url hash="49e76b3e">2024.lrec-main.306</url>
      <bibkey>an-etal-2024-code-defect</bibkey>
    </paper>
    <paper id="307">
      <title>Code-Mixed Probes Show How Pre-Trained Models Generalise on Code-Switched Text</title>
      <author><first>Frances Adriana</first><last>Laureano De Leon</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>3457–3468</pages>
      <abstract>Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily stemming from the scarcity of labelled data and available resources. In this study we investigate how pre-trained Language Models handle code-switched text in three dimensions: a) the ability of PLMs to detect code-switched text, b) variations in the structural information that PLMs utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that pre-trained language models are effective in generalising to code-switched text, shedding light on abilities of these models to generalise representations to CS corpora. We release all our code and data, including the novel corpus, at https://github.com/francesita/code-mixed-probes.</abstract>
      <url hash="f41f3e9e">2024.lrec-main.307</url>
      <bibkey>laureano-de-leon-etal-2024-code-mixed</bibkey>
    </paper>
    <paper id="308">
      <title>Code-Mixed Text Augmentation for <fixed-case>L</fixed-case>atvian <fixed-case>ASR</fixed-case></title>
      <author><first>Martins</first><last>Kronis</last></author>
      <author><first>Askars</first><last>Salimbajevs</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <pages>3469–3479</pages>
      <abstract>Code-mixing has become mainstream in the modern, globalised world and affects low-resource languages, such as Latvian, in particular. Solutions to developing an automatic speech recognition system (ASR) for code-mixed speech often rely on specially created audio-text corpora, which are expensive and time-consuming to create. In this work, we attempt to tackle code-mixed Latvian-English speech recognition by improving the language model (LM) of a hybrid ASR system. We make a distinction between inflected transliterations and phonetic transcriptions as two different foreign word types. We propose an inflected transliteration model and a phonetic transcription model for the automatic generation of said word types. We then leverage a large human-translated English-Latvian parallel text corpus to generate synthetic code-mixed Latvian sentences by substituting in generated foreign words. Using the newly created augmented corpora, we train a new LM and combine it with our existing Latvian acoustic model (AM). For evaluation, we create a specialised foreign word test set on which our methods yield up to 15% relative CER improvement. We then further validate these results in a human evaluation campaign.</abstract>
      <url hash="e103b054">2024.lrec-main.308</url>
      <bibkey>kronis-etal-2024-code-mixed</bibkey>
    </paper>
    <paper id="309">
      <title>Cognitive Information Bottleneck: Extracting Minimal Sufficient Cognitive Language Processing Signals</title>
      <author><first>Yuto</first><last>Harada</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <pages>3480–3489</pages>
      <abstract>In Reinforcement Learning from Human Feedback (RLHF), explicit human feedback, such as rankings, is employed to align Natural Language Processing (NLP) models with human preferences. In contrast, the potential of implicit human feedback, encompassing cognitive processing signals like eye-tracking and brain activity, remains underexplored. These signals capture unconscious human responses but are often marred by noise and redundancy, complicating their application to specific tasks. To address this issue, we introduce the Cognitive Information Bottleneck (CIB), a method that extracts only the task-relevant information from cognitive processing signals. Grounded in the principles of the information bottleneck, CIB aims to learn representations that maximize the mutual information between the representations and targets while minimizing the mutual information between inputs and representations. By employing CIB to filter out redundant information from cognitive processing signals, our goal is to provide representations that are both minimal and sufficient. This approach enables more efficient fitting of models to inputs. Our results show that the proposed method outperforms existing methods in efficiently compressing various cognitive processing signals and significantly enhances performance on downstream tasks. Evaluated on public datasets, our model surpasses contemporary state-of-the-art models. Furthermore, by analyzing these compressed representations, we offer insights into how cognitive processing signals can be leveraged to improve performance.</abstract>
      <url hash="73b33760">2024.lrec-main.309</url>
      <bibkey>harada-oseki-2024-cognitive-information</bibkey>
    </paper>
    <paper id="310">
      <title><fixed-case>C</fixed-case>ollab<fixed-case>KG</fixed-case>: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction</title>
      <author><first>Xiang</first><last>Wei</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Ning</first><last>Cheng</last></author>
      <author><first>Xingyu</first><last>Cui</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <pages>3490–3506</pages>
      <abstract>In order to construct or extend entity-centric and event-centric knowledge graphs (KG and EKG), the information extraction (IE) annotation toolkit is essential. However, existing IE toolkits have several non-trivial problems, such as not supporting multi-tasks, and not supporting automatic updates. In this work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for KG and EKG construction. Specifically, for the multi-task issue, CollabKG unifies different IE subtasks, including named entity recognition (NER), entity-relation triple extraction (RE), and event extraction (EE), and supports both KG and EKG. Then, combining advanced prompting-based IE technology, the human-machine-cooperation mechanism with Large Language Models (LLMs) as the assistant machine is presented which can provide a lower cost as well as a higher performance. Lastly, owing to the two-way interaction between the human and machine, CollabKG with learning ability allows self-renewal. Besides, CollabKG has several appealing features (e.g., customization, training-free, and label propagation) that make the system powerful and high-productivity. We holistically compare our toolkit with other existing tools on these features. Human evaluation quantitatively illustrates that CollabKG significantly improves annotation quality, efficiency, and stability simultaneously.</abstract>
      <url hash="5619063e">2024.lrec-main.310</url>
      <bibkey>wei-etal-2024-collabkg-learnable</bibkey>
    </paper>
    <paper id="311">
      <title>Collecting and Analyzing Dialogues in a Tagline Co-Writing Task</title>
      <author><first>Xulin</first><last>Zhou</last></author>
      <author><first>Takuma</first><last>Ichikawa</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>3507–3517</pages>
      <abstract>The potential usage scenarios of dialogue systems will be greatly expanded if they are able to collaborate more creatively with humans. Many studies have examined ways of building such systems, but most of them focus on problem-solving dialogues, and relatively little research has been done on systems that can engage in creative collaboration with users. In this study, we designed a tagline co-writing task in which two people collaborate to create taglines via text chat, created an interface for data collection, and collected dialogue logs, editing logs, and questionnaire results. In total, we collected 782 Japanese dialogues. We describe the characteristic interactions comprising the tagline co-writing task and report the results of our analysis, in which we examined the kind of utterances that appear in the dialogues as well as the most frequent expressions found in highly rated dialogues in subjective evaluations. We also analyzed the relationship between subjective evaluations and workflow utilized in the dialogues and the interplay between taglines and utterances.</abstract>
      <url hash="9663d762">2024.lrec-main.311</url>
      <bibkey>zhou-etal-2024-collecting-analyzing</bibkey>
    </paper>
    <paper id="312">
      <title>Collecting Human-Agent Dialogue Dataset with Frontal Brain Signal toward Capturing Unexpressed Sentiment</title>
      <author><first>Shun</first><last>Katada</last></author>
      <author><first>Ryu</first><last>Takeda</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <pages>3518–3528</pages>
      <abstract>Multimodal information such as text and audiovisual data has been used for emotion/sentiment estimation during human-agent dialogue; however, user sentiments are not necessarily expressed explicitly during dialogues. Biosignals such as brain signals recorded using an electroencephalogram (EEG) sensor have been the subject of focus in affective computing regions to capture unexpressed emotional changes in a controlled experimental environment. In this study, we collect and analyze multimodal data with an EEG during a human-agent dialogue toward capturing unexpressed sentiment. Our contributions are as follows: (1) a new multimodal human-agent dialogue dataset is created, which includes not only text and audiovisual data but also frontal EEGs and physiological signals during the dialogue. In total, about 500-minute chat dialogues were collected from thirty participants aged 20 to 70. (2) We present a novel method for dealing with eye-blink noise for frontal EEGs denoising. This method applies facial landmark tracking to detect and delete eye-blink noise. (3) An experimental evaluation showed the effectiveness of the frontal EEGs. It improved sentiment estimation performance when used with other modalities by multimodal fusion, although it only has three channels.</abstract>
      <url hash="ff07983f">2024.lrec-main.312</url>
      <bibkey>katada-etal-2024-collecting-human</bibkey>
    </paper>
    <paper id="313">
      <title>Collecting Linguistic Resources for Assessing Children’s Pronunciation of <fixed-case>N</fixed-case>ordic Languages</title>
      <author><first>Anne Marte Haug</first><last>Olstad</last></author>
      <author><first>Anna</first><last>Smolander</last></author>
      <author><first>Sofia</first><last>Strömbergsson</last></author>
      <author><first>Sari</first><last>Ylinen</last></author>
      <author><first>Minna</first><last>Lehtonen</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <author><first>Yaroslav</first><last>Getman</last></author>
      <author><first>Tamás</first><last>Grósz</last></author>
      <author><first>Xinwei</first><last>Cao</last></author>
      <author><first>Torbjørn</first><last>Svendsen</last></author>
      <author><first>Giampiero</first><last>Salvi</last></author>
      <pages>3529–3537</pages>
      <abstract>This paper reports on the experience collecting a number of corpora of Nordic languages spoken by children. The aim of the data collection is providing annotated data to develop and evaluate computer assisted pronunciation assessment systems both for non-native children learning a Nordic language (L2) and for L1 children with speech sound disorder (SSD). The paper presents the challenges encountered recording and annotating data for Finnish, Swedish and Norwegian, as well as the ethical considerations related with making this data publicly available. We hope that sharing this experience will encourage others to collect similar data for other languages. Of the different data collections, we were able to make the Norwegian corpus publicly available in the hope that it will serve as a reference in pronunciation assessment research.</abstract>
      <url hash="172e3666">2024.lrec-main.313</url>
      <bibkey>olstad-etal-2024-collecting-linguistic</bibkey>
    </paper>
    <paper id="314">
      <title>Combining Discourse Coherence with Large Language Models for More Inclusive, Equitable, and Robust Task-Oriented Dialogue</title>
      <author><first>Katherine</first><last>Atwell</last></author>
      <author><first>Mert</first><last>Inan</last></author>
      <author><first>Anthony B.</first><last>Sicilia</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>3538–3552</pages>
      <abstract>Large language models (LLMs) are capable of generating well-formed responses, but using LLMs to generate responses on the fly is not yet feasible for many task-oriented systems. Modular architectures are often still required for safety and privacy guarantees on the output. We hypothesize that an offline generation approach using discourse theories, formal grammar rules, and LLMs can allow us to generate human-like, coherent text in a more efficient, robust, and inclusive manner within a task-oriented setting. To this end, we present the first discourse-aware multimodal task-oriented dialogue system that combines discourse theories with offline LLM generation. We deploy our bot as an app to the general public and keep track of the user ratings for six months. Our user ratings show an improvement from 2.8 to 3.5 out of 5 with the introduction of discourse coherence theories. We also show that our model reduces misunderstandings in the dialect of African-American Vernacular English from 93% to 57%. While terms of use prevent us from releasing our entire codebase, we release our code in a format that can be integrated into most existing dialogue systems.</abstract>
      <url hash="f6ea4f2c">2024.lrec-main.314</url>
      <bibkey>atwell-etal-2024-combining-discourse</bibkey>
    </paper>
    <paper id="315">
      <title><fixed-case>COMET</fixed-case> for Low-Resource Machine Translation Evaluation: A Case Study of <fixed-case>E</fixed-case>nglish-<fixed-case>M</fixed-case>altese and <fixed-case>S</fixed-case>panish-<fixed-case>B</fixed-case>asque</title>
      <author><first>Júlia</first><last>Falcão</last></author>
      <author><first>Claudia</first><last>Borg</last></author>
      <author><first>Nora</first><last>Aranberri</last></author>
      <author><first>Kurt</first><last>Abela</last></author>
      <pages>3553–3565</pages>
      <abstract>Trainable metrics for machine translation evaluation have been scoring the highest correlations with human judgements in the latest meta-evaluations, outperforming traditional lexical overlap metrics such as BLEU, which is still widely used despite its well-known shortcomings. In this work we look at COMET, a prominent neural evaluation system proposed in 2020, to analyze the extent of its language support restrictions, and to investigate strategies to extend this support to new, under-resourced languages. Our case study focuses on English-Maltese and Spanish-Basque. We run a crowd-based evaluation campaign to collect direct assessments and use the annotated dataset to evaluate COMET-22, further fine-tune it, and to train COMET models from scratch for the two language pairs. Our analysis suggests that COMET’s performance can be improved with fine-tuning, and that COMET can be highly susceptible to the distribution of scores in the training data, which especially impacts low-resource scenarios.</abstract>
      <url hash="04d65ae1">2024.lrec-main.315</url>
      <bibkey>falcao-etal-2024-comet-low</bibkey>
    </paper>
    <paper id="316">
      <title><fixed-case>COMICORDA</fixed-case>: Dialogue Act Recognition in Comic Books</title>
      <author><first>Jiri</first><last>Martinek</last></author>
      <author><first>Pavel</first><last>Kral</last></author>
      <author><first>Ladislav</first><last>Lenc</last></author>
      <author><first>Josef</first><last>Baloun</last></author>
      <pages>3566–3578</pages>
      <abstract>Dialogue act (DA) recognition is usually realized from a speech signal that is transcribed and segmented into text. However, only a little work in DA recognition from images exists. Therefore, this paper concentrates on this modality and presents a novel DA recognition approach for image documents, namely comic books. To the best of our knowledge, this is the first study investigating dialogue acts from comic books and represents the first steps to building a model for comic book understanding. The proposed method is composed of the following steps: speech balloon segmentation, optical character recognition (OCR), and DA recognition itself. We use YOLOv8 for balloon segmentation, Google Vision for OCR, and Transformer-based models for DA classification. The experiments are performed on a newly created dataset comprising 1,438 annotated comic panels. It contains bounding boxes, transcriptions, and dialogue act annotation. We have achieved nearly 98% average precision for speech balloon segmentation and exceeded the accuracy of 70% for the DA recognition task. We also present an analysis of dialogue structure in the comics domain and compare it with the standard DA datasets, representing another contribution of this paper.</abstract>
      <url hash="1181203a">2024.lrec-main.316</url>
      <bibkey>martinek-etal-2024-comicorda-dialogue</bibkey>
    </paper>
    <paper id="317">
      <title>Common <fixed-case>E</fixed-case>uropean Language Data Space</title>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <author><first>Katrin</first><last>Marheinecke</last></author>
      <author><first>Victoria</first><last>Arranz</last></author>
      <author><first>Aivars</first><last>Bērziņš</last></author>
      <author><first>Miltos</first><last>Deligiannis</last></author>
      <author><first>Dimitris</first><last>Galanis</last></author>
      <author><first>Maria</first><last>Giagkou</last></author>
      <author><first>Katerina</first><last>Gkirtzou</last></author>
      <author><first>Dimitris</first><last>Gkoumas</last></author>
      <author><first>Annika</first><last>Grützner-Zahn</last></author>
      <author><first>Athanasia</first><last>Kolovou</last></author>
      <author><first>Penny</first><last>Labropoulou</last></author>
      <author><first>Andis</first><last>Lagzdiņš</last></author>
      <author><first>Elena</first><last>Leitner</last></author>
      <author><first>Valérie</first><last>Mapelli</last></author>
      <author><first>Hélène</first><last>Mazo</last></author>
      <author><first>Simon</first><last>Ostermann</last></author>
      <author><first>Stefania</first><last>Racioppa</last></author>
      <author><first>Mickaël</first><last>Rigault</last></author>
      <author><first>Leon</first><last>Voukoutis</last></author>
      <pages>3579–3586</pages>
      <abstract>The Common European Language Data Space (LDS) is an integral part of the EU data strategy, which aims at developing a single market for data. Its decentralised technical infrastructure and governance scheme are currently being developed by the LDS project, which also has dedicated tasks for proof-of-concept prototypes, handling legal aspects, raising awareness and promoting the LDS through events and social media channels. The LDS is part of a broader vision for establishing all necessary components to develop European large language models.</abstract>
      <url hash="6773594a">2024.lrec-main.317</url>
      <bibkey>rehm-etal-2024-common-european</bibkey>
    </paper>
    <paper id="318">
      <title>Common Ground Tracking in Multimodal Dialogue</title>
      <author><first>Ibrahim Khalil</first><last>Khebour</last></author>
      <author><first>Kenneth</first><last>Lai</last></author>
      <author><first>Mariah</first><last>Bradford</last></author>
      <author><first>Yifan</first><last>Zhu</last></author>
      <author><first>Richard A.</first><last>Brutti</last></author>
      <author><first>Christopher</first><last>Tam</last></author>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Benjamin A.</first><last>Ibarra</last></author>
      <author><first>Nathaniel</first><last>Blanchard</last></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>3587–3602</pages>
      <abstract>Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on “dialogue state tracking” (DST), which is the ability to update the representations of the speaker’s needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is “common ground tracking” (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and ”questions under discussion” (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep neural model to predict moves toward construction of common ground. Model outputs cascade into a set of formal closure rules derived from situated evidence and belief axioms and update operations. We empirically assess the contribution of each feature type toward successful construction of common ground relative to ground truth, establishing a benchmark in this novel, challenging task.</abstract>
      <url hash="163e5072">2024.lrec-main.318</url>
      <bibkey>khebour-etal-2024-common-ground</bibkey>
    </paper>
    <paper id="319">
      <title>Comparative Analysis of Sign Language Interpreting Agents Perception: A Study of the Deaf</title>
      <author><first>Alfarabi</first><last>Imashev</last></author>
      <author><first>Nurziya</first><last>Oralbayeva</last></author>
      <author><first>Gulmira</first><last>Baizhanova</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>3603–3609</pages>
      <abstract>Prior research on sign language recognition has already demonstrated encouraging outcomes in achieving highly accurate and dependable automatic sign language recognition. The use of virtual characters as virtual assistants has significantly increased in the past decade. However, the progress in sign language generation and output that closely resembles physiologically believable human motions is still in its early stages. This assertion explains the lack of progress in virtual intelligent signing generative systems. Aside from the development of signing systems, scholarly research have revealed a significant deficiency in evaluating sign language generation systems by those who are deaf and use sign language. This paper presents the findings of a user study conducted with deaf signers. The study is aimed at comparing a state-of-the-art sign language generation system with a skilled sign language interpreter. The study focused on testing established metrics to gain insights into usability of such metrics for deaf signers and how deaf signers perceive signing agents.</abstract>
      <url hash="9a014d1f">2024.lrec-main.319</url>
      <bibkey>imashev-etal-2024-comparative-analysis</bibkey>
    </paper>
    <paper id="320">
      <title>Comparing Static and Contextual Distributional Semantic Models on Intrinsic Tasks: An Evaluation on <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese Datasets</title>
      <author><first>A</first><last>Pranav</last></author>
      <author><first>Yan</first><last>Cong</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <pages>3610–3627</pages>
      <abstract>The field of Distributional Semantics has recently undergone important changes, with the contextual representations produced by Transformers taking the place of static word embeddings models. Noticeably, previous studies comparing the two types of vectors have only focused on the English language and a limited number of models. In our study, we present a comparative evaluation of static and contextualized distributional models for Mandarin Chinese, focusing on a range of intrinsic tasks. Our results reveal that static models remain stronger for some of the classical tasks that consider word meaning independent of context, while contextualized models excel in identifying semantic relations between word pairs and in the categorization of words into abstract semantic classes.</abstract>
      <url hash="b61bec95">2024.lrec-main.320</url>
      <bibkey>pranav-etal-2024-comparing-static</bibkey>
    </paper>
    <paper id="321">
      <title>Comparison of Conventional Hybrid and <fixed-case>CTC</fixed-case>/Attention Decoders for Continuous Visual Speech Recognition</title>
      <author><first>David</first><last>Gimeno-Gómez</last></author>
      <author><first>Carlos-D.</first><last>Martínez-Hinarejos</last></author>
      <pages>3628–3638</pages>
      <abstract>Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.</abstract>
      <url hash="0b8d6ca6">2024.lrec-main.321</url>
      <bibkey>gimeno-gomez-martinez-hinarejos-2024-comparison-conventional</bibkey>
    </paper>
    <paper id="322">
      <title>Comparison of the Intimacy Process between Real and Acting-based Long-term Text Chats</title>
      <author><first>Tsunehiro</first><last>Arimoto</last></author>
      <author><first>Hiroaki</first><last>Sugiyama</last></author>
      <author><first>Hiromi</first><last>Narimatsu</last></author>
      <author><first>Masahiro</first><last>Mizukami</last></author>
      <pages>3639–3644</pages>
      <abstract>Long-term chatbots are expected to develop relationships with users. The major trend in this field’s recent long-term chatbot studies is to train systems with virtual long-term chat data called Multi-Session Chat (MSC), which collects text chat from multiple sessions of crowd workers playing the roles of speakers with defined personas. However, no investigation has attempted to determine whether such virtual long-term chat can successfully simulate relationship-building between speakers. To clarify the difference between an actual long-term intimacy process and an MSC intimacy process, this study collects real long-term chat and MSC in Japanese and compares them in terms of speech form and dialogue acts. The results of analyzing these factors suggest that MSC have an unnatural tendency to behave as if they have a close relationship with non-polite speech levels compared to actual long-term chats, but also as if they have a shallow relationship with more questions than real long-term chats.</abstract>
      <url hash="c96129f5">2024.lrec-main.322</url>
      <bibkey>arimoto-etal-2024-comparison-intimacy</bibkey>
    </paper>
    <paper id="323">
      <title>Complex Word Identification: A Comparative Study between <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> and a Dedicated Model for This Task</title>
      <author><first>Abdelhak</first><last>Kelious</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <author><first>Christophe</first><last>Coeur</last></author>
      <pages>3645–3653</pages>
      <abstract>There are several works in natural language processing for identifying lexical complexity. This can be for various reasons, either for simplification, the selection of more suitable content, or for other specific tasks. Words can have multiple definitions and degrees of complexity depending on the context in which they appear. One solution being investigated is lexical complexity prediction, where computational methods are used to evaluate the difficulty of vocabulary for language learners and offer personalized assistance. In this work, we explore deep learning methods to assess the complexity of a word based on its context. Specifically, we investigate how to use pre-trained language models to encode both the sentence and the target word, and then fine-tune them by combining them with additional frequency-based features. Our approach achieved superior results compared to the best systems in SemEval-2021 (Shardlow et al., 2021), as demonstrated by an R2 score of 0.65. Finally, we carry out a comparative study with ChatGPT to assess its potential for predicting lexical complexity, to see whether prompt engineering can be an alternative to this task, we will discuss the advantages and limitations of ChatGPT.</abstract>
      <url hash="47d69e35">2024.lrec-main.323</url>
      <bibkey>kelious-etal-2024-complex-word</bibkey>
    </paper>
    <paper id="324">
      <title>Comprehensive Study on <fixed-case>G</fixed-case>erman Language Models for Clinical and Biomedical Text Understanding</title>
      <author><first>Ahmad</first><last>Idrissi-Yaghir</last></author>
      <author><first>Amin</first><last>Dada</last></author>
      <author><first>Henning</first><last>Schäfer</last></author>
      <author><first>Kamyar</first><last>Arzideh</last></author>
      <author><first>Giulia</first><last>Baldini</last></author>
      <author><first>Jan</first><last>Trienes</last></author>
      <author><first>Max</first><last>Hasin</last></author>
      <author><first>Jeanette</first><last>Bewersdorff</last></author>
      <author><first>Cynthia S.</first><last>Schmidt</last></author>
      <author><first>Marie</first><last>Bauer</last></author>
      <author><first>Kaleb E.</first><last>Smith</last></author>
      <author><first>Jiang</first><last>Bian</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <author><first>Jörg</first><last>Schlötterer</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <author><first>Peter A.</first><last>Horn</last></author>
      <author><first>Christin</first><last>Seifert</last></author>
      <author><first>Felix</first><last>Nensa</last></author>
      <author><first>Jens</first><last>Kleesiek</last></author>
      <author><first>Christoph M.</first><last>Friedrich</last></author>
      <pages>3654–3665</pages>
      <abstract>Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can struggle in specialized domains such as medicine, where unique domain-specific terminologies, domain-specific abbreviations, and varying document structures are common. This paper explores strategies for adapting these models to domain-specific requirements, primarily through continuous pre-training on domain-specific data. We pre-trained several German medical language models on 2.4B tokens derived from translated public English medical data and 3B tokens of German clinical data. The resulting models were evaluated on various German downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question answering. Our results suggest that models augmented by clinical and translation-based pre-training typically outperform general domain models in medical contexts. We conclude that continuous pre-training has demonstrated the ability to match or even exceed the performance of clinical models trained from scratch. Furthermore, pre-training on clinical data or leveraging translated texts have proven to be reliable methods for domain adaptation in medical NLP tasks.</abstract>
      <url hash="a7ca2ffb">2024.lrec-main.324</url>
      <bibkey>idrissi-yaghir-etal-2024-comprehensive-study</bibkey>
    </paper>
    <paper id="325">
      <title>Computational Modelling of Plurality and Definiteness in <fixed-case>C</fixed-case>hinese Noun Phrases</title>
      <author><first>Yuqi</first><last>Liu</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Kees</first><last>van Deemter</last></author>
      <pages>3666–3676</pages>
      <abstract>Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are “cooler” than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using both classic machine learning models and state-of-the-art pre-trained language models to predict the plurality and definiteness of each NP. We report on the performance of these models and analyse their behaviours.</abstract>
      <url hash="b9b934d3">2024.lrec-main.325</url>
      <bibkey>liu-etal-2024-computational-modelling</bibkey>
    </paper>
    <paper id="326">
      <title><fixed-case>CONAN</fixed-case>-<fixed-case>MT</fixed-case>-<fixed-case>SP</fixed-case>: A <fixed-case>S</fixed-case>panish Corpus for Counternarrative Using <fixed-case>GPT</fixed-case> Models</title>
      <author><first>María Estrella</first><last>Vallecillo Rodríguez</last></author>
      <author><first>Maria Victoria</first><last>Cantero Romero</last></author>
      <author><first>Isabel</first><last>Cabrera De Castro</last></author>
      <author><first>Arturo</first><last>Montejo Ráez</last></author>
      <author><first>María Teresa</first><last>Martín Valdivia</last></author>
      <pages>3677–3688</pages>
      <abstract>This paper describes the automated generation of CounterNarratives (CNs) for Hate Speech (HS) in Spanish using GPT-based models. Our primary objective is to evaluate the performance of these models in comparison to human capabilities. For this purpose, the English CONAN Multitarget corpus is taken as a starting point and we use the DeepL API to automatically translate into Spanish. Two GPT-based models, GPT-3 and GPT-4, are applied to the HS segment through a few-shot prompting strategy to generate a new CN. As a consequence of our research, we have created a high quality corpus in Spanish that includes the original HS-CN pairs translated into Spanish, in addition to the CNs generated automatically with the GPT models and that have been evaluated manually. The resulting CONAN-MT-SP corpus and its evaluation will be made available to the research community, representing the most extensive linguistic resource of CNs in Spanish to date. The results demonstrate that, although the effectiveness of GPT-4 outperforms GPT-3, both models can be used as systems to automatically generate CNs to combat the HS. Moreover, these models consistently outperform human performance in most instances.</abstract>
      <url hash="347b3ba4">2024.lrec-main.326</url>
      <bibkey>vallecillo-rodriguez-etal-2024-conan-mt</bibkey>
    </paper>
    <paper id="327">
      <title>Conceptual Pacts for Reference Resolution Using Small, Dynamically Constructed Language Models: A Study in Puzzle Building Dialogues</title>
      <author><first>Julian</first><last>Hough</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <author><first>Casey</first><last>Kennington</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>3689–3699</pages>
      <abstract>Using Brennan and Clark’s theory of a Conceptual Pact, that when interlocutors agree on a name for an object, they are forming a temporary agreement on how to conceptualize that object, we present an extension to a simple reference resolver which simulates this process over time with different conversation pairs. In a puzzle construction domain, we model pacts with small language models for each referent which update during the interaction. When features from these pact models are incorporated into a simple bag-of-words reference resolver, the accuracy increases compared to using a standard pre-trained model. The model performs equally to a competitor using the same data but with exhaustive re-training after each prediction, while also being more transparent, faster and less resource-intensive. We also experiment with reducing the number of training interactions, and can still achieve reference resolution accuracies of over 80% in testing from observing a single previous interaction, over 20% higher than a pre-trained baseline. While this is a limited domain, we argue the model could be applicable to larger real-world applications in human and human-robot interaction and is an interpretable and transparent model.</abstract>
      <url hash="0dd0ec69">2024.lrec-main.327</url>
      <bibkey>hough-etal-2024-conceptual-pacts</bibkey>
    </paper>
    <paper id="328">
      <title><fixed-case>C</fixed-case>on<fixed-case>EC</fixed-case>: Earnings Call Dataset with Real-world Contexts for Benchmarking Contextual Speech Recognition</title>
      <author><first>Ruizhe</first><last>Huang</last></author>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Jan</first><last>Trmal</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Desh</first><last>Raj</last></author>
      <author><first>Leibny Paola</first><last>Garcia</last></author>
      <author><first>Alexei V.</first><last>Ivanov</last></author>
      <author><first>Patrick</first><last>Ehlen</last></author>
      <author><first>Mingzhi</first><last>Yu</last></author>
      <author><first>Dan</first><last>Povey</last></author>
      <author><first>Sanjeev</first><last>Khudanpur</last></author>
      <pages>3700–3706</pages>
      <abstract>Knowing the particular context associated with a conversation can help improving the performance of an automatic speech recognition (ASR) system. For example, if we are provided with a list of in-context words or phrases — such as the speaker’s contacts or recent song playlists — during inference, we can bias the recognition process towards this list. There are many works addressing contextual ASR; however, there is few publicly available real benchmark for evaluation, making it difficult to compare different solutions. To this end, we provide a corpus (“ConEC”) and baselines to evaluate contextual ASR approaches, grounded on real-world applications. The ConEC corpus is based on public-domain earnings calls (ECs) and associated supplementary materials, such as presentation slides, earnings news release as well as a list of meeting participants’ names and affiliations. We demonstrate that such real contexts are noisier than artificially synthesized contexts that contain the ground truth, yet they still make great room for future improvement of contextual ASR technology</abstract>
      <url hash="fd5a1ec2">2024.lrec-main.328</url>
      <bibkey>huang-etal-2024-conec-earnings</bibkey>
    </paper>
    <paper id="329">
      <title>Conjoin after Decompose: Improving Few-Shot Performance of Named Entity Recognition</title>
      <author><first>Chengcheng</first><last>Han</last></author>
      <author><first>Renyu</first><last>Zhu</last></author>
      <author><first>Jun</first><last>Kuang</last></author>
      <author><first>Fengjiao</first><last>Chen</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <author><first>Xuezhi</first><last>Cao</last></author>
      <author><first>Yunsen</first><last>Xian</last></author>
      <pages>3707–3717</pages>
      <abstract>Prompt-based methods have been widely used in few-shot named entity recognition (NER). In this paper, we first conduct a preliminary experiment and observe that the key to affecting the performance of prompt-based NER models is the capability to detect entity boundaries. However, most existing models fail to boost such capability. To solve the issue, we propose a novel model, ParaBART, which consists of a BART encoder and a specially designed parabiotic decoder. Specifically, the parabiotic decoder includes two BART decoders and a conjoint module. The two decoders are responsible for entity boundary detection and entity type classification, respectively. They are connected by the conjoint module, which is used to replace unimportant tokens’ embeddings in one decoder with the average embedding of all the tokens in the other. We further present a novel boundary expansion strategy to enhance the model’s capability in entity type classification. Experimental results show that ParaBART can achieve significant performance gains over state-of-the-art competitors.</abstract>
      <url hash="6a219fce">2024.lrec-main.329</url>
      <bibkey>han-etal-2024-conjoin-decompose</bibkey>
    </paper>
    <paper id="330">
      <title><fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>#: Fine-grained Error Analysis and a Corrected Test Set for <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>-03 <fixed-case>E</fixed-case>nglish</title>
      <author><first>Andrew</first><last>Rueda</last></author>
      <author><first>Elena</first><last>Alvarez-Mellado</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <pages>3718–3728</pages>
      <abstract>Modern named entity recognition systems have steadily improved performance in the age of larger and more powerful neural models. However, over the past several years, the state-of-the-art has seemingly hit another plateau on the benchmark CoNLL-03 English dataset. In this paper, we perform a deep dive into the test outputs of the highest-performing NER models, conducting a fine-grained evaluation of their performance by introducing new document-level annotations on the test set. We go beyond F1 scores by categorizing errors in order to interpret the true state of the art for NER and guide future work. We review previous attempts at correcting the various flaws of the test set and introduce CoNLL#, a new corrected version of the test set that addresses its systematic and most prevalent errors, allowing for low-noise, interpretable error analysis.</abstract>
      <url hash="b135895f">2024.lrec-main.330</url>
      <bibkey>rueda-etal-2024-conll-fine</bibkey>
    </paper>
    <paper id="331">
      <title>Connecting Language Technologies with Rich, Diverse Data Sources Covering Thousands of Languages</title>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Sandy</first><last>Ritchie</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <author><first>Ishank</first><last>Saxena</last></author>
      <author><first>Isaac</first><last>Caswell</last></author>
      <pages>3729–3746</pages>
      <abstract>Contrary to common belief, there are rich and diverse data sources available for many thousands of languages, which can be used to develop technologies for these languages. In this paper, we provide an overview of some of the major online data sources, the types of data that they provide access to, potential applications of this data, and the number of languages that they cover. Even this covers only a small fraction of the data that exists; for example, printed books are published in many languages but few online aggregators exist.</abstract>
      <url hash="6f8d0b89">2024.lrec-main.331</url>
      <bibkey>van-esch-etal-2024-connecting-language</bibkey>
    </paper>
    <paper id="332">
      <title>Constructing a Dependency Treebank for Second Language Learners of <fixed-case>K</fixed-case>orean</title>
      <author><first>Hakyung</first><last>Sung</last></author>
      <author><first>Gyu-Ho</first><last>Shin</last></author>
      <pages>3747–3758</pages>
      <abstract>We introduce a manually annotated syntactic treebank based on Universal Dependencies, derived from the written data of second language (L2) Korean learners. In developing this new dataset, we critically evaluated previous works and revised the annotation guidelines to better reflect the linguistic properties of Korean and the characteristics of L2 learners. The L2 Korean treebank encompasses 7,530 sentences (66,982 words; 129,333 morphemes) and is publicly available at: https://github.com/NLPxL2Korean/L2KW-corpus.</abstract>
      <url hash="35f72c14">2024.lrec-main.332</url>
      <bibkey>sung-shin-2024-constructing-dependency</bibkey>
    </paper>
    <paper id="333">
      <title>Constructing <fixed-case>I</fixed-case>ndonesian-<fixed-case>E</fixed-case>nglish Travelogue Dataset</title>
      <author><first>Eunike Andriani</first><last>Kardinata</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>3759–3771</pages>
      <abstract>Research in low-resource language is often hampered due to the under-representation of how the language is being used in reality. This is particularly true for Indonesian language because there is a limited variety of textual datasets, and majority were acquired from official sources with formal writing style. All the more for the task of geoparsing, which could be implemented for navigation and travel planning applications, such datasets are rare, even in the high-resource languages, such as English. Being aware of the need for a new resource in both languages for this specific task, we constructed a new dataset comprising both Indonesian and English from personal travelogue articles. Our dataset consists of 88 articles, exactly half of them written in each language. We covered both named and nominal expressions of four entity types related to travel: location, facility, transportation, and line. We also conducted experiments by training classifiers to recognise named entities and their nominal expressions. The results of our experiments showed a promising future use of our dataset as we obtained F1-score above 0.9 for both languages.</abstract>
      <url hash="792b6199">2024.lrec-main.333</url>
      <bibkey>kardinata-etal-2024-constructing-indonesian</bibkey>
    </paper>
    <paper id="334">
      <title>Constructing <fixed-case>K</fixed-case>orean Learners’ <fixed-case>L</fixed-case>2 Speech Corpus of Seven Languages for Automatic Pronunciation Assessment</title>
      <author><first>Seunghee</first><last>Han</last></author>
      <author><first>Sunhee</first><last>Kim</last></author>
      <author><first>Minhwa</first><last>Chung</last></author>
      <pages>3772–3781</pages>
      <abstract>Multilingual L2 speech corpora for developing automatic speech assessment are currently available, but they lack comprehensive annotations of L2 speech from non-native speakers of various languages. This study introduces the methodology of designing a Korean learners’ L2 speech corpus of seven languages: English, Japanese, Chinese, French, German, Spanish, and Russian. We describe the development of reading scripts, reading tasks, scoring criteria, and expert evaluation methods in detail. Our corpus contains 1,200 hours of L2 speech data from Korean learners (400 hours for English, 200 hours each for Japanese and Chinese, 100 hours each for French, German, Spanish, and Russian). The corpus is annotated with spelling and pronunciation transcription, expert pronunciation assessment scores (accuracy of pronunciation and fluency of prosody), and metadata such as gender, age, self-reported language proficiency, and pronunciation error types. We also propose a practical verification method and a reliability threshold to ensure the reliability and objectivity of large-scale subjective evaluation data.</abstract>
      <url hash="ad37c5f3">2024.lrec-main.334</url>
      <bibkey>han-etal-2024-constructing-korean</bibkey>
    </paper>
    <paper id="335">
      <title>Construction of Paired Knowledge Graph - Text Datasets Informed by Cyclic Evaluation</title>
      <author><first>Ali</first><last>Mousavi</last></author>
      <author><first>Xin</first><last>Zhan</last></author>
      <author><first>He</first><last>Bai</last></author>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Theodoros</first><last>Rekatsinas</last></author>
      <author><first>Benjamin</first><last>Han</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Jeffrey</first><last>Pound</last></author>
      <author><first>Joshua M.</first><last>Susskind</last></author>
      <author><first>Natalie</first><last>Schluter</last></author>
      <author><first>Ihab F.</first><last>Ilyas</last></author>
      <author><first>Navdeep</first><last>Jaitly</last></author>
      <pages>3782–3803</pages>
      <abstract>Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Informed by these observations, we construct a new, improved dataset called <b>LAGRANGE</b> using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.</abstract>
      <url hash="21a93829">2024.lrec-main.335</url>
      <bibkey>mousavi-etal-2024-construction-paired</bibkey>
    </paper>
    <paper id="336">
      <title>Constructions Are So Difficult That <fixed-case>E</fixed-case>ven Large Language Models Get Them Right for the Wrong Reasons</title>
      <author><first>Shijia</first><last>Zhou</last></author>
      <author><first>Leonie</first><last>Weissweiler</last></author>
      <author><first>Taiqi</first><last>He</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <pages>3804–3811</pages>
      <abstract>In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM’s understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don’t adequately represent their meaning or capture the lexical properties of phrasal heads.</abstract>
      <url hash="86ad7ce0">2024.lrec-main.336</url>
      <bibkey>zhou-etal-2024-constructions-difficult</bibkey>
      <revision id="1" href="2024.lrec-main.336v1" hash="d6347d2b"/>
      <revision id="2" href="2024.lrec-main.336v2" hash="86ad7ce0" date="2024-06-09">This revision corrected a typo in table 2.</revision>
    </paper>
    <paper id="337">
      <title>Context-Aware Non-Autoregressive Document-Level Translation with Sentence-Aligned Connectionist Temporal Classification</title>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Anqi</first><last>Zhao</last></author>
      <author><first>Junpeng</first><last>Liu</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>3812–3824</pages>
      <abstract>Previous studies employ the autoregressive translation (AT) paradigm in the document-to-document neural machine translation. These methods extend the translation unit from a single sentence to a pseudo-document and encodes the full pseudo-document, avoiding the redundant computation problem in context. However, the AT methods cannot parallelize decoding and struggle with error accumulation, especially when the length of sentences increases. In this work, we propose a context-aware non-autoregressive framework with the sentence-aligned connectionist temporal classification (SA-CTC) loss for document-level neural machine translation. In particular, the SA-CTC loss reduces the search space of the decoding path by fixing the positions of the beginning and end tokens for each sentence in the document. Meanwhile, the context-aware architecture introduces preset nodes to represent sentence-level information and utilizes a hierarchical attention structure to regulate the attention hypothesis space. Experimental results show that our proposed method can achieve competitive performance compared with several strong baselines. Our method implements non-autoregressive modeling in Doc-to-Doc translation manner, achieving an average 46X decoding speedup compared to the document-level AT baselines on three benchmarks.</abstract>
      <url hash="7151753e">2024.lrec-main.337</url>
      <bibkey>yu-etal-2024-context-aware</bibkey>
    </paper>
    <paper id="338">
      <title>Context Matters: Enhancing Metaphor Recognition in Proverbs</title>
      <author><first>Gamze</first><last>Goren</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>3825–3830</pages>
      <abstract>Despite the remarkable achievements of Large Language Models (LLMs) in various Natural Language Processing tasks, their competence in abstract language understanding remains a relatively under-explored territory. Figurative language interpretation serves as ideal testbed for assessing this as it requires models to navigate beyond the literal meaning and delve into underlying semantics of the figurative expressions. In this paper, we seek to examine the performance of GPT-3.5 in zero-shot setting through word-level metaphor detection. Specifically, we frame the task as annotation of word-level metaphors in proverbs. To this end, we employ a dataset of English proverbs and evaluated its performance by applying different prompting strategies. Our results show that the model shows a satisfactory performance at identifying word-level metaphors, particularly when it is prompted with a hypothetical context preceding the proverb. This observation underscores the pivotal role of well-designed prompts for zero-shot settings through which these models can be leveraged as annotators for subjective NLP tasks.</abstract>
      <url hash="5884e1ae">2024.lrec-main.338</url>
      <bibkey>goren-strapparava-2024-context-matters</bibkey>
    </paper>
    <paper id="339">
      <title>Context Shapes Emergent Communication about Concepts at Different Levels of Abstraction</title>
      <author><first>Kristina</first><last>Kobrock</last></author>
      <author><first>Xenia Isabel</first><last>Ohmer</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <author><first>Nicole</first><last>Gotzner</last></author>
      <pages>3831–3848</pages>
      <abstract>We study the communication of concepts at different levels of abstraction and in different contexts in an agent-based, interactive reference game. While playing the concept-level reference game, the neural network agents develop a communication system from scratch. We use a novel symbolic dataset that disentangles concept type (ranging from specific to generic) and context (ranging from fine to coarse) to study the influence of these factors on the emerging language. We compare two game scenarios: one in which speaker agents have access to context information (context-aware) and one in which the speaker agents do not have access to context information (context-unaware). First, we find that the agents learn higher-level concepts from the object inputs alone. Second, an analysis of the emergent communication system shows that only context-aware agents learn to communicate efficiently by adapting their messages to the context conditions and relying on context for unambiguous reference. Crucially, this behavior is not explicitly incentivized by the game, but efficient communication emerges and is driven by the availability of context alone. The emerging language we observe is reminiscent of evolutionary pressures on human languages and highlights the pivotal role of context in a communication system.</abstract>
      <url hash="55e44f88">2024.lrec-main.339</url>
      <bibkey>kobrock-etal-2024-context-shapes</bibkey>
    </paper>
    <paper id="340">
      <title>Contextualizing Generated Citation Texts</title>
      <author><first>Biswadip</first><last>Mandal</last></author>
      <author><first>Xiangci</first><last>Li</last></author>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <pages>3849–3854</pages>
      <abstract>Abstractive citation text generation is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference paper’s main contribution, ignoring the citation context’s focus on a different topic. To address this problem, we propose a simple modification to the citation text generation task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and allows the generation model to make use of contextual clues about what topic to discuss and what stance to take.</abstract>
      <url hash="868d5a49">2024.lrec-main.340</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8d54e85f">2024.lrec-main.340.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>mandal-etal-2024-contextualizing-generated</bibkey>
    </paper>
    <paper id="341">
      <title>Contextual Modeling for Document-level <fixed-case>ASR</fixed-case> Error Correction</title>
      <author><first>Jin</first><last>Jiang</last></author>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Rongjun</first><last>Li</last></author>
      <author><first>Jingyuan</first><last>Yang</last></author>
      <author><first>Yanquan</first><last>Zhou</last></author>
      <pages>3855–3867</pages>
      <abstract>Contextual information, including the sentences in the same document and in other documents of the dataset, plays a crucial role in improving the accuracy of document-level ASR Error Correction (AEC), while most previous works ignore this. In this paper, we propose a context-aware method that utilizes a <tex-math>k</tex-math>-Nearest Neighbors (<tex-math>k</tex-math>NN) approach to enhance the AEC model by retrieving a datastore containing contextual information. We conduct experiments on two English and two Chinese datasets, and the results demonstrate that our proposed model can effectively utilize contextual information to improve document-level AEC. Furthermore, the context information from the whole dataset provides even better results.</abstract>
      <url hash="5a161ab3">2024.lrec-main.341</url>
      <bibkey>jiang-etal-2024-contextual-modeling</bibkey>
    </paper>
    <paper id="342">
      <title>Continual Few-shot Event Detection via Hierarchical Augmentation Networks</title>
      <author><first>Chenlong</first><last>Zhang</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>3868–3880</pages>
      <abstract>Traditional continual event detection relies on abundant labeled data for training, which is often impractical to obtain in real-world applications. In this paper, we introduce continual few-shot event detection (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible. The CFED task is challenging as it involves memorizing previous event types and learning new event types with few-shot samples. To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Network (HANet). To memorize previous event types with limited memory, we incorporate prototypical augmentation into the memory set. For the issue of learning new event types in few-shot scenarios, we propose a contrastive augmentation module for token representations. Despite comparing with previous state-of-the-art methods, we also conduct comparisons with ChatGPT. Experiment results demonstrate that our method significantly outperforms all of these methods in multiple continual few-shot event detection tasks.</abstract>
      <url hash="dd99e78d">2024.lrec-main.342</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8b2e602d">2024.lrec-main.342.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>zhang-etal-2024-continual-shot</bibkey>
    </paper>
    <paper id="343">
      <title>Continual Reinforcement Learning for Controlled Text Generation</title>
      <author><first>Velizar</first><last>Shulev</last></author>
      <author><first>Khalil</first><last>Sima’an</last></author>
      <pages>3881–3889</pages>
      <abstract>Controlled Text Generation (CTG) steers the generation of continuations of a given context (prompt) by a Large Language Model (LLM) towards texts possessing a given attribute (e.g., topic, sentiment). In this paper we view CTG as a Continual Learning problem: how to learn at every step to steer next-word generation, without having to wait for end-of-sentence. This continual view is useful for online applications such as CTG for speech, where end-of-sentence is often uncertain. We depart from an existing model, the Plug-and-Play language models (PPLM), which perturbs the context at each step to better predict next-words that posses the desired attribute. While PPLM is intricate and has many hyper-parameters, we provide a proof that the PPLM objective function can be reduced to a Continual Reinforcement Learning (CRL) reward function, thereby simplifying PPLM and endowing it with a better understood learning framework. Subsequently, we present, the first of its kind, CTG algorithm that is fully based on CRL and exhibit promising empirical results.</abstract>
      <url hash="67c4c83d">2024.lrec-main.343</url>
      <bibkey>shulev-simaan-2024-continual-reinforcement</bibkey>
    </paper>
    <paper id="344">
      <title>Continued Pre-training on Sentence Analogies for Translation with Small Data</title>
      <author><first>Liyan</first><last>Wang</last></author>
      <author><first>Haotong</first><last>Wang</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>3890–3896</pages>
      <abstract>This paper introduces Continued Pre-training on Analogies (CPoA) to incorporate pre-trained language models with analogical abilities, aiming at improving performance in low-resource translations without data augmentation. We continue training the models on sentence analogies retrieved from a translation corpus. Considering the sparsity of analogy in corpora, especially in low-resource scenarios, we propose exploring approximate analogies between sentences. We attempt to find sentence analogies that might not conform to formal criteria for entire sentences but partial pieces. When training the models, we introduce a weighting scalar pertaining to the quality of analogies to adjust the influence: emphasizing closer analogies while diminishing the impact of far ones. We evaluate our approach on a low-resource translation task: German-Upper Sorbian. The results show that CPoA using 10 times fewer instances can effectively attain gains of +1.4 and +1.3 BLEU points over the original model in two translation directions. This improvement is more pronounced when there are fewer parallel examples.</abstract>
      <url hash="cc31080f">2024.lrec-main.344</url>
      <bibkey>wang-etal-2024-continued-pre</bibkey>
    </paper>
    <paper id="345">
      <title>Continuous Relational Diffusion Driven Topic Model with Multi-grained Text for Microblog</title>
      <author><first>Chenhao</first><last>Wu</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <pages>3897–3906</pages>
      <abstract>Topic model is a statistical model that leverages unsupervised learning to mine hidden topics in document collections. The data sparsity and colloquialism of social texts make it difficult to accurately mine the topics. Traditional methods assume that there are only 0/1-state relationships between the two parties in the social networks, but the relationship status in real life is more complicated, such as continuously changing relationships with different degrees of intimacy. This paper proposes a continuous relational diffusion driven topic model (CRTM) with multi-grained text for microblog to realize the continuous representation of the relationship state and make up for the context and structural information lost by previous representation methods. Multi-grained text representation learning distinguishes the impact of formal and informal expression on the topics further and alleviates colloquialism problems. Specifically, based on the original social network, the reconstructed social network with continuous relationship status is obtained by using information diffusion technology. The graph convolution model is utilized to learn node embeddings through the new social network. Finally, the neural variational inference is applied to generate topics according to continuous relationships. We validate CRTM on three real datasets, and the experimental results show the effectiveness of the scheme.</abstract>
      <url hash="4653dbe9">2024.lrec-main.345</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fde9128b">2024.lrec-main.345.OptionalSupplementaryMaterial.rar</attachment>
      <bibkey>wu-etal-2024-continuous-relational</bibkey>
    </paper>
    <paper id="346">
      <title><fixed-case>C</fixed-case>ontrast<fixed-case>WSD</fixed-case>: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure</title>
      <author><first>Mohamad</first><last>Elzohbi</last></author>
      <author><first>Richard</first><last>Zhao</last></author>
      <pages>3907–3915</pages>
      <abstract>This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.</abstract>
      <url hash="fab59ae0">2024.lrec-main.346</url>
      <bibkey>elzohbi-zhao-2024-contrastwsd-enhancing</bibkey>
    </paper>
    <paper id="347">
      <title>Contribution of Move Structure to Automatic Genre Identification: An Annotated Corpus of <fixed-case>F</fixed-case>rench Tourism Websites</title>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Trang Tran Hanh</first><last>Pham</last></author>
      <author><first>Julien</first><last>Zakhia Doueihi</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>3916–3926</pages>
      <abstract>The present work studies the contribution of move structure to automatic genre identification. This concept - well known in other branches of genre analysis - seems to have little application in natural language processing. We describe how we collect a corpus of websites in French related to tourism and annotate it with move structure. We conduct experiments on automatic genre identification with our corpus. Our results show that our approach for informing a model with move structure can increase its performance for automatic genre identification, and reduce the need for annotated data and computational power.</abstract>
      <url hash="422fd857">2024.lrec-main.347</url>
      <bibkey>cardon-etal-2024-contribution-move</bibkey>
    </paper>
    <paper id="348">
      <title>Controllable Paraphrase Generation for Semantic and Lexical Similarities</title>
      <author><first>Yuya</first><last>Ogasa</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>3927–3942</pages>
      <abstract>We developed a controllable paraphrase generation model for semantic and lexical similarities using a simple and intuitive mechanism: attaching tags to specify these values at the head of the input sentence. Lexically diverse paraphrases have been long coveted for data augmentation. However, their generation is not straightforward because diversifying surfaces easily degrades semantic similarity. Furthermore, our experiments revealed two critical features in data augmentation by paraphrasing: appropriate similarities of paraphrases are highly downstream task-dependent, and mixing paraphrases of various similarities negatively affects the downstream tasks. These features indicated that the controllability in paraphrase generation is crucial for successful data augmentation. We tackled these challenges by fine-tuning a pre-trained sequence-to-sequence model employing tags that indicate the semantic and lexical similarities of synthetic paraphrases selected carefully based on the similarities. The resultant model could paraphrase an input sentence according to the tags specified. Extensive experiments on data augmentation for contrastive learning and pre-fine-tuning of pretrained masked language models confirmed the effectiveness of the proposed model. We release our paraphrase generation model and a corpus of 87 million diverse paraphrases. (https://github.com/Ogamon958/ConPGS)</abstract>
      <url hash="92b9004a">2024.lrec-main.348</url>
      <bibkey>ogasa-etal-2024-controllable-paraphrase</bibkey>
    </paper>
    <paper id="349">
      <title>Controllable Sentence Simplification in <fixed-case>S</fixed-case>wedish Using Control Prefixes and Mined Paraphrases</title>
      <author><first>Julius</first><last>Monsen</last></author>
      <author><first>Arne</first><last>Jonsson</last></author>
      <pages>3943–3954</pages>
      <abstract>Making information accessible to diverse target audiences, including individuals with dyslexia and cognitive disabilities, is crucial. Automatic Text Simplification (ATS) systems aim to facilitate readability and comprehension by reducing linguistic complexity. However, they often lack customizability to specific user needs, and training data for smaller languages can be scarce. This paper addresses ATS in a Swedish context, using methods that provide more control over the simplification. A dataset of Swedish paraphrases is mined from large amounts of text and used to train ATS models utilizing prefix-tuning with control prefixes. We also introduce a novel data-driven method for selecting complexity attributes for controlling the simplification and compare it with previous approaches. Evaluation of the trained models using SARI and BLEU demonstrates significant improvements over the baseline — a fine-tuned Swedish BART model — and compared to previous Swedish ATS results. These findings highlight the effectiveness of employing paraphrase data in conjunction with controllable generation mechanisms for simplification. Additionally, the set of explored attributes yields similar results compared to previously used attributes, indicating their ability to capture important simplification aspects.</abstract>
      <url hash="bc3531bc">2024.lrec-main.349</url>
      <bibkey>monsen-jonsson-2024-controllable-sentence</bibkey>
    </paper>
    <paper id="350">
      <title>Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>3955–3961</pages>
      <abstract>In Grammatical Error Correction (GEC), it is crucial to ensure the user’s comprehension of a reason for correction. Existing studies present tokens, examples, and hints for corrections, but do not directly explain the reasons in natural language. Although methods that use Large Language Models (LLMs) to provide direct explanations in natural language have been proposed for various tasks, no such method exists for GEC. Generating explanations for GEC corrections involves aligning input and output tokens, identifying correction points, and presenting corresponding explanations consistently. However, it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts. This study introduces a method called controlled generation with Prompt Insertion (PI) so that LLMs can explain the reasons for corrections in natural language. In PI, LLMs first correct the input text, and then we automatically extract the correction points based on the rules. The extracted correction points are sequentially inserted into the LLM’s explanation output as prompts, guiding the LLMs to generate explanations for the correction points. We also create an Explainable GEC (XGEC) dataset of correction reasons by annotating NUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3.5 and ChatGPT using original prompts miss some correction points, the generation control using PI can explicitly guide to describe explanations for all correction points, contributing to improved performance in generating correction reasons.</abstract>
      <url hash="00bd6ae2">2024.lrec-main.350</url>
      <bibkey>kaneko-okazaki-2024-controlled-generation</bibkey>
      <revision id="1" href="2024.lrec-main.350v1" hash="a80f2f90"/>
      <revision id="2" href="2024.lrec-main.350v2" hash="00bd6ae2" date="2024-05-30">The original paper is anonymized.</revision>
    </paper>
    <paper id="351">
      <title><fixed-case>C</fixed-case>ontroversial<fixed-case>QA</fixed-case>: Exploring Controversy in Question Answering</title>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Peide</first><last>Zhu</last></author>
      <author><first>Jie</first><last>Yang</last></author>
      <pages>3962–3966</pages>
      <abstract>Controversy is widespread online. Previous studies mainly define controversy based on vague assumptions of its relation to sentiment such as hate speech and offensive words. This paper introduces the first question-answering dataset that defines content controversy by user perception, i.e., votes from plenty of users. It contains nearly 10K questions, and each question has a best answer and a most controversial answer. Experimental results reveal that controversy detection in question answering is essential and challenging, and there is no strong correlation between controversy and sentiment tasks. We also show that controversial answers and most acceptable answers cannot be distinguished by retrieval-based QA models, which may cause controversy issues. With these insights, we believe ControversialQA can inspire future research on controversy in QA systems.</abstract>
      <url hash="10935b73">2024.lrec-main.351</url>
      <bibkey>wang-etal-2024-controversialqa-exploring</bibkey>
    </paper>
    <paper id="352">
      <title>Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units</title>
      <author><first>Biswesh</first><last>Mohapatra</last></author>
      <author><first>Seemab</first><last>Hassan</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Justine</first><last>Cassell</last></author>
      <pages>3967–3977</pages>
      <abstract>Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities. Traum (Traum, 1995) provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding. We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs. Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs.</abstract>
      <url hash="11cde183">2024.lrec-main.352</url>
      <bibkey>mohapatra-etal-2024-conversational-grounding</bibkey>
    </paper>
    <paper id="353">
      <title>Converting Legacy Data to <fixed-case>CLDF</fixed-case>: A <fixed-case>FAIR</fixed-case> Exit Strategy for Linguistic Web Apps</title>
      <author><first>Robert</first><last>Forkel</last></author>
      <author><first>Daniel G.</first><last>Swanson</last></author>
      <author><first>Steven</first><last>Moran</last></author>
      <pages>3978–3982</pages>
      <abstract>In the mid 2000s, there were several large-scale US National Science Foundation (NSF) grants awarded to projects aiming at developing digital infrastructure and standards for different forms of linguistics data. For example, MultiTree encoded language family trees as phylogenies in XML and LL-MAP converted detailed geographic maps of endangered languages into KML. As early stand-alone website applications, these projects allowed researchers interested in comparative linguistics to explore language genealogies and areality, respectively. However as time passed, the technologies that supported these web apps became deprecated, unsupported, and inaccessible. Here we take a future-oriented approach to digital obsolescence and illustrate how to convert legacy linguistic resources into FAIR data via the Cross-Linguistic Data Formats (CLDF). CLDF is built on the W3C recommendations Model for Tabular Data and Metadata on the Web and Metadata Vocabulary for Tabular Data developed by the CSVW (CSV on the Web) working group. Thus, each dataset is modeled as a set of tabular data files described by metadata in JSON. These standards and the tools built to validate and manipulate them provide an accessible and extensible format for converting legacy linguistic web apps into FAIR datasets.</abstract>
      <url hash="2a83ab49">2024.lrec-main.353</url>
      <bibkey>forkel-etal-2024-converting-legacy</bibkey>
    </paper>
    <paper id="354">
      <title><fixed-case>C</fixed-case>ooking<fixed-case>S</fixed-case>ense: A Culinary Knowledgebase with Multidisciplinary Assertions</title>
      <author><first>Donghee</first><last>Choi</last></author>
      <author><first>Mogan</first><last>Gim</last></author>
      <author><first>Donghyeon</first><last>Park</last></author>
      <author><first>Mujeen</first><last>Sung</last></author>
      <author><first>Hyunjae</first><last>Kim</last></author>
      <author><first>Jaewoo</first><last>Kang</last></author>
      <author><first>Jihun</first><last>Choi</last></author>
      <pages>3983–3996</pages>
      <abstract>This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired. CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions. Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems. From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models. We also validate the quality and variety of assertions in CookingSense through qualitative analysis.</abstract>
      <url hash="b04f5adc">2024.lrec-main.354</url>
      <bibkey>choi-etal-2024-cookingsense-culinary</bibkey>
    </paper>
    <paper id="355">
      <title><fixed-case>C</fixed-case>o<fixed-case>R</fixed-case>elation: Boosting Automatic <fixed-case>ICD</fixed-case> Coding through Contextualized Code Relation Learning</title>
      <author><first>Junyu</first><last>Luo</last></author>
      <author><first>Xiaochen</first><last>Wang</last></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <author><first>Aofei</first><last>Chang</last></author>
      <author><first>Yaqing</first><last>Wang</last></author>
      <author><first>Fenglong</first><last>Ma</last></author>
      <pages>3997–4007</pages>
      <abstract>Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines.</abstract>
      <url hash="0eb3c75a">2024.lrec-main.355</url>
      <bibkey>luo-etal-2024-corelation-boosting</bibkey>
    </paper>
    <paper id="356">
      <title><fixed-case>CORI</fixed-case>: <fixed-case>CJKV</fixed-case> Benchmark with <fixed-case>R</fixed-case>omanization Integration - a Step towards Cross-lingual Transfer beyond Textual Scripts</title>
      <author><first>Hoang</first><last>Nguyen</last></author>
      <author><first>Chenwei</first><last>Zhang</last></author>
      <author><first>Ye</first><last>Liu</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <author><first>Eugene</first><last>Rohrbaugh</last></author>
      <author><first>Philip S.</first><last>Yu</last></author>
      <pages>4008–4020</pages>
      <abstract>Naively assuming English as a source language may hinder cross-lingual transfer for many languages by failing to consider the importance of language contact. Some languages are more well-connected than others, and target languages can benefit from transferring from closely related languages; for many languages, the set of closely related languages does not include English. In this work, we study the impact of source language for cross-lingual transfer, demonstrating the importance of selecting source languages that have high contact with the target language. We also construct a novel benchmark dataset for close contact Chinese-Japanese-Korean-Vietnamese (CJKV) languages to further encourage in-depth studies of language contact. To comprehensively capture contact between these languages, we propose to integrate Romanized transcription beyond textual scripts via Contrastive Learning objectives, leading to enhanced cross-lingual representations and effective zero-shot cross-lingual transfer.</abstract>
      <url hash="da56deac">2024.lrec-main.356</url>
      <bibkey>nguyen-etal-2024-cori-cjkv</bibkey>
    </paper>
    <paper id="357">
      <title>Corpus Creation and Automatic Alignment of Historical <fixed-case>D</fixed-case>utch Dialect Speech</title>
      <author><first>Martijn</first><last>Bentum</last></author>
      <author><first>Eric</first><last>Sanders</last></author>
      <author><first>Antal P.J.</first><last>van den Bosch</last></author>
      <author><first>Douwe</first><last>Zeldenrust</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <pages>4021–4029</pages>
      <abstract>The Dutch Dialect Database (also known as the ‘Nederlandse Dialectenbank’) contains dialectal variations of Dutch that were recorded all over the Netherlands in the second half of the twentieth century. A subset of these recordings of about 300 hours were enriched with manual orthographic transcriptions, using non-standard approximations of dialectal speech. In this paper we describe the creation of a corpus containing both the audio recordings and their corresponding transcriptions and focus on our method for aligning the recordings with the transcriptions and the metadata.</abstract>
      <url hash="6de1d284">2024.lrec-main.357</url>
      <bibkey>bentum-etal-2024-corpus-creation</bibkey>
    </paper>
    <paper id="358">
      <title>Corpus Services: A Framework to Curate <fixed-case>XML</fixed-case> Corpus Data</title>
      <author><first>Aleksandr</first><last>Riaposov</last></author>
      <author><first>Elena</first><last>Lazarenko</last></author>
      <pages>4030–4035</pages>
      <abstract>This paper provides a comprehensive description of the Corpus Services framework—a collection of Java validation tools for language corpora compiled in XML-based data formats, in particular those using EXMARaLDA corpus software. Having successfully found application in several research projects, the core functionality of the framework is currently integrated in the automated curation and publication workflows for EXMARaLDA-driven corpora of Northern Eurasian languages, as developed by the long-term project INEL. Preliminary stages of development and examples of practical use cases are covered, a structured explanation of the framework’s current functionality and operational mechanisms is provided. Furthermore, the utilization of Corpus Services is extensively illustrated within the context of INEL workflows.</abstract>
      <url hash="ba8724ba">2024.lrec-main.358</url>
      <bibkey>riaposov-lazarenko-2024-corpus-services</bibkey>
    </paper>
    <paper id="359">
      <title>Correcting Language Model Bias for Text Classification in True Zero-Shot Learning</title>
      <author><first>Feng</first><last>Zhao</last></author>
      <author><first>Wan</first><last>Xianlin</last></author>
      <author><first>Cheng</first><last>Yan</last></author>
      <author><first>Chu Kiong</first><last>Loo</last></author>
      <pages>4036–4046</pages>
      <abstract>Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art results, depending on the quality of the manual templates. In this paper, we show that this instability stems from the fact that language models tend toward predicting certain label words of text classification, and manual templates can influence this tendency. To address this, we develop a novel pipeline for annotating and filtering a few examples from unlabeled examples. Moreover, we propose a new method to measure model bias on label words that utilizes unlabeled examples as a validation set when tuning language models. Our approach does not require any pre-labeled examples. Experimental results on six text classification tasks demonstrate that the proposed approach significantly outperforms standard prompt learning in zero-shot settings, achieving up to 19.7% absolute improvement and 13.8% average improvement. More surprisingly, on IMDB and SST-2, our approach even exceeds all few-shot baselines.</abstract>
      <url hash="32bb5b79">2024.lrec-main.359</url>
      <bibkey>zhao-etal-2024-correcting-language</bibkey>
    </paper>
    <paper id="360">
      <title>Correcting Pronoun Homophones with Subtle Semantics in <fixed-case>C</fixed-case>hinese Speech Recognition</title>
      <author><first>Zhaobo</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Gan</last></author>
      <author><first>Pingpeng</first><last>Yuan</last></author>
      <author><first>Hai</first><last>Jin</last></author>
      <pages>4047–4058</pages>
      <abstract>Speech recognition is becoming prevalent in daily life. However, due to the similar semantic context of the entities and the overlap of Chinese pronunciation, the pronoun homophone, especially “他/她/它 (he/she/it)”, (their pronunciation is “Tā”) is usually recognized incorrectly. It poses a challenge to automatically correct them during the post-processing of Chinese speech recognition. In this paper, we propose three models to address the common confusion issues in this domain, tailored to various application scenarios. We implement the language model, the LSTM model with semantic features, and the rule-based assisted Ngram model, enabling our models to adapt to a wide range of requirements, from high-precision to low-resource offline devices. The extensive experiments show that our models achieve the highest recognition rate for “Tā” correction with improvements from 70% in the popular voice input methods up to 90%. Further ablation analysis underscores the effectiveness of our models in enhancing recognition accuracy. Therefore, our models improve the overall experience of Chinese speech recognition of “Tā” and reduce the burden of manual transcription corrections.</abstract>
      <url hash="2668abca">2024.lrec-main.360</url>
      <bibkey>zhang-etal-2024-correcting-pronoun</bibkey>
    </paper>
    <paper id="361">
      <title>Correlations between Multilingual Language Model Geometry and Crosslingual Transfer Performance</title>
      <author><first>Cheril</first><last>Shah</last></author>
      <author><first>Yashashree</first><last>Chandak</last></author>
      <author><first>Atharv Mahesh</first><last>Mane</last></author>
      <author><first>Benjamin</first><last>Bergen</last></author>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <pages>4059–4066</pages>
      <abstract>A common approach to interpreting multilingual language models is to evaluate their internal representations. For example, studies have found that languages occupy distinct subspaces in the models’ representation spaces, and geometric distances between languages often reflect linguistic properties such as language families and typological features. In our work, we investigate whether geometric distances between language representations correlate with zero-shot crosslingual transfer performance for POS-tagging and NER in three multilingual language models. We consider four distance metrics, including new metrics that identify a basis for a multilingual representation space that sorts axes based on their language-separability. We find that each distance metric either only moderately correlates or does not correlate with crosslingual transfer performance, and metrics do not generalize well across models, layers, and tasks. Although pairwise language separability is a reasonable predictor of crosslingual transfer, representational geometry overall is an inconsistent predictor for the crosslingual performance of multilingual language models.</abstract>
      <url hash="dc720b54">2024.lrec-main.361</url>
      <bibkey>shah-etal-2024-correlations-multilingual</bibkey>
    </paper>
    <paper id="362">
      <title>Cost-Effective Discourse Annotation in the <fixed-case>P</fixed-case>rague <fixed-case>C</fixed-case>zech–<fixed-case>E</fixed-case>nglish <fixed-case>D</fixed-case>ependency <fixed-case>T</fixed-case>reebank</title>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Pavlína</first><last>Synková</last></author>
      <author><first>Lucie</first><last>Polakova</last></author>
      <author><first>Marie</first><last>Paclíková</last></author>
      <pages>4067–4077</pages>
      <abstract>We present a cost-effective method for obtaining a high-quality annotation of explicit discourse relations in the Czech part of the Prague Czech–English Dependency Treebank, a corpus of almost 50 thousand sentences coming from the Czech translation of the Wall Street Journal part of the Penn Treebank. We use three different sources of information and combine them to obtain the discourse annotation: (i) annotation projection from the Penn Discourse Treebank 3.0, (ii) manual tectogrammatical (deep syntax) representation of sentences of the corpus, and (iii) the Lexicon of Czech Discourse Connectives CzeDLex. After solving as many discrepancies as possible automatically, the final discourse annotation is achieved by manual inspection of the remaining problematic cases. The discourse annotation of the corpus will be available both in the Prague format (on top of tectogrammatical trees) with the Prague taxonomy of discourse types, and in the Penn format (on plain texts) with the Penn Discourse Treebank 3.0 sense taxonomy.</abstract>
      <url hash="df46c014">2024.lrec-main.362</url>
      <bibkey>mirovsky-etal-2024-cost-effective</bibkey>
    </paper>
    <paper id="363">
      <title>Counterfactual Dialog Mixing as Data Augmentation for Task-Oriented Dialog Systems</title>
      <author><first>Sebastian</first><last>Steindl</last></author>
      <author><first>Ulrich</first><last>Schäfer</last></author>
      <author><first>Bernd</first><last>Ludwig</last></author>
      <pages>4078–4087</pages>
      <abstract>High-quality training data for Task-Oriented Dialog (TOD) systems is costly to come by if no corpora are available. One method to extend available data is data augmentation. Yet, the research into and adaptation of data augmentation techniques for TOD systems is limited in comparison with other data modalities. We propose a novel, causally-flavored data augmentation technique called Counterfactual Dialog Mixing (CDM) that generates realistic synthetic dialogs via counterfactuals to increase the amount of training data. We demonstrate the method on a benchmark dataset and show that a model trained to classify the counterfactuals from the original data fails to do so, which strengthens the claim of creating realistic synthetic dialogs. To evaluate the effectiveness of CDM, we train a current architecture on a benchmark dataset and compare the performance with and without CDM. By doing so, we achieve state-of-the-art on some metrics. We further investigate the external generalizability and a lower resource setting. To evaluate the models, we adopted an interactive evaluation scheme.</abstract>
      <url hash="3b51b466">2024.lrec-main.363</url>
      <bibkey>steindl-etal-2024-counterfactual-dialog</bibkey>
    </paper>
    <paper id="364">
      <title>Creating Terminological Resources in the Digital Age for Less-resourced Languages</title>
      <author><first>Mercè</first><last>Vàzquez</last></author>
      <pages>4088–4091</pages>
      <abstract>Multilingual terminological resources contain the most representative knowledge of specialized domains and allow professionals to create and translate specialized content in order to spread knowledge. Today, representative and useful multilingual terminological resources are available for the most resourced languages. This reduces or limits the development of knowledge in less-resourced languages across different specialized domains, mainly those that are constantly evolving and creating or adapting new concepts as needed. In this paper we present our methodology for carrying out terminological projects in Catalan, based entirely on open access linguistic resources and using natural language processing tools. The main objective of this research is to maximize the Catalan terminology currently available in open access, using a combination of natural language processing tools. The results are supervised by linguists and terminologist experts before being publicly available to the public. The findings of our research provide a new approach to terminology work, making it possible to design high-volume multilingual terminological projects that are manually revised by linguists and terminologists in the context of less-resourced languages.</abstract>
      <url hash="e5c4c3d6">2024.lrec-main.364</url>
      <bibkey>vazquez-2024-creating-terminological</bibkey>
    </paper>
    <paper id="365">
      <title>Creation and Analysis of an International Corpus of Privacy Laws</title>
      <author><first>Sonu</first><last>Gupta</last></author>
      <author><first>Geetika</first><last>Gopi</last></author>
      <author><first>Harish</first><last>Balaji</last></author>
      <author><first>Ellen</first><last>Poplavska</last></author>
      <author><first>Nora</first><last>O’Toole</last></author>
      <author><first>Siddhant</first><last>Arora</last></author>
      <author><first>Thomas</first><last>Norton</last></author>
      <author><first>Norman</first><last>Sadeh</last></author>
      <author><first>Shomir</first><last>Wilson</last></author>
      <pages>4092–4105</pages>
      <abstract>The landscape of privacy laws and regulations around the world is complex and ever-changing. National and super-national laws, agreements, decrees, and other government-issued rules form a patchwork that companies must follow to operate internationally. To examine the status and evolution of this patchwork, we introduce the Privacy Law Corpus, of 1,043 privacy laws, regulations, and guidelines, covering 183 jurisdictions. This corpus enables a large-scale quantitative and qualitative examination of legal focus on privacy. We examine the temporal distribution of when privacy laws were created and illustrate the dramatic increase in privacy legislation over the past 50 years, although a finer-grained examination reveals that the rate of increase varies depending on the personal data types that privacy laws address. Our exploration also demonstrates that most privacy laws respectively address relatively few personal data types. Additionally, topic modeling results show the prevalence of common themes in privacy laws, such as finance, healthcare, and telecommunications. Finally, we release the corpus to the research community to promote further study.</abstract>
      <url hash="9e260448">2024.lrec-main.365</url>
      <bibkey>gupta-etal-2024-creation-analysis</bibkey>
    </paper>
    <paper id="366">
      <title><fixed-case>C</fixed-case>roatian Idioms Integration: Enhancing the <fixed-case>LI</fixed-case>dioms Multilingual Linked Idioms Dataset</title>
      <author><first>Ivana</first><last>Filipović Petrović</last></author>
      <author><first>Miguel</first><last>López Otal</last></author>
      <author><first>Slobodan</first><last>Beliga</last></author>
      <pages>4106–4112</pages>
      <abstract>Idioms, also referred to as phraseological units in some language terminologies, are a subset within the broader category of multi-word expressions. However, there is a lack of representation of idioms in Croatian, a low-resourced language, in the Linguistic Linked Open Data cloud (LLOD). To address this gap, we propose an extension of an existing RDF-based multilingual representation of idioms, referred to as the LIdioms dataset, which currently includes idioms from English, German, Italian, Portuguese, and Russian. This paper expands the existing resource by incorporating 1,042 Croatian idioms in an Ontolex Lemon format. In addition, to foster translation initiatives and facilitate intercultural exchange, these added Croatian idioms have also been linked to other idioms of the LIdioms dataset, with which they share similar meanings despite their differences in the expression aspect. This addition enriches the knowledge base of the LLOD community with a new language resource that includes Croatian idioms.</abstract>
      <url hash="a86cb63f">2024.lrec-main.366</url>
      <bibkey>filipovic-petrovic-etal-2024-croatian-idioms</bibkey>
    </paper>
    <paper id="367">
      <title><fixed-case>C</fixed-case>ro<fixed-case>C</fixed-case>o<fixed-case>S</fixed-case>um: A Benchmark Dataset for Cross-Lingual Code-Switched Summarization</title>
      <author><first>Ruochen</first><last>Zhang</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>4113–4126</pages>
      <abstract>Cross-lingual summarization (CLS) has attracted increasing interest in recent years due to the availability of large-scale web-mined datasets and the advancements of multilingual language models. However, given the rareness of naturally occurring CLS resources, the majority of datasets are forced to rely on translation which can contain overly literal artifacts. This restricts our ability to observe naturally occurring CLS pairs that capture organic diction, including instances of code-switching. This alteration between languages in mid-message is a common phenomenon in multilingual settings yet has been largely overlooked in cross-lingual contexts due to data scarcity. To address this gap, we introduce CroCoSum, a dataset of cross-lingual code-switched summarization of technology news. It consists of over 24,000 English source articles and 18,000 human-written Chinese news summaries, with more than 92% of the summaries containing code-switched phrases. For reference, we evaluate the performance of existing approaches including pipeline, end-to-end, and zero-shot methods. We show that leveraging existing CLS resources as a pretraining step does not improve performance on CroCoSum, indicating the limited generalizability of current datasets. Finally, we discuss the challenges of evaluating cross-lingual summarizers on code-switched generation through qualitative error analyses.</abstract>
      <url hash="11910575">2024.lrec-main.367</url>
      <bibkey>zhang-eickhoff-2024-crocosum-benchmark</bibkey>
    </paper>
    <paper id="368">
      <title>Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in <fixed-case>T</fixed-case>urkish</title>
      <author><first>Recep Firat</first><last>Cekinel</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <author><first>Pinar</first><last>Karagoz</last></author>
      <pages>4127–4142</pages>
      <abstract>The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.</abstract>
      <url hash="1d960c6b">2024.lrec-main.368</url>
      <bibkey>cekinel-etal-2024-cross-lingual</bibkey>
    </paper>
    <paper id="369">
      <title>Cross-lingual Named Entity Corpus for <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Michał</first><last>Marcińczuk</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>4143–4157</pages>
      <abstract>This paper presents a corpus manually annotated with named entities for six Slavic languages — Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017–2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5,017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits — single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models — XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.</abstract>
      <url hash="cf27f09a">2024.lrec-main.369</url>
      <bibkey>piskorski-etal-2024-cross-lingual</bibkey>
    </paper>
    <paper id="370">
      <title>Cross-Lingual <fixed-case>NLU</fixed-case>: Mitigating Language-Specific Impact in Embeddings Leveraging Adversarial Learning</title>
      <author><first>Saedeh</first><last>Tahery</last></author>
      <author><first>Sahar</first><last>Kianian</last></author>
      <author><first>Saeed</first><last>Farzi</last></author>
      <pages>4158–4163</pages>
      <abstract>Low-resource languages and computational expenses pose significant challenges in the domain of large language models (LLMs). Currently, researchers are actively involved in various efforts to tackle these challenges. Cross-lingual natural language processing (NLP) remains one of the most promising strategies to address these issues. In this paper, we introduce a novel approach that utilizes adversarial techniques to mitigate the impact of language-specific information in contextual embeddings generated by large multilingual language models, with potential applications in cross-lingual tasks. The study encompasses five different languages, including both Latin and non-Latin ones, in the context of two fundamental tasks in natural language understanding: intent detection and slot filling. The results primarily show that our current approach excels in zero-shot scenarios for Latin languages like Spanish. However, it encounters limitations when applied to languages distant from English, such as Thai and Persian. This highlights that while our approach effectively reduces the effect of language-specific information on the core meaning, it performs better for Latin languages that share language-specific nuances with English, as certain characteristics persist in the overall meaning within embeddings.</abstract>
      <url hash="f95b8019">2024.lrec-main.370</url>
      <bibkey>tahery-etal-2024-cross-lingual</bibkey>
    </paper>
    <paper id="371">
      <title>Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity</title>
      <author><first>Sho</first><last>Hoshino</last></author>
      <author><first>Akihiko</first><last>Kato</last></author>
      <author><first>Soichiro</first><last>Murakami</last></author>
      <author><first>Peinan</first><last>Zhang</last></author>
      <pages>4164–4173</pages>
      <abstract>Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind. In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS: - (a): _cross-lingual transfer_ that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and - (b) _machine translation_ that coverts English data into pseudo non-English training data in advance. In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par. In addition, we find a superiority of Wikipedia domain over NLI domain as unlabeled training data for these languages. Combining our findings, we further demonstrate that the cross-lingual transfer of Wikipedia data exhibits improved performance.</abstract>
      <url hash="d735367b">2024.lrec-main.371</url>
      <bibkey>hoshino-etal-2024-cross-lingual</bibkey>
    </paper>
    <paper id="372">
      <title>Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets</title>
      <author><first>Shadi</first><last>Manafi</last></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <pages>4174–4184</pages>
      <abstract>Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs—MBERT and XLM-R—on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger. Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL. Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages.</abstract>
      <url hash="93d2291b">2024.lrec-main.372</url>
      <bibkey>manafi-krishnaswamy-2024-cross-lingual</bibkey>
    </paper>
    <paper id="373">
      <title><fixed-case>C</fixed-case>ross<fixed-case>T</fixed-case>une: Black-Box Few-Shot Classification with Label Enhancement</title>
      <author><first>Danqing</first><last>Luo</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>4185–4197</pages>
      <abstract>Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning. A switch mechanism is implemented to exclude low-quality ChatGPT-generated data. Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average. Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach.</abstract>
      <url hash="bf9002a5">2024.lrec-main.373</url>
      <bibkey>luo-etal-2024-crosstune-black</bibkey>
    </paper>
    <paper id="374">
      <title>Cross-type <fixed-case>F</fixed-case>rench Multiword Expression Identification with Pre-trained Masked Language Models</title>
      <author><first>Van-Tuan</first><last>Bui</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <pages>4198–4204</pages>
      <abstract>Multiword expressions (MWEs) pose difficulties for natural language processing (NLP) due to their linguistic features, such as syntactic and semantic properties, which distinguish them from regular word groupings. This paper describes a combination of two systems: one that learns verbal multiword expressions (VMWEs) and another that learns non-verbal MWEs (nVMWEs). Together, these systems leverage training data from both types of MWEs to enhance performance on a cross-type dataset containing both VMWEs and nVMWEs. Such scenarios emerge when datasets are developed using differing annotation schemes. We explore the fine-tuning of several state-of-the-art neural transformers for each MWE type. Our experiments demonstrate the advantages of the combined system over multi-task approaches or single-task models, addressing the challenges posed by diverse tagsets within the training data. Specifically, we evaluated the combined system on a French treebank named Sequoia, which features an annotation layer encompassing all syntactic types of French MWEs. With this combined approach, we improved the F1-score by approximately 3% on the Sequoia dataset.</abstract>
      <url hash="e36bea80">2024.lrec-main.374</url>
      <bibkey>bui-savary-2024-cross-type</bibkey>
    </paper>
    <paper id="375">
      <title><fixed-case>CSSW</fixed-case>iki: A <fixed-case>C</fixed-case>hinese Sentence Simplification Dataset with Linguistic and Content Operations</title>
      <author><first>Fengkai</first><last>Liu</last></author>
      <author><first>John S. Y.</first><last>Lee</last></author>
      <pages>4205–4213</pages>
      <abstract>Sentence Simplification aims to make sentences easier to read and understand. With most effort on corpus development focused on English, the amount of annotated data is limited in Chinese. To address this need, we introduce CSSWiki, an open-source dataset for Chinese sentence simplification based on Wikipedia. This dataset contains 1.6k source sentences paired with their simplified versions. Each sentence pair is annotated with operation tags that distinguish between linguistic and content modifications. We analyze differences in annotation scheme and data statistics between CSSWiki and existing datasets. We then report baseline sentence simplification performance on CSSWiki using zero-shot and few-shot approaches with Large Language Models.</abstract>
      <url hash="4696f7c5">2024.lrec-main.375</url>
      <bibkey>liu-lee-2024-csswiki-chinese</bibkey>
    </paper>
    <paper id="376">
      <title><fixed-case>CTSM</fixed-case>: Combining Trait and State Emotions for Empathetic Response Model</title>
      <author><first>Yufeng</first><last>Wang</last></author>
      <author><first>Chao</first><last>Chen</last></author>
      <author><first>Zhou</first><last>Yang</last></author>
      <author><first>Shuhui</first><last>Wang</last></author>
      <author><first>Xiangwen</first><last>Liao</last></author>
      <pages>4214–4225</pages>
      <abstract>Empathetic response generation endeavors to empower dialogue systems to perceive speakers’ emotions and generate empathetic responses accordingly. Psychological research demonstrates that emotion, as an essential factor in empathy, encompasses trait emotions, which are static and context-independent, and state emotions, which are dynamic and context-dependent. However, previous studies treat them in isolation, leading to insufficient emotional perception of the context, and subsequently, less effective empathetic expression. To address this problem, we propose Combining Trait and State emotions for Empathetic Response Model (CTSM). Specifically, to sufficiently perceive emotions in dialogue, we first construct and encode trait and state emotion embeddings, and then we further enhance emotional perception capability through an emotion guidance module that guides emotion representation. In addition, we propose a cross-contrastive learning decoder to enhance the model’s empathetic expression capability by aligning trait and state emotions between generated responses and contexts. Both automatic and manual evaluation results demonstrate that CTSM outperforms state-of-the-art baselines and can generate more empathetic responses. Our code is available at https://github.com/wangyufeng-empty/CTSM</abstract>
      <url hash="02f850c8">2024.lrec-main.376</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f955fc71">2024.lrec-main.376.OptionalSupplementaryMaterial.rar</attachment>
      <bibkey>wang-etal-2024-ctsm-combining</bibkey>
    </paper>
    <paper id="377">
      <title><fixed-case>C</fixed-case>ultura<fixed-case>X</fixed-case>: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages</title>
      <author><first>Thuat</first><last>Nguyen</last></author>
      <author><first>Chien Van</first><last>Nguyen</last></author>
      <author><first>Viet Dac</first><last>Lai</last></author>
      <author><first>Hieu</first><last>Man</last></author>
      <author><first>Nghia Trung</first><last>Ngo</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Ryan A.</first><last>Rossi</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>4226–4237</pages>
      <abstract>Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.</abstract>
      <url hash="c14dd8e5">2024.lrec-main.377</url>
      <bibkey>nguyen-etal-2024-culturax-cleaned</bibkey>
    </paper>
    <paper id="378">
      <title>Curation of Benchmark Templates for Measuring Gender Bias in Named Entity Recognition Models</title>
      <author><first>Ana</first><last>Cimitan</last></author>
      <author><first>Ana</first><last>Alves Pinto</last></author>
      <author><first>Michaela</first><last>Geierhos</last></author>
      <pages>4238–4246</pages>
      <abstract>Named Entity Recognition (NER) constitutes a popular machine learning technique that empowers several natural language processing applications. As with other machine learning applications, NER models have been shown to be susceptible to gender bias. The latter is often assessed using benchmark datasets, which in turn are curated specifically for a given Natural Language Processing (NLP) task. In this work, we investigate the robustness of benchmark templates to detect gender bias and propose a novel method to improve the curation of such datasets. The method, based on masked token prediction, aims to filter out benchmark templates with a higher probability of detecting gender bias in NER models. We tested the method for English and German, using the corresponding fine-tuned BERT base model (cased) as the NER model. The gender gaps detected with templates classified as appropriate by the method were statistically larger than those detected with inappropriate templates. The results were similar for both languages and support the use of the proposed method in the curation of templates designed to detect gender bias.</abstract>
      <url hash="f15d8130">2024.lrec-main.378</url>
      <bibkey>cimitan-etal-2024-curation-benchmark</bibkey>
    </paper>
    <paper id="379">
      <title><fixed-case>C</fixed-case>u<fixed-case>RIAM</fixed-case>: Corpus Re Interpretation and Metalanguage in <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Opinions</title>
      <author><first>Michael</first><last>Kranzlein</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Kevin</first><last>Tobia</last></author>
      <pages>4247–4258</pages>
      <abstract>Most judicial decisions involve the interpretation of legal texts. As such, judicial opinions use language as the medium to comment on or draw attention to other language (for example, through definitions and hypotheticals about the meaning of a term from a statute). Language used this way is called metalanguage. Focusing on the U.S. Supreme Court, we view metalanguage as reflective of justices’ interpretive processes, bearing on current debates and theories about textualism in law and political science. As a step towards large-scale metalinguistic analysis with NLP, we identify 9 categories prominent in metalinguistic discussions, including key terms, definitions, and different kinds of sources. We annotate these concepts in a corpus of U.S. Supreme Court opinions. Our analysis of the corpus reveals high interannotator agreement, frequent use of quotes and sources, and several notable frequency differences between majority, concurring, and dissenting opinions. We observe fewer instances than expected of several legal interpretive categories. We discuss some of the challenges in developing the annotation schema and applying it and provide recommendations for how this corpus can be used for broader analyses.</abstract>
      <url hash="f5d85646">2024.lrec-main.379</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a37163e4">2024.lrec-main.379.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kranzlein-etal-2024-curiam-corpus</bibkey>
    </paper>
    <paper id="380">
      <title>Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition</title>
      <author><first>Cam-Van Thi</first><last>Nguyen</last></author>
      <author><first>Cao-Bach</first><last>Nguyen</last></author>
      <author><first>Duc-Trong</first><last>Le</last></author>
      <author><first>Quang-Thuy</first><last>Ha</last></author>
      <pages>4259–4265</pages>
      <abstract>Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model’s performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models. We release the code for and experiments: <url>https://github.com/vanntc711/MultiDAG-CL</url>.</abstract>
      <url hash="5c30c81e">2024.lrec-main.380</url>
      <bibkey>nguyen-etal-2024-curriculum-learning</bibkey>
    </paper>
    <paper id="381">
      <title><fixed-case>C</fixed-case>u<fixed-case>SIN</fixed-case>e<fixed-case>S</fixed-case>: Curriculum-driven Structure Induced Negative Sampling for Statutory Article Retrieval</title>
      <author><first>Santosh</first><last>T.y.s.s.</last></author>
      <author><first>Kristina</first><last>Kaiser</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>4266–4272</pages>
      <abstract>In this paper, we introduce CuSINeS, a negative sampling approach to enhance the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key contributions. Firstly, it employs a curriculum-based negative sampling strategy guiding the model to focus on easier negatives initially and progressively tackle more difficult ones. Secondly, it leverages the hierarchical and sequential information derived from the structural organization of statutes to evaluate the difficulty of samples. Lastly, it introduces a dynamic semantic difficulty assessment using the being-trained model itself, surpassing conventional static methods like BM25, adapting the negatives to the model’s evolving competence. Experimental results on a real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS across four different baselines, demonstrating its versatility.</abstract>
      <url hash="99f4ed0c">2024.lrec-main.381</url>
      <bibkey>t-y-s-s-etal-2024-cusines-curriculum</bibkey>
    </paper>
    <paper id="382">
      <title><fixed-case>CWTM</fixed-case>: Leveraging Contextualized Word Embeddings from <fixed-case>BERT</fixed-case> for Neural Topic Modeling</title>
      <author><first>Zheng</first><last>Fang</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <author><first>Rob</first><last>Procter</last></author>
      <pages>4273–4286</pages>
      <abstract>Most existing topic models rely on bag-of-words (BOW) representation, which limits their ability to capture word order information and leads to challenges with out-of-vocabulary (OOV) words in new documents. Contextualized word embeddings, however, show superiority in word sense disambiguation and effectively address the OOV issue. In this work, we introduce a novel neural topic model called the Contextlized Word Topic Model (CWTM), which integrates contextualized word embeddings from BERT. The model is capable of learning the topic vector of a document without BOW information. In addition, it can also derive the topic vectors for individual words within a document based on their contextualized word embeddings. Experiments across various datasets show that CWTM generates more coherent and meaningful topics compared to existing topic models, while also accommodating unseen words in newly encountered documents.</abstract>
      <url hash="7d02ae69">2024.lrec-main.382</url>
      <bibkey>fang-etal-2024-cwtm-leveraging</bibkey>
    </paper>
    <paper id="383">
      <title><fixed-case>C</fixed-case>yber<fixed-case>A</fixed-case>gression<fixed-case>A</fixed-case>do-v2: Leveraging Pragmatic-Level Information to Decipher Online Hate in <fixed-case>F</fixed-case>rench Multiparty Chats</title>
      <author><first>Anais</first><last>Ollagnier</last></author>
      <pages>4287–4298</pages>
      <abstract>As a part of the release of the <i>CyberAgressionAdo-V2</i> dataset, this paper introduces a new tagset that includes tags marking pragmatic-level information occurring in cyberbullying situations. The previous version of this dataset, <i>CyberAgressionAdo-V1</i>, consists of aggressive multiparty chats in French annotated using a hierarchical tagset developed to describe bullying narrative events including the participant roles, the presence of hate speech, the type of verbal abuse, among others. In contrast, <i>CyberAgressionAdo-V2</i> uses a multi-label, fine-grained tagset marking the discursive role of exchanged messages as well as the context in which they occur — for instance, attack (ATK), defend (DFN), counterspeech (CNS), abet/instigate (AIN), gaslight (GSL), etc. This paper provides a comprehensive overview of the annotation tagset and presents statistical insights derived from its application. Additionally, we address the challenges encountered when annotating pragmatic-level information in this context, conducting a thorough analysis of annotator disagreements. The resulting dataset comprises 19 conversations that have been manually annotated and is now available to facilitate further research in the field.</abstract>
      <url hash="3c87b78c">2024.lrec-main.383</url>
      <bibkey>ollagnier-2024-cyberagressionado-v2</bibkey>
    </paper>
    <paper id="384">
      <title><fixed-case>C</fixed-case>zech Dataset for Complex Aspect-Based Sentiment Analysis Tasks</title>
      <author><first>Jakub</first><last>Šmíd</last></author>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Ondrej</first><last>Prazak</last></author>
      <author><first>Pavel</first><last>Kral</last></author>
      <pages>4299–4310</pages>
      <abstract>In this paper, we introduce a novel Czech dataset for aspect-based sentiment analysis (ABSA), which consists of 3.1K manually annotated reviews from the restaurant domain. The dataset is built upon the older Czech dataset, which contained only separate labels for the basic ABSA tasks such as aspect term extraction or aspect polarity detection. Unlike its predecessor, our new dataset is specifically designed to allow its usage for more complex tasks, e.g. target-aspect-category detection. These advanced tasks require a unified annotation format, seamlessly linking sentiment elements (labels) together. Our dataset follows the format of the well-known SemEval-2016 datasets. This design choice allows effortless application and evaluation in cross-lingual scenarios, ultimately fostering cross-language comparisons with equivalent counterpart datasets in other languages. The annotation process engaged two trained annotators, yielding an impressive inter-annotator agreement rate of approximately 90%. Additionally, we provide 24M reviews without annotations suitable for unsupervised learning. We present robust monolingual baseline results achieved with various Transformer-based models and insightful error analysis to supplement our contributions. Our code and dataset are freely available for non-commercial research purposes.</abstract>
      <url hash="16303c6e">2024.lrec-main.384</url>
      <bibkey>smid-etal-2024-czech-dataset</bibkey>
    </paper>
    <paper id="385">
      <title><fixed-case>DACL</fixed-case>: Disfluency Augmented Curriculum Learning for Fluent Text Generation</title>
      <author><first>Rohan</first><last>Chaudhury</last></author>
      <author><first>Maria</first><last>Teleki</last></author>
      <author><first>Xiangjue</first><last>Dong</last></author>
      <author><first>James</first><last>Caverlee</last></author>
      <pages>4311–4321</pages>
      <abstract>Voice-driven software systems are in abundance. However, language models that power these systems are traditionally trained on fluent, written text corpora. Hence there can be a misalignment between the inherent disfluency of transcribed spoken content and the fluency of the written training data. Furthermore, gold-standard disfluency annotations of various complexities for incremental training can be expensive to collect. So, we propose in this paper a Disfluency Augmented Curriculum Learning (DACL) approach to tackle the complex structure of disfluent sentences and generate fluent texts from them, by using Curriculum Learning (CL) coupled with our synthetically augmented disfluent texts of various levels. DACL harnesses the tiered structure of our generated synthetic disfluent data using CL, by training the model on basic samples (i.e. more fluent) first before training it on more complex samples (i.e. more disfluent). In contrast to the random data exposure paradigm, DACL focuses on a simple-to-complex learning process. We comprehensively evaluate DACL on Switchboard Penn Treebank-3 and compare it to the state-of-the-art disfluency removal models. Our model surpasses existing techniques in word-based precision (by up to 1%) and has shown favorable recall and F1 scores.</abstract>
      <url hash="eb250197">2024.lrec-main.385</url>
      <bibkey>chaudhury-etal-2024-dacl-disfluency</bibkey>
    </paper>
    <paper id="386">
      <title><fixed-case>DADIT</fixed-case>: A Dataset for Demographic Classification of <fixed-case>I</fixed-case>talian <fixed-case>T</fixed-case>witter Users and a Comparison of Prediction Methods</title>
      <author><first>Lorenzo</first><last>Lupo</last></author>
      <author><first>Paul</first><last>Bose</last></author>
      <author><first>Mahyar</first><last>Habibi</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Carlo</first><last>Schwarz</last></author>
      <pages>4322–4332</pages>
      <abstract>Social scientists increasingly use demographically stratified social media data to study the attitudes, beliefs, and behavior of the general public. To facilitate such analyses, we construct, validate, and release publicly the representative DADIT dataset of 30M tweets of 20k Italian Twitter users, along with their bios and profile pictures. We enrich the user data with high-quality labels for gender, age, and location. DADIT enables us to train and compare the performance of various state-of-the-art models for the prediction of the gender and age of social media users. In particular, we investigate if tweets contain valuable information for the task, since popular classifiers like M3 don’t leverage them. Our best XLM-based classifier improves upon the commonly used competitor M3 by up to 53% F1. Especially for age prediction, classifiers profit from including tweets as features. We also confirm these findings on a German test set.</abstract>
      <url hash="d59f8060">2024.lrec-main.386</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e8d4d700">2024.lrec-main.386.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lupo-etal-2024-dadit-dataset</bibkey>
    </paper>
    <paper id="387">
      <title><fixed-case>DANCER</fixed-case>: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition</title>
      <author><first>Yi-Cheng</first><last>Wang</last></author>
      <author><first>Hsin-Wei</first><last>Wang</last></author>
      <author><first>Bi-Cheng</first><last>Yan</last></author>
      <author><first>Chi-Han</first><last>Lin</last></author>
      <author><first>Berlin</first><last>Chen</last></author>
      <pages>4333–4342</pages>
      <abstract>End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on pho-netic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic con-fusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task. A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach. DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities. More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities. The code is available at https://github.com/Amiannn/Dancer.</abstract>
      <url hash="031c73bb">2024.lrec-main.387</url>
      <bibkey>wang-etal-2024-dancer-entity</bibkey>
    </paper>
    <paper id="388">
      <title><fixed-case>D</fixed-case>ante<fixed-case>LLM</fixed-case>: Let’s Push <fixed-case>I</fixed-case>talian <fixed-case>LLM</fixed-case> Research Forward!</title>
      <author><first>Andrea</first><last>Bacciu</last></author>
      <author><first>Cesare</first><last>Campagnano</last></author>
      <author><first>Giovanni</first><last>Trappolini</last></author>
      <author><first>Fabrizio</first><last>Silvestri</last></author>
      <pages>4343–4355</pages>
      <abstract>In recent years, the dominance of Large Language Models (LLMs) in the English language has become evident. However, there remains a pronounced gap in resources and evaluation tools tailored for non-English languages, underscoring a significant disparity in the global AI landscape. This paper seeks to bridge this gap, specifically focusing on the Italian linguistic context. We introduce a novel benchmark, and an open LLM Leaderboard, designed to evaluate LLMs’ performance in Italian, providing a rigorous framework for comparative analysis. In our assessment of currently available models, we highlight their respective strengths and limitations against this standard. Crucially, we propose “DanteLLM”, a state-of-the-art LLM dedicated to Italian. Our empirical evaluations underscore Dante’s superiority, as it emerges as the most performant model on our benchmark, with improvements by up to 6 points. This research not only marks a significant stride in Italian-centric natural language processing but also offers a blueprint for the development and evaluation of LLMs in other languages, championing a more inclusive AI paradigm. Our code at: https://github.com/RSTLess-research/DanteLLM</abstract>
      <url hash="e96bf2e9">2024.lrec-main.388</url>
      <bibkey>bacciu-etal-2024-dantellm-lets</bibkey>
    </paper>
    <paper id="389">
      <title><fixed-case>DARIUS</fixed-case>: A Comprehensive Learner Corpus for Argument Mining in <fixed-case>G</fixed-case>erman-Language Essays</title>
      <author><first>Nils-Jonathan</first><last>Schaller</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Lars Ingver</first><last>Höft</last></author>
      <author><first>Yuning</first><last>Ding</last></author>
      <author><first>Jan Luca</first><last>Bahr</last></author>
      <author><first>Jennifer</first><last>Meyer</last></author>
      <author><first>Thorben</first><last>Jansen</last></author>
      <pages>4356–4367</pages>
      <abstract>In this paper, we present the DARIUS (Digital Argumentation Instruction for Science) corpus for argumentation quality on 4589 essays written by 1839 German secondary school students. The corpus is annotated according to a fine-grained annotation scheme, ranging from a broader perspective like content zones, to more granular features like argumentation coverage/reach and argumentative discourse units like claims and warrants. The features have inter-annotator agreements up to 0.83 Krippendorff’s α. The corpus and dataset are publicly available for further research in argument mining.</abstract>
      <url hash="7fb86109">2024.lrec-main.389</url>
      <bibkey>schaller-etal-2024-darius-comprehensive</bibkey>
    </paper>
    <paper id="390">
      <title>Data Collection Pipeline for Low-Resource Languages: A Case Study on Constructing a Tetun Text Corpus</title>
      <author><first>Gabriel</first><last>de Jesus</last></author>
      <author><first>Sérgio Sobral</first><last>Nunes</last></author>
      <pages>4368–4380</pages>
      <abstract>This paper proposes Labadain Crawler, a data collection pipeline tailored to automate and optimize the process of constructing textual corpora from the web, with a specific target to low-resource languages. The system is built on top of Nutch, an open-source web crawler and data extraction framework, and incorporates language processing components such as a tokenizer and a language identification model. The pipeline efficacy is demonstrated through successful testing with Tetun, one of Timor-Leste’s official languages, resulting in the construction of a high-quality Tetun text corpus comprising 321.7k sentences extracted from over 22k web pages. The contributions of this paper include the development of a Tetun tokenizer, a Tetun language identification model, and a Tetun text corpus, marking an important milestone in Tetun text information retrieval.</abstract>
      <url hash="55181d98">2024.lrec-main.390</url>
      <bibkey>de-jesus-nunes-2024-data-collection</bibkey>
    </paper>
    <paper id="391">
      <title>Data Drift in Clinical Outcome Prediction from Admission Notes</title>
      <author><first>Paul</first><last>Grundmann</last></author>
      <author><first>Jens-Michalis</first><last>Papaioannou</last></author>
      <author><first>Tom</first><last>Oberhauser</last></author>
      <author><first>Thomas</first><last>Steffek</last></author>
      <author><first>Amy</first><last>Siu</last></author>
      <author><first>Wolfgang</first><last>Nejdl</last></author>
      <author><first>Alexander</first><last>Loeser</last></author>
      <pages>4381–4391</pages>
      <abstract>Clinical NLP research faces a scarcity of publicly available datasets due to privacy concerns. MIMIC-III marked a significant milestone, enabling substantial progress, and now, with MIMIC-IV, the dataset has expanded significantly, offering a broader scope. In this paper, we focus on the task of predicting clinical outcomes from clinical text. This is crucial in modern healthcare, aiding in preventive care, differential diagnosis, and capacity planning. We introduce a novel clinical outcome prediction dataset derived from MIMIC-IV. Furthermore, we provide initial insights into the performance of models trained on MIMIC-III when applied to our new dataset, with specific attention to potential data drift. We investigate challenges tied to evolving documentation standards and changing codes in the International Classification of Diseases (ICD) taxonomy, such as the transition from ICD-9 to ICD-10. We also explore variations in clinical text across different hospital wards. Our study aims to probe the robustness and generalization of clinical outcome prediction models, contributing to the ongoing advancement of clinical NLP in healthcare.</abstract>
      <url hash="63cf2442">2024.lrec-main.391</url>
      <bibkey>grundmann-etal-2024-data-drift</bibkey>
    </paper>
    <paper id="392">
      <title>Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks</title>
      <author><first>Ileana</first><last>Rugina</last></author>
      <author><first>Rumen</first><last>Dangovski</last></author>
      <author><first>Li</first><last>Jing</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Marin</first><last>Soljacic</last></author>
      <pages>4392–4403</pages>
      <abstract>Attention mechanisms play a crucial role in the neural revolution of Natural Language Processing (NLP). With the growth of attention-based models, several pruning techniques have been developed to identify and exploit sparseness, making these models more efficient. Most efforts focus on hard-coding attention patterns or pruning attention weights based on training data. We propose Attention Pruning (AP), a framework that observes attention patterns in a fixed dataset and generates a global sparseness mask. AP saves 90% of attention computation for language modeling and about 50% for machine translation and GLUE tasks, maintaining result quality. Our method reveals important distinctions between self- and cross-attention patterns, guiding future NLP research. Our framework can reduce both latency and memory requirements for any attention-based model, aiding in the development of improved models for existing or new NLP applications. We have demonstrated this with encoder and autoregressive transformer models using Triton GPU kernels and make our code publicly available at https://github.com/irugina/AP</abstract>
      <url hash="ae1724d9">2024.lrec-main.392</url>
      <bibkey>rugina-etal-2024-data-informed</bibkey>
    </paper>
    <paper id="393">
      <title>Dataset for Identification of Homophobia and Transphobia for <fixed-case>T</fixed-case>elugu, <fixed-case>K</fixed-case>annada, and <fixed-case>G</fixed-case>ujarati</title>
      <author><first>Prasanna Kumar</first><last>Kumaresan</last></author>
      <author><first>Rahul</first><last>Ponnusamy</last></author>
      <author><first>Dhruv</first><last>Sharma</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>4404–4411</pages>
      <abstract>Users of social media platforms are negatively affected by the proliferation of hate or abusive content. There has been a rise in homophobic and transphobic content in recent years targeting LGBT+ individuals. The increasing levels of homophobia and transphobia online can make online platforms harmful and threatening for LGBT+ persons, potentially inhibiting equality, diversity, and inclusion. We are introducing a new dataset for three languages, namely Telugu, Kannada, and Gujarati. Additionally, we have created an expert-labeled dataset to automatically identify homophobic and transphobic content within comments collected from YouTube. We provided comprehensive annotation rules to educate annotators in this process. We collected approximately 10,000 comments from YouTube for all three languages. Marking the first dataset of these languages for this task, we also developed a baseline model with pre-trained transformers.</abstract>
      <url hash="03dcefeb">2024.lrec-main.393</url>
      <bibkey>kumaresan-etal-2024-dataset-identification</bibkey>
    </paper>
    <paper id="394">
      <title>Dataset of Quotation Attribution in <fixed-case>G</fixed-case>erman News Articles</title>
      <author><first>Fynn</first><last>Petersen-Frey</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>4412–4422</pages>
      <abstract>Extracting who says what to whom is a crucial part in analyzing human communication in today’s abundance of data such as online news articles. Yet, the lack of annotated data for this task in German news articles severely limits the quality and usability of possible systems. To remedy this, we present a new, freely available, creative-commons-licensed dataset for quotation attribution in German news articles based on WIKINEWS. The dataset provides curated, high-quality annotations across 1000 documents (250,000 tokens) in a fine-grained annotation schema enabling various downstream uses for the dataset. The annotations not only specify who said what but also how, in which context, to whom and define the type of quotation. We specify our annotation schema, describe the creation of the dataset and provide a quantitative analysis. Further, we describe suitable evaluation metrics, apply two existing systems for quotation attribution, discuss their results to evaluate the utility of our dataset and outline use cases of our dataset in downstream tasks.</abstract>
      <url hash="8b0928a3">2024.lrec-main.394</url>
      <bibkey>petersen-frey-biemann-2024-dataset-quotation</bibkey>
    </paper>
    <paper id="395">
      <title><fixed-case>DC</fixed-case>-<fixed-case>MBR</fixed-case>: Distributional Cooling for Minimum <fixed-case>B</fixed-case>ayesian Risk Decoding</title>
      <author><first>Jianhao</first><last>Yan</last></author>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>4423–4437</pages>
      <abstract>Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding algorithm in Neural Machine Translation. However, MBR performs poorly with label smoothing, which is surprising as label smoothing provides decent improvement with beam search and improves generality in various tasks. In this work, we show that the issue arises from the inconsistency of label smoothing on the token-level and sequence-level distributions. We demonstrate that even though label smoothing only causes a slight change in the token level, the sequence-level distribution is highly skewed. We coin the issue <i>autoregressive over-smoothness</i>. To address this issue, we propose a simple and effective method, Distributional Cooling MBR (DC-MBR), which manipulates the entropy of output distributions by tuning down the Softmax temperature. We theoretically prove the equivalence between the pre-tuning label smoothing factor and distributional cooling. Extensive experiments on NMT benchmarks validate that distributional cooling improves MBR in various settings.</abstract>
      <url hash="a9f6466d">2024.lrec-main.395</url>
      <bibkey>yan-etal-2024-dc-mbr</bibkey>
    </paper>
    <paper id="396">
      <title><fixed-case>DD</fixed-case>x<fixed-case>G</fixed-case>ym: Online Transformer Policies in a Knowledge Graph Based Natural Language Environment</title>
      <author><first>Benjamin</first><last>Winter</last></author>
      <author><first>Alexei Gustavo</first><last>Figueroa Rosero</last></author>
      <author><first>Alexander</first><last>Loeser</last></author>
      <author><first>Felix Alexander</first><last>Gers</last></author>
      <author><first>Nancy Katerina</first><last>Figueroa Rosero</last></author>
      <author><first>Ralf</first><last>Krestel</last></author>
      <pages>4438–4448</pages>
      <abstract>Differential diagnosis (DDx) is vital for physicians and challenging due to the existence of numerous diseases and their complex symptoms. Model training for this task is generally hindered by limited data access due to privacy concerns. To address this, we present DDxGym, a specialized OpenAI Gym environment for clinical differential diagnosis. DDxGym formulates DDx as a natural-language-based reinforcement learning (RL) problem, where agents emulate medical professionals, selecting examinations and treatments for patients with randomly sampled diseases. This RL environment utilizes data labeled from online resources, evaluated by medical professionals for accuracy. Transformers, while effective for encoding text in DDxGym, are unstable in online RL. For that reason we propose a novel training method using an auxiliary masked language modeling objective for policy optimization, resulting in model stabilization and significant performance improvement over strong baselines. Following this approach, our agent effectively navigates large action spaces and identifies universally applicable actions. All data, environment details, and implementation, including experiment reproduction code, are made publicly available.</abstract>
      <url hash="f5c4aaf6">2024.lrec-main.396</url>
      <bibkey>winter-etal-2024-ddxgym-online</bibkey>
    </paper>
    <paper id="397">
      <title>Dealing with Data Scarcity in Spoken Question Answering</title>
      <author><first>Merve</first><last>Ünlü Menevşe</last></author>
      <author><first>Yusufcan</first><last>Manav</last></author>
      <author><first>Ebru</first><last>Arisoy</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>4449–4455</pages>
      <abstract>This paper focuses on dealing with data scarcity in spoken question answering (QA) using automatic question-answer generation and a carefully selected fine-tuning strategy that leverages limited annotated data (paragraphs and question-answer pairs). Spoken QA is a challenging task due to using spoken documents, i.e., erroneous automatic speech recognition (ASR) transcriptions, and the scarcity of spoken QA data. We propose a framework for utilizing limited annotated data effectively to improve spoken QA performance. To deal with data scarcity, we train a question-answer generation model with annotated data and then produce large amounts of question-answer pairs from unannotated data (paragraphs). Our experiments demonstrate that incorporating limited annotated data and the automatically generated data through a carefully selected fine-tuning strategy leads to 5.5% relative F1 gain over the model trained only with annotated data. Moreover, the proposed framework is also effective in high ASR errors.</abstract>
      <url hash="2e8f8caf">2024.lrec-main.397</url>
      <bibkey>unlu-menevse-etal-2024-dealing-data</bibkey>
    </paper>
    <paper id="398">
      <title>Debiasing Multi-Entity Aspect-Based Sentiment Analysis with Norm-Based Data Augmentation</title>
      <author><first>Scott</first><last>Friedman</last></author>
      <author><first>Joan</first><last>Zheng</last></author>
      <author><first>Hillel</first><last>Steinmetz</last></author>
      <pages>4456–4461</pages>
      <abstract>Bias in NLP models may arise from using pre-trained transformer models trained on biased corpora, or by training or fine-tuning directly on corpora with systemic biases. Recent research has explored strategies for reduce measurable biases in NLP predictions while maintaining prediction accuracy on held-out test sets, e.g., by modifying word embedding geometry after training, using purpose-built neural modules for training, or automatically augmenting training data with examples designed to reduce bias. This paper focuses on a debiasing strategy for aspect-based sentiment analysis (ABSA) by augmenting the training data using norm-based language templates derived from previous language resources. We show that the baseline model predicts lower sentiment toward some topics and individuals than others and has relatively high prediction bias (measured by standard deviation), even when the context is held constant. Our results show that our norm-based data augmentation reduces topical bias to less than half while maintaining prediction quality (measured by RMSE), by augmenting the training data by only 1.8%.</abstract>
      <url hash="602d8d27">2024.lrec-main.398</url>
      <bibkey>friedman-etal-2024-debiasing-multi</bibkey>
    </paper>
    <paper id="399">
      <title>Deciphering Emotional Landscapes in the <fixed-case>I</fixed-case>liad: A Novel <fixed-case>F</fixed-case>rench-Annotated Dataset for Emotion Recognition</title>
      <author><first>Davide</first><last>Picca</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <pages>4462–4467</pages>
      <abstract>One of the most significant pieces of ancient Greek literature, the Iliad, is part of humanity’s collective cultural heritage. This work aims to provide the scientific community with an emotion-labeled dataset for classical literature and Western mythology in particular. To model the emotions of the poem, we use a multi-variate time series. We also evaluated the dataset by means of two methods. We compare the manual classification against a dictionary-based benchmark as well as employ a state-of-the-art deep learning masked language model that has been tuned using our data. Both evaluations return encouraging results (MSE and MAE Macro Avg 0.101 and 0.188 respectively) and highlight some interesting phenomena.</abstract>
      <url hash="bb7a11a2">2024.lrec-main.399</url>
      <bibkey>picca-pavlopoulos-2024-deciphering-emotional</bibkey>
    </paper>
    <paper id="400">
      <title><fixed-case>DECM</fixed-case>: Evaluating Bilingual <fixed-case>ASR</fixed-case> Performance on a Code-switching/mixing Benchmark</title>
      <author><first>Enes Yavuz</first><last>Ugan</last></author>
      <author><first>Ngoc-Quan</first><last>Pham</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>4468–4475</pages>
      <abstract>Automatic Speech Recognition has made significant progress, but challenges persist. Code-switched (CSW) Speech presents one such challenge, involving the mixing of multiple languages by a speaker. Even when multilingual ASR models are trained, each utterance on its own usually remains monolingual. We introduce an evaluation dataset for German-English CSW, with German as the matrix language and English as the embedded language. The dataset comprises spontaneous speech from diverse domains, enabling realistic CSW evaluation in German-English. It includes splits with varying degrees of CSW to facilitate specialized model analysis. As it is difficult to collect CSW data for all language pairs, the provision of such evaluation data, is crucial for developing and analyzing ASR models capable of generalizing across unseen pairs. Detailed data statistics are presented, and state-of-the-art (SOTA) multilingual models are evaluated showing challanges of CSW speech.</abstract>
      <url hash="db8e81b3">2024.lrec-main.400</url>
      <bibkey>ugan-etal-2024-decm-evaluating</bibkey>
    </paper>
    <paper id="401">
      <title>Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chenxi</first><last>Sun</last></author>
      <author><first>Hongzhi</first><last>Zhang</last></author>
      <author><first>Zijia</first><last>Lin</last></author>
      <author><first>Jingyuan</first><last>Zhang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Zhongyuan</first><last>Wang</last></author>
      <author><first>Bin</first><last>Chen</last></author>
      <author><first>Chengru</first><last>Song</last></author>
      <author><first>Di</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Gai</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>4476–4487</pages>
      <abstract>Large language models have demonstrated exceptional capability in natural language understanding and generation. However, their generation speed is limited by the inherently sequential nature of their decoding process, posing challenges for real-time applications. This paper introduces Lexical Unit Decoding (LUD), a novel decoding methodology implemented in a data-driven manner, accelerating the decoding process without sacrificing output quality. The core of our approach is the observation that a pre-trained language model can confidently predict multiple contiguous tokens, forming the basis for a lexical unit, in which these contiguous tokens could be decoded in parallel. Extensive experiments validate that our method substantially reduces decoding time while maintaining generation quality, i.e., 33% speed up on natural language generation with no quality loss, and 30% speed up on code generation with a negligible quality loss of 3%. Distinctively, LUD requires no auxiliary models and does not require changes to existing architectures. It can also be integrated with other decoding acceleration methods, thus achieving an even more pronounced inference efficiency boost. We posit that the foundational principles of LUD could define a new decoding paradigm for future language models, enhancing their applicability for a broader spectrum of applications. All codes are be publicly available at https://github.com/tjunlp-lab/Lexical-Unit-Decoding-LUD-.</abstract>
      <url hash="10fdb56f">2024.lrec-main.401</url>
      <bibkey>sun-etal-2024-decoding-speed</bibkey>
    </paper>
    <paper id="402">
      <title>Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models Using Minimal Pairs</title>
      <author><first>Linyang</first><last>He</last></author>
      <author><first>Peili</first><last>Chen</last></author>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Yuanning</first><last>Li</last></author>
      <author><first>Jonathan R.</first><last>Brennan</last></author>
      <pages>4488–4497</pages>
      <abstract>Inspired by cognitive neuroscience studies, we introduce a novel “decoding probing” method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the brain and its representations as “neural activations”, we decode grammaticality labels of minimal pairs from the intermediate layers’ representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.</abstract>
      <url hash="3e08a940">2024.lrec-main.402</url>
      <bibkey>he-etal-2024-decoding-probing</bibkey>
    </paper>
    <paper id="403">
      <title>Decompose, Prioritize, and Eliminate: Dynamically Integrating Diverse Representations for Multimodal Named Entity Recognition</title>
      <author><first>Zihao</first><last>Zheng</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Zexin</first><last>Wang</last></author>
      <author><first>Ruiji</first><last>Fu</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Zhongyuan</first><last>Wang</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>4498–4508</pages>
      <abstract>Multi-modal Named Entity Recognition, a fundamental task for multi-modal knowledge graph construction, requires integrating multi-modal information to extract named entities from text. Previous research has explored the integration of multi-modal representations at different granularities. However, they struggle to integrate all these multi-modal representations to provide rich contextual information to improve multi-modal named entity recognition. In this paper, we propose DPE-MNER, which is an iterative reasoning framework that dynamically incorporates all the diverse multi-modal representations following the strategy of “decompose, prioritize, and eliminate”. Within the framework, the fusion of diverse multi-modal representations is <b>decomposed</b> into hierarchically connected fusion layers that are easier to handle. The incorporation of multi-modal information <b>prioritizes</b> transitioning from “easy-to-hard” and “coarse-to-fine”. The explicit modeling of cross-modal relevance <b>eliminate</b> the irrelevances that will mislead the MNER prediction. Extensive experiments on two public datasets have demonstrated the effectiveness of our approach.</abstract>
      <url hash="e83fb503">2024.lrec-main.403</url>
      <bibkey>zheng-etal-2024-decompose-prioritize</bibkey>
    </paper>
    <paper id="404">
      <title>Deconstructing In-Context Learning: Understanding Prompts via Corruption</title>
      <author><first>Namrata</first><last>Shivagunde</last></author>
      <author><first>Vladislav</first><last>Lialin</last></author>
      <author><first>Sherin</first><last>Muckatira</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <pages>4509–4529</pages>
      <abstract>The ability of large language models (LLMs) to “learn in context” based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt attributes and often produced contradictory results. Additionally, previous research either focused on models with fewer than 15 billion parameters or exclusively examined black-box models like GPT-3 or PaLM, making replication challenging. In the present study, we decompose the entire prompt into four components: task description, demonstration inputs, labels, and inline instructions provided for each demonstration. We investigate the effects of structural and semantic corruptions of these elements on model performance. We study models ranging from 1.5B to 70B in size, using ten datasets covering classification and generation tasks. We find that repeating text within the prompt boosts model performance, and bigger models (≥30B) are more sensitive to the semantics of the prompt. Finally, we observe that adding task and inline instructions to the demonstrations enhances model performance even when the instructions are semantically corrupted. The code is available at this URL.</abstract>
      <url hash="5193e8ac">2024.lrec-main.404</url>
      <bibkey>shivagunde-etal-2024-deconstructing-context</bibkey>
    </paper>
    <paper id="405">
      <title><fixed-case>DEEM</fixed-case>: Dynamic Experienced Expert Modeling for Stance Detection</title>
      <author><first>Xiaolong</first><last>Wang</last></author>
      <author><first>Yile</first><last>Wang</last></author>
      <author><first>Sijie</first><last>Cheng</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>4530–4541</pages>
      <abstract>Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on three standard benchmarks, outperforms methods with self-consistency reasoning, and reduces the bias of LLMs.</abstract>
      <url hash="5e958a67">2024.lrec-main.405</url>
      <bibkey>wang-etal-2024-deem-dynamic</bibkey>
      <revision id="1" href="2024.lrec-main.405v1" hash="ab09ba39"/>
      <revision id="2" href="2024.lrec-main.405v2" hash="a176b822" date="2024-06-03">Typo correction.</revision>
      <revision id="3" href="2024.lrec-main.405v3" hash="5e958a67" date="2024-06-10">Added conference and pages information.</revision>
    </paper>
    <paper id="406">
      <title>Deep Learning Based Named Entity Recognition Models for Recipes</title>
      <author><first>Ayush</first><last>Agarwal</last></author>
      <author><first>Janak</first><last>Kapuriya</last></author>
      <author><first>Shubham</first><last>Agrawal</last></author>
      <author><first>Akhil Vamshi</first><last>Konam</last></author>
      <author><first>Mansi</first><last>Goel</last></author>
      <author><first>Rishabh</first><last>Gupta</last></author>
      <author><first>Shrey</first><last>Rastogi</last></author>
      <author><first>Niharika</first><last>Niharika</last></author>
      <author><first>Ganesh</first><last>Bagler</last></author>
      <pages>4542–4554</pages>
      <abstract>Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of NER approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (LLMs) provides deep insights. We conclude that few-shot prompting on LLMs has abysmal performance, whereas the fine-tuned spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.</abstract>
      <url hash="56f26f96">2024.lrec-main.406</url>
      <bibkey>agarwal-etal-2024-deep-learning</bibkey>
    </paper>
    <paper id="407">
      <title>Deep Reinforcement Learning-based Dialogue Policy with Graph Convolutional <fixed-case>Q</fixed-case>-network</title>
      <author><first>Kai</first><last>Xu</last></author>
      <author><first>Zhengyu</first><last>Wang</last></author>
      <author><first>Yuxuan</first><last>Long</last></author>
      <author><first>Qiaona</first><last>Zhao</last></author>
      <pages>4555–4565</pages>
      <abstract>Deep Reinforcement learning (DRL) has been successfully applied to the dialogue policy of task-oriented dialogue systems. However, one challenge in the existing DRL-based dialogue policy methods is their unstructured state-action representations without the ability to learn the relationship between dialogue states and actions. To alleviate this problem, we propose a graph-structured dialogue policy framework for task-oriented dialogue systems. More specifically, we use an unsupervised approach to construct two different bipartite graphs. Then, we generate the user-related and knowledge-related subgraphs based on the matching dialogue sub-states with bipartite graph nodes. A variant of graph convolutional network is employed to encode dialogue subgraphs. After that, we use a bidirectional gated cycle unit (BGRU) and self-attention mechanism to obtain the high-level historical state representations and employ a neural network for the high-level current state representations. The two state representations are joined to learn the action value of dialogue policy. Experiments implemented with different DRL algorithms demonstrate that the proposed framework significantly improves the effectiveness and stability of dialogue policies.</abstract>
      <url hash="a9633332">2024.lrec-main.407</url>
      <bibkey>xu-etal-2024-deep-reinforcement</bibkey>
    </paper>
    <paper id="408">
      <title>Deep Reinforcement Learning with Hierarchical Action Exploration for Dialogue Generation</title>
      <author><first>Itsugun</first><last>Cho</last></author>
      <author><first>Ryota</first><last>Takahashi</last></author>
      <author><first>Yusaku</first><last>Yanase</last></author>
      <author><first>Hiroaki</first><last>Saito</last></author>
      <pages>4566–4579</pages>
      <abstract>Traditionally, approximate dynamic programming is employed in dialogue generation with greedy policy improvement through action sampling, as the natural language action space is vast. However, this practice is inefficient for reinforcement learning (RL) due to the sparsity of eligible responses with high action values, which leads to weak improvement sustained by random sampling. This paper presents theoretical analysis and experiments that reveal the performance of the dialogue policy is positively correlated with the sampling size. To overcome this limitation, we introduce a novel dual-granularity Q-function that explores the most promising response category to intervene in the sampling process. Our approach extracts actions based on a grained hierarchy, thereby achieving the optimum with fewer policy iterations. Additionally, we use offline RL and learn from multiple reward functions designed to capture emotional nuances in human interactions. Empirical studies demonstrate that our algorithm outperforms baselines across automatic metrics and human evaluations. Further testing reveals that our algorithm exhibits both explainability and controllability, as well as generates responses with higher expected rewards.</abstract>
      <url hash="569b36f2">2024.lrec-main.408</url>
      <bibkey>cho-etal-2024-deep-reinforcement</bibkey>
    </paper>
    <paper id="409">
      <title><fixed-case>D</fixed-case>e<fixed-case>F</fixed-case>akt<fixed-case>S</fixed-case>: A <fixed-case>G</fixed-case>erman Dataset for Fine-Grained Disinformation Detection through Social Media Framing</title>
      <author><first>Shaina</first><last>Ashraf</last></author>
      <author><first>Isabel</first><last>Bezzaoui</last></author>
      <author><first>Ionut</first><last>Andone</last></author>
      <author><first>Alexander</first><last>Markowetz</last></author>
      <author><first>Jonas</first><last>Fegert</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>4580–4591</pages>
      <abstract>In today’s rapidly evolving digital age, disinformation poses a significant threat to public sentiment and socio-political dynamics. To address this, we introduce a new dataset “DeFaktS”, designed to understand and counter disinformation within German media. Distinctively curated across various news topics, DeFaktS offers an unparalleled insight into the diverse facets of disinformation. Our dataset, containing 105,855 posts with 20,008 meticulously labeled tweets, serves as a rich platform for in-depth exploration of disinformation’s diverse characteristics. A key attribute that sets DeFaktS apart is, its fine-grain annotations based on polarized categories. Our annotation framework, grounded in the textual characteristics of news content, eliminates the need for external knowledge sources. Unlike most existing corpora that typically assign a singular global veracity value to news, our methodology seeks to annotate every structural component and semantic element of a news piece, ensuring a comprehensive and detailed understanding. In our experiments, we employed a mix of classical machine learning and advanced transformer-based models. The results underscored the potential of DeFaktS, with transformer models, especially the German variant of BERT, exhibiting pronounced effectiveness in both binary and fine-grained classifications.</abstract>
      <url hash="c4bf6ef5">2024.lrec-main.409</url>
      <bibkey>ashraf-etal-2024-defakts-german</bibkey>
    </paper>
    <paper id="410">
      <title><fixed-case>DEIE</fixed-case>: Benchmarking Document-level Event Information Extraction with a Large-scale <fixed-case>C</fixed-case>hinese News Dataset</title>
      <author><first>Yubing</first><last>Ren</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Zixuan ZM</first><last>Ma</last></author>
      <author><first>Fang</first><last>Fang</last></author>
      <author><first>Ping</first><last>Guo</last></author>
      <author><first>Wei</first><last>Ma</last></author>
      <pages>4592–4604</pages>
      <abstract>A text corpus centered on events is foundational to research concerning the detection, representation, reasoning, and harnessing of online events. The majority of current event-based datasets mainly target sentence-level tasks, thus to advance event-related research spanning from sentence to document level, this paper introduces DEIE, a unified large-scale document-level event information extraction dataset with over 56,000+ events and 242,000+ arguments. Three key features stand out: large-scale manual annotation (20,000 documents), comprehensive unified annotation (encompassing event trigger/argument, summary, and relation at once), and emergency events annotation (covering 19 emergency types). Notably, our experiments reveal that current event-related models struggle with DEIE, signaling a pressing need for more advanced event-related research in the future.</abstract>
      <url hash="aa70c1b5">2024.lrec-main.410</url>
      <bibkey>ren-etal-2024-deie-benchmarking</bibkey>
    </paper>
    <paper id="411">
      <title><fixed-case>DELAN</fixed-case>: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning</title>
      <author><first>Mengfei</first><last>Du</last></author>
      <author><first>Binhao</first><last>Wu</last></author>
      <author><first>Jiwen</first><last>Zhang</last></author>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Zejun</first><last>Li</last></author>
      <author><first>Ruipu</first><last>Luo</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>4605–4616</pages>
      <abstract>Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.</abstract>
      <url hash="ebc0b906">2024.lrec-main.411</url>
      <bibkey>du-etal-2024-delan-dual</bibkey>
    </paper>
    <paper id="412">
      <title>Demonstration Retrieval-Augmented Generative Event Argument Extraction</title>
      <author><first>Shiming</first><last>He</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Shuai</first><last>Yang</last></author>
      <author><first>Jianmin</first><last>Yao</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>4617–4625</pages>
      <abstract>We tackle Event Argument Extraction (EAE) in the manner of template-based generation. Based on our exploration of generative EAE, it suffers from several issues, such as multiple arguments of one role, generating words out of context and inconsistency with prescribed format. We attribute it to the weakness of following complex input prompts. To address these problems, we propose the demonstration retrieval-augmented generative EAE (DRAGEAE), containing two components: event knowledge-injected generator (EKG) and demonstration retriever (DR). EKG employs event knowledge prompts to capture role dependencies and semantics. DR aims to search informative demonstrations from training data, facilitating the conditional generation of EKG. To train DR, we use the probability-based rankings from large language models (LLMs) as supervised signals. Experimental results on ACE-2005, RAMS and WIKIEVENTS demonstrate that our method outperforms all strong baselines and it can be generalized to various datasets. Further analysis is conducted to discuss the impact of diverse LLMs and prove that our model alleviates the above issues.</abstract>
      <url hash="2b368db7">2024.lrec-main.412</url>
      <bibkey>he-etal-2024-demonstration-retrieval</bibkey>
    </paper>
    <paper id="413">
      <title>Denoising Labeled Data for Comment Moderation Using Active Learning</title>
      <author><first>Andraž</first><last>Pelicon</last></author>
      <author><first>Mladen</first><last>Karan</last></author>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>4626–4633</pages>
      <abstract>Noisily labeled textual data is ample on internet platforms that allow user-created content. Training models, such as offensive language detection models for comment moderation, on such data may prove difficult as the noise in the labels prevents the model to converge. In this work, we propose to use active learning methods for the purposes of denoising training data for model training. The goal is to sample examples the most informative examples with noisy labels with active learning and send them to the oracle for reannotation thus reducing the overall cost of reannotation. In this setting we tested three existing active learning methods, namely DBAL, Variance of Gradients (VoG) and BADGE. The proposed approach to data denoising is tested on the problem of offensive language detection. We observe that active learning can be effectively used for the purposes of data denoising, however care should be taken when choosing the algorithm for this purpose.</abstract>
      <url hash="f0eb6a77">2024.lrec-main.413</url>
      <bibkey>pelicon-etal-2024-denoising-labeled</bibkey>
    </paper>
    <paper id="414">
      <title>Denoising Table-Text Retrieval for Open-Domain Question Answering</title>
      <author><first>Deokhyung</first><last>Kang</last></author>
      <author><first>Baikjin</first><last>Jung</last></author>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Gary Geunbae</first><last>Lee</last></author>
      <pages>4634–4640</pages>
      <abstract>In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder to identify minimum and maximum values within a column. Experimental results demonstrate that DoTTeR significantly outperforms strong baselines on both retrieval recall and downstream QA tasks. Our code is available at https://github.com/deokhk/DoTTeR.</abstract>
      <url hash="c4b43422">2024.lrec-main.414</url>
      <bibkey>kang-etal-2024-denoising-table</bibkey>
    </paper>
    <paper id="415">
      <title>Dependencies over Times and Tools (<fixed-case>D</fixed-case>o<fixed-case>TT</fixed-case>)</title>
      <author><first>Andy</first><last>Luecking</last></author>
      <author><first>Giuseppe</first><last>Abrami</last></author>
      <author><first>Leon</first><last>Hammerla</last></author>
      <author><first>Marc</first><last>Rahn</last></author>
      <author><first>Daniel</first><last>Baumartz</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>4641–4653</pages>
      <abstract>Purpose: Based on the examples of English and German, we investigate to what extent parsers trained on modern variants of these languages can be transferred to older language levels without loss. Methods: We developed a treebank called DoTT (https://github.com/texttechnologylab/DoTT) which covers, roughly, the time period from 1800 until today, in conjunction with the further development of the annotation tool DependencyAnnotator. DoTT consists of a collection of diachronic corpora enriched with dependency annotations using 3 parsers, 6 pre-trained language models, 5 newly trained models for German, and two tag sets (TIGER and Universal Dependencies). To assess how the different parsers perform on texts from different time periods, we created a gold standard sample as a benchmark. Results: We found that the parsers/models perform quite well on modern texts (document-level LAS ranging from 82.89 to 88.54) and slightly worse on older texts, as expected (average document-level LAS 84.60 vs. 86.14), but not significantly. For German texts, the (German) TIGER scheme achieved slightly better results than UD. Conclusion: Overall, this result speaks for the transferability of parsers to past language levels, at least dating back until around 1800. This very transferability, it is however argued, means that studies of language change in the field of dependency syntax can draw on dependency distance but miss out on some grammatical phenomena.</abstract>
      <url hash="eb21b78a">2024.lrec-main.415</url>
      <bibkey>luecking-etal-2024-dependencies-times</bibkey>
    </paper>
    <paper id="416">
      <title>Depth Aware Hierarchical Replay Continual Learning for Knowledge Based Question Answering</title>
      <author><first>Zhixiong</first><last>Cao</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last></author>
      <author><first>Yangning</first><last>Li</last></author>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Rongsheng</first><last>Li</last></author>
      <author><first>Hong-Gee</first><last>Kim</last></author>
      <pages>4654–4664</pages>
      <abstract>Continual learning is an emerging area of machine learning that deals with the issue where models adapt well to the latest data but lose the ability to remember past data due to changes in the data source. A widely adopted solution is by keeping a small memory of previous learned data that use replay. Most of the previous studies on continual learning focused on classification tasks, such as image classification and text classification, where the model needs only to categorize the input data. Inspired by the human ability to incrementally learn knowledge and solve different problems using learned knowledge, we considered a more pratical scenario, knowledge based quesiton answering about continual learning. In this scenario, each single question is different from others(means different fact trippes to answer them) while classification tasks only need to find feature boundaries of different categories, which are the curves or surfaces that separate different categories in the feature space. To address this issue, we proposed a depth aware hierarchical replay framework which include a tree structure classfier to have a sense of knowledge distribution and fill the gap between text classfication tasks and question-answering tasks for continual learning, a local sampler to grasp these critical samples and a depth aware learning network to reconstructe the feature space of a single learning round. In our experiments, we have demonstrated that our proposed model outperforms previous continual learning methods in mitigating the issue of catastrophic forgetting.</abstract>
      <url hash="67fc41ca">2024.lrec-main.416</url>
      <bibkey>cao-etal-2024-depth-aware</bibkey>
    </paper>
    <paper id="417">
      <title>Depth-Wise Attention (<fixed-case>DWA</fixed-case>tt): A Layer Fusion Method for Data-Efficient Classification</title>
      <author><first>Muhammad</first><last>ElNokrashy</last></author>
      <author><first>Badr</first><last>AlKhamissi</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>4665–4674</pages>
      <abstract>Language Models pretrained on large textual data have been shown to encode different types of knowledge simultaneously. Traditionally, only the features from the last layer are used when adapting to new tasks or data. We put forward that, when using or finetuning deep pretrained models, intermediate layer features that may be relevant to the downstream task are buried too deep to be used efficiently in terms of needed samples or steps. To test this, we propose a new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface signals from non-final layers. We compare DWAtt to a basic concatenation-based layer fusion method (Concat), and compare both to a deeper model baseline—all kept within a similar parameter budget. Our findings show that DWAtt and Concat are more step- and sample-efficient than the baseline, especially in the few-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03 NER, layer fusion shows 3.68 − 9.73% F1 gain at different few-shot sizes. The layer fusion models presented significantly outperform the baseline in various training scenarios with different data sizes, architectures, and training constraints.</abstract>
      <url hash="dc3d8042">2024.lrec-main.417</url>
      <bibkey>elnokrashy-etal-2024-depth-wise</bibkey>
    </paper>
    <paper id="418">
      <title>Deriving Entity-Specific Embeddings from Multi-Entity Sequences</title>
      <author><first>Connor</first><last>Heaton</last></author>
      <author><first>Prasenjit</first><last>Mitra</last></author>
      <pages>4675–4684</pages>
      <abstract>Underpinning much of the recent progress in deep learning is the transformer architecture, which takes as input a sequence of embeddings E and emits an updated sequence of embeddings E’. A special [CLS] embedding is often included in this sequence, serving as a description of the sequence once processed and used as the basis for subsequent sequence-level tasks. The processed [CLS] embedding loses utility, however, when the model is presented with a multi-entity sequence and asked to perform an entity-specific task. When processing a multi-speaker dialogue, for example, the [CLS] embedding describes the entire dialogue, not any individual utterance/speaker. Existing methods toward entity-specific prediction involve redundant computation or post-processing outside of the transformer. We present a novel methodology for deriving entity-specific embeddings from a multi-entity sequence completely within the transformer, with a loose definition of entity amenable to many problem spaces. To show the generic applicability of our method, we apply it to widely different tasks: emotion recognition in conversation and player performance projection in baseball and show that it can be used to achieve SOTA in both. Code can be found at https://github.com/c-heat16/EntitySpecificEmbeddings.</abstract>
      <url hash="96a10fb0">2024.lrec-main.418</url>
      <bibkey>heaton-mitra-2024-deriving-entity</bibkey>
    </paper>
    <paper id="419">
      <title><fixed-case>DET</fixed-case>: A Dual-Encoding Transformer for Relational Graph Embedding</title>
      <author><first>Lingbing</first><last>Guo</last></author>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Jiaoyan</first><last>Chen</last></author>
      <author><first>Qiang</first><last>Zhang</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>4685–4696</pages>
      <abstract>Despite recent successes in natural language processing and computer vision, Transformer faces scalability issues when processing graphs, e.g., computing the full node-to-node attention on knowledge graphs (KGs) with million of entities is still infeasible. The existing methods mitigate this problem by considering only the local neighbors, sacrificing the Transformer’s ability to attend to elements at any distance. This paper proposes a new Transformer architecture called Dual-Encoding Transformer (DET). DET comprises a structural encoder to aggregate information from nearby neighbors, and a semantic encoder to seek for semantically relevant nodes. We adopt a semantic neighbor search approach inspired by multiple sequence alignment (MSA) algorithms used in biological sciences. By stacking the two encoders alternately, similar to the MSA Transformer for protein representation, our method achieves superior performance compared to state-of-the-art attention-based methods on complex relational graphs like KGs and citation networks. Additionally, DET remains competitive for smaller graphs such as molecules.</abstract>
      <url hash="80c4fcc9">2024.lrec-main.419</url>
      <bibkey>guo-etal-2024-det-dual</bibkey>
    </paper>
    <paper id="420">
      <title>Detecting Conceptual Abstraction in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Michaela</first><last>Regneri</last></author>
      <author><first>Alhassan</first><last>Abdelhalim</last></author>
      <author><first>Soeren</first><last>Laue</last></author>
      <pages>4697–4704</pages>
      <abstract>We show a novel approach to detecting noun abstraction within a large language model (LLM). Starting from a psychologically motivated set of noun pairs in taxonomic relationships, we instantiate surface patterns indicating hypernymy and analyze the attention matrices produced by BERT. We compare the results to two sets of counterfactuals and show that we can detect hypernymy in the abstraction mechanism, which cannot solely be related to the distributional similarity of noun pairs. Our findings are a first step towards the explainability of conceptual abstraction in LLMs.</abstract>
      <url hash="5fe3dff1">2024.lrec-main.420</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4dd3b61a">2024.lrec-main.420.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>regneri-etal-2024-detecting-conceptual</bibkey>
    </paper>
    <paper id="421">
      <title>Detecting Critical Errors Considering Cross-Cultural Factors in <fixed-case>E</fixed-case>nglish-<fixed-case>K</fixed-case>orean Translation</title>
      <author><first>Sugyeong</first><last>Eo</last></author>
      <author><first>Jungwoo</first><last>Lim</last></author>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>DaHyun</first><last>Jung</last></author>
      <author><first>Seonmin</first><last>Koo</last></author>
      <author><first>Hyeonseok</first><last>Moon</last></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>4705–4716</pages>
      <abstract>Recent machine translation (MT) systems have overcome language barriers for a wide range of users, yet they still carry the risk of critical meaning deviation. Critical error detection (CED) is a task that identifies an inherent risk of catastrophic meaning distortions in the machine translation output. With the importance of reflecting cultural elements in detecting critical errors, we introduce the culture-aware “Politeness” type in detecting English-Korean critical translation errors. Besides, we facilitate two tasks by providing multiclass labels: critical error detection and critical error type classification (CETC). Empirical evaluations reveal that our introduced data augmentation approach using a newly presented perturber significantly outperforms existing baselines in both tasks. Further analysis highlights the significance of multiclass labeling by demonstrating its superior effectiveness compared to binary labels.</abstract>
      <url hash="7eb1c422">2024.lrec-main.421</url>
      <bibkey>eo-etal-2024-detecting-critical</bibkey>
    </paper>
    <paper id="422">
      <title>Detecting Cybercrimes in Accordance with <fixed-case>P</fixed-case>akistani Law: Dataset and Evaluation Using <fixed-case>PLM</fixed-case>s</title>
      <author><first>Faizad</first><last>Ullah</last></author>
      <author><first>Ali</first><last>Faheem</last></author>
      <author><first>Ubaid</first><last>Azam</last></author>
      <author><first>Muhammad Sohaib</first><last>Ayub</last></author>
      <author><first>Faisal</first><last>Kamiran</last></author>
      <author><first>Asim</first><last>Karim</last></author>
      <pages>4717–4728</pages>
      <abstract>Cybercrime is a serious and growing threat affecting millions of people worldwide. Detecting cybercrimes from text messages is challenging, as it requires understanding the linguistic and cultural nuances of different languages and regions. Roman Urdu is a widely used language in Pakistan and other South Asian countries, however, it lacks sufficient resources and tools for natural language processing and cybercrime detection. To address this problem, we make three main contributions in this paper. (1) We create and release CRU, a benchmark dataset for text-based cybercrime detection in Roman Urdu, which covers a number of cybercrimes as defined by the Prevention of Electronic Crimes Act (PECA) of Pakistan. This dataset is annotated by experts following a standardized procedure based on Pakistan’s legal framework. (2) We perform experiments on four pre-trained language models (PLMs) for cybercrime text classification in Roman Urdu. Our results show that xlm-roberta-base is the best model for this task, achieving the highest performance on all metrics. (3) We explore the utility of prompt engineering techniques, namely prefix and cloze prompts, for enhancing the performance of PLMs for low-resource languages such as Roman Urdu. We analyze the impact of different prompt shapes and <tex-math>k</tex-math>-shot settings on the performance of xlm-roberta-base and bert-base-multilingual-cased. We find that prefix prompts are more effective than cloze prompts for Roman Urdu classification tasks, as they provide more contextually relevant completions for the models. Our work provides useful insights and resources for future research on cybercrime detection and text classification in low-resource languages.</abstract>
      <url hash="378c29b7">2024.lrec-main.422</url>
      <bibkey>ullah-etal-2024-detecting-cybercrimes</bibkey>
    </paper>
    <paper id="423">
      <title>Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Katrin</first><last>Tomanek</last></author>
      <author><first>Jessica</first><last>Hoffmann</last></author>
      <author><first>Nithum</first><last>Thain</last></author>
      <author><first>Erin</first><last>MacMurray van Liemt</last></author>
      <author><first>Kathleen</first><last>Meier-Hellstern</last></author>
      <author><first>Lucas</first><last>Dixon</last></author>
      <pages>4729–4743</pages>
      <abstract>We explore a strategy to handle controversial topics in LLM-based chatbots based on Wikipedia’s Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our results demonstrate that LLM-based classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for coverage error detection on unambiguous error cases. We show that when no training data is available, our other methods still yield good results on hallucination (84.0%) and coverage error (85.2%) detection.</abstract>
      <url hash="bb651845">2024.lrec-main.423</url>
      <bibkey>chang-etal-2024-detecting-hallucination</bibkey>
    </paper>
    <paper id="424">
      <title>Detecting Impact Relevant Sections in Scientific Research</title>
      <author><first>Maria</first><last>Becker</last></author>
      <author><first>Kanyao</first><last>Han</last></author>
      <author><first>Haejin</first><last>Lee</last></author>
      <author><first>Antonina</first><last>Werthmann</last></author>
      <author><first>Rezvaneh</first><last>Rezapour</last></author>
      <author><first>Jana</first><last>Diesner</last></author>
      <author><first>Andreas</first><last>Witt</last></author>
      <pages>4744–4749</pages>
      <abstract>Impact assessment is an evolving area of research that aims at measuring and predicting the potential effects of projects or programs. Measuring the impact of scientific research is a vibrant subdomain, closely intertwined with impact assessment. A recurring obstacle pertains to the absence of an efficient framework which can facilitate the analysis of lengthy reports and text labeling. To address this issue, we propose a framework for automatically assessing the impact of scientific research projects by identifying pertinent sections in project reports that indicate the potential impacts. We leverage a mixed-method approach, combining manual annotations with supervised machine learning, to extract these passages from project reports. We experiment with different machine learning algorithms, including traditional statistical models as well as pre-trained transformer language models. Our experiments show that our proposed method achieves accuracy scores up to 0.81, and that our method is generalizable to scientific research from different domains and different languages.</abstract>
      <url hash="b409106e">2024.lrec-main.424</url>
      <attachment type="OptionalSupplementaryMaterial" hash="af1611b3">2024.lrec-main.424.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>becker-etal-2024-detecting-impact</bibkey>
    </paper>
    <paper id="425">
      <title>Detecting Loanwords in Emakhuwa: An Extremely Low-Resource <fixed-case>B</fixed-case>antu Language Exhibiting Significant Borrowing from <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Felermino Dario Mario</first><last>Ali</last></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last></author>
      <author><first>Rui</first><last>Sousa-Silva</last></author>
      <pages>4750–4759</pages>
      <abstract>The accurate identification of loanwords within a given text holds significant potential as a valuable tool for addressing data augmentation and mitigating data sparsity issues. Such identification can improve the performance of various natural language processing tasks, particularly in the context of low-resource languages that lack standardized spelling conventions.This research proposes a supervised method to identify loanwords in Emakhuwa, borrowed from Portuguese. Our methodology encompasses a two-fold approach. Firstly, we employ traditional machine learning algorithms incorporating handcrafted features, including language-specific and similarity-based features. We build upon prior studies to extract similarity features and propose utilizing two external resources: a Sequence-to-Sequence model and a dictionary. This innovative approach allows us to identify loanwords solely by analyzing the target word without prior knowledge about its donor counterpart. Furthermore, we fine-tune the pre-trained CANINE model for the downstream task of loanword detection, which culminates in the impressive achievement of the F1-score of 93%. To the best of our knowledge, this study is the first of its kind focusing on Emakhuwa, and the preliminary results are promising as they pave the way to further advancements.</abstract>
      <url hash="c97185fb">2024.lrec-main.425</url>
      <bibkey>ali-etal-2024-detecting-loanwords</bibkey>
    </paper>
    <paper id="426">
      <title>Detecting Offensive Language in an Open Chatbot Platform</title>
      <author><first>Hyeonho</first><last>Song</last></author>
      <author><first>Jisu</first><last>Hong</last></author>
      <author><first>Chani</first><last>Jung</last></author>
      <author><first>Hyojin</first><last>Chin</last></author>
      <author><first>Mingi</first><last>Shin</last></author>
      <author><first>Yubin</first><last>Choi</last></author>
      <author><first>Junghoi</first><last>Choi</last></author>
      <author><first>Meeyoung</first><last>Cha</last></author>
      <pages>4760–4771</pages>
      <abstract>While detecting offensive language in online spaces remains an important societal issue, there is still a significant gap in existing research and practial datasets specific to chatbots. Furthermore, many of the current efforts by service providers to automatically filter offensive language are vulnerable to users’ deliberate text manipulation tactics, such as misspelling words. In this study, we analyze offensive language patterns in real logs of 6,254,261 chat utterance pairs from the commercial chat service Simsimi, which cover a variety of conversation topics. Based on the observed patterns, we introduce a novel offensive language detection method—a contrastive learning model that embeds chat content with a random masking strategy. We show that this model outperforms existing models in detecting offensive language in open-domain chat conversations while also demonstrating robustness against users’ deliberate text manipulation tactics when using offensive language. We release our curated chatbot dataset to foster research on offensive language detection in open-domain conversations and share lessons learned from mitigating offensive language on a live platform.</abstract>
      <url hash="237e22a3">2024.lrec-main.426</url>
      <bibkey>song-etal-2024-detecting-offensive</bibkey>
    </paper>
    <paper id="427">
      <title>Detecting Sexual Content at the Sentence Level in First Millennium <fixed-case>L</fixed-case>atin Texts</title>
      <author><first>Thibault</first><last>Clerice</last></author>
      <pages>4772–4783</pages>
      <abstract>In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013 training samples), and show that, while our models perform worse, they still offer a high enough precision and TPR, even without MLM, respectively 69% and 51%. Given the result, we provide an analysis of the attention mechanism as a supporting added value for humanists in order to produce more data.</abstract>
      <url hash="7853aa84">2024.lrec-main.427</url>
      <bibkey>clerice-2024-detecting-sexual</bibkey>
    </paper>
    <paper id="428">
      <title>Detection, Diagnosis, and Explanation: A Benchmark for <fixed-case>C</fixed-case>hinese Medial Hallucination Evaluation</title>
      <author><first>Chengfeng</first><last>Dou</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Yanyuan</first><last>Chen</last></author>
      <author><first>Zhi</first><last>Jin</last></author>
      <author><first>Wenpin</first><last>Jiao</last></author>
      <author><first>Haiyan</first><last>Zhao</last></author>
      <author><first>Yu</first><last>Huang</last></author>
      <pages>4784–4794</pages>
      <abstract>Large Language Models (LLMs) have made significant progress recently. However, their practical use in healthcare is hindered by their tendency to generate hallucinations. One specific type, called snowballing hallucination, occurs when LLMs encounter misleading information, and poses a security threat to LLMs. To understand how well LLMs can resist these hallucination, we create the Chinese Medical Hallucination Evaluation benchmark (CMHE). This benchmark can be used to evaluate LLMs’ ability to detect medical hallucinations, make accurate diagnoses in noisy conditions, and provide plausible explanations. The creation of this benchmark involves a combination of manual and model-based approaches. In addition, we use ICD-10 as well as MeSH, two specialized glossaries, to aid in the evaluation. Our experiments show that the LLM struggles to identify fake medical terms and makes poor diagnoses in distracting environments. However, improving the model’s understanding of medical concepts can help it resist interference to some extent.</abstract>
      <url hash="90042dc9">2024.lrec-main.428</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c03586d4">2024.lrec-main.428.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>dou-etal-2024-detection-diagnosis</bibkey>
    </paper>
    <paper id="429">
      <title>Developing a Benchmark for Pronunciation Feedback: Creation of a Phonemically Annotated Speech Corpus of isi<fixed-case>Z</fixed-case>ulu Language Learner Speech</title>
      <author><first>Alexandra</first><last>O’Neil</last></author>
      <author><first>Nils</first><last>Hjortnaes</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Zinhle</first><last>Nkosi</last></author>
      <author><first>Thulile</first><last>Ndlovu</last></author>
      <author><first>Zanele</first><last>Mlondo</last></author>
      <author><first>Ngami Phumzile</first><last>Pewa</last></author>
      <pages>4795–4801</pages>
      <abstract>Pronunciation of the phonemic inventory of a new language often presents difficulties to second language (L2) learners. These challenges can be alleviated by the development of pronunciation feedback tools that take speech input from learners and return information about errors in the utterance. This paper presents the development of a corpus designed for use in pronunciation feedback research. The corpus is comprised of gold standard recordings from isiZulu teachers and recordings from isiZulu L2 learners that have been annotated for pronunciation errors. Exploring the potential benefits of word-level versus phoneme-level feedback necessitates a speech corpus that has been annotated for errors on the phoneme-level. To aid in this discussion, this corpus of isiZulu L2 speech has been annotated for phoneme-errors in utterances, as well as suprasegmental errors in tone.</abstract>
      <url hash="86d388a2">2024.lrec-main.429</url>
      <bibkey>oneil-etal-2024-developing-benchmark</bibkey>
    </paper>
    <paper id="430">
      <title>Developing a <fixed-case>R</fixed-case>hetorical <fixed-case>S</fixed-case>tructure <fixed-case>T</fixed-case>heory Treebank for <fixed-case>C</fixed-case>zech</title>
      <author><first>Lucie</first><last>Polakova</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Šárka</first><last>Zikánová</last></author>
      <author><first>Eva</first><last>Hajicova</last></author>
      <pages>4802–4810</pages>
      <abstract>We introduce the first version of the Czech RST Discourse Treebank, a collection of Czech journalistic texts manually annotated using the Rhetorical Structure Theory (RST), a global coherence model proposed by Mann and Thompson (1988). Each document in the corpus is represented as a single tree-like structure, where discourse units are interconnected through hierarchical rhetorical relations and their relative importance for the main purpose of a text is modeled by the nuclearity principle. The treebank is freely available in the LINDAT/CLARIAH-CZ repository under the Creative Commons license; for some documents, it includes two gold annotations representing divergent yet relevant interpretations. The paper outlines the annotation process, provides corpus statistics and evaluation, and discusses the issue of consistency associated with the global level of textual interpretation. In general, good agreement on the structure and labeling could be achieved on the lowest, local tree level and on the identification of the most central (nuclear) elementary discourse units. Disagreements mostly concerned segmentation and, in the structure, differences in the stepwise process of linking the largest text blocks. The project contributes to the advancement of RST research and its application to real-world text analysis challenges.</abstract>
      <url hash="da3e23d6">2024.lrec-main.430</url>
      <bibkey>polakova-etal-2024-developing-rhetorical</bibkey>
    </paper>
    <paper id="431">
      <title>Development and Evaluation of Pre-trained Language Models for Historical <fixed-case>D</fixed-case>anish and <fixed-case>N</fixed-case>orwegian Literary Texts</title>
      <author><first>Ali</first><last>Al-Laith</last></author>
      <author><first>Alexander</first><last>Conroy</last></author>
      <author><first>Jens</first><last>Bjerring-Hansen</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <pages>4811–4819</pages>
      <abstract>We develop and evaluate the first pre-trained language models specifically tailored for historical Danish and Norwegian texts. Three models are trained on a corpus of 19th-century Danish and Norwegian literature: two directly on the corpus with no prior pre-training, and one with continued pre-training. To evaluate the models, we utilize an existing sentiment classification dataset, and additionally introduce a new annotated word sense disambiguation dataset focusing on the concept of fate. Our assessment reveals that the model employing continued pre-training outperforms the others in two downstream NLP tasks on historical texts. Specifically, we observe substantial improvement in sentiment classification and word sense disambiguation compared to models trained on contemporary texts. These results highlight the effectiveness of continued pre-training for enhancing performance across various NLP tasks in historical text analysis.</abstract>
      <url hash="d38599e6">2024.lrec-main.431</url>
      <bibkey>al-laith-etal-2024-development-evaluation</bibkey>
    </paper>
    <paper id="432">
      <title>Development of Community-Oriented Text-to-Speech Models for <fixed-case>M</fixed-case>āori ‘Avaiki <fixed-case>N</fixed-case>ui (<fixed-case>C</fixed-case>ook <fixed-case>I</fixed-case>slands <fixed-case>M</fixed-case>āori)</title>
      <author><first>Jesin</first><last>James</last></author>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <author><first>Sally Akevai</first><last>Nicholas</last></author>
      <author><first>Joshua</first><last>Zhu</last></author>
      <author><first>Bovey</first><last>Yu</last></author>
      <author><first>Fuki</first><last>Babasaki</last></author>
      <author><first>Jenny Tyler</first><last>Wang</last></author>
      <author><first>Nicholas</first><last>Derby</last></author>
      <pages>4820–4831</pages>
      <abstract>In this paper we describe the development of a text-to-speech system for Māori ‘Avaiki Nui (Cook Islands Māori). We provide details about the process of community-collaboration that was followed throughout the project, a continued engagement where we are trying to develop speech and language technology for the benefit of the community. During this process we gathered a group of recordings that we used to train a TTS system. When training we used two approaches, the HMM-system MaryTTS (Schröder et al., 2011) and the deep learning system FastSpeech2 (Ren et al., 2020). We performed two evaluation tasks on the models: First, we measured their quality by having the synthesized speech transcribed by ASR. The human produced ground truth had lower error rates (CER=4.3, WER=18), but the FastSpeech2 audio has lower error rates (CER=11.8 and WER=42.7) than the MaryTTS voice (CER=17.9 and WER=48.1). The second evaluation was a survey amongst speakers of the language so they could judge the voice’s quality. The ground truth was rated with the highest quality (MOS=4.6), but the FastSpeech2 voice had an overall quality of MOS=3.2, which was significantly higher than that of the MaryTTS synthesized recordings (MOS=2.0). We intend to use the FastSpeech2 model to create language learning tools for community members both on the Cook Islands and in the diaspora.</abstract>
      <url hash="a572c0c2">2024.lrec-main.432</url>
      <bibkey>james-etal-2024-development-community</bibkey>
    </paper>
    <paper id="433">
      <title><fixed-case>DG</fixed-case>o<fixed-case>T</fixed-case>: Dynamic Graph of Thoughts for Scientific Abstract Generation</title>
      <author><first>Xinyu</first><last>Ning</last></author>
      <author><first>Yutong</first><last>Zhao</last></author>
      <author><first>Yitong</first><last>Liu</last></author>
      <author><first>Hongwen</first><last>Yang</last></author>
      <pages>4832–4846</pages>
      <abstract>The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts. However, such models face problems of generalization and expensive training costs. The use of large language models (LLMs) to solve the task of generating paper abstracts saves the cost of model training. However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs. In this paper, we propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages of the existing GoT prompt approach, but also dynamically adjust the graph structure according to data characteristics while reducing model reasoning cost. Experimental results show that our method’s cost-effectiveness in abstract generation tasks is only 43.7% to 56.4% of other multi-round query prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.</abstract>
      <url hash="d6588568">2024.lrec-main.433</url>
      <bibkey>ning-etal-2024-dgot-dynamic</bibkey>
    </paper>
    <paper id="434">
      <title><fixed-case>DGS</fixed-case>-Fabeln-1: A Multi-Angle Parallel Corpus of Fairy Tales between <fixed-case>G</fixed-case>erman <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage and <fixed-case>G</fixed-case>erman Text</title>
      <author><first>Fabrizio</first><last>Nunnari</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Marco</first><last>González</last></author>
      <author><first>Anna</first><last>Hennes</last></author>
      <author><first>Patrick</first><last>Gebhard</last></author>
      <pages>4847–4857</pages>
      <abstract>We present the acquisition process and the data of DGS-Fabeln-1, a parallel corpus of German text and videos containing German fairy tales interpreted into the German Sign Language (DGS) by a native DGS signer. The corpus contains 573 segments of videos with a total duration of 1 hour and 32 minutes, corresponding with 1428 written sentences. It is the first corpus of semi-naturally expressed DGS that has been filmed from 7 angles, and one of the few sign language (SL) corpora globally which have been filmed from more than 3 angles and where the listener has been simultaneously filmed. The corpus aims at aiding research at SL linguistics, SL machine translation and affective computing, and is freely available for research purposes at the following address: https://doi.org/10.5281/zenodo.10822097.</abstract>
      <url hash="c4ccc252">2024.lrec-main.434</url>
      <bibkey>nunnari-etal-2024-dgs-fabeln</bibkey>
    </paper>
    <paper id="435">
      <title>Dialogue Systems Can Generate Appropriate Responses without the Use of Question Marks?– a Study of the Effects of “?” for Spoken Dialogue Systems –</title>
      <author><first>Tomoya</first><last>Mizumoto</last></author>
      <author><first>Takato</first><last>Yamazaki</last></author>
      <author><first>Katsumasa</first><last>Yoshikawa</last></author>
      <author><first>Masaya</first><last>Ohagi</last></author>
      <author><first>Toshiki</first><last>Kawamoto</last></author>
      <author><first>Toshinori</first><last>Sato</last></author>
      <pages>4858–4864</pages>
      <abstract>When individuals engage in spoken discourse, various phenomena can be observed that differ from those that are apparent in text-based conversation. While written communication commonly uses a question mark to denote a query, in spoken discourse, queries are frequently indicated by a rising intonation at the end of a sentence. However, numerous speech recognition engines do not append a question mark to recognized queries, presenting a challenge when creating a spoken dialogue system. Specifically, the absence of a question mark at the end of a sentence can impede the generation of appropriate responses to queries in spoken dialogue systems. Hence, we investigate the impact of question marks on dialogue systems, with the results showing that they have a significant impact. Moreover, we analyze specific examples in an effort to determine which types of utterances have the impact on dialogue systems.</abstract>
      <url hash="06a2ff17">2024.lrec-main.435</url>
      <bibkey>mizumoto-etal-2024-dialogue-systems</bibkey>
    </paper>
    <paper id="436">
      <title><fixed-case>D</fixed-case>ia<fixed-case>S</fixed-case>et: An Annotated Dataset of <fixed-case>A</fixed-case>rabic Conversations</title>
      <author><first>Abraham</first><last>Israeli</last></author>
      <author><first>Aviv</first><last>Naaman</last></author>
      <author><first>Guy</first><last>Maduel</last></author>
      <author><first>Rawaa</first><last>Makhoul</last></author>
      <author><first>Dana</first><last>Qaraeen</last></author>
      <author><first>Amir</first><last>Ejmail</last></author>
      <author><first>Dina</first><last>Lisnanskey</last></author>
      <author><first>Julian</first><last>Jubran</last></author>
      <author><first>Shai</first><last>Fine</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>4865–4876</pages>
      <abstract>We introduce DiaSet, a novel dataset of dialectical Arabic speech, manually transcribed and annotated for two specific downstream tasks: sentiment analysis and named entity recognition. The dataset encapsulates the Palestine dialect, predominantly spoken in Palestine, Israel, and Jordan. Our dataset incorporates authentic conversations between YouTube influencers and their respective guests. Furthermore, we have enriched the dataset with simulated conversations initiated by inviting participants from various locales within the said regions. The participants were encouraged to engage in dialogues with our interviewer. Overall, DiaSet consists of 644.8K tokens and 23.2K annotated instances. Uniform writing standards were upheld during the transcription process. Additionally, we established baseline models by leveraging some of the pre-existing Arabic BERT language models, showcasing the potential applications and efficiencies of our dataset. We make DiaSet publicly available for further research.</abstract>
      <url hash="8233d181">2024.lrec-main.436</url>
      <bibkey>israeli-etal-2024-diaset-annotated</bibkey>
    </paper>
    <paper id="437">
      <title>Did You Get It? A Zero-Shot Approach to Locate Information Transfers in Conversations</title>
      <author><first>Eliot</first><last>Maës</last></author>
      <author><first>Hossam</first><last>Boudraa</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Leonor</first><last>Becerra-Bonache</last></author>
      <pages>4877–4890</pages>
      <abstract>Interaction theories suggest that the emergence of mutual understanding between speakers in natural conversations depends on the construction of a shared knowledge base (<i>common ground</i>), but the details of which information and the circumstances under which it is memorized are not explained by any model. Previous works have looked at metrics derived from Information Theory to quantify the dynamics of information exchanged between participants, but do not provide an efficient way to locate information that will enter the common ground. We propose a new method based on the segmentation of a conversation into themes followed by their summarization. We then obtain the location of information transfers by computing the distance between the theme summary and the different utterances produced by a speaker. We evaluate two Large Language Models (LLMs) on this pipeline, on the French conversational corpus Paco-Cheese. More generally, we explore how the recent developments in the field of LLMs provide us with the means to implement these new methods and more generally support research into questions that usually heavily relies on human annotators.</abstract>
      <url hash="37f39356">2024.lrec-main.437</url>
      <bibkey>maes-etal-2024-get-zero</bibkey>
    </paper>
    <paper id="438">
      <title>Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction</title>
      <author><first>Unggi</first><last>Lee</last></author>
      <author><first>Sungjun</first><last>Yoon</last></author>
      <author><first>Joon Seo</first><last>Yun</last></author>
      <author><first>Kyoungsoo</first><last>Park</last></author>
      <author><first>YoungHoon</first><last>Jung</last></author>
      <author><first>Damji</first><last>Stratton</last></author>
      <author><first>Hyeoncheol</first><last>Kim</last></author>
      <pages>4891–4900</pages>
      <abstract>This paper presents novel techniques for enhancing the performance of knowledge tracing (KT) models by focusing on the crucial factor of question and concept difficulty level. Despite the acknowledged significance of difficulty, previous KT research has yet to exploit its potential for model optimization and has struggled to predict difficulty from unseen data. To address these problems, we propose a difficulty-centered contrastive learning method for KT models and a Large Language Model (LLM)-based framework for difficulty prediction. These innovative methods seek to improve the performance of KT models and provide accurate difficulty estimates for unseen data. Our ablation study demonstrates the efficacy of these techniques by demonstrating enhanced KT model performance. Nonetheless, the complex relationship between language and difficulty merits further investigation.</abstract>
      <url hash="a6c497d9">2024.lrec-main.438</url>
      <attachment type="OptionalSupplementaryMaterial" hash="58ba925e">2024.lrec-main.438.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lee-etal-2024-difficulty-focused</bibkey>
    </paper>
    <paper id="439">
      <title>Diffusion Based Counterfactual Augmentation for Dual Sentiment Classification</title>
      <author><first>Dancheng</first><last>Xin</last></author>
      <author><first>Jiawei</first><last>Yuan</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <pages>4901–4911</pages>
      <abstract>State-of-the-art NLP models have demonstrated exceptional performance across various tasks, including sentiment analysis. However, concerns have been raised about their robustness and susceptibility to systematic biases in both training and test data, which may lead to performance challenges when these models encounter out-of-distribution data in real-world applications. Although various data augmentation and adversarial perturbation techniques have shown promise in tackling these issues, prior methods such as word embedding perturbation or synonymous sentence expansion have failed to mitigate the spurious association problem inherent in the original data. Recent counterfactual augmentation methods have attempted to tackle this issue, but they have been limited by rigid rules, resulting in inconsistent context and disrupted semantics. In response to these challenges, we introduce a diffusion-based counterfactual data augmentation (DCA) framework. It utilizes an antonymous paradigm to guide the continuous diffusion model and employs reinforcement learning in combination with contrastive learning to optimize algorithms for generating counterfactual samples with high diversity and quality. Furthermore, we use a dual sentiment classifier to validate the generated antonymous samples and subsequently perform sentiment classification. Our experiments on four benchmark datasets demonstrate that DCA achieves state-of-the-art performance in sentiment classification tasks.</abstract>
      <url hash="76060e2e">2024.lrec-main.439</url>
      <bibkey>xin-etal-2024-diffusion-based</bibkey>
    </paper>
    <paper id="440">
      <title><fixed-case>D</fixed-case>iffusion<fixed-case>D</fixed-case>ialog: A Diffusion Model for Diverse Dialog Generation with Latent Space</title>
      <author><first>Jianxiang</first><last>Xiang</last></author>
      <author><first>Zhenhua</first><last>Liu</last></author>
      <author><first>Haodong</first><last>Liu</last></author>
      <author><first>Yin</first><last>Bai</last></author>
      <author><first>Jia</first><last>Cheng</last></author>
      <author><first>Wenliang</first><last>Chen</last></author>
      <pages>4912–4921</pages>
      <abstract>In real-life conversations, the content is diverse, and there exist one-to-many problems that require diverse generation. Previous studies attempted to introduce discrete or Gaussian-based latent variables to address the one-to-many problem, but the diversity is limited. Recently, diffusion models have made breakthroughs in computer vision and some attempts have been made in natural language processing. In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of diffusion model. In our approach, we introduce the continuous latent variables in the diffusion model instead of the discrete ones or VAE, which are often used in the previous studies. The problem of using discrete variables in dialog task is how to build a effective prior of latent space and inferring process to infer the proper latent given the context. Combining the encoder and latent-based diffusion model, we encode the latent of response in a continuous space as the prior instead of fixed Gaussian distribution in VAE or simply discrete ones, and we infer the latent by denoising step by step with diffusion model. The experimental results show that our model greatly enhance the diversity of dialog response while keeping the coherence. In further analysis, we find that our diffusion model achieved high inference efficiency which is the main challenge of applying diffusion model in natural language processing.</abstract>
      <url hash="814d7211">2024.lrec-main.440</url>
      <bibkey>xiang-etal-2024-diffusiondialog-diffusion</bibkey>
    </paper>
    <paper id="441">
      <title><fixed-case>D</fixed-case>im<fixed-case>A</fixed-case>: A Parameter-efficient Fine-tuning Method with Knowledge Transfer Based on Transformer</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Min</first><last>Huang</last></author>
      <author><first>Zhuoyang</first><last>Song</last></author>
      <author><first>Qinghai</first><last>Miao</last></author>
      <pages>4922–4934</pages>
      <abstract>Fine-tuning is a widely used technique for leveraging pre-trained language models (PLMs) in downstream tasks, but it can be computationally expensive and storage-intensive. To address this challenge, researchers have developed parameter-efficient methods that balance performance and resource cost. However, these methods often come with trade-offs like increased inference latency, token length usage, or limited adaptability for multitasking scenarios. This paper introduces a novel parameter-efficient method called DimA(Dimensionality Augmentation), which enhances the Transformer architecture by increasing the dimensionality. DimA achieves state-of-the-art results in GLUE and XSUM tasks while utilizing less than 1% of the original model’s parameters. Moreover, DimA introduces a novel approach to knowledge transfer that enables the simultaneous utilization of knowledge learned from multiple tasks to handle new tasks. This method significantly enhances the performance of the model on new tasks. Its versatility in model structure also enables its application to various Transformer-based models.</abstract>
      <url hash="2475dac3">2024.lrec-main.441</url>
      <bibkey>zhang-etal-2024-dima-parameter</bibkey>
    </paper>
    <paper id="442">
      <title>Disambiguating Homographs and Homophones Simultaneously: A Regrouping Method for <fixed-case>J</fixed-case>apanese</title>
      <author><first>Yo</first><last>Sato</last></author>
      <pages>4935–4939</pages>
      <abstract>We present a method that re-groups surface forms into clusters representing synonyms, and help disambiguate homographs as well as homophone. The method is applied post-hoc to trained contextual word embeddings. It is beneficial to languages where both homographs and homophones abound, which compromise the efficiency of language model and causes the underestimation problem in evaluation. Taking Japanese as an example, we evaluate how accurate such disambiguation can be, and how much the underestimation can be mitigated.</abstract>
      <url hash="5be6b16a">2024.lrec-main.442</url>
      <bibkey>sato-2024-disambiguating-homographs</bibkey>
    </paper>
    <paper id="443">
      <title><fixed-case>D</fixed-case>isco<fixed-case>G</fixed-case>e<fixed-case>M</fixed-case> 2.0: A Parallel Corpus of <fixed-case>E</fixed-case>nglish, <fixed-case>G</fixed-case>erman, <fixed-case>F</fixed-case>rench and <fixed-case>C</fixed-case>zech Implicit Discourse Relations</title>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Sarka</first><last>Zikanova</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>4940–4956</pages>
      <abstract>We present DiscoGeM 2.0, a crowdsourced, parallel corpus of 12,834 implicit discourse relations, with English, German, French and Czech data. We propose and validate a new single-step crowdsourcing annotation method and apply it to collect new annotations in German, French and Czech. The corpus was constructed by having crowdsourced annotators choose a suitable discourse connective for each relation from a set of unambiguous candidates. Every instance was annotated by 10 workers. Our corpus hence represents the first multi-lingual resource that contains distributions of discourse interpretations for implicit relations. The results show that the connective insertion method of discourse annotation can be reliably extended to other languages. The resulting multi-lingual annotations also reveal that implicit relations inferred in one language may differ from those inferred in the translation, meaning the annotations are not always directly transferable. DiscoGem 2.0 promotes the investigation of cross-linguistic differences in discourse marking and could improve automatic discourse parsing applications. It is openly downloadable here: https://github.com/merelscholman/DiscoGeM.</abstract>
      <url hash="62fb34a5">2024.lrec-main.443</url>
      <bibkey>yung-etal-2024-discogem-2</bibkey>
    </paper>
    <paper id="444">
      <title>Discourse Structure for the <fixed-case>M</fixed-case>inecraft Corpus</title>
      <author><first>Kate</first><last>Thompson</last></author>
      <author><first>Julie</first><last>Hunter</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <pages>4957–4967</pages>
      <abstract>We provide a new linguistic resource: The Minecraft Structured Dialogue Corpus (MSDC), a discourse annotated version of the Minecraft Dialogue Corpus (MDC; Narayan-Chen et al., 2019), with complete, situated discourse structures in the style of SDRT (Asher and Lascarides, 2003). Our structures feature both linguistic discourse moves and nonlinguistic actions. To show computational tractability, we train a discourse parser with a novel “2 pass architecture” on MSDC that gives excellent results on attachment prediction and relation labeling tasks especially long distance attachments.</abstract>
      <url hash="75f625df">2024.lrec-main.444</url>
      <bibkey>thompson-etal-2024-discourse-structure</bibkey>
    </paper>
    <paper id="445">
      <title>Discriminative Language Model as Semantic Consistency Scorer for Prompt-based Few-Shot Text Classification</title>
      <author><first>Zhipeng</first><last>Xie</last></author>
      <author><first>Yahe</first><last>Li</last></author>
      <pages>4968–4977</pages>
      <abstract>A successful prompt-based finetuning method should have three prerequisites: task compatibility, input compatibility, and evidence abundance. Bearing this belief in mind, this paper designs a novel prompt-based method (called DLM-SCS) for few-shot text classification, which utilizes the discriminative language model ELECTRA that is pretrained to distinguish whether a token is original or replaced. The method is built upon the intuitive idea that the prompt instantiated with the true label should have higher semantic consistency score than other prompts with false labels. Since a prompt usually consists of several components (or parts), its semantic consistency can be decomposed accordingly, which means each part can provide information for semantic consistency discrimination. The semantic consistency of each component is then computed by making use of the pretrained ELECTRA model, where no extra parameters get introduced. Extensive experiments have shown that our model outperforms several state-of-the-art prompt-based few-shot methods on 10 widely-used text classification tasks.</abstract>
      <url hash="ccc590da">2024.lrec-main.445</url>
      <bibkey>xie-li-2024-discriminative-language</bibkey>
    </paper>
    <paper id="446">
      <title>Disentangling Pretrained Representation to Leverage Low-Resource Languages in Multilingual Machine Translation</title>
      <author><first>Frederikus</first><last>Hudi</last></author>
      <author><first>Zhi</first><last>Qu</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>4978–4989</pages>
      <abstract>Multilingual neural machine translation aims to encapsulate multiple languages into a single model. However, it requires an enormous dataset, leaving the low-resource language (LRL) underdeveloped. As LRLs may benefit from shared knowledge of multilingual representation, we aspire to find effective ways to integrate unseen languages in a pre-trained model. Nevertheless, the intricacy of shared representation among languages hinders its full utilisation. To resolve this problem, we employed target language prediction and a central language-aware layer to improve representation in integrating LRLs. Focusing on improving LRLs in the linguistically diverse country of Indonesia, we evaluated five languages using a parallel corpus of 1,000 instances each, with experimental results measured by BLEU showing zero-shot improvement of 7.4 from the baseline score of 7.1 to a score of 15.5 at best. Further analysis showed that the gains in performance are attributed more to the disentanglement of multilingual representation in the encoder with the shift of the target language-specific representation in the decoder.</abstract>
      <url hash="47d9eb9e">2024.lrec-main.446</url>
      <bibkey>hudi-etal-2024-disentangling-pretrained</bibkey>
    </paper>
    <paper id="447">
      <title><fixed-case>DISRPT</fixed-case>: A Multilingual, Multi-domain, Cross-framework Benchmark for Discourse Processing</title>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Laura</first><last>Rivière</last></author>
      <author><first>Yang Janet</first><last>Liu</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Tatsuya</first><last>Aoyama</last></author>
      <pages>4990–5005</pages>
      <abstract>This paper presents DISRPT, a multilingual, multi-domain, and cross-framework benchmark dataset for discourse processing, covering the tasks of discourse unit segmentation, connective identification, and relation classification. DISRPT includes 13 languages, with data from 24 corpora covering about 4 millions tokens and around 250,000 discourse relation instances from 4 discourse frameworks: RST, SDRT, PDTB, and Discourse Dependencies. We present an overview of the data, its development across three NLP shared tasks on discourse processing carried out in the past five years, and the latest modifications and added extensions. We also carry out an evaluation of state-of-the-art multilingual systems trained on the data for each task, showing plateau performance on segmentation, but important room for improvement for connective identification and relation classification. The DISRPT benchmark employs a unified format that we make available on GitHub and HuggingFace in order to encourage future work on discourse processing across languages, domains, and frameworks.</abstract>
      <url hash="53b60c4e">2024.lrec-main.447</url>
      <bibkey>braud-etal-2024-disrpt-multilingual</bibkey>
    </paper>
    <paper id="448">
      <title>Distantly Supervised Contrastive Learning for Low-Resource Scripting Language Summarization</title>
      <author><first>Junzhe</first><last>Liang</last></author>
      <author><first>Haifeng</first><last>Sun</last></author>
      <author><first>Zirui</first><last>Zhuang</last></author>
      <author><first>Qi</first><last>Qi</last></author>
      <author><first>Jingyu</first><last>Wang</last></author>
      <author><first>Jianxin</first><last>Liao</last></author>
      <pages>5006–5017</pages>
      <abstract>Code summarization provides a natural language description for a given piece of code. In this work, we focus on scripting code—programming languages that interact with specific devices through commands. The low-resource nature of scripting languages makes traditional code summarization methods challenging to apply. To address this, we introduce a novel framework: distantly supervised contrastive learning for low-resource scripting language summarization. This framework leverages limited atomic commands and category constraints to enhance code representations. Extensive experiments demonstrate our method’s superiority over competitive baselines.</abstract>
      <url hash="6b871ed8">2024.lrec-main.448</url>
      <bibkey>liang-etal-2024-distantly-supervised</bibkey>
    </paper>
    <paper id="449">
      <title>Distillation with Explanations from Large Language Models</title>
      <author><first>Hanyu</first><last>Zhang</last></author>
      <author><first>Xiting</first><last>Wang</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Qing</first><last>He</last></author>
      <pages>5018–5028</pages>
      <abstract>Free-text explanations are crucial for enhancing the interpretability of AI models. However, training models to generate high-quality free-text explanations is challenging, primarily due to the requirement of a substantial amount of human-written explanations, which can be expensive. Recently, Large language models (LLMs) like ChatGPT and GPT-4 have made remarkable progress in various NLP tasks while also providing explanations alongside their answers. Leveraging LLMs for data labeling offers a more cost-effective alternative. However, a key concern arises from the fact that the answers provided by LLMs are not entirely accurate, potentially introducing noise to both task outputs and explanation generation. To remedy this, we propose a new mechanism, Distillation with Explanations from LLMs. we observe that despite the incorrectness in LLMs-generated answers, their explanations are consistent with their answers. Leveraging this consistency, our method combines the ground truth labels and answers-explanations generated by LLMs, to simultaneously generate more accurate answers and the corresponding free-text explanations. Experimental results demonstrate that our approach achieves improved predictive performance and also generates explanations that exhibit greater alignment with the model’s task outputs.</abstract>
      <url hash="58b0c21e">2024.lrec-main.449</url>
      <bibkey>zhang-etal-2024-distillation-explanations</bibkey>
    </paper>
    <paper id="450">
      <title>Distill, Fuse, Pre-train: Towards Effective Event Causality Identification with Commonsense-Aware Pre-trained Model</title>
      <author><first>Peixin</first><last>Huang</last></author>
      <author><first>Xiang</first><last>Zhao</last></author>
      <author><first>Minghao</first><last>Hu</last></author>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Weidong</first><last>Xiao</last></author>
      <pages>5029–5040</pages>
      <abstract>Event Causality Identification (ECI) aims to detect causal relations between events in unstructured texts. This task is challenged by the lack of data and explicit causal clues. Some methods incorporate explicit knowledge from external knowledge graphs (KGs) into Pre-trained Language Models (PLMs) to tackle these issues, achieving certain accomplishments. However, they ignore that existing KGs usually contain trivial knowledge which may prejudice the performance. Moreover, they simply integrate the concept triplets, underutilizing the deep interaction between the text and external graph. In this paper, we propose an effective pipeline DFP, i.e., Distill, Fuse and Pre-train, to build a commonsense-aware pre-trained model which integrates reliable task-specific knowledge from commonsense graphs. This pipeline works as follows: (1) To leverage the reliable knowledge, commonsense graph distillation is proposed to distill commonsense graphs and obtain the meta-graph which contain credible task-oriented knowledge. (2) To model the deep interaction between the text and external graph, heterogeneous information fusion is proposed to fuse them through a commonsense-aware memory network. (3) Continual pre-training designs three continual pre-training tasks to further align and fuse the text and the commonsense meta-graph. Through extensive experiments on two benchmarks, we demonstrate the validity of our pipeline.</abstract>
      <url hash="4e7a1efb">2024.lrec-main.450</url>
      <bibkey>huang-etal-2024-distill-fuse</bibkey>
    </paper>
    <paper id="451">
      <title>Distilling Causal Effect of Data in Continual Few-shot Relation Learning</title>
      <author><first>Weihang</first><last>Ye</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Gao</last></author>
      <author><first>Moyao</first><last>Wang</last></author>
      <pages>5041–5051</pages>
      <abstract>Continual Few-Shot Relation Learning (CFRL) aims to learn an increasing number of new relational patterns from a data stream. However, due to the limited number of samples and the continual training mode, this method frequently encounters the catastrophic forgetting issues. The research on causal inference suggests that this issue is caused by the loss of causal effects from old data during the new training process. Inspired by the causal graph, we propose a unified causal framework for CFRL to restore the causal effects. Specifically, we establish two additional causal paths from old data to predictions by having the new data and memory data collide with old data separately in the old feature space. This augmentation allows us to preserve causal effects effectively and enhance the utilization of valuable information within memory data, thereby alleviating the phenomenon of catastrophic forgetting. Furthermore, we introduce a self-adaptive weight to achieve a delicate balance of causal effects between the new and old relation types. Extensive experiments demonstrate the superiority of our method over existing state-of-the-art approaches in CFRL task settings. Our codes are publicly available at: https://github.com/ywh140/CECF.</abstract>
      <url hash="a555d715">2024.lrec-main.451</url>
      <bibkey>ye-etal-2024-distilling-causal</bibkey>
    </paper>
    <paper id="452">
      <title>Distractor Generation Using Generative and Discriminative Capabilities of Transformer-based Models</title>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Luca</first><last>Benedetto</last></author>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>5052–5063</pages>
      <abstract>Multiple Choice Questions (MCQs) are very common in both high-stakes and low-stakes examinations, and their effectiveness in assessing students relies on the quality and diversity of distractors, which are the incorrect answer options provided alongside the correct answer. Motivated by the progress in generative language models, we propose a two-step automatic distractor generation approach which is based on text to text transfer transformer models. Unlike most of previous methods for distractor generation, our approach does not rely on the correct answer options. Instead, it first generates both correct and incorrect answer options, and then discriminates potential correct options from distractors. Identified distractors are finally categorised based on semantic similarity scores into separate clusters, and the cluster heads are selected as our final distinct distractors. Experiments on two publicly available datasets show that our approach outperforms previous models both in the case of single-word answer options and longer-sequence reading comprehension questions.</abstract>
      <url hash="e61e1918">2024.lrec-main.452</url>
      <bibkey>taslimipoor-etal-2024-distractor-generation</bibkey>
    </paper>
    <paper id="453">
      <title>Distribution Aware Metrics for Conditional Natural Language Generation</title>
      <author><first>David M.</first><last>Chan</last></author>
      <author><first>Yiming</first><last>Ni</last></author>
      <author><first>David</first><last>Ross</last></author>
      <author><first>Sudheendra</first><last>Vijayanarasimhan</last></author>
      <author><first>Austin</first><last>Myers</last></author>
      <author><first>John</first><last>Canny</last></author>
      <pages>5064–5095</pages>
      <abstract>Traditional automated metrics for evaluating conditional natural language generation rely on pairwise comparisons between a single generated text and the best-matching gold-standard reference. This method is effective when ground truth data diversity can be attributed to noise, however, it falls short when diversity in references holds valuable contextual information, as in visual description or summarization, as it does not evaluate the ability of a model to generate text matching the diversity of the ground truth samples. In this paper, we challenge the adequacy of existing metrics in such semantically diverse contexts and introduce a novel approach for evaluating conditional language generation models, leveraging a family of meta-metrics that build on existing pairwise distance functions. These meta-metrics assess not just single-samples, but distributions of reference and model-generated captions using small sample sets. We demonstrate our approach through a case study of visual description in the English language which reveals not only how current models prioritize single-description quality over diversity, but further sheds light on the impact of sampling methods and temperature settings on description quality and diversity.</abstract>
      <url hash="788a0795">2024.lrec-main.453</url>
      <bibkey>chan-etal-2024-distribution-aware</bibkey>
    </paper>
    <paper id="454">
      <title>Diversifying Question Generation over Knowledge Base via External Natural Questions</title>
      <author><first>Shasha</first><last>Guo</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Xirui</first><last>Ke</last></author>
      <author><first>Cuiping</first><last>Li</last></author>
      <author><first>Hong</first><last>Chen</last></author>
      <pages>5096–5108</pages>
      <abstract>Previous methods on knowledge base question generation (KBQG) primarily focus on refining the quality of a single generated question. However, considering the remarkable paraphrasing ability of humans, we believe that diverse texts can express identical semantics through varied expressions. The above insights make diversifying question generation an intriguing task, where the first challenge is evaluation metrics for diversity. Current metrics inadequately assess the aforementioned diversity. They calculate the ratio of unique n-grams in the generated question, which tends to measure duplication rather than true diversity. Accordingly, we devise a new diversity evaluation metric, which measures the diversity among top-k generated questions for each instance while ensuring their relevance to the ground truth. Clearly, the second challenge is how to enhance diversifying question generation. To address this challenge, we introduce a dual model framework interwoven by two selection strategies to generate diverse questions leveraging external natural questions. The main idea of our dual framework is to extract more diverse expressions and integrate them into the generation model to enhance diversifying question generation. Extensive experiments on widely used benchmarks for KBQG show that our approach can outperform pre-trained language model baselines and text-davinci-003 in diversity while achieving comparable performance with ChatGPT.</abstract>
      <url hash="e0c2c665">2024.lrec-main.454</url>
      <bibkey>guo-etal-2024-diversifying-question</bibkey>
    </paper>
    <paper id="455">
      <title><fixed-case>DMON</fixed-case>: A Simple Yet Effective Approach for Argument Structure Learning</title>
      <author><first>Sun</first><last>Wei</last></author>
      <author><first>Mingxiao</first><last>Li</last></author>
      <author><first>Jingyuan</first><last>Sun</last></author>
      <author><first>Jesse</first><last>Davis</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>5109–5118</pages>
      <abstract>Argument structure learning (ASL) entails predicting relations between arguments. Because it can structure a document to facilitate its understanding, it has been widely applied in many fields (medical, commercial, and scientific domains). Despite its broad utilization, ASL remains a challenging task because it involves examining the complex relationships between the sentences in a potentially unstructured discourse. To resolve this problem, we have developed a simple yet effective approach called Dual-tower Multi-scale cOnvolution neural Network (DMON) for the ASL task. Specifically, we organize arguments into a relationship matrix that together with the argument embeddings forms a relationship tensor and design a mechanism to capture relations with contextual arguments. Experimental results on three different-domain argument mining datasets demonstrate that our framework outperforms state-of-the-art models. We will release the code after paper acceptance.</abstract>
      <url hash="0b4c0f9d">2024.lrec-main.455</url>
      <bibkey>wei-etal-2024-dmon-simple</bibkey>
    </paper>
    <paper id="456">
      <title><fixed-case>D</fixed-case>oc2<fixed-case>S</fixed-case>oar<fixed-case>G</fixed-case>raph: Discrete Reasoning over Visually-Rich Table-Text Documents via Semantic-Oriented Hierarchical Graphs</title>
      <author><first>Fengbin</first><last>Zhu</last></author>
      <author><first>Chao</first><last>Wang</last></author>
      <author><first>Fuli</first><last>Feng</last></author>
      <author><first>Zifeng</first><last>Ren</last></author>
      <author><first>Moxin</first><last>Li</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>5119–5131</pages>
      <abstract>Table-text document (e.g., financial reports) understanding has attracted increasing attention in recent two years. TAT-DQA is a realistic setting for the understanding of visually-rich table-text documents, which involves answering associated questions requiring discrete reasoning. Most existing work relies on token-level semantics, falling short in the reasoning across document elements such as quantities and dates. To address this limitation, we propose a novel Doc2SoarGraph model that exploits element-level semantics and employs Semantic-oriented hierarchical Graph structures to capture the differences and correlations among different elements within the given document and question. Extensive experiments on the TAT-DQA dataset reveal that our model surpasses the state-of-the-art conventional method (i.e., MHST) and large language model (i.e., ChatGPT) by 17.73 and 6.49 points respectively in terms of Exact Match (EM) metric, demonstrating exceptional effectiveness.</abstract>
      <url hash="2bc95598">2024.lrec-main.456</url>
      <bibkey>zhu-etal-2024-doc2soargraph-discrete</bibkey>
    </paper>
    <paper id="457">
      <title><fixed-case>DOC</fixed-case>-<fixed-case>RAG</fixed-case>: <fixed-case>ASR</fixed-case> Language Model Personalization with Domain-Distributed Co-occurrence Retrieval Augmentation</title>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Zhe</first><last>Liu</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Yingyi</first><last>Ma</last></author>
      <author><first>Gil</first><last>Karen</last></author>
      <author><first>Zeeshan</first><last>Ahmed</last></author>
      <author><first>Dinesh</first><last>Manocha</last></author>
      <author><first>Xuedong</first><last>Zhang</last></author>
      <pages>5132–5139</pages>
      <abstract>We propose DOC-RAG - Domain-distributed Co-occurrence Retrieval Augmentation for ASR language model personalization aiming to improve the automatic speech recognition of rare word patterns in unseen domains. Our approach involves contrastively training a document retrieval module to rank external knowledge domains based on their semantic similarity with respect to the input query. We further use n-gram co-occurrence distribution to recognize rare word patterns associated with specific domains. We aggregate the next word probability distribution based on the relative importance of different domains. Extensive experiments on three user-specific speech-to-text tasks for meetings, TED talks, and financial earnings calls show that DOC-RAG significantly outperforms strong baselines with an 8-15% improvement in terms of perplexity and a 4-7% reduction in terms of Word Error Rates in various settings.</abstract>
      <url hash="3d1c700c">2024.lrec-main.457</url>
      <bibkey>mathur-etal-2024-doc-rag</bibkey>
    </paper>
    <paper id="458">
      <title><fixed-case>D</fixed-case>oc<fixed-case>S</fixed-case>cript: Document-level Script Event Prediction</title>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Vlad I.</first><last>Morariu</last></author>
      <author><first>Aparna</first><last>Garimella</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Jiuxiang</first><last>Gu</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Dinesh</first><last>Manocha</last></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <pages>5140–5155</pages>
      <abstract>We present a novel task of document-level script event prediction, which aims to predict the next event given a candidate list of narrative events in long-form documents. To enable this, we introduce DocSEP, a challenging dataset in two new domains - contractual documents and Wikipedia articles, where timeline events may be paragraphs apart and may require multi-hop temporal and causal reasoning. We benchmark existing baselines and present a novel architecture called DocScript to learn sequential ordering between events at the document scale. Our experimental results on the DocSEP dataset demonstrate that learning longer-range dependencies between events is a key challenge and show that contemporary LLMs such as ChatGPT and FlanT5 struggle to solve this task, indicating their lack of reasoning abilities for understanding causal relationships and temporal sequences within long texts.</abstract>
      <url hash="0ce8b1bd">2024.lrec-main.458</url>
      <bibkey>mathur-etal-2024-docscript-document</bibkey>
    </paper>
    <paper id="459">
      <title>Document-Level Event Extraction via Information Interaction Based on Event Relation and Argument Correlation</title>
      <author><first>Bangze</first><last>Pan</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Suge</first><last>Wang</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Deyu</first><last>Li</last></author>
      <author><first>Jian</first><last>Liao</last></author>
      <author><first>Jianxing</first><last>Zheng</last></author>
      <pages>5156–5166</pages>
      <abstract>Document-level Event Extraction (DEE) is a vital task in NLP as it seeks to automatically recognize and extract event information from a document. However, current approaches often overlook intricate relationships among events and subtle correlations among arguments within a document, which can significantly impact the effectiveness of event type recognition and the extraction of cross-sentence arguments in DEE task. This paper proposes a novel Correlation Association Interactive Network (CAINet), comprising two key components: event relationship graph and argument correlation graph. In particular, the event relationship graph models the relationship among various events through structural associations among event nodes and sentence nodes, to improve the accuracy of event recognition. On the other hand, the arguments correlation graph models the correlations among arguments by quantifying the strength of association among arguments, to effectively aggregate cross-sentence arguments, contributing to the overall success of DEE. Furthermore, we use the large language model to execute DEE task experiments. Experimental results show the proposed CAINet outperforms existing state-of-the-art models and large language models in terms of F1-score across two benchmark datasets.</abstract>
      <url hash="e2bf9c95">2024.lrec-main.459</url>
      <bibkey>pan-etal-2024-document-level</bibkey>
    </paper>
    <paper id="460">
      <title>Document Set Expansion with Positive-Unlabeled Learning Using Intractable Density Estimation</title>
      <author><first>Haiyang</first><last>Zhang</last></author>
      <author><first>Qiuyi</first><last>Chen</last></author>
      <author><first>Yanjie</first><last>Zou</last></author>
      <author><first>Jia</first><last>Wang</last></author>
      <author><first>Yushan</first><last>Pan</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>5167–5173</pages>
      <abstract>The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents. Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task. However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection. To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models. Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE. Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE.</abstract>
      <url hash="beec81b4">2024.lrec-main.460</url>
      <bibkey>zhang-etal-2024-document-set</bibkey>
    </paper>
    <paper id="461">
      <title>Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study</title>
      <author><first>Peiyu</first><last>Liu</last></author>
      <author><first>Zikang</first><last>Liu</last></author>
      <author><first>Ze-Feng</first><last>Gao</last></author>
      <author><first>Dawei</first><last>Gao</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Bolin</first><last>Ding</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>5174–5190</pages>
      <abstract>Despite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on <i>emergent abilities</i>, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs.</abstract>
      <url hash="d83f3033">2024.lrec-main.461</url>
      <bibkey>liu-etal-2024-emergent-abilities</bibkey>
    </paper>
    <paper id="462">
      <title>Does <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Know That It Does Not Know? Evaluating the Black-Box Calibration of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>Youliang</first><last>Yuan</last></author>
      <author><first>Wenxuan</first><last>Wang</last></author>
      <author><first>Qingshuo</first><last>Guo</last></author>
      <author><first>Yiming</first><last>Xiong</last></author>
      <author><first>Chihao</first><last>Shen</last></author>
      <author><first>Pinjia</first><last>He</last></author>
      <pages>5191–5201</pages>
      <abstract>Recently, ChatGPT has demonstrated remarkable performance in various downstream tasks such as open-domain question answering, machine translation, and code generation. As a general-purpose task solver, an intriguing inquiry arises: Does ChatGPT itself know that it does not know, without any access to internal states? In response to this query, we present an initial evaluation of ChatGPT for black-box calibration. We designed three types of proxy confidence, from three perspectives to assess its performance. Experiments are conducted on five datasets, spanning four tasks, and the results show that ChatGPT has a degree of capability for black-box calibration. Specifically, proxy confidence displayed a significantly positive Pearson correlation (95.16%) with accuracy in the TruthfulQA dataset, while revealing a negative correlation in the ModAr dataset. We delved deeper into ChatGPT’s black-box calibration ability by examining failure cases in the ModAr dataset. Our analysis revealed that ChatGPT’s tendency to exhibit overconfidence may stem from its reliance on semantic priors. Furthermore, we investigated why ChatGPT performs relatively well in TruthfulQA. The findings suggest that ChatGPT might implicitly acquire calibration skills during the reinforcement learning process, rather than relying solely on simplistic heuristics.</abstract>
      <url hash="897ed7a5">2024.lrec-main.462</url>
      <bibkey>yuan-etal-2024-chatgpt-know</bibkey>
    </paper>
    <paper id="463">
      <title>Does the Generator Mind Its Contexts? An Analysis of Generative Model Faithfulness under Context Transfer</title>
      <author><first>Xinshuo</first><last>Hu</last></author>
      <author><first>Dongfang</first><last>Li</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Yuxiang</first><last>Wu</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <pages>5202–5211</pages>
      <abstract>he present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations. To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives.</abstract>
      <url hash="a6f6d92a">2024.lrec-main.463</url>
      <bibkey>hu-etal-2024-generator-mind</bibkey>
    </paper>
    <paper id="464">
      <title>Does the Language Matter? Curriculum Learning over Neo-<fixed-case>L</fixed-case>atin Languages</title>
      <author><first>Giulia</first><last>Pucci</last></author>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <pages>5212–5220</pages>
      <abstract>Curriculum Learning (CL) is emerging as a relevant technique to reduce the cost of pre-training Large Language Models. The idea, tested for the English language, is to train LLMs by organizing training examples from the simplest to the most complex. Complexity measures may depend on the specific language. Hence, this paper aims to investigate whether CL and the complexity measure can be easily exported to other languages. For this reason, we present a set of linguistically motivated measures to determine the complexity of examples, which has been used in English: these measures are based on text length, rarity, and comprehensibility. We then test the approach to two Romance languages: Italian and French. Our results show that the technique can be easily exported to languages other than English without adaptation.</abstract>
      <url hash="8d23ec5e">2024.lrec-main.464</url>
      <bibkey>pucci-ranaldi-2024-language-matter</bibkey>
    </paper>
    <paper id="465">
      <title>Do Language Models Care about Text Quality? Evaluating Web-Crawled Corpora across 11 Languages</title>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Taja</first><last>Kuzman</last></author>
      <author><first>Peter</first><last>Rupnik</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>5221–5234</pages>
      <abstract>Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion’s share of the training data in virtually all recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However, despite this importance, relatively little attention has been given to the quality of these corpora. In this paper, we compare four of the currently most relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across eleven lower-resourced European languages. Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks. We find that there are clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining the best results. However, during the extrinsic evaluation, we actually find that the CC100 corpus achieves the highest scores. We conclude that, in our experiments, the quality of the web-crawled corpora does not seem to play a significant role when training LMs.</abstract>
      <url hash="eccd69a5">2024.lrec-main.465</url>
      <bibkey>van-noord-etal-2024-language-models</bibkey>
    </paper>
    <paper id="466">
      <title>Do Large Language Models Understand Mansplaining? Well, Actually...</title>
      <author><first>Carla</first><last>Perez Almendros</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>5235–5246</pages>
      <abstract>Gender bias has been widely studied by the NLP community. However, other more subtle variations of it, such as mansplaining, have yet received little attention. Mansplaining is a discriminatory behaviour that consists of a condescending treatment or discourse towards women. In this paper, we introduce and analyze Well, actually..., a corpus of 886 mansplaining stories experienced by women. We analyze the corpus in terms of features such as offensiveness, sentiment or misogyny, among others. We also explore to what extent Large Language Models (LLMs) can understand and identify mansplaining and other gender-related microaggressions. Specifically, we experiment with ChatGPT-3.5-Turbo and LLaMA-2 (13b and 70b), with both targeted and open questions. Our findings suggest that, although they can identify mansplaining to some extent, LLMs still struggle to point out this attitude and will even reproduce some of the social patterns behind mansplaining situations, for instance by praising men for giving unsolicited advice to women.</abstract>
      <url hash="5db33564">2024.lrec-main.466</url>
      <bibkey>perez-almendros-camacho-collados-2024-large-language</bibkey>
    </paper>
    <paper id="467">
      <title>Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling</title>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <pages>5247–5259</pages>
      <abstract>Recent studies have demonstrated that the ability of dense retrieval models to generalize to target domains with different distributions is limited, which contrasts with the results obtained with interaction-based models. Prior attempts to mitigate this challenge involved leveraging adversarial learning and query generation approaches, but both approaches nevertheless resulted in limited improvements. In this paper, we propose to combine the query-generation approach with a self-supervision approach in which pseudo-relevance labels are automatically generated on the target domain. To accomplish this, a T5-3B model is utilized for pseudo-positive labeling, and meticulous hard negatives are chosen. We also apply this strategy on conversational dense retrieval model for conversational search. A similar pseudo-labeling approach is used, but with the addition of a query-rewriting module to rewrite conversational queries for subsequent labeling. This proposed approach enables a model’s domain adaptation with real queries and documents from the target dataset. Experiments on standard dense retrieval and conversational dense retrieval models both demonstrate improvements on baseline models when they are fine-tuned on the pseudo-relevance labeled data.</abstract>
      <url hash="2b4b2821">2024.lrec-main.467</url>
      <bibkey>li-gaussier-2024-domain-adaptation</bibkey>
    </paper>
    <paper id="468">
      <title>Domain-Agnostic Adapter Architecture for Deception Detection: Extensive Evaluations with the <fixed-case>DIF</fixed-case>rau<fixed-case>D</fixed-case> Benchmark</title>
      <author><first>Dainis A.</first><last>Boumber</last></author>
      <author><first>Fatima Zahra</first><last>Qachfar</last></author>
      <author><first>Rakesh</first><last>Verma</last></author>
      <pages>5260–5274</pages>
      <abstract>Despite significant strides in training expansive transformer models, their deployment for niche tasks remains intricate. This paper delves into deception detection, assessing domain adaptation methodologies from a cross-domain lens using transformer Large Language Models (LLMs). We roll out a new corpus with roughly 100,000 honest and misleading statements in seven domains, designed to serve as a benchmark for multidomain deception detection. As a primary contribution, we present a novel parameter-efficient finetuning adapter, PreXIA, which was proposed and implemented as part of this work. The design is model-, domain- and task-agnostic, with broad applications that are not limited by the confines of deception or classification tasks. We comprehensively analyze and rigorously evaluate LLM tuning methods and our original design using the new benchmark, highlighting their strengths, pointing out weaknesses, and suggesting potential areas for improvement. The proposed adapter consistently outperforms all competition on the DIFrauD benchmark used in this study. To the best of our knowledge, it improves on the state-of-the-art in its class for the deception task. In addition, the evaluation process leads to unexpected findings that, at the very least, cast doubt on the conclusions made in some of the recently published research regarding reasoning ability’s unequivocal dominance over representations quality with respect to the relative contribution of each one to a model’s performance and predictions.</abstract>
      <url hash="ea52dddb">2024.lrec-main.468</url>
      <bibkey>boumber-etal-2024-domain-agnostic</bibkey>
    </paper>
    <paper id="469">
      <title>Domain-aware and Co-adaptive Feature Transformation for Domain Adaption Few-shot Relation Extraction</title>
      <author><first>Yijun</first><last>Liu</last></author>
      <author><first>Feifei</first><last>Dai</last></author>
      <author><first>Xiaoyan</first><last>Gu</last></author>
      <author><first>Minghui</first><last>Zhai</last></author>
      <author id="bo-li-cas"><first>Bo</first><last>Li</last></author>
      <author><first>Meiou</first><last>Zhang</last></author>
      <pages>5275–5285</pages>
      <abstract>Few-shot relation extraction (FSRE) can alleviate the data scarcity problem in relation extraction. However, FSRE models often suffer a significant decline in performance when adapting to new domains. To overcome this issue, many researchers have focused on domain adaption FSRE (DAFSRE). Nevertheless, existing approaches primarily concentrate on the source domain, which makes it difficult to accurately transfer useful knowledge to the target domain. Additionally, the lack of distinction between relations further restricts the model performance. In this paper, we propose the domain-aware and co-adaptive feature transformation approach to address these issues. Specifically, we introduce a domain-aware transformation module that leverages the target domain distribution features to guide the domain-aware feature transformations. This can enhance the model’s adaptability across domains, leading to improved target domain performance. Furthermore, we design co-adaptive prototypical networks to perform co-adaptive feature transformation through a transformer mechanism. This results in more robust and distinguishable relation prototypes. Experiments on DAFSRE benchmark datasets demonstrate the effectiveness of our method, which outperforms existing models and achieves state-of-the-art performance.</abstract>
      <url hash="06ba1506">2024.lrec-main.469</url>
      <bibkey>liu-etal-2024-domain-aware</bibkey>
    </paper>
    <paper id="470">
      <title>Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis</title>
      <author><first>Siyin</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Qin</first><last>Chen</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>5286–5298</pages>
      <abstract>Domain adaption has been widely adapted for cross-domain sentiment analysis to transfer knowledge from the source domain to the target domain. Whereas, most methods are proposed under the assumption that the target (test) domain is known, making them fail to generalize well on unknown test data that is not always available in practice. In this paper, we focus on the problem of domain generalization for cross-domain sentiment analysis. Specifically, we propose a backdoor adjustment-based causal model to disentangle the domain-specific and domain-invariant representations that play essential roles in tackling domain shift. First, we rethink the cross-domain sentiment analysis task in a causal view to model the causal-and-effect relationships among different variables. Then, to learn an invariant feature representation, we remove the effect of domain confounders (e.g., domain knowledge) using the backdoor adjustment. A series of experiments over many homologous and diverse datasets show the great performance and robustness of our model by comparing it with the state-of-the-art domain generalization baselines.</abstract>
      <url hash="1cecbb8e">2024.lrec-main.470</url>
      <bibkey>wang-etal-2024-domain-generalization</bibkey>
    </paper>
    <paper id="471">
      <title>Domain Transferable Semantic Frames for Expert Interview Dialogues</title>
      <author><first>Taishi</first><last>Chika</last></author>
      <author><first>Taro</first><last>Okahisa</last></author>
      <author><first>Takashi</first><last>Kodama</last></author>
      <author><first>Yin Jou</first><last>Huang</last></author>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>5299–5308</pages>
      <abstract>Interviews are an effective method to elicit critical skills to perform particular processes in various domains. In order to understand the knowledge structure of these domain-specific processes, we consider semantic role and predicate annotation based on Frame Semantics. We introduce a dataset of interview dialogues with experts in the culinary and gardening domains, each annotated with semantic frames. This dataset consists of (1) 308 interview dialogues related to the culinary domain, originally assembled by Okahisa et al. (2022), and (2) 100 interview dialogues associated with the gardening domain, which we newly acquired. The labeling specifications take into account the domain-transferability by adopting domain-agnostic labels for frame elements. In addition, we conducted domain transfer experiments from the culinary domain to the gardening domain to examine the domain transferability with our dataset. The experimental results showed the effectiveness of our domain-agnostic labeling scheme.</abstract>
      <url hash="0e6a7255">2024.lrec-main.471</url>
      <bibkey>chika-etal-2024-domain-transferable</bibkey>
    </paper>
    <paper id="472">
      <title>Do Neural Language Models Inferentially Compose Concepts the Way Humans Can?</title>
      <author><first>Amilleah</first><last>Rodriguez</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Liina</first><last>Pylkkänen</last></author>
      <pages>5309–5314</pages>
      <abstract>While compositional interpretation is the core of language understanding, humans also derive meaning via inference. For example, while the phrase “the blue hat” introduces a blue hat into the discourse via the direct composition of “blue” and “hat,” the same discourse entity is introduced by the phrase “the blue color of this hat” despite the absence of any local composition between “blue” and “hat.” Instead, we infer that if the color is blue and it belongs to the hat, the hat must be blue. We tested the performance of neural language models and humans on such inferentially driven conceptual compositions, eliciting probability estimates for a noun in a minimally composed phrase, “This blue hat”, following contexts that had introduced the conceptual combinations of those nouns and adjectives either syntactically or inferentially. Surprisingly, our findings reveal significant disparities between the performance of neural language models and human judgments. Among the eight models evaluated, RoBERTa, BERT-large, and GPT-2 exhibited the closest resemblance to human responses, while other models faced challenges in accurately identifying compositions in the provided contexts. Our study reveals that language models and humans may rely on different approaches to represent and compose lexical items across sentence structure. All data and code are accessible at https://github.com/wangshaonan/BlueHat.</abstract>
      <url hash="2cc1067f">2024.lrec-main.472</url>
      <bibkey>rodriguez-etal-2024-neural-language</bibkey>
    </paper>
    <paper id="473">
      <title><fixed-case>DORE</fixed-case>: A Dataset for <fixed-case>P</fixed-case>ortuguese Definition Generation</title>
      <author><first>Anna Beatriz</first><last>Dimas Furtado</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Frederic</first><last>Blain</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>5315–5322</pages>
      <abstract>Definition modelling (DM) is the task of automatically generating a dictionary definition of a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for <b>D</b>efinition M<b>O</b>delling for Po<b>R</b>tugu<b>E</b>se containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings of this paper will facilitate research and study of Portuguese in wider contexts.</abstract>
      <url hash="e73630aa">2024.lrec-main.473</url>
      <bibkey>dimas-furtado-etal-2024-dore-dataset</bibkey>
    </paper>
    <paper id="474">
      <title><fixed-case>DOSA</fixed-case>: A Dataset of Social Artifacts from Different <fixed-case>I</fixed-case>ndian Geographical Subcultures</title>
      <author><first>Agrima</first><last>Seth</last></author>
      <author><first>Sanchit</first><last>Ahuja</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <pages>5323–5337</pages>
      <abstract>Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce DOSA, the first community-generated Dataset of 615 Social Artifacts, by engaging with 260 participants from 19 different Indian geographic subcultures. We use a gamified framework that relies on collective sensemaking to collect the names and descriptions of these artifacts such that the descriptions semantically align with the shared sensibilities of the individuals from those cultures. Next, we benchmark four popular LLMs and find that they show significant variation across regional sub-cultures in their ability to infer the artifacts.</abstract>
      <url hash="be582caa">2024.lrec-main.474</url>
      <bibkey>seth-etal-2024-dosa-dataset</bibkey>
    </paper>
    <paper id="475">
      <title><fixed-case>DP</fixed-case>-<fixed-case>CRE</fixed-case>: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation</title>
      <author><first>Mengyi</first><last>Huang</last></author>
      <author><first>Meng</first><last>Xiao</last></author>
      <author><first>Ludi</first><last>Wang</last></author>
      <author><first>Yi</first><last>Du</last></author>
      <pages>5338–5349</pages>
      <abstract>Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data. Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across two datasets.</abstract>
      <url hash="75886d47">2024.lrec-main.475</url>
      <bibkey>huang-etal-2024-dp-cre</bibkey>
    </paper>
    <paper id="476">
      <title>Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering</title>
      <author><first>Yuan</first><last>Gao</last></author>
      <author><first>Yiheng</first><last>Zhu</last></author>
      <author><first>Yuanbin</first><last>Cao</last></author>
      <author><first>Yinzhi</first><last>Zhou</last></author>
      <author><first>Zhen</first><last>Wu</last></author>
      <author><first>Yujie</first><last>Chen</last></author>
      <author><first>Shenglan</first><last>Wu</last></author>
      <author><first>Haoyuan</first><last>Hu</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <pages>5350–5364</pages>
      <abstract>Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources. Recently, Large Language Models (LLMs) have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, reasoning, and utilizing tools. However, LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance. To alleviate this issue, we propose the Discriminate→Re-Compose→Re- Solve→Re-Decompose (Dr3) mechanism. Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to judge whether the generated answers are off-topic. In cases where an off-topic answer is detected, the Corrector performs step-wise revisions along the reversed reasoning chain (Re-Compose→Re-Solve→Re-Decompose) until the final answer becomes on-topic. Experimental results on the HotpotQA and 2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving the performance in Exact Match (EM) by nearly 3% compared to the baseline method without the Dr3 mechanism.</abstract>
      <url hash="6b168926">2024.lrec-main.476</url>
      <bibkey>gao-etal-2024-dr3-ask</bibkey>
    </paper>
    <paper id="477">
      <title><fixed-case>DRAMA</fixed-case>: Dynamic Multi-Granularity Graph Estimate Retrieval over Tabular and Textual Question Answering</title>
      <author><first>Ruize</first><last>Yuan</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Li</first><last>Zeng</last></author>
      <author><first>Qing</first><last>He</last></author>
      <pages>5365–5375</pages>
      <abstract>The TableTextQA task requires finding the answer to the question from a combination of tabular and textual data, which has been gaining increasing attention. The row-based approaches have demonstrated remarkable effectiveness. However, they suffer from the following limitations: (1) a lack of interaction between rows; (2) excessively long input lengths; and (3) question attention shifts in the multi-hop QA task. To this end, we propose a novel method: Dynamic Multi-Granularity Graph Estimate Retrieval - DRAMA. Our method incorporates an interaction mechanism among multiple rows. Specifically, we utilize a memory bank to store the features of each row, thereby facilitating the construction of a heterogeneous graph with multi-row information. Besides, a Dynamic Graph Attention Network (DGAT) module is engaged to gauge the attention shift in the multi-hop question and eliminate the noise information dynamically. Empirical results on the widely used HybridQA and TabFact datasets demonstrate that the proposed model is effective.</abstract>
      <url hash="99297a5f">2024.lrec-main.477</url>
      <bibkey>yuan-etal-2024-drama-dynamic</bibkey>
    </paper>
    <paper id="478">
      <title><fixed-case>D</fixed-case>r<fixed-case>B</fixed-case>enchmark: A Large Language Understanding Evaluation Benchmark for <fixed-case>F</fixed-case>rench Biomedical Domain</title>
      <author><first>Yanis</first><last>Labrak</last></author>
      <author><first>Adrien</first><last>Bazoge</last></author>
      <author><first>Oumaima</first><last>El Khettari</last></author>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Pacome</first><last>Constant Dit Beaufils</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <author><first>Solen</first><last>Quiniou</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Pierre-Antoine</first><last>Gourraud</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <pages>5376–5390</pages>
      <abstract>The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, or classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.</abstract>
      <url hash="a8b3f79e">2024.lrec-main.478</url>
      <bibkey>labrak-etal-2024-drbenchmark-large</bibkey>
    </paper>
    <paper id="479">
      <title>Dual Complex Number Knowledge Graph Embeddings</title>
      <author><first>Yao</first><last>Dong</last></author>
      <author><first>Qingchao</first><last>Kong</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Yin</first><last>Luo</last></author>
      <pages>5391–5400</pages>
      <abstract>Knowledge graph embedding, which aims to learn representations of entities and relations in large scale knowledge graphs, plays a crucial part in various downstream applications. The performance of knowledge graph embedding models mainly depends on the ability of modeling relation patterns, such as symmetry/antisymmetry, inversion and composition (commutative composition and non-commutative composition). Most existing methods fail in modeling the non-commutative composition patterns. Several methods support this kind of pattern by modeling in quaternion space or dihedral group. However, extending to such sophisticated spaces leads to a substantial increase in the amount of parameters, which greatly reduces the parameter efficiency. In this paper, we propose a new knowledge graph embedding method called dual complex number knowledge graph embeddings (DCNE), which maps entities to the dual complex number space, and represents relations as rotations in 2D space via dual complex number multiplication. The non-commutativity of the dual complex number multiplication empowers DCNE to model the non-commutative composition patterns. In the meantime, modeling relations as rotations in 2D space can effectively improve the parameter efficiency. Extensive experiments on multiple benchmark knowledge graphs empirically show that DCNE achieves significant performance in link prediction and path query answering.</abstract>
      <url hash="f8ec628d">2024.lrec-main.479</url>
      <bibkey>dong-etal-2024-dual-complex</bibkey>
    </paper>
    <paper id="480">
      <title>Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction</title>
      <author><first>Xiaowei</first><last>Zhao</last></author>
      <author><first>Yong</first><last>Zhou</last></author>
      <author><first>Xiujuan</first><last>Xu</last></author>
      <pages>5401–5413</pages>
      <abstract>Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a <i>Dual Encoder: Exploiting the potential of Syntactic and Semantic</i> model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these modules to harness the significant potential of syntactic and semantic information in ASTE tasks. Testing on public benchmarks, our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its effectiveness.</abstract>
      <url hash="a9617517">2024.lrec-main.480</url>
      <bibkey>zhao-etal-2024-dual-encoder</bibkey>
    </paper>
    <paper id="481">
      <title><fixed-case>D</fixed-case>uet<fixed-case>S</fixed-case>im: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues</title>
      <author><first>Xiang</first><last>Luo</last></author>
      <author><first>Zhiwen</first><last>Tang</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>5414–5424</pages>
      <abstract>User Simulators play a pivotal role in training and evaluating task-oriented dialogue systems. Traditional user simulators typically rely on human-engineered agendas, resulting in generated responses that often lack diversity and spontaneity. Although large language models (LLMs) exhibit a remarkable capacity for generating coherent and contextually appropriate utterances, they may fall short when tasked with generating responses that effectively guide users towards their goals, particularly in dialogues with intricate constraints and requirements. This paper introduces DuetSim, a novel framework designed to address the intricate demands of task-oriented dialogues by leveraging LLMs. DuetSim stands apart from conventional approaches by employing two LLMs in tandem: one dedicated to response generation and the other focused on verification. This dual LLM approach empowers DuetSim to produce responses that not only exhibit diversity but also demonstrate accuracy and are preferred by human users. We validate the efficacy of our method through extensive experiments conducted on the MultiWOZ dataset, highlighting improvements in response quality and correctness, largely attributed to the incorporation of the second LLM.</abstract>
      <url hash="a3c614fc">2024.lrec-main.481</url>
      <bibkey>luo-etal-2024-duetsim-building</bibkey>
    </paper>
    <paper id="482">
      <title>Dynamic Knowledge Prompt for Chest <fixed-case>X</fixed-case>-ray Report Generation</title>
      <author><first>Shenshen</first><last>Bu</last></author>
      <author><first>Yujie</first><last>Song</last></author>
      <author><first>Taiji</first><last>Li</last></author>
      <author><first>Zhiming</first><last>Dai</last></author>
      <pages>5425–5436</pages>
      <abstract>Automatic generation of radiology reports can relieve the burden of radiologist. In the radiology library, the biased dataset and the sparse features of chest X-ray image make it difficult to generate reports. Many approaches strive to integrate prior information to enhance generation, but they fail to dynamically utilize pulmonary lesion knowledge at the instance-level. To alleviate above problem, we propose a novel Dynamic Knowledge Prompt (DKP) framework for chest X-ray report generation. The DKP can dynamically incorporate the pulmonary lesion information at the instance-level to facilitate report generation. Initially, we design a knowledge prompt for each pulmonary lesion using numerous radiology reports. After that, the DKP using an anomaly detector generates the dynamic knowledge prompt by extracting discriminative lesion features in the corresponding X-ray image. Finally, the knowledge prompt is encoded and fused with hidden states extracted from decoder, to form multi-modal features that guide visual features to generate reports. Extensive experiments on the public datasets MIMIC-CXR and IU X-Ray show that our approach achieves state-of-the-art performance.</abstract>
      <url hash="4e55d7bc">2024.lrec-main.482</url>
      <bibkey>bu-etal-2024-dynamic-knowledge</bibkey>
    </paper>
    <paper id="483">
      <title>Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation</title>
      <author><first>Do June</first><last>Min</last></author>
      <author><first>Veronica</first><last>Perez-Rosas</last></author>
      <author><first>Ken</first><last>Resnicow</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>5437–5449</pages>
      <abstract>In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models.</abstract>
      <url hash="eeb75a9e">2024.lrec-main.483</url>
      <bibkey>min-etal-2024-dynamic-reward</bibkey>
    </paper>
    <paper id="484">
      <title>Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition</title>
      <author><first>Lianyu</first><last>Hu</last></author>
      <author><first>Liqing</first><last>Gao</last></author>
      <author><first>Zekang</first><last>Liu</last></author>
      <author><first>Wei</first><last>Feng</last></author>
      <pages>5450–5460</pages>
      <abstract>Skeleton-aware sign language recognition (SLR) has gained popularity due to its ability to remain unaffected by background information and its lower computational requirements. Current methods utilize spatial graph modules and temporal modules to capture spatial and temporal features, respectively. However, their spatial graph modules are typically built on fixed graph structures such as graph convolutional networks or a single learnable graph, which only partially explore joint relationships. Additionally, a simple temporal convolution kernel is used to capture temporal information, which may not fully capture the complex movement patterns of different signers. To overcome these limitations, we propose a new spatial architecture consisting of two concurrent branches, which build input-sensitive joint relationships and incorporates specific domain knowledge for recognition, respectively. These two branches are followed by an aggregation process to distinguishe important joint connections. We then propose a new temporal module to model multi-scale temporal information to capture complex human dynamics. Our method achieves state-of-the-art accuracy compared to previous skeleton-aware methods on four large-scale SLR benchmarks. Moreover, our method demonstrates superior accuracy compared to RGB-based methods in most cases while requiring much fewer computational resources, bringing better accuracy-computation trade-off. Code is available at https://github.com/hulianyuyy/DSTA-SLR.</abstract>
      <url hash="fb532bee">2024.lrec-main.484</url>
      <bibkey>hu-etal-2024-dynamic-spatial</bibkey>
    </paper>
    <paper id="485">
      <title><fixed-case>E</fixed-case>co<fixed-case>V</fixed-case>erse: An Annotated <fixed-case>T</fixed-case>witter Dataset for Eco-Relevance Classification, Environmental Impact Analysis, and Stance Detection</title>
      <author><first>Francesca</first><last>Grasso</last></author>
      <author><first>Stefano</first><last>Locci</last></author>
      <author><first>Giovanni</first><last>Siragusa</last></author>
      <author><first>Luigi</first><last>Di Caro</last></author>
      <pages>5461–5472</pages>
      <abstract>Anthropogenic ecological crisis constitutes a significant challenge that all within the academy must urgently face, including the Natural Language Processing (NLP) community. While recent years have seen increasing work revolving around climate-centric discourse, crucial environmental and ecological topics outside of climate change remain largely unaddressed, despite their prominent importance. Mainstream NLP tasks, such as sentiment analysis, dominate the scene, but there remains an untouched space in the literature involving the analysis of environmental impacts of certain events and practices. To address this gap, this paper presents EcoVerse, an annotated English Twitter dataset of 3,023 tweets spanning a wide spectrum of environmental topics. We propose a three-level annotation scheme designed for Eco-Relevance Classification, Stance Detection, and introducing an original approach for Environmental Impact Analysis. We detail the data collection, filtering, and labeling process that led to the creation of the dataset. Remarkable Inter-Annotator Agreement indicates that the annotation scheme produces consistent annotations of high quality. Subsequent classification experiments using BERT-based models, including ClimateBERT, are presented. These yield encouraging results, while also indicating room for a model specifically tailored for environmental texts. The dataset is made freely available to stimulate further research.</abstract>
      <url hash="2d70fdd8">2024.lrec-main.485</url>
      <bibkey>grasso-etal-2024-ecoverse-annotated</bibkey>
    </paper>
    <paper id="486">
      <title><fixed-case>EC</fixed-case>t<fixed-case>HR</fixed-case>-<fixed-case>PCR</fixed-case>: A Dataset for Precedent Understanding and Prior Case Retrieval in the <fixed-case>E</fixed-case>uropean Court of Human Rights</title>
      <author><first>Santosh</first><last>T.y.s.s.</last></author>
      <author><first>Rashid</first><last>Haddad</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>5473–5483</pages>
      <abstract>In common law jurisdictions, legal practitioners rely on precedents to construct arguments, in line with the doctrine of stare decisis. As the number of cases grow over the years, prior case retrieval (PCR) has garnered significant attention. Besides lacking real-world scale, existing PCR datasets do not simulate a realistic setting, because their queries use complete case documents while only masking references to prior cases. The query is thereby exposed to legal reasoning not yet available when constructing an argument for an undecided case as well as spurious patterns left behind by citation masks, potentially short-circuiting a comprehensive understanding of case facts and legal principles. To address these limitations, we introduce a PCR dataset based on judgements from the European Court of Human Rights (ECtHR), which explicitly separate facts from arguments and exhibit precedential practices, aiding us to develop this PCR dataset to foster systems’ comprehensive understanding. We benchmark different lexical and dense retrieval approaches with various negative sampling strategies, adapting them to deal with long text sequences using hierarchical variants. We found that difficulty-based negative sampling strategies were not effective for the PCR task, highlighting the need for investigation into domain-specific difficulty criteria. Furthermore, we observe performance of the dense models degrade with time and calls for further research into temporal adaptation of retrieval models. Additionally, we assess the influence of different views , Halsbury’s and Goodhart’s, in practice in ECtHR jurisdiction using PCR task.</abstract>
      <url hash="3023ff89">2024.lrec-main.486</url>
      <bibkey>t-y-s-s-etal-2024-ecthr-pcr</bibkey>
    </paper>
    <paper id="487">
      <title><fixed-case>EDDA</fixed-case>: An Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection</title>
      <author><first>Daijun</first><last>Ding</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Zhichao</first><last>Huang</last></author>
      <author><first>Guangning</first><last>Xu</last></author>
      <author><first>Xu</first><last>Huang</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Liwen</first><last>Jing</last></author>
      <author><first>Bowen</first><last>Zhang</last></author>
      <pages>5484–5494</pages>
      <abstract>Stance detection aims to determine the attitude expressed in text towards a given target. Zero-shot stance detection (ZSSD) has emerged to classify stances towards unseen targets during inference. Recent data augmentation techniques for ZSSD increase transferable knowledge between targets through text or target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source text, while text augmentation relies solely on training data, resulting in insufficient generalization. To address these issues, we propose an encoder-decoder data augmentation (EDDA) framework. The encoder leverages large language models and chain-of-thought prompting to summarize texts into target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syntactic diversity. We also analyze the generated expressions to develop a rationale-enhanced network that fully utilizes the augmented data. Experiments on benchmark datasets demonstrate our approach substantially improves over state-of-the-art ZSSD techniques. The proposed EDDA framework increases semantic relevance and syntactic variety in augmented texts while enabling interpretable rationale-based learning.</abstract>
      <url hash="a00b5b3b">2024.lrec-main.487</url>
      <bibkey>ding-etal-2024-edda-encoder</bibkey>
    </paper>
    <paper id="488">
      <title><fixed-case>EDEN</fixed-case>: A Dataset for Event Detection in <fixed-case>N</fixed-case>orwegian News</title>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Jeanett</first><last>Murstad</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Lubos</first><last>Steskal</last></author>
      <author><first>Lilja Charlotte</first><last>Storset</last></author>
      <author><first>Huiling</first><last>You</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>5495–5506</pages>
      <abstract>We present EDEN, the first Norwegian dataset annotated with event information at the sentence level, adapting the widely used ACE event schema to Norwegian. The paper describes the manual annotation of Norwegian text as well as transcribed speech in the news domain, together with inter-annotator agreement and discussions of relevant dataset statistics. We also present preliminary modeling results using a graph-based event parser. The resulting dataset will be freely available for download and use.</abstract>
      <url hash="056ad8af">2024.lrec-main.488</url>
      <bibkey>touileb-etal-2024-eden-dataset</bibkey>
    </paper>
    <paper id="489">
      <title>Educational Dialogue Systems for Visually Impaired Students: Introducing a Task-Oriented User-Agent Corpus</title>
      <author><first>Elisa</first><last>Di Nuovo</last></author>
      <author><first>Manuela</first><last>Sanguinetti</last></author>
      <author><first>Pier Felice</first><last>Balestrucci</last></author>
      <author><first>Luca</first><last>Anselma</last></author>
      <author><first>Cristian</first><last>Bernareggi</last></author>
      <author><first>Alessandro</first><last>Mazzei</last></author>
      <pages>5507–5519</pages>
      <abstract>This paper describes a corpus consisting of real-world dialogues in English between users and a task-oriented conversational agent, with interactions revolving around the description of finite state automata. The creation of this corpus is part of a larger research project aimed at developing tools for an easier access to educational content, especially in STEM fields, for users with visual impairments. The development of this corpus was precisely motivated by the aim of providing a useful resource to support the design of such tools. The core feature of this corpus is that its creation involved both sighted and visually impaired participants, thus allowing for a greater diversity of perspectives and giving the opportunity to identify possible differences in the way the two groups of participants interacted with the agent. The paper introduces this corpus, giving an account of the process that led to its creation, i.e. the methodology followed to obtain the data, the annotation scheme adopted, and the analysis of the results. Finally, the paper reports the results of a classification experiment on the annotated corpus, and an additional experiment to assess the annotation capabilities of three large language models, in view of a further expansion of the corpus.</abstract>
      <url hash="4bb9c34f">2024.lrec-main.489</url>
      <bibkey>di-nuovo-etal-2024-educational-dialogue</bibkey>
    </paper>
    <paper id="490">
      <title><fixed-case>EEE</fixed-case>-<fixed-case>QA</fixed-case>: Exploring Effective and Efficient Question-Answer Representations</title>
      <author><first>Zhanghao</first><last>Hu</last></author>
      <author><first>Yijun</first><last>Yang</last></author>
      <author><first>Junjie</first><last>Xu</last></author>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <pages>5520–5525</pages>
      <abstract>Current approaches to question answering rely on pre-trained language models (PLMs) like RoBERTa. This work challenges the existing question-answer encoding convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a question representation for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the question. This enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different PLMs, and with and without the integration of knowledge graphs. Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100% throughput with 26-65% speedups on consumer-grade GPUs by allowing for considerably larger batch sizes. Our work sends a message to the community with promising directions in both representation quality and efficiency for the question-answering task in natural language processing.</abstract>
      <url hash="070a97c6">2024.lrec-main.490</url>
      <bibkey>hu-etal-2024-eee-qa</bibkey>
    </paper>
    <paper id="491">
      <title>Eesthetic: A Paralex Lexicon of <fixed-case>E</fixed-case>stonian Paradigms</title>
      <author><first>Sacha</first><last>Beniamine</last></author>
      <author><first>Mari</first><last>Aigro</last></author>
      <author><first>Matthew</first><last>Baerman</last></author>
      <author><first>Jules</first><last>Bouton</last></author>
      <author><first>Maria</first><last>Copot</last></author>
      <pages>5526–5537</pages>
      <abstract>We introduce Eesthetic, a comprehensive Estonian noun and verb lexicon sourced from the Ekilex database. It documents 5475 nouns inflecting for 28 paradigm cells and 5076 verbs inflecting for 51 cells, and comprises a total of 452885 inflected forms. Our openly accessible machine-readable dataset adheres to the Paralex standard. It comprises CSV tables linked by formal relationships. Metadata in JSON format, following the Frictionless standard, provides detailed descriptions of the tables and dataset. The lexicon offers extensive linguistic annotations, including orthographic forms, automatically transcribed phonemic transcriptions, non-canonical morphological phenomena such as overabundance and defectiveness, rich mapping of the paradigm cells and feature-values to other notation schemes, a decomposition of phonemes in distinctive features, and annotation of inflection classes. It is suited for both monolingual and comparative research, enabling qualitative and quantitative analysis. This paper outlines the creation process, rationale, and resulting structure, along with our set of rules for automatic orthography-to-phonemic transcription conversion.</abstract>
      <url hash="27aefa00">2024.lrec-main.491</url>
      <bibkey>beniamine-etal-2024-eesthetic-paralex</bibkey>
    </paper>
    <paper id="492">
      <title>Effective Distillation of Table-based Reasoning Ability from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Bohao</first><last>Yang</last></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Kun</first><last>Zhao</last></author>
      <author><first>Chenghao</first><last>Xiao</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>5538–5550</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their enormous parameter size and extremely high requirements for compute power pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. However, there has been no prior work focusing on table reasoning skills in smaller models specifically tailored for scientific table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation approach, with the aim of distilling LLMs into tailored smaller models. Our experimental results have shown that a 220 million parameter model (Flan-T5-base) fine-tuned using distilled data, not only achieves a significant improvement compared to traditionally fine-tuned baselines, but also surpasses specific LLMs on a scientific table-to-text generation dataset. Our code is available at https://github.com/Bernard-Yang/DistillTableCoT.</abstract>
      <url hash="20c74312">2024.lrec-main.492</url>
      <bibkey>yang-etal-2024-effective-distillation</bibkey>
    </paper>
    <paper id="493">
      <title>Effective Integration of Text Diffusion and Pre-Trained Language Models with Linguistic Easy-First Schedule</title>
      <author><first>Yimin</first><last>Ou</last></author>
      <author><first>Ping</first><last>Jian</last></author>
      <pages>5551–5561</pages>
      <abstract>Diffusion models have become a powerful generative modeling paradigm, achieving great success in continuous data patterns. However, the discrete nature of text data results in compatibility issues between continuous diffusion models (CDMs) and pre-trained language models (PLMs). That is, the performance of diffusion models even degrades when combined with PLMs. To alleviate this issue, we propose to utilize a pre-trained decoder to convert the denoised embedding vectors into natural language instead of using the widely used rounding operation. In this way, CDMs can be more effectively combined with PLMs. Additionally, considering that existing noise schedules in text diffusion models do not take into account the linguistic differences among tokens, which violates the easy-first policy for text generation, we propose a linguistic easy-first schedule that incorporates the measure of word importance, conforming to easy-first-generation linguistic features and bringing about improved generation quality. Experiment results on the E2E dataset and five controllable tasks show that our approach can combine the merits of CDMs and PLMs, significantly outperforming other diffusion-based models.</abstract>
      <url hash="3ddcde47">2024.lrec-main.493</url>
      <bibkey>ou-jian-2024-effective-integration</bibkey>
    </paper>
    <paper id="494">
      <title>Efficiency and Effectiveness in Task-Oriented Dialogue: On Construction Repetition, Information Rate, and Task Success</title>
      <author><first>Jun Sen</first><last>Yee</last></author>
      <author><first>Mario</first><last>Giulianelli</last></author>
      <author><first>Arabella J.</first><last>Sinclair</last></author>
      <pages>5562–5577</pages>
      <abstract>We investigate the roles that efficiency and effectiveness play in speakers’ repetition of shared word sequences, or constructions, in task-oriented dialogue. We find that repeating constructions has negative effects on information rate and positive effects on rate of delivery, that information rate managing strategies are predictive of task success, and that this varies by the communicative function of the constructions being repeated. More effective dialogue is characterised by greater levels of shared construction usage and more efficient task-related repetition; while task-agnostic repetition can seem redundant, it can serve important efficiency and effectiveness functions. Our results provide a nuanced picture of the importance of repetition and of developing a shared lexicon for both efficiency and effectiveness in task-oriented dialogue.</abstract>
      <url hash="71a58030">2024.lrec-main.494</url>
      <bibkey>yee-etal-2024-efficiency-effectiveness</bibkey>
    </paper>
    <paper id="495">
      <title>Efficient <fixed-case>AMR</fixed-case> Parsing with <fixed-case>CLAP</fixed-case>: Compact Linearization with an Adaptable Parser</title>
      <author><first>Abelardo Carlos</first><last>Martinez Lorenzo</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>5578–5584</pages>
      <abstract>Sequence-to-sequence models have become the de facto standard for Abstract Meaning Representation (AMR) parsing due to their high-quality performance. However, these systems face efficiency challenges because of their large model size and computational time, which limit their accessibility within the research community. This paper aims to break down these barriers by introducing a novel linearization and system that significantly enhances the efficiency and accessibility of previous AMR parsers. First, we propose our novel Compact linearization that simplifies encoding, thereby reducing the number of tokens by between 40% and 50%. Second, we present CLAP, an innovative modular system that maintains the model’s high performance while achieving remarkable 80% reduction in training and inference times. Furthermore, CLAP is compatible with multiple autoregressive Language Models (LM) and tokenizers, such as BART, T5, and others. These advancements underscore the importance of optimizing sequence-to-sequence models in AMR parsing, thus democratizing access to high-quality semantic analysis. Our code is publicly available at https://github.com/SapienzaNLP/clap/.</abstract>
      <url hash="7965f948">2024.lrec-main.495</url>
      <bibkey>martinez-lorenzo-navigli-2024-efficient-amr</bibkey>
    </paper>
    <paper id="496">
      <title>Efficient and Accurate Contextual Re-Ranking for Knowledge Graph Question Answering</title>
      <author><first>Kexuan</first><last>Sun</last></author>
      <author><first>Nicolaas Paul</first><last>Jedema</last></author>
      <author><first>Karishma</first><last>Sharma</last></author>
      <author><first>Ruben</first><last>Janssen</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Pedro</first><last>Szekely</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>5585–5595</pages>
      <abstract>The efficacy of neural “retrieve and generate” systems is well established for question answering (QA) over unstructured text. Recent efforts seek to extend this approach to knowledge graph (KG) QA by converting structured triples to unstructured text. However, the relevance of KG triples retrieved by these systems limits their accuracy. In this paper, we improve the relevance of retrieved triples using a carefully designed re-ranker. Specifically, our pipeline (i) retrieves over documents of triples grouped by entity, (ii) re-ranks triples from these documents with context: triples in the 1-hop neighborhood of the documents’ subject entity, and (iii) generates an answer from highly relevant re-ranked triples. To train our re-ranker, we propose a novel “triple-level” labeling strategy that infers fine-grained labels and shows that these significantly improve the relevance of retrieved information. We show that the resulting “retrieve, re-rank, and generate” pipeline significantly improves upon prior KGQA systems, achieving a new state-of-the-art on FreebaseQA by 5.56% Exact Match. We perform multiple ablations that reveal the distinct benefits of our contextual re-ranker and labeling strategy and conclude with a case study that highlights opportunities for future works.</abstract>
      <url hash="907012d2">2024.lrec-main.496</url>
      <bibkey>sun-etal-2024-efficient-accurate</bibkey>
    </paper>
    <paper id="497">
      <title><fixed-case>EFTNAS</fixed-case>: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks</title>
      <author><first>Juan Pablo</first><last>Munoz</last></author>
      <author><first>Yi</first><last>Zheng</last></author>
      <author><first>Nilesh</first><last>Jain</last></author>
      <pages>5596–5608</pages>
      <abstract>Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, e.g., computer vision. Depending on the size of these models, which have grown exponentially in the past few years, machine learning practitioners might be restricted from deploying them in resource-constrained environments. This paper discusses the compression of transformer-based models for multiple resource budgets. Integrating neural architecture search (NAS) and network pruning techniques, we effectively generate and train weight-sharing super-networks that contain efficient, high-performing, and compressed transformer-based models. A common challenge in NAS is the design of the search space, for which we propose a method to automatically obtain the boundaries of the search space and then derive the rest of the intermediate possible architectures using a first-order weight importance technique. The proposed end-to-end NAS solution, EFTNAS, discovers efficient subnetworks that have been compressed and fine-tuned for downstream NLP tasks. We demonstrate EFTNAS on the General Language Understanding Evaluation (GLUE) benchmark and the Stanford Question Answering Dataset (SQuAD), obtaining high-performing smaller models with a reduction of more than 5x in size without or with little degradation in performance.</abstract>
      <url hash="a7e5c581">2024.lrec-main.497</url>
      <bibkey>munoz-etal-2024-eftnas-searching</bibkey>
    </paper>
    <paper id="498">
      <title>Eliciting Motivational Interviewing Skill Codes in Psychotherapy with <fixed-case>LLM</fixed-case>s: A Bilingual Dataset and Analytical Study</title>
      <author><first>Xin</first><last>Sun</last></author>
      <author><first>Jiahuan</first><last>Pei</last></author>
      <author><first>Jan de</first><last>Wit</last></author>
      <author><first>Mohammad</first><last>Aliannejadi</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <author><first>Jos T.P.</first><last>Dobber</last></author>
      <author><first>Jos A.</first><last>Bosch</last></author>
      <pages>5609–5621</pages>
      <abstract>Behavioral coding (BC) in motivational interviewing (MI) holds great potential for enhancing the efficacy of MI counseling. However, manual coding is labor-intensive, and automation efforts are hindered by the lack of data due to the privacy of psychotherapy. To address these challenges, we introduce BiMISC, a bilingual dataset of MI conversations in English and Dutch, sourced from real counseling sessions. Expert annotations in BiMISC adhere strictly to the motivational interviewing skills code (MISC) scheme, offering a pivotal resource for MI research. Additionally, we present a novel approach to elicit the MISC expertise from Large language models (LLMs) for MI coding. Through the in-depth analysis of BiMISC and the evaluation of our proposed approach, we demonstrate that the LLM-based approach yields results closely aligned with expert annotations and maintains consistent performance across different languages. Our contributions not only furnish the MI community with a valuable bilingual dataset but also spotlight the potential of LLMs in MI coding, laying the foundation for future MI research.</abstract>
      <url hash="7e4aeddc">2024.lrec-main.498</url>
      <bibkey>sun-etal-2024-eliciting-motivational</bibkey>
    </paper>
    <paper id="499">
      <title><fixed-case>ELLEN</fixed-case>: Extremely Lightly Supervised Learning for Efficient Named Entity Recognition</title>
      <author><first>Haris</first><last>Riaz</last></author>
      <author><first>Razvan Gabriel</first><last>Dumitru</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>5622–5636</pages>
      <abstract>In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as “One Sense Per Discourse”, using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also achieves over 75% of the performance of a strong, fully supervised model trained on gold data. Our code is publicly available.</abstract>
      <url hash="25b8457e">2024.lrec-main.499</url>
      <bibkey>riaz-etal-2024-ellen-extremely</bibkey>
    </paper>
    <paper id="500">
      <title><fixed-case>EMAD</fixed-case>: A Bridge Tagset for Unifying <fixed-case>A</fixed-case>rabic <fixed-case>POS</fixed-case> Annotations</title>
      <author><first>Omar</first><last>Kallas</last></author>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>5637–5643</pages>
      <abstract>There have been many attempts to model the morphological richness and complexity of Arabic, leading to numerous Part-of-Speech (POS) tagsets that differ in terms of (a) which morphological features they represent, (b) how they represent them, and (c) the degree of specification of said features. Tagset granularity plays an important role in determining how annotated data can be used and for what applications. Due to the diversity among existing tagsets, many annotated corpora for Arabic cannot be easily combined, which exacerbates the Arabic resource poverty situation. In this work, we propose an intermediate tagset designed to facilitate the conversion and unification of different tagsets used to annotate Arabic corpora. This new tagset acts as a bridge between different annotation schemes, simplifying the integration of annotated corpora and promoting collaboration across the projects using them.</abstract>
      <url hash="4aee04da">2024.lrec-main.500</url>
      <bibkey>kallas-etal-2024-emad-bridge</bibkey>
    </paper>
    <paper id="501">
      <title>Emancipating Event Extraction from the Constraints of Long-Tailed Distribution Data Utilizing Large Language Models</title>
      <author><first>Zhigang</first><last>Kan</last></author>
      <author><first>Liwen</first><last>Peng</last></author>
      <author><first>Linbo</first><last>Qiao</last></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <pages>5644–5653</pages>
      <abstract>Event Extraction (EE) is a challenging task that aims to extract structural event-related information from unstructured text. Traditional methods for EE depend on manual annotations, which are both expensive and scarce. Furthermore, the existing datasets mostly follow the long-tail distribution, severely hindering the previous methods of modeling tail types. Two techniques can address this issue: transfer learning and data generation. However, the existing methods based on transfer learning still rely on pre-training with a large amount of labeled data in the source domain. Additionally, the quality of data generated by previous data generation methods is difficult to control. In this paper, leveraging Large Language Models (LLMs), we propose novel methods for event extraction and generation based on dialogues, overcoming the problems of relying on source domain data and maintaining data quality. Specifically, this paper innovatively transforms the EE task into multi-turn dialogues, guiding LLMs to learn event schemas from historical dialogue information and output structural events. Furthermore, we introduce a novel LLM-based method for generating high-quality data, significantly improving traditional models’ performance with various paradigms and structures, especially on tail types. Adequate experiments on real-world datasets demonstrate the effectiveness of the proposed event extraction and data generation methods.</abstract>
      <url hash="4efdd0ac">2024.lrec-main.501</url>
      <bibkey>kan-etal-2024-emancipating-event</bibkey>
    </paper>
    <paper id="502">
      <title><fixed-case>EMOLIS</fixed-case> App and Dataset to Find Emotionally Close Cartoons</title>
      <author><first>Soëlie</first><last>Lerch</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Elisabeth</first><last>Murisasco</last></author>
      <author><first>Emmanuel</first><last>Bruno</last></author>
      <pages>5654–5659</pages>
      <abstract>We propose EMOLIS Dataset that contains annotated emotional transcripts of scenes from Walt Disney cartoons at the same time as physiological signals from spectators (breathing, ECG, eye movements). The dataset is used in EMOLIS App, our second proposal. EMOLIS App allows to display the identified emotions while a video is playing and suggest emotionally comparable videos. We propose to estimate an emotional distance between videos using multimodal neural representations (text, audio, video) that also combine physiological signals. This enables personalized results that can be used for cognitive therapies focusing on awareness of felt emotions. The dataset is designed to be suitable for all audiences and autistic people who have difficulties to recognize and express emotions.</abstract>
      <url hash="f87331a3">2024.lrec-main.502</url>
      <bibkey>lerch-etal-2024-emolis-app</bibkey>
    </paper>
    <paper id="503">
      <title><fixed-case>E</fixed-case>mo<fixed-case>P</fixed-case>rogress: Cumulated Emotion Progression Analysis in Dreams and Customer Service Dialogues</title>
      <author><first>Eileen</first><last>Wemmer</last></author>
      <author><first>Sofie</first><last>Labat</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>5660–5677</pages>
      <abstract>Emotion analysis often involves the categorization of isolated textual units, but these are parts of longer discourses, like dialogues or stories. This leads to two different established emotion classification setups: (1) Classification of a longer text into one or multiple emotion categories. (2) Classification of the parts of a longer text (sentences or utterances), either (2a) with or (2b) without consideration of the context. None of these settings, does, however, enable to answer the question which emotion is presumably experienced at a specific moment in time. For instance, a customer’s request of “My computer broke.” would be annotated with anger. This emotion persists in a potential follow-up reply “It is out of warranty.” which would also correspond to the global emotion label. An alternative reply “We will send you a new one.” might, in contrast, lead to relief. Modeling these label relations requires classification of textual parts under consideration of the past, but without access to the future. Consequently, we propose a novel annotation setup for emotion categorization corpora, in which the annotations reflect the emotion up to the annotated sentence. We ensure this by uncovering the textual parts step-by-step to the annotator, asking for a label in each step. This perspective is important to understand the final, global emotion, while having access to the individual sentence’s emotion contributions to this final emotion. In modeling experiments, we use these data to check if the context is indeed required to automatically predict such cumulative emotion progressions.</abstract>
      <url hash="71beed6e">2024.lrec-main.503</url>
      <bibkey>wemmer-etal-2024-emoprogress-cumulated</bibkey>
    </paper>
    <paper id="504">
      <title><fixed-case>E</fixed-case>mo<fixed-case>P</fixed-case>rompt-<fixed-case>ECPE</fixed-case>: Emotion Knowledge-aware Prompt-tuning for Emotion-Cause Pair Extraction</title>
      <author><first>Xue</first><last>Gu</last></author>
      <author><first>Zhihan</first><last>Zhou</last></author>
      <author><first>Ziyao</first><last>Meng</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Tiago</first><last>Gomes</last></author>
      <author><first>Adriano</first><last>Tavares</last></author>
      <author><first>Hao</first><last>Xu</last></author>
      <pages>5678–5688</pages>
      <abstract>Emotion-cause pair extraction (ECPE) main focus is on extracting all potential emotion clauses and corresponding cause clauses from unannotated documents. Existing methods achieve promising results with the help of fine-tuning and prompt paradigms, but they present three downsides. First, most approaches cannot distinguish between the emotion-cause pairs that belong to different types of emotions, limiting the existing approaches’ applicability. Second, existing prompt methods utilize a one-to-one mapping relation to achieve label words to category mapping, which brings considerable bias to the results. Third, existing methods achieve the cause extraction task supported by explicit semantic understanding or basic prompt templates, ignoring the implicit information contained in the cause clauses themselves. To solve these issues, we propose an Emotion knowledge-aware Prompt-tuning for Emotion-Cause Pair Extraction (EmoPrompt-ECPE) method, which integrate the knowledge of emotion categories in the ECPE task and mine the implicit knowledge of cause clauses. Specifically, we inject the latent knowledge of the cause clauses and the emotion types into the prompt template. Besides, we extend the emotion labels for many-to-one mapping of label words to categories with an external emotion word base. Furthermore, we utilize the cosine similarity filtering of the label word base to reduce the noise caused by knowledge introduction. Experiments on both Chinese and English benchmark datasets show that our approach can achieve state-of-the-art results. Our code and data can be found at: https://github.com/xy-xiaotudou/EmoPrompt-ECPE.</abstract>
      <url hash="6f3638ef">2024.lrec-main.504</url>
      <attachment type="OptionalSupplementaryMaterial" hash="bb5d73e5">2024.lrec-main.504.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>gu-etal-2024-emoprompt-ecpe</bibkey>
    </paper>
    <paper id="505">
      <title>Emotags: Computer-Assisted Verbal Labelling of Expressive Audiovisual Utterances for Expressive Multimodal <fixed-case>TTS</fixed-case></title>
      <author><first>Gérard</first><last>Bailly</last></author>
      <author><first>Romain</first><last>Legrand</last></author>
      <author><first>Martin</first><last>Lenglet</last></author>
      <author><first>Frédéric</first><last>Elisei</last></author>
      <author><first>Maëva</first><last>Hueber</last></author>
      <author><first>Olivier</first><last>Perrotin</last></author>
      <pages>5689–5695</pages>
      <abstract>We developped a web app for ascribing verbal descriptions to expressive audiovisual utterances. These descriptions are limited to lists of adjectives that are either suggested via a navigation in emotional latent spaces built using discriminant analysis of BERT embeddings or entered freely by subjects. We show that such verbal descriptions collected on-line via Prolific on massive data (310 participants, 12620 labelled utterances up-to-now) provide Expressive Multimodal Text-to-Speech Synthesis with precise verbal control over desired emotional content</abstract>
      <url hash="e2e57320">2024.lrec-main.505</url>
      <bibkey>bailly-etal-2024-emotags-computer</bibkey>
    </paper>
    <paper id="506">
      <title>Emotion Analysis in <fixed-case>NLP</fixed-case>: Trends, Gaps and Roadmap for Future Directions</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>Alba A.</first><last>Cercas Curry</last></author>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>5696–5710</pages>
      <abstract>Emotions are a central aspect of communication. Consequently, emotion analysis (EA) is a rapidly growing field in natural language processing (NLP). However, there is no consensus on scope, direction, or methods. In this paper, we conduct a thorough review of 154 relevant NLP publications from the last decade. Based on this review, we address four different questions: (1) How are EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and which emotions are modeled? (3) Is the subjectivity of emotions considered in terms of demographics and cultural factors? and (4) What are the primary NLP applications for EA? We take stock of trends in EA and tasks, emotion frameworks used, existing datasets, methods, and applications. We then discuss four lacunae: (1) the absence of demographic and cultural aspects does not account for the variation in how emotions are perceived, but instead assumes they are universally experienced in the same manner; (2) the poor fit of emotion categories from the two main emotion theories to the task; (3) the lack of standardized EA terminology hinders gap identification, comparison, and future goals; and (4) the absence of interdisciplinary research isolates EA from insights in other fields. Our work will enable more focused research into EA and a more holistic approach to modeling emotions in NLP.</abstract>
      <url hash="3803d0eb">2024.lrec-main.506</url>
      <bibkey>plaza-del-arco-etal-2024-emotion-analysis</bibkey>
    </paper>
    <paper id="507">
      <title>Emotion Recognition in Conversation via Dynamic Personality</title>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Yachao</first><last>Zhao</last></author>
      <author><first>Dongming</first><last>Zhao</last></author>
      <author><first>Xiaojia</first><last>Jin</last></author>
      <author><first>Jijun</first><last>Zhang</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <pages>5711–5722</pages>
      <abstract>Emotion recognition in conversation (ERC) is a field that aims to classify the emotion of each utterance within conversational contexts. This presents significant challenges, particularly in handling emotional ambiguity across various speakers and contextual factors. Existing ERC approaches have primarily focused on modeling conversational contexts while incorporating only superficial speaker attributes such as names, memories, and interactions. Recent works introduce personality as an essential deep speaker factor for emotion recognition, but relies on static personality, overlooking dynamic variability during conversations. Advances in personality psychology conceptualize personality as dynamic, proposing that personality states can change across situations. In this paper, we introduce ERC-DP, a novel model considering the dynamic personality of speakers during conversations. ERC-DP accounts for past utterances from the same speaker as situation impacting dynamic personality. It combines personality modeling with prompt design and fine-grained classification modules. Through a series of comprehensive experiments, ERC-DP demonstrates superior performance on three benchmark conversational datasets.</abstract>
      <url hash="c617503d">2024.lrec-main.507</url>
      <bibkey>wang-etal-2024-emotion-recognition</bibkey>
    </paper>
    <paper id="508">
      <title><fixed-case>E</fixed-case>mo<fixed-case>T</fixed-case>rans: Emotional Transition-based Model for Emotion Recognition in Conversation</title>
      <author><first>Zhongquan</first><last>Jian</last></author>
      <author><first>Ante</first><last>Wang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Junfeng</first><last>Yao</last></author>
      <author><first>Meihong</first><last>Wang</last></author>
      <author><first>Qingqiang</first><last>Wu</last></author>
      <pages>5723–5733</pages>
      <abstract>In an emotional conversation, emotions are causally transmitted among communication participants, constituting a fundamental conversational feature that can facilitate the comprehension of intricate changes in emotional states during the conversation and contribute to neutralizing emotional semantic bias in utterance caused by the absence of modality information. Therefore, emotional transition (ET) plays a crucial role in the task of Emotion Recognition in Conversation (ERC) that has not received sufficient attention in current research. In light of this, an Emotional Transition-based Emotion Recognizer (EmoTrans) is proposed in this paper. Specifically, we concatenate the most recent utterances with their corresponding speakers to construct the model input, known as samples, each with several placeholders to implicitly express the emotions of contextual utterances. Based on these placeholders, two components are developed to make the model sensitive to emotions and effectively capture the ET features in the sample. Furthermore, an ET-based Contrastive Learning (CL) is developed to compact the representation space, making the model achieve more robust sample representations. We conducted exhaustive experiments on four widely used datasets and obtained competitive experimental results, especially, new state-of-the-art results obtained on MELD and IEMOCAP, demonstrating the superiority of EmoTrans.</abstract>
      <url hash="c6b32689">2024.lrec-main.508</url>
      <bibkey>jian-etal-2024-emotrans-emotional</bibkey>
    </paper>
    <paper id="509">
      <title><fixed-case>E</fixed-case>mp<fixed-case>CRL</fixed-case>: Controllable Empathetic Response Generation via In-Context Commonsense Reasoning and Reinforcement Learning</title>
      <author><first>Mingxiu</first><last>Cai</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>5734–5746</pages>
      <abstract>Empathetic response generation aims to understand the user’s feelings emotionally and generate responses with appropriate emotion. According to psychological theories, empathy consists of two main aspects: affection and cognition. However, existing works lack the perception of fine-grained dialogue emotion propagation, as well as have limitations in reasoning about the intentions of users on cognition, which affect the quality of empathetic response. To this end, we propose to generate Empathetic response based on in-context Commonsense reasoning and Reinforcement Learning (EmpCRL). First, we use a current popular large language model combined with multi-view contextual reasoning to broaden the cognitive boundaries through in-context learning. Furthermore, we infer the response emotion by jointly modeling the dialogue history and emotion flow, and achieve the control of response emotion and diversity through reinforcement learning. Extensive experiments on EmpatheticDialogues dataset show that our model outperforms state-of-the-art models in both automatic and human evaluation.</abstract>
      <url hash="224320cb">2024.lrec-main.509</url>
      <bibkey>cai-etal-2024-empcrl-controllable</bibkey>
    </paper>
    <paper id="510">
      <title>Empowering Low-Resource Regional Languages with Lexicons : A Comparative Study of <fixed-case>NLP</fixed-case> Tools for Morphosyntactic Analysis</title>
      <author><first>Cristina</first><last>Garcia Holgado</last></author>
      <author><first>Marianne</first><last>Vergez-Couret</last></author>
      <pages>5747–5756</pages>
      <abstract>We investigate the effect of integrating lexicon information to an extremely low-resource language when annotated data is scarce for morpho-syntactic analysis. Obtaining such data and linguistic resources for these languages are usually constrained by a lack of human and financial resources making this task particularly challenging. In this paper, we describe the collection and leverage of a bilingual lexicon for Poitevin-Saintongeais, a regional language of France, to create augmented data through a neighbor-based distributional method. We assess this lexicon-driven approach in improving POS tagging while using different lexicon and augmented data sizes. To evaluate this strategy, we compare two distinct paradigms: neural networks, which typically require extensive data, and a conventional probabilistic approach, in which a lexicon is instrumental in its performance. Our findings reveal that the lexicon is a valuable asset for all models, but in particular for neural, demonstrating an enhanced generalization across diverse classes without requiring an extensive lexicon size.</abstract>
      <url hash="57b5cd14">2024.lrec-main.510</url>
      <bibkey>garcia-holgado-vergez-couret-2024-empowering-low</bibkey>
    </paper>
    <paper id="511">
      <title>Empowering <fixed-case>O</fixed-case>neida Language Revitalization: Development of an <fixed-case>O</fixed-case>neida Verb Conjugator</title>
      <author><first>Yanfei</first><last>Lu</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <author><first>Keren</first><last>Rice</last></author>
      <pages>5757–5767</pages>
      <abstract>In this paper, we present the development of a digital Oneida verb conjugator through using the Gramble framework. This project is a collaborative effort with the Twatati Adult Oneida Language program. Oneida is a polysynthetic North American Indigenous language. Its verb roots can be conjugated with multiple affixes, and long verbal complexes can be used as utterances. Each Oneida affix encodes important grammatical information, and its form often varies based on various factors, such as its position in the utterance and its phonological environment. The distinct morphosyntactic structures complicate acquisition of the language by learners who are native speakers of English. With an alarmingly small number of native speakers of Oneida, supporting and accelerating adult second language leaners’ acquisition process has become a pressing necessity. The Oneida verb conjugator can demonstrate its users the correct conjugations of verbs and can also let learners generate practice materials tailored to their unique learning trajectories. This paper presents the preliminary stages and outcomes of the project and outlines the areas for improvement to be addressed in our subsequent endeavors.</abstract>
      <url hash="e8d42e69">2024.lrec-main.511</url>
      <bibkey>lu-etal-2024-empowering-oneida</bibkey>
    </paper>
    <paper id="512">
      <title>Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings</title>
      <author><first>Albert</first><last>Sawczyn</last></author>
      <author><first>Jakub</first><last>Binkowski</last></author>
      <author><first>Piotr</first><last>Bielak</last></author>
      <author><first>Tomasz</first><last>Kajdanowicz</last></author>
      <pages>5768–5782</pages>
      <abstract>Knowledge-intensive tasks pose a significant challenge for Machine Learning (ML) techniques. Commonly adopted methods, such as Large Language Models (LLMs), often exhibit limitations when applied to such tasks. Nevertheless, there have been notable endeavours to mitigate these challenges, with a significant emphasis on augmenting LLMs through Knowledge Graphs (KGs). While KGs provide many advantages for representing knowledge, their development costs can deter extensive research and applications. Addressing this limitation, we introduce a framework for enriching embeddings of small-scale domain-specific Knowledge Graphs with well-established general-purpose KGs. Adopting our method, a modest domain-specific KG can benefit from a performance boost in downstream tasks when linked to a substantial general-purpose KG. Experimental evaluations demonstrate a notable enhancement, with up to a 44% increase observed in the Hits@10 metric. This relatively unexplored research direction can catalyze more frequent incorporation of KGs in knowledge-intensive tasks, resulting in more robust, reliable ML implementations, which hallucinates less than prevalent LLM solutions.</abstract>
      <url hash="aeb03e6f">2024.lrec-main.512</url>
      <bibkey>sawczyn-etal-2024-empowering-small</bibkey>
    </paper>
    <paper id="513">
      <title>Empowering Tree-structured Entailment Reasoning: Rhetorical Perception and <fixed-case>LLM</fixed-case>-driven Interpretability</title>
      <author><first>Longyin</first><last>Zhang</last></author>
      <author><first>Bowei</first><last>Zou</last></author>
      <author><first>Ai Ti</first><last>Aw</last></author>
      <pages>5783–5793</pages>
      <abstract>The study delves into the construction of entailment trees for science question answering (SQA), employing a novel framework termed Tree-structured Entailment Reasoning (TER). Current research on entailment tree construction presents significant challenges, primarily due to the ambiguities and similarities among candidate science facts, which considerably complicate the fact retrieval process. Moreover, the existing models exhibit limitations in effectively modeling the sequence of reasoning states, understanding the intricate relations between neighboring entailment tree nodes, and generating intermediate conclusions. To this end, we explore enhancing the TER performance from three aspects: First, improving retrieval capabilities by modeling and referring to the chained reasoning states; Second, enhancing TER by infusing knowledge that bridges the gap between reasoning types and rhetorical relations. Third, exploring a task-specific large language model tuning scheme to mitigate deficiencies in intermediate conclusion generation. Experiments on the English EntailmentBank demonstrate the effectiveness of the proposed methods in augmenting the quality of tree-structured entailment reasoning to a certain extent.</abstract>
      <url hash="62c1a936">2024.lrec-main.513</url>
      <bibkey>zhang-etal-2024-empowering-tree</bibkey>
    </paper>
    <paper id="514">
      <title>Emstremo: Adapting Emotional Support Response with Enhanced Emotion-Strategy Integrated Selection</title>
      <author><first>Junlin</first><last>Li</last></author>
      <author><first>Bo</first><last>Peng</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <pages>5794–5805</pages>
      <abstract>To provide effective support, it is essential for a skilled supporter to emotionally resonate with the help-seeker’s current emotional state. In conversational interactions, this emotional alignment is further influenced by the comforting strategies employed by the supporter. Different strategies guide the interlocutors to align their emotions in nuanced patterns. However, the incorporation of strategy into emotional alignment in the context of emotional support agents remains underexplored. To address this limitation, we propose an improved emotional support agent called Emstremo. Emstremo aims to achieve strategic control of emotional alignment by perceiving and responding to the user’s emotions. Our system’s state-of-the-art performance emphasizes the importance of integrating emotions and strategies in modeling conversations that provide emotional support.</abstract>
      <url hash="7fc4159c">2024.lrec-main.514</url>
      <bibkey>li-etal-2024-emstremo-adapting</bibkey>
    </paper>
    <paper id="515">
      <title>Encoding Gesture in Multimodal Dialogue: Creating a Corpus of Multimodal <fixed-case>AMR</fixed-case></title>
      <author><first>Kenneth</first><last>Lai</last></author>
      <author><first>Richard</first><last>Brutti</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>5806–5818</pages>
      <abstract>Abstract Meaning Representation (AMR) is a general-purpose meaning representation that has become popular for its clear structure, ease of annotation and available corpora, and overall expressiveness. While AMR was designed to represent sentence meaning in English text, recent research has explored its adaptation to broader domains, including documents, dialogues, spatial information, cross-lingual tasks, and gesture. In this paper, we present an annotated corpus of multimodal (speech and gesture) AMR in a task-based setting. Our corpus is multilayered, containing temporal alignments to both the speech signal and to descriptions of gesture morphology. We also capture coreference relationships across modalities, enabling fine-grained analysis of how the semantics of gesture and natural language interact. We discuss challenges that arise when identifying cross-modal coreference and anaphora, as well as in creating and evaluating multimodal corpora in general. Although we find AMR’s abstraction away from surface form (in both language and gesture) occasionally too coarse-grained to capture certain cross-modal interactions, we believe its flexibility allows for future work to fill in these gaps. Our corpus and annotation guidelines are available at https://github.com/klai12/encoding-gesture-multimodal-dialogue.</abstract>
      <url hash="d2edda25">2024.lrec-main.515</url>
      <bibkey>lai-etal-2024-encoding-gesture</bibkey>
    </paper>
    <paper id="516">
      <title>Endowing Neural Language Learners with Human-like Biases: A Case Study on Dependency Length Minimization</title>
      <author><first>Yuqing</first><last>Zhang</last></author>
      <author><first>Tessa</first><last>Verhoef</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <pages>5819–5832</pages>
      <abstract>Natural languages show a tendency to minimize the linear distance between heads and their dependents in a sentence, known as dependency length minimization (DLM). Such a preference, however, has not been consistently replicated with neural agent simulations. Comparing the behavior of models with that of human learners can reveal which aspects affect the emergence of this phenomenon. In this work, we investigate the minimal conditions that may lead neural learners to develop a DLM preference. We add three factors to the standard neural-agent language learning and communication framework to make the simulation more realistic, namely: (i) the presence of noise during listening, (ii) context-sensitivity of word use through non-uniform conditional word distributions, and (iii) incremental sentence processing, or the extent to which an utterance’s meaning can be guessed before hearing it entirely. While no preference appears in production, we show that the proposed factors can contribute to a small but significant learning advantage of DLM for listeners of verb-initial languages.</abstract>
      <url hash="cfc6c009">2024.lrec-main.516</url>
      <bibkey>zhang-etal-2024-endowing-neural</bibkey>
    </paper>
    <paper id="517">
      <title>End-to-end Parsing of Procedural Text into Flow Graphs</title>
      <author><first>Dhaivat J.</first><last>Bhatt</last></author>
      <author><first>Seyed Ahmad</first><last>Abdollahpouri Hosseini</last></author>
      <author><first>Federico</first><last>Fancellu</last></author>
      <author><first>Afsaneh</first><last>Fazly</last></author>
      <pages>5833–5842</pages>
      <abstract>We focus on the problem of parsing procedural text into fine-grained flow graphs that encode actions and entities, as well as their interactions. Specifically, we focus on parsing cooking recipes, and address a few limitations of existing parsers. Unlike SOTA approaches to flow graph parsing that work in two separate stages identifying actions and entities (tagging) and encoding their interactions via connecting edges (graph generation). we propose an end-to-end multi-task framework that simultaneously performs tagging and graph generation. In addition, due to the end-to-end nature of our proposed model, we can unify the input representation, and moreover can use compact encoders, resulting in small models with significantly fewer parameters than SOTA models. Another key challenge in training flow graph parsers is the lack of sufficient annotated data, due to the costly nature of the fine-grained annotations. We address this problem by taking advantage of the abundant unlabelled recipes, and show that pre-training on automatically-generated noisy silver annotations (from unlabelled recipes) results in a large improvement in flow graph parsing.</abstract>
      <url hash="7941c5ff">2024.lrec-main.517</url>
      <bibkey>bhatt-etal-2024-end-end</bibkey>
    </paper>
    <paper id="518">
      <title>Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis</title>
      <author><first>Jin</first><last>Cui</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <author><first>Xinfeng</first><last>Wang</last></author>
      <author><first>Yoshimi</first><last>Suzuki</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Noriko</first><last>Tomuro</last></author>
      <author><first>Wanzeng</first><last>Kong</last></author>
      <pages>5843–5855</pages>
      <abstract>Aspect-category-based sentiment analysis (ACSA), which aims to identify aspect categories and predict their sentiments has been intensively studied due to its wide range of NLP applications. Most approaches mainly utilize intrasentential features. However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review. Even in a sentence, there is more than one aspect category with its sentiments, and they are entangled intra-sentence, which makes the model fail to discriminately preserve all sentiment characteristics. In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and sentiment identification. To address the issue of multiple aspect categories and sentiment entanglement, we propose a hierarchical disentanglement module to extract distinct categories and sentiment features. Extensive experimental and visualization results show that our ECAN effectively decouples multiple categories and sentiments entangled in the coherence representations and achieves state-of-the-art (SOTA) performance. Our codes and data are available online: https://github.com/cuijin-23/ECAN.</abstract>
      <url hash="d6380b1a">2024.lrec-main.518</url>
      <bibkey>cui-etal-2024-enhanced-coherence</bibkey>
    </paper>
    <paper id="519">
      <title>Enhanced Facet Generation with <fixed-case>LLM</fixed-case> Editing</title>
      <author><first>Joosung</first><last>Lee</last></author>
      <author><first>Jinhong</first><last>Kim</last></author>
      <pages>5856–5865</pages>
      <abstract>In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user’s query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine. The first strategy is multi-task learning to predict SERP. By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules. The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model. Overall performance improves when small model and LLM are combined rather than facet generation individually.</abstract>
      <url hash="c83397e1">2024.lrec-main.519</url>
      <bibkey>lee-kim-2024-enhanced-facet</bibkey>
    </paper>
    <paper id="520">
      <title>Enhance Robustness of Language Models against Variation Attack through Graph Integration</title>
      <author><first>Zi</first><last>Xiong</last></author>
      <author><first>Lizhi</first><last>Qing</last></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Jiawei</first><last>Liu</last></author>
      <author><first>Hongsong</first><last>Li</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>5866–5877</pages>
      <abstract>The widespread use of pre-trained language models (PLMs) in natural language processing (NLP) has greatly improved performance outcomes. However, these models’ vulnerability to adversarial attacks (e.g., camouflaged hints from drug dealers), particularly in the Chinese language with its rich character diversity/variation and complex structures, hatches vital apprehension. In this study, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE), to increase the robustness of PLMs against character variation attacks in Chinese content. CHANGE presents a novel approach to incorporate a Chinese character variation graph into the PLMs. Through designing different supplementary tasks utilizing the graph structure, CHANGE essentially enhances PLMs’ interpretation of adversarially manipulated text. Experiments conducted in a multitude of NLP tasks show that CHANGE outperforms current language models in combating against adversarial attacks and serves as a valuable contribution to robust language model research. Moreover, these findings highlight the substantial potential of graph-guided pre-training strategies for real-world applications.</abstract>
      <url hash="27c7544b">2024.lrec-main.520</url>
      <bibkey>xiong-etal-2024-enhance-robustness</bibkey>
    </paper>
    <paper id="521">
      <title>Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhihong</first><last>Sun</last></author>
      <author><first>Chen</first><last>Lyu</last></author>
      <author><first>Bolun</first><last>Li</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Ge</first><last>Li</last></author>
      <author><first>Zhi</first><last>Jin</last></author>
      <pages>5878–5895</pages>
      <abstract>Large Language Models (LLMs) have recently made significant advances in code generation through the ‘Chain-of-Thought’ prompting technique. This technique empowers the model to autonomously devise “solution plans” to tackle intricate programming challenges, thereby improving its performance in code generation. Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities. Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation. Consequently, there arises a compelling need for transferring LLMs’ code generation reasoning abilities to the smaller models. In this paper, we propose the CodePLAN framework, which aims to transfer LLMs’ reasoning capabilities to smaller models through distillation. We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of smaller model. To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies. Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model’s code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark.</abstract>
      <url hash="7c80acd5">2024.lrec-main.521</url>
      <bibkey>sun-etal-2024-enhancing-code</bibkey>
    </paper>
    <paper id="522">
      <title>Enhancing Court View Generation with Knowledge Injection and Guidance</title>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Yiquan</first><last>Wu</last></author>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Kun</first><last>Kuang</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Ming</first><last>Cai</last></author>
      <pages>5896–5906</pages>
      <abstract>Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead. Moreover, to further enhance the model’s ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model’s architecture, making it readily transferable. Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%.</abstract>
      <url hash="d7ccddda">2024.lrec-main.522</url>
      <attachment type="OptionalSupplementaryMaterial" hash="331d6ba4">2024.lrec-main.522.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2024-enhancing-court</bibkey>
    </paper>
    <paper id="523">
      <title>Enhancing Cross-Document Event Coreference Resolution by Discourse Structure and Semantic Information</title>
      <author><first>Qiang</first><last>Gao</last></author>
      <author><first>Bobo</first><last>Li</last></author>
      <author><first>Zixiang</first><last>Meng</last></author>
      <author><first>Yunlong</first><last>Li</last></author>
      <author><first>Jun</first><last>Zhou</last></author>
      <author><first>Fei</first><last>Li</last></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>5907–5921</pages>
      <abstract>Existing cross-document event coreference resolution models, which either compute mention similarity directly or enhance mention representation by extracting event arguments (such as location, time, agent, and patient), lackingmthe ability to utilize document-level information. As a result, they struggle to capture long-distance dependencies. This shortcoming leads to their underwhelming performance in determining coreference for the events where their argument information relies on long-distance dependencies. In light of these limitations, we propose the construction of document-level Rhetorical Structure Theory (RST) trees and cross-document Lexical Chains to model the structural and semantic information of documents. Subsequently, cross-document heterogeneous graphs are constructed and GAT is utilized to learn the representations of events. Finally, a pair scorer calculates the similarity between each pair of events and co-referred events can be recognized using standard clustering algorithm. Additionally, as the existing cross-document event coreference datasets are limited to English, we have developed a large-scale Chinese cross-document event coreference dataset to fill this gap, which comprises 53,066 event mentions and 4,476 clusters. After applying our model on the English and Chinese datasets respectively, it outperforms all baselines by large margins.</abstract>
      <url hash="4cc0e571">2024.lrec-main.523</url>
      <bibkey>gao-etal-2024-enhancing-cross</bibkey>
    </paper>
    <paper id="524">
      <title>Enhancing Distantly Supervised Named Entity Recognition with Strong Label Guided Lottery Training</title>
      <author><first>Zhiyuan</first><last>Ma</last></author>
      <author><first>Jintao</first><last>Du</last></author>
      <author><first>Changhua</first><last>Meng</last></author>
      <author><first>Weiqiang</first><last>Wang</last></author>
      <pages>5922–5929</pages>
      <abstract>In low-resource Named Entity Recognition (NER) scenarios, only a limited quantity of strongly labeled data is available, while a vast amount of weakly labeled data can be easily acquired through distant supervision. However, weakly labeled data may fail to improve the model performance or even harm it due to the inevitable noise. While training on noisy data, only certain parameters are essential for model learning, termed safe parameters, whereas the other parameters tend to fit noise. In this paper, we propose a noise-robust learning framework where safe parameters can be identified with guidance from the small set of strongly labeled data, and non-safe parameters are suppressed during training on weakly labeled data for better generalization. Our method can effectively mitigate the impact of noise in weakly labeled data, and it can be easily integrated with data level noise-robust learning methods for NER. We conduct extensive experiments on multiple datasets and the results show that our approach outperforms the state-of-the-art methods.</abstract>
      <url hash="ff397113">2024.lrec-main.524</url>
      <bibkey>ma-etal-2024-enhancing-distantly</bibkey>
    </paper>
    <paper id="525">
      <title>Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation</title>
      <author><first>Kyohoon</first><last>Jin</last></author>
      <author><first>Junho</first><last>Lee</last></author>
      <author><first>Juhwan</first><last>Choi</last></author>
      <author><first>Sangmin</first><last>Song</last></author>
      <author><first>Youngbin</first><last>Kim</last></author>
      <pages>5930–5943</pages>
      <abstract>Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods, such as mixup and cutout, is limited due to the discrete characteristics of the textual data. While methods using pre trained language models have exhibited good efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive experiments. Furthermore, the ablation study demonstrates the effect of soft labels and mid-K sampling and the extensibility of the method with curriculum data augmentation.</abstract>
      <url hash="112d2713">2024.lrec-main.525</url>
      <bibkey>jin-etal-2024-enhancing-effectiveness</bibkey>
    </paper>
    <paper id="526">
      <title>Enhancing Emotion Prediction in News Headlines: Insights from <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> and <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Models for Free-Text Generation</title>
      <author><first>Ge</first><last>Gao</last></author>
      <author><first>Jongin</first><last>Kim</last></author>
      <author><first>Sejin</first><last>Paik</last></author>
      <author><first>Ekaterina</first><last>Novozhilova</last></author>
      <author><first>Yi</first><last>Liu</last></author>
      <author><first>Sarah T.</first><last>Bonna</last></author>
      <author><first>Margrit</first><last>Betke</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>5944–5955</pages>
      <abstract>Predicting emotions elicited by news headlines can be challenging as the task is largely influenced by the varying nature of people’s interpretations and backgrounds. Previous works have explored classifying discrete emotions directly from news headlines. We provide a different approach to tackling this problem by utilizing people’s explanations of their emotion, written in free-text, on how they feel after reading a news headline. Using the dataset BU-NEmo+ (Gao et al., 2022), we found that for emotion classification, the free-text explanations have a strong correlation with the dominant emotion elicited by the headlines. The free-text explanations also contain more sentimental context than the news headlines alone and can serve as a better input to emotion classification models. Therefore, in this work we explored generating emotion explanations from headlines by training a sequence-to-sequence transformer model and by using pretrained large language model, ChatGPT (GPT-4). We then used the generated emotion explanations for emotion classification. In addition, we also experimented with training the pretrained T5 model for the intermediate task of explanation generation before fine-tuning it for emotion classification. Using McNemar’s significance test, methods that incorporate GPT-generated free-text emotion explanations demonstrated significant improvement (P-value &lt; 0.05) in emotion classification from headlines, compared to methods that only use headlines. This underscores the value of using intermediate free-text explanations for emotion prediction tasks with headlines.</abstract>
      <url hash="bec381d2">2024.lrec-main.526</url>
      <bibkey>gao-etal-2024-enhancing-emotion-prediction</bibkey>
    </paper>
    <paper id="527">
      <title>Enhancing Few-Shot Topic Classification with Verbalizers. a Study on Automatic Verbalizer and Ensemble Methods</title>
      <author><first>Quang Anh</first><last>Nguyen</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Mustapha</first><last>Lebbah</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <author><first>Hanene</first><last>Azzag</last></author>
      <author><first>Santiago</first><last>Cordoba Muñoz</last></author>
      <pages>5956–5965</pages>
      <abstract>As pretrained language model emerge and consistently develop, prompt-based training has become a well-studied paradigm to improve the exploitation of models for many natural language processing tasks. Furthermore, prompting demonstrates great performance compared to conventional fine-tuning in scenarios with limited annotated data, such as zero-shot or few-shot situations. Verbalizers are crucial in this context, as they help interpret masked word distributions generated by language models into output predictions. This study introduces a benchmarking approach to assess three common baselines of verbalizers for topic classification in few-shot learning scenarios. Additionally, we find that increasing the number of label words for automatic label word searching enhances model performance. Moreover, we investigate the effectiveness of template assembling with various aggregation strategies to develop stronger classifiers that outperform models trained with individual templates. Our approach achieves comparable results to prior research while using significantly fewer resources. Our code is available at https://github.com/quang-anh-nguyen/verbalizer_benchmark.git.</abstract>
      <url hash="c3a54240">2024.lrec-main.527</url>
      <bibkey>nguyen-etal-2024-enhancing-shot</bibkey>
    </paper>
    <paper id="528">
      <title>Enhancing <fixed-case>H</fixed-case>indi Feature Representation through Fusion of Dual-Script Word Embeddings</title>
      <author><first>Lianxi</first><last>Wang</last></author>
      <author><first>Yujia</first><last>Tian</last></author>
      <author><first>Zhuowei</first><last>Chen</last></author>
      <pages>5966–5976</pages>
      <abstract>Pretrained language models excel in various natural language processing tasks but often neglect the integration of different scripts within a language, constraining their ability to capture richer semantic information, such as in Hindi. In this work, we present a dual-script enhanced feature representation method for Hindi. We combine single-script features from Devanagari and Romanized Hindi Roberta using concatenation, addition, cross-attention, and convolutional networks. The experiment results show that using a dual-script approach significantly improves model performance across various tasks. The addition fusion technique excels in sequence generation tasks, while for text classification, the CNN-based dual-script enhanced representation performs best with longer sentences, and the addition fusion technique is more effective for shorter sequences. Our approach shows significant advantages in multiple natural language processing tasks, providing a new perspective on feature representation for Hindi. Our code has been released on https://github.com/JohnnyChanV/Hindi-Fusion.</abstract>
      <url hash="62054abf">2024.lrec-main.528</url>
      <bibkey>wang-etal-2024-enhancing-hindi</bibkey>
    </paper>
    <paper id="529">
      <title>Enhancing Image-to-Text Generation in Radiology Reports through Cross-modal Multi-Task Learning</title>
      <author><first>Nurbanu</first><last>Aksoy</last></author>
      <author><first>Nishant</first><last>Ravikumar</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>5977–5985</pages>
      <abstract>Image-to-text generation involves automatically generating descriptive text from images and has applications in medical report generation. However, traditional approaches often exhibit a semantic gap between visual and textual information. In this paper, we propose a multi-task learning framework to leverage both visual and non-imaging data for generating radiology reports. Along with chest X-ray images, 10 additional features comprising numeric, binary, categorical, and text data were incorporated to create a unified representation. The model was trained to generate text, predict the degree of patient severity, and identify medical findings. Multi-task learning, especially with text generation prioritisation, improved performance over single-task baselines across language generation metrics. The framework also mitigated overfitting in auxiliary tasks compared to single-task models. Qualitative analysis showed logically coherent narratives and accurate identification of findings, though some repetition and disjointed phrasing remained. This work demonstrates the benefits of multi-modal, multi-task learning for image-to-text generation applications.</abstract>
      <url hash="e9ce8ab0">2024.lrec-main.529</url>
      <bibkey>aksoy-etal-2024-enhancing-image</bibkey>
    </paper>
    <paper id="530">
      <title>Enhancing Knowledge Retrieval with Topic Modeling for Knowledge-Grounded Dialogue</title>
      <author><first>Nhat</first><last>Tran</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <pages>5986–5995</pages>
      <abstract>Knowledge retrieval is one of the major challenges in building a knowledge-grounded dialogue system. A common method is to use a neural retriever with a distributed approximate nearest-neighbor database to quickly find the relevant knowledge sentences. In this work, we propose an approach that utilizes topic modeling on the knowledge base to further improve retrieval accuracy and as a result, improve response generation. Additionally, we experiment with a large language model (LLM), ChatGPT, to take advantage of the improved retrieval performance to further improve the generation results. Experimental results on two datasets show that our approach can increase retrieval and generation performance. The results also indicate that ChatGPT is a better response generator for knowledge-grounded dialogue when relevant knowledge is provided.</abstract>
      <url hash="58f5ec9a">2024.lrec-main.530</url>
      <bibkey>tran-litman-2024-enhancing-knowledge</bibkey>
    </paper>
    <paper id="531">
      <title>Enhancing Knowledge Selection via Multi-level Document Semantic Graph</title>
      <author><first>Haoran</first><last>Zhang</last></author>
      <author><first>Tan</first><last>Yongmei</last></author>
      <pages>5996–6006</pages>
      <abstract>Knowledge selection is a crucial sub-task of Document Grounded Dialogue System. Existing methods view knowledge selection as a sentence matching or classification. However, those methods can’t capture the semantic relationships within complex document. We propose a flexible method that can construct multi-level document semantic graph from the grounding document automatically and store semantic relationships within the documents effectively. Besides, we also devise an auxiliary task to leverage the graph more efficiently and can help the optimization of knowledge selection task. We conduct extensive experiments on public datasets: WoW(CITATION) and Holl-E(CITATION). And we achieves state-of-the-art result on WoW. Our code has been released at https://github.com/ddf62/multi-level-semantic-document-graph.</abstract>
      <url hash="bf730f2a">2024.lrec-main.531</url>
      <bibkey>zhang-yongmei-2024-enhancing-knowledge</bibkey>
    </paper>
    <paper id="532">
      <title>Enhancing Large Language Models through Transforming Reasoning Problems into Classification Tasks</title>
      <author><first>Tarun</first><last>Raheja</last></author>
      <author><first>Raunak</first><last>Sinha</last></author>
      <author><first>Advit</first><last>Deepak</last></author>
      <author><first>Will</first><last>Healy</last></author>
      <author><first>Jayanth</first><last>Srinivasa</last></author>
      <author><first>Myungjin</first><last>Lee</last></author>
      <author><first>Ramana</first><last>Kompella</last></author>
      <pages>6007–6016</pages>
      <abstract>In this paper, we introduce a novel approach for enhancing the reasoning capabilities of large language models (LLMs) for constraint satisfaction problems (CSPs), by converting reasoning problems into classification tasks. Our method leverages the LLM’s ability to decide when to call a function from a set of logical-linguistic primitives, each of which can interact with a local “scratchpad” memory and logical inference engine. Invocation of these primitives in the correct order writes the constraints to the scratchpad memory and enables the logical engine to verifiably solve the problem. We additionally propose a formal framework for exploring the “linguistic” hardness of CSP reasoning-problems for LLMs. Our experimental results demonstrate that under our proposed method, tasks with significant computational hardness can be converted to a form that is easier for LLMs to solve and yields a 40% improvement over baselines. This opens up new avenues for future research into hybrid cognitive models that integrate symbolic and neural approaches.</abstract>
      <url hash="567228a1">2024.lrec-main.532</url>
      <bibkey>raheja-etal-2024-enhancing-large</bibkey>
    </paper>
    <paper id="533">
      <title>Enhancing Low-Resource <fixed-case>LLM</fixed-case>s Classification with <fixed-case>PEFT</fixed-case> and Synthetic Data</title>
      <author><first>Parth</first><last>Patwa</last></author>
      <author><first>Simone</first><last>Filice</last></author>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Giuseppe</first><last>Castellucci</last></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <pages>6017–6023</pages>
      <abstract>Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.</abstract>
      <url hash="0e2f37a2">2024.lrec-main.533</url>
      <bibkey>patwa-etal-2024-enhancing-low</bibkey>
    </paper>
    <paper id="534">
      <title>Enhancing Parameter-efficient Fine-tuning with Simple Calibration Based on Stable Rank</title>
      <author><first>Peiyu</first><last>Liu</last></author>
      <author><first>Ze-Feng</first><last>Gao</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>6024–6035</pages>
      <abstract>Lightweight fine-tuning is widely used as an important technique for efficiently adapting pre-trained language models (PLM) to downstream tasks. Despite the reduction in trainable parameters, existing lightweight fine-tuning methods are found to be effective in low-resource settings but often fail in high-resource settings, leading to unreliable outcomes. This limitation can be attributed to inflexible strategies: they identify the parameters of the model to be trained before fine-tuning and remain unchanged without taking into account the inherent variance of generalization ability in model components (<i>i.e.</i>, feed-forward, attention layers) and potential changes during the fine-tuning process. In this paper, we introduce a simple but effective calibration for lightweight fine-tuning PLMs based on the matrix’s stable rank according to both model components and the training process. We proposed both theoretical analyses and experimental verification for the proposed calibration strategy. Considering efficiency, we further propose time-aware and structure-aware strategies to determine the most crucial time to commence the fine-tuning procedure and selectively apply parameter matrices for lightweight fine-tuning, respectively. Extensive experiments demonstrate the superiority of our proposed fine-tuning approach (average improvement 3.1 for GLUE score compared to lightweight fine-tuning method).</abstract>
      <url hash="28a5a908">2024.lrec-main.534</url>
      <bibkey>liu-etal-2024-enhancing-parameter</bibkey>
    </paper>
    <paper id="535">
      <title>Enhancing Phrase Representation by Information Bottleneck Guided Text Diffusion Process for Keyphrase Extraction</title>
      <author><first>Yuanzhen</first><last>Luo</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Feng</first><last>Zhou</last></author>
      <pages>6036–6047</pages>
      <abstract>Keyphrase extraction (KPE) is an important task in Natural Language Processing for many scenarios, which aims to extract keyphrases that are present in a given document. Many existing supervised methods treat KPE as sequential labeling, span-level classification, or generative tasks. However, these methods lack the ability to utilize keyphrase information, which may result in biased results. In this study, we propose Diff-KPE, which leverages the supervised Variational Information Bottleneck (VIB) to guide the text diffusion process for generating enhanced keyphrase representations. Diff-KPE first generates the desired keyphrase embeddings conditioned on the entire document and then injects the generated keyphrase embeddings into each phrase representation. A ranking network and VIB are then optimized together with rank loss and classification loss, respectively. This design of Diff-KPE allows us to rank each candidate phrase by utilizing both the information of keyphrases and the document. Experiments show that Diff-KPE outperforms existing KPE methods on a large open domain keyphrase extraction benchmark, OpenKP, and a scientific domain dataset, KP20K.</abstract>
      <url hash="6911a2df">2024.lrec-main.535</url>
      <bibkey>luo-etal-2024-enhancing-phrase</bibkey>
    </paper>
    <paper id="536">
      <title>Enhancing Scientific Document Summarization with Research Community Perspective and Background Knowledge</title>
      <author><first>Sudipta</first><last>Singha Roy</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <pages>6048–6058</pages>
      <abstract>Scientific paper summarization has been the focus of much recent research. Unlike previous research which summarizes only the paper in question, or which summarizes the paper and the papers that it references, or which summarizes the paper and the citing sentences from the papers that cite it, this work puts all three of these summarization techniques together. To accomplish this, we have, by utilizing the citation network, introduced a corpus for scientific document summarization that provides information about the document being summarized, the papers referenced by it, as well as the papers that have cited it. The proposed summarizer model utilizes the referenced articles as background information and citing articles to capture the impact of the scientific document on the research community. Another aspect of the proposed model is its ability to generate both the extractive and abstractive summaries in parallel. The parallel training helps the counterparts to improve their individual performance. Results have shown that the summaries are of high quality when considering the standard metrics.</abstract>
      <url hash="7651ea04">2024.lrec-main.536</url>
      <bibkey>singha-roy-mercer-2024-enhancing-scientific</bibkey>
    </paper>
    <paper id="537">
      <title>Enhancing Semantics in Multimodal Chain of Thought via Soft Negative Sampling</title>
      <author><first>Guangmin</first><last>Zheng</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Xiaobing</first><last>Zhou</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>6059–6076</pages>
      <abstract>Chain of thought (CoT) has proven useful for problems requiring complex reasoning. Many of these problems are both textual and multimodal. Given the inputs in different modalities, a model generates a rationale and then uses it to answer a question. Because of the hallucination issue, the generated soft negative rationales with high textual quality but illogical semantics do not always help improve answer accuracy. This study proposes a rationale generation method using soft negative sampling (SNSE-CoT) to mitigate hallucinations in multimodal CoT. Five methods were applied to generate soft negative samples that shared highly similar text but had different semantics from the original. Bidirectional margin loss (BML) was applied to introduce them into the traditional contrastive learning framework that involves only positive and negative samples. Extensive experiments on the ScienceQA dataset demonstrated the effectiveness of the proposed method. Code and data are released at https://github.com/zgMin/SNSE-CoT.</abstract>
      <url hash="1961897c">2024.lrec-main.537</url>
      <bibkey>zheng-etal-2024-enhancing-semantics</bibkey>
    </paper>
    <paper id="538">
      <title>Enhancing <fixed-case>T</fixed-case>aiwanese Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems</title>
      <author><first>Bo-Han</first><last>Lu</last></author>
      <author><first>Yi-Hsuan</first><last>Lin</last></author>
      <author><first>Annie</first><last>Lee</last></author>
      <author><first>Richard Tzong-Han</first><last>Tsai</last></author>
      <pages>6077–6090</pages>
      <abstract>Machine translation focuses mainly on high-resource languages (HRLs), while low-resource languages (LRLs) like Taiwanese Hokkien are relatively under-explored. The study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA 2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien as well as between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus still further improves the model’s Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we introduce an evaluation method incorporating back-translation and GPT-4 to ensure reliable translation quality assessment even for LRLs. The study contributes to narrowing the resource gap for Taiwanese Hokkien and empirically investigates the advantages and limitations of pre-training and fine-tuning based on LLaMA 2.</abstract>
      <url hash="0f3e1e64">2024.lrec-main.538</url>
      <bibkey>lu-etal-2024-enhancing-taiwanese</bibkey>
    </paper>
    <paper id="539">
      <title>Enhancing Text-to-<fixed-case>SQL</fixed-case> Capabilities of Large Language Models through Tailored Promptings</title>
      <author><first>Zhao</first><last>Tan</last></author>
      <author><first>Xiping</first><last>Liu</last></author>
      <author><first>Qing</first><last>Shu</last></author>
      <author><first>Xi</first><last>Li</last></author>
      <author><first>Changxuan</first><last>Wan</last></author>
      <author><first>Dexi</first><last>Liu</last></author>
      <author><first>Qizhi</first><last>Wan</last></author>
      <author><first>Guoqiong</first><last>Liao</last></author>
      <pages>6091–6109</pages>
      <abstract>Large language models (LLMs) with prompting have achieved encouraging results on many natural language processing (NLP) tasks based on task-tailored promptings. Text-to-SQL is a critical task that generates SQL queries from natural language questions. However, prompting on LLMs haven’t show superior performance on Text-to-SQL task due to the absence of tailored promptings. In this work, we propose three promptings specifically designed for Text-to-SQL: SL-prompt, CC-prompt, and SL+CC prompt. SL-prompt is designed to guide LLMs to identify relevant tables; CC-prompt directs LLMs to generate SQL clause by clause; and SL+CC prompt is proposed to combine the strengths of these above promptings. The three prompting strategies makes three solutions for Text-to-SQL. Then, another prompting strategy, the RS-prompt is proposed to direct LLMs to select the best answer from the results of the solutions. We conducted extensive experiments, and experimental results show that our method achieved an execution accuracy of 86.2% and a test-suite accuracy of 76.9%, which is 1.1%, and 2.7% higher than the current state-of-the-art Text-to-SQL methods, respectively. The results confirmed that the proposed promptings enhanced the capabilities of LLMs on Text-to-SQL. Experimental results also show that the granularity of schema linking and the order of clause generation have great impact on the performance, which are considered little in previous research.</abstract>
      <url hash="0e76fd27">2024.lrec-main.539</url>
      <bibkey>tan-etal-2024-enhancing-text</bibkey>
      <revision id="1" href="2024.lrec-main.539v1" hash="c5c0ab96"/>
      <revision id="2" href="2024.lrec-main.539v2" hash="0e76fd27" date="2024-05-28">Deanonymize and add Acknowledgements.</revision>
    </paper>
    <paper id="540">
      <title>Enhancing Translation Ability of Large Language Models by Leveraging Task-Related Layers</title>
      <author><first>Pei</first><last>Cheng</last></author>
      <author><first>Xiayang</first><last>Shi</last></author>
      <author><first>Yinlin</first><last>Li</last></author>
      <pages>6110–6121</pages>
      <abstract>Fine-tuning Large Language Models (LLMs) for machine translation is effective but costly. It also increases the risk of overfitting and catastrophic forgetting, especially when training data is limited. To tackle these challenges, we propose a novel method that involves adjusting task-related layers in large models to better harness their machine translation capabilities. This method aims to retain the model’s knowledge on other tasks while optimizing performance on translation tasks. By revealing the structure and characteristics of attention weights through singular value decomposition (SVD), we can make fine adjustments to specific layers, leveraging the model’s potential for more accurate and efficient translations. Our method not only addresses computational resource consumption and catastrophic forgetting but also offers a new perspective on utilizing the capabilities of large models effectively. Experimental validation shows that adjusting task-related layers significantly improves performance on translation tasks while maintaining stability and accuracy on other tasks. This finding provides valuable insights for fine-tuning and applying large models, advancing the field of machine translation.</abstract>
      <url hash="7f929d6e">2024.lrec-main.540</url>
      <bibkey>cheng-etal-2024-enhancing-translation</bibkey>
    </paper>
    <paper id="541">
      <title>Enhancing Unrestricted Cross-Document Event Coreference with Graph Reconstruction Networks</title>
      <author><first>Loic</first><last>de Langhe</last></author>
      <author><first>Orphee</first><last>de Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>6122–6133</pages>
      <abstract>Event Coreference Resolution remains a challenging discourse-oriented task within the domain of Natural Language Processing. In this paper we propose a methodology where we combine traditional mention-pair coreference models with a lightweight and modular graph reconstruction algorithm. We show that building graph models on top of existing mention-pair models leads to improved performance for both a wide range of baseline mention-pair algorithms as well as a recently developed state-of-the-art model and this at virtually no added computational cost. Moreover, additional experiments seem to indicate that our method is highly robust in low-data settings and that its performance scales with increases in performance for the underlying mention-pair models.</abstract>
      <url hash="ad15c3e4">2024.lrec-main.541</url>
      <bibkey>de-langhe-etal-2024-enhancing-unrestricted</bibkey>
    </paper>
    <paper id="542">
      <title>Enhancing Writing Proficiency Classification in Developmental Education: The Quest for Accuracy</title>
      <author><first>Miguel</first><last>Da Corte</last></author>
      <author><first>Jorge</first><last>Baptista</last></author>
      <pages>6134–6143</pages>
      <abstract>Developmental Education (DevEd) courses align students’ college-readiness skills with higher education literacy demands. These courses often use automated assessment tools like Accuplacer for student placement. Existing literature raises concerns about these exams’ accuracy and placement precision due to their narrow representation of the writing process. These concerns warrant further attention within the domain of automatic placement systems, particularly in the establishment of a reference corpus of annotated essays for these systems’ machine/deep learning. This study aims at an enhanced annotation procedure to assess college students’ writing patterns more accurately. It examines the efficacy of machine-learning-based DevEd placement, contrasting Accuplacer’s classification of 100 college-intending students’ essays into two levels (Level 1 and 2) against that of 6 human raters. The classification task encompassed the assessment of the 6 textual criteria currently used by Accuplacer: mechanical conventions, sentence variety &amp; style, idea development &amp; support, organization &amp; structure, purpose &amp; focus, and critical thinking. Results revealed low inter-rater agreement, both on the individual criteria and the overall classification, suggesting human assessment of writing proficiency can be inconsistent in this context. To achieve a more accurate determination of writing proficiency and improve DevEd placement, more robust classification methods are thus required.</abstract>
      <url hash="181be53f">2024.lrec-main.542</url>
      <bibkey>da-corte-baptista-2024-enhancing-writing</bibkey>
    </paper>
    <paper id="543">
      <title>Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic</title>
      <author><first>Xufeng</first><last>Zhao</last></author>
      <author><first>Mengdi</first><last>Li</last></author>
      <author><first>Wenhao</first><last>Lu</last></author>
      <author><first>Cornelius</first><last>Weber</last></author>
      <author><first>Jae Hee</first><last>Lee</last></author>
      <author><first>Kun</first><last>Chu</last></author>
      <author><first>Stefan</first><last>Wermter</last></author>
      <pages>6144–6166</pages>
      <abstract>Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT.</abstract>
      <url hash="388e1ada">2024.lrec-main.543</url>
      <bibkey>zhao-etal-2024-enhancing-zero</bibkey>
    </paper>
    <paper id="544">
      <title>Enough Is Enough! a Case Study on the Effect of Data Size for Evaluation Using <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <pages>6167–6176</pages>
      <abstract>When creating a new dataset for evaluation, one of the first considerations is the size of the dataset. If our evaluation data is too small, we risk making unsupported claims based on the results on such data. If, on the other hand, the data is too large, we waste valuable annotation time and costs that could have been used to widen the scope of our evaluation (i.e. annotate for more domains/languages). Hence, we investigate the effect of the size and a variety of sampling strategies of evaluation data to optimize annotation efforts, using dependency parsing as a test case. We show that for in-language in-domain datasets, 5,000 tokens is enough to obtain a reliable ranking of different parsers; especially if the data is distant enough from the training split (otherwise, we recommend 10,000). In cross-domain setups, the same amounts are required, but in cross-lingual setups much less (2,000 tokens) is enough.</abstract>
      <url hash="4b006843">2024.lrec-main.544</url>
      <bibkey>van-der-goot-etal-2024-enough-enough</bibkey>
    </paper>
    <paper id="545">
      <title>Enriching a Time-Domain Astrophysics Corpus with Named Entity, Coreference and Astrophysical Relationship Annotations</title>
      <author><first>Atilla Kaan</first><last>Alkan</last></author>
      <author><first>Felix</first><last>Grezes</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Fabian</first><last>Schussler</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>6177–6188</pages>
      <abstract>Interest in Astrophysical Natural Language Processing (NLP) has increased recently, fueled by the development of specialized language models for information extraction. However, the scarcity of annotated resources for this domain is still a significant challenge. Most existing corpora are limited to Named Entity Recognition (NER) tasks, leaving a gap in resource diversity. To address this gap and facilitate a broader spectrum of NLP research in astrophysics, we introduce astroECR, an extension of our previously built Time-Domain Astrophysics Corpus (TDAC). Our contributions involve expanding it to cover named entities, coreferences, annotations related to astrophysical relationships, and normalizing celestial object names. We showcase practical utility through baseline models for four NLP tasks and provide the research community access to our corpus, code, and models.</abstract>
      <url hash="dee97ba0">2024.lrec-main.545</url>
      <bibkey>alkan-etal-2024-enriching-time</bibkey>
    </paper>
    <paper id="546">
      <title>Enriching Word Usage Graphs with Cluster Definitions</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Mariia</first><last>Fedorova</last></author>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <pages>6189–6198</pages>
      <abstract>We present a dataset of word usage graphs (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by fine-tuned encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling.</abstract>
      <url hash="39635d01">2024.lrec-main.546</url>
      <bibkey>kutuzov-etal-2024-enriching-word</bibkey>
    </paper>
    <paper id="547">
      <title>Ensembles of Hybrid and End-to-End Speech Recognition.</title>
      <author><first>Aditya Kamlesh</first><last>Parikh</last></author>
      <author><first>Louis</first><last>ten Bosch</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <pages>6199–6205</pages>
      <abstract>We propose a method to combine the hybrid Kaldi-based Automatic Speech Recognition (ASR) system with the end-to-end wav2vec 2.0 XLS-R ASR using confidence measures. Our research is focused on the low-resource Irish language. Given the limited available open-source resources, neither the standalone hybrid ASR nor the end-to-end ASR system can achieve optimal performance. By applying the Recognizer Output Voting Error Reduction (ROVER) technique, we illustrate how ensemble learning could facilitate mutual error correction between both ASR systems. This paper outlines the strategies for merging the hybrid Kaldi ASR model and the end-to-end XLS-R model with the help of confidence scores. Although contemporary state-of-the-art end-to-end ASR models face challenges related to prediction overconfidence, we utilize Renyi’s entropy-based confidence approach, tuned with temperature scaling, to align it with the Kaldi ASR confidence. Although there was no significant difference in the Word Error Rate (WER) between the hybrid and end-to-end ASR, we could achieve a notable reduction in WER after ensembling through ROVER. This resulted in an almost 14% Word Error Rate Reduction (WERR) on our primary test set and an approximately 20% WERR on other noisy and imbalanced test data.</abstract>
      <url hash="4c751171">2024.lrec-main.547</url>
      <bibkey>parikh-etal-2024-ensembles-hybrid</bibkey>
    </paper>
    <paper id="548">
      <title><fixed-case>E</fixed-case>pi<fixed-case>GEN</fixed-case>: An Efficient Multi-Api Code <fixed-case>GEN</fixed-case>eration Framework under Enterprise Scenario</title>
      <author><first>Sijie</first><last>Li</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Shuyang</first><last>Li</last></author>
      <author><first>Kai</first><last>Chen</last></author>
      <author><first>Jianyong</first><last>Yuan</last></author>
      <author><first>Yi</first><last>Cao</last></author>
      <author><first>Lvqing</first><last>Yang</last></author>
      <pages>6206–6215</pages>
      <abstract>In recent years, Large Language Models (LLMs) have demonstrated exceptional performance in code-generation tasks. However, under enterprise scenarios where private APIs are pre-built, general LLMs often fail to meet expectations. Existing approaches are confronted with drawbacks of high resource consumption and inadequate handling of multi-API tasks. To address these challenges, we propose EpiGEN, an Efficient multi-Api code GENeration framework under enterprise scenario. It consists of three core modules: Task Decomposition Module (TDM), API Retrieval Module (ARM), and Code Generation Module (CGM), in which Langchain played an important role. Through a series of experiments, EpiGEN shows good acceptability and readability, compared to fully fine-tuned LLM with a larger number of parameters. Particularly, in medium and hard level tasks, the performance of EpiGEN on a single-GPU machine even surpasses that of a fully fine-tuned LLM that requires multi-GPU configuration. Generally, EpiGEN is model-size agnostic, facilitating a balance between the performance of code generation and computational requirements.</abstract>
      <url hash="17833063">2024.lrec-main.548</url>
      <bibkey>li-etal-2024-epigen-efficient</bibkey>
    </paper>
    <paper id="549">
      <title><fixed-case>E</fixed-case>p<fixed-case>LSA</fixed-case>: Synergy of Expert-prefix Mixtures and Task-Oriented Latent Space Adaptation for Diverse Generative Reasoning</title>
      <author><first>Fujun</first><last>Zhang</last></author>
      <author><first>Xiangdong</first><last>Su</last></author>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Rong</first><last>Yan</last></author>
      <author><first>Guanglai</first><last>Gao</last></author>
      <pages>6216–6227</pages>
      <abstract>Existing models for diverse generative reasoning still struggle to generate multiple unique and plausible results. Through an in-depth examination, we argue that it is critical to leverage a mixture of experts as prefixes to enhance the diversity of generated results and make task-oriented adaptation in the latent space of the generation models to improve the quality of the responses. At this point, we propose EpLSA, an innovative model based on the synergy of expert-prefix mixtures and task-oriented latent space adaptation for diverse generative reasoning. Specifically, we use expert-prefixes mixtures to encourage the model to create multiple responses with different semantics and design a loss function to address the problem that the semantics is interfered by the expert-prefixes. Meanwhile, we design a task-oriented adaptation block to make the pre-trained encoder within the generation model more effectively adapted to the pre-trained decoder in the latent space, thus further improving the quality of the generated text. Extensive experiments on three different types of generative reasoning tasks demonstrate that EpLSA outperforms existing baseline models in terms of both the quality and diversity of the generated outputs. Our code is publicly available at https://github.com/IMU-MachineLearningSXD/EpLSA.</abstract>
      <url hash="900354c3">2024.lrec-main.549</url>
      <bibkey>zhang-etal-2024-eplsa-synergy</bibkey>
    </paper>
    <paper id="550">
      <title><fixed-case>EPOQUE</fixed-case>: An <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>ersian Quality Estimation Dataset</title>
      <author><first>Mohammed Hossein</first><last>Jafari Harandi</last></author>
      <author><first>Fatemeh</first><last>Azadi</last></author>
      <author><first>Mohammad Javad</first><last>Dousti</last></author>
      <author><first>Heshaam</first><last>Faili</last></author>
      <pages>6228–6235</pages>
      <abstract>Translation quality estimation (QE) is an important component in real-world machine translation applications. Unfortunately, human labeled QE datasets, which play an important role in developing and assessing QE models, are only available for limited language pairs. In this paper, we present the first English-Persian QE dataset, called EPOQUE, which has manually annotated direct assessment labels. EPOQUE contains 1000 sentences translated from English to Persian and annotated by three human annotators. It is publicly available, and thus can be used as a zero-shot test set, or for other scenarios in future work. We also evaluate and report the performance of two state-of-the-art QE models, i.e., Transquest and CometKiwi, as baselines on our dataset. Furthermore, our experiments show that using a small subset of the proposed dataset containing 300 sentences to fine-tune Transquest, can improve its performance by more that 8% in terms of the Pearson correlation with a held-out test set.</abstract>
      <url hash="241471c7">2024.lrec-main.550</url>
      <bibkey>jafari-harandi-etal-2024-epoque-english</bibkey>
    </paper>
    <paper id="551">
      <title><fixed-case>EROS</fixed-case>:Entity-Driven Controlled Policy Document Summarization</title>
      <author><first>Joykirat</first><last>Singh</last></author>
      <author><first>Sehban</first><last>Fazili</last></author>
      <author><first>Rohan</first><last>Jain</last></author>
      <author><first>Md. Shad</first><last>Akhtar</last></author>
      <pages>6236–6246</pages>
      <abstract>Privacy policy documents have a crucial role in educating individuals about the collection, usage, and protection of users’ personal data by organizations. However, they are notorious for their lengthy, complex, and convoluted language especially involving privacy-related entities. Hence, they pose a significant challenge to users who attempt to comprehend organization’s data usage policy. In this paper, we propose to enhance the interpretability and readability of policy documents by using controlled abstractive summarization – we enforce the generated summaries to include critical privacy-related entities (e.g., data and medium) and organization’s rationale (e.g., target and reason) in collecting those entities. To achieve this, we develop PD-Sum, a policy-document summarization dataset with marked privacy-related entity labels. Our proposed model, EROS, identifies critical entities through a span-based entity extraction model and employs them to control the information content of the summaries using proximal policy optimization (PPO). Comparison shows encouraging improvement over various baselines. Furthermore, we furnish qualitative and human evaluations to establish the efficacy of EROS.</abstract>
      <url hash="80c6e562">2024.lrec-main.551</url>
      <bibkey>singh-etal-2024-eros-entity</bibkey>
    </paper>
    <paper id="552">
      <title>Error Analysis of <fixed-case>NLP</fixed-case> Models and Non-Native Speakers of <fixed-case>E</fixed-case>nglish Identifying Sarcasm in <fixed-case>R</fixed-case>eddit Comments</title>
      <author><first>Oliver</first><last>Cakebread-Andrews</last></author>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Ingo</first><last>Frommholz</last></author>
      <author><first>Burcu</first><last>Can</last></author>
      <pages>6247–6256</pages>
      <abstract>This paper summarises the differences and similarities found between humans and three natural language processing models when attempting to identify whether English online comments are sarcastic or not. Three models were used to analyse 300 comments from the FigLang 2020 Reddit Dataset, with and without context. The same 300 comments were also given to 39 non-native speakers of English and the results were compared. The aim was to find whether there were any results that could be applied to English as a Foreign Language (EFL) teaching. The results showed that there were similarities between the models and non-native speakers, in particular the logistic regression model. They also highlighted weaknesses with both non-native speakers and the models in detecting sarcasm when the comments included political topics or were phrased as questions. This has potential implications for how the EFL teaching industry could implement the results of error analysis of NLP models in teaching practices.</abstract>
      <url hash="28a66ba0">2024.lrec-main.552</url>
      <bibkey>cakebread-andrews-etal-2024-error-analysis</bibkey>
    </paper>
    <paper id="553">
      <title>Error-Robust Retrieval for <fixed-case>C</fixed-case>hinese Spelling Check</title>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xinyu</first><last>Hu</last></author>
      <author><first>Jin</first><last>Jiang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>6257–6267</pages>
      <abstract>Chinese Spelling Check (CSC) aims to detect and correct error tokens in Chinese contexts, which has a wide range of applications. However, it is confronted with the challenges of insufficient annotated data and the issue that previous methods may actually not fully leverage the existing datasets. In this paper, we introduce our plug-and-play retrieval method with error-robust information for Chinese Spelling Check (RERIC), which can be directly applied to existing CSC models. The datastore for retrieval is built completely based on the training data, with elaborate designs according to the characteristics of CSC. Specifically, we employ multimodal representations that fuse phonetic, morphologic, and contextual information in the calculation of query and key during retrieval to enhance robustness against potential errors. Furthermore, in order to better judge the retrieved candidates, the n-gram surrounding the token to be checked is regarded as the value and utilized for specific reranking. The experiment results on the SIGHAN benchmarks demonstrate that our proposed method achieves substantial improvements over existing work.</abstract>
      <url hash="4281c53b">2024.lrec-main.553</url>
      <bibkey>yin-etal-2024-error-robust</bibkey>
    </paper>
    <paper id="554">
      <title><fixed-case>E</fixed-case>s<fixed-case>C</fixed-case>o<fixed-case>LA</fixed-case>: <fixed-case>S</fixed-case>panish Corpus of Linguistic Acceptability</title>
      <author><first>Nuria</first><last>Bel</last></author>
      <author><first>Marta</first><last>Punsola</last></author>
      <author><first>Valle</first><last>Ruíz-Fernández</last></author>
      <pages>6268–6277</pages>
      <abstract>Acceptability is one of the General Language Understanding Evaluation Benchmark (GLUE) probing tasks proposed to assess the linguistic capabilities acquired by a deep-learning transformer-based language model (LM). In this paper, we introduce the Spanish Corpus of Linguistic Acceptability EsCoLA. EsCoLA has been developed following the example of other linguistic acceptability data sets for English, Italian, Norwegian or Russian, with the aim of having a complete GLUE benchmark for Spanish. EsCoLA consists of 11,174 sentences and their acceptability judgements as found in well-known Spanish reference grammars. Additionally, all sentences have been annotated with the class of linguistic phenomenon the sentence is an example of, also following previous practices. We also provide as task baselines the results of fine-tuning four different language models with this data set and the results of a human annotation experiment. Results are also analyzed and commented to guide future research. EsCoLA is released under a CC-BY 4.0 license and freely available at https://doi.org/10.34810/data1138.</abstract>
      <url hash="3d1d60b3">2024.lrec-main.554</url>
      <bibkey>bel-etal-2024-escola-spanish</bibkey>
    </paper>
    <paper id="555">
      <title><fixed-case>ESCP</fixed-case>: Enhancing Emotion Recognition in Conversation with Speech and Contextual Prefixes</title>
      <author><first>Xiujuan</first><last>Xu</last></author>
      <author><first>Xiaoxiao</first><last>Shi</last></author>
      <author><first>Zhehuan</first><last>Zhao</last></author>
      <author><first>Yu</first><last>Liu</last></author>
      <pages>6278–6287</pages>
      <abstract>Emotion Recognition in Conversation (ERC) aims to analyze the speaker’s emotional state in a conversation. Fully mining the information in multimodal and historical utterances plays a crucial role in the performance of the model. However, recent works in ERC focus on historical utterances modeling and generally concatenate the multimodal features directly, which neglects mining deep multimodal information and brings redundancy at the same time. To address the shortcomings of existing models, we propose a novel model, termed Enhancing Emotion Recognition in Conversation with Speech and Contextual Prefixes (ESCP). ESCP employs a directed acyclic graph (DAG) to model historical utterances in a conversation and incorporates a contextual prefix containing the sentiment and semantics of historical utterances. By adding speech and contextual prefixes, the inter- and intra-modal emotion information is efficiently modeled using the prior knowledge of the large-scale pre-trained model. Experiments conducted on several public benchmarks demonstrate that the proposed approach achieves state-of-the-art (SOTA) performances. These results affirm the effectiveness of the novel ESCP model and underscore the significance of incorporating speech and contextual prefixes to guide the pre-trained model.</abstract>
      <url hash="05280470">2024.lrec-main.555</url>
      <bibkey>xu-etal-2024-escp-enhancing</bibkey>
    </paper>
    <paper id="556">
      <title><fixed-case>ESDM</fixed-case>: Early <fixed-case>S</fixed-case>ensing Depression Model in Social Media Streams</title>
      <author><first>Bichen</first><last>Wang</last></author>
      <author><first>Yuzhe</first><last>Zi</last></author>
      <author><first>Yanyan</first><last>Zhao</last></author>
      <author><first>Pengfei</first><last>Deng</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>6288–6298</pages>
      <abstract>Depression impacts millions worldwide, with increasing efforts to use social media data for early detection and intervention. Traditional Risk Detection (TRD) uses a user’s complete posting history for predictions, while Early Risk Detection (ERD) seeks early detection in a user’s posting history, emphasizing the importance of prediction earliness. However, ERD remains relatively underexplored due to challenges in balancing accuracy and earliness, especially with evolving partial data. To address this, we introduce the Early Sensing Depression Model (ESDM), which comprises two modules classification with partial information module (CPI) and decision for classification moment module (DMC), alongside an early detection loss function. Experiments show ESDM outperforms benchmarks in both earliness and accuracy.</abstract>
      <url hash="bacfe16d">2024.lrec-main.556</url>
      <bibkey>wang-etal-2024-esdm-early</bibkey>
    </paper>
    <paper id="557">
      <title>Esposito: An <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>ersian Scientific Parallel Corpus for Machine Translation</title>
      <author><first>Mersad</first><last>Esalati</last></author>
      <author><first>Mohammad Javad</first><last>Dousti</last></author>
      <author><first>Heshaam</first><last>Faili</last></author>
      <pages>6299–6308</pages>
      <abstract>Neural machine translation requires large number of parallel sentences along with in-domain parallel data to attain best results. Nevertheless, no scientific parallel corpus for English-Persian language pair is available. In this paper, a parallel corpus called Esposito is introduced, which contains 3.5 million parallel sentences in the scientific domain for English-Persian language pair. In addition, we present a manually validated scientific test set that might serve as a baseline for future studies. We show that a system trained using Esposito along with other publicly available data improves the baseline on average by 7.6 and 8.4 BLEU scores for En-&gt;Fa and Fa-&gt;En directions, respectively. Additionally, domain analysis using the 5-gram KenLM model revealed notable distinctions between our parallel corpus and the existing generic parallel corpus. This dataset will be available to the public upon the acceptance of the paper.</abstract>
      <url hash="d30a9ca8">2024.lrec-main.557</url>
      <bibkey>esalati-etal-2024-esposito-english</bibkey>
    </paper>
    <paper id="558">
      <title>Estimating Lexical Complexity from Document-Level Distributions</title>
      <author><first>Sondre</first><last>Wold</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Oddbjørn</first><last>Hove</last></author>
      <pages>6309–6318</pages>
      <abstract>Existing methods for complexity estimation are typically developed for entire documents. This limitation in scope makes them inapplicable for shorter pieces of text, such as health assessment tools. These typically consist of lists of independent sentences, all of which are too short for existing methods to apply. The choice of wording in these assessment tools is crucial, as both the cognitive capacity and the linguistic competency of the intended patient groups could vary substantially. As a first step towards creating better tools for supporting health practitioners, we develop a two-step approach for estimating lexical complexity that does not rely on any pre-annotated data. We implement our approach for the Norwegian language and verify its effectiveness using statistical testing and a qualitative evaluation of samples from real assessment tools. We also investigate the relationship between our complexity measure and certain features typically associated with complexity in the literature, such as word length, frequency, and the number of syllables.</abstract>
      <url hash="454c56f2">2024.lrec-main.558</url>
      <bibkey>wold-etal-2024-estimating-lexical</bibkey>
    </paper>
    <paper id="559">
      <title>Estimating the Causal Effects of Natural Logic Features in Transformer-Based <fixed-case>NLI</fixed-case> Models</title>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>6319–6329</pages>
      <abstract>Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language reasoning problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of reasoning with enough structure and regularity to identify and quantify systematic reasoning failures in widely-used models. In this vein, we pick a portion of the NLI task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on the entailment label is mediated by the relation between these words). Extending related work on causal analysis of NLP models in different settings, we perform an extensive interventional study on the NLI task to investigate robustness to irrelevant changes and sensitivity to impactful changes of Transformers. The results strongly bolster the fact that similar benchmark accuracy scores may be observed for models that exhibit very different behaviour. Moreover, our methodology reinforces previously suspected biases from a causal perspective, including biases in favour of upward-monotone contexts and ignoring the effects of negation markers.</abstract>
      <url hash="531fadb6">2024.lrec-main.559</url>
      <bibkey>rozanova-etal-2024-estimating-causal</bibkey>
    </paper>
    <paper id="560">
      <title>Ethical Reasoning and Moral Value Alignment of <fixed-case>LLM</fixed-case>s Depend on the Language We Prompt Them in</title>
      <author><first>Utkarsh</first><last>Agarwal</last></author>
      <author><first>Kumar</first><last>Tanmay</last></author>
      <author><first>Aditi</first><last>Khandelwal</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>6330–6340</pages>
      <abstract>Ethical reasoning is a crucial skill for Large Language Models (LLMs). However, moral values are not universal, but rather influenced by language and culture. This paper explores how three prominent LLMs – GPT-4, ChatGPT, and Llama2Chat-70B – perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted. We extend the study of ethical reasoning of LLMs by (CITATION) to a multilingual setup following their framework of probing LLMs with ethical dilemmas and policies from three branches of normative ethics: deontology, virtue, and consequentialism. We experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most consistent and unbiased ethical reasoner across languages, while ChatGPT and Llama2Chat-70B show significant moral value bias when we move to languages other than English. Interestingly, the nature of this bias significantly vary across languages for all LLMs, including GPT-4.</abstract>
      <url hash="34e3b9ee">2024.lrec-main.560</url>
      <bibkey>agarwal-etal-2024-ethical-reasoning</bibkey>
    </paper>
    <paper id="561">
      <title><fixed-case>E</fixed-case>thio<fixed-case>LLM</fixed-case>: Multilingual Large Language Models for <fixed-case>E</fixed-case>thiopian Languages with Task Evaluation</title>
      <author><first>Atnafu Lambebo</first><last>Tonja</last></author>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Tadesse Destaw</first><last>Belay</last></author>
      <author><first>Mesay Gemeda</first><last>Yigezu</last></author>
      <author><first>Moges Ahmed Ah</first><last>Mehamed</last></author>
      <author><first>Abinew Ali</first><last>Ayele</last></author>
      <author><first>Ebrahim Chekol</first><last>Jibril</last></author>
      <author><first>Michael Melese</first><last>Woldeyohannis</last></author>
      <author><first>Olga</first><last>Kolesnikova</last></author>
      <author><first>Philipp</first><last>Slusallek</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <pages>6341–6352</pages>
      <abstract>Large language models (LLMs) have gained popularity recently due to their outstanding performance in various downstream Natural Language Processing (NLP) tasks. However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs. Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts, and are imbued with profound religious and cultural significance. This paper introduces EthioLLM – multilingual large language models for five Ethiopian languages (Amharic, Ge’ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark – a new benchmark dataset for various downstream NLP tasks. We evaluate the performance of these models across five downstream NLP tasks. We open-source our multilingual language models, new benchmark datasets for various downstream tasks, and task-specific fine-tuned language models and discuss the performance of the models. Our dataset and models are available at the https://huggingface.co/EthioNLP repository.</abstract>
      <url hash="453734d9">2024.lrec-main.561</url>
      <bibkey>tonja-etal-2024-ethiollm-multilingual</bibkey>
    </paper>
    <paper id="562">
      <title><fixed-case>E</fixed-case>uropean Language Grid: One Year after</title>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Dimitris</first><last>Galanis</last></author>
      <author><first>Penny</first><last>Labropoulou</last></author>
      <author><first>Maria</first><last>Giagkou</last></author>
      <author><first>Miltos</first><last>Deligiannis</last></author>
      <author><first>Leon</first><last>Voukoutis</last></author>
      <author><first>Martin</first><last>Courtois</last></author>
      <author><first>Julian</first><last>Moreno-Schneider</last></author>
      <author><first>Katrin</first><last>Marheinecke</last></author>
      <pages>6353–6362</pages>
      <abstract>The European Language Grid (ELG) is a cloud platform for the whole European Language Technology community. While the EU project that developed the platform successfully concluded in June 2022, the ELG initiative has continued. This article provides a description of the current state of ELG in terms of user adoption and number of language resources and technologies available in early 2024. It also provides an overview of the various activities with regard to ELG since the end of the project and since the publication of the ELG book, especially the co-authors’ attempt to integrate the ELG platform into various data space initiatives. The article also provides an overview of the Digital Language Equality (DLE) dashboard and the current state of DLE in Europe.</abstract>
      <url hash="fe0e401e">2024.lrec-main.562</url>
      <bibkey>rehm-etal-2024-european-language</bibkey>
    </paper>
    <paper id="563">
      <title>Evaluating Automatic Subtitling: Correlating Post-editing Effort and Automatic Metrics</title>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>Mauro</first><last>Cettolo</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <pages>6363–6369</pages>
      <abstract>Systems that automatically generate subtitles from video are gradually entering subtitling workflows, both for supporting subtitlers and for accessibility purposes. Even though robust metrics are essential for evaluating the quality of automatically-generated subtitles and for estimating potential productivity gains, there is limited research on whether existing metrics, some of which directly borrowed from machine translation (MT) evaluation, can fulfil such purposes. This paper investigates how well such MT metrics correlate with measures of post-editing (PE) effort in automatic subtitling. To this aim, we collect and publicly release a new corpus containing product-, process- and participant-based data from post-editing automatic subtitles in two language pairs (en→de,it). We find that different types of metrics correlate with different aspects of PE effort. Specifically, edit distance metrics have high correlation with technical and temporal effort, while neural metrics correlate well with PE speed.</abstract>
      <url hash="7d328985">2024.lrec-main.563</url>
      <bibkey>karakanta-etal-2024-evaluating-automatic</bibkey>
    </paper>
    <paper id="564">
      <title>Evaluating <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> against Functionality Tests for Hate Speech Detection</title>
      <author><first>Mithun</first><last>Das</last></author>
      <author><first>Saurabh Kumar</first><last>Pandey</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>6370–6380</pages>
      <abstract>Large language models like ChatGPT have recently shown a great promise in performing several tasks, including hate speech detection. However, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. To bridge this gap, our study aims to evaluate the strengths and weaknesses of the ChatGPT model in detecting hate speech at a granular level across 11 languages. Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold. In addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the ChatGPT model. Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research and improvements in the workings of these models.</abstract>
      <url hash="2ccfccff">2024.lrec-main.564</url>
      <bibkey>das-etal-2024-evaluating-chatgpt</bibkey>
    </paper>
    <paper id="565">
      <title>Evaluating Code-Switching Translation with Large Language Models</title>
      <author><first>Muhammad</first><last>Huzaifah</last></author>
      <author><first>Weihua</first><last>Zheng</last></author>
      <author><first>Nattapol</first><last>Chanpaisit</last></author>
      <author><first>Kui</first><last>Wu</last></author>
      <pages>6381–6394</pages>
      <abstract>Recent advances in large language models (LLMs) have shown they can match or surpass finetuned models on many natural language processing tasks. Currently, more studies are being carried out to assess whether this performance carries over across different languages. In this paper, we present a thorough evaluation of LLMs for the less well-researched code-switching translation setting, where inputs include a mixture of different languages. We benchmark the performance of six state-of-the-art LLMs across seven datasets, with GPT-4 and GPT-3.5 displaying strong ability relative to supervised translation models and commercial engines. GPT-4 was also found to be particularly robust against different code-switching conditions. Several methods to further improve code-switching translation are proposed including leveraging in-context learning and pivot translation. Through our code-switching experiments, we argue that LLMs show promising ability for cross-lingual understanding.</abstract>
      <url hash="73444f91">2024.lrec-main.565</url>
      <bibkey>huzaifah-etal-2024-evaluating-code</bibkey>
    </paper>
    <paper id="566">
      <title>Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels</title>
      <author><first>Panatchakorn</first><last>Anantaprayoon</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>6395–6408</pages>
      <abstract>Discriminatory gender biases have been found in Pre-trained Language Models (PLMs) for multiple languages. In Natural Language Inference (NLI), existing bias evaluation methods have focused on the prediction results of one specific label out of three labels, such as neutral. However, such evaluation methods can be inaccurate since unique biased inferences are associated with unique prediction labels. Addressing this limitation, we propose a bias evaluation method for PLMs, called NLI-CoAL, which considers all the three labels of NLI task. First, we create three evaluation data groups that represent different types of biases. Then, we define a bias measure based on the corresponding label output of each data group. In the experiments, we introduce a meta-evaluation technique for NLI bias measures and use it to confirm that our bias measure can distinguish biased, incorrect inferences from non-biased incorrect inferences better than the baseline, resulting in a more accurate bias evaluation. We create the datasets in English, Japanese, and Chinese, and successfully validate the compatibility of our bias measure across multiple languages. Lastly, we observe the bias tendencies in PLMs of different languages. To our knowledge, we are the first to construct evaluation datasets and measure PLMs’ bias from NLI in Japanese and Chinese.</abstract>
      <url hash="399ba6df">2024.lrec-main.566</url>
      <bibkey>anantaprayoon-etal-2024-evaluating-gender</bibkey>
    </paper>
    <paper id="567">
      <title>Evaluating Generative Language Models in Information Extraction as Subjective Question Correction</title>
      <author><first>Yuchen</first><last>Fan</last></author>
      <author><first>Yantao</first><last>Liu</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>6409–6417</pages>
      <abstract>Modern Large Language Models (LLMs) have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like relation extraction and event extraction due to two issues in conventional evaluation. (1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation benchmarks, primarily due to restrictive human annotation schemas, resulting in underestimated LLM performances. Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels. Additionally, by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness by acknowledging correct yet previously omitted answers. Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art LLMs and provide insights for future research for information extraction. Dataset and associated codes can be accessed at our &lt;a href=https://github.com/THU-KEG/SQC-Score&gt; GitHub repository &lt;/a&gt;.</abstract>
      <url hash="5bf1ec45">2024.lrec-main.567</url>
      <bibkey>fan-etal-2024-evaluating-generative</bibkey>
    </paper>
    <paper id="568">
      <title>Evaluating Performance of Pre-trained Word Embeddings on <fixed-case>A</fixed-case>ssamese, a Low-resource Language</title>
      <author><first>Dhrubajyoti</first><last>Pathak</last></author>
      <author><first>Sukumar</first><last>Nandi</last></author>
      <author><first>Priyankoo</first><last>Sarmah</last></author>
      <pages>6418–6425</pages>
      <abstract>Word embeddings and Language models are the building blocks of modern Deep Neural Network-based Natural Language Processing. They are extensively explored in high-resource languages and provide state-of-the-art (SOTA) performance for a wide range of downstream tasks. Nevertheless, these word embeddings are not explored in languages such as Assamese, where resources are limited. Furthermore, there has been limited study into the performance evaluation of these word embeddings for low-resource languages in downstream tasks. In this research, we explore the current state of Assamese pre-trained word embeddings. We evaluate these embeddings’ performance on sequence labeling tasks such as Parts-of-speech and Named Entity Recognition. In order to assess the efficiency of the embeddings, experiments are performed utilizing both ensemble and individual word embedding approaches. The ensembling approach that uses three word embeddings outperforms the others. In the paper, the outcomes of the investigations are described. The results of this comparative performance evaluation may assist researchers in choosing an Assamese pre-trained word embedding for subsequent tasks.</abstract>
      <url hash="7cd74ebe">2024.lrec-main.568</url>
      <bibkey>pathak-etal-2024-evaluating-performance</bibkey>
    </paper>
    <paper id="569">
      <title>Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency</title>
      <author><first>Min</first><last>Zeng</last></author>
      <author><first>Jiexin</first><last>Kuang</last></author>
      <author><first>Mengyang</first><last>Qiu</last></author>
      <author><first>Jayoung</first><last>Song</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>6426–6430</pages>
      <abstract>This paper proposes an analysis of prompting strategies for grammatical error correction (GEC) with selected large language models (LLM) based on language proficiency. GEC using generative LLMs has been known for overcorrection where results obtain higher recall measures than precision measures. The writing examples of English language learners may be different from those of native speakers. Given that there is a significant differences in second language (L2) learners’ error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between LLM’s performance and L2 language proficiency. Our method focuses on zero-shot and few-shot prompting and fine-tuning models for GEC for learners of English as a foreign language based on the different proficiency. We investigate GEC results and find that overcorrection happens primarily in advanced language learners’ writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures. To make our claim concrete, we conduct a comprehensive examination of GEC outcomes and their evaluation results based on language proficiency.</abstract>
      <url hash="5a9fd03a">2024.lrec-main.569</url>
      <bibkey>zeng-etal-2024-evaluating-prompting</bibkey>
    </paper>
    <paper id="570">
      <title>Evaluating Saliency Explanations in <fixed-case>NLP</fixed-case> by Crowdsourcing</title>
      <author><first>Xiaotian</first><last>Lu</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Xiaofeng</first><last>Lin</last></author>
      <author><first>Koh</first><last>Takeuchi</last></author>
      <author><first>Hisashi</first><last>Kashima</last></author>
      <pages>6431–6443</pages>
      <abstract>Deep learning models have performed well on many NLP tasks. However, their internal mechanisms are typically difficult for humans to understand. The development of methods to explain models has become a key issue in the reliability of deep learning models in many important applications. Various saliency explanation methods, which give each feature of input a score proportional to the contribution of output, have been proposed to determine the part of the input which a model values most. Despite a considerable body of work on the evaluation of saliency methods, whether the results of various evaluation metrics agree with human cognition remains an open question. In this study, we propose a new human-based method to evaluate saliency methods in NLP by crowdsourcing. We recruited 800 crowd workers and empirically evaluated seven saliency methods on two datasets with the proposed method. We analyzed the performance of saliency methods, compared our results with existing automated evaluation methods, and identified notable differences between NLP and computer vision (CV) fields when using saliency methods. The instance-level data of our crowdsourced experiments and the code to reproduce the explanations are available at https://github.com/xtlu/lreccoling_evaluation.</abstract>
      <url hash="e126fc99">2024.lrec-main.570</url>
      <bibkey>lu-etal-2024-evaluating-saliency</bibkey>
    </paper>
    <paper id="571">
      <title>Evaluating Self-Supervised Speech Representations for Indigenous <fixed-case>A</fixed-case>merican Languages</title>
      <author><first>Chih-Chen</first><last>Chen</last></author>
      <author><first>William</first><last>Chen</last></author>
      <author><first>Rodolfo Joel</first><last>Zevallos</last></author>
      <author><first>John E.</first><last>Ortega</last></author>
      <pages>6444–6450</pages>
      <abstract>The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In this work, benchmark the efficacy of large SSL models on 6 indigenous America languages: Quechua, Guarani , Bribri, Kotiria, Wa’ikhana, and Totonac on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.</abstract>
      <url hash="95237486">2024.lrec-main.571</url>
      <bibkey>chen-etal-2024-evaluating-self</bibkey>
    </paper>
    <paper id="572">
      <title>Evaluating Shortest Edit Script Methods for Contextual Lemmatization</title>
      <author><first>Olia</first><last>Toporkov</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <pages>6451–6463</pages>
      <abstract>Modern contextual lemmatizers often rely on automatically induced Shortest Edit Scripts (SES), namely, the number of edit operations to transform a word form into its lemma. In fact, different methods of computing SES have been proposed as an integral component in the architecture of several state-of-the-art contextual lemmatizers currently available. However, previous work has not investigated the direct impact of SES in the final lemmatization performance. In this paper we address this issue by focusing on lemmatization as a token classification task where the only input that the model receives is the word-label pairs in context, where the labels correspond to previously induced SES. Thus, by modifying in our lemmatization system only the SES labels that the model needs to learn, we may then objectively conclude which SES representation produces the best lemmatization results. We experiment with seven languages of different morphological complexity, namely, English, Spanish, Basque, Russian, Czech, Turkish and Polish, using multilingual and language-specific pre-trained masked language encoder-only models as a backbone to build our lemmatizers. Comprehensive experimental results, both in- and out-of-domain, indicate that computing the casing and edit operations separately is beneficial overall, but much more clearly for languages with high-inflected morphology. Notably, multilingual pre-trained language models consistently outperform their language-specific counterparts in every evaluation setting.</abstract>
      <url hash="b4606cfb">2024.lrec-main.572</url>
      <bibkey>toporkov-agerri-2024-evaluating-shortest</bibkey>
    </paper>
    <paper id="573">
      <title>Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model</title>
      <author><first>Siyang</first><last>Wang</last></author>
      <author><first>Eva</first><last>Szekely</last></author>
      <pages>6464–6474</pages>
      <abstract>Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model’s strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model’s performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.</abstract>
      <url hash="23885e2e">2024.lrec-main.573</url>
      <bibkey>wang-szekely-2024-evaluating-text</bibkey>
    </paper>
    <paper id="574">
      <title>Evaluating the Efficacy of Large Acoustic Model for Documenting Non-Orthographic Tribal Languages in <fixed-case>I</fixed-case>ndia</title>
      <author><first>Tonmoy</first><last>Rajkhowa</last></author>
      <author><first>Amartya Roy</first><last>Chowdhury</last></author>
      <author><first>Hrishikesh Ravindra</first><last>Karande</last></author>
      <author><first>S. R. Mahadeva</first><last>Prasanna</last></author>
      <pages>6475–6483</pages>
      <abstract>Pre-trained Large Acoustic Models, when fine-tuned, have largely shown to improve the performances in various tasks related to spoken language technologies. However, their evaluation has been mostly on datasets that contain English or other widely spoken languages, and their potential for novel under-resourced languages is not fully known. In this work, four novel under-resourced tribal languages that do not have a standard writing system were introduced and the application of such large pre-trained models was assessed to document such languages using Automatic Speech Recognition and Direct Speech-to-Text Translation systems. The transcriptions for these tribal languages were generated by adapting scripts from those languages that held a prominent presence in the geographical regions where these tribal languages are spoken. The results from this study suggest a viable direction to document these languages in the electronic domain by using Spoken Language Technologies that incorporate LAMs. Additionally, this study helped in understanding the varying performances exhibited by the Large Acoustic Model between these four languages. This study not only informs the adoption of appropriate scripts for transliterating spoken-only languages based on the language family but also aids in making informed decisions in analyzing the behavior of particular Large Acoustic Model in linguistic contexts.</abstract>
      <url hash="f3aa2fb7">2024.lrec-main.574</url>
      <bibkey>rajkhowa-etal-2024-evaluating-efficacy</bibkey>
    </paper>
    <paper id="575">
      <title>Evaluating the <fixed-case>IWSLT</fixed-case>2023 Speech Translation Tasks: Human Annotations, Automatic Metrics, and Segmentation</title>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Dávid</first><last>Javorský</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Peter</first><last>Polák</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>6484–6495</pages>
      <abstract>Human evaluation is a critical component in machine translation system development and has received much attention in text translation research. However, little prior work exists on the topic of human evaluation for speech translation, which adds additional challenges such as noisy data and segmentation mismatches. We take the first steps to fill this gap by conducting a comprehensive human evaluation of the results of several shared tasks from the last International Workshop on Spoken Language Translation (IWSLT 2023). We propose an effective evaluation strategy based on automatic resegmentation and direct assessment with segment context. Our analysis revealed that: 1) the proposed evaluation strategy is robust and scores well-correlated with other types of human judgements; 2) automatic metrics are usually, but not always, well-correlated with direct assessment scores; and 3) COMET as a slightly stronger automatic metric than chrF, despite the segmentation noise introduced by the resegmentation step systems. We release the collected human-annotated data in order to encourage further investigation.</abstract>
      <url hash="71671ceb">2024.lrec-main.575</url>
      <bibkey>sperber-etal-2024-evaluating-iwslt2023</bibkey>
    </paper>
    <paper id="576">
      <title>Evaluating the Potential of Language-family-specific Generative Models for Low-resource Data Augmentation: A <fixed-case>F</fixed-case>aroese Case Study</title>
      <author><first>Barbara</first><last>Scalvini</last></author>
      <author><first>Iben Nyholm</first><last>Debess</last></author>
      <pages>6496–6503</pages>
      <abstract>We investigate GPT-SW3, a generative language model for the Nordic languages, to assess its understanding of the low-resourced Faroese language. Our aim is to demonstrate the advantages of using language-family-specific generative models to augment data for related languages with fewer resources. We evaluate GPT-SW3 by prompting it for Faroese to English translation in a zero, one, and few-shot setting. We assess such translations with an ensemble score consisting of an arithmetic average between the BLEU and a semantic similarity score (SBERT). Moreover, we challenge the model’s Faroese language understanding capabilities on a small dataset of curated Faroese trick sentences. There, we make a qualitative comparison of the model’s performance with respect to Open AI’s GPT-3.5 and GPT-4, demonstrating the advantages of using a language-family-specific generative model for navigating non-trivial scenarios. We evaluate the pipeline thus created and use it, as a proof of concept, to create an automatically annotated Faroese semantic textual similarity (STS) dataset.</abstract>
      <url hash="bb72bb59">2024.lrec-main.576</url>
      <bibkey>scalvini-debess-2024-evaluating-potential</bibkey>
    </paper>
    <paper id="577">
      <title>Evaluating the Quality of a Corpus Annotation Scheme Using Pretrained Language Models</title>
      <author><first>Furkan</first><last>Akkurt</last></author>
      <author><first>Onur</first><last>Gungor</last></author>
      <author><first>Büşra</first><last>Marşan</last></author>
      <author><first>Tunga</first><last>Gungor</last></author>
      <author><first>Balkiz</first><last>Ozturk Basaran</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <author><first>Susan</first><last>Uskudarli</last></author>
      <pages>6504–6514</pages>
      <abstract>Pretrained language models and large language models are increasingly used to assist in a great variety of natural language tasks. In this work, we explore their use in evaluating the quality of alternative corpus annotation schemes. For this purpose, we analyze two alternative annotations of the Turkish BOUN treebank, versions 2.8 and 2.11, in the Universal Dependencies framework using large language models. Using a suitable prompt generated using treebank annotations, large language models are used to recover the surface forms of sentences. Based on the idea that the large language models capture the characteristics of the languages, we expect that the better annotation scheme would yield the sentences with higher success. The experiments conducted on a subset of the treebank show that the new annotation scheme (2.11) results in a successful recovery percentage of about 2 points higher. All the code developed for this work is available at https://github.com/boun-tabi/eval-ud .</abstract>
      <url hash="cef3a707">2024.lrec-main.577</url>
      <bibkey>akkurt-etal-2024-evaluating-quality</bibkey>
    </paper>
    <paper id="578">
      <title>Evaluating Topic Model on Asymmetric and Multi-Domain Financial Corpus</title>
      <author><first>Corentin</first><last>Masson</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>6515–6529</pages>
      <abstract>Multiple recent research works in Finance try to quantify the exposure of market assets to various risks from text and how assets react if the risk materialize itself. We consider risk sections from french Financial Corporate Annual Reports, which are regulated documents with a mandatory section containing important risks the company is facing, to extract an accurate risk profile and exposure of companies. We identify multiple pitfalls of topic models when applied to corporate filing financial domain data for unsupervised risk distribution extraction which has not yet been studied on this domain. We propose two new metrics to evaluate the behavior of different types of topic models with respect to pitfalls previously mentioned about document risk distribution extraction. Our evaluation will focus on three aspects: regularizations, down-sampling and data augmentation. In our experiments, we found that classic Topic Models require down-sampling to obtain unbiased risks, while Topic Models using metadata and in-domain pre-trained word-embeddings partially correct the coherence imbalance per subdomain and remove sector’s specific language from the detected themes. We then demonstrate the relevance and usefulness of the extracted information with visualizations that help to understand the content of such corpus and its evolution along the years.</abstract>
      <url hash="92e1cdbf">2024.lrec-main.578</url>
      <bibkey>masson-paroubek-2024-evaluating-topic</bibkey>
    </paper>
    <paper id="579">
      <title>Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained Sentence Embeddings</title>
      <author><first>Gaifan</first><last>Zhang</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>6530–6543</pages>
      <abstract>Sentence embeddings produced by Pretrained Language Models (PLMs) have received wide attention from the NLP community due to their superior performance when representing texts in numerous downstream applications. However, the high dimensionality of the sentence embeddings produced by PLMs is problematic when representing large numbers of sentences in memory- or compute-constrained devices. As a solution, we evaluate unsupervised dimensionality reduction methods to reduce the dimensionality of sentence embeddings produced by PLMs. Our experimental results show that simple methods such as Principal Component Analysis (PCA) can reduce the dimensionality of sentence embeddings by almost 50%, without incurring a significant loss in performance in multiple downstream tasks. Surprisingly, reducing the dimensionality further <i>improves</i> performance over the original high dimensional versions for the sentence embeddings produced by some PLMs in some tasks.</abstract>
      <url hash="eebf7360">2024.lrec-main.579</url>
      <bibkey>zhang-etal-2024-evaluating-unsupervised</bibkey>
    </paper>
    <paper id="580">
      <title>Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations</title>
      <author><first>Stephanie</first><last>Brandl</last></author>
      <author><first>Oliver</first><last>Eberle</last></author>
      <author><first>Tiago</first><last>Ribeiro</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <pages>6544–6556</pages>
      <abstract>Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguistic insights that could be leveraged to infer task difficulty and further show a comparable ranking of explainability methods to that of human rationales.</abstract>
      <url hash="348a4081">2024.lrec-main.580</url>
      <bibkey>brandl-etal-2024-evaluating-webcam</bibkey>
    </paper>
    <paper id="581">
      <title>Evaluating Word Expansion for Multilingual Sentiment Analysis of Parliamentary Speech</title>
      <author><first>Yana</first><last>Nikolova</last></author>
      <author><first>Costanza</first><last>Navarretta</last></author>
      <pages>6557–6563</pages>
      <abstract>This paper replicates and evaluates the word expansion (WE) method for sentiment lexicon generation from Rheault et al. (2016), applying it to two novel corpora of parliamentary speech from Denmark and Bulgaria. GloVe embeddings and vector similarity are leveraged to expand synonym seed lists with domain-specific terms from the speech corpora. The resulting Danish and Bulgarian lexica are compared to other multilingual lexica by analyzing a gold standard of speech excerpts annotated for sentiment. WE correlates best with hand-coded annotations for Danish, while a machine-translated Lexicoder dictionary does best for Bulgarian. WE performance is also found to be very sensitive to processing and scoring techniques, though this is also an issue with the other lexica. Overall, automatic lexicon translation best balances computational complexity and accuracy across both languages, but robust language-agnosticism remains elusive. Theoretical and practical problems of WE are discussed.</abstract>
      <url hash="8f0a84be">2024.lrec-main.581</url>
      <bibkey>nikolova-navarretta-2024-evaluating-word</bibkey>
    </paper>
    <paper id="582">
      <title>Evaluating Workflows for Creating Orthographic Transcripts for Oral Corpora by Transcribing from Scratch or Correcting <fixed-case>ASR</fixed-case>-Output</title>
      <author><first>Jan</first><last>Gorisch</last></author>
      <author><first>Thomas</first><last>Schmidt</last></author>
      <pages>6564–6574</pages>
      <abstract>Research projects incorporating spoken data require either a selection of existing speech corpora, or they plan to record new data. In both cases, recordings need to be transcribed to make them accessible to analysis. Underestimating the effort of transcribing can be risky. Automatic Speech Recognition (ASR) holds the promise to considerably reduce transcription effort. However, few studies have so far attempted to evaluate this potential. The present paper compares efforts for manual transcription vs. correction of ASR-output. We took recordings from corpora of varying settings (interview, colloquial talk, dialectal, historic) and (i) compared two methods for creating orthographic transcripts: transcribing from scratch vs. correcting automatically created transcripts. And (ii) we evaluated the influence of the corpus characteristics on the correcting efficiency. Results suggest that for the selected data and transcription conventions, transcribing and correcting still take equally long with 7 times real-time on average. The more complex the primary data, the more time has to be spent on corrections. Despite the impressive latest developments in speech technology, to be a real help for conversation analysts or dialectologists, ASR systems seem to require even more improvement, or we need sufficient and appropriate data for training such systems.</abstract>
      <url hash="afc3e16d">2024.lrec-main.582</url>
      <bibkey>gorisch-schmidt-2024-evaluating-workflows</bibkey>
    </paper>
    <paper id="583">
      <title>Evaluation Dataset for Lexical Translation Consistency in <fixed-case>C</fixed-case>hinese-to-<fixed-case>E</fixed-case>nglish Document-level Translation</title>
      <author><first>Xiangyu</first><last>Lei</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>6575–6581</pages>
      <abstract>Lexical translation consistency is one of the most common discourse phenomena in Chinese-to-English document-level translation. To better evaluate the performance of lexical translation consistency, previous researches assumes that all repeated source words should be translated consistently. However, constraining translations of repeated source words to be consistent will hurt word diversity and human translators tend to use different words in translation. Therefore, in this paper we construct a test set of 310 bilingual news articles to properly evaluate lexical translation consistency. We manually differentiate those repeated source words whose translations are consistent into two types: true consistency and false consistency. Then based on the constructed test set, we evaluate the performance of lexical translation consistency for several typical NMT systems.</abstract>
      <url hash="93c20183">2024.lrec-main.583</url>
      <bibkey>lei-etal-2024-evaluation-dataset</bibkey>
    </paper>
    <paper id="584">
      <title>Evaluation of Really Good Grammatical Error Correction</title>
      <author><first>Robert</first><last>Östling</last></author>
      <author><first>Katarina</first><last>Gillholm</last></author>
      <author><first>Murathan</first><last>Kurfalı</last></author>
      <author><first>Marie</first><last>Mattson</last></author>
      <author><first>Mats</first><last>Wirén</last></author>
      <pages>6582–6593</pages>
      <abstract>Traditional evaluation methods for Grammatical Error Correction (GEC) fail to fully capture the full range of system capabilities and objectives. The emergence of large language models (LLMs) has further highlighted the shortcomings of these evaluation strategies, emphasizing the need for a paradigm shift in evaluation methodology. In the current study, we perform a comprehensive evaluation of various GEC systems using a recently published dataset of Swedish learner texts. The evaluation is performed using established evaluation metrics as well as human judges. We find that GPT-3 in a few-shot setting by far outperforms previous grammatical error correction systems for Swedish, a language comprising only about 0.1% of its training data. We also found that current evaluation methods contain undesirable biases that a human evaluation is able to reveal. We suggest using human post-editing of GEC system outputs to analyze the amount of change required to reach native-level human performance on the task, and provide a dataset annotated with human post-edits and assessments of grammaticality, fluency and meaning preservation of GEC system outputs.</abstract>
      <url hash="151a1053">2024.lrec-main.584</url>
      <bibkey>ostling-etal-2024-evaluation-really</bibkey>
    </paper>
    <paper id="585">
      <title>Event-enhanced Retrieval in Real-time Search</title>
      <author><first>Yanan</first><last>Zhang</last></author>
      <author><first>Xiaoling</first><last>Bai</last></author>
      <author><first>Tianhua</first><last>Zhou</last></author>
      <pages>6594–6606</pages>
      <abstract>The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating LLM illusions. However, existing EBR models often face the “semantic drift” problem and insufficient focus on key information, leading to a low adoption rate of retrieval results in subsequent steps. This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event information. To tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR. We incorporate contrastive learning to accompany pairwise learning for encoder optimization. Furthermore, to strengthen the focus on critical event information in events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on prompt-tuning, and correlate the events with query encoder optimization through comparative learning. This decoder module can be removed during inference. Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance. We believe that this approach will provide new perspectives in the field of information retrieval. The codes and dataset are available at https://github.com/open-event-hub/Event-enhanced_Retrieval.</abstract>
      <url hash="ece2d742">2024.lrec-main.585</url>
      <bibkey>zhang-etal-2024-event-enhanced</bibkey>
    </paper>
    <paper id="586">
      <title>Event Extraction in <fixed-case>B</fixed-case>asque: Typologically Motivated Cross-Lingual Transfer-Learning Analysis</title>
      <author><first>Mikel</first><last>Zubillaga</last></author>
      <author><first>Oscar</first><last>Sainz</last></author>
      <author><first>Ainara</first><last>Estarrona</last></author>
      <author><first>Oier</first><last>Lopez de Lacalle</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>6607–6621</pages>
      <abstract>Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.</abstract>
      <url hash="8a0ea731">2024.lrec-main.586</url>
      <bibkey>zubillaga-etal-2024-event-extraction</bibkey>
    </paper>
    <paper id="587">
      <title><fixed-case>E</fixed-case>vent<fixed-case>G</fixed-case>round: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs</title>
      <author><first>Cheng</first><last>Jiayang</last></author>
      <author><first>Lin</first><last>Qiu</last></author>
      <author><first>Chunkit</first><last>Chan</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <pages>6622–6642</pages>
      <abstract>Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial information extraction methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with graph neural network (GNN) or large language model (LLM) based graph reasoning models. Our framework, incorporating grounded knowledge, achieves state-of-the-art performance while providing interpretable evidence.</abstract>
      <url hash="4cc22282">2024.lrec-main.587</url>
      <bibkey>jiayang-etal-2024-eventground-narrative</bibkey>
    </paper>
    <paper id="588">
      <title>Event Representation Learning with Multi-Grained Contrastive Learning and Triple-Mixture of Experts</title>
      <author><first>Tianqi</first><last>Hu</last></author>
      <author><first>Lishuang</first><last>Li</last></author>
      <author><first>Xueyang</first><last>Qin</last></author>
      <author><first>Yubo</first><last>Feng</last></author>
      <pages>6643–6654</pages>
      <abstract>Event representation learning plays a crucial role in numerous natural language processing (NLP) tasks, as it facilitates the extraction of semantic features associated with events. Current methods of learning event representation based on contrastive learning processes positive examples with single-grain random masked language model (MLM), but fall short in learn information inside events from multiple aspects. In this paper, we introduce multi-grained contrastive learning and triple-mixture of experts (MCTM) for event representation learning. Our proposed method extends the random MLM by incorporating a specialized MLM designed to capture different grammatical structures within events, which allows the model to learn token-level knowledge from multiple perspectives. Furthermore, we have observed that mask tokens with different granularities affect the model differently, therefore, we incorporate mixture of experts (MoE) to learn importance weights associated with different granularities. Our experiments demonstrate that MCTM outperforms other baselines in tasks such as hard similarity and transitive sentence similarity, highlighting the superiority of our method.</abstract>
      <url hash="9235c276">2024.lrec-main.588</url>
      <bibkey>hu-etal-2024-event-representation</bibkey>
    </paper>
    <paper id="589">
      <title>Every Verb in Its Right Place? A Roadmap for Operationalizing Developmental Stages in the Acquisition of <fixed-case>L</fixed-case>2 <fixed-case>G</fixed-case>erman</title>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Matthias</first><last>Schwendemann</last></author>
      <author><first>Annette</first><last>Portmann</last></author>
      <author><first>Katrin</first><last>Wisniewski</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>6655–6670</pages>
      <abstract>Developmental stages are a linguistic concept claiming that language learning, despite its large inter-individual variance, generally progresses in an ordered, step-like manner. At the core of research has been the acquisition of verb placement by learners, as conceptualized within Processability Theory (Pienemann, 1989). The computational implementation of a system detecting developmental stages is a prerequisite for an automated analysis of L2 language development. However, such an implementation faces two main challenges. The first is the lack of a fully fleshed out, coherent linguistic specification of the stages. The second concerns the translation of the linguistic specification into computational procedures that can extract clauses from learner-produced text and assign them to a developmental stage based on verb placement. Our contribution provides the necessary linguistic specification of the stages as well as detaiiled discussion and recommendations regarding computational implementation.</abstract>
      <url hash="00ca0aa9">2024.lrec-main.589</url>
      <bibkey>ruppenhofer-etal-2024-every-verb</bibkey>
    </paper>
    <paper id="590">
      <title>Evidence-guided Inference for Neutralized Zero-shot Transfer</title>
      <author><first>Xiaotong</first><last>Feng</last></author>
      <author><first>Meng-Fen</first><last>Chiang</last></author>
      <author><first>Wang-Chien</first><last>Lee</last></author>
      <author><first>Zixin</first><last>Kuang</last></author>
      <pages>6671–6681</pages>
      <abstract>Human annotation is costly and impractical when it comes to scarcely labeled data. Besides, the presence of biased language in well-known benchmarks notably misleads predictive models to perform incredibly well, not because of the model capability but due to the hidden false correlations in the linguistic corpus. Motivated by this, we propose a neutralized Knowledge Transfer framework (NKT) to equip pre-trained language models with neutralized transferability. Specifically, we construct debiased multi-source corpora (CV and EL) for two exemplary knowledge transfer tasks: claim verification and evidence learning, respectively. To counteract biased language, we design a neutralization mechanism in the presence of label skewness. We also design a label adaptation mechanism in light of the mixed label systems in the multi-source corpora. In extensive experiments, the proposed NKT framework shows effective transferability contrarily to the disability of dominant baselines, particularly in the zero-shot cross-domain transfer setting.</abstract>
      <url hash="fcdb44d8">2024.lrec-main.590</url>
      <bibkey>feng-etal-2024-evidence-guided</bibkey>
    </paper>
    <paper id="591">
      <title><fixed-case>EV</fixed-case>il-Probe - a Composite Benchmark for Extensive Visio-Linguistic Probing</title>
      <author><first>Marie</first><last>Bexte</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>6682–6700</pages>
      <abstract>Research probing the language comprehension of visio-linguistic models has gained traction due to their remarkable performance on various tasks. We introduce EViL-Probe, a composite benchmark that processes existing probing datasets into a unified format and reorganizes them based on the linguistic categories they probe. On top of the commonly used negative probes, this benchmark introduces positive probes to more rigorously test the robustness of models. Since the language side alone may introduce a bias models could exploit in solving the probes, we estimate the difficulty of the individual subsets with a language-only baseline. Using the benchmark to probe a set of state-of-the-art visio-linguistic models sheds light on how sensitive they are to the different linguistic categories. Results show that the benchmark is challenging for all models we probe, as their performance is around the chance baseline for many of the categories. The only category all models are able to handle relatively well are nouns. Additionally, models that use a Vision Transformer to process the images are also somewhat robust against probes targeting color and image type. Among these models, our enrichment of EViL-Probe with positive probes helps further discriminate performance, showing BLIP to be the overall best-performing model.</abstract>
      <url hash="80169bfd">2024.lrec-main.591</url>
      <bibkey>bexte-etal-2024-evil-probe</bibkey>
    </paper>
    <paper id="592">
      <title><fixed-case>E</fixed-case>vo<fixed-case>G</fixed-case>rad: A Dynamic Take on the <fixed-case>W</fixed-case>inograd Schema Challenge with Human Adversaries</title>
      <author><first>Jing Han</first><last>Sun</last></author>
      <author><first>Ali</first><last>Emami</last></author>
      <pages>6701–6716</pages>
      <abstract>While Large Language Models (LLMs) excel at the Winograd Schema Challenge (WSC), a coreference resolution task testing common-sense reasoning through pronoun disambiguation, they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a human-in-the-loop approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging ChatGPT’s capabilities, we expand our task instances from 182 to 3691, setting a new benchmark for diverse common-sense reasoning datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92.8% accuracy without perturbation errors. This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.</abstract>
      <url hash="dab82d68">2024.lrec-main.592</url>
      <bibkey>sun-emami-2024-evograd-dynamic</bibkey>
    </paper>
    <paper id="593">
      <title>Evolving Knowledge Distillation with Large Language Models and Active Learning</title>
      <author><first>Chengyuan</first><last>Liu</last></author>
      <author><first>Fubang</first><last>Zhao</last></author>
      <author><first>Kun</first><last>Kuang</last></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Zhuoren</first><last>Jiang</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <pages>6717–6731</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model’s weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide iterative feedback to the LLMs regarding the student model’s performance to continuously construct diversified and challenging samples. Experiments and analysis on different NLP tasks, namely, text classification and named entity recognition show the effectiveness of EvoKD.</abstract>
      <url hash="a90bc2a7">2024.lrec-main.593</url>
      <bibkey>liu-etal-2024-evolving-knowledge</bibkey>
    </paper>
    <paper id="594">
      <title>Examining Temporalities on Stance Detection towards <fixed-case>COVID</fixed-case>-19 Vaccination</title>
      <author><first>Yida</first><last>Mu</last></author>
      <author><first>Mali</first><last>Jin</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <pages>6732–6738</pages>
      <abstract>Previous studies have highlighted the importance of vaccination as an effective strategy to control the transmission of the COVID-19 virus. It is crucial for policymakers to have a comprehensive understanding of the public’s stance towards vaccination on a large scale. However, attitudes towards COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved over time on social media. Thus, it is necessary to account for possible temporal shifts when analysing these stances. This study aims to examine the impact of temporal concept drift on stance detection towards COVID-19 vaccination on Twitter. To this end, we evaluate a range of transformer-based models using chronological (splitting the training, validation, and test sets in order of time) and random splits (randomly splitting these three sets) of social media data. Our findings reveal significant discrepancies in model performance between random and chronological splits in several existing COVID-19-related datasets; specifically, chronological splits significantly reduce the accuracy of stance classification. Therefore, real-world stance detection approaches need to be further refined to incorporate temporal factors as a key consideration.</abstract>
      <url hash="43eac67b">2024.lrec-main.594</url>
      <bibkey>mu-etal-2024-examining-temporalities</bibkey>
    </paper>
    <paper id="595">
      <title>Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets</title>
      <author><first>Yida</first><last>Mu</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>6739–6751</pages>
      <abstract>A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source post as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors’ source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept drift in static datasets during the training of rumor detection methods.</abstract>
      <url hash="e1a81c4d">2024.lrec-main.595</url>
      <bibkey>mu-etal-2024-examining-limitations-computational</bibkey>
    </paper>
    <paper id="596">
      <title>Executing Natural Language-Described Algorithms with Large Language Models: An Investigation</title>
      <author><first>Xin</first><last>Zheng</last></author>
      <author><first>Qiming</first><last>Zhu</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>6752–6837</pages>
      <abstract>Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs’ code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our findings contribute to evaluating LLMs’ code execution abilities and would encourage further investigation and application for the computation power of LLMs.</abstract>
      <url hash="88f730ce">2024.lrec-main.596</url>
      <bibkey>zheng-etal-2024-executing-natural</bibkey>
    </paper>
    <paper id="597">
      <title>Experimental versus In-Corpus Variation in Referring Expression Choice</title>
      <author><first>T. Mark</first><last>Ellison</last></author>
      <author><first>Fahime</first><last>Same</last></author>
      <pages>6838–6848</pages>
      <abstract>In this paper, we compare the results of three studies. The first explored feature-conditioned distributions of referring expression (RE) forms in the original corpus from which the contexts were taken. The second is a crowdsourcing study in which we asked participants to express entities within a pre-existing context, given fully specified referents. The third study replicates the crowdsourcing experiment using Large Language Models (LLMs). We evaluate how well the corpus itself can model the variation found when multiple informants (either human participants or LLMs) choose REs in the same contexts. We measure the similarity of the conditional distributions of form categories using the Jensen-Shannon Divergence metric and Description Length metric. We find that the experimental methodology introduces substantial noise, but by taking this noise into account, we can model the variation captured from the corpus and RE form choices made during experiments. Furthermore, we compared the three conditional distributions over the corpus, the human experimental results, and the GPT models. Against our expectations, the divergence is greatest between the corpus and the GPT model.</abstract>
      <url hash="5e513f4a">2024.lrec-main.597</url>
      <bibkey>ellison-same-2024-experimental-versus</bibkey>
    </paper>
    <paper id="598">
      <title>Experiments on Speech Synthesis for Teochew, Can <fixed-case>T</fixed-case>aiwanese Help ?</title>
      <author><first>Pierre</first><last>Magistry</last></author>
      <author><first>Ilaine</first><last>Wang</last></author>
      <author><first>Ty Eng</first><last>Lim</last></author>
      <pages>6849–6854</pages>
      <abstract>This paper reports on our preliminary experiments in speech processing for Teochew, an under-resourced Sinitic language spoken both in China and around the world in diasporan communities. Following the recent uptick of interest in Teochew from heritage speakers of the diaspora and in order to respond to the needs of this community, we develop a Teochew Text-to-Speech system. We describe experiments to build this system and to assess the possible contribution of available resources in Taiwanese Hokkien, the closest language with a significant body of resources. The results of these experiments are not as conclusive as we expected: the Taiwanese dataset did not help our model significantly, but considering our objectives, we find it encouraging that they show that a large training dataset was not necessary for this precise task. A promising model could still be obtained with only a small dataset of Teochew. We hope that this work inspires other communities of speakers of languages in a revitalization phase.</abstract>
      <url hash="38e22348">2024.lrec-main.598</url>
      <bibkey>magistry-etal-2024-experiments-speech</bibkey>
    </paper>
    <paper id="599">
      <title>Explainable Multi-hop Question Generation: An End-to-End Approach without Intermediate Question Labeling</title>
      <author><first>Seonjeong</first><last>Hwang</last></author>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Gary Geunbae</first><last>Lee</last></author>
      <pages>6855–6866</pages>
      <abstract>In response to the increasing use of interactive artificial intelligence, the demand for the capacity to handle complex questions has increased. Multi-hop question generation aims to generate complex questions that requires multi-step reasoning over several documents. Previous studies have predominantly utilized end-to-end models, wherein questions are decoded based on the representation of context documents. However, these approaches lack the ability to explain the reasoning process behind the generated multi-hop questions. Additionally, the question rewriting approach, which incrementally increases the question complexity, also has limitations due to the requirement of labeling data for intermediate-stage questions. In this paper, we introduce an end-to-end question rewriting model that increases question complexity through sequential rewriting. The proposed model has the advantage of training with only the final multi-hop questions, without intermediate questions. Experimental results demonstrate the effectiveness of our model in generating complex questions, particularly 3- and 4-hop questions, which are appropriately paired with input answers. We also prove that our model logically and incrementally increases the complexity of questions, and the generated multi-hop questions are also beneficial for training question answering models.</abstract>
      <url hash="6c0a899f">2024.lrec-main.599</url>
      <bibkey>hwang-etal-2024-explainable-multi</bibkey>
    </paper>
    <paper id="600">
      <title>Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings</title>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Hendrik</first><last>Schuff</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>6867–6875</pages>
      <abstract>Attribution scores indicate the importance of different input parts and can, thus, explain model behaviour. Currently, prompt-based models are gaining popularity, i.a., due to their easier adaptability in low-resource settings. However, the quality of attribution scores extracted from prompt-based models has not been investigated yet. In this work, we address this topic by analyzing attribution scores extracted from prompt-based models w.r.t. plausibility and faithfulness and comparing them with attribution scores extracted from fine-tuned models and large language models. In contrast to previous work, we introduce training size as another dimension into the analysis. We find that using the prompting paradigm (with either encoder-based or decoder-based models) yields more plausible explanations than fine-tuning the models in low-resource settings and Shapley Value Sampling consistently outperforms attention and Integrated Gradients in terms of leading to more plausible and faithful explanations.</abstract>
      <url hash="bcb6aa6a">2024.lrec-main.600</url>
      <bibkey>zhou-etal-2024-explaining-pre</bibkey>
    </paper>
    <paper id="601">
      <title>Explicit over Implict: Explicit Diversity Conditions for Effective Question Answer Generation</title>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Hyuk joon</first><last>Kwon</last></author>
      <author><first>Vijay</first><last>Srinivasan</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>6876–6882</pages>
      <abstract>Question Answer Generation (QAG) is an effective data augmentation technique to improve the accuracy of question answering systems, especially in low-resource domains. While recent pretrained and large language model-based QAG methods have made substantial progress, they face the critical issue of redundant QA pair generation, affecting downstream QA systems. Implicit diversity techniques such as sampling and diverse beam search are proven effective solutions but often yield smaller diversity. We present explicit diversity conditions for QAG, focusing on spatial aspects, question types, and entities, substantially increasing diversity in QA generation. Our work emphasizes the need of explicit diversity conditions for generating diverse question-answer synthetic data by showing significant improvements in downstream QA task over existing implicit diversity techniques. In particular, generated QA pairs from explicit diversity conditions result in an average 4.1% exact match and 4.5% F1 improvement over implicit sampling techniques on SQuAD-DU. Our work emphasizes the need for explicit diversity conditions even more in low-resource datasets (SubjQA), where average QA performance improvements are ~12% EM.</abstract>
      <url hash="5a483730">2024.lrec-main.601</url>
      <bibkey>yadav-etal-2024-explicit-implict</bibkey>
    </paper>
    <paper id="602">
      <title>Exploring and Mitigating Shortcut Learning for Generative Large Language Models</title>
      <author><first>Zechen</first><last>Sun</last></author>
      <author><first>Yisheng</first><last>Xiao</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Yixin</first><last>Ji</last></author>
      <author><first>Wenliang</first><last>Chen</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>6883–6893</pages>
      <abstract>Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new scaling law, in which the continuous improvement of model capacity yields emergent capabilities, e.g., reasoning and universal generalization. However, we point out that recent LLMs still show shortcut learning behavior, where the models tend to exploit spurious correlations between non-robust features and labels for prediction, which might lead to overestimating model capabilities. LLMs memorize more complex spurious correlations (i.e., task <tex-math>\leftrightarrow</tex-math> feature <tex-math>\leftrightarrow</tex-math> label) compared with that learned from previous pre-training and task-specific fine-tuning paradigm (i.e., feature <tex-math>\leftrightarrow</tex-math> label). Based on our findings, we propose FSLI, a framework for encouraging LLMs to <b>F</b>orget <b>S</b>purious correlations and <b>L</b>earn from <b>I</b>n-context information. Experiments on three tasks show that FSFI can effectively mitigate shortcut learning. Besides, we argue not to overestimate the capabilities of LLMs and conduct evaluations in more challenging and complete test scenarios.</abstract>
      <url hash="c2539a05">2024.lrec-main.602</url>
      <bibkey>sun-etal-2024-exploring-mitigating</bibkey>
    </paper>
    <paper id="603">
      <title>Exploring <fixed-case>BERT</fixed-case>-Based Classification Models for Detecting Phobia Subtypes: A Novel Tweet Dataset and Comparative Analysis</title>
      <author><first>Anik</first><last>Das</last></author>
      <author><first>Milton</first><last>King</last></author>
      <author><first>James Alexander</first><last>Hughes</last></author>
      <pages>6894–6908</pages>
      <abstract>Phobias, characterized by irrational fears of specific objects or situations, can profoundly affect an individual’s quality of life. This research presents a comprehensive investigation into phobia classification, where we propose a novel dataset of 811,569 English tweets from user timelines spanning 102 phobia subtypes over six months, including 47,614 self-diagnosed phobia users. BERT models were leveraged to differentiate non-phobia from phobia users and classify them into 65 specific phobia subtypes. The study produced promising results, with the highest f1-score of 78.44% in binary classification (phobic user or not phobic user) and 24.01% in a multi-class classification (detecting the specific phobia subtype of a user). This research provides insights into people with phobias on social media and emphasizes the capacity of natural language processing and machine learning to automate the evaluation and support of mental health.</abstract>
      <url hash="66c33891">2024.lrec-main.603</url>
      <bibkey>das-etal-2024-exploring-bert</bibkey>
    </paper>
    <paper id="604">
      <title>Exploring Geometric Representational Disparities between Multilingual and Bilingual Translation Models</title>
      <author><first>Neha</first><last>Verma</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>6909–6921</pages>
      <abstract>Multilingual machine translation has proven immensely useful for both parameter efficiency and overall performance across many language pairs via complete multilingual parameter sharing. However, some language pairs in multilingual models can see worse performance than in bilingual models, especially in the one-to-many translation setting. Motivated by their empirical differences, we examine the geometric differences in representations from bilingual models versus those from one-to-many multilingual models. Specifically, we compute the isotropy of these representations using intrinsic dimensionality and IsoScore, in order to measure how the representations utilize the dimensions in their underlying vector space. Using the same evaluation data in both models, we find that for a given language pair, its multilingual model decoder representations are consistently less isotropic and occupy fewer dimensions than comparable bilingual model decoder representations. Additionally, we show that much of the anisotropy in multilingual decoder representations can be attributed to modeling language-specific information, therefore limiting remaining representational capacity.</abstract>
      <url hash="8fd233d0">2024.lrec-main.604</url>
      <bibkey>verma-etal-2024-exploring-geometric</bibkey>
    </paper>
    <paper id="605">
      <title>Exploring Interpretability of Independent Components of Word Embeddings with Automated Word Intruder Test</title>
      <author><first>Tomáš</first><last>Musil</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>6922–6928</pages>
      <abstract>Independent Component Analysis (ICA) is an algorithm originally developed for finding separate sources in a mixed signal, such as a recording of multiple people in the same room speaking at the same time. Unlike Principal Component Analysis (PCA), ICA permits the representation of a word as an unstructured set of features, without any particular feature being deemed more significant than the others. In this paper, we used ICA to analyze word embeddings. We have found that ICA can be used to find semantic features of the words and these features can easily be combined to search for words that satisfy the combination. We show that most of the independent components represent such features. To quantify the interpretability of the components, we use the word intruder test, performed both by humans and by large language models. We propose to use the automated version of the word intruder test as a fast and inexpensive way of quantifying vector interpretability without the need for human effort.</abstract>
      <url hash="0759a210">2024.lrec-main.605</url>
      <bibkey>musil-marecek-2024-exploring-interpretability</bibkey>
    </paper>
    <paper id="606">
      <title>Exploring Neural Topic Modeling on a Classical <fixed-case>L</fixed-case>atin Corpus</title>
      <author><first>Ginevra</first><last>Martinelli</last></author>
      <author><first>Paola</first><last>Impicciché</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <author><first>Francesco</first><last>Mambrini</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <pages>6929–6934</pages>
      <abstract>The large availability of processable textual resources for Classical Latin has made it possible to study Latin literature through methods and tools that support distant reading. This paper describes a number of experiments carried out to test the possibility of investigating the thematic distribution of the Classical Latin corpus Opera Latina by means of topic modeling. For this purpose, we train, optimize and compare two neural models, Product-of-Experts LDA (ProdLDA) and Embedded Topic Model (ETM), opportunely revised to deal with the textual data from a Classical Latin corpus, to evaluate which one performs better both on the basis of topic diversity and topic coherence metrics, and from a human judgment point of view. Our results show that the topics extracted by neural models are coherent and interpretable and that they are significant from the perspective of a Latin scholar. The source code of the proposed model is available at https://github.com/MIND-Lab/LatinProdLDA.</abstract>
      <url hash="84306e2a">2024.lrec-main.606</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5b4d0024">2024.lrec-main.606.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>martinelli-etal-2024-exploring-neural</bibkey>
    </paper>
    <paper id="607">
      <title>Exploring Pathological Speech Quality Assessment with <fixed-case>ASR</fixed-case>-Powered <fixed-case>W</fixed-case>av2<fixed-case>V</fixed-case>ec2 in Data-Scarce Context</title>
      <author><first>Tuan</first><last>Nguyen</last></author>
      <author><first>Corinne</first><last>Fredouille</last></author>
      <author><first>Alain</first><last>Ghio</last></author>
      <author><first>Mathieu</first><last>Balaguer</last></author>
      <author><first>Virginie</first><last>Woisard</last></author>
      <pages>6935–6944</pages>
      <abstract>Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients’ audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average MSE = 0.73 and MSE = 1.15 for the prediction of intelligibility and severity scores respectively, using only 95 training samples. It shows that the ASR based Wav2Vec2 model brings the best results and may indicate a strong correlation between ASR and speech quality assessment. We also measure its ability on variable segment durations and speech content, exploring factors influencing its decision.</abstract>
      <url hash="95fba2fd">2024.lrec-main.607</url>
      <bibkey>nguyen-etal-2024-exploring-pathological</bibkey>
    </paper>
    <paper id="608">
      <title>Exploring the Emotional Dimension of <fixed-case>F</fixed-case>rench Online Toxic Content</title>
      <author><first>Valentina</first><last>Dragos</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Fatou</first><last>Sow</last></author>
      <author><first>Aline</first><last>Etienne</last></author>
      <pages>6945–6954</pages>
      <abstract>One of the biggest hurdles for the effective analysis of data collected on social platforms is the need for deeper insights on the content and meaning of this data. Emotion annotation can bring new perspectives on this issue and can enable the identification of content–specific features. This study aims at investigating the ways in which variation in online content can be explored through emotion annotation and corpus-based analysis. The paper describes the emotion annotation of three data sets in French composed of extremist, sexist and hateful messages respectively. To this end, first a fine-grained, corpus annotation scheme was used to annotate the data sets and then several empirical studies were carried out to characterize the content in the light of emotional categories. Results suggest that emotion annotations can provide new insights for online content analysis and stronger empirical background for automatic content detection.</abstract>
      <url hash="2dad71cb">2024.lrec-main.608</url>
      <bibkey>dragos-etal-2024-exploring-emotional</bibkey>
    </paper>
    <paper id="609">
      <title>Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers across Diseases</title>
      <author><first>Yumeng</first><last>Yang</last></author>
      <pages>6955–6965</pages>
      <abstract>Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disproportionately prevalent in cancer trials, like prior malignancy. We also experiment with few-shot learning, demonstrating that a limited number of disease-specific examples can partially overcome this performance gap. We are releasing this new dataset of annotated eligibility statements to promote the development of cross-disease generalization in clinical trial classification.</abstract>
      <url hash="ae3ed72c">2024.lrec-main.609</url>
      <bibkey>yang-2024-exploring-generalization</bibkey>
    </paper>
    <paper id="610">
      <title>Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation</title>
      <author><first>Sarah E.</first><last>Finch</last></author>
      <author><first>James D.</first><last>Finch</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>6966–6973</pages>
      <abstract>Human evaluation has been widely accepted as the standard for evaluating chat-oriented dialogue systems. However, there is a significant variation in previous work regarding who gets recruited as evaluators. Evaluator groups such as domain experts, university students, and crowdworkers have been used to assess and compare dialogue systems, although it is unclear to what extent the choice of an evaluator group can affect results. This paper analyzes the evaluator group impact on dialogue system evaluation by testing 4 state-of-the-art dialogue systems using 4 distinct evaluator groups. Our analysis reveals a robustness towards evaluator groups for Likert evaluations that is not seen for Pairwise, with only minor differences observed when changing evaluator groups. Furthermore, two notable limitations to this robustness are observed, which reveal discrepancies between evaluators with different levels of chatbot expertise and indicate that evaluator objectivity is beneficial for certain dialogue metrics.</abstract>
      <url hash="d98ece94">2024.lrec-main.610</url>
      <bibkey>finch-etal-2024-exploring-impact</bibkey>
    </paper>
    <paper id="611">
      <title>Exploring the Potential of Large Language Models (<fixed-case>LLM</fixed-case>s) for Low-resource Languages: A Study on Named-Entity Recognition (<fixed-case>NER</fixed-case>) and Part-Of-Speech (<fixed-case>POS</fixed-case>) Tagging for <fixed-case>N</fixed-case>epali Language</title>
      <author><first>Bipesh</first><last>Subedi</last></author>
      <author><first>Sunil</first><last>Regmi</last></author>
      <author><first>Bal Krishna</first><last>Bal</last></author>
      <author><first>Praveen</first><last>Acharya</last></author>
      <pages>6974–6979</pages>
      <abstract>Large Language Models (LLMs) have made significant advancements in Natural Language Processing (NLP) by excelling in various NLP tasks. This study specifically focuses on evaluating the performance of LLMs for Named Entity Recognition (NER) and Part-of-Speech (POS) tagging for a low-resource language, Nepali. The aim is to study the effectiveness of these models for languages with limited resources by conducting experiments involving various parameters and fine-tuning and evaluating two datasets namely, ILPRL and EBIQUITY. In this work, we have experimented with eight LLMs for Nepali NER and POS tagging. While some prior works utilized larger datasets than ours, our contribution lies in presenting a comprehensive analysis of multiple LLMs in a unified setting. The findings indicate that NepBERTa, trained solely in the Nepali language, demonstrated the highest performance with F1-scores of 0.76 and 0.90 in ILPRL dataset. Similarly, it achieved 0.79 and 0.97 in EBIQUITY dataset for NER and POS respectively. This study not only highlights the potential of LLMs in performing classification tasks for low-resource languages but also compares their performance with that of alternative approaches deployed for the tasks.</abstract>
      <url hash="e665257a">2024.lrec-main.611</url>
      <bibkey>subedi-etal-2024-exploring-potential</bibkey>
    </paper>
    <paper id="612">
      <title>Exploring the Synergy of Dual-path Encoder and Alignment Module for Better Graph-to-Text Generation</title>
      <author><first>Tianxin</first><last>Zhao</last></author>
      <author><first>Yingxin</first><last>Liu</last></author>
      <author><first>Xiangdong</first><last>Su</last></author>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Guanglai</first><last>Gao</last></author>
      <pages>6980–6991</pages>
      <abstract>The mainstream approaches view the knowledge graph-to-text (KG-to-text) generation as a sequence-to-sequence task and fine-tune the pre-trained model (PLM) to generate the target text from the linearized knowledge graph. However, the linearization of knowledge graphs and the structure of PLMs lead to the loss of a large amount of graph structure information. Moreover, PLMs lack an explicit graph-text alignment strategy because of the discrepancy between structural and textual information. To solve these two problems, we propose a synergetic KG-to-text model with a dual-path encoder, an alignment module, and a guidance module. The dual-path encoder consists of a graph structure encoder and a text encoder, which can better encode the structure and text information of the knowledge graph. The alignment module contains a two-layer Transformer block and an MLP block, which aligns and integrates the information from the dual encoder. The guidance module combines an improved pointer network and an MLP block to avoid error-generated entities and ensures the fluency and accuracy of the generated text. Our approach obtains very competitive performance on three benchmark datasets. Our code is available from https://github.com/IMu-MachineLearningsxD/G2T.</abstract>
      <url hash="fcd10701">2024.lrec-main.612</url>
      <bibkey>zhao-etal-2024-exploring-synergy</bibkey>
    </paper>
    <paper id="613">
      <title>Exploring the Usability of Persuasion Techniques for Downstream Misinformation-related Classification Tasks</title>
      <author><first>Nikolaos</first><last>Nikolaidis</last></author>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Nicolas</first><last>Stefanovitch</last></author>
      <pages>6992–7006</pages>
      <abstract>We systematically explore the predictive power of features derived from Persuasion Techniques detected in texts, for solving different tasks of interest for media analysis; notably: detecting mis/disinformation, fake news, propaganda, partisan news and conspiracy theories. Firstly, we propose a set of meaningful features, aiming to capture the persuasiveness of a text. Secondly, we assess the discriminatory power of these features in different text classification tasks on 8 selected datasets from the literature using two metrics. We also evaluate the per-task discriminatory power of each Persuasion Technique and report on different insights. We find out that most of these features have a noticeable potential to distinguish conspiracy theories, hyperpartisan news and propaganda, while we observed mixed results in the context of fake news detection.</abstract>
      <url hash="b5733aca">2024.lrec-main.613</url>
      <bibkey>nikolaidis-etal-2024-exploring-usability</bibkey>
    </paper>
    <paper id="614">
      <title>Extending <fixed-case>AZ</fixed-case>ee with Non-manual Gesture Rules for <fixed-case>F</fixed-case>rench <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Camille</first><last>Challant</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>7007–7016</pages>
      <abstract>This paper presents a study on non-manual gestures, using a formal model named AZee. This is an approach which allows to formally represent Sign Language (SL) discourses, but also to animate them with a virtual signer. As non-manual gestures are essential in SL and therefore necessary for a quality synthesis, we wanted to extend AZee with them, by adding some production rules to the AZee production set. For this purpose, we applied a methodology which allows to find new production rules on a corpus representing one hour of French Sign Language, the 40 brèves (Challant and Filhol, 2022). 23 production rules for non-manual gestures in LSF have thus been determined. We took advantage of this study to directly insert these new rules in the first corpus of AZee discourses expressions, which describe with AZee the productions in SL of the 40 brèves corpus. 533 non-manual rules were inserted in the corpus, and some updates were made. This article proposes a new version of this AZee expressions corpus.</abstract>
      <url hash="aac71e39">2024.lrec-main.614</url>
      <bibkey>challant-filhol-2024-extending-azee</bibkey>
    </paper>
    <paper id="615">
      <title>Extending the Discourse Analysis Tool Suite with Whiteboards for Visual Qualitative Analysis</title>
      <author><first>Tim</first><last>Fischer</last></author>
      <author><first>Florian</first><last>Schneider</last></author>
      <author><first>Fynn</first><last>Petersen-Frey</last></author>
      <author><first>Anja Silvia Mollah</first><last>Haque</last></author>
      <author><first>Isabel</first><last>Eiser</last></author>
      <author><first>Gertraud</first><last>Koch</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>7017–7022</pages>
      <abstract>In this system demonstration paper, we describe the Whiteboards extension for an existing web-based platform for digital qualitative discourse analysis. Whiteboards comprise interactive graph-based interfaces to organize and manipulate objects, which can be qualitative research data, such as documents, images, etc., and analyses of these research data, such as annotations, tags, and code structures. The proposed extension offers a customizable view of the material and a wide range of actions that enable new ways of interacting and working with such resources. We show that the visualizations facilitate various use cases of qualitative data analysis, including reflection of the research process through sampling maps, creation of actor networks, and refining code taxonomies.</abstract>
      <url hash="cc1ab904">2024.lrec-main.615</url>
      <bibkey>fischer-etal-2024-extending-discourse</bibkey>
    </paper>
    <paper id="616">
      <title>Extracting Biomedical Entities from Noisy Audio Transcripts</title>
      <author><first>Nima</first><last>Ebadi</last></author>
      <author><first>Kellen</first><last>Morgan</last></author>
      <author><first>Adrian</first><last>Tan</last></author>
      <author><first>Billy</first><last>Linares</last></author>
      <author><first>Sheri</first><last>Osborn</last></author>
      <author><first>Emma</first><last>Majors</last></author>
      <author><first>Jeremy</first><last>Davis</last></author>
      <author><first>Anthony</first><last>Rios</last></author>
      <pages>7023–7034</pages>
      <abstract>Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems. Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied. Named Entity Recognition (NER), an essential clinical task, is particularly affected by such noise, often termed the ASR-NLP gap. Prior works have primarily studied ASR’s efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments. This paper introduces a novel dataset, BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a comprehensive collection of almost 2,000 clean and noisy recordings. In addressing the noise challenge, we present an innovative transcript-cleaning method using GPT-4, investigating both zero-shot and few-shot methodologies. Our study further delves into an error analysis, shedding light on the types of errors in transcription software, corrections by GPT-4, and the challenges GPT-4 faces. This paper aims to foster improved understanding and potential solutions for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation practices.</abstract>
      <url hash="df2e4890">2024.lrec-main.616</url>
      <bibkey>ebadi-etal-2024-extracting-biomedical</bibkey>
    </paper>
    <paper id="617">
      <title>Extracting Financial Events from Raw Texts via Matrix Chunking</title>
      <author><first>Yusheng</first><last>Huang</last></author>
      <author><first>Ning</first><last>Hu</last></author>
      <author><first>Kunping</first><last>Li</last></author>
      <author><first>Nan</first><last>Wang</last></author>
      <author><first>Zhouhan</first><last>Lin</last></author>
      <pages>7035–7044</pages>
      <abstract>Event Extraction (EE) is widely used in the Chinese financial field to provide valuable structured information. However, there are two key challenges for Chinese financial EE in application scenarios. First, events need to be extracted from raw texts, which sets it apart from previous works like the Automatic Content Extraction (ACE) EE task, where EE is treated as a classification problem given the entity spans. Second, recognizing financial entities can be laborious, as they may involve multiple elements. In this paper, we introduce CFTE, a novel task for Chinese Financial Text-to-Event extraction, which directly extracts financial events from raw texts. We further present FINEED, a Chinese FINancial Event Extraction Dataset, and an efficient MAtrix-ChunKing method called MACK, designed for the extraction of financial events from raw texts. Specifically, FINEED is manually annotated with rich linguistic features. We propose a novel two-dimensional annotation method for FINEED, which can visualize the interactions among text components. Our MACK method is fault-tolerant by preserving the tag frequency distribution when identifying financial entities. We conduct extensive experiments and the results verify the effectiveness of our MACK method.</abstract>
      <url hash="2e3ecae8">2024.lrec-main.617</url>
      <bibkey>huang-etal-2024-extracting-financial</bibkey>
    </paper>
    <paper id="618">
      <title>Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods</title>
      <author><first>Yujuan</first><last>Fu</last></author>
      <author><first>Giridhar Kaushik</first><last>Ramachandran</last></author>
      <author><first>Nicholas J.</first><last>Dobbins</last></author>
      <author><first>Namu</first><last>Park</last></author>
      <author><first>Michael</first><last>Leu</last></author>
      <author><first>Abby R.</first><last>Rosenberg</last></author>
      <author><first>Kevin</first><last>Lybarger</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Özlem</first><last>Uzuner</last></author>
      <author><first>Meliha</first><last>Yetisgen</last></author>
      <pages>7045–7056</pages>
      <abstract>Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance at 78.4 F1 for event arguments. In-context learning approaches with GPT-4 demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers.</abstract>
      <url hash="c0b49a2e">2024.lrec-main.618</url>
      <bibkey>fu-etal-2024-extracting-social</bibkey>
    </paper>
    <paper id="619">
      <title>Eye-Tracking Features Masking Transformer Attention in Question-Answering Tasks</title>
      <author><first>Leran</first><last>Zhang</last></author>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <pages>7057–7070</pages>
      <abstract>Eye movement features are considered to be direct signals reflecting human attention distribution with a low cost to obtain, inspiring researchers to augment language models with eye-tracking (ET) data. In this study, we select first fixation duration (FFD) and total reading time (TRT) as the cognitive signals to guide Transformer attention in question-answering (QA) tasks. We design three different ET attention masks based on the two features, either collected from human reading events or generated by a gaze-predicting model. We augment BERT and ALBERT models with attention masks structured based on the ET data. We find that augmenting a model with ET data carries linguistic features complementing the information captured by the model. It improves the models’ performance but compromises the stability. Different Transformer models benefit from different types of ET attention masks, while ALBERT performs better than BERT. Moreover, ET data collected from real-life reading events has better model augmenting ability than the model-predicted data.</abstract>
      <url hash="b873b20c">2024.lrec-main.619</url>
      <bibkey>zhang-hollenstein-2024-eye-tracking</bibkey>
    </paper>
    <paper id="620">
      <title>Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation</title>
      <author><first>Zhigang</first><last>Chen</last></author>
      <author><first>Benjia</first><last>Zhou</last></author>
      <author><first>Jun</first><last>Li</last></author>
      <author><first>Jun</first><last>Wan</last></author>
      <author><first>Zhen</first><last>Lei</last></author>
      <author><first>Ning</first><last>Jiang</last></author>
      <author><first>Quan</first><last>Lu</last></author>
      <author><first>Guoqing</first><last>Zhao</last></author>
      <pages>7071–7081</pages>
      <abstract>Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained LLM to inspire the LLM’s translation potential. This factorized training strategy proves to be highly effective as evidenced by significant improvements achieved across three SLT datasets which are all conducted under the gloss-free setting.</abstract>
      <url hash="d98a372b">2024.lrec-main.620</url>
      <bibkey>chen-etal-2024-factorized-learning</bibkey>
    </paper>
    <paper id="621">
      <title><fixed-case>F</fixed-case>a<fixed-case>GAN</fixed-case>et: An Evidence-Based Fact-Checking Model with Integrated Encoder Leveraging Contextual Information</title>
      <author><first>Weiyao</first><last>Luo</last></author>
      <author><first>Junfeng</first><last>Ran</last></author>
      <author><first>Zailong</first><last>Tian</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>7082–7088</pages>
      <abstract>In the face of the rapidly growing spread of false and misleading information in the real world, manual evidence-based fact-checking efforts become increasingly challenging and time-consuming. In order to tackle this issue, we propose FaGANet, an automated and accurate fact-checking model that leverages the power of sentence-level attention and graph attention network to enhance performance. This model adeptly integrates encoder-only models with graph attention network, effectively fusing claims and evidence information for accurate identification of even well-disguised data. Experiment results showcase the significant improvement in accuracy achieved by our FaGANet model, as well as its state-of-the-art performance in the evidence-based fact-checking task. We release our code and data in https://github.com/WeiyaoLuo/FaGANet.</abstract>
      <url hash="994f5c1f">2024.lrec-main.621</url>
      <bibkey>luo-etal-2024-faganet-evidence</bibkey>
    </paper>
    <paper id="622">
      <title><fixed-case>F</fixed-case>ai<fixed-case>MA</fixed-case>: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis</title>
      <author><first>Songhua</first><last>Yang</last></author>
      <author><first>Xinke</first><last>Jiang</last></author>
      <author><first>Hanjie</first><last>Zhao</last></author>
      <author><first>Wenxuan</first><last>Zeng</last></author>
      <author><first>Hongde</first><last>Liu</last></author>
      <author><first>Yuxiang</first><last>Jia</last></author>
      <pages>7089–7100</pages>
      <abstract>Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture fine-grained sentiment across diverse domains. While existing research narrowly focuses on single-domain applications constrained by methodological limitations and data scarcity, the reality is that sentiment naturally traverses multiple domains. Although large language models (LLMs) offer a promising solution for ABSA, it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy. To alleviate this problem, we propose a novel framework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The core insight of FaiMA is to utilize in-context learning (ICL) as a feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA tasks. Specifically, we employ a multi-head graph attention network as a text encoder optimized by heuristic rules for linguistic, domain, and sentiment features. Through contrastive learning, we optimize sentence representations by focusing on these diverse features. Additionally, we construct an efficient indexing mechanism, allowing FaiMA to stably retrieve highly relevant examples across multiple dimensions for any given input. To evaluate the efficacy of FaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive experimental results demonstrate that FaiMA achieves significant performance improvements in multiple domains compared to baselines, increasing F1 by 2.07% on average. Source code and data sets are available at https://github.com/SupritYoung/FaiMA.</abstract>
      <url hash="7409dd95">2024.lrec-main.622</url>
      <attachment type="OptionalSupplementaryMaterial" hash="378cb5eb">2024.lrec-main.622.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>yang-etal-2024-faima-feature</bibkey>
    </paper>
    <paper id="623">
      <title><fixed-case>FAIR</fixed-case>ification of <fixed-case>L</fixed-case>ei<fixed-case>L</fixed-case>an<fixed-case>D</fixed-case></title>
      <author><first>Eric</first><last>Sanders</last></author>
      <author><first>Sara</first><last>Petrollino</last></author>
      <author><first>Gilles R.</first><last>Scheifer</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Christopher</first><last>Handy</last></author>
      <pages>7101–7106</pages>
      <abstract>LeiLanD (Leiden Language Data) is a searchable catalogue initiated by the Leiden University Centre for Linguistics (LUCL) with the support of CLARIAH. The catalogue contains metadata about language datasets collected at LUCL and other institutes of Leiden University. This paper describes a project to FAIRify the datasets increasing their findability and accessibility through a standardised metadata format CMDI so as to obtain a rich metadata description for all resources and to make them findable through CLARIN’s Virtual Language Observatory. The paper describes the creation of the catalogue and the steps that led from unstructured metadata to CMDI standards. This FAIRifi- cation of LeiLanD has enhanced the findability and accessibility of incredibly diverse collection of language datasets.</abstract>
      <url hash="a92be5b2">2024.lrec-main.623</url>
      <bibkey>sanders-etal-2024-fairification-leiland</bibkey>
    </paper>
    <paper id="624">
      <title><fixed-case>F</fixed-case>al<fixed-case>AI</fixed-case>: A Dataset for End-to-end Spoken Language Understanding in a Low-Resource Scenario</title>
      <author><first>Andres</first><last>Pineiro-Martin</last></author>
      <author><first>Carmen</first><last>Garcia-Mateo</last></author>
      <author><first>Laura</first><last>Docio-Fernandez</last></author>
      <author><first>Maria del Carmen</first><last>Lopez-Perez</last></author>
      <author><first>Jose</first><last>Gandarela-Rodriguez</last></author>
      <pages>7107–7116</pages>
      <abstract>End-to-end (E2E) Spoken Language Understanding (SLU) systems infer structured information directly from the speech signal using a single model. Due to the success of virtual assistants and the increasing demand for speech interfaces, these architectures are being actively researched for their potential to improve system performance by exploiting acoustic information and avoiding the cascading errors of traditional architectures. However, these systems require large amounts of specific, well-labelled speech data for training, which is expensive to obtain even in English, where the number of public audio datasets for SLU is limited. In this paper, we release the FalAI dataset, the largest public SLU dataset in terms of hours (250 hours), recordings (260,000) and participants (over 10,000), which is also the first SLU dataset in Galician and the first to be obtained in a low-resource scenario. Furthermore, we present new measures of complexity for the text corpora, the strategies followed for the design, collection and validation of the dataset, and we define splits for noisy audio, hesitant audio and audio where the sentence has changed but the structured information is preserved. These novel splits provide a unique resource for testing SLU systems in challenging, real-world scenarios.</abstract>
      <url hash="064d71a0">2024.lrec-main.624</url>
      <bibkey>pineiro-martin-etal-2024-falai-dataset</bibkey>
    </paper>
    <paper id="625">
      <title>Fast Adaptation via Prompted Data: An Efficient Cross-Domain Fine-tuning Method for Large Language Models</title>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Hantao</first><last>Yang</last></author>
      <author><first>Haobo</first><last>Wang</last></author>
      <author><first>Jake</first><last>Zhao</last></author>
      <pages>7117–7132</pages>
      <abstract>Large language models (LLMs) have achieved great success in a variety of natural language understanding tasks. However, domain discrepancies between the downstream task and the pre-training corpora may have hurdled LLMs to excel further in the vertical applications. Contrary to prior computational-heavy methods, we propose a lightweight solution to further bridge the gap in applying LLMs to diverse downstream tasks — a Fast Adaptation method for LLMs via Prompted Data, in short FAvPD. Notably, with FAvPD, we establish an additional adaptive tuning procedure, wherein we integrate downstream text corpora, gold labels as well as external knowledge sources and then envelop them into a form of highly controllable prompt. As a simple, easy-to-use, and versatile solution, FAvPD lies in the intersection of regimes like knowledge-augmented LLMs, fine-tuning, and adaptation techniques. With extensive experiments, we prove that FAvPD excels in both performance efficacy and training efficiency over related prior works. FAvPD is publicly available at https://github.com/Hyatio/FAvPD.</abstract>
      <url hash="61e13f18">2024.lrec-main.625</url>
      <bibkey>zhang-etal-2024-fast-adaptation</bibkey>
    </paper>
    <paper id="626">
      <title><fixed-case>F</fixed-case>ast<fixed-case>S</fixed-case>pell: The <fixed-case>L</fixed-case>ang<fixed-case>I</fixed-case>d Magic Spell</title>
      <author><first>Marta</first><last>Bañón</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Jaume</first><last>Zaragoza-Bernabeu</last></author>
      <author><first>Sergio</first><last>Ortiz Rojas</last></author>
      <pages>7133–7140</pages>
      <abstract>Language identification is a crucial component in the automated production of language resources, particularly in multilingual and big data contexts. However, commonly used language identifiers struggle to differentiate between similar or closely-related languages. This paper introduces FastSpell, a language identifier that combines fastText (a pre-trained language identifier tool) and Hunspell (a spell checker) with the aim of having a refined second-opinion before deciding which language should be assigned to a text. We provide a description of the FastSpell algorithm along with an explanation on how to use and configure it. To that end, we motivate the need of such a tool and present a benchmark including some popular language identifiers evaluated during the development of FastSpell. We show how FastSpell is useful not only to improve identification of similar languages, but also to identify new ones ignored by other tools.</abstract>
      <url hash="960a62b7">2024.lrec-main.626</url>
      <bibkey>banon-etal-2024-fastspell-langid</bibkey>
    </paper>
    <paper id="627">
      <title><fixed-case>FCDS</fixed-case>: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction</title>
      <author><first>Xudong</first><last>Zhu</last></author>
      <author><first>Zhao</first><last>Kang</last></author>
      <author><first>Bei</first><last>Hui</last></author>
      <pages>7141–7152</pages>
      <abstract>Document-level Relation Extraction (DocRE) aims to identify relation labels between entities within a single document. It requires handling several sentences and reasoning over them. State-of-the-art DocRE methods use a graph structure to connect entities across the document to capture dependency syntax information. However, this is insufficient to fully exploit the rich syntax information in the document. In this work, we propose to fuse constituency and dependency syntax into DocRE. It uses constituency syntax to aggregate the whole sentence information and select the instructive sentences for the pairs of targets. It exploits dependency syntax in a graph structure with constituency syntax enhancement and chooses the path between entity pairs based on the dependency graph. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed method.</abstract>
      <url hash="41237613">2024.lrec-main.627</url>
      <bibkey>zhu-etal-2024-fcds-fusing</bibkey>
    </paper>
    <paper id="628">
      <title>Feature Structure Matching for Multi-source Sentiment Analysis with Efficient Adaptive Tuning</title>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Cheng</first><last>Liu</last></author>
      <author><first>Yu</first><last>Tong</last></author>
      <author><first>Jiang</first><last>Dazhi</last></author>
      <pages>7153–7162</pages>
      <abstract>Recently, fine-tuning the large pre-trained language models on the labeled sentiment dataset achieves appealing performance. However, the obtained model may not generalize well to the other domains due to the domain shift, and it is expensive to update the entire parameters within the large models. Although some existing domain matching methods are proposed to alleviate the above issues, there are multiple relevant source domains in practice which makes the whole training more costly and complicated. To this end, we focus on the efficient unsupervised multi-source sentiment adaptation task which is more challenging and beneficial for real-world applications. Specifically, we propose to extract multi-layer features from the large pre-trained model, and design a dynamic parameters fusion module to exploit these features for both efficient and adaptive tuning. Furthermore, we propose a novel feature structure matching constraint, which enforces similar feature-wise correlations across different domains. Compared with the traditional domain matching methods which tend to pull all feature instances close, we show that the proposed feature structure matching is more robust and generalizable in the multi-source scenario. Extensive experiments on several multi-source sentiment analysis benchmarks demonstrate the effectiveness and superiority of our proposed framework.</abstract>
      <url hash="962162d0">2024.lrec-main.628</url>
      <bibkey>li-etal-2024-feature-structure</bibkey>
    </paper>
    <paper id="629">
      <title>Federated Document-Level Biomedical Relation Extraction with Localized Context Contrast</title>
      <author><first>Yan</first><last>Xiao</last></author>
      <author><first>Yaochu</first><last>Jin</last></author>
      <author><first>Kuangrong</first><last>Hao</last></author>
      <pages>7163–7173</pages>
      <abstract>Existing studies on relation extraction focus at the document level in a centralized training environment, requiring the collection of documents from various sources. However, this raises concerns about privacy protection, especially in sensitive domains such as finance and healthcare. For the first time, this work extends document-level relation extraction to a federated environment. The proposed federated framework, called FedLCC, is tailored for biomedical relation extraction that enables collaborative training without sharing raw medical texts. To fully exploit the models of all participating clients and improve the local training on individual clients, we propose a novel concept of localized context contrast on the basis of contrastive learning. By comparing and rectifying the similarity of localized context in documents between clients and the central server, the global model can better represent the documents on individual clients. Due to the lack of a widely accepted measure of non-IID text data, we introduce a novel non-IID scenario based on graph structural entropy. Experimental results on three document-level biomedical relation extraction datasets demonstrate the effectiveness of our method. Our code is available at https://github.com/xxxxyan/FedLCC.</abstract>
      <url hash="c56b5e81">2024.lrec-main.629</url>
      <bibkey>xiao-etal-2024-federated-document</bibkey>
    </paper>
    <paper id="630">
      <title>Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models</title>
      <author><first>Sixing</first><last>Yu</last></author>
      <author><first>Juan Pablo</first><last>Munoz</last></author>
      <author><first>Ali</first><last>Jannesari</last></author>
      <pages>7174–7184</pages>
      <abstract>Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains. In this paper, we propose the Federated Foundation Models (FFMs) paradigm, which combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple end-users. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further outline potential future research avenues in FFM, including FFM pre-training, FFM fine-tuning, and federated prompt tuning, which allow the development of more personalized and context-aware models while ensuring data privacy. Moreover, we explore the possibility of continual/lifelong learning in FFMs, as increased computational power at the edge may unlock the potential for optimizing FMs using newly generated private data close to the data source. The proposed FFM concepts offer a flexible and scalable framework for training large language models in a privacy-preserving manner, setting the stage for subsequent advancements in both FM training and federated learning.</abstract>
      <url hash="5eed95bc">2024.lrec-main.630</url>
      <bibkey>yu-etal-2024-federated-foundation</bibkey>
    </paper>
    <paper id="631">
      <title>Few-Shot Learning for Cold-Start Recommendation</title>
      <author><first>Mingming</first><last>Li</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <author><first>Fuqing</first><last>Zhu</last></author>
      <author><first>Qiannan</first><last>Zhu</last></author>
      <pages>7185–7195</pages>
      <abstract>Cold-start is a significant problem in recommender systems. Recently, with the development of few-shot learning and meta-learning techniques, many researchers have devoted themselves to adopting meta-learning into recommendation as the natural scenario of few-shots. Nevertheless, we argue that recent work has a huge gap between few-shot learning and recommendations. In particular, users are locally dependent, not globally independent in recommendation. Therefore, it is necessary to formulate the local relationships between users. To accomplish this, we present a novel Few-shot learning method for Cold-Start (FCS) recommendation that consists of three hierarchical structures. More concretely, this first hierarchy is the global-meta parameters for learning the global information of all users; the second hierarchy is the local-meta parameters whose goal is to learn the adaptive cluster of local users; the third hierarchy is the specific parameters of the target user. Both the global and local information are formulated, addressing the new user’s problem in accordance with the few-shot records rapidly. Experimental results on two public real-world datasets show that the FCS method could produce stable improvements compared with the state-of-the-art.</abstract>
      <url hash="584b32b0">2024.lrec-main.631</url>
      <bibkey>li-etal-2024-shot-learning</bibkey>
    </paper>
    <paper id="632">
      <title>Few-shot Link Prediction on Hyper-relational Facts</title>
      <author><first>Jiyao</first><last>Wei</last></author>
      <author><first>Saiping</first><last>Guan</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>7196–7207</pages>
      <abstract>Hyper-relational facts, which consist of a primary triple (head entity, relation, tail entity) and auxiliary attribute-value pairs, are widely present in real-world Knowledge Graphs (KGs). Link Prediction on Hyper-relational Facts (LPHFs) is to predict a missing element in a hyper-relational fact, which helps populate and enrich KGs. However, existing LPHFs studies usually require an amount of high-quality data. They overlook few-shot relations, which have limited instances, yet are common in real-world scenarios. Thus, we introduce a new task, Few-Shot Link Prediction on Hyper-relational Facts (FSLPHFs). It aims to predict a missing entity in a hyper-relational fact with limited support instances. To tackle FSLPHFs, we propose MetaRH, a model that learns Meta Relational information in Hyper-relational facts. MetaRH comprises three modules: relation learning, support-specific adjustment, and query inference. By capturing meta relational information from limited support instances, MetaRH can accurately predict the missing entity in a query. As there is no existing dataset available for this new task, we construct three datasets to validate the effectiveness of MetaRH. Experimental results on these datasets demonstrate that MetaRH significantly outperforms existing representative models.</abstract>
      <url hash="f9e3ee07">2024.lrec-main.632</url>
      <bibkey>wei-etal-2024-shot-link</bibkey>
    </paper>
    <paper id="633">
      <title>Few-Shot Multimodal Named Entity Recognition Based on Mutlimodal Causal Intervention Graph</title>
      <author><first>Feihong</first><last>Lu</last></author>
      <author><first>Xiaocui</first><last>Yang</last></author>
      <author><first>Qian</first><last>Li</last></author>
      <author><first>Qingyun</first><last>Sun</last></author>
      <author><first>Ke</first><last>Jiang</last></author>
      <author><first>Cheng</first><last>Ji</last></author>
      <author><first>Jianxin</first><last>Li</last></author>
      <pages>7208–7219</pages>
      <abstract>Multimodal Named Entity Recognition (MNER) models typically require a significant volume of labeled data for effective training to extract relations between entities. In real-world scenarios, we frequently encounter unseen relation types. Nevertheless, existing methods are predominantly tailored for complete datasets and are not equipped to handle these new relation types. In this paper, we introduce the Few-shot Multimodal Named Entity Recognition (FMNER) task to address these novel relation types. FMNER trains in the source domain (seen types) and tests in the target domain (unseen types) with different distributions. Due to limited available resources for sampling, each sampling instance yields different content, resulting in data bias and alignment problems of multimodal units (image patches and words). To alleviate the above challenge, we propose a novel Multimodal causal Intervention graphs (MOUSING) model for FMNER. Specifically, we begin by constructing a multimodal graph that incorporates fine-grained information from multiple modalities. Subsequently, we introduce the Multimodal Causal Intervention Strategy to update the multimodal graph. It aims to decrease spurious correlations and emphasize accurate correlations between multimodal units, resulting in effectively aligned multimodal representations. Extensive experiments on two multimodal named entity recognition datasets demonstrate the superior performance of our model in the few-shot setting.</abstract>
      <url hash="8f5171d9">2024.lrec-main.633</url>
      <bibkey>lu-etal-2024-shot-multimodal</bibkey>
    </paper>
    <paper id="634">
      <title>Few-shot Named Entity Recognition via Superposition Concept Discrimination</title>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Bin</first><last>Dong</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>7220–7231</pages>
      <abstract>Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models. To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases. Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts.</abstract>
      <url hash="09586eeb">2024.lrec-main.634</url>
      <bibkey>chen-etal-2024-shot-named-entity</bibkey>
    </paper>
    <paper id="635">
      <title>Few-Shot Relation Extraction with Hybrid Visual Evidence</title>
      <author><first>Jiaying</first><last>Gong</last></author>
      <author><first>Hoda</first><last>Eldardiry</last></author>
      <pages>7232–7247</pages>
      <abstract>The goal of few-shot relation extraction is to predict relations between name entities in a sentence when only a few labeled instances are available for training. Existing few-shot relation extraction methods focus on uni-modal information such as text only. This reduces performance when there is no clear contexts between the name entities described in text. We propose a multi-modal few-shot relation extraction model (MFS-HVE) that leverages both textual and visual semantic information to learn a multi-modal representation jointly. The MFS-HVE includes semantic feature extractors and multi-modal fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE multi-modal fusion unit integrates information from various modalities using image-guided attention, object-guided attention, and hybrid feature attention to fully capture the semantic interaction between visual regions of images and relevant texts. Extensive experiments conducted on two public datasets demonstrate that semantic visual information significantly improves performance of few-shot relation prediction.</abstract>
      <url hash="9fa755f1">2024.lrec-main.635</url>
      <bibkey>gong-eldardiry-2024-shot-relation</bibkey>
    </paper>
    <paper id="636">
      <title>Few-Shot Semantic Dependency Parsing via Graph Contrastive Learning</title>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Yunlong</first><last>Fan</last></author>
      <author><first>Yikemaiti</first><last>Sataer</last></author>
      <author><first>Chuanqi</first><last>Shi</last></author>
      <author><first>Miao</first><last>Gao</last></author>
      <author><first>Zhiqiang</first><last>Gao</last></author>
      <pages>7248–7258</pages>
      <abstract>Graph neural networks (GNNs) have achieved promising performance on semantic dependency parsing (SDP), owing to their powerful graph representation learning ability. However, training a high-performing GNN-based model requires a large amount of labeled data and it is prone to over-fitting in the absence of sufficient labeled data. To address this drawback, we propose a syntax-guided graph contrastive learning framework to pre-train GNNs with plenty of unlabeled data and fine-tune pre-trained GNNs with few-shot labeled SDP data. Through extensive experiments conducted on the SemEval-2015 Task 18 English dataset in three formalisms (DM, PAS, and PSD), we demonstrate that our framework achieves promising results when few-shot training samples are available. Furthermore, benefiting from the pre-training process, our framework exhibits notable advantages in the out-of-domain test sets.</abstract>
      <url hash="2df5738b">2024.lrec-main.636</url>
      <bibkey>li-etal-2024-shot-semantic-dependency</bibkey>
    </paper>
    <paper id="637">
      <title>Few-shot Temporal Pruning Accelerates Diffusion Models for Text Generation</title>
      <author><first>Bocheng</first><last>Li</last></author>
      <author><first>Zhujin</first><last>Gao</last></author>
      <author><first>Yongxin</first><last>Zhu</last></author>
      <author><first>Kun</first><last>Yin</last></author>
      <author><first>Haoyu</first><last>Cao</last></author>
      <author><first>Deqiang</first><last>Jiang</last></author>
      <author><first>Linli</first><last>Xu</last></author>
      <pages>7259–7269</pages>
      <abstract>Diffusion models have achieved significant success in computer vision and shown immense potential in natural language processing applications, particularly for text generation tasks. However, generating high-quality text using these models often necessitates thousands of iterations, leading to slow sampling rates. Existing acceleration methods either neglect the importance of the distribution of sampling steps, resulting in compromised performance with smaller number of iterations, or require additional training, introducing considerable computational overheads. In this paper, we present Few-shot Temporal Pruning, a novel technique designed to accelerate diffusion models for text generation without supplementary training while effectively leveraging limited data. Employing a Bayesian optimization approach, our method effectively eliminates redundant sampling steps during the sampling process, thereby enhancing the generation speed. A comprehensive evaluation of discrete and continuous diffusion models across various tasks, including machine translation, question generation, and paraphrasing, reveals that our approach achieves competitive performance even with minimal sampling steps after down to less than 1 minute of optimization, yielding a significant acceleration of up to 400x in text generation tasks.</abstract>
      <url hash="2729f2f2">2024.lrec-main.637</url>
      <bibkey>li-etal-2024-shot-temporal-pruning</bibkey>
    </paper>
    <paper id="638">
      <title><fixed-case>FFSTC</fixed-case>: Fongbe to <fixed-case>F</fixed-case>rench Speech Translation Corpus</title>
      <author><first>D. Fortuné</first><last>Kponou</last></author>
      <author><first>Fréjus A. A.</first><last>Laleye</last></author>
      <author><first>Eugène Cokou</first><last>Ezin</last></author>
      <pages>7270–7276</pages>
      <abstract>In this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC). This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq’s transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score BLEU of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus.</abstract>
      <url hash="52ecf238">2024.lrec-main.638</url>
      <bibkey>kponou-etal-2024-ffstc-fongbe</bibkey>
    </paper>
    <paper id="639">
      <title><fixed-case>F</fixed-case>in<fixed-case>C</fixed-case>orpus-<fixed-case>DE</fixed-case>10k: A Corpus for the <fixed-case>G</fixed-case>erman Financial Domain</title>
      <author><first>Serhii</first><last>Hamotskyi</last></author>
      <author><first>Nata</first><last>Kozaeva</last></author>
      <author><first>Christian</first><last>Hänig</last></author>
      <pages>7277–7285</pages>
      <abstract>We introduce a predominantly German corpus comprising 12.5k PDF documents sourced from the financial domain. The corresponding extracted textual data encompasses more than 165 million tokens derived predominantly from German, and to a lesser extent, bilingual documents. We provide detailed information about the document types included in the corpus, such as final terms, base prospectuses, annual reports, information materials, law documents, international financial reporting standards, and monthly reports from the Bundesbank, accompanied by comprehensive statistical analysis. To our knowledge, it is the first non-email German financial corpus available, and we hope it will fill this gap and foster further research in the financial domain both in the German language and in multilingual contexts.</abstract>
      <url hash="9f1d3716">2024.lrec-main.639</url>
      <bibkey>hamotskyi-etal-2024-fincorpus-de10k</bibkey>
    </paper>
    <paper id="640">
      <title>Finding Educationally Supportive Contexts for Vocabulary Learning with Attention-Based Models</title>
      <author><first>Sungjin</first><last>Nam</last></author>
      <author><first>Kevyn</first><last>Collins-Thompson</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Xin</first><last>Tong</last></author>
      <pages>7286–7295</pages>
      <abstract>When learning new vocabulary, both humans and machines acquire critical information about the meaning of an unfamiliar word through contextual information in a sentence or passage. However, not all contexts are equally helpful for learning an unfamiliar ‘target’ word. Some contexts provide a rich set of semantic clues to the target word’s meaning, while others are less supportive. We explore the task of finding educationally supportive contexts with respect to a given target word for vocabulary learning scenarios, particularly for improving student literacy skills. Because of their inherent context-based nature, attention-based deep learning methods provide an ideal starting point. We evaluate attention-based approaches for predicting the amount of educational support from contexts, ranging from a simple custom model using pre-trained embeddings with an additional attention layer, to a commercial Large Language Model (LLM). Using an existing major benchmark dataset for educational context support prediction, we found that a sophisticated but generic LLM had poor performance, while a simpler model using a custom attention-based approach achieved the best-known performance to date on this dataset.</abstract>
      <url hash="b6f9cfe6">2024.lrec-main.640</url>
      <bibkey>nam-etal-2024-finding-educationally</bibkey>
    </paper>
    <paper id="641">
      <title>Finding Spoken Identifications: Using <fixed-case>GPT</fixed-case>-4 Annotation for an Efficient and Fast Dataset Creation Pipeline</title>
      <author><first>Maliha</first><last>Jahan</last></author>
      <author><first>Helin</first><last>Wang</last></author>
      <author><first>Thomas</first><last>Thebaud</last></author>
      <author><first>Yinglun</first><last>Sun</last></author>
      <author><first>Giang Ha</first><last>Le</last></author>
      <author><first>Zsuzsanna</first><last>Fagyal</last></author>
      <author><first>Odette</first><last>Scharenborg</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Laureano</first><last>Moro Velazquez</last></author>
      <author><first>Najim</first><last>Dehak</last></author>
      <pages>7296–7306</pages>
      <abstract>The growing emphasis on fairness in speech-processing tasks requires datasets with speakers from diverse subgroups that allow training and evaluating fair speech technology systems. However, creating such datasets through manual annotation can be costly. To address this challenge, we present a semi-automated dataset creation pipeline that leverages large language models. We use this pipeline to generate a dataset of speakers identifying themself or another speaker as belonging to a particular race, ethnicity, or national origin group. We use OpenaAI’s GPT-4 to perform two complex annotation tasks- separating files relevant to our intended dataset from the irrelevant ones (filtering) and finding and extracting information on identifications within a transcript (tagging). By evaluating GPT-4’s performance using human annotations as ground truths, we show that it can reduce resources required by dataset annotation while barely losing any important information. For the filtering task, GPT-4 had a very low miss rate of 6.93%. GPT-4’s tagging performance showed a trade-off between precision and recall, where the latter got as high as 97%, but precision never exceeded 45%. Our approach reduces the time required for the filtering and tagging tasks by 95% and 80%, respectively. We also present an in-depth error analysis of GPT-4’s performance.</abstract>
      <url hash="22661a37">2024.lrec-main.641</url>
      <bibkey>jahan-etal-2024-finding-spoken</bibkey>
    </paper>
    <paper id="642">
      <title>Find-the-Common: A Benchmark for Explaining Visual Patterns from Images</title>
      <author><first>Yuting</first><last>Shi</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Houjing</first><last>Wei</last></author>
      <author><first>Yufeng</first><last>Zhao</last></author>
      <author><first>Tao</first><last>Jin</last></author>
      <pages>7307–7313</pages>
      <abstract>Recent advances in Instruction-fine-tuned Vision and Language Models (IVLMs), such as GPT-4V and InstructBLIP, have prompted some studies have started an in-depth analysis of the reasoning capabilities of IVLMs. However, Inductive Visual Reasoning, a vital skill for text-image understanding, remains underexplored due to the absence of benchmarks. In this paper, we introduce Find-the-Common (FTC): a new vision and language task for Inductive Visual Reasoning. In this task, models are required to identify an answer that explains the common attributes across visual scenes. We create a new dataset for the FTC and assess the performance of several contemporary approaches including Image-Based Reasoning, Text-Based Reasoning, and Image-Text-Based Reasoning with various models. Extensive experiments show that even state-of-the-art models like GPT-4V can only archive with 48% accuracy on the FTC, for which, the FTC is a new challenge for the visual reasoning research community. Our dataset has been released and is available online: https://github.com/SSSSSeki/Find-the-common.</abstract>
      <url hash="37d22a80">2024.lrec-main.642</url>
      <bibkey>shi-etal-2024-find-common</bibkey>
    </paper>
    <paper id="643">
      <title>Fine-grained Classification of Circumstantial Meanings within the <fixed-case>P</fixed-case>rague Dependency Treebank Annotation Scheme</title>
      <author><first>Marie</first><last>Mikulova</last></author>
      <pages>7314–7323</pages>
      <abstract>In the contribution, we propose a formally and semantically based fine-grained classification of circumstantial meanings based on the analysis of a large number of valuable examples from the Prague Dependency Treebanks. The methodology and principles of the presented approach are elaborated in detail and demonstrated on two case studies. The classification of circumstantial meanings is carried out for the Czech language, but the methodology and principles used are language independent. The contribution also addresses the question of language universality and specificity through a comparison with English. The aim of this work is to enrich the annotation in the Prague Dependency Treebanks with detailed information on circumstantial meanings but it may also be useful for other semantically oriented projects. To the best of our knowledge, a similar corpus-based and corpus-verified elaborate classification of circumstantial meanings has not yet been proposed in any annotation project. The contribution presents the results of an ongoing work.</abstract>
      <url hash="f133967c">2024.lrec-main.643</url>
      <bibkey>mikulova-2024-fine-grained</bibkey>
    </paper>
    <paper id="644">
      <title>Fine-Grained Legal Argument-Pair Extraction via Coarse-Grained Pre-training</title>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Yutao</first><last>Sun</last></author>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Wenbin</first><last>Zhang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>7324–7335</pages>
      <abstract>Legal Argument-Pair Extraction (LAE) is dedicated to the identification of interactive arguments targeting the same subject matter within legal complaints and corresponding defenses. This process serves as a foundation for automatically recognizing the focal points of disputes. Current methodologies predominantly conceptualize LAE as a supervised sentence-pair classification problem and usually necessitate extensive manual annotations, thereby constraining their scalability and general applicability. To this end, we present an innovative approach to LAE that focuses on fine-grained alignment of argument pairs, building upon coarse-grained complaint-defense pairs. This strategy stems from two key observations: 1) In general, every argument presented in a legal complaint is likely to be addressed by at least one corresponding argument in the defense. 2) It’s rare for multiple complaint arguments to be addressed by a single defense argument; rather, each complaint argument usually corresponds to a unique defense argument. Motivated by these insights, we develop a specialized pre-training framework. Our model employs pre-training objectives designed to exploit the coarse-grained supervision signals. This enables expressive representations of legal arguments for LAE, even when working with a limited amount of labeled data. To verify the effectiveness of our model, we construct the largest LAE datasets from two representative causes, private lending, and contract dispute. The experimental results demonstrate that our model can effectively capture informative argument knowledge from unlabeled complaint-defense pairs and outperform the unsupervised and supervised baselines by 3.7 and 2.4 points on average respectively. Besides, our model can reach superior accuracy with only half manually annotated data. The datasets and code can be found in https://github.com/thunlp/LAE.</abstract>
      <url hash="a3ddf54d">2024.lrec-main.644</url>
      <bibkey>xiao-etal-2024-fine-grained</bibkey>
    </paper>
    <paper id="645">
      <title>Fine-Tuning a Pre-Trained <fixed-case>W</fixed-case>av2<fixed-case>V</fixed-case>ec2 Model for Automatic Speech Recognition- Experiments with De Zahrar Sproche</title>
      <author><first>Andrea</first><last>Gulli</last></author>
      <author><first>Francesco</first><last>Costantini</last></author>
      <author><first>Diego</first><last>Sidraschi</last></author>
      <author><first>Emanuela</first><last>Li Destri</last></author>
      <pages>7336–7342</pages>
      <abstract>We present the results of an Automatic Speech Recognition system developed to support linguistic documentation efforts. The test case is the zahrar sproche language, a Southern Bavarian variety spoken in the language island of Sauris/Zahre in Italy. We collected a dataset of 9,000 words and approximately 80 minutes of speech. The goal is to reduce the transcription workload of field linguists. The method used is a deep learning approach based on the language-specific tuning of a generic pre-trained representation model, XLS-R. The transcription quality of the experiments on the collected dataset is promising. We test the model’s performance on some fieldwork historical recordings, report the results, and evaluate them qualitatively. Finally, we indicate possibilities for improvement in this challenging task.</abstract>
      <url hash="72fa4c59">2024.lrec-main.645</url>
      <bibkey>gulli-etal-2024-fine-tuning</bibkey>
    </paper>
    <paper id="646">
      <title>First Steps Towards the Integration of Resources on Historical Glossing Traditions in the History of <fixed-case>C</fixed-case>hinese: A Collection of Standardized Fǎnqiè Spellings from the Guǎngyùn</title>
      <author><first>Michele</first><last>Pulini</last></author>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <pages>7343–7348</pages>
      <abstract>Due to the peculiar nature of the Chinese writing system, it is difficult to assess the pronunciation of historical varieties of Chinese. In order to reconstruct ancient pronunciations, historical glossing practices play a crucial role. However, although studied thoroughly by numerous scholars, most research has been carried out in a qualitative manner, and no attempt at providing integrated resources of historical glossing practices has been made so far. Here, we present a first step towards the integration of resources on historical glossing traditions in the history of Chinese. Our starting point are so-called fǎnqiè spellings in the Guǎngyùn, one of the early rhyme books in the history of Chinese, providing pronunciations for more than 20000 Chinese characters. By standardizing digital versions of the resource using tools from computational historical linguistics, we show that we can predict historical spellings with high precision and at the same time shed light on the precision of ancient glossing practices. Although a considerably small first step, our resource could be the starting point for an integrated, standardized collection that could ultimately shed new light on the history of Chinese.</abstract>
      <url hash="2af5d550">2024.lrec-main.646</url>
      <bibkey>pulini-list-2024-first-steps</bibkey>
    </paper>
    <paper id="647">
      <title>Fisher Mask Nodes for Language Model Merging</title>
      <author><first>Thennal</first><last>D K</last></author>
      <author><first>Ganesh</first><last>Nathan</last></author>
      <author><first>Suchithra</first><last>M S</last></author>
      <pages>7349–7355</pages>
      <abstract>Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance increase across various models in the BERT family, outperforming full-scale Fisher-weighted averaging in a fraction of the computational cost, with baseline performance improvements of up to +6.5 and a speedup between 57.4x and 321.7x across models. Our results prove the potential of our method in current multi-task learning environments and suggest its scalability and adaptability to new model architectures and learning scenarios.</abstract>
      <url hash="1fd2be88">2024.lrec-main.647</url>
      <bibkey>d-k-etal-2024-fisher-mask</bibkey>
    </paper>
    <paper id="648">
      <title><fixed-case>F</fixed-case>latten<fixed-case>Q</fixed-case>uant: Breaking through the Inference Compute-bound for Large Language Models with Per-tensor Quantization</title>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Yang</last></author>
      <author><first>Shuang</first><last>Peng</last></author>
      <author><first>Fangyu</first><last>Wang</last></author>
      <author><first>Aimin</first><last>Pan</last></author>
      <pages>7356–7365</pages>
      <abstract>Large language models (LLMs) have demonstrated state-of-the-art accuracies across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the larger channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layer using 8 bits. The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation. Our work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.</abstract>
      <url hash="28254531">2024.lrec-main.648</url>
      <bibkey>zhang-etal-2024-flattenquant-breaking</bibkey>
    </paper>
    <paper id="649">
      <title>Flexible Lexicalization in Rule-based Text Realization</title>
      <author><first>Avril</first><last>Gazeau</last></author>
      <author><first>Francois</first><last>Lareau</last></author>
      <pages>7366–7376</pages>
      <abstract>GenDR is a text realizer that takes as input a graph-based semantic representation and outputs the corresponding syntactic dependency trees. One of the tasks in this transduction is lexicalization, i.e., choosing the right lexical units to express a given semanteme. To do so, GenDR uses a semantic dictionary that maps semantemes to corresponding lexical units in a given language. This study aims to develop a flexible lexicalization module to automatically build a rich semantic dictionary for French. To achieve this, we tried two methods. The first one consisted in extracting information from the French Lexical Network, a large-scale French lexical resource, and adapting it to GenDR. The second one was to test a contextual neural language model’s ability to generate potential additional lexicalizations. The first method significantly broadened the coverage of GenDR, while the additional lexicalizations produced by the language model turned out to be of limited use, which brings us to the conclusion that it is not suited to perform the task we’ve asked from it.</abstract>
      <url hash="9701e77e">2024.lrec-main.649</url>
      <bibkey>gazeau-lareau-2024-flexible-lexicalization</bibkey>
    </paper>
    <paper id="650">
      <title><fixed-case>FLOR</fixed-case>: On the Effectiveness of Language Adaptation</title>
      <author><first>Severino</first><last>Da Dalt</last></author>
      <author><first>Joan</first><last>Llop</last></author>
      <author><first>Irene</first><last>Baucells</last></author>
      <author><first>Marc</first><last>Pamies</last></author>
      <author><first>Yishi</first><last>Xu</last></author>
      <author><first>Aitor</first><last>Gonzalez-Agirre</last></author>
      <author><first>Marta</first><last>Villegas</last></author>
      <pages>7377–7388</pages>
      <abstract>Large language models have amply proven their great capabilities, both in downstream tasks and real-life settings. However, low- and mid-resource languages do not have access to the necessary means to train such models from scratch, and often have to rely on multilingual models despite being underrepresented in the training data. For the particular case of the Catalan language, we prove that continued pre-training with vocabulary adaptation is a better alternative to take the most out of already pre-trained models, even if these have not seen any Catalan data during their pre-training phase. We curate a 26B tokens corpus and use it to further pre-train BLOOM, giving rise to the FLOR models. We perform an extensive evaluation to assess the effectiveness of our method, obtaining consistent gains across Catalan and Spanish tasks. The models, training data, and evaluation framework are made freely available under permissive licenses.</abstract>
      <url hash="88a2e30a">2024.lrec-main.650</url>
      <bibkey>da-dalt-etal-2024-flor-effectiveness</bibkey>
    </paper>
    <paper id="651">
      <title><fixed-case>F</fixed-case>o<fixed-case>RC</fixed-case>4<fixed-case>CL</fixed-case>: A Fine-grained Field of Research Classification and Annotated Dataset of <fixed-case>NLP</fixed-case> Articles</title>
      <author><first>Raia Abu</first><last>Ahmad</last></author>
      <author><first>Ekaterina</first><last>Borisova</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>7389–7394</pages>
      <abstract>The steep increase in the number of scholarly publications has given rise to various digital repositories, libraries and knowledge graphs aimed to capture, manage, and preserve scientific data. Efficiently navigating such databases requires a system able to classify scholarly documents according to the respective research (sub-)field. However, not every digital repository possesses a relevant classification schema for categorising publications. For instance, one of the largest digital archives in Computational Linguistics (CL) and Natural Language Processing (NLP), the ACL Anthology, lacks a system for classifying papers into topics and sub-topics. This paper addresses this gap by constructing a corpus of 1,500 ACL Anthology publications annotated with their main contributions using a novel hierarchical taxonomy of core CL/NLP topics and sub-topics. The corpus is used in a shared task with the goal of classifying CL/NLP papers into their respective sub-topics.</abstract>
      <url hash="806ff516">2024.lrec-main.651</url>
      <bibkey>ahmad-etal-2024-forc4cl-fine</bibkey>
    </paper>
    <paper id="652">
      <title><fixed-case>FORECAST</fixed-case>2023: A Forecast and Reasoning Corpus of Argumentation Structures</title>
      <author><first>Kamila</first><last>Górska</last></author>
      <author><first>John</first><last>Lawrence</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <pages>7395–7405</pages>
      <abstract>It is known from large-scale crowd experimentation that some people are innately better at analysing complex situations and making justified predictions – the so-called ‘superforecasters’. Surprisingly, however, there has to date been no work exploring the role played by the reasoning in those justifications. Bag-of-words analyses might tell us something, but the real value lies in understanding what features of reasoning and argumentation lead to better forecasts – both in providing an objective measure for argument quality, and even more importantly, in providing guidance on how to improve forecasting performance. The work presented here covers the creation of a unique dataset of such prediction rationales, the structure of which naturally lends itself to partially automated annotation which in turn is used as the basis for subsequent manual enhancement that provides a uniquely fine-grained and close characterisation of the structure of argumentation, with potential impact on forecasting domains from intelligence analysis to investment decision-making.</abstract>
      <url hash="55491650">2024.lrec-main.652</url>
      <bibkey>gorska-etal-2024-forecast2023-forecast</bibkey>
    </paper>
    <paper id="653">
      <title><fixed-case>F</fixed-case>o<fixed-case>T</fixed-case>o: Targeted Visual Topic Modeling for Focused Analysis of Short Texts</title>
      <author><first>Sanuj</first><last>Kumar</last></author>
      <author><first>Tuan</first><last>Le</last></author>
      <pages>7406–7416</pages>
      <abstract>Given a corpus of documents, focused analysis aims to find topics relevant to aspects that a user is interested in. The aspects are often expressed by a set of keywords provided by the user. Short texts such as microblogs and tweets pose several challenges to this task because the sparsity of word co-occurrences may hinder the extraction of meaningful and relevant topics. Moreover, most of the existing topic models perform a full corpus analysis that treats all topics equally, which may make the learned topics not be on target. In this paper, we propose a novel targeted topic model for semantic short-text embedding which aims to learn all topics and low-dimensional visual representations of documents, while preserving relevant topics for focused analysis of short texts. To preserve the relevant topics in the visualization space, we propose jointly modeling topics and the pairwise document ranking based on document-keyword distances in the visualization space. The extensive experiments on several real-world datasets demonstrate the effectiveness of our proposed model in terms of targeted topic modeling and visualization.</abstract>
      <url hash="286ffc04">2024.lrec-main.653</url>
      <bibkey>kumar-le-2024-foto-targeted</bibkey>
    </paper>
    <paper id="654">
      <title><fixed-case>FRACAS</fixed-case>: a <fixed-case>FR</fixed-case>ench Annotated Corpus of Attribution relations in new<fixed-case>S</fixed-case></title>
      <author><first>Ange</first><last>Richard</last></author>
      <author><first>Laura Cristina</first><last>Alonzo Canul</last></author>
      <author><first>François</first><last>Portet</last></author>
      <pages>7417–7428</pages>
      <abstract>Quotation extraction is a widely useful task both from a sociological and from a Natural Language Processing perspective. However, very little data is available to study this task in languages other than English. In this paper, we present FRACAS, a manually annotated corpus of 1,676 newswire texts in French for quotation extraction and source attribution. We first describe the composition of our corpus and the choices that were made in selecting the data. We then detail the annotation guidelines, the annotation process and give relevant statistics about our corpus. We give results for the inter-annotator agreement, which is substantially high for such a difficult linguistic phenomenon. We use this new resource to test the ability of a neural state-of-the-art relation extraction system to extract quotes and their source and we compare this model to the latest available system for quotation extraction for the French language, which is rule-based. Experiments using our dataset on the state-of-the-art system show very promising results considering the difficulty of the task at hand.</abstract>
      <url hash="213dd24d">2024.lrec-main.654</url>
      <bibkey>richard-etal-2024-fracas-french</bibkey>
    </paper>
    <paper id="655">
      <title>Frame2: A <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et-based Multimodal Dataset for Tackling Text-image Interactions in Video</title>
      <author><first>Frederico</first><last>Belcavello</last></author>
      <author><first>Tiago</first><last>Timponi Torrent</last></author>
      <author><first>Ely E.</first><last>Matos</last></author>
      <author><first>Adriana S.</first><last>Pagano</last></author>
      <author><first>Maucha</first><last>Gamonal</last></author>
      <author><first>Natalia</first><last>Sigiliano</last></author>
      <author><first>Lívia Vicente</first><last>Dutra</last></author>
      <author><first>Helen</first><last>de Andrade Abreu</last></author>
      <author><first>Mairon</first><last>Samagaio</last></author>
      <author><first>Mariane</first><last>Carvalho</last></author>
      <author><first>Franciany</first><last>Campos</last></author>
      <author><first>Gabrielly</first><last>Azalim</last></author>
      <author><first>Bruna</first><last>Mazzei</last></author>
      <author><first>Mateus Fonseca</first><last>de Oliveira</last></author>
      <author><first>Ana Carolina</first><last>Loçasso Luz</last></author>
      <author><first>Lívia</first><last>Pádua Ruiz</last></author>
      <author><first>Júlia</first><last>Bellei</last></author>
      <author><first>Amanda</first><last>Pestana</last></author>
      <author><first>Josiane</first><last>Costa</last></author>
      <author><first>Iasmin</first><last>Rabelo</last></author>
      <author><first>Anna Beatriz</first><last>Silva</last></author>
      <author><first>Raquel</first><last>Roza</last></author>
      <author><first>Mariana</first><last>Souza</last></author>
      <author><first>Igor</first><last>Oliveira</last></author>
      <pages>7429–7437</pages>
      <abstract>This paper presents the Frame2 dataset, a multimodal dataset built from a corpus of a Brazilian travel TV show annotated for FrameNet categories for both the text and image communicative modes. Frame2 comprises 230 minutes of video, which are correlated with 2,915 sentences either transcribing the audio spoken during the episodes or the subtitling segments of the show where the host conducts interviews in English. For this first release of the dataset, a total of 11,796 annotation sets for the sentences and 6,841 for the video are included. Each of the former includes a target lexical unit evoking a frame or one or more frame elements. For each video annotation, a bounding box in the image is correlated with a frame, a frame element and lexical unit evoking a frame in FrameNet.</abstract>
      <url hash="2f64689e">2024.lrec-main.655</url>
      <bibkey>belcavello-etal-2024-frame2-framenet</bibkey>
    </paper>
    <paper id="656">
      <title>Framed <fixed-case>M</fixed-case>ulti30<fixed-case>K</fixed-case>: A Frame-Based Multimodal-Multilingual Dataset</title>
      <author><first>Marcelo</first><last>Viridiano</last></author>
      <author><first>Arthur</first><last>Lorenzi</last></author>
      <author><first>Tiago</first><last>Timponi Torrent</last></author>
      <author><first>Ely E.</first><last>Matos</last></author>
      <author><first>Adriana S.</first><last>Pagano</last></author>
      <author><first>Natália</first><last>Sathler Sigiliano</last></author>
      <author><first>Maucha</first><last>Gamonal</last></author>
      <author><first>Helen</first><last>de Andrade Abreu</last></author>
      <author><first>Lívia</first><last>Vicente Dutra</last></author>
      <author><first>Mairon</first><last>Samagaio</last></author>
      <author><first>Mariane</first><last>Carvalho</last></author>
      <author><first>Franciany</first><last>Campos</last></author>
      <author><first>Gabrielly</first><last>Azalim</last></author>
      <author><first>Bruna</first><last>Mazzei</last></author>
      <author><first>Mateus</first><last>Fonseca de Oliveira</last></author>
      <author><first>Ana Carolina</first><last>Luz</last></author>
      <author><first>Livia</first><last>Padua Ruiz</last></author>
      <author><first>Júlia</first><last>Bellei</last></author>
      <author><first>Amanda</first><last>Pestana</last></author>
      <author><first>Josiane</first><last>Costa</last></author>
      <author><first>Iasmin</first><last>Rabelo</last></author>
      <author><first>Anna Beatriz</first><last>Silva</last></author>
      <author><first>Raquel</first><last>Roza</last></author>
      <author><first>Mariana</first><last>Souza Mota</last></author>
      <author><first>Igor</first><last>Oliveira</last></author>
      <author><first>Márcio Henrique</first><last>Pelegrino de Freitas</last></author>
      <pages>7438–7449</pages>
      <abstract>This paper presents Framed Multi30K (FM30K), a novel frame-based Brazilian Portuguese multimodal-multilingual dataset which i) extends the Multi30K dataset (Elliot et al., 2016) with 158,915 original Brazilian Portuguese descriptions, and 30,104 Brazilian Portuguese translations from original English descriptions; and ii) adds 2,677,613 frame evocation labels to the 158,915 English descriptions and to the ones created for Brazilian Portuguese; (iii) extends the Flickr30k Entities dataset (Plummer et al., 2015) with 190,608 frames and Frame Elements correlations with the existing phrase-to-region correlations.</abstract>
      <url hash="d908f8fb">2024.lrec-main.656</url>
      <bibkey>viridiano-etal-2024-framed-multi30k</bibkey>
    </paper>
    <paper id="657">
      <title><fixed-case>FRASIMED</fixed-case>: A Clinical <fixed-case>F</fixed-case>rench Annotated Resource Produced through Crosslingual <fixed-case>BERT</fixed-case>-Based Annotation Projection</title>
      <author><first>Jamil</first><last>Zaghir</last></author>
      <author><first>Mina</first><last>Bjelogrlic</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Soukaïna</first><last>Aananou</last></author>
      <author><first>Christophe</first><last>Gaudet-Blavignac</last></author>
      <author><first>Christian</first><last>Lovis</last></author>
      <pages>7450–7460</pages>
      <abstract>Natural language processing (NLP) applications such as named entity recognition (NER) for low-resource corpora do not benefit from recent advances in the development of large language models (LLMs) where there is still a need for larger annotated datasets. This research article introduces a methodology for generating translated versions of annotated datasets through crosslingual annotation projection and is freely available on GitHub (link: https://github.com/JamilProg/crosslingual_bert_annotation_projection). Leveraging a language agnostic BERT-based approach, it is an efficient solution to increase low-resource corpora with few human efforts and by only using already available open data resources. Quantitative and qualitative evaluations are often lacking when it comes to evaluating the quality and effectiveness of semi-automatic data generation strategies. The evaluation of our crosslingual annotation projection approach showed both effectiveness and high accuracy in the resulting dataset. As a practical application of this methodology, we present the creation of French Annotated Resource with Semantic Information for Medical Entities Detection (FRASIMED), an annotated corpus comprising 2’051 synthetic clinical cases in French. The corpus is now available for researchers and practitioners to develop and refine French natural language processing (NLP) applications in the clinical field (https://zenodo.org/record/8355629), making it the largest open annotated corpus with linked medical concepts in French.</abstract>
      <url hash="efc3d117">2024.lrec-main.657</url>
      <bibkey>zaghir-etal-2024-frasimed-clinical</bibkey>
    </paper>
    <paper id="658">
      <title><fixed-case>FR</fixed-case>e<fixed-case>ND</fixed-case>: A <fixed-case>F</fixed-case>rench Resource of Negation Data</title>
      <author><first>Hafida</first><last>Le Cloirec - Ait Yahya</last></author>
      <author><first>Olga</first><last>Seminck</last></author>
      <author><first>Pascal</first><last>Amsili</last></author>
      <pages>7461–7468</pages>
      <abstract>FReND is a freely available corpus of French language in which negations are hand-annotated. Negations are annotated by their cues and scopes. Comprising 590K tokens and over 8.9K negations, it is the largest dataset available for French. A variety of types of textual genres are covered: literature, blog posts, Wikipedia articles, political debates, clinical reports and newspaper articles. As the understanding of negation is not yet mastered by current state of the art AI-models, FReND is not only a valuable resource for linguistic research into negation, but also as training data for AI tasks such as negation detection.</abstract>
      <url hash="ebfed15e">2024.lrec-main.658</url>
      <bibkey>le-cloirec-ait-yahya-etal-2024-frend-french</bibkey>
    </paper>
    <paper id="659">
      <title>From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction</title>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Qiangchao</first><last>Chen</last></author>
      <author><first>Yiquan</first><last>Wu</last></author>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Kun</first><last>Kuang</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Ming</first><last>Cai</last></author>
      <pages>7469–7479</pages>
      <abstract>Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge’s reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the model’s attention towards the differentiating information for each charge within the context, we expand the attention mechanism and introduce a new loss function with attention supervision through words in the word bag. We construct the confusing charges dataset from real-world judicial documents. Experiments demonstrate the effectiveness of our method, especially in maintaining exceptional performance in imbalanced label distributions.</abstract>
      <url hash="263095b6">2024.lrec-main.659</url>
      <attachment type="OptionalSupplementaryMaterial" hash="62f4e590">2024.lrec-main.659.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2024-graph-word</bibkey>
    </paper>
    <paper id="660">
      <title>From Laughter to Inequality: Annotated Dataset for Misogyny Detection in <fixed-case>T</fixed-case>amil and <fixed-case>M</fixed-case>alayalam Memes</title>
      <author><first>Rahul</first><last>Ponnusamy</last></author>
      <author><first>Kathiravan</first><last>Pannerselvam</last></author>
      <author><first>Saranya</first><last>R</last></author>
      <author><first>Prasanna Kumar</first><last>Kumaresan</last></author>
      <author><first>Sajeetha</first><last>Thavareesan</last></author>
      <author><first>Bhuvaneswari</first><last>S</last></author>
      <author><first>Anshid</first><last>K.a</last></author>
      <author><first>Susminu S</first><last>Kumar</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>7480–7488</pages>
      <abstract>In this digital era, memes have become a prevalent online expression, humor, sarcasm, and social commentary. However, beneath their surface lies concerning issues such as the propagation of misogyny, gender-based bias, and harmful stereotypes. To overcome these issues, we introduced MDMD (Misogyny Detection Meme Dataset) in this paper. This article focuses on creating an annotated dataset with detailed annotation guidelines to delve into online misogyny within the Tamil and Malayalam-speaking communities. Through analyzing memes, we uncover the intricate world of gender bias and stereotypes in these communities, shedding light on their manifestations and impact. This dataset, along with its comprehensive annotation guidelines, is a valuable resource for understanding the prevalence, origins, and manifestations of misogyny in various contexts, aiding researchers, policymakers, and organizations in developing effective strategies to combat gender-based discrimination and promote equality and inclusivity. It enables a deeper understanding of the issue and provides insights that can inform strategies for cultivating a more equitable and secure online environment. This work represents a crucial step in raising awareness and addressing gender-based discrimination in the digital space.</abstract>
      <url hash="de7727d9">2024.lrec-main.660</url>
      <bibkey>ponnusamy-etal-2024-laughter-inequality</bibkey>
    </paper>
    <paper id="661">
      <title>From Linguistic Linked Data to Big Data</title>
      <author><first>Dimitar</first><last>Trajanov</last></author>
      <author><first>Elena</first><last>Apostol</last></author>
      <author><first>Radovan</first><last>Garabik</last></author>
      <author><first>Katerina</first><last>Gkirtzou</last></author>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Cosimo</first><last>Palma</last></author>
      <author><first>Michael</first><last>Rosner</last></author>
      <author><first>Alexia</first><last>Sampri</last></author>
      <author><first>Gilles</first><last>Sérasset</last></author>
      <author><first>Blerina</first><last>Spahiu</last></author>
      <author><first>Ciprian-Octavian</first><last>Truică</last></author>
      <author><first>Giedre</first><last>Valunaite Oleskeviciene</last></author>
      <pages>7489–7502</pages>
      <abstract>With advances in the field of Linked (Open) Data (LOD), language data on the LOD cloud has grown in number, size, and variety. With an increased volume and variety of language data, optimizations of methods for distributing, storing, and querying these data become more central. To this end, this position paper investigates use cases at the intersection of LLOD and Big Data, existing approaches to utilizing Big Data techniques within the context of linked data, and discusses the challenges and benefits of this union.</abstract>
      <url hash="1a67b9d9">2024.lrec-main.661</url>
      <bibkey>trajanov-etal-2024-linguistic-linked</bibkey>
    </paper>
    <paper id="662">
      <title>From News to Summaries: Building a <fixed-case>H</fixed-case>ungarian Corpus for Extractive and Abstractive Summarization</title>
      <author><first>Botond</first><last>Barta</last></author>
      <author><first>Dorina</first><last>Lakatos</last></author>
      <author><first>Attila</first><last>Nagy</last></author>
      <author><first>Milán Konor</first><last>Nyist</last></author>
      <author><first>Judit</first><last>Ács</last></author>
      <pages>7503–7509</pages>
      <abstract>Training summarization models requires substantial amounts of training data. However for less resourceful languages like Hungarian, openly available models and datasets are notably scarce. To address this gap our paper introduces an open-source Hungarian corpus suitable for training abstractive and extractive summarization models. The dataset is assembled from segments of the Common Crawl corpus undergoing thorough cleaning, preprocessing and deduplication. In addition to abstractive summarization we generate sentence-level labels for extractive summarization using sentence similarity. We train baseline models for both extractive and abstractive summarization using the collected dataset. To demonstrate the effectiveness of the trained models, we perform both quantitative and qualitative evaluation. Our models and dataset will be made publicly available, encouraging replication, further research, and real-world applications across various domains.</abstract>
      <url hash="23cfa26b">2024.lrec-main.662</url>
      <bibkey>barta-etal-2024-news-summaries</bibkey>
    </paper>
    <paper id="663">
      <title>From Technology to Market. Bilingual Corpus on the Evaluation of Technology Opportunity Discovery</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Kazuyuki</first><last>Motohashi</last></author>
      <author><first>Chen</first><last>Zhu</last></author>
      <pages>7510–7520</pages>
      <abstract>As companies aim to enhance and expand their product portfolios, Technology Opportunity Discovery (TOD) has gained increasing interest. To comprehend the role of emerging technologies in innovation, we introduce a novel technology-market corpus in English and Japanese languages, and conduct a comprehensive empirical evaluation of the linkage between technology and the market. Our dataset comprises English patents extracted from the USPTO database and Japanese patents from the Japanese Patent Office (JPO), along with their associated products for each stock market company. We compare several static and contextualized word embedding methods to construct a technology-market space and propose an effective methodology based on a fine-tuned BERT model for linking technology to the market.</abstract>
      <url hash="500398c1">2024.lrec-main.663</url>
      <bibkey>hazem-etal-2024-technology-market</bibkey>
    </paper>
    <paper id="664">
      <title>From Text to Historical Ecological Knowledge: The Construction and Application of the <fixed-case>S</fixed-case>han Jing Knowledge Base</title>
      <author><first>Ke</first><last>Liang</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Xin-Lan</first><last>Jiang</last></author>
      <pages>7521–7530</pages>
      <abstract>Traditional Ecological Knowledge (TEK) has been recognized as a shared cultural heritage and a crucial instrument to tackle today’s environmental challenges. In this paper, we deal with historical ecological knowledge, a special type of TEK that is based on ancient language texts. In particular, we aim to build a language resource based on Shanhai Jing (The Classic of Mountains and Seas). Written 2000 years ago, Shanhai Jing is a record of flora and fauna in ancient China, anchored by mountains (shan) and seas (hai). This study focuses on the entities in the Shan Jing part and builds a knowledge base for them. We adopt a pattern-driven and bottom-up strategy to accommodate two features of the source: highly stylized narrative and juxtaposition of knowledge from multiple domains. The PRF values of both entity and relationship extraction are above 96%. Quality assurance measures like entity disambiguation and resolution were done by domain experts. Neo4j graph database is used to visualize the result. We think the knowledge base, containing 1432 systematically classified entities and 3294 relationships, can provide the foundation for the construction of a historical ecological knowledge base of China. Additionally, the ruled-based text-matching method can be helpful in ancient language processing.</abstract>
      <url hash="60575580">2024.lrec-main.664</url>
      <bibkey>liang-etal-2024-text-historical</bibkey>
    </paper>
    <paper id="665">
      <title>From Text to Source: Results in Detecting Large Language Model-Generated Content</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>7531–7543</pages>
      <abstract>The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates “Cross-Model Detection,” by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.</abstract>
      <url hash="d4c0120f">2024.lrec-main.665</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5ef42cf0">2024.lrec-main.665.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>antoun-etal-2024-text-source</bibkey>
    </paper>
    <paper id="666">
      <title><fixed-case>FUSE</fixed-case> - <fixed-case>F</fixed-case>r<fixed-case>U</fixed-case>stration and Surprise Expressions: A Subtle Emotional Multimodal Language Corpus</title>
      <author><first>Rajesh</first><last>Titung</last></author>
      <author><first>Cecilia Ovesdotter</first><last>Alm</last></author>
      <pages>7544–7555</pages>
      <abstract>This study introduces a novel multimodal corpus for expressive task-based spoken language and dialogue, focused on language use under frustration and surprise, elicited from three tasks motivated by prior research and collected in an IRB-approved experiment. The resource is unique both because these are understudied affect states for emotion modeling in language, and also because it provides both individual and dyadic multimodally grounded language. The study includes a detailed analysis of annotations and performance results for multimodal emotion inference in language use.</abstract>
      <url hash="d18cf775">2024.lrec-main.666</url>
      <bibkey>titung-alm-2024-fuse-frustration</bibkey>
    </paper>
    <paper id="667">
      <title>Fusion-in-T5: Unifying Variant Signals for Simple and Effective Document Ranking with Attention Fusion</title>
      <author><first>Shi</first><last>Yu</last></author>
      <author><first>Chenghao</first><last>Fan</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>David</first><last>Jin</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Zhenghao</first><last>Liu</last></author>
      <pages>7556–7561</pages>
      <abstract>Common document ranking pipelines in search systems are cascade systems that involve multiple ranking layers to integrate different information step-by-step. In this paper, we propose a novel re-ranker Fusion-in-T5 (FiT5), which integrates text matching information, ranking features, and global document information into one single unified model via templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5, as one single model, significantly improves ranking performance over complex cascade pipelines. Analysis finds that through attention fusion, FiT5 jointly utilizes various forms of ranking information via gradually attending to related documents and ranking features, and improves the detection of subtle nuances. Our code is open-sourced at https://github.com/OpenMatch/FiT5 . Keywords: document ranking, attention, fusion</abstract>
      <url hash="9c59c918">2024.lrec-main.667</url>
      <bibkey>yu-etal-2024-fusion-t5</bibkey>
    </paper>
    <paper id="668">
      <title><fixed-case>GAATME</fixed-case>: A Genetic Algorithm for Adversarial Translation Metrics Evaluation</title>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>7562–7569</pages>
      <abstract>Building on a recent method for decoding translation candidates from a Machine Translation (MT) model via a genetic algorithm, we modify it to generate adversarial translations to test and challenge MT evaluation metrics. The produced translations score very well in an arbitrary MT evaluation metric selected beforehand, despite containing serious, deliberately introduced errors. The method can be used to create adversarial test sets to analyze the biases and shortcomings of the metrics. We publish various such test sets for the Czech to English language pair, as well as the code to convert any parallel data into a similar adversarial test set.</abstract>
      <url hash="66215a12">2024.lrec-main.668</url>
      <bibkey>jon-bojar-2024-gaatme-genetic</bibkey>
    </paper>
    <paper id="669">
      <title><fixed-case>GCN</fixed-case>et: Global-and-Context Collaborative Learning for Aspect-Based Sentiment Analysis</title>
      <author><first>Ting</first><last>Zhou</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <pages>7570–7580</pages>
      <abstract>Aspect-Based Sentiment Analysis (ABSA) aims to determine the sentiment polarities of specified aspect terms in a sentence. Most previous approaches mainly use an attention mechanism or graph neural networks based on dependency trees to explicitly model the connections between aspect terms and opinion words. However, these methods may not effectively address cases where the sentiment of an aspect term is implicitly described, as the corresponding opinion words may not directly appear in the sentence. To alleviate this issue, in this paper, we propose a GCNet that explicitly leverages global semantic information to guide context encoding. Particularly, we design a semantics encoding module that incorporates global semantic features into sequential modeling process to enable the consideration of the overall sentiment tendency of a sentence, while the global semantic features are also refined by adaptively focusing on different parts of the sentence. Moreover, for a comprehensive sentence analysis, we also include a syntactic feature encoding module along with a pre-fusion module to integrate the refined global features with the syntactic representations. Extensive experiments on three public datasets demonstrate that our model outperforms state-of-the-art methods, indicating the robustness and effectiveness of our approach.</abstract>
      <url hash="11f59c81">2024.lrec-main.669</url>
      <bibkey>zhou-etal-2024-gcnet-global</bibkey>
    </paper>
    <paper id="670">
      <title><fixed-case>GECS</fixed-case>um: Generative Evaluation-Driven Sequence Level Contrastive Learning for Abstractive Summarization</title>
      <author><first>Jiawen</first><last>Xie</last></author>
      <author><first>Shaoting</first><last>Zhang</last></author>
      <author><first>Xiaofan</first><last>Zhang</last></author>
      <pages>7581–7595</pages>
      <abstract>While dominant in abstractive summarization, transformer-based language models with the standard maximum likelihood estimation (MLE) training remain challenged by two discrepancies: the misalignment between token-level training and sequence-level evaluation, and the divergence between teacher-forcing training manner and auto-regressive generation behavior. Recent studies have shown that sequence-level contrastive learning, which utilizes the quality differences between multiple summaries as prior information, can effectively mitigate these issues. However, as certain evaluation metrics often determine the contrastive signals in existing methods, this leads to the model performance aligning with the preferences of these metrics being limited by the evaluation capabilities of these metrics. Inspired by prior works that treat the evaluation of generated text as a text generation problem, we propose a generative evaluation-driven contrastive learning framework, which leverages the semantic understanding capabilities of the abstractive model itself to evaluate summary in reference-based settings. In this way, our method establishes a connection between the model’s reference-based evaluation and reference-free generation scenarios, allowing them to share the benefits of model capability enhancements. Extensive experiments on four summarization datasets demonstrate that our method outperforms the previous state-of-the-art regarding comprehensive performance. Various empirical analyses further substantiate the effectiveness of our method.</abstract>
      <url hash="d4e34dde">2024.lrec-main.670</url>
      <bibkey>xie-etal-2024-gecsum-generative</bibkey>
    </paper>
    <paper id="671">
      <title>Gendered Grammar or Ingrained Bias? Exploring Gender Bias in <fixed-case>I</fixed-case>celandic Language Models</title>
      <author><first>Steinunn Rut</first><last>Friðriksdóttir</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>7596–7610</pages>
      <abstract>Large language models, trained on vast datasets, exhibit increased output quality in proportion to the amount of data that is used to train them. This data-driven learning process has brought forth a pressing issue where these models may not only reflect but also amplify gender bias, racism, religious prejudice, and queerphobia present in their training data that may not always be recent. This study explores gender bias in language models trained on Icelandic, focusing on occupation-related terms. Icelandic is a highly grammatically gendered language that favors the masculine when referring to groups of people with indeterminable genders. Our aim is to explore whether language models merely mirror gender distributions within the corresponding professions or if they exhibit biases tied to their grammatical genders. Results indicate a significant overall predisposition towards the masculine but specific occupation terms consistently lean toward a particular gender, indicating complex interplays of societal and linguistic influences.</abstract>
      <url hash="033e1385">2024.lrec-main.671</url>
      <bibkey>fridriksdottir-einarsson-2024-gendered-grammar</bibkey>
    </paper>
    <paper id="672">
      <title>Generating Clarification Questions for Disambiguating Contracts</title>
      <author><first>Anmol</first><last>Singhal</last></author>
      <author><first>Chirag</first><last>Jain</last></author>
      <author><first>Preethu Rose</first><last>Anish</last></author>
      <author><first>Arkajyoti</first><last>Chakraborty</last></author>
      <author><first>Smita</first><last>Ghaisas</last></author>
      <pages>7611–7622</pages>
      <abstract>Enterprises frequently enter into commercial contracts that can serve as vital sources of project-specific requirements. Contractual clauses are obligatory, and the requirements derived from contracts can detail the downstream implementation activities that non-legal stakeholders, including requirement analysts, engineers, and delivery personnel, need to conduct. However, comprehending contracts is cognitively demanding and error-prone for such stakeholders due to the extensive use of Legalese and the inherent complexity of contract language. Furthermore, contracts often contain ambiguously worded clauses to ensure comprehensive coverage. In contrast, non-legal stakeholders require a detailed and unambiguous comprehension of contractual clauses to craft actionable requirements. In this work, we introduce a novel legal NLP task that involves generating clarification questions for contracts. These questions aim to identify contract ambiguities on a document level, thereby assisting non-legal stakeholders in obtaining the necessary details for eliciting requirements. This task is challenged by three core issues: (1) data availability, (2) the length and unstructured nature of contracts, and (3) the complexity of legal text. To address these issues, we propose ConRAP, a retrieval-augmented prompting framework for generating clarification questions to disambiguate contractual text. Experiments conducted on contracts sourced from the publicly available CUAD dataset show that ConRAP with ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the generated clarification questions are deemed useful by human evaluators.</abstract>
      <url hash="9bd52f98">2024.lrec-main.672</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7dd14288">2024.lrec-main.672.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>singhal-etal-2024-generating-clarification</bibkey>
    </paper>
    <paper id="673">
      <title>Generating Contextual Images for Long-Form Text</title>
      <author><first>Avijit</first><last>Mitra</last></author>
      <author><first>Nalin</first><last>Gupta</last></author>
      <author><first>Chetan</first><last>Naik</last></author>
      <author><first>Abhinav</first><last>Sethy</last></author>
      <author><first>Kinsey</first><last>Bice</last></author>
      <author><first>Zeynab</first><last>Raeesy</last></author>
      <pages>7623–7633</pages>
      <abstract>We investigate the problem of synthesizing relevant visual imagery from generic long-form text, leveraging Large Language Models (LLMs) and Text-to-Image Models (TIMs). Current Text-to-Image models require short prompts that describe the image content and style explicitly. Unlike image prompts, generation of images from general long-form text requires the image synthesis system to derive the visual content and style elements from the text. In this paper, we study zero-shot prompting and supervised fine-tuning approaches that use LLMs and TIMs jointly for synthesizing images. We present an empirical study on generating images for Wikipedia articles covering a broad spectrum of topic and image styles. We compare these systems using a suite of metrics, including a novel metric specifically designed to evaluate the semantic correctness of generated images. Our study offers a preliminary understanding of existing models’ strengths and limitation for the task of image generation from long-form text, and sets up an evaluation framework and establishes baselines for future research.</abstract>
      <url hash="dc11df0d">2024.lrec-main.673</url>
      <bibkey>mitra-etal-2024-generating-contextual</bibkey>
    </paper>
    <paper id="674">
      <title>Generating Hard-Negative Out-of-Scope Data with <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for Intent Classification</title>
      <author><first>Zhijian</first><last>Li</last></author>
      <author><first>Stefan</first><last>Larson</last></author>
      <author><first>Kevin</first><last>Leach</last></author>
      <pages>7634–7646</pages>
      <abstract>Intent classifiers must be able to distinguish when a user’s utterance does not belong to any supported intent to avoid producing incorrect and unrelated system responses. Although out-of-scope (OOS) detection for intent classifiers has been studied, previous work has not yet studied changes in classifier performance against hard-negative out-of-scope utterances (i.e., inputs that share common features with in-scope data, but are actually out-of-scope). We present an automated technique to generate hard-negative OOS data using ChatGPT. We use our technique to build five new hard-negative OOS datasets, and evaluate each against three benchmark intent classifiers. We show that classifiers struggle to correctly identify hard-negative OOS utterances more than general OOS utterances. Finally, we show that incorporating hard-negative OOS data for training improves model robustness when detecting hard-negative OOS data and general OOS data. Our technique, datasets, and evaluation address an important void in the field, offering a straightforward and inexpensive way to collect hard-negative OOS data and improve intent classifiers’ robustness.</abstract>
      <url hash="f0cfa37b">2024.lrec-main.674</url>
      <bibkey>li-etal-2024-generating-hard</bibkey>
    </paper>
    <paper id="675">
      <title>Generating Multiple-choice Questions for Medical Question Answering with Distractors and Cue-masking</title>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Kanimozhi</first><last>Uma</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>7647–7653</pages>
      <abstract>Medical multiple-choice question answering (MCQA) is a challenging evaluation for medical natural language processing and a helpful task in itself. Medical questions may describe patient symptoms and ask for the correct diagnosis, which requires domain knowledge and complex reasoning. Standard language modeling pretraining alone is not sufficient to achieve the best results with BERT-base size (Devlin et al., 2019) encoders. Jin et al. (2020) showed that focusing masked language modeling on disease name prediction when using medical encyclopedic paragraphs as input leads to considerable MCQA accuracy improvement. In this work, we show that (1) fine-tuning on generated MCQA dataset outperforms the masked language modeling based objective and (2) correctly masking the cues to the answers is critical for good performance. We release new pretraining datasets and achieve state-of-the-art results on 4 MCQA datasets, notably +5.7% with base-size model on MedQA-USMLE.</abstract>
      <url hash="d7385864">2024.lrec-main.675</url>
      <bibkey>sileo-etal-2024-generating-multiple</bibkey>
    </paper>
    <paper id="676">
      <title>Generative Multimodal Entity Linking</title>
      <author><first>Senbao</first><last>Shi</last></author>
      <author><first>Zhenran</first><last>Xu</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>7654–7665</pages>
      <abstract>Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base. Existing MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a Generative Multimodal Entity Linking framework based on LLMs, which directly generates target entity names. We keep the vision and language model frozen and only train a feature mapper to enable cross-modality interactions. To adapt LLMs to the MEL task, we leverage the in-context learning capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that, with only ∼0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art results on two well-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8% accuracy gains on WikiMEL). The performance gain stems from mitigating the popularity bias of LLM predictions and disambiguating less common entities effectively. Further analysis verifies the generality and scalability of GEMEL. Our framework is compatible with any off-the-shelf language model, paving the way towards an efficient and general solution for utilizing LLMs in the MEL task. Our code is available at https://github.com/HITsz-TMG/GEMEL.</abstract>
      <url hash="3a50ce61">2024.lrec-main.676</url>
      <bibkey>shi-etal-2024-generative-multimodal</bibkey>
    </paper>
    <paper id="677">
      <title><fixed-case>GENTRAC</fixed-case>: A Tool for Tracing Trauma in Genocide and Mass Atrocity Court Transcripts</title>
      <author><first>Miriam</first><last>Schirmer</last></author>
      <author><first>Christian</first><last>Brechenmacher</last></author>
      <author><first>Endrit</first><last>Jashari</last></author>
      <author><first>Juergen</first><last>Pfeffer</last></author>
      <pages>7666–7671</pages>
      <abstract>This paper introduces GENTRAC, an open-access web-based tool built to interactively detect and analyze potentially traumatic content in witness statements of genocide and mass atrocity trials. Harnessing recent developments in natural language processing (NLP) to detect trauma, GENTRAC processes and formats court transcripts for NLP analysis through a sophisticated parsing algorithm and detects the likelihood of traumatic content for each speaker segment. The tool visualizes the density of such content throughout a trial day and provides statistics on the overall amount of traumatic content and speaker distribution. Capable of processing transcripts from four prominent international criminal courts, including the International Criminal Court (ICC), GENTRAC’s reach is vast, tailored to handle millions of pages of documents from past and future trials. Detecting potentially re-traumatizing examination methods can enhance the development of trauma-informed legal procedures. GENTRAC also serves as a reliable resource for legal, human rights, and other professionals, aiding their comprehension of mass atrocities’ emotional toll on survivors.</abstract>
      <url hash="59d650a5">2024.lrec-main.677</url>
      <bibkey>schirmer-etal-2024-gentrac-tool</bibkey>
    </paper>
    <paper id="678">
      <title>Geographically-Informed Language Identification</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <author><first>Lane</first><last>Edwards-Brown</last></author>
      <pages>7672–7682</pages>
      <abstract>This paper develops an approach to language identification in which the set of languages considered by the model depends on the geographic origin of the text in question. Given that many digital corpora can be geo-referenced at the country level, this paper formulates 16 region-specific models, each of which contains the languages expected to appear in countries within that region. These regional models also each include 31 widely-spoken international languages in order to ensure coverage of these linguae francae regardless of location. An upstream evaluation using traditional language identification testing data shows an improvement in f-score ranging from 1.7 points (Southeast Asia) to as much as 10.4 points (North Africa). A downstream evaluation on social media data shows that this improved performance has a significant impact on the language labels which are applied to large real-world corpora. The result is a highly-accurate model that covers 916 languages at a sample size of 50 characters, the performance improved by incorporating geographic information into the model.</abstract>
      <url hash="20da5710">2024.lrec-main.678</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1acd38a3">2024.lrec-main.678.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>dunn-edwards-brown-2024-geographically-informed</bibkey>
    </paper>
    <paper id="679">
      <title><fixed-case>G</fixed-case>er<fixed-case>DISDETECT</fixed-case>: A <fixed-case>G</fixed-case>erman Multilabel Dataset for Disinformation Detection</title>
      <author><first>Mina</first><last>Schütz</last></author>
      <author><first>Daniela</first><last>Pisoiu</last></author>
      <author><first>Daria</first><last>Liakhovets</last></author>
      <author><first>Alexander</first><last>Schindler</last></author>
      <author><first>Melanie</first><last>Siegel</last></author>
      <pages>7683–7695</pages>
      <abstract>Disinformation has become increasingly relevant in recent years both as a political issue and as object of research. Datasets for training machine learning models, especially for other languages than English, are sparse and the creation costly. Annotated datasets often have only binary or multiclass labels, which provide little information about the grounds and system of such classifications. We propose a novel textual dataset GerDISDETECT for German disinformation. To provide comprehensive analytical insights, a fine-grained taxonomy guided annotation scheme is required. The goal of this dataset, instead of providing a direct assessment regarding true or false, is to provide wide-ranging semantic descriptors that allow for complex interpretation as well as inferred decision-making regarding information and trustworthiness of potentially critical articles. This allows this dataset to be also used for other tasks. The dataset was collected in the first three months of 2022 and contains 39 multilabel classes with 5 top-level categories for a total of 1,890 articles: General View (3 labels), Offensive Language (11 labels), Reporting Style (15 labels), Writing Style (6 labels), and Extremism (4 labels). As a baseline, we further pre-trained a multilingual XLM-R model on around 200,000 unlabeled news articles and fine-tuned it for each category.</abstract>
      <url hash="1933966d">2024.lrec-main.679</url>
      <bibkey>schutz-etal-2024-gerdisdetect-german</bibkey>
    </paper>
    <paper id="680">
      <title><fixed-case>G</fixed-case>erman Also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset</title>
      <author><first>Laura</first><last>Mascarell</last></author>
      <author><first>Ribin</first><last>Chalumattu</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <pages>7696–7706</pages>
      <abstract>The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents Absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and release the Absinth dataset to foster further research on hallucination detection in German.</abstract>
      <url hash="99f56634">2024.lrec-main.680</url>
      <bibkey>mascarell-etal-2024-german-also</bibkey>
    </paper>
    <paper id="681">
      <title><fixed-case>G</fixed-case>erman Parliamentary Corpus (<fixed-case>G</fixed-case>er<fixed-case>P</fixed-case>ar<fixed-case>C</fixed-case>or) Reloaded</title>
      <author><first>Giuseppe</first><last>Abrami</last></author>
      <author><first>Mevlüt</first><last>Bagci</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>7707–7716</pages>
      <abstract>In 2022, the largest German-speaking corpus of parliamentary protocols from three different centuries, on a national and federal level from the countries of Germany, Austria, Switzerland and Liechtenstein, was collected and published - GerParCor. Through GerParCor, it became possible to provide for the first time various parliamentary protocols which were not available digitally and, moreover, could not be retrieved and processed in a uniform manner. Furthermore, GerParCor was additionally preprocessed using NLP methods and made available in XMI format. In this paper, GerParCor is significantly updated by including all new parliamentary protocols in the corpus, as well as adding and preprocessing further parliamentary protocols previously not covered, so that a period up to 1797 is now covered. Besides the integration of a new, state-of-the-art and appropriate NLP preprocessing for the handling of large text corpora, this update also provides an overview of the further reuse of GerParCor by presenting various provisioning capabilities such as API’s, among others.</abstract>
      <url hash="b46cb7ca">2024.lrec-main.681</url>
      <bibkey>abrami-etal-2024-german-parliamentary</bibkey>
    </paper>
    <paper id="682">
      <title><fixed-case>G</fixed-case>erman <fixed-case>SRL</fixed-case>: Corpus Construction and Model Training</title>
      <author><first>Maxim</first><last>Konca</last></author>
      <author><first>Andy</first><last>Luecking</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>7717–7727</pages>
      <abstract>A useful semantic role-annotated resource for training semantic role models for the German language is missing. We point out some problems of previous resources and provide a new one due to a combined translation and alignment process: The gold standard CoNLL-2012 semantic role annotations are translated into German. Semantic role labels are transferred due to alignment models. The resulting dataset is used to train a German semantic role model. With F1-scores around 0.7, the major roles achieve competitive evaluation scores, but avoid limitations of previous approaches. The described procedure can be applied to other languages as well.</abstract>
      <url hash="92bd6c82">2024.lrec-main.682</url>
      <bibkey>konca-etal-2024-german-srl</bibkey>
    </paper>
    <paper id="683">
      <title><fixed-case>GERMS</fixed-case>-<fixed-case>AT</fixed-case>: A Sexism/Misogyny Dataset of Forum Comments from an <fixed-case>A</fixed-case>ustrian Online Newspaper</title>
      <author><first>Brigitte</first><last>Krenn</last></author>
      <author><first>Johann</first><last>Petrak</last></author>
      <author><first>Marina</first><last>Kubina</last></author>
      <author><first>Christian</first><last>Burger</last></author>
      <pages>7728–7739</pages>
      <abstract>Brigitte Krenn, Johann Petrak, Marina Kubina, Christian Burger This paper presents a sexism/misogyny dataset extracted from comments of a large online forum of an Austrian newspaper. The comments are in Austrian German language, and in some cases interspersed with dialectal or English elements. We describe the data collection, the annotation guidelines and the annotation process resulting in a corpus of approximately 8 000 comments which were annotated with 5 levels of sexism/misogyny, ranging from 0 (not sexist/misogynist) to 4 (highly sexist/misogynist). The professional forum moderators (self-identified females and males) of the online newspaper were involved as experts in the creation of the annotation guidelines and the annotation of the user comments. In addition, we also describe first results of training transformer-based classification models for both binarized and original label classification of the corpus.</abstract>
      <url hash="0cb1f57c">2024.lrec-main.683</url>
      <bibkey>krenn-etal-2024-germs-sexism</bibkey>
    </paper>
    <paper id="684">
      <title><fixed-case>GIL</fixed-case>-<fixed-case>GAL</fixed-case>a<fixed-case>D</fixed-case>: Gender Inclusive Language - <fixed-case>G</fixed-case>erman Auto-Assembled Large Database</title>
      <author><first>Anna-Katharina</first><last>Dick</last></author>
      <author><first>Matthias</first><last>Drews</last></author>
      <author><first>Valentin</first><last>Pickard</last></author>
      <author><first>Victoria</first><last>Pierz</last></author>
      <pages>7740–7745</pages>
      <abstract>As the need for gender-inclusive language has become a highly debated topic over the years, gendered biases in speech are unfortunately often picked up and propagated by modern language models trained on large amounts of text. While remedial efforts are underway, grammatically gendered languages such as German pose some unique challenges in generating gender-inclusive language for corrective model training or fine-tuning. We assembled GIL-GALaD, a corpus of German gender-inclusive language from different sources such as social media, news articles, public speeches and academic publications. Our corpus includes the most common types of modifications of generic masculine forms of nouns and spans 30 years (1993-2023), containing over 800,000 instances of gender-inclusive language. Tools for corpus usage and extension are to be included in the release. During corpus assembly, we were also able to gain some insights into which types of gender-inclusive language were used in practice throughout the years and across different domains.</abstract>
      <url hash="ff1c36ef">2024.lrec-main.684</url>
      <bibkey>dick-etal-2024-gil-galad</bibkey>
    </paper>
    <paper id="685">
      <title><fixed-case>GLAMR</fixed-case>: Augmenting <fixed-case>AMR</fixed-case> with <fixed-case>GL</fixed-case>-<fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et Event Structure</title>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Timothy</first><last>Obiso</last></author>
      <author><first>Bingyang</first><last>Ye</last></author>
      <author><first>Kyeongmin</first><last>Rim</last></author>
      <author><first>Keer</first><last>Xu</last></author>
      <author><first>Liulu</first><last>Yue</last></author>
      <author><first>Susan Windisch</first><last>Brown</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>7746–7759</pages>
      <abstract>This paper introduces GLAMR, an Abstract Meaning Representation (AMR) interpretation of Generative Lexicon (GL) semantic components. It includes a structured subeventual interpretation of linguistic predicates, and encoding of the opposition structure of property changes of event arguments. Both of these features are recently encoded in VerbNet (VN), and form the scaffolding for the semantic form associated with VN frame files. We develop a new syntax, concepts, and roles for subevent structure based on VN for connecting subevents to atomic predicates. Our proposed extension is compatible with current AMR specification. We also present an approach to automatically augment AMR graphs by inserting subevent structure of the predicates and identifying the subevent arguments from the semantic roles. A pilot annotation of GLAMR graphs of 65 documents (486 sentences), based on procedural texts as a source, is presented as a public dataset. The annotation includes subevents, argument property change, and document-level anaphoric links. Finally, we provide baseline models for converting text to GLAMR and vice versa, along with the application of GLAMR for generating enriched paraphrases with details on subevent transformation and arguments that are not present in the surface form of the texts.</abstract>
      <url hash="0046bbdb">2024.lrec-main.685</url>
      <bibkey>tu-etal-2024-glamr-augmenting</bibkey>
    </paper>
    <paper id="686">
      <title>Global and Local Hierarchical Prompt Tuning Framework for Multi-level Implicit Discourse Relation Recognition</title>
      <author><first>Lei</first><last>Zeng</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Haowen</first><last>Sun</last></author>
      <author><first>Jing</first><last>Xu</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <pages>7760–7773</pages>
      <abstract>Multi-level implicit discourse relation recognition (MIDRR) is a challenging task to recognize the hierarchical discourse relations between the arguments with the absence of connectives. Recent methods tend to incorporate the static hierarchical structure containing all senses (defined as global hierarchy) into prompt tuning through a path prompt template or hierarchical label refining. Howerver, hierarchical modeling is independent of the verbalizer, resulting in a failure to effectively utilize the output probability distribution information of verbalizer. Besides, they ignore the utilization of the dynamic hierarchical label sequence for each instance (defined as local hierarchy) in prompt tuning. In this paper, we propose a global and local hierarchical prompt tuning (GLHPT) framework, which utilize prior knowledge of PLMs while better incorporating hierarchical information from two aspects. We leverage bottom-up propagated probability as the global hierarchy to inject it into multi-level verbalizer (MLV). Furthermore, we design a local hierarchy-driven contrastive learning (LHCL) to improve the probability distribution of MLV. Finally, our model achieves competitive results on two benchmacks.</abstract>
      <url hash="bade3328">2024.lrec-main.686</url>
      <bibkey>zeng-etal-2024-global-local</bibkey>
    </paper>
    <paper id="687">
      <title><fixed-case>G</fixed-case>lot<fixed-case>S</fixed-case>cript: A Resource and Tool for Low Resource Writing System Identification</title>
      <author><first>Amir Hossein</first><last>Kargaran</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>7774–7784</pages>
      <abstract>We present GlotScript, an open resource and tool for low resource writing system identification. GlotScript-R is a resource that provides the attested writing systems for more than 7,000 languages. It is compiled by aggregating information from existing writing system resources. GlotScript-T is a writing system identification tool that covers all 161 Unicode 15.0 scripts. For an input text, it returns its script distribution where scripts are identified by ISO 15924 codes. We also present two use cases for GlotScript. First, we demonstrate that GlotScript can help cleaning multilingual corpora such as mC4 and OSCAR. Second, we analyze the tokenization of a number of language models such as GPT-4 using GlotScript and provide insights on the coverage of low resource scripts and languages by each language model. We hope that GlotScript will become a useful resource for work on low resource languages in the NLP community. GlotScript-R and GlotScript-T are available at https://github.com/cisnlp/GlotScript.</abstract>
      <url hash="52072f1c">2024.lrec-main.687</url>
      <bibkey>kargaran-etal-2024-glotscript-resource</bibkey>
    </paper>
    <paper id="688">
      <title><fixed-case>GMEG</fixed-case>-<fixed-case>EXP</fixed-case>: A Dataset of Human- and <fixed-case>LLM</fixed-case>-Generated Explanations of Grammatical and Fluency Edits</title>
      <author><first>S. Magalí</first><last>López Cortez</last></author>
      <author><first>Mark Josef</first><last>Norris</last></author>
      <author><first>Steve</first><last>Duman</last></author>
      <pages>7785–7800</pages>
      <abstract>Recent work has explored the ability of large language models (LLMs) to generate explanations of existing labeled data. In this work, we investigate the ability of LLMs to explain revisions in sentences. We introduce a new dataset demonstrating a novel task, which we call explaining text revisions. We collected human- and LLM-generated explanations of grammatical and fluency edits and defined criteria for the human evaluation of the explanations along three dimensions: Coverage, Informativeness, and Correctness. The results of a side-by-side evaluation show an Overall preference for human explanations, but there are many instances in which annotators show no preference. Annotators prefer human-generated explanations for Informativeness and Correctness, but they show no preference for Coverage. We also examined the extent to which the number of revisions in a sentence influences annotators’ Overall preference for the explanations. We found that the preference for human explanations increases as the number of revisions in the sentence increases. Additionally, we show that the Overall preference for human explanations depends on the type of error being explained. We discuss explanation styles based on a qualitative analysis of 300 explanations. We release our dataset and annotation guidelines to encourage future research.</abstract>
      <url hash="5108b30b">2024.lrec-main.688</url>
      <bibkey>lopez-cortez-etal-2024-gmeg-exp</bibkey>
    </paper>
    <paper id="689">
      <title><fixed-case>GOLEM</fixed-case>: <fixed-case>GO</fixed-case>ld Standard for Learning and Evaluation of Motifs</title>
      <author><first>W. Victor</first><last>Yarlott</last></author>
      <author><first>Anurag</first><last>Acharya</last></author>
      <author><first>Diego</first><last>Castro Estrada</last></author>
      <author><first>Diana</first><last>Gomez</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>7801–7813</pages>
      <abstract>Motifs are distinctive, recurring, widely used idiom-like words or phrases, often originating from folklore, whose meaning are anchored in a narrative. Motifs have significance as communicative devices because they concisely imply a constellation of culturally relevant information. Their broad usage suggests their cognitive importance as touchstones of cultural knowledge. We present GOLEM, the first dataset annotated for motific information. The dataset comprises 7,955 English articles (2,039,424 words). The corpus identifies 26,078 motif candidates across 34 motif types from three cultural or national groups: Jewish, Irish, and Puerto Rican. Each motif candidate is labeled with the type of usage (Motific, Referential, Eponymic, or Unrelated), resulting in 1,723 actual motific instances. Annotation was performed by individuals identifying as members of each group and achieved a Fleiss’ kappa of &gt;0.55. We demonstrate that classification of candidate type is a challenging task for LLMs using a few-shot approach; recent models such as T5, FLAN-T5, GPT-2, and Llama 2 (7B) achieved a performance of 41% accuracy at best. These data will support development of new models and approaches for detecting (and reasoning about) motific information in text. We release the corpus, the annotation guide, and the code to support other researchers building on this work.</abstract>
      <url hash="25e725a1">2024.lrec-main.689</url>
      <bibkey>yarlott-etal-2024-golem-gold</bibkey>
    </paper>
    <paper id="690">
      <title>Good or Bad News? Exploring <fixed-case>GPT</fixed-case>-4 for Sentiment Analysis for <fixed-case>F</fixed-case>aroese on a Public News Corpora</title>
      <author><first>Iben Nyholm</first><last>Debess</last></author>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>7814–7824</pages>
      <abstract>Sentiment analysis in low-resource languages presents unique challenges that Large Language Models may help address. This study explores the efficacy of GPT-4 for sentiment analysis on Faroese news texts, an uncharted task for this language. On the basis of guidelines presented, the sentiment analysis was performed with a multi-class approach at the sentence and document level with 225 sentences analysed in 170 articles. When comparing GPT-4 to human annotators, we observe that GPT-4 performs remarkably well. We explored two prompt configurations and observed a benefit from having clear instructions for the sentiment analysis task, but no benefit from translating the articles to English before the sentiment analysis task. Our results indicate that GPT-4 can be considered as a valuable tool for generating Faroese test data. Furthermore, our investigation reveals the intricacy of news sentiment. This motivates a more nuanced approach going forward, and we suggest a multi-label approach for future research in this domain. We further explored the efficacy of GPT-4 in topic classification on news texts and observed more negative sentiments expressed in international than national news. Overall, this work demonstrates GPT-4’s proficiency on a novel task and its utility for augmenting resources in low-data languages.</abstract>
      <url hash="7cbb28b0">2024.lrec-main.690</url>
      <bibkey>debess-etal-2024-good-bad</bibkey>
    </paper>
    <paper id="691">
      <title>Gos 2: A New Reference Corpus of Spoken <fixed-case>S</fixed-case>lovenian</title>
      <author><first>Darinka</first><last>Verdonik</last></author>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>7825–7830</pages>
      <abstract>This paper introduces a new version of the Gos reference corpus of spoken Slovenian, which was recently extended to more than double the original size (300 hours, 2.4 million words) by adding speech recordings and transcriptions from two related initiatives, the Gos VideoLectures corpus of public academic speech, and the Artur speech recognition database. We describe this process by first presenting the criteria guiding the balanced selection of the newly added data and the challenges encountered when merging language resources with divergent designs, followed by the presentation of other major enhancements of the new Gos corpus, such as improvements in lemmatization and morphosyntactic annotation, word-level speech alignment, a new XML schema and the development of a specialized online concordancer.</abstract>
      <url hash="91350a93">2024.lrec-main.691</url>
      <bibkey>verdonik-etal-2024-gos-2</bibkey>
    </paper>
    <paper id="692">
      <title><fixed-case>GPT</fixed-case>-3.5 for Grammatical Error Correction</title>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>7831–7843</pages>
      <abstract>This paper investigates the application of GPT-3.5 for Grammatical Error Correction (GEC) in multiple languages in several settings: zero-shot GEC, fine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses generated by other GEC models. In the zero-shot setting, we conduct automatic evaluations of the corrections proposed by GPT-3.5 using several methods: estimating grammaticality with language models (LMs), the Scribendy test, and comparing the semantic embeddings of sentences. GPT-3.5 has a known tendency to over-correct erroneous sentences and propose alternative corrections. For several languages, such as Czech, German, Russian, Spanish, and Ukrainian, GPT-3.5 substantially alters the source sentences, including their semantics, which presents significant challenges for evaluation with reference-based metrics. For English, GPT-3.5 demonstrates high recall, generates fluent corrections, and generally preserves sentence semantics. However, human evaluation for both English and Russian reveals that, despite its strong error-detection capabilities, GPT-3.5 struggles with several error types, including punctuation mistakes, tense errors, syntactic dependencies between words, and lexical compatibility at the sentence level.</abstract>
      <url hash="2eaa2c7a">2024.lrec-main.692</url>
      <bibkey>katinskaia-yangarber-2024-gpt-3</bibkey>
    </paper>
    <paper id="693">
      <title><fixed-case>GPTE</fixed-case>val: A Survey on Assessments of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> and <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Xulang</first><last>Zhang</last></author>
      <author><first>Frank</first><last>Guerin</last></author>
      <author><first>Erik</first><last>Cambria</last></author>
      <pages>7844–7866</pages>
      <abstract>The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research.</abstract>
      <url hash="f070ec94">2024.lrec-main.693</url>
      <bibkey>mao-etal-2024-gpteval-survey</bibkey>
    </paper>
    <paper id="694">
      <title><fixed-case>GPT</fixed-case>-<fixed-case>H</fixed-case>ate<fixed-case>C</fixed-case>heck: Can <fixed-case>LLM</fixed-case>s Write Better Functional Tests for Hate Speech Detection?</title>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <author><first>Alexander</first><last>Shvets</last></author>
      <pages>7867–7885</pages>
      <abstract>Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind “You are just a [slur] to me.” However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations. Crowd-sourced annotation demonstrates that the generated test cases are of high quality. Using the new functional tests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset.</abstract>
      <url hash="659d521d">2024.lrec-main.694</url>
      <bibkey>jin-etal-2024-gpt-hatecheck</bibkey>
    </paper>
    <paper id="695">
      <title><fixed-case>GPT</fixed-case>-<fixed-case>SW</fixed-case>3: An Autoregressive Language Model for the <fixed-case>S</fixed-case>candinavian Languages</title>
      <author><first>Ariel</first><last>Ekgren</last></author>
      <author><first>Amaru</first><last>Cuba Gyllensten</last></author>
      <author><first>Felix</first><last>Stollenwerk</last></author>
      <author><first>Joey</first><last>Öhman</last></author>
      <author><first>Tim</first><last>Isbister</last></author>
      <author><first>Evangelia</first><last>Gogoulou</last></author>
      <author><first>Fredrik</first><last>Carlsson</last></author>
      <author><first>Judit</first><last>Casademont</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>7886–7900</pages>
      <abstract>This paper details the process of developing the first native large generative language model for the North Germanic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation, applications, and considerations for release strategies. We discuss pros and cons of developing large language models for smaller languages and in relatively peripheral regions of the globe, and we hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.</abstract>
      <url hash="14766933">2024.lrec-main.695</url>
      <bibkey>ekgren-etal-2024-gpt-sw3</bibkey>
    </paper>
    <paper id="696">
      <title>Gradient Consistency-based Parameter Allocation for Multilingual Neural Machine Translation</title>
      <author><first>Wenshuai</first><last>Huo</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Yichong</first><last>Huang</last></author>
      <author><first>Chengpeng</first><last>Fu</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>7901–7912</pages>
      <abstract>Multilingual neural machine translation handles the translation of multiple languages with one unified model. However, this joint-training paradigm incurs the notorious issue of parameter interference, where the model compromises with the language diversity to find a common solution. Recent research has explored avoiding this problem by selecting certain parameters for each language direction from the original model to form language-specific sub-networks. However, determining how many parameters to choose and which parameters to select is still a serious challenge. In this work, we propose an approach called CaPA (Consistency-based Parameter Allocation), which dynamically allocates parameters of appropriate scale to each language direction based on the consistency between the gradient of the individual language and the average gradient. Specifically, CaPA allocates more parameters to languages with higher gradient consistency as these languages tend to have a more positive impact on other languages. Furthermore, considering the varying levels of interference across different parts of the model, we propose an adaptive parameter allocation based on module-level gradient consistency. Experimental results show the correlation between gradient consistency and parameter interference, as well as the effectiveness of our proposed method.</abstract>
      <url hash="4e561cd4">2024.lrec-main.696</url>
      <bibkey>huo-etal-2024-gradient-consistency</bibkey>
    </paper>
    <paper id="697">
      <title>Gramble: A Tabular Programming Language for Collaborative Linguistic Modeling</title>
      <author><first>Patrick</first><last>Littell</last></author>
      <author><first>Darlene</first><last>Stewart</last></author>
      <author><first>Fineen</first><last>Davis</last></author>
      <author><first>Aidan</first><last>Pine</last></author>
      <author><first>Roland</first><last>Kuhn</last></author>
      <pages>7913–7925</pages>
      <abstract>We introduce Gramble, a domain-specific programming language for linguistic parsing and generation, in the tradition of XFST, TWOLC, and Kleene. Gramble features an intuitive tabular syntax and supports live group programming, allowing community experts to participate more directly in system development without having to be programmers themselves. A cross-platform interpreter is available for Windows, MacOS, and UNIX, supports collaborative programming on the web via Google Sheets, and is released open-source under the MIT license.</abstract>
      <url hash="75d62da5">2024.lrec-main.697</url>
      <bibkey>littell-etal-2024-gramble-tabular</bibkey>
    </paper>
    <paper id="698">
      <title>Grammatical Error Correction for Code-Switched Sentences by Learners of <fixed-case>E</fixed-case>nglish</title>
      <author><first>Kelvin Wey Han</first><last>Chan</last></author>
      <author><first>Christopher</first><last>Bryant</last></author>
      <author><first>Li</first><last>Nguyen</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <pages>7926–7938</pages>
      <abstract>Code-switching (CSW) is a common phenomenon among multilingual speakers where multiple languages are used in a single discourse or utterance. Mixed language utterances may still contain grammatical errors however, yet most existing Grammar Error Correction (GEC) systems have been trained on monolingual data and not developed with CSW in mind. In this work, we conduct the first exploration into the use of GEC systems on CSW text. Through this exploration, we propose a novel method of generating synthetic CSW GEC datasets by translating different spans of text within existing GEC corpora. We then investigate different methods of selecting these spans based on CSW ratio, switch-point factor and linguistic constraints, and identify how they affect the performance of GEC systems on CSW text. Our best model achieves an average increase of 1.57 F0.5 across 3 CSW test sets (English-Chinese, English-Korean and English-Japanese) without affecting the model’s performance on a monolingual dataset. We furthermore discovered that models trained on one CSW language generalise relatively well to other typologically similar CSW languages.</abstract>
      <url hash="f6d48539">2024.lrec-main.698</url>
      <bibkey>chan-etal-2024-grammatical-error</bibkey>
    </paper>
    <paper id="699">
      <title>Granular Change Accuracy: A More Accurate Performance Metric for Dialogue State Tracking</title>
      <author><first>Taha</first><last>Aksu</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>7939–7948</pages>
      <abstract>Current metrics for evaluating Dialogue State Tracking (DST) systems exhibit three primary limitations. They: i) erroneously presume a uniform distribution of slots throughout the dialog, ii) neglect to assign partial scores for individual turns, iii) frequently overestimate or underestimate performance by repeatedly counting the models’ successful or failed predictions. To address these shortcomings, we introduce a novel metric: Granular Change Accuracy (GCA). GCA focuses on evaluating the predicted changes in dialogue state over the entire dialogue history. Benchmarking reveals that GCA effectively reduces biases arising from distribution uniformity and the positioning of errors across turns, resulting in a more precise evaluation. Notably, we find that these biases are particularly pronounced when evaluating few-shot or zero-shot trained models, becoming even more evident as the model’s error rate increases. Hence, GCA offers significant promise, particularly for assessing models trained with limited resources. Our GCA implementation is a useful addition to the pool of DST metrics.</abstract>
      <url hash="ec4c451c">2024.lrec-main.699</url>
      <bibkey>aksu-chen-2024-granular-change</bibkey>
    </paper>
    <paper id="700">
      <title><fixed-case>G</fixed-case>reek<fixed-case>BART</fixed-case>: The First Pretrained <fixed-case>G</fixed-case>reek Sequence-to-Sequence Model</title>
      <author><first>Iakovos</first><last>Evdaimon</last></author>
      <author><first>Hadi</first><last>Abdine</last></author>
      <author><first>Christos</first><last>Xypolopoulos</last></author>
      <author><first>Stamatis</first><last>Outsios</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <author><first>Giorgos</first><last>Stamou</last></author>
      <pages>7949–7962</pages>
      <abstract>The era of transfer learning has revolutionized the fields of Computer Vision and Natural Language Processing, bringing powerful pretrained models with exceptional performance across a variety of tasks. Specifically, Natural Language Processing tasks have been dominated by transformer-based language models. In Natural Language Inference and Natural Language Generation tasks, the BERT model and its variants, as well as the GPT model and its successors, demonstrated exemplary performance. However, the majority of these models are pretrained and assessed primarily for the English language or on a multilingual corpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on BART-base architecture and pretrained on a large-scale Greek corpus. We evaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a variety of discriminative tasks. In addition, we examine its performance on two NLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek language. The model, the code, and the new summarization dataset will be publicly available.</abstract>
      <url hash="29b24a97">2024.lrec-main.700</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c01b1911">2024.lrec-main.700.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>evdaimon-etal-2024-greekbart-first</bibkey>
    </paper>
    <paper id="701">
      <title><fixed-case>GRIT</fixed-case>: A Dataset of Group Reference Recognition in <fixed-case>I</fixed-case>talian</title>
      <author><first>Sergio E.</first><last>Zanotto</last></author>
      <author><first>Qi</first><last>Yu</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <author><first>Diego</first><last>Frassinelli</last></author>
      <pages>7963–7970</pages>
      <abstract>For the analysis of political discourse a reliable identification of group references, i.e., linguistic components that refer to individuals or groups of people, is useful. However, the task of automatically recognizing group references has not yet gained much attention within NLP. To address this gap, we introduce GRIT (Group Reference for Italian), a large-scale, multi-domain manually annotated dataset for group reference recognition in Italian. GRIT represents a new resource for automatic and generalizable recognition of group references. With this dataset, we aim to establish group reference recognition as a valid classification task, which extends the domain of Named Entity Recognition by expanding its focus to literal and figurative mentions of social groups. We verify the potential of achieving automated group reference recognition for Italian through an experiment employing a fine-tuned BERT model. Our experimental results substantiate the validity of the task, implying a huge potential for applying automated systems to multiple fields of analysis, such as political text or social media analysis.</abstract>
      <url hash="7c0ad839">2024.lrec-main.701</url>
      <bibkey>zanotto-etal-2024-grit-dataset</bibkey>
    </paper>
    <paper id="702">
      <title>Grounded Multimodal Procedural Entity Recognition for Procedural Documents: A New Dataset and Baseline</title>
      <author><first>Haopeng</first><last>Ren</last></author>
      <author><first>Yushi</first><last>Zeng</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <author><first>Zhenqi</first><last>Ye</last></author>
      <author><first>Li</first><last>Yuan</last></author>
      <author><first>Pinli</first><last>Zhu</last></author>
      <pages>7971–7981</pages>
      <abstract>Much of commonsense knowledge in real world is the form of procudures or sequences of steps to achieve particular goals. In recent years, knowledge extraction on procedural documents has attracted considerable attention. However, they often focus on procedural text but ignore a common multimodal scenario in the real world. Images and text can complement each other semantically, alleviating the semantic ambiguity suffered in text-only modality. Motivated by these, in this paper, we explore a problem of grounded multimodal procedural entity recognition (GMPER), aiming to detect the entity and the corresponding bounding box groundings in image (i.e., visual entities). A new dataset (Wiki-GMPER) is bult and extensive experiments are conducted to evaluate the effectiveness of our proposed model.</abstract>
      <url hash="e352001d">2024.lrec-main.702</url>
      <bibkey>ren-etal-2024-grounded-multimodal</bibkey>
    </paper>
    <paper id="703">
      <title>Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language</title>
      <author><first>Alistair</first><last>Plum</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Christoph</first><last>Purschke</last></author>
      <pages>7982–7992</pages>
      <abstract>Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as well. Furthermore, we experiment with multilingual and cross-lingual zero-shot experiments that could benefit many low-resource languages.</abstract>
      <url hash="2a52a9b8">2024.lrec-main.703</url>
      <bibkey>plum-etal-2024-guided-distant</bibkey>
    </paper>
    <paper id="704">
      <title><fixed-case>HAE</fixed-case>-<fixed-case>RAE</fixed-case> Bench: Evaluation of <fixed-case>K</fixed-case>orean Knowledge in Language Models</title>
      <author><first>Guijin</first><last>Son</last></author>
      <author><first>Hanwool</first><last>Lee</last></author>
      <author><first>Suwan</first><last>Kim</last></author>
      <author><first>Huiseo</first><last>Kim</last></author>
      <author><first>Jae cheol</first><last>Lee</last></author>
      <author><first>Je Won</first><last>Yeom</last></author>
      <author><first>Jihyu</first><last>Jung</last></author>
      <author><first>Jung woo</first><last>Kim</last></author>
      <author><first>Songseong</first><last>Kim</last></author>
      <pages>7993–8007</pages>
      <abstract>Large language models (LLMs) trained on massive corpora demonstrate impressive capabilities in a wide range of tasks. While there are ongoing efforts to adapt these models to languages beyond English, the attention given to their evaluation methodologies remains limited. Current multilingual benchmarks often rely on back translations or re-implementations of English tests, limiting their capacity to capture unique cultural and linguistic nuances. To bridge this gap for the Korean language, we introduce the HAE-RAE Bench, a dataset curated to challenge models lacking Korean cultural and contextual depth. The dataset encompasses six downstream tasks across four domains: vocabulary, history, general knowledge, and reading comprehension. Unlike traditional evaluation suites focused on token and sequence classification or mathematical and logical reasoning, the HAE-RAE Bench emphasizes a model’s aptitude for recalling Korean-specific knowledge and cultural contexts. Comparative analysis with prior Korean benchmarks indicates that the HAE-RAE Bench presents a greater challenge to non-Korean models by disturbing abilities and knowledge learned from English being transferred.</abstract>
      <url hash="5d92bdb6">2024.lrec-main.704</url>
      <bibkey>son-etal-2024-hae-rae</bibkey>
    </paper>
    <paper id="705">
      <title>Halwasa: Quantify and Analyze Hallucinations in Large Language Models: <fixed-case>A</fixed-case>rabic as a Case Study</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Hend</first><last>Al-Khalifa</last></author>
      <author><first>Khaloud Suliman</first><last>Alkhalefah</last></author>
      <pages>8008–8015</pages>
      <abstract>Large Language Models (LLMs) have shown superb abilities to generate texts that are indistinguishable from human-generated texts in many cases. However, sometimes they generate false, incorrect, or misleading content, which is often described as “hallucinations”. Quantifying and analyzing hallucination in LLMs can increase their reliability and usage. While hallucination is being actively studied for English and other languages, and different benchmarking datsets have been created, this area is not studied at all for Arabic. In our paper, we create the first Arabic dataset that contains 10K of generated sentences by LLMs and annotate it for factuality and correctness. We provide detailed analysis of the dataset to analyze factual and linguistic errors. We found that 25% of the generated sentences are factually incorrect. We share the dataset with the research community.</abstract>
      <url hash="74aa871b">2024.lrec-main.705</url>
      <bibkey>mubarak-etal-2024-halwasa-quantify</bibkey>
    </paper>
    <paper id="706">
      <title><fixed-case>H</fixed-case>arm<fixed-case>P</fixed-case>ot: An Annotation Framework for Evaluating Offline Harm Potential of Social Media Text</title>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Ojaswee</first><last>Bhalla</last></author>
      <author><first>Madhu</first><last>Vanthi</last></author>
      <author><first>Shehlat Maknoon</first><last>Wani</last></author>
      <author><first>Siddharth</first><last>Singh</last></author>
      <pages>8016–8034</pages>
      <abstract>In this paper, we discuss the development of an annotation schema to build datasets for evaluating the offline harm potential of social media texts. We define “harm potential” as the potential for an online public post to cause real-world physical harm (i.e., violence). Understanding that real-world violence is often spurred by a web of triggers, often combining several online tactics and pre-existing intersectional fissures in the social milieu, to result in targeted physical violence, we do not focus on any single divisive aspect (i.e., caste, gender, religion, or other identities of the victim and perpetrators) nor do we focus on just hate speech or mis/dis-information. Rather, our understanding of the intersectional causes of such triggers focuses our attempt at measuring the harm potential of online content, irrespective of whether it is hateful or not. In this paper, we discuss the development of a framework/annotation schema that allows annotating the data with different aspects of the text including its socio-political grounding and intent of the speaker (as expressed through mood and modality) that together contribute to it being a trigger for offline harm. We also give a comparative analysis and mapping of our framework with some of the existing frameworks.</abstract>
      <url hash="bff487a7">2024.lrec-main.706</url>
      <bibkey>kumar-etal-2024-harmpot-annotation</bibkey>
    </paper>
    <paper id="707">
      <title>Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing</title>
      <author><first>Zhenyu</first><last>Qian</last></author>
      <author><first>Yiming</first><last>Qian</last></author>
      <author><first>Yuting</first><last>Song</last></author>
      <author><first>Fei</first><last>Gao</last></author>
      <author><first>Hai</first><last>Jin</last></author>
      <author><first>Chen</first><last>Yu</last></author>
      <author><first>Xia</first><last>Xie</last></author>
      <pages>8035–8049</pages>
      <abstract>Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across ten diverse benchmark datasets. Moreover, to address the challenge of explainability, we propose an uncertainty estimation based on perturbation, along with a calibration scheme to quantify the confidence scores of the generated answers. Our confidence measure achieves an AUC of 0.8 or higher on seven out of the ten datasets in predicting the correctness of the answer generated by LLM.</abstract>
      <url hash="6f158333">2024.lrec-main.707</url>
      <bibkey>qian-etal-2024-harnessing-power</bibkey>
    </paper>
    <paper id="708">
      <title>Has It All Been Solved? Open <fixed-case>NLP</fixed-case> Research Questions Not Solved by Large Language Models</title>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Artem</first><last>Abzaliev</last></author>
      <author><first>Laura</first><last>Biester</last></author>
      <author><first>Santiago</first><last>Castro</last></author>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Xinyi</first><last>Gao</last></author>
      <author><first>Aylin Ece</first><last>Gunal</last></author>
      <author><first>Jacky</first><last>He</last></author>
      <author><first>Ashkan</first><last>Kazemi</last></author>
      <author><first>Muhammad</first><last>Khalifa</last></author>
      <author><first>Namho</first><last>Koh</last></author>
      <author><first>Andrew</first><last>Lee</last></author>
      <author><first>Siyang</first><last>Liu</last></author>
      <author><first>Do June</first><last>Min</last></author>
      <author><first>Shinka</first><last>Mori</last></author>
      <author><first>Joan C.</first><last>Nwatu</last></author>
      <author><first>Veronica</first><last>Perez-Rosas</last></author>
      <author><first>Siqi</first><last>Shen</last></author>
      <author><first>Zekun</first><last>Wang</last></author>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>8050–8094</pages>
      <abstract>Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that “it’s all been solved.” Not surprisingly, this has, in turn, made many NLP researchers – especially those at the beginning of their careers – worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research directions to include: https://bit.ly/nlp-era-llm.</abstract>
      <url hash="9ceaa6d1">2024.lrec-main.708</url>
      <bibkey>ignat-etal-2024-solved-open</bibkey>
    </paper>
    <paper id="709">
      <title><fixed-case>H</fixed-case>ealth<fixed-case>FC</fixed-case>: Verifying Health Claims with Evidence-Based Medical Fact-Checking</title>
      <author><first>Juraj</first><last>Vladika</last></author>
      <author><first>Phillip</first><last>Schneider</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>8095–8107</pages>
      <abstract>In the digital age, seeking health advice on the Internet has become a common practice. At the same time, determining the trustworthiness of online medical content is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance automated Natural Language Processing (NLP) solutions for this task, in this paper we introduce a novel dataset HealthFC. It consists of 750 health-related claims in German and English, labeled for veracity by medical experts and backed with evidence from systematic reviews and clinical trials. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for NLP tasks related to automated fact-checking, such as evidence retrieval, claim verification, or explanation generation. For testing purposes, we provide baseline systems based on different approaches, examine their performance, and discuss the findings. We show that the dataset is a challenging test bed with a high potential for future use.</abstract>
      <url hash="5f90030e">2024.lrec-main.709</url>
      <bibkey>vladika-etal-2024-healthfc-verifying</bibkey>
    </paper>
    <paper id="710">
      <title>Hierarchical Graph Convolutional Network Approach for Detecting Low-Quality Documents</title>
      <author><first>Jaeyoung</first><last>Lee</last></author>
      <author><first>Joonwon</first><last>Jang</last></author>
      <author><first>Misuk</first><last>Kim</last></author>
      <pages>8108–8121</pages>
      <abstract>Consistency within a document is a crucial feature indicative of its quality. Recently, within the vast amount of information produced across various media, there exists a significant number of low-quality documents that either lack internal consistency or contain content utterly unrelated to their headlines. Such low-quality documents induce fatigue in readers and undermine the credibility of the media source that provided them. Consequently, research to automatically detect these low-quality documents based on natural language processing is imperative. In this study, we introduce a hierarchical graph convolutional network (HGCN) that can detect internal inconsistencies within a document and incongruences between the title and body. Moreover, we constructed the Inconsistency Dataset, leveraging published news data and its meta-data, to train our model to detect document inconsistencies. Experimental results demonstrated that the HGCN achieved superior performance with an accuracy of 91.20% on our constructed Inconsistency Dataset, outperforming other comparative models. Additionally, on the publicly available incongruent-related dataset, the proposed methodology demonstrated a performance of 92.00%, validating its general applicability. Finally, an ablation study further confirmed the significant impact of meta-data utilization on performance enhancement. We anticipate that our model can be universally applied to detect and filter low-quality documents in the real world.</abstract>
      <url hash="8b2039ad">2024.lrec-main.710</url>
      <bibkey>lee-etal-2024-hierarchical-graph</bibkey>
    </paper>
    <paper id="711">
      <title>Hierarchical Selection of Important Context for Generative Event Causality Identification with Optimal Transports</title>
      <author><first>Hieu</first><last>Man</last></author>
      <author><first>Chien Van</first><last>Nguyen</last></author>
      <author><first>Nghia Trung</first><last>Ngo</last></author>
      <author><first>Linh</first><last>Ngo</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>8122–8132</pages>
      <abstract>We study the problem of Event Causality Identification (ECI) that seeks to predict causal relation between event mentions in the text. In contrast to previous classification-based models, a few recent ECI methods have explored generative models to deliver state-of-the-art performance. However, such generative models cannot handle document-level ECI where long context between event mentions must be encoded to secure correct predictions. In addition, previous generative ECI methods tend to rely on external toolkits or human annotation to obtain necessary training signals. To address these limitations, we propose a novel generative framework that leverages Optimal Transport (OT) to automatically select the most important sentences and words from full documents. Specifically, we introduce hierarchical OT alignments between event pairs and the document to extract pertinent contexts. The selected sentences and words are provided as input and output to a T5 encoder-decoder model which is trained to generate both the causal relation label and salient contexts. This allows richer supervision without external tools. We conduct extensive evaluations on different datasets with multiple languages to demonstrate the benefits and state-of-the-art performance of ECI.</abstract>
      <url hash="536a3bc9">2024.lrec-main.711</url>
      <bibkey>man-etal-2024-hierarchical-selection</bibkey>
    </paper>
    <paper id="712">
      <title>Hierarchical Topic Modeling via Contrastive Learning and Hyperbolic Embedding</title>
      <author><first>Zhicheng</first><last>Lin</last></author>
      <author><first>HeGang</first><last>Chen</last></author>
      <author><first>Yuyin</first><last>Lu</last></author>
      <author><first>Yanghui</first><last>Rao</last></author>
      <author><first>Hao</first><last>Xu</last></author>
      <author><first>Hanjiang</first><last>Lai</last></author>
      <pages>8133–8143</pages>
      <abstract>Hierarchical topic modeling, which can mine implicit semantics in the corpus and automatically construct topic hierarchical relationships, has received considerable attention recently. However, the current hierarchical topic models are mainly based on Euclidean space, which cannot well retain the implicit hierarchical semantic information in the corpus, leading to irrational structure of the generated topics. On the other hand, the existing Generative Adversarial Network (GAN) based neural topic models perform satisfactorily, but they remain constrained by pattern collapse due to the discontinuity of latent space. To solve the above problems, with the hypothesis of hyperbolic space, we propose a novel GAN-based hierarchical topic model to mine high-quality topics by introducing contrastive learning to capture information from documents. Furthermore, the distinct tree-like property of hyperbolic space preserves the implicit hierarchical semantics of documents in topic embeddings, which are projected into the hyperbolic space. Finally, we use a multi-head self-attention mechanism to learn implicit hierarchical semantics of topics and mine topic structure information. Experiments on real-world corpora demonstrate the remarkable performance of our model on topic coherence and topic diversity, as well as the rationality of the topic hierarchy.</abstract>
      <url hash="3ac039c4">2024.lrec-main.712</url>
      <bibkey>lin-etal-2024-hierarchical-topic</bibkey>
    </paper>
    <paper id="713">
      <title>High-order Joint Constituency and Dependency Parsing</title>
      <author><first>Yanggan</first><last>Gu</last></author>
      <author><first>Yang</first><last>Hou</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Xinyu</first><last>Duan</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <pages>8144–8154</pages>
      <abstract>This work revisits the topic of jointly parsing constituency and dependency trees, i.e., to produce compatible constituency and dependency trees simultaneously for input sentences, which is attractive considering that the two types of trees are complementary in representing syntax. The original work of Zhou and Zhao (2019) performs joint parsing only at the inference phase. They train two separate parsers under the multi-task learning framework (i.e., one shared encoder and two independent decoders). They design an ad-hoc dynamic programming-based decoding algorithm of <tex-math>O(n^5)</tex-math> time complexity for finding optimal compatible tree pairs. Compared to their work, we make progress in three aspects: (1) adopting a much more efficient decoding algorithm of <tex-math>O(n^4)</tex-math> time complexity, (2) exploring joint modeling at the training phase, instead of only at the inference phase, (3) proposing high-order scoring components to promote constituent-dependency interaction. We conduct experiments and analysis on seven languages, covering both rich-resource and low-resource scenarios. Results and analysis show that joint modeling leads to a modest overall performance boost over separate modeling, but substantially improves the complete matching ratio of whole trees, thanks to the explicit modeling of tree compatibility.</abstract>
      <url hash="61c49f68">2024.lrec-main.713</url>
      <bibkey>gu-etal-2024-high-order</bibkey>
    </paper>
    <paper id="714">
      <title>High-Order Semantic Alignment for Unsupervised Fine-Grained Image-Text Retrieval</title>
      <author><first>Rui</first><last>Gao</last></author>
      <author><first>Miaomiao</first><last>Cheng</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Wei</first><last>Song</last></author>
      <pages>8155–8165</pages>
      <abstract>Cross-modal retrieval is an important yet challenging task due to the semantic discrepancy between visual content and language. To measure the correlation between images and text, most existing research mainly focuses on learning global or local correspondence, failing to explore fine-grained local-global alignment. To infer more accurate similarity scores, we introduce a novel High Order Semantic Alignment (HOSA) model that can provide complementary and comprehensive semantic clues. Specifically, to jointly learn global and local alignment and emphasize local-global interaction, we employ tensor-product (t-product) operation to reconstruct one modal’s representation based on another modal’s information in a common semantic space. Such a cross-modal reconstruction strategy would significantly enhance inter-modal correlation learning in a fine-grained manner. Extensive experiments on two benchmark datasets validate that our model significantly outperforms several state-of-the-art baselines, especially in retrieving the most relevant results.</abstract>
      <url hash="f67c3531">2024.lrec-main.714</url>
      <bibkey>gao-etal-2024-high-order</bibkey>
    </paper>
    <paper id="715">
      <title><fixed-case>H</fixed-case>o<fixed-case>LM</fixed-case>: Analyzing the Linguistic Unexpectedness in <fixed-case>H</fixed-case>omeric Poetry</title>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Ryan</first><last>Sandell</last></author>
      <author><first>Maria</first><last>Konstantinidou</last></author>
      <author><first>Chiara</first><last>Bozzone</last></author>
      <pages>8166–8172</pages>
      <abstract>The authorship of the Homeric poems has been a matter of debate for centuries. Computational approaches such as language modeling exist that can aid experts in making crucial headway. We observe, however, that such work has, thus far, only been carried out at the level of lengthier excerpts, but not individual verses, the level at which most suspected interpolations occur. We address this weakness by presenting a corpus of Homeric verses, each complemented with a score quantifying linguistic unexpectedness based on Perplexity. We assess the nature of these scores by exploring their correlation with named entities, the frequency of character n-grams, and (inverse) word frequency, revealing robust correlations with the latter two. This apparent bias can be partly overcome by simply dividing scores for unexpectedness by the maximum term frequency per verse.</abstract>
      <url hash="5c657887">2024.lrec-main.715</url>
      <bibkey>pavlopoulos-etal-2024-holm-analyzing</bibkey>
    </paper>
    <paper id="716">
      <title>How Diplomats Dispute: The <fixed-case>UN</fixed-case> Security Council Conflict Corpus</title>
      <author><first>Karolina</first><last>Zaczynska</last></author>
      <author><first>Peter</first><last>Bourgonje</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>8173–8183</pages>
      <abstract>We investigate disputes in the United Nations Security Council (UNSC) by studying the linguistic means of expressing conflicts. As a result, we present the UNSC Conflict Corpus (UNSCon), a collection of 87 UNSC speeches that are annotated for conflicts. We explain and motivate our annotation scheme and report on a series of experiments for automatic conflict classification. Further, we demonstrate the difficulty when dealing with diplomatic language - which is highly complex and often implicit along various dimensions - by providing corpus examples, readability scores, and classification results.</abstract>
      <url hash="fef62d3b">2024.lrec-main.716</url>
      <bibkey>zaczynska-etal-2024-diplomats-dispute</bibkey>
    </paper>
    <paper id="717">
      <title>How Do Hyenas Deal with Human Speech? Speech Recognition and Translation with <fixed-case>C</fixed-case>onf<fixed-case>H</fixed-case>yena</title>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <pages>8184–8191</pages>
      <abstract>The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (∼1%), which, in most cases, is not statistically significant.</abstract>
      <url hash="7bc96f7c">2024.lrec-main.717</url>
      <bibkey>gaido-etal-2024-hyenas-deal</bibkey>
    </paper>
    <paper id="718">
      <title>How Far Is Too Far? Studying the Effects of Domain Discrepancy on Masked Language Models</title>
      <author><first>Subhradeep</first><last>Kayal</last></author>
      <author><first>Alexander</first><last>Rakhlin</last></author>
      <author><first>Ali</first><last>Dashti</last></author>
      <author><first>Serguei</first><last>Stepaniants</last></author>
      <pages>8192–8199</pages>
      <abstract>Pre-trained masked language models, such as BERT, perform strongly on a wide variety of NLP tasks and have become ubiquitous in recent years. The typical way to use such models is to fine-tune them on downstream data. In this work, we aim to study how the difference in domains between the pre-trained model and the task effects its final performance. We first devise a simple mechanism to quantify the domain difference (using a cloze task) and use it to partition our dataset. Using these partitions of varying domain discrepancy, we focus on answering key questions around the impact of discrepancy on final performance, robustness to out-of-domain test-time examples and effect of domain-adaptive pre-training. We base our experiments on a large-scale openly available e-commerce dataset, and our findings suggest that in spite of pre-training the performance of BERT degrades on datasets with high domain discrepancy, especially in low resource cases. This effect is somewhat mitigated by continued pre-training for domain adaptation. Furthermore, the domain-gap also makes BERT sensitive to out-of-domain examples during inference, even in high resource tasks, and it is prudent to use as diverse a dataset as possible during fine-tuning to make it robust to domain shift.</abstract>
      <url hash="16463578">2024.lrec-main.718</url>
      <bibkey>kayal-etal-2024-far-far</bibkey>
    </paper>
    <paper id="719">
      <title>How Gender Interacts with Political Values: A Case Study on <fixed-case>C</fixed-case>zech <fixed-case>BERT</fixed-case> Models</title>
      <author><first>Adnan</first><last>Al Ali</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <pages>8200–8210</pages>
      <abstract>Neural language models, which reach state-of-the-art results on most natural language processing tasks, are trained on large text corpora that inevitably contain value-burdened content and often capture undesirable biases, which the models reflect. This case study focuses on the political biases of pre-trained encoders in Czech and compares them with a representative value survey. Because Czech is a gendered language, we also measure how the grammatical gender coincides with responses to men and women in the survey. We introduce a novel method for measuring the model’s perceived political values. We find that the models do not assign statement probability following value-driven reasoning, and there is no systematic difference between feminine and masculine sentences. We conclude that BERT-sized models do not manifest systematic alignment with political values and that the biases observed in the models are rather due to superficial imitation of training data patterns than systematic value beliefs encoded in the models.</abstract>
      <url hash="569da0b1">2024.lrec-main.719</url>
      <bibkey>al-ali-libovicky-2024-gender-interacts</bibkey>
    </paper>
    <paper id="720">
      <title>How Good Are <fixed-case>LLM</fixed-case>s at Out-of-Distribution Detection?</title>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Li-Ming</first><last>Zhan</last></author>
      <author><first>Zexin</first><last>Lu</last></author>
      <author><first>Yujie</first><last>Feng</last></author>
      <author><first>Lei</first><last>Xue</last></author>
      <author><first>Xiao-Ming</first><last>Wu</last></author>
      <pages>8211–8222</pages>
      <abstract>Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning models. As large language models (LLMs) become more prevalent, the applicability of prior research on OOD detection that utilized smaller-scale Transformers such as BERT, RoBERTa, and GPT-2 may be challenged, due to the significant differences in the scale of these models, their pre-training objectives, and the paradigms used for inference. This paper initiates a pioneering empirical investigation into the OOD detection capabilities of LLMs, focusing on the LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly used OOD detectors, examining their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. We have released the source code at <url>https://github.com/Awenbocc/LLM-OOD</url> for other researchers to reproduce our results.</abstract>
      <url hash="6064775a">2024.lrec-main.720</url>
      <bibkey>liu-etal-2024-good-llms</bibkey>
    </paper>
    <paper id="721">
      <title>How Important Is Tokenization in <fixed-case>F</fixed-case>rench Medical Masked Language Models?</title>
      <author><first>Yanis</first><last>Labrak</last></author>
      <author><first>Adrien</first><last>Bazoge</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <pages>8223–8234</pages>
      <abstract>Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate this knowledge, leading to inconsistent tokenization strategies for common terms. In this paper, we seek to delve into the complexities of subword tokenization in French biomedical domain across a variety of NLP tasks and pinpoint areas where further enhancements can be made. We analyze classical tokenization algorithms, including BPE and SentencePiece, and introduce an original tokenization strategy that integrates morpheme-enriched word segmentation into existing tokenization methods.</abstract>
      <url hash="426592d3">2024.lrec-main.721</url>
      <bibkey>labrak-etal-2024-important-tokenization</bibkey>
    </paper>
    <paper id="722">
      <title>How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study</title>
      <author><first>Tianjie</first><last>Ju</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Wei</first><last>Du</last></author>
      <author><first>Xinwei</first><last>Yuan</last></author>
      <author><first>Zhaochun</first><last>Ren</last></author>
      <author><first>Gongshen</first><last>Liu</last></author>
      <pages>8235–8246</pages>
      <abstract>Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ <tex-math>\mathcal V</tex-math>-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence. Code is publicly available at https://github.com/Jometeorie/probing_llama.</abstract>
      <url hash="fbd6b168">2024.lrec-main.722</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a2c21eb7">2024.lrec-main.722.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>ju-etal-2024-large-language</bibkey>
    </paper>
    <paper id="723">
      <title>How Much Do Robots Understand Rudeness? Challenges in Human-Robot Interaction</title>
      <author><first>Michael Andrew</first><last>Orme</last></author>
      <author><first>Yanchao</first><last>Yu</last></author>
      <author><first>Zhiyuan</first><last>Tan</last></author>
      <pages>8247–8257</pages>
      <abstract>This paper concerns the pressing need to understand and manage inappropriate language within the evolving human-robot interaction (HRI) landscape. As intelligent systems and robots transition from controlled laboratory settings to everyday households, the demand for polite and culturally sensitive conversational abilities becomes paramount, especially for younger individuals. This study explores data cleaning methods, focussing on rudeness and contextual similarity, to identify and mitigate inappropriate language in real-time interactions. State-of-the-art natural language models are also evaluated for their proficiency in discerning rudeness. This multifaceted investigation highlights the challenges of handling inappropriate language, including its tendency to hide within idiomatic expressions and its context-dependent nature. This study will further contribute to the future development of AI systems capable of engaging in intelligent conversations and upholding the values of courtesy and respect across diverse cultural and generational boundaries.</abstract>
      <url hash="8f661c8a">2024.lrec-main.723</url>
      <bibkey>orme-etal-2024-much-robots</bibkey>
    </paper>
    <paper id="724">
      <title>How Robust Are the <fixed-case>QA</fixed-case> Models for Hybrid Scientific Tabular Data? A Study Using Customized Dataset</title>
      <author><first>Akash</first><last>Ghosh</last></author>
      <author><first>Venkata Sahith</first><last>Bathini</last></author>
      <author><first>Niloy</first><last>Ganguly</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>8258–8264</pages>
      <abstract>Question-answering (QA) on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical reasoning. In recent years, while tabular QA has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any benchmark dataset. To investigate the robustness of the existing state-of-the-art QA models on scientific hybrid tabular data, we propose a new dataset, “SciTabQA”, consisting of 822 question-answer pairs from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular QA models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific reasoning tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that “SciTabQA” is an innovative dataset to study question-answering over scientific heterogeneous data. We benchmark three state-of-the-art Tabular QA models, and find that the best F1 score is only 0.462.</abstract>
      <url hash="42be2912">2024.lrec-main.724</url>
      <bibkey>ghosh-etal-2024-robust-qa</bibkey>
    </paper>
    <paper id="725">
      <title>How Speculative Can Speculative Decoding Be?</title>
      <author><first>Zhuorui</first><last>Liu</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>8265–8275</pages>
      <abstract>Large language models (LLMs) have drawn great attention from the field of natural language processing and beyond, due to their impressive capability of autoregressive modeling, yet bringing an obvious problem, i.e., the largely increased latency. An emerging idea to alleviate this problem is speculative decoding, which first uses a draft model to draft tokens autoregressively and then makes the target model verify these tokens in parallel. The draft model is typically smaller than the target model, and it essentially trades generation quality for speed. Thereby, speculative decoding can be viewed as a speculative game for the target model in term of verification failures. That is, the lengthy draft tokens proposed by the small draft models could fail in the verification stage. Naturally, a critical question arises: how speculative can speculative decoding be, or in other words, how small can an adequate draft model be and how large can an appropriate number of draft tokens be? This work aims to investigate these questions and demonstrate how the scale of the draft model and the number of draft tokens would have an impact on the overall latency of the speculative decoding. We theoretically show that neither of above two factors will be infinitely speculative. Namely, there is a certain turning point for each of them. We then empirically show that the scale of the draft model could be 10-20<tex-math>\times</tex-math> smaller than the target model and the optimal number of draft tokens should lie in 3-5.</abstract>
      <url hash="a501f997">2024.lrec-main.725</url>
      <bibkey>liu-etal-2024-speculative-speculative</bibkey>
    </paper>
    <paper id="726">
      <title>How Susceptible Are <fixed-case>LLM</fixed-case>s to Logical Fallacies?</title>
      <author><first>Amirreza</first><last>Payandeh</last></author>
      <author><first>Dan</first><last>Pluth</last></author>
      <author><first>Jordan</first><last>Hosier</last></author>
      <author><first>Xuesu</first><last>Xiao</last></author>
      <author><first>Vijay K.</first><last>Gurbani</last></author>
      <pages>8276–8286</pages>
      <abstract>This paper investigates the rational thinking capability of Large Language Models (LLMs) in multi-round argumentative debates by exploring the impact of fallacious arguments on their logical reasoning performance. More specifically, we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM involves two agents: a persuader and a debater engaging in a multi-round debate on a controversial topic, where the persuader tries to convince the debater of the correctness of its claim. First, LOGICOM assesses the potential of LLMs to change their opinions through reasoning. Then, it evaluates the debater’s performance in logical reasoning by contrasting the scenario where the persuader employs logical fallacies against one where logical reasoning is used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4 using a dataset containing controversial topics, claims, and reasons supporting them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their opinion through reasoning. However, when presented with logical fallacies, GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often, respectively, compared to when logical reasoning is used. Finally, we introduce a new dataset containing over 5k pairs of logical vs. fallacious arguments.</abstract>
      <url hash="9a898266">2024.lrec-main.726</url>
      <bibkey>payandeh-etal-2024-susceptible-llms</bibkey>
    </paper>
    <paper id="727">
      <title>How to Do Politics with Words: Investigating Speech Acts in Parliamentary Debates</title>
      <author><first>Ines</first><last>Reinig</last></author>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>8287–8300</pages>
      <abstract>This paper presents a new perspective on framing through the lens of speech acts and investigates how politicians make use of different pragmatic speech act functions in political debates. To that end, we created a new resource of German parliamentary debates, annotated with fine-grained speech act types. Our hierarchical annotation scheme distinguishes between cooperation and conflict communication, further structured into six subtypes, such as informative, declarative or argumentative-critical speech acts, with 14 fine-grained classes at the lowest level. We present classification baselines on our new data and show that the fine-grained classes in our schema can be predicted with an avg. F1 of around 82.0%. We then use our classifier to analyse the use of speech acts in a large corpus of parliamentary debates over a time span from 2003–2023.</abstract>
      <url hash="8de61668">2024.lrec-main.727</url>
      <bibkey>reinig-etal-2024-politics-words</bibkey>
    </paper>
    <paper id="728">
      <title>How to Encode Domain Information in Relation Classification</title>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Viggo Unmack</first><last>Gascou</last></author>
      <author><first>Frida Nøhr</first><last>Laustsen</last></author>
      <author><first>Gustav</first><last>Kristensen</last></author>
      <author><first>Marie Haahr</first><last>Petersen</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>8301–8306</pages>
      <abstract>Current language models require a lot of training data to obtain high performance. For Relation Classification (RC), many datasets are domain-specific, so combining datasets to obtain better performance is non-trivial. We explore a multi-domain training setup for RC, and attempt to improve performance by encoding domain information. Our proposed models improve &gt; 2 Macro-F1 against the baseline setup, and our analysis reveals that not all the labels benefit the same: The classes which occupy a similar space across domains (i.e., their interpretation is close across them, for example “physical”) benefit the least, while domain-dependent relations (e.g., “part-of”) improve the most when encoding domain information.</abstract>
      <url hash="22603e38">2024.lrec-main.728</url>
      <bibkey>bassignana-etal-2024-encode-domain</bibkey>
    </paper>
    <paper id="729">
      <title>How to Solve Few-Shot Abusive Content Detection Using the Data We Actually Have</title>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>8307–8322</pages>
      <abstract>Due to the broad range of social media platforms, the requirements of abusive language detection systems are varied and ever-changing. Already a large set of annotated corpora with different properties and label sets were created, such as hate or misogyny detection, but the form and targets of abusive speech are constantly evolving. Since, the annotation of new corpora is expensive, in this work we leverage datasets we already have, covering a wide range of tasks related to abusive language detection. Our goal is to build models cheaply for a new target label set and/or language, using only a few training examples of the target domain. We propose a two-step approach: first we train our model in a multitask fashion. We then carry out few-shot adaptation to the target requirements. Our experiments show that using already existing datasets and only a few-shots of the target task the performance of models improve both monolingually and across languages. Our analysis also shows that our models acquire a general understanding of abusive language, since they improve the prediction of labels which are present only in the target dataset and can benefit from knowledge about labels which are not directly used for the target task.</abstract>
      <url hash="8891ab46">2024.lrec-main.729</url>
      <bibkey>hangya-fraser-2024-solve-shot</bibkey>
    </paper>
    <paper id="730">
      <title>How to Understand “Support”? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding</title>
      <author><first>Jiamin</first><last>Luo</last></author>
      <author><first>Jianing</first><last>Zhao</last></author>
      <author><first>Jingjing</first><last>Wang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>8323–8333</pages>
      <abstract>Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction.</abstract>
      <url hash="24c649c7">2024.lrec-main.730</url>
      <bibkey>luo-etal-2024-understand-support</bibkey>
    </paper>
    <paper id="731">
      <title>How Well Can <fixed-case>BERT</fixed-case> Learn the Grammar of an Agglutinative and Flexible-Order Language? The Case of <fixed-case>B</fixed-case>asque.</title>
      <author><first>Gorka</first><last>Urbizu</last></author>
      <author><first>Muitze</first><last>Zulaika</last></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <author><first>Ander</first><last>Corral</last></author>
      <pages>8334–8348</pages>
      <abstract>This work investigates the acquisition of formal linguistic competence by neural language models, hypothesizing that languages with complex grammar, such as Basque, present substantial challenges during the pre-training phase. Basque is distinguished by its complex morphology and flexible word order, potentially complicating grammar extraction. In our analysis, we evaluated the grammatical knowledge of BERT models trained under various pre-training configurations, considering factors such as corpus size, model size, number of epochs, and the use of lemmatization. To assess this grammatical knowledge, we constructed the BL2MP (Basque L2 student-based Minimal Pairs) test set. This test set consists of minimal pairs, each containing both a grammatically correct and an incorrect sentence, sourced from essays authored by students at different proficiency levels in the Basque language. Additionally, our analysis explores the difficulties in learning various grammatical phenomena, the challenges posed by flexible word order, and the influence of the student’s proficiency level on the difficulty of correcting grammar errors.</abstract>
      <url hash="619e907a">2024.lrec-main.731</url>
      <bibkey>urbizu-etal-2024-well-bert</bibkey>
    </paper>
    <paper id="732">
      <title><fixed-case>HS</fixed-case>-<fixed-case>GC</fixed-case>: Holistic Semantic Embedding and Global Contrast for Effective Text Clustering</title>
      <author><first>Chen</first><last>Yang</last></author>
      <author><first>Bin</first><last>Cao</last></author>
      <author><first>Jing</first><last>Fan</last></author>
      <pages>8349–8359</pages>
      <abstract>In this paper, we introduce Holistic Semantic Embedding and Global Contrast (HS-GC), an end-to-end approach to learn the instance- and cluster-level representation. Specifically, for instance-level representation learning, we introduce a new loss function that exploits different layers of semantic information in a deep neural network to provide a more holistic semantic text representation. Contrastive learning is applied to these representations to improve the model’s ability to represent text instances. Additionally, for cluster-level representation learning we propose two strategies that utilize global update to construct cluster centers from a global view. The extensive experimental evaluation on five text datasets shows that our method outperforms the state-of-the-art model. Particularly on the SearchSnippets dataset, our method leads by 4.4% in normalized mutual information against the latest comparison method. On the StackOverflow and TREC datasets, our method improves the clustering accuracy of 5.9% and 3.2%, respectively.</abstract>
      <url hash="8eb0204f">2024.lrec-main.732</url>
      <bibkey>yang-etal-2024-hs-gc</bibkey>
    </paper>
    <paper id="733">
      <title><fixed-case>H</fixed-case>u<fixed-case>LU</fixed-case>: <fixed-case>H</fixed-case>ungarian Language Understanding Benchmark Kit</title>
      <author><first>Noémi</first><last>Ligeti-Nagy</last></author>
      <author><first>Gergő</first><last>Ferenczi</last></author>
      <author><first>Enikő</first><last>Héja</last></author>
      <author><first>László János</first><last>Laki</last></author>
      <author><first>Noémi</first><last>Vadász</last></author>
      <author><first>Zijian Győző</first><last>Yang</last></author>
      <author><first>Tamás</first><last>Váradi</last></author>
      <pages>8360–8371</pages>
      <abstract>The paper introduces the Hungarian Language Understanding (HuLU) benchmark, a comprehensive assessment framework designed to evaluate the performance of neural language models on Hungarian language tasks. Inspired by the renowned GLUE and SuperGLUE benchmarks, HuLU aims to address the challenges specific to Hungarian language processing. The benchmark consists of various datasets, each representing different linguistic phenomena and task complexities. Moreover, the paper presents a web service developed for HuLU, offering a user-friendly interface for model evaluation. This platform not only ensures consistent assessment but also fosters transparency by maintaining a leaderboard showcasing model performances. Preliminary evaluations of various LMMs on HuLU datasets indicate that while Hungarian models show promise, there’s room for improvement to match the proficiency of English-centric models in their native language.</abstract>
      <url hash="482b9c3c">2024.lrec-main.733</url>
      <bibkey>ligeti-nagy-etal-2024-hulu-hungarian</bibkey>
    </paper>
    <paper id="734">
      <title>Human and System Perspectives on the Expression of Irony: An Analysis of Likelihood Labels and Rationales</title>
      <author><first>Aaron</first><last>Maladry</last></author>
      <author><first>Alessandra Teresa</first><last>Cignarella</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <author><first>Cynthia</first><last>van Hee</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>8372–8382</pages>
      <abstract>In this paper, we examine the recognition of irony by both humans and automatic systems. We achieve this by enhancing the annotations of an English benchmark data set for irony detection. This enhancement involves a layer of human-annotated irony likelihood using a 7-point Likert scale that combines binary annotation with a confidence measure. Additionally, the annotators indicated the trigger words that led them to perceive the text as ironic, which leveraged necessary theoretical insights into the definition of irony and its various forms. By comparing these trigger word spans across annotators, we determine the extent to which humans agree on the source of irony in a text. Finally, we compare the human-annotated spans with sub-token importance attributions for fine-tuned transformers using Layer Integrated Gradients, a state-of-the-art interpretability metric. Our results indicate that our model achieves better performance on tweets that were annotated with high confidence and high agreement. Although automatic systems can identify trigger words with relative success, they still attribute a significant amount of their importance to the wrong tokens.</abstract>
      <url hash="c0c78de4">2024.lrec-main.734</url>
      <bibkey>maladry-etal-2024-human-system</bibkey>
    </paper>
    <paper id="735">
      <title><fixed-case>H</fixed-case>uman<fixed-case>E</fixed-case>val-<fixed-case>XL</fixed-case>: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</title>
      <author><first>Qiwei</first><last>Peng</last></author>
      <author><first>Yekun</first><last>Chai</last></author>
      <author><first>Xuhong</first><last>Li</last></author>
      <pages>8383–8394</pages>
      <abstract>Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual code generation. We make our evaluation code and data publicly available at https://github.com/FloatAI/HumanEval-XL.</abstract>
      <url hash="465582cc">2024.lrec-main.735</url>
      <bibkey>peng-etal-2024-humaneval-xl</bibkey>
    </paper>
    <paper id="736">
      <title>Human in the Loop: How to Effectively Create Coherent Topics by Manually Labeling Only a Few Documents per Class</title>
      <author><first>Anton F.</first><last>Thielmann</last></author>
      <author><first>Christoph</first><last>Weisser</last></author>
      <author><first>Benjamin</first><last>Säfken</last></author>
      <pages>8395–8405</pages>
      <abstract>Few-shot methods for accurate modeling under sparse label-settings have improved significantly. However, the applications of few-shot modeling in natural language processing remain solely in the field of document classification. With recent performance improvements, supervised few-shot methods, combined with a simple topic extraction method pose a significant challenge to unsupervised topic modeling methods. Our research shows that supervised few-shot learning, combined with a simple topic extraction method, can outperform unsupervised topic modeling techniques in terms of generating coherent topics, even when only a few labeled documents per class are used. The code is available at the following link: https://github.com/AnFreTh/STREAM</abstract>
      <url hash="904935dd">2024.lrec-main.736</url>
      <bibkey>thielmann-etal-2024-human-loop</bibkey>
    </paper>
    <paper id="737">
      <title>Humanistic Buddhism Corpus: A Challenging Domain-Specific Dataset of <fixed-case>E</fixed-case>nglish Translations for Classical and <fixed-case>M</fixed-case>odern <fixed-case>C</fixed-case>hinese</title>
      <author><first>Youheng W.</first><last>Wong</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <author><first>Erdem</first><last>Koyuncu</last></author>
      <pages>8406–8417</pages>
      <abstract>We introduce the Humanistic Buddhism Corpus (HBC), a dataset containing over 80,000 Chinese-English parallel phrases extracted and translated from publications in the domain of Buddhism. HBC is one of the largest free domain-specific datasets that is publicly available for research, containing text from both classical and modern Chinese. Moreover, since HBC originates from religious texts, many phrases in the dataset contain metaphors and symbolism, and are subject to multiple interpretations. Compared to existing machine translation datasets, HBC presents difficult unique challenges. In this paper, we describe HBC in detail. We evaluate HBC within a machine translation setting, validating its use by establishing performance benchmarks using a Transformer model with different transfer learning setups.</abstract>
      <url hash="e397532e">2024.lrec-main.737</url>
      <bibkey>wong-etal-2024-humanistic-buddhism</bibkey>
    </paper>
    <paper id="738">
      <title>Humanitarian Corpora for <fixed-case>E</fixed-case>nglish, <fixed-case>F</fixed-case>rench and <fixed-case>S</fixed-case>panish</title>
      <author><first>Loryn</first><last>Isaacs</last></author>
      <author><first>Santiago</first><last>Chambó</last></author>
      <author><first>Pilar</first><last>León-Araúz</last></author>
      <pages>8418–8426</pages>
      <abstract>This paper presents three corpora of English, French and Spanish humanitarian documents compiled with reports obtained from ReliefWeb through its API. ReliefWeb is a leading database of humanitarian documents operated by the UN Office for the Coordination of Humanitarian Affairs (OCHA). To compile these corpora, documents were selected with language identification and noise reduction techniques. They were subsequently tokenized, lemmatized, tagged by part of speech, and enriched with metadata for use by linguists in corpus query software. These corpora were compiled to satisfy the research needs of the Humanitarian Encyclopedia, a project with a focus on conceptual variation. However, they can also be useful for other humanitarian endeavors, whether they are research- or practitioner-oriented; the source code for generating the corpora is available on GitHub. To compare materials, an exploratory analysis of definitional and generic-specific information was conducted for the concept of ARMED ACTOR with lexical data extracted from an English legacy corpus (where the concept is underrepresented) as well as on the new English and Spanish corpora. Lexical data were compared among corpora and presented by means of online data visualization to illustrate its potential to inform conceptual modelling.</abstract>
      <url hash="922e5d4f">2024.lrec-main.738</url>
      <bibkey>isaacs-etal-2024-humanitarian-corpora</bibkey>
    </paper>
    <paper id="739">
      <title>Humanizing Machine-Generated Content: Evading <fixed-case>AI</fixed-case>-Text Detection through Adversarial Attack</title>
      <author><first>Ying</first><last>Zhou</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>8427–8437</pages>
      <abstract>With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks, such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model’s robustness against such attacks. The empirical results reveal that the current detection model can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model’s robustness over iterative adversarial learning. Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.</abstract>
      <url hash="ba2a7bbd">2024.lrec-main.739</url>
      <bibkey>zhou-etal-2024-humanizing-machine</bibkey>
    </paper>
    <paper id="740">
      <title>Humans Need Context, What about Machines? Investigating Conversational Context in Abusive Language Detection</title>
      <author><first>Tom</first><last>Bourgeade</last></author>
      <author><first>Zongmin</first><last>Li</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Jian</first><last>Su</last></author>
      <author><first>Aixin</first><last>Sun</last></author>
      <pages>8438–8452</pages>
      <abstract>A crucial aspect in abusive language on social media platforms (toxicity, hate speech, harmful stereotypes, etc.) is its inherent contextual nature. In this paper, we focus on the role of conversational context in abusive language detection, one of the most “direct” forms of context in this domain, as given by the conversation threads (e.g., directly preceding message, original post). The incorporation of surrounding messages has proven vital for the accurate human annotation of harmful content. However, many prior works have either ignored this aspect, collecting and processing messages in isolation, or have obtained inconsistent results when attempting to embed such contextual information into traditional classification methods. The reasons behind these findings have not yet been properly addressed. To this end, we propose an analysis of the impact of conversational context in abusive language detection, through: (1) an analysis of prior works and the limitations of the most common concatenation-based approach, which we attempt to address with two alternative architectures; (2) an evaluation of these methods on existing datasets in English, and a new dataset of French tweets annotated for hate speech and stereotypes; and (3) a qualitative analysis showcasing the necessity for context-awareness in ALD, but also its difficulties.</abstract>
      <url hash="2f09c610">2024.lrec-main.740</url>
      <bibkey>bourgeade-etal-2024-humans-need</bibkey>
    </paper>
    <paper id="741">
      <title>Human vs. Machine Perceptions on Immigration Stereotypes</title>
      <author><first>Wolfgang S.</first><last>Schmeisser-Nieto</last></author>
      <author><first>Pol</first><last>Pastells</last></author>
      <author><first>Simona</first><last>Frenda</last></author>
      <author><first>Mariona</first><last>Taule</last></author>
      <pages>8453–8463</pages>
      <abstract>The increasing popularity of natural language processing has led to a race to improve machine learning models that often leaves aside the core study object, the language itself. In this study, we present classification models designed to detect stereotypes related to immigrants, along with both quantitative and qualitative analyses, shedding light on linguistic distinctions in how humans and various models perceive stereotypes. Given the subjective nature of this task, one of the models incorporates the judgments of all annotators by utilizing soft labels. Through a comparative analysis of BERT-based models using both hard and soft labels, along with predictions from GPT-4, we gain a clearer understanding of the linguistic challenges posed by texts containing stereotypes. Our dataset comprises Spanish Twitter posts collected as responses to immigrant-related hoaxes, annotated with binary values indicating the presence of stereotypes, implicitness, and the requirement for conversational context to understand the stereotype. Our findings suggest that both model prediction confidence and inter-annotator agreement are higher for explicit stereotypes, while stereotypes conveyed through irony and other figures of speech prove more challenging to detect than other implicit stereotypes.</abstract>
      <url hash="149174db">2024.lrec-main.741</url>
      <bibkey>schmeisser-nieto-etal-2024-human-vs</bibkey>
    </paper>
    <paper id="742">
      <title>Hybrid of Spans and Table-Filling for Aspect-Level Sentiment Triplet Extraction</title>
      <author><first>Minghua</first><last>Nuo</last></author>
      <author><first>Chaofan</first><last>Guo</last></author>
      <pages>8464–8473</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in sentiment analysis research. Recently, researchers have proposed different tagging schemes, containing tagging of words, tagging of word pairs, and tagging of spans. However, the first two of these methods are often insufficient for the identification of multi-word terms, while the span tagging can label the entire phrase span, but it lacks the interactive information between words. In this paper, we propose Span in Table(S&amp;T) model which combining span with table-filling. Specifically, S&amp;T model achieve full fusion of syntactic and contextual features through cross-attention and generate the structures of word-pair table through Biaffine. Then, our model converts it to a span table by computing semantic distance based on syntactic dependency tree, which can enrich each unit of span table with semantic and interactive information. Meanwhile, the initial sentence features are constructed as simple phrase tables to enhance textual information of the phrase itself. In decoding, we define 8 types of labels for identifying three dimensions including aspect, opinion, and sentiment. Finally, the extensive experiments on D2 dataset show S&amp;T model achieves competitive results in ASTE task, the results certify the effectiveness and robustness of our S&amp;T model.</abstract>
      <url hash="f695265d">2024.lrec-main.742</url>
      <bibkey>nuo-guo-2024-hybrid-spans</bibkey>
    </paper>
    <paper id="743">
      <title>Hyperbolic Graph Neural Network for Temporal Knowledge Graph Completion</title>
      <author><first>Yancong</first><last>Li</last></author>
      <author><first>Xiaoming</first><last>Zhang</last></author>
      <author><first>Ying</first><last>Cui</last></author>
      <author><first>Shuai</first><last>Ma</last></author>
      <pages>8474–8486</pages>
      <abstract>Temporal Knowledge Graphs (TKGs) represent a crucial source of structured temporal information and exhibit significant utility in various real-world applications. However, TKGs are susceptible to incompleteness, necessitating Temporal Knowledge Graph Completion (TKGC) to predict missing facts. Existing models have encountered limitations in effectively capturing the intricate temporal dynamics and hierarchical relations within TKGs. To address these challenges, HyGNet is proposed, leveraging hyperbolic geometry to effectively model temporal knowledge graphs. The model comprises two components: the Hyperbolic Gated Graph Neural Network (HGGNN) and the Hyperbolic Convolutional Neural Network (HCNN). HGGNN aggregates neighborhood information in hyperbolic space, effectively capturing the contextual information and dependencies between entities. HCNN interacts with embeddings in hyperbolic space, effectively modeling the complex interactions between entities, relations, and timestamps. Additionally, a consistency loss is introduced to ensure smooth transitions in temporal embeddings. The extensive experimental results conducted on four benchmark datasets for TKGC highlight the effectiveness of HyGNet. It achieves state-of-the-art performance in comparison to previous models, showcasing its potential for real-world applications that involve temporal reasoning and knowledge prediction.</abstract>
      <url hash="602f3def">2024.lrec-main.743</url>
      <bibkey>li-etal-2024-hyperbolic-graph</bibkey>
    </paper>
    <paper id="744">
      <title>Hyperbolic Representations for Prompt Learning</title>
      <author><first>Nan</first><last>Chen</last></author>
      <author><first>Xiangdong</first><last>Su</last></author>
      <author><first>Feilong</first><last>Bao</last></author>
      <pages>8487–8492</pages>
      <abstract>Continuous prompt tuning has gained significant attention for its ability to train only continuous prompts while freezing the language model. This approach greatly reduces the training time and storage for downstream tasks. In this work, we delve into the hierarchical relationship between the prompts and downstream text inputs. In prompt learning, the prefix prompt acts as a module to guide the downstream language model, establishing a hierarchical relationship between the prefix prompt and subsequent inputs. Furthermore, we explore the benefits of leveraging hyperbolic space for modeling hierarchical structures. We project representations of pre-trained models from Euclidean space into hyperbolic space using the Poincaré disk which effectively captures the hierarchical relationship between the prompt and input text. The experiments on natural language understanding (NLU) tasks illustrate that hyperbolic space can model the hierarchical relationship between prompt and text input. We release our code at <url>https://github.com/myaxxxxx/Hyperbolic-Prompt-Learning</url>.</abstract>
      <url hash="87ef272c">2024.lrec-main.744</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d15c4733">2024.lrec-main.744.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>chen-etal-2024-hyperbolic-representations</bibkey>
    </paper>
    <paper id="745">
      <title>Hypergraph-Based Session Modeling: A Multi-Collaborative Self-Supervised Approach for Enhanced Recommender Systems</title>
      <author><first>Xiangping</first><last>Zheng</last></author>
      <author><first>Bo</first><last>Wu</last></author>
      <author><first>Alex X.</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <pages>8493–8504</pages>
      <abstract>Session-based recommendation (SBR) is a challenging task that involves predicting a user’s next item click based on their recent session history. Presently, many state-of-the-art methodologies employ graph neural networks to model item transitions. Notwithstanding their impressive performance, graph-based models encounter significant challenges when confronted with intricate session dependencies and data sparsity in real-world scenarios, ultimately constraining their capacity to enhance recommendation accuracy. In recognition of these challenges, we introduce an innovative methodology known as ‘Mssen,’ which stands for Multi-collaborative self-supervised learning in hypergraph neural networks. Mssen is meticulously crafted to adeptly discern user intent. Our approach initiates by representing session-based data as a hypergraph, adeptly capturing intricate, high-order relationships. Subsequently, we employ self-supervised learning on item-session hypergraphs to mitigate the challenges of data sparsity, all without necessitating manual fine-tuning, extensive search, or domain-specific expertise in augmentation selection. Comprehensive experimental analyses conducted across multiple datasets consistently underscore the superior performance of our approach when compared to existing methodologies.</abstract>
      <url hash="35281394">2024.lrec-main.745</url>
      <bibkey>zheng-etal-2024-hypergraph-based</bibkey>
    </paper>
    <paper id="746">
      <title><fixed-case>H</fixed-case>yper<fixed-case>MR</fixed-case>: Hyperbolic Hypergraph Multi-hop Reasoning for Knowledge-based Visual Question Answering</title>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Fuyong</first><last>Xu</last></author>
      <author><first>Peiyu</first><last>Liu</last></author>
      <author><first>Zhenfang</first><last>Zhu</last></author>
      <pages>8505–8515</pages>
      <abstract>Knowledge-based Visual Question Answering (KBVQA) is a challenging task, which aims to answer an image related question based on external knowledge. Most of the works describe the semantic distance using the actual Euclidean distance between two nodes, which leads to distortion in modeling knowledge graphs with hierarchical and scale-free structure in KBVQA, and limits the multi-hop reasoning capability of the model. In contrast, the hyperbolic space shows exciting prospects for low-distortion embedding of graphs with hierarchical and free-scale structure. In addition, we map the different stages of reasoning into multiple adjustable hyperbolic spaces, achieving low-distortion, fine-grained reasoning. Extensive experiments on the KVQA, PQ and PQL datasets demonstrate the effectiveness of HyperMR for strong-hierarchy knowledge graphs.</abstract>
      <url hash="e79d2d90">2024.lrec-main.746</url>
      <bibkey>wang-etal-2024-hypermr-hyperbolic</bibkey>
    </paper>
    <paper id="747">
      <title><fixed-case>HYPERTTS</fixed-case>: Parameter Efficient Adaptation in Text to Speech Using Hypernetworks</title>
      <author><first>Yingting</first><last>Li</last></author>
      <author><first>Rishabh</first><last>Bhardwaj</last></author>
      <author><first>Ambuj</first><last>Mehrish</last></author>
      <author><first>Bo</first><last>Cheng</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>8516–8527</pages>
      <abstract>Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal from the text domain to the speech domain. While developing TTS architectures that train and test on the same set of speakers has seen significant improvements, out-of-domain speaker performance still faces enormous limitations. Domain adaptation on a new set of speakers can be achieved by fine-tuning the whole model for each new domain, thus making it parameter-inefficient. This problem can be solved by Adapters that provide a parameter-efficient alternative to domain adaptation. Although famous in NLP, speech synthesis has not seen much improvement from Adapters. In this work, we present <b>HyperTTS</b>, which comprises a small learnable network, “hypernetwork”, that generates parameters of the Adapter blocks, allowing us to condition Adapters on speaker representations and making them dynamic. Extensive evaluations of two domain adaptation settings demonstrate its effectiveness in achieving state-of-the-art performance in the parameter-efficient regime. We also compare different variants of , comparing them with baselines in different studies. Promising results on the dynamic adaptation of adapter parameters using hypernetworks open up new avenues for domain-generic multi-speaker TTS systems. The audio samples and code are available at <url>https://github.com/declare-lab/HyperTTS</url>.</abstract>
      <url hash="4b9be743">2024.lrec-main.747</url>
      <bibkey>li-etal-2024-hypertts-parameter</bibkey>
    </paper>
    <paper id="748">
      <title><fixed-case>HYRR</fixed-case>: Hybrid Infused Reranking for Passage Retrieval</title>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Keith</first><last>Hall</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Jianmo</first><last>Ni</last></author>
      <pages>8528–8534</pages>
      <abstract>Existing passage retrieval systems typically adopt a two-stage retrieve-then-rerank pipeline. To obtain an effective reranking model, many prior works have focused on improving the model architectures, such as leveraging powerful pretrained large language models (LLM) and designing better objective functions. However, less attention has been paid to the issue of collecting high-quality training data. In this paper, we propose HYRR, a framework for training robust reranking models. Specifically, we propose a simple but effective approach to select training data using hybrid retrievers. Our experiments show that the rerankers trained with HYRR are robust to different first-stage retrievers. Moreover, evaluations using MS MARCO and BEIR data sets demonstrate our proposed framework effectively generalizes to both supervised and zero-shot retrieval settings.</abstract>
      <url hash="ccf87e5b">2024.lrec-main.748</url>
      <bibkey>lu-etal-2024-hyrr-hybrid</bibkey>
    </paper>
    <paper id="749">
      <title><fixed-case>IAD</fixed-case>: In-Context Learning Ability Decoupler of Large Language Models in Meta-Training</title>
      <author><first>Yuhan</first><last>Liu</last></author>
      <author><first>Xiuying</first><last>Chen</last></author>
      <author><first>Gao</first><last>Xing</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>8535–8545</pages>
      <abstract>Large Language Models (LLMs) exhibit remarkable In-Context Learning (ICL) ability, where the model learns tasks from prompts consisting of input-output examples. However, the pre-training objectives of LLMs often misalign with ICL objectives. They’re mainly pre-trained with methods like masked language modeling and next-sentence prediction. On the other hand, ICL leverages example pairs to guide the model in generating task-aware responses such as text classification and question-answering tasks. The basic pre-training task-related capabilities can sometimes overshadow or conflict with task-specific subtleties required in ICL. To address this, we propose an In-context learning Ability Decoupler (IAD). The model aims to separate the ICL ability from the general ability of LLMs in the meta-training phase, where the ICL-related parameters are separately tuned to adapt for ICL tasks. Concretely, we first identify the parameters that are suitable for ICL by transference-driven gradient importance. We then propose a new max-margin loss to emphasize the separation of the general and ICL abilities. The loss is defined as the difference between the output of ICL and the original LLM, aiming to prevent the overconfidence of the LLM. By meta-training these ICL-related parameters with max-margin loss, we enable the model to learn and adapt to new tasks with limited data effectively. Experimental results show that IAD’s capability yields state-of-the-art performance on benchmark datasets by utilizing only 30% of the model’s parameters. Ablation study and detailed analysis prove the separation of the two abilities.</abstract>
      <url hash="8914a598">2024.lrec-main.749</url>
      <bibkey>liu-etal-2024-iad-context</bibkey>
    </paper>
    <paper id="750">
      <title><fixed-case>IDC</fixed-case>: Boost Text-to-image Retrieval via Indirect and Direct Connections</title>
      <author><first>Guowei</first><last>Ge</last></author>
      <author><first>Kuangrong</first><last>Hao</last></author>
      <author><first>Lingguang</first><last>Hao</last></author>
      <pages>8546–8555</pages>
      <abstract>The Dual Encoders (DE) framework maps image and text inputs into a coordinated representation space, and calculates their similarity directly. On the other hand, the Cross Attention (CA) framework performs modalities interactions after completing the feature embedding of images and text, and then outputs a similarity score. For scenarios with bulk query requests or large query sets, the latter is more accurate, but the former is faster. Therefore, this work finds a new way to improve the retrieval accuracy of the DE framework by borrowing the advantages of the CA framework. Drawing inspiration from image captioning, we introduce a text decoder in the model training stage to simulate the cross-modal interaction function, like the CA framework. The text decoder is eventually discarded, aligning our model with the DE framework. Finally, to ensure training stability and prevent overfitting, we modify the Self-Distillation from Last Mini-Batch and apply it to the retrieval areas. Extensive experiments conducted on the MSCOCO and Flickr30K datasets validate the effectiveness of our proposed methods. Notably, our model achieves competitive results compared to state-of-the-art approaches on the Flickr30K dataset.</abstract>
      <url hash="76dc3090">2024.lrec-main.750</url>
      <bibkey>ge-etal-2024-idc-boost</bibkey>
    </paper>
    <paper id="751">
      <title><fixed-case>IDEATE</fixed-case>: Detecting <fixed-case>AI</fixed-case>-Generated Text Using Internal and External Factual Structures</title>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Licheng</first><last>Zhang</last></author>
      <author><first>Zikang</first><last>Guo</last></author>
      <author><first>Zhendong</first><last>Mao</last></author>
      <pages>8556–8568</pages>
      <abstract>The effective detection of AI-generated text is a vital principle to ensure responsible use of large language models (LLMs). Previous studies mainly focused on discovering and utilizing internal evidences contained in the text itself to perform the detection, while ignoring external evidences implicated in an established knowledge graph (KG) which may also be key discriminative factors between AI-generated and human-written text. To address this deficiency, we propose IDEATE, a novel hierarchical graph network that utilizes both internal and external factual structures to detect AI-generated text. IDEATE consists of a mention-level subgraph at the bottom to describe internal factual structures of mentioned entities reflected in the input text, and an entity-level subgraph at the top to describe external factual structures of mentioned entities reflected in an external KG. Hierarchical graph convolution is then applied successively on the two subgraphs, through which the two types of factual structures will be embedded into the output and used for the final detection. Extensive experiments on four benchmarking datasets show that IDEATE consistently outperforms current state-of-the-art methods in detecting text generated by various LLMs, ranging from GPT-2 to the more powerful ChatGPT, verifying the necessity and superiority of introducing external evidences for AI-generated text detection.</abstract>
      <url hash="0b544924">2024.lrec-main.751</url>
      <bibkey>wang-etal-2024-ideate-detecting</bibkey>
    </paper>
    <paper id="752">
      <title><fixed-case>IDEM</fixed-case>: The <fixed-case>ID</fixed-case>ioms with <fixed-case>EM</fixed-case>otions Dataset for Emotion Recognition</title>
      <author><first>Alexander</first><last>Prochnow</last></author>
      <author><first>Johannes E.</first><last>Bendler</last></author>
      <author><first>Caroline</first><last>Lange</last></author>
      <author><first>Foivos Ioannis</first><last>Tzavellos</last></author>
      <author><first>Bas Marco</first><last>Göritzer</last></author>
      <author><first>Marijn</first><last>ten Thij</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>8569–8579</pages>
      <abstract>Idiomatic expressions are used in everyday language and typically convey affect, i.e., emotion. However, very little work investigating the extent to which automated methods can recognise emotions expressed in idiom-containing text has been undertaken. This can be attributed to the lack of emotion-labelled datasets that support the development and evaluation of such methods. In this paper, we present the IDioms with EMotions (IDEM) dataset consisting of a total of 9685 idiom-containing sentences that were generated and labelled with any one of 36 emotion types, with the help of the GPT-4 generative language model. Human validation by two independent annotators showed that more than 51% of the generated sentences are ideal examples, with the annotators reaching an agreement rate of 62% measured in terms of Cohen’s Kappa coefficient. To establish baseline performance on IDEM, various transformer-based emotion recognition approaches were implemented and evaluated. Results show that a RoBERTa model fine-tuned as a sequence classifier obtains a weighted F1-score of 58.73%, when the sequence provided as input specifies the idiom contained in a given sentence, together with its definition. Since this input configuration is based on the assumption that the idiom contained in the given sentence is already known, we also sought to assess the feasibility of automatically identifying the idioms contained in IDEM sentences. To this end, a hybrid idiom identification approach combining a rule-based method and a deep learning-based model was developed, whose performance on IDEM was determined to be 84.99% in terms of F1-score.</abstract>
      <url hash="298d39e7">2024.lrec-main.752</url>
      <bibkey>prochnow-etal-2024-idem-idioms</bibkey>
    </paper>
    <paper id="753">
      <title>Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence</title>
      <author><first>Anthony James</first><last>Hughes</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <pages>8580–8593</pages>
      <abstract>Evidence-based medicine is the practise of making medical decisions that adhere to the latest, and best known evidence at that time. Currently, the best evidence is often found in the form of documents, such as randomized control trials, meta-analyses and systematic reviews. This research focuses on aligning medical claims made on social media platforms with this medical evidence. By doing so, individuals without medical expertise can more effectively assess the veracity of such medical claims. We study three core tasks: identifying medical claims, extracting medical vocabulary from these claims, and retrieving evidence relevant to those identified medical claims. We propose a novel system that can generate synthetic medical claims to aid each of these core tasks. We additionally introduce a novel dataset produced by our synthetic generator that, when applied to these tasks, demonstrates not only a more flexible and holistic approach, but also an improvement in all comparable metrics. We make our dataset, the Expansive Medical Claim Corpus (EMCC), available at https://zenodo.org/records/8321460.</abstract>
      <url hash="76862d6d">2024.lrec-main.753</url>
      <bibkey>hughes-song-2024-identifying-aligning</bibkey>
    </paper>
    <paper id="754">
      <title>Identifying Fine-grained Depression Signs in Social Media Posts</title>
      <author><first>Augusto R.</first><last>Mendes</last></author>
      <author><first>Helena</first><last>Caseli</last></author>
      <pages>8594–8604</pages>
      <abstract>Natural Language Processing has already proven to be an effective tool for helping in the identification of mental health disorders in text. However, most studies limit themselves to a binary classification setup or base their label set on pre-established resources. By doing so, they don’t explicitly model many common ways users can express their depression online, limiting our understanding of what kind of depression signs such models can accurately classify. This study evaluates how machine learning techniques deal with the classification of a fine-grained set of 21 depression signs in social media posts from Brazilian undergraduate students. We found out that model performance is not necessarily driven by a depression sign’s frequency on social media posts, since evaluated machine learning techniques struggle to classify the majority of signs of depression typically present in posts. Thus, model performance seems to be more related to the inherent difficulty of identifying a given sign than with its occurrence frequency.</abstract>
      <url hash="caef3ab4">2024.lrec-main.754</url>
      <bibkey>mendes-caseli-2024-identifying-fine</bibkey>
    </paper>
    <paper id="755">
      <title>Identifying Source Language Expressions for Pre-editing in Machine Translation</title>
      <author><first>Norizo</first><last>Sakaguchi</last></author>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>8605–8616</pages>
      <abstract>Machine translation-mediated communication can benefit from pre-editing source language texts to ensure accurate transmission of intended meaning in the target language. The primary challenge lies in identifying source language expressions that pose difficulties in translation. In this paper, we hypothesize that such expressions tend to be distinctive features of texts originally written in the source language (native language) rather than translations generated from the target language into the source language (machine translation). To identify such expressions, we train a neural classifier to distinguish native language from machine translation, and subsequently isolate the expressions that contribute to the model’s prediction of native language. Our manual evaluation revealed that our method successfully identified characteristic expressions of the native language, despite the noise and the inherent nuances of the task. We also present case studies where we edit the identified expressions to improve translation quality.</abstract>
      <url hash="b02740a2">2024.lrec-main.755</url>
      <bibkey>sakaguchi-etal-2024-identifying-source</bibkey>
    </paper>
    <paper id="756">
      <title>Ideological Knowledge Representation: Framing Climate Change in <fixed-case>E</fixed-case>co<fixed-case>L</fixed-case>exicon</title>
      <author><first>Arianne</first><last>Reimerink</last></author>
      <author><first>Melania</first><last>Cabezas-García</last></author>
      <author><first>Pilar</first><last>León-Araúz</last></author>
      <author><first>Pamela</first><last>Faber</last></author>
      <pages>8617–8626</pages>
      <abstract>Culture is underrepresented in terminological resources and ideology is an especially complicated cultural aspect to convey. This complexity stems from the intertwined relationships among the discourse community of politicians, the media and the general public, as well as their interactions with scientific knowledge. Nevertheless, terminological resources should provide the necessary information to understand the political perspective taken in discourse on scientific issues with a high political profile. As in all specialized domains, environmental concepts and terms are subject to dynamism and variation (León-Araúz, 2017). Cognitive term variants (e.g., climate change, climate crisis) are of particular interest because of their presence in political discourse and their potential to influence climate actions. They can be used to reflect multidimensionality, imprecision or ideological attachment. This paper describes a method based on framing in Communication Studies to extract ideological knowledge from corpora. We used Spanish and English parliamentary debates (ParlaMint 2.1) and annotated the interventions that included a term variant of climate change according to an adapted version of the frames proposed by Bolsen and Shapiro (2018). The results showed how climate change discourse changes across de ideological spectrum and we give a proposal on how to represent that knowledge in an environmental TKB on the environment.</abstract>
      <url hash="87c88f37">2024.lrec-main.756</url>
      <bibkey>reimerink-etal-2024-ideological-knowledge</bibkey>
    </paper>
    <paper id="757">
      <title><fixed-case>ILC</fixed-case>ite<fixed-case>R</fixed-case>: Evidence-grounded Interpretable Local Citation Recommendation</title>
      <author><first>Sayar</first><last>Ghosh Roy</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>8627–8638</pages>
      <abstract>Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training. We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans.</abstract>
      <url hash="5d13f147">2024.lrec-main.757</url>
      <bibkey>ghosh-roy-han-2024-ilciter-evidence</bibkey>
    </paper>
    <paper id="758">
      <title><fixed-case>ILLUMINER</fixed-case>: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler</title>
      <author><first>Paramita</first><last>Mirza</last></author>
      <author><first>Viju</first><last>Sudhi</last></author>
      <author><first>Soumya Ranjan</first><last>Sahoo</last></author>
      <author><first>Sinchana Ramakanth</first><last>Bhat</last></author>
      <pages>8639–8651</pages>
      <abstract>State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1–32.2 percentage points. Additionally, our in-depth ablation study demonstrates that parameter-efficient fine-tuning requires less than 6% of training data to yield comparable performance with traditional full-weight fine-tuning.</abstract>
      <url hash="1be9019c">2024.lrec-main.758</url>
      <bibkey>mirza-etal-2024-illuminer-instruction</bibkey>
    </paper>
    <paper id="759">
      <title>Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection</title>
      <author><first>Huixuan</first><last>Zhang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>8652–8661</pages>
      <abstract>Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection of hyperbole is an important part of understanding human expression. There have been several studies on hyperbole detection, but most of which focus on text modality only. However, with the development of social media, people can create hyperbolic expressions with various modalities, including text, images, videos, etc. In this paper, we focus on multimodal hyperbole detection. We create a multimodal detection dataset from Weibo (a Chinese social media) and carry out some studies on it. We treat the text and image from a piece of weibo as two modalities and explore the role of text and image for hyperbole detection. Different pre-trained multimodal encoders are also evaluated on this downstream task to show their performance. Besides, since this dataset is constructed from five different keywords, we also evaluate the cross-domain performance of different models. These studies can serve as a benchmark and point out the direction of further study on multimodal hyperbole detection.</abstract>
      <url hash="c5942281">2024.lrec-main.759</url>
      <bibkey>zhang-wan-2024-image-matters</bibkey>
    </paper>
    <paper id="760">
      <title>Impact of Task Adapting on Transformer Models for Targeted Sentiment Analysis in <fixed-case>C</fixed-case>roatian Headlines</title>
      <author><first>Sofia</first><last>Lee</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <pages>8662–8674</pages>
      <abstract>Transformer models, such as BERT, are often taken off-the-shelf and then fine-tuned on a downstream task. Although this is sufficient for many tasks, low-resource settings require special attention. We demonstrate an approach of performing an extra stage of self-supervised task-adaptive pre-training to a number of Croatian-supporting Transformer models. In particular, we focus on approaches to language, domain, and task adaptation. The task in question is targeted sentiment analysis for Croatian news headlines. We produce new state-of-the-art results (F1 = 0.781), but the highest performing model still struggles with irony and implicature. Overall, we find that task-adaptive pre-training benefits massively multilingual models but not Croatian-dominant models.</abstract>
      <url hash="e7447c18">2024.lrec-main.760</url>
      <bibkey>lee-bloem-2024-impact-task</bibkey>
    </paper>
    <paper id="761">
      <title>Impoverished Language Technology: The Lack of (Social) Class in <fixed-case>NLP</fixed-case></title>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Zeerak</first><last>Talat</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>8675–8682</pages>
      <abstract>Since Labov’s foundational 1964 work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov’s initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.</abstract>
      <url hash="de50d13a">2024.lrec-main.761</url>
      <bibkey>cercas-curry-etal-2024-impoverished-language</bibkey>
    </paper>
    <paper id="762">
      <title>Improved Neural Protoform Reconstruction via Reflex Prediction</title>
      <author><first>Liang</first><last>Lu</last></author>
      <author><first>Jingzhi</first><last>Wang</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <pages>8683–8707</pages>
      <abstract>Protolanguage reconstruction is central to historical linguistics. The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change. Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem. We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms. Leveraging another line of research—reflex prediction—we propose a system in which candidate protoforms from a reconstruction model are reranked by a reflex prediction model. We show that this more complete implementation of the comparative method allows us to surpass state-of-the-art protoform reconstruction methods on three of four Chinese and Romance datasets.</abstract>
      <url hash="50c7df19">2024.lrec-main.762</url>
      <bibkey>lu-etal-2024-improved-neural</bibkey>
    </paper>
    <paper id="763">
      <title>Improved Out-of-Scope Intent Classification with Dual Encoding and Threshold-based Re-Classification</title>
      <author><first>Hossam</first><last>Zawbaa</last></author>
      <author><first>Wael</first><last>Rashwan</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Haytham</first><last>Assem</last></author>
      <pages>8708–8718</pages>
      <abstract>Detecting out-of-scope user utterances is essential for task-oriented dialogues and intent classification. Current methodologies face difficulties with the unpredictable distribution of outliers and often rely on assumptions about data distributions. We present the Dual Encoder for Threshold-Based Re-Classification (DETER) to address these challenges. This end-to-end framework efficiently detects out-of-scope intents without requiring assumptions on data distributions or additional post-processing steps. The core of DETER utilizes dual text encoders, the Universal Sentence Encoder (USE) and the Transformer-based Denoising AutoEncoder (TSDAE), to generate user utterance embeddings, which are classified through a branched neural architecture. Further, DETER generates synthetic outliers using self-supervision and incorporates out-of-scope phrases from open-domain datasets. This approach ensures a comprehensive training set for out-of-scope detection. Additionally, a threshold-based re-classification mechanism refines the model’s initial predictions. Evaluations on the CLINC-150, Stackoverflow, and Banking77 datasets demonstrate DETER’s efficacy. Our model outperforms previous benchmarks, achieving an increase of up to 13% and 5% in F1 score for known and unknown intents on CLINC-150 and Stackoverflow, and 16% for known and 24% for unknown intents on Banking77. The source code has been released at https://github.com/Hossam-Mohammed-tech/Intent_Classification_OOS.</abstract>
      <url hash="481ad0c6">2024.lrec-main.763</url>
      <bibkey>zawbaa-etal-2024-improved-scope</bibkey>
    </paper>
    <paper id="764">
      <title>Improving <fixed-case>B</fixed-case>engali and <fixed-case>H</fixed-case>indi Large Language Models</title>
      <author><first>Arif</first><last>Shahriar</last></author>
      <author><first>Denilson</first><last>Barbosa</last></author>
      <pages>8719–8731</pages>
      <abstract>Despite being widely spoken worldwide, Bengali and Hindi are low-resource languages. The state-of-the-art in modeling such languages uses BERT and the Wordpiece tokenizer. We observed that the Wordpiece tokenizer often breaks words into meaningless tokens, failing to separate roots from affixes. Moreover, Wordpiece does not take into account fine-grained character-level information. We hypothesize that modeling fine-grained character-level information or interactions between roots and affixes helps with modeling highly inflected and morphologically complex languages such as Bengali and Hindi. We used BERT with two different tokenizers - a Unigram tokenizer and a character-level tokenizer and observed better performance. Then, we pretrained four language models accordingly - Bengali Unigram BERT, Hindi Unigram BERT, Bengali Character BERT, and Hindi Character BERT, and evaluated them for masked token detection, both in correct and erroneous settings, across many NLU tasks. We provide experimental evidence that Unigram and character-level tokenizers lead to better pretrained models for Bengali and Hindi, outperforming the previous state-of-the-art and BERT with Wordpiece vocabulary. We conduct the first study investigating the efficacy of different tokenization methods in modeling Bengali and Hindi.</abstract>
      <url hash="2479af17">2024.lrec-main.764</url>
      <bibkey>shahriar-barbosa-2024-improving-bengali</bibkey>
    </paper>
    <paper id="765">
      <title>Improving <fixed-case>C</fixed-case>hinese Named Entity Recognition with Multi-grained Words and Part-of-Speech Tags via Joint Modeling</title>
      <author><first>Chenhui</first><last>Dou</last></author>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>8732–8742</pages>
      <abstract>Nowadays, character-based sequence labeling becomes the mainstream Chinese named entity recognition (CNER) approach, instead of word-based methods, since the latter degrades performance due to propagation of word segmentation (WS) errors. To make use of WS information, previous studies usually learn CNER and WS simultaneously with multi-task learning (MTL) framework, or treat WS information as extra guide features for CNER model, in which the utilization of WS information is indirect and shallow. In light of the complementary information inside multi-grained words, and the close connection between named entities and part-of-speech (POS) tags, this work proposes a tree parsing approach for joint modeling CNER, multi-grained word segmentation (MWS) and POS tagging tasks simultaneously. Specifically, we first propose a unified tree representation for MWS, POS tagging, and CNER.Then, we automatically construct the MWS-POS-NER data based on the unified tree representation for model training. Finally, we present a two-stage joint tree parsing framework. Experimental results on OntoNotes4 and OntoNotes5 show that our proposed approach of jointly modeling CNER with MWS and POS tagging achieves better or comparable performance with latest methods.</abstract>
      <url hash="ff98ac21">2024.lrec-main.765</url>
      <bibkey>dou-etal-2024-improving-chinese</bibkey>
    </paper>
    <paper id="766">
      <title>Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users</title>
      <author><first>Yejin</first><last>Kim</last></author>
      <author><first>Scott</first><last>Rome</last></author>
      <author><first>Kevin</first><last>Foley</last></author>
      <author><first>Mayur</first><last>Nankani</last></author>
      <author><first>Rimon</first><last>Melamed</last></author>
      <author><first>Javier</first><last>Morales</last></author>
      <author><first>Abhay K.</first><last>Yadav</last></author>
      <author><first>Maria</first><last>Peifer</last></author>
      <author><first>Sardar</first><last>Hamidian</last></author>
      <author><first>H. Howie</first><last>Huang</last></author>
      <pages>8743–8755</pages>
      <abstract>Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding. Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals. A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task. It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through rate, Recall, etc. In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions. We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata. Our approach allows the model to better understand the relationships between entities within the knowledge graph by utilizing semantic information from text. It leads to more accurate, relevant, and diverse user recommendations and a benefit that extends even to cold-start users who have few interactions with items. We perform extensive experiments on two widely used datasets to validate the effectiveness of our approach. Our findings demonstrate that jointly training user-item interactions and item-based signals using synopsis text is highly effective. Furthermore, our results provide evidence that item-based contrastive learning enhances the quality of entity embeddings, as indicated by metrics such as uniformity and alignment.</abstract>
      <url hash="800cc651">2024.lrec-main.766</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f6581470">2024.lrec-main.766.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kim-etal-2024-improving-content</bibkey>
    </paper>
    <paper id="767">
      <title>Improving Continual Few-shot Relation Extraction through Relational Knowledge Distillation and Prototype Augmentation</title>
      <author><first>Zhiheng</first><last>Zhang</last></author>
      <author><first>Daojian</first><last>Zeng</last></author>
      <author><first>Xue</first><last>Bai</last></author>
      <pages>8756–8767</pages>
      <abstract>In this paper, we focus on the challenging yet practical problem of Continual Few-shot Relation Extraction (CFRE), which involves extracting relations in the continuous and iterative arrival of new data with only a few labeled examples. The main challenges in CFRE are overfitting due to few-shot learning and catastrophic forgetting caused by continual learning. To address these problems, we propose a novel framework called RK2DA, which seamlessly integrates prototype-based data augmentation and relational knowledge distillation. Specifically, RK2DA generates pseudo data by introducing Gaussian noise to the prototype embeddings and utilizes a novel two-phase multi-teacher relational knowledge distillation method to transfer various knowledge from different embedding spaces. Experimental results on the FewRel and TACRED datasets demonstrate that our method outperforms the state-of-the-art baselines.</abstract>
      <url hash="7f7b891a">2024.lrec-main.767</url>
      <bibkey>zhang-etal-2024-improving-continual-shot</bibkey>
    </paper>
    <paper id="768">
      <title>Improving Copy-oriented Text Generation via <fixed-case>EDU</fixed-case> Copy Mechanism</title>
      <author><first>Tianxiang</first><last>Wu</last></author>
      <author><first>Han</first><last>Chen</last></author>
      <author><first>Luozheng</first><last>Qin</last></author>
      <author><first>Ziqiang</first><last>Cao</last></author>
      <author><first>Chunhui</first><last>Ai</last></author>
      <pages>8768–8780</pages>
      <abstract>Many text generation tasks are copy-oriented. For instance, nearly 30% content of news summaries is copied. The copy rate is even higher in Grammatical Error Correction (GEC). However, existing generative models generate texts through word-by-word decoding, which may lead to factual inconsistencies and slow inference. While Elementary Discourse Units (EDUs) are outstanding extraction units, EDU-based extractive methods can alleviate the aforementioned problems. As a consequence, we propose EDUCopy, a framework that integrates the behavior of copying EDUs into generative models. The main idea of EDUCopy is to use special index tags to represent the copied EDUs during generation. Specifically, we extract important EDUs from input sequences, finetune generative models to generate sequences with special index tags, and restore the generated special index tags into corresponding text spans. By doing so, EDUCopy reduces the number of generated tokens significantly. To verify the effectiveness of EDUCopy, we conduct experiments on the news summarization datasets CNNDM, NYT and the GEC datasets FCE, WI-LOCNESS. While achieving notable ROUGE and M2 scores, GPT-4 evaluation validates the strength of our models in terms of factual consistency, fluency, and overall performance. Moreover, compared to baseline models, EDUCopy achieves a significant acceleration of 1.65x.</abstract>
      <url hash="b639d3ac">2024.lrec-main.768</url>
      <bibkey>wu-etal-2024-improving-copy</bibkey>
    </paper>
    <paper id="769">
      <title>Improving Cross-lingual Transfer with Contrastive Negative Learning and Self-training</title>
      <author><first>Guanlin</first><last>Li</last></author>
      <author><first>Xuechen</first><last>Zhao</last></author>
      <author><first>Amir</first><last>Jafari</last></author>
      <author><first>Wenhao</first><last>Shao</last></author>
      <author><first>Reza</first><last>Farahbakhsh</last></author>
      <author><first>Noel</first><last>Crespi</last></author>
      <pages>8781–8791</pages>
      <abstract>Recent studies improve the cross-lingual transfer learning by better aligning the internal representations within the multilingual model or exploring the information of the target language using self-training. However, the alignment-based methods exhibit intrinsic limitations such as non-transferable linguistic elements, while most of the self-training based methods ignore the useful information hidden in the low-confidence samples. To address this issue, we propose CoNLST (Contrastive Negative Learning and Self-Training) to leverage the information of low-confidence samples. Specifically, we extend the negative learning to the metric space by selecting negative pairs based on the complementary labels and then employ self-training to iteratively train the model to converge on the obtained clean pseudo-labels. We evaluate our approach on the widely-adopted cross-lingual benchmark XNLI. The experiment results show that our method improves upon the baseline models and can serve as a beneficial complement to the alignment-based methods.</abstract>
      <url hash="5308e7de">2024.lrec-main.769</url>
      <bibkey>li-etal-2024-improving-cross-lingual-transfer</bibkey>
    </paper>
    <paper id="770">
      <title>Improving Factual Consistency in Abstractive Summarization with Sentence Structure Pruning</title>
      <author><first>Dingxin</first><last>Hu</last></author>
      <author><first>Xuanyu</first><last>Zhang</last></author>
      <author><first>Xingyue</first><last>Zhang</last></author>
      <author><first>Yiyang</first><last>Li</last></author>
      <author><first>Dongsheng</first><last>Chen</last></author>
      <author><first>Marina</first><last>Litvak</last></author>
      <author><first>Natalia</first><last>Vanetik</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <author><first>Yanquan</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yuze</first><last>Li</last></author>
      <author><first>Yingqi</first><last>Zhu</last></author>
      <pages>8792–8803</pages>
      <abstract>State-of-the-art abstractive summarization models still suffer from the content contradiction between the summaries and the input text, which is referred to as the factual inconsistency problem. Recently, a large number of works have also been proposed to evaluate factual consistency or improve it by post-editing methods. However, these post-editing methods typically focus on replacing suspicious entities, failing to identify and modify incorrect content hidden in sentence structures. In this paper, we first verify that the correctable errors can be enriched by leveraging sentence structure pruning operation, and then we propose a post-editing method based on that. In the correction process, the pruning operation on possible errors is performed on the syntactic dependency tree with the guidance of multiple factual evaluation metrics. Experimenting on the FRANK dataset shows a great improvement in factual consistency compared with strong baselines and, when combined with them, can achieve even better performance. All the codes and data will be released on paper acceptance.</abstract>
      <url hash="79eb3c93">2024.lrec-main.770</url>
      <bibkey>hu-etal-2024-improving-factual</bibkey>
    </paper>
    <paper id="771">
      <title>Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency</title>
      <author><first>Taiji</first><last>Li</last></author>
      <author><first>Zhi</first><last>Li</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <pages>8804–8817</pages>
      <abstract>Despite large language models (LLMs) have demonstrated impressive performance in various tasks, they are still suffering from the factual inconsistency problem called hallucinations. For instance, LLMs occasionally generate content that diverges from source article, and prefer to extract information that appears at the beginning and end of the context, especially in long document summarization. Inspired by these findings, we propose to improve the faithfulness of LLMs in summarization by impelling them to process the entire article more fairly and faithfully. We present a novel summary generation strategy, namely SliSum, which exploits the ideas of sliding windows and self-consistency. Specifically, SliSum divides the source article into overlapping windows, and utilizes LLM to generate local summaries for the content in the windows. Finally, SliSum aggregates all local summaries using clustering and majority voting algorithm to produce more faithful summary of entire article. Extensive experiments demonstrate that SliSum significantly improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and GPT-3.5 in both short and long text summarization, while maintaining their fluency and informativeness and without additional fine-tuning and resources. We further conduct qualitative and quantitative studies to investigate why SliSum works and impacts of hyperparameters in SliSum on performance.</abstract>
      <url hash="e09d064e">2024.lrec-main.771</url>
      <bibkey>li-etal-2024-improving-faithfulness-large</bibkey>
    </paper>
    <paper id="772">
      <title>Improving Grammatical Error Correction by Correction Acceptability Discrimination</title>
      <author><first>Bin</first><last>Cao</last></author>
      <author><first>Kai</first><last>Jiang</last></author>
      <author><first>Fayu</first><last>Pan</last></author>
      <author><first>Chenlei</first><last>Bao</last></author>
      <author><first>Jing</first><last>Fan</last></author>
      <pages>8818–8827</pages>
      <abstract>Existing Grammatical Error Correction (GEC) methods often overlook the assessment of sentence-level syntax and semantics in the corrected sentence. This oversight results in final corrections that may not be acceptable in the context of the original sentence. In this paper, to improve the performance of Grammatical Error Correction methods, we propose the post-processing task of Correction Acceptability Discrimination (CAD) which aims to remove invalid corrections by comparing the source sentence and its corrected version from the perspective of “sentence-level correctness”. To solve the CAD task, we propose a pipeline method where the acceptability of each possible correction combination based on the predicted corrections for a source sentence will be judged by a discriminator. Within the discriminator, we design a symmetrical comparison operator to overcome the conflicting results that might be caused by the sentence concatenation order. Experiments show that our method can averagely improve <tex-math>F_{0.5}</tex-math> score by 1.01% over 13 GEC systems in the BEA-2019 test set.</abstract>
      <url hash="f134bd27">2024.lrec-main.772</url>
      <bibkey>cao-etal-2024-improving-grammatical</bibkey>
    </paper>
    <paper id="773">
      <title>Improving Implicit Discourse Relation Recognition with Semantics Confrontation</title>
      <author><first>Mingyang</first><last>Cai</last></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Ping</first><last>Jian</last></author>
      <pages>8828–8839</pages>
      <abstract>Implicit Discourse Relation Recognition (IDRR), which infers discourse logical relations without explicit connectives, is one of the most challenging tasks in natural language processing (NLP). Recently, pre-trained language models (PLMs) have yielded impressive results across numerous NLP tasks, but their performance still remains unsatisfactory in IDRR. We argue that prior studies have not fully harnessed the potential of PLMs, thereby resulting in a mixture of logical semantics, which determine the logical relations between discourse arguments, and general semantics, which encapsulate the non-logical contextual aspects (detailed in Sec.1). Such a mixture would inevitably compromise the logic reasoning ability of PLMs. Therefore, we propose a novel method that trains the PLMs through two semantics enhancers to implicitly differentiate logical and general semantics, ultimately achieving logical semantics enhancement. Due to the characteristic of PLM in word representation learning, these two semantics enhancers will inherently confront with each other, facilitating an augmentation of logical semantics by disentangling them from general semantics. The experimental results on PDTB 2.0 dataset show that the confrontation approach exceeds our baseline by 3.81% F1 score, and the effectiveness of the semantics confrontation method is validated by comprehensive ablation experiments.</abstract>
      <url hash="c6c7bb8a">2024.lrec-main.773</url>
      <bibkey>cai-etal-2024-improving-implicit</bibkey>
    </paper>
    <paper id="774">
      <title>Improving Language Model Reasoning with Self-motivated Learning</title>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>8840–8852</pages>
      <abstract>Large-scale high-quality training data is important for improving the performance of models. After trained with data that has rationales (reasoning steps), models gain reasoning capability. However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost. To address this issue, we propose <i>Self-motivated Learning</i> framework. The framework motivates the model itself to automatically generate rationales on existing datasets. Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability. Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of reasoning through reinforcement learning. Experiment results of Llama2 7B on multiple reasoning datasets show that our method significantly improves the reasoning ability of models, even outperforming InstructGPT in some datasets.</abstract>
      <url hash="9796526b">2024.lrec-main.774</url>
      <bibkey>feng-etal-2024-improving-language</bibkey>
    </paper>
    <paper id="775">
      <title>Improving Low-Resource Keyphrase Generation through Unsupervised Title Phrase Generation</title>
      <author><first>Byungha</first><last>Kang</last></author>
      <author><first>Youhyun</first><last>Shin</last></author>
      <pages>8853–8865</pages>
      <abstract>This paper introduces a novel approach called title phrase generation (TPG) for unsupervised keyphrase generation (UKG), leveraging a pseudo label generated from a document title. Previous UKG method extracts all phrases from a corpus to build a phrase bank, then draws candidate absent keyphrases related to a document from the phrase bank to generate a pseudo label. However, we observed that when separating the document title from the document body, a significant number of phrases absent from the document body are included in the title. Based on this observation, we propose an effective method for generating pseudo labels using phrases mined from the document title. We initially train BART using these pseudo labels (TPG) and then perform supervised fine-tuning on a small amount of human-annotated data, which we term low-resource fine-tuning (LRFT). Experimental results on five benchmark datasets demonstrate that our method outperforms existing low-resource keyphrase generation approaches even with fewer labeled data, showing strength in generating absent keyphrases. Moreover, our model trained solely with TPG, without any labeled data, surpasses previous UKG method, highlighting the effectiveness of utilizing titles over a phrase bank. The code is available at https://github.com/kangnlp/low-resource-kpgen-through-TPG.</abstract>
      <url hash="317ab970">2024.lrec-main.775</url>
      <bibkey>kang-shin-2024-improving-low</bibkey>
    </paper>
    <paper id="776">
      <title>Improving Multi-view Document Clustering: Leveraging Multi-structure Processor and Hybrid Ensemble Clustering Module</title>
      <author><first>Ruina</first><last>Bai</last></author>
      <author><first>Qi</first><last>Bai</last></author>
      <pages>8866–8876</pages>
      <abstract>We introduce a multi-view document clustering model called DMsECN (Deep Multi-structure Ensemble Clustering Network), comprising a multi-structure processor and a hybrid ensemble clustering module. Unlike existing models, DMsECN distinguishes itself by creating a consensus structure from multiple clustering structures. The multi-structure processor comprises two stages, each contributing to the extraction of clustering structures that preserve both consistency and complementarity across multiple views. Representation learning extracts both view and view-fused representations from multi-views through the use of contrastive learning. Subsequently, multi-structure learning employs distinct view clustering guidance to generate the corresponding clustering structures. The hybrid ensemble clustering module merges two ensemble methods to amalgamate multiple structures, producing a consensus structure that guarantees both the separability and compactness of clusters within the clustering results. The attention-based ensemble primarily concentrates on learning the contribution weights of diverse clustering structures, while the similarity-based ensemble employs cluster assignment similarity and cluster classification dissimilarity to guide the refinement of the consensus structure. Experimental results demonstrate that DMsECN outperforms other models, achieving new state-of-the-art results on four multi-view document clustering datasets.</abstract>
      <url hash="34e1df1b">2024.lrec-main.776</url>
      <bibkey>bai-bai-2024-improving-multi</bibkey>
    </paper>
    <paper id="777">
      <title>Improving Personalized Sentiment Representation with Knowledge-enhanced and Parameter-efficient Layer Normalization</title>
      <author><first>You</first><last>Zhang</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Liang-Chih</first><last>Yu</last></author>
      <author><first>Dan</first><last>Xu</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>8877–8889</pages>
      <abstract>Existing studies on personalized sentiment classification consider a document review as an overall text unit and incorporate backgrounds (i.e., user and product information) to learn sentiment representation. However, it is difficult when these methods meet the current pretrained language models (PLMs) owing to quadratic costs that increase with text length and heterogeneous mixes of randomly initialized background information and textual information initialized from well-pretrained checkpoints during information incorporation. To address these problems, we propose a knowledge-enhanced and parameter-efficient layer normalization (E2LN) for efficient and effective review modeling via leveraging LN in transformer structures. Initially, a knowledge base is introduced that stores well-pretrained checkpoints, structured text information, and background information. Based on such a knowledge base, the ability of LN can be magnified as being a crucial component of transformer structure and then improve the performance of PLMs in downstream tasks. Moreover, the proposed E2LN can make PLMs capable of modeling long document reviews and incorporating background information with parameter-efficient fine-tuning and knowledge injecting. Extensive experimental results were obtained for three document-level sentiment classification benchmark datasets. By comparing the results, the effectiveness and efficiency of the proposed model was demonstrated. Code and Data are released at https://github.com/yoyo-yun/E2LN.</abstract>
      <url hash="8566e83c">2024.lrec-main.777</url>
      <bibkey>zhang-etal-2024-improving-personalized-sentiment</bibkey>
    </paper>
    <paper id="778">
      <title>Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction</title>
      <author><first>Zepeng</first><last>Ding</last></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Jiaqing</first><last>Liang</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <author><first>Deqing</first><last>Yang</last></author>
      <pages>8890–8901</pages>
      <abstract>Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition. Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions. However, they often miss out when extracting from complex sentences. In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks. The framework includes an evaluation model that can extract related entity pairs with high precision. We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model. We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples. Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences.</abstract>
      <url hash="da055575">2024.lrec-main.778</url>
      <bibkey>ding-etal-2024-improving-recall</bibkey>
    </paper>
    <paper id="779">
      <title>Improving Robustness of <fixed-case>GNN</fixed-case>-based Anomaly Detection by Graph Adversarial Training</title>
      <author><first>Xiangping</first><last>Zheng</last></author>
      <author><first>Bo</first><last>Wu</last></author>
      <author><first>Alex X.</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <pages>8902–8912</pages>
      <abstract>Graph neural networks (GNNs) play a fundamental role in anomaly detection, excelling at the identification of node anomalies by aggregating information from neighboring nodes. Nonetheless, they exhibit vulnerability to attacks, with even minor alterations in the graph structure or node attributes resulting in substantial performance degradation. To address this critical challenge, we introduce an innovative mechanism for graph adversarial training, meticulously designed to bolster GNN-based anomaly detection systems against potential poisoning attacks. This novel approach follows a two-step framework. (1) In the initial phase, we employ a Multiple-Objective Generative Adversarial Attack (MO-GAA), which focuses on generating feature modifications and inducing structural disruptions within the graph. Its primary objective is to mimic the adversarial behavior of potential attackers on the anomaly detection graph, with the explicit intention of confounding the anomaly detector. (2) In the subsequent stage, we introduce Purification-Based Adversarial Attack Defense (PB-AAD), a method specifically designed to rectify any contamination and restore the integrity of the graph. The central aim of PB-AAD is to counteract the destructive actions carried out by potential attackers. Our empirical findings, derived from extensive experiments conducted on four real-world anomaly detection datasets, serve to demonstrate how MO-GAA systematically disrupts the graph, compromising the effectiveness of GNN-based detectors, while PB-AAD effectively mitigates these adversarial actions, thereby enhancing the overall robustness of GNN-based anomaly detectors.</abstract>
      <url hash="dca9106d">2024.lrec-main.779</url>
      <bibkey>zheng-etal-2024-improving-robustness</bibkey>
    </paper>
    <paper id="780">
      <title>Improving Role-Oriented Dialogue Summarization with Interaction-Aware Contrastive Learning</title>
      <author><first>Weihong</first><last>Guan</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Faliang</first><last>Huang</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <author><first>Yuan</first><last>Cui</last></author>
      <pages>8913–8924</pages>
      <abstract>Role-oriented dialogue summarization aims at generating summaries for different roles in dialogue, e.g., user and agent. Interaction between different roles is vital for the task. Existing methods could not fully capture interaction patterns between roles when encoding dialogue, thus are prone to ignore the interaction-related key information. In this paper, we propose a contrastive learning based interaction-aware model for the role-oriented dialogue summarization namely CIAM. An interaction-aware contrastive objective is constructed to guide the encoded dialogue representation to learn role-level interaction. The representation is then used by the decoder to generate role-oriented summaries. The contrastive objective is trained jointly with the primary dialogue summarization task. Additionally, we innovatively utilize different decoder start tokens to control what kind of summary to generate, thus could generate different role-oriented summaries with a unified model. Experimental results show that our method achieves new state-of-the-art results on two public datasets. Extensive analyses further demonstrate that our method excels at capturing interaction information between different roles and producing informative summaries.</abstract>
      <url hash="c3094611">2024.lrec-main.780</url>
      <bibkey>guan-etal-2024-improving-role</bibkey>
    </paper>
    <paper id="781">
      <title>Improving Text Readability through Segmentation into Rheses</title>
      <author><first>Antoine</first><last>Jamelot</last></author>
      <author><first>Solen</first><last>Quiniou</last></author>
      <author><first>Sophie</first><last>Hamon</last></author>
      <pages>8925–8930</pages>
      <abstract>Enhancing text readability is crucial for readers with challenges like dyslexia. This paper delves into the segmentation of sentences into rheses, i.e. rhythmic and semantic units. Their aim is to clarify sentence structures for improved comprehension, through a harmonious balance between syntactic accuracy, the natural rhythm of reading aloud, and the delineation of meaningful units. This study relates and compares our various attempts to improve a pre-existing rhesis segmentation tool, which is based on the selection of candidate segmentations. We also release TeRheSe (Texts with Rhesis Segmentation), a bilingual dataset, segmented into rheses, comprising 12 books from classic literature in French and English. We evaluated our approaches on this dataset, showing the efficiency of a novel approach based on token classification, reaching a F1-score of 90.0% in English (previously 85.3%) and 91.3% in French (previously 88.0%). We also study the potential of leveraging prosodic elements, though its definitive impact remains inconclusive.</abstract>
      <url hash="eea9c26c">2024.lrec-main.781</url>
      <attachment type="OptionalSupplementaryMaterial" hash="58b4f43f">2024.lrec-main.781.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>jamelot-etal-2024-improving-text</bibkey>
    </paper>
    <paper id="782">
      <title>Improving the Robustness of Large Language Models via Consistency Alignment</title>
      <author><first>Yukun</first><last>Zhao</last></author>
      <author><first>Lingyong</first><last>Yan</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Guoliang</first><last>Xing</last></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Chong</first><last>Meng</last></author>
      <author><first>Zhicong</first><last>Cheng</last></author>
      <author><first>Zhaochun</first><last>Ren</last></author>
      <author><first>Dawei</first><last>Yin</last></author>
      <pages>8931–8941</pages>
      <abstract>Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.</abstract>
      <url hash="6db99a8d">2024.lrec-main.782</url>
      <bibkey>zhao-etal-2024-improving-robustness</bibkey>
    </paper>
    <paper id="783">
      <title>Improving Unsupervised Neural Machine Translation via Training Data Self-Correction</title>
      <author><first>Jinliang</first><last>Lu</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <pages>8942–8954</pages>
      <abstract>Unsupervised neural machine translation (UNMT) models are trained with pseudo-parallel sentences constructed by on-the-fly back-translation using monolingual corpora. However, the quality of pseudo-parallel sentences cannot be guaranteed, which hinders the final performance of UNMT. This paper demonstrates that although UNMT usually generates mistakes during pseudo-parallel data construction, some of them can be corrected by the token-level translations that exist in the embedding table. Therefore, we propose a self-correction method to automatically improve the quality of pseudo-parallel sentences during training, thereby enhancing translation performance. Specifically, for a pseudo sentence pair, our self-correction method first estimates the alignment relations between tokens by treating and solving it as an optimal transport problem. Then, we measure the translation reliability for each token and detect the mis-translated ones. Finally, the mis-translated tokens are corrected with real-time computed token-by-token translations based on the embedding table, yielding a better training example. Considering that the modified examples are semantically equivalent to the original ones when UNMT converges, we introduce second-phase training to strengthen the output consistency between them, further improving the generalization capability and translation performance. Empirical results on widely used UNMT datasets demonstrate the effectiveness of our method and it significantly outperforms several strong baselines.</abstract>
      <url hash="3aa7facd">2024.lrec-main.783</url>
      <bibkey>lu-zhang-2024-improving-unsupervised</bibkey>
    </paper>
    <paper id="784">
      <title>Improving <fixed-case>V</fixed-case>ietnamese-<fixed-case>E</fixed-case>nglish Medical Machine Translation</title>
      <author><first>Nhu</first><last>Vo</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Dung D.</first><last>Le</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <pages>8955–8962</pages>
      <abstract>Machine translation for Vietnamese-English in the medical domain is still an under-explored research area. In this paper, we introduce MedEV—a high-quality Vietnamese-English parallel dataset constructed specifically for the medical domain, comprising approximately 360K sentence pairs. We conduct extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo), state-of-the-art Vietnamese-English neural machine translation models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset. Experimental results show that the best performance is achieved by fine-tuning “vinai-translate” for each translation direction. We publicly release our dataset to promote further research.</abstract>
      <url hash="816a56d6">2024.lrec-main.784</url>
      <bibkey>vo-etal-2024-improving-vietnamese</bibkey>
    </paper>
    <paper id="785">
      <title><fixed-case>I</fixed-case>na<fixed-case>GVAD</fixed-case> : A Challenging <fixed-case>F</fixed-case>rench <fixed-case>TV</fixed-case> and Radio Corpus Annotated for Speech Activity Detection and Speaker Gender Segmentation</title>
      <author><first>David</first><last>Doukhan</last></author>
      <author><first>Christine</first><last>Maertens</last></author>
      <author><first>William</first><last>Le Personnic</last></author>
      <author><first>Ludovic</first><last>Speroni</last></author>
      <author><first>Reda</first><last>Dehak</last></author>
      <pages>8963–8974</pages>
      <abstract>InaGVAD is an audio corpus collected from 10 French radio and 18 TV channels categorized into 4 groups: generalist radio, music radio, news TV, and generalist TV. It contains 277 1-minute-long annotated recordings aimed at representing the acoustic diversity of French audiovisual programs and was primarily designed to build systems able to monitor men’s and women’s speaking time in media. inaGVAD is provided with Voice Activity Detection (VAD) and Speaker Gender Segmentation (SGS) annotations extended with overlap, speaker traits (gender, age, voice quality), and 10 non-speech event categories. Annotation distributions are detailed for each channel category. This dataset is partitioned into a 1h development and a 3h37 test subset, allowing fair and reproducible system evaluation. A benchmark of 6 freely available VAD software is presented, showing diverse abilities based on channel and non-speech event categories. Two existing SGS systems are evaluated on the corpus and compared against a baseline X-vector transfer learning strategy, trained on the development subset. Results demonstrate that our proposal, trained on a single - but diverse - hour of data, achieved competitive SGS results. The entire inaGVAD package; including corpus, annotations, evaluation scripts, and baseline training code; is made freely accessible, fostering future advancement in the domain.</abstract>
      <url hash="c2461ea2">2024.lrec-main.785</url>
      <bibkey>doukhan-etal-2024-inagvad-challenging</bibkey>
    </paper>
    <paper id="786">
      <title>In-Context Example Retrieval from Multi-Perspectives for Few-Shot Aspect-Based Sentiment Analysis</title>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Hongling</first><last>Xu</last></author>
      <author><first>Keyang</first><last>Ding</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>8975–8985</pages>
      <abstract>In this paper, we focus on few-shot aspect-based sentiment analysis (ABSA) and try to solve it with in-context learning (ICL) paradigm. However, the effectiveness of ICL is highly affected by retrieved in-context examples. Previous works generally leverage the semantic similarity between the candidate examples and test input to retrieve examples. However, they may yield sub-optimal results for this task. This is because considering only the overall semantic perspective may leave some useful examples, which have syntactic structural relevance to the test input or share identical sentiments and similar aspects to one unretrievable. To address this shortcoming, we advocate retrieving in-context examples for few-shot ABSA by simultaneously considering three perspectives, overall semantics, syntactic structure relevance, and aspect-sentiment semantics. To achieve this, we construct positive and negative pairs from these three perspectives and train the demonstration retriever using contrastive learning. Experimental results on four ABSA datasets show that our retrieval framework can significantly outperform baselines across the board. Moreover, to understand factors influencing ICL performance on few-shot ABSA, we conduct extensive analysis in various scenarios, which can inspire and advance future research.</abstract>
      <url hash="1a977255">2024.lrec-main.786</url>
      <bibkey>wang-etal-2024-context-example</bibkey>
    </paper>
    <paper id="787">
      <title>Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer</title>
      <author><first>Jianyu</first><last>Zheng</last></author>
      <author><first>Fengfei</first><last>Fan</last></author>
      <author><first>Jianquan</first><last>Li</last></author>
      <pages>8986–8997</pages>
      <abstract>Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision. Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information. Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach. In this paper, we present a novel framework called “Lexicon-Syntax Enhanced Multilingual BERT” that combines both lexical and syntactic knowledge. Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities. The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure. To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously. Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0 3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks.</abstract>
      <url hash="1f95cc85">2024.lrec-main.787</url>
      <bibkey>zheng-etal-2024-incorporating-lexical</bibkey>
    </paper>
    <paper id="788">
      <title>Incorporating Word-level Phonemic Decoding into Readability Assessment</title>
      <author><first>Christine</first><last>Pinney</last></author>
      <author><first>Casey</first><last>Kennington</last></author>
      <author><first>Maria Soledad</first><last>Pera</last></author>
      <author><first>Katherine</first><last>Landau Wright</last></author>
      <author><first>Jerry Alan</first><last>Fails</last></author>
      <pages>8998–9009</pages>
      <abstract>Current approaches in automatic readability assessment have found success with the use of large language models and transformer architectures. These techniques lead to accuracy improvement, but they do not offer the interpretability that is uniquely required by the audience most often employing readability assessment tools: teachers and educators. Recent work that employs more traditional machine learning methods has highlighted the linguistic importance of considering semantic and syntactic characteristics of text in readability assessment by utilizing handcrafted feature sets. Research in Education suggests that, in addition to semantics and syntax, phonetic and orthographic instruction are necessary for children to progress through the stages of reading and spelling development; children must first learn to decode the letters and symbols on a page to recognize words and phonemes and their connection to speech sounds. Here, we incorporate this word-level phonemic decoding process into readability assessment by crafting a phonetically-based feature set for grade-level classification for English. Our resulting feature set shows comparable performance to much larger, semantically- and syntactically-based feature sets, supporting the linguistic value of orthographic and phonetic considerations in readability assessment.</abstract>
      <url hash="db362383">2024.lrec-main.788</url>
      <bibkey>pinney-etal-2024-incorporating-word</bibkey>
    </paper>
    <paper id="789">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>: Financial Natural Language Processing for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Sohom</first><last>Ghosh</last></author>
      <author><first>Arnab</first><last>Maji</last></author>
      <author><first>Aswartha</first><last>Narayana</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <pages>9010–9018</pages>
      <abstract>Applications of Natural Language Processing (NLP) in the finance domain have been very popular of late. For financial NLP, (FinNLP) while various datasets exist for widely spoken languages like English and Chinese, datasets are scarce for low resource languages,particularly for Indian languages. In this paper, we address this challenges by presenting IndicFinNLP – a collection of 9 datasets consisting of three tasks relating to FinNLP for three Indian languages. These tasks are Exaggerated Numeral Detection, Sustainability Classification, and ESG Theme Determination of financial texts in Hindi, Bengali, and Telugu. Moreover, we release the datasets under CC BY-NC-SA 4.0 license for the benefit of the research community.</abstract>
      <url hash="fa971b1b">2024.lrec-main.789</url>
      <bibkey>ghosh-etal-2024-indicfinnlp-financial</bibkey>
    </paper>
    <paper id="790">
      <title><fixed-case>I</fixed-case>ndic-<fixed-case>TEDST</fixed-case>: Datasets and Baselines for Low-Resource Speech to Text Translation</title>
      <author><first>Nivedita</first><last>Sethiya</last></author>
      <author><first>Saanvi</first><last>Nair</last></author>
      <author><first>Chandresh</first><last>Maurya</last></author>
      <pages>9019–9024</pages>
      <abstract>Speech-to-text (ST) task is the translation of speech in a language to text in a different language. It has use cases in subtitling, dubbing, etc. Traditionally, ST task has been solved by cascading automatic speech recognition (ASR) and machine translation (MT) models which leads to error propagation, high latency, and training time. To minimize such issues, end-to-end models have been proposed recently. However, we find that only a few works have reported results of ST models on a limited number of low-resource languages. To take a step further in this direction, we release datasets and baselines for low-resource ST tasks. Concretely, our dataset has 9 language pairs and benchmarking has been done against SOTA ST models. The low performance of SOTA ST models on Indic-TEDST data indicates the necessity of the development of ST models specifically designed for low-resource languages.</abstract>
      <url hash="29f440de">2024.lrec-main.790</url>
      <bibkey>sethiya-etal-2024-indic-tedst</bibkey>
    </paper>
    <paper id="791">
      <title><fixed-case>I</fixed-case>ndirect<fixed-case>QA</fixed-case>: Understanding Indirect Answers to Implicit Polar Questions in <fixed-case>F</fixed-case>rench and <fixed-case>S</fixed-case>panish</title>
      <author><first>Christin</first><last>Müller</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>9025–9035</pages>
      <abstract>Polar questions are common in dialogue and expect exactly one of two answers (yes/no). It is however not uncommon for speakers to bypass these expected choices and answer, for example, “Islands are generally by the sea” to the question: “An island? By the sea?”. While such answers are natural in spoken dialogues, conversational systems still struggle to interpret them. Seminal work to interpret indirect answers were made in recent years—but only for English and with strict question formulations. In this work, we present a new corpus for French and Spanish—IndirectQA —where we mine subtitle data for indirect answers to study the labeling task with six different labels, while broadening polar questions to include also implicit polar questions (statements that trigger a yes/no-answer which are not necessarily formulated as a question). We opted for subtitles since they are a readily available source of conversation in various languages, but also come with peculiarities and challenges which we will discuss. Overall, we provide the first results on French and Spanish. They show that the task is challenging: the baseline accuracy scores drop from 61.43 on English to 44.06 for French and Spanish.</abstract>
      <url hash="38e00c37">2024.lrec-main.791</url>
      <bibkey>muller-plank-2024-indirectqa-understanding</bibkey>
    </paper>
    <paper id="792">
      <title>Inductive Knowledge Graph Completion with <fixed-case>GNN</fixed-case>s and Rules: An Analysis</title>
      <author><first>Akash</first><last>Anil</last></author>
      <author><first>Victor</first><last>Gutierrez-Basulto</last></author>
      <author><first>Yazmin</first><last>Ibanez-Garcia</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>9036–9049</pages>
      <abstract>The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which means that they largely keep the interpretability advantage of rule-based methods. Moreover, we show that a further variant, which does look at the full KG, consistently outperforms NBFNet.</abstract>
      <url hash="afb33c37">2024.lrec-main.792</url>
      <bibkey>anil-etal-2024-inductive-knowledge</bibkey>
    </paper>
    <paper id="793">
      <title><fixed-case>I</fixed-case>nfer<fixed-case>BR</fixed-case>: A Natural Language Inference Dataset in <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Luciana</first><last>Bencke</last></author>
      <author><first>Francielle Vasconcellos</first><last>Pereira</last></author>
      <author><first>Moniele Kunrath</first><last>Santos</last></author>
      <author><first>Viviane</first><last>Moreira</last></author>
      <pages>9050–9060</pages>
      <abstract>Natural Language Inference semantic concepts are central to all aspects of natural language meaning. Portuguese has few NLI-annotated datasets created through automatic translation followed by manual checking. The manual creation of NLI datasets is complex and requires many efforts that are sometimes unavailable. Thus, investments to produce good quality synthetic instances that could be used to train machine learning models for NLI are welcome. This work produced InferBR, an NLI dataset for Portuguese. We relied on a semiautomatic process to generate premises and an automatic process to generate hypotheses. The dataset was manually revised, showing that 97.4% of the sentence pairs had good quality, and nearly 100% of the instances had the correct label assigned. The model trained with InferBR is better at recognizing entailment classes in the other Portuguese datasets than the reverse. Because of its diversity and many unique sentences, InferBR can potentially be further augmented. In addition to the dataset, a key contribution is our proposed generation processes for premises and hypotheses that can easily be adapted to other languages and tasks.</abstract>
      <url hash="f5b8552a">2024.lrec-main.793</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5dfe552f">2024.lrec-main.793.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>bencke-etal-2024-inferbr-natural</bibkey>
    </paper>
    <paper id="794">
      <title><fixed-case>I</fixed-case>nf<fixed-case>F</fixed-case>eed: Influence Functions as a Feedback to Improve the Performance of Subjective Tasks</title>
      <author><first>Somnath</first><last>Banerjee</last></author>
      <author><first>Maulindu</first><last>Sarkar</last></author>
      <author><first>Punyajoy</first><last>Saha</last></author>
      <author><first>Binny</first><last>Mathew</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>9061–9072</pages>
      <abstract>Recently, influence functions present an apparatus for achieving explainability for deep neural models by quantifying the perturbation of individual train instances that might impact a test prediction. Our objectives in this paper are twofold. First we incorporate influence functions as a feedback into the model to improve its performance. Second, in a dataset extension exercise, using influence functions to automatically identify data points that have been initially ‘silver’ annotated by some existing method and need to be cross-checked (and corrected) by annotators to improve the model performance. To meet these objectives, in this paper, we introduce InfFeed, which uses influence functions to compute the influential instances for a target instance. Toward the first objective, we adjust the label of the target instance based on its influencer(s) label. In doing this, InfFeed outperforms the state-of-the-art baselines (including LLMs) by a maximum macro F1-score margin of almost 4% for hate speech classification, 3.5% for stance classification, and 3% for irony and 2% for sarcasm detection. Toward the second objective we show that manually re-annotating only those silver annotated data points in the extension set that have a negative influence can immensely improve the model performance bringing it very close to the scenario where all the data points in the extension set have gold labels. This allows for huge reduction of the number of data points that need to be manually annotated since out of the silver annotated extension dataset, the influence function scheme picks up ~1/1000 points that need manual correction.</abstract>
      <url hash="938d6154">2024.lrec-main.794</url>
      <bibkey>banerjee-etal-2024-inffeed-influence</bibkey>
    </paper>
    <paper id="795">
      <title><fixed-case>I</fixed-case>nfo<fixed-case>E</fixed-case>nh: Towards Multimodal Sentiment Analysis via Information Bottleneck Filter and Optimal Transport Alignment</title>
      <author><first>Yifeng</first><last>Xie</last></author>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Xuan</first><last>Lu</last></author>
      <author><first>Zhiqi</first><last>Huang</last></author>
      <author><first>Haoran</first><last>Xiong</last></author>
      <pages>9073–9083</pages>
      <abstract>In recent years, Multimodal Sentiment Analysis (MSA) leveraging deep learning has demonstrated exceptional performance in a wide range of domains. Its success lies in effectively utilizing information from multiple modalities to analyze sentiments. Despite these advancements, MSA is confronted with two significant challenges. Firstly, each modality often has a surplus of unimportance data, which can overshadow the essential information. Secondly, the crucial cues for sentiment analysis may conflict across different modalities, thereby complicating the analysis process. These issues have a certain impact on the model’s effectiveness in MSA tasks. To address these challenges, this paper introduces a novel method tailored for MSA, termed InfoEnh. This approach utilizes a masking technique as the bottleneck for information filtering, simultaneously maximizing mutual information to retain crucial data. Furthermore, the method integrates all modalities into a common feature space via domain adaptation, which is enhanced by the application of optimal transport. Extensive experiments conducted on two benchmark MSA datasets demonstrate the effectiveness of our proposed approach. Further analyzes indicate significant improvements over the baselines.</abstract>
      <url hash="92b24d78">2024.lrec-main.795</url>
      <bibkey>xie-etal-2024-infoenh-towards</bibkey>
    </paper>
    <paper id="796">
      <title>Information Extraction with Differentiable Beam Search on Graph <fixed-case>RNN</fixed-case>s</title>
      <author><first>Niama</first><last>El Khbir</last></author>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>9084–9096</pages>
      <abstract>Information extraction (IE) from text documents is an important NLP task that includes entity, relation, and event extraction. These tasks are often addressed jointly as a graph generation problem, where entities and event triggers represent nodes and where relations and event arguments represent edges. Most existing systems use local classifiers for nodes and edges, trained using cross-entropy loss, and employ inference strategies such as beam search to approximate the optimal graph structure. These approaches typically suffer from exposure bias due to the discrepancy between training and decoding. In this paper, we tackle this problem by casting graph generation as auto-regressive sequence labeling and making its training aware of the decoding procedure by using a differentiable version of beam search. We evaluate the effectiveness of our approach through extensive experiments conducted on the ACE05 and ConLL04 datasets across diverse languages. Our experimental findings affirm that our model outperforms its non-decoding-aware version for all datasets employed. Furthermore, we conduct ablation studies that emphasize the effectiveness of aligning training and inference. Additionally, we introduce a novel quantification of exposure bias within this context, providing valuable insights into the functioning of our model.</abstract>
      <url hash="242ef5ba">2024.lrec-main.796</url>
      <bibkey>el-khbir-etal-2024-information-extraction</bibkey>
    </paper>
    <paper id="797">
      <title><fixed-case>INMT</fixed-case>-Lite: Accelerating Low-Resource Language Data Collection via Offline Interactive Neural Machine Translation</title>
      <author><first>Harshita</first><last>Diddee</last></author>
      <author><first>Anurag</first><last>Shukla</last></author>
      <author><first>Tanuja</first><last>Ganu</last></author>
      <author><first>Vivek</first><last>Seshadri</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <pages>9097–9109</pages>
      <abstract>A steady increase in the performance of Massively Multilingual Models (MMLMs) has contributed to their rapidly increasing use in data collection pipelines. Interactive Neural Machine Translation (INMT) systems are one class of tools that can utilize MMLMs to promote such data collection in several under-resourced languages. However, these tools are often not adapted to the deployment constraints that native language speakers operate in, as bloated, online inference-oriented MMLMs trained for data-rich languages, drive them. INMT-Lite addresses these challenges through its support of (1) three different modes of Internet-independent deployment and (2) a suite of four assistive interfaces suitable for (3) data-sparse languages. We perform an extensive user study for INMT-Lite with an under-resourced language community, Gondi, to find that INMT-Lite improves the data generation experience of community members along multiple axes, such as cognitive load, task productivity, and interface interaction time and effort, without compromising on the quality of the generated translations.INMT-Lite’s code is open-sourced to further research in this domain.</abstract>
      <url hash="077de10d">2024.lrec-main.797</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1c6eae97">2024.lrec-main.797.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>diddee-etal-2024-inmt-lite</bibkey>
    </paper>
    <paper id="798">
      <title>Integrating Headedness Information into an Auto-generated Multilingual <fixed-case>CCG</fixed-case>bank for Improved Semantic Interpretation</title>
      <author><first>Tu-Anh</first><last>Tran</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>9110–9119</pages>
      <abstract>Previously, we introduced a method to generate a multilingual Combinatory Categorial Grammar (CCG) treebank by converting from the Universal Dependencies (UD). However, the method only produces bare CCG derivations without any accompanying semantic representations, which makes it difficult to obtain satisfactory analyses for constructions that involve non-local dependencies, such as control/raising or relative clauses, and limits the general applicability of the treebank. In this work, we present an algorithm that adds semantic representations to existing CCG derivations, in the form of predicate-argument structures. Through hand-crafted rules, we enhance each CCG category with headedness information, with which both local and non-local dependencies can be properly projected. This information is extracted from various sources, including UD, Enhanced UD, and proposition banks. Evaluation of our projected dependencies on the English PropBank and the Universal PropBank 2.0 shows that they can capture most of the semantic dependencies in the target corpora. Further error analysis measures the effectiveness of our algorithm for each language tested, and reveals several issues with the previous method and source data.</abstract>
      <url hash="64b6595c">2024.lrec-main.798</url>
      <bibkey>tran-miyao-2024-integrating-headedness</bibkey>
    </paper>
    <paper id="799">
      <title>Integrating Representation Subspace Mapping with Unimodal Auxiliary Loss for Attention-based Multimodal Emotion Recognition</title>
      <author><first>Xulong</first><last>Du</last></author>
      <author><first>Xingnan</first><last>Zhang</last></author>
      <author><first>Dandan</first><last>Wang</last></author>
      <author><first>Yingying</first><last>Xu</last></author>
      <author><first>Zhiyuan</first><last>Wu</last></author>
      <author><first>Shiqing</first><last>Zhang</last></author>
      <author><first>Xiaoming</first><last>Zhao</last></author>
      <author><first>Jun</first><last>Yu</last></author>
      <author><first>Liangliang</first><last>Lou</last></author>
      <pages>9120–9130</pages>
      <abstract>Multimodal emotion recognition (MER) aims to identify emotions by utilizing affective information from multiple modalities. Due to the inherent disparities among these heterogeneous modalities, there is a large modality gap in their representations, leading to the challenge of fusing multiple modalities for MER. To address this issue, this work proposes a novel attention-based MER framework by integrating representation subspace mapping with unimodal auxiliary loss for enhancing multimodal fusion capabilities. Initially, a representation subspace mapping module is proposed to map each modality into two distinct subspaces. One is modality-public, enabling the acquisition of common representations and reducing the discrepancies across modalities. The other is modality-unique, retaining the unique characteristics of each modality while eliminating redundant inter-modal attributes. Then, a cross-modality attention is leveraged to bridge the modality gap in unique representations and facilitate modality adaptation. Additionally, our method designs an unimodal auxiliary loss to remove the noise unrelated to emotion classification, resulting in robust and meaningful representations for MER. Comprehensive experiments are conducted on the IEMOCAP and MSP-Improv datasets, and experiment results show that our method achieves superior performance to state-of-the-art MER methods. Keywords: Multimodal emotion recognition, representation subspace mapping, cross-modality attention, unimodal auxiliary loss, fusion</abstract>
      <url hash="a1bf0980">2024.lrec-main.799</url>
      <bibkey>du-etal-2024-integrating-representation</bibkey>
    </paper>
    <paper id="800">
      <title>Intent-Aware and Hate-Mitigating Counterspeech Generation via Dual-Discriminator Guided <fixed-case>LLM</fixed-case>s</title>
      <author><first>Haiyang</first><last>Wang</last></author>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Xin</first><last>Song</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Yuchen</first><last>Pan</last></author>
      <author><first>Hongkui</first><last>Tu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Bin</first><last>Zhou</last></author>
      <pages>9131–9142</pages>
      <abstract>Counterspeech is an effective way to combat online hate speech. Considering the multifaceted nature of online hate speech, counterspeech with varying intents (e.g., denouncing or empathy) has significant potential to mitigate hate speech effectively. Recently, controlled approaches based on large language models (LLMs) have been explored to generate intent-specific counterspeech. Due to the lack of attention to intent-specific information by LLMs during the decoding process, those methods cater more to the semantic information rather than matching with the desired intents. Further, there are still limitations in quantitatively evaluating the effectiveness of counterspeech with different intents in mitigating hate speech. In this paper, to address the above issues, we propose DART, an LLMs-based DuAl-discRiminaTor guided framework for counterspeech generation. We employ an intent-aware discriminator and hate-mitigating discriminator to jointly guide the decoding preferences of LLMs, which facilitates the model towards generating counterspeech catering to specific intent and hate mitigation. We apply a maximum-margin relative objective for training discriminators. This objective leverages the distance between counterspeech aligned with the desired target (such as specific intent or effectiveness in hate mitigation) and undesired as an effective learning signal. Extensive experiments show that DART achieves excellent performances in matching the desired intent and mitigating hate.</abstract>
      <url hash="391971c6">2024.lrec-main.800</url>
      <bibkey>wang-etal-2024-intent-aware</bibkey>
    </paper>
    <paper id="801">
      <title>Intention and Face in Dialog</title>
      <author><first>Adil</first><last>Soubki</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <pages>9143–9153</pages>
      <abstract>The notion of face described by Brown and Levinson (1987) has been studied in great detail, but a critical aspect of the framework, that which focuses on how intentions mediate the planning of turns which impose upon face, has received far less attention. We present an analysis of three computational systems trained for classifying both intention and politeness, focusing on how the former influences the latter. In politeness theory, agents attend to the desire to have their wants appreciated (positive face), and a complementary desire to act unimpeded and maintain freedom (negative face). Similar to speech acts, utterances can perform so-called face acts which can either raise or threaten the positive or negative face of the speaker or hearer. We begin by using an existing corpus to train a model which classifies face acts, achieving a new SoTA in the process. We then observe that every face act has an underlying intention that motivates it and perform additional experiments integrating dialog act annotations to provide these intentions by proxy. Our analysis finds that dialog acts improve performance on face act detection for minority classes and points to a close relationship between aspects of face and intent.</abstract>
      <url hash="937e972d">2024.lrec-main.801</url>
      <bibkey>soubki-rambow-2024-intention-face</bibkey>
    </paper>
    <paper id="802">
      <title><fixed-case>I</fixed-case>nte<fixed-case>R</fixed-case>ead: An Eye Tracking Dataset of Interrupted Reading</title>
      <author><first>Francesca</first><last>Zermiani</last></author>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Ekta</first><last>Sood</last></author>
      <author><first>Fabian</first><last>Kögel</last></author>
      <author><first>Andreas</first><last>Bulling</last></author>
      <author><first>Maria</first><last>Wirzberger</last></author>
      <pages>9154–9169</pages>
      <abstract>Eye movements during reading offer a window into cognitive processes and language comprehension, but the scarcity of reading data with interruptions – which learners frequently encounter in their everyday learning environments – hampers advances in the development of intelligent learning technologies. We introduce InteRead – a novel 50-participant dataset of gaze data recorded during self-paced reading of real-world text. InteRead further offers fine-grained annotations of interruptions interspersed throughout the text as well as resumption lags incurred by these interruptions. Interruptions were triggered automatically once readers reached predefined target words. We validate our dataset by reporting interdisciplinary analyses on different measures of gaze behavior. In line with prior research, our analyses show that the interruptions as well as word length and word frequency effects significantly impact eye movements during reading. We also explore individual differences within our dataset, shedding light on the potential for tailored educational solutions. InteRead is accessible from our datasets web-page: https://www.ife.uni-stuttgart.de/en/llis/research/datasets/.</abstract>
      <url hash="ec98b155">2024.lrec-main.802</url>
      <bibkey>zermiani-etal-2024-interead-eye</bibkey>
    </paper>
    <paper id="803">
      <title>Interpretable Assessment of Speech Intelligibility Using Deep Learning: A Case Study on Speech Disorders Due to Head and Neck Cancers</title>
      <author><first>Sondes</first><last>Abderrazek</last></author>
      <author><first>Corinne</first><last>Fredouille</last></author>
      <author><first>Alain</first><last>Ghio</last></author>
      <author><first>Muriel</first><last>Lalain</last></author>
      <author><first>Christine</first><last>Meunier</last></author>
      <author><first>Mathieu</first><last>Balaguer</last></author>
      <author><first>Virginie</first><last>Woisard</last></author>
      <pages>9170–9179</pages>
      <abstract>This paper sheds light on a relatively unexplored area which is deep learning interpretability for speech disorder assessment and characterization. Building upon a state-of-the-art methodology for the explainability and interpretability of hidden representation inside a deep-learning speech model, we provide a deeper understanding and interpretation of the final intelligibility assessment of patients experiencing speech disorders due to Head and Neck Cancers (HNC). Promising results have been obtained regarding the prediction of speech intelligibility and severity of HNC patients while giving relevant interpretations of the final assessment both at the phonemes and phonetic feature levels. The potential of this approach becomes evident as clinicians can acquire more valuable insights for speech therapy. Indeed, this can help identify the specific linguistic units that affect intelligibility from an acoustic point of view and enable the development of tailored rehabilitation protocols to improve the patient’s ability to communicate effectively, and thus, the patient’s quality of life.</abstract>
      <url hash="84701aef">2024.lrec-main.803</url>
      <bibkey>abderrazek-etal-2024-interpretable-assessment</bibkey>
    </paper>
    <paper id="804">
      <title>Interpretable Short Video Rumor Detection Based on Modality Tampering</title>
      <author><first>Kaixuan</first><last>Wu</last></author>
      <author><first>Yanghao</first><last>Lin</last></author>
      <author><first>Donglin</first><last>Cao</last></author>
      <author><first>Dazhen</first><last>Lin</last></author>
      <pages>9180–9189</pages>
      <abstract>With the rapid development of social media and short video applications in recent years, browsing short videos has become the norm. Due to its large user base and unique appeal, spreading rumors via short videos has become a severe social problem. Many methods simply fuse multimodal features for rumor detection, which lack interpretability. For short video rumors, rumor makers create rumors by modifying and/or splicing different modal information, so we should consider how to detect rumors from the perspective of modality tampering. Inspired by cross-modal contrastive learning, we propose a novel short video rumor detection framework by designing two pretraining tasks: modality tampering detection and inter-modal matching, imbuing the model with the ability to detect modality tampering and employing it for downstream rumor detection tasks. In addition, we design an interpretability mechanism to make the rumor detection results more reasonable by backtracking the model’s decision-making process. The experimental results show that the method on the short video rumor dataset has an improvement of about 4.6%-12% in macro-F1 compared with other models and can explain whether the short video is a rumor or not through the perspective of modality tampering.</abstract>
      <url hash="b5680076">2024.lrec-main.804</url>
      <bibkey>wu-etal-2024-interpretable-short</bibkey>
    </paper>
    <paper id="805">
      <title>Interpreting Themes from Educational Stories</title>
      <author><first>Yigeng</first><last>Zhang</last></author>
      <author><first>Fabio</first><last>Gonzalez</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>9190–9203</pages>
      <abstract>Reading comprehension continues to be a crucial research focus in the NLP community. Recent advances in Machine Reading Comprehension (MRC) have mostly centered on literal comprehension, referring to the surface-level understanding of content. In this work, we focus on the next level - interpretive comprehension, with a particular emphasis on inferring the themes of a narrative text. We introduce the first dataset specifically designed for interpretive comprehension of educational narratives, providing corresponding well-edited theme texts. The dataset spans a variety of genres and cultural origins and includes human-annotated theme keywords with varying levels of granularity. We further formulate NLP tasks under different abstractions of interpretive comprehension toward the main idea of a story. After conducting extensive experiments with state-of-the-art methods, we found the task to be both challenging and significant for NLP research. The dataset and source code have been made publicly available to the research community at https://github.com/RiTUAL-UH/EduStory.</abstract>
      <url hash="d7355be3">2024.lrec-main.805</url>
      <bibkey>zhang-etal-2024-interpreting-themes</bibkey>
    </paper>
    <paper id="806">
      <title>Intrinsic Subgraph Generation for Interpretable Graph Based Visual Question Answering</title>
      <author><first>Pascal</first><last>Tilli</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>9204–9223</pages>
      <abstract>The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the evaluations of human assessors, acting as automatic metrics for the generated explanatory subgraphs. Our code will be made publicly available at link removed due to anonymity period.</abstract>
      <url hash="be419525">2024.lrec-main.806</url>
      <bibkey>tilli-vu-2024-intrinsic-subgraph</bibkey>
    </paper>
    <paper id="807">
      <title>Introducing a Parsed Corpus of Historical <fixed-case>H</fixed-case>igh <fixed-case>G</fixed-case>erman</title>
      <author><first>Christopher D.</first><last>Sapp</last></author>
      <author><first>Elliott</first><last>Evans</last></author>
      <author><first>Rex</first><last>Sprouse</last></author>
      <author><first>Daniel</first><last>Dakota</last></author>
      <pages>9224–9233</pages>
      <abstract>We outline the ongoing development of the Indiana Parsed Corpus of (Historical) High German. Once completed, this corpus will fill the gap in Penn-style treebanks for Germanic languages by spanning High German from 1050 to 1950. This paper describes the process of building the corpus: selection of texts, decisions on part-of-speech tags and other labels, the process of annotation, and illustrative annotation issues unique to historical High German. The construction of the corpus has led to a refinement of the Penn labels, tailored to the particulars of this language.</abstract>
      <url hash="b427c91c">2024.lrec-main.807</url>
      <bibkey>sapp-etal-2024-introducing-parsed</bibkey>
    </paper>
    <paper id="808">
      <title>Introducing <fixed-case>CQ</fixed-case>u<fixed-case>AE</fixed-case> : A New <fixed-case>F</fixed-case>rench Contextualised Question-Answering Corpus for the Education Domain</title>
      <author><first>Thomas</first><last>Gerald</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Sofiane</first><last>Ettayeb</last></author>
      <author><first>Louis</first><last>Tamames</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>9234–9244</pages>
      <abstract>We present a new question answering corpus in French designed to educational domain. To be useful in such domain, we have to propose more complex questions and to be able to justify the answers on validated material. We analyze some properties of this corpus. The last part of this paper will be devoted to present the first experiments we have carried out to demonstrate the value of this dataset for learning a Retrieval Augmented Genration framework. Different experiments are proposed, with an automatic evaluation. A human evaluation is proposed to confirm or infirm this automatic evaluation.</abstract>
      <url hash="a7f0853b">2024.lrec-main.808</url>
      <bibkey>gerald-etal-2024-introducing-cquae</bibkey>
    </paper>
    <paper id="809">
      <title>Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study</title>
      <author><first>Myrthe</first><last>Reuver</last></author>
      <author><first>Suzan</first><last>Verberne</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>9245–9260</pages>
      <abstract>For a viewpoint-diverse news recommender, identifying whether two news articles express the same viewpoint is essential. One way to determine “same or different” viewpoint is stance detection. In this paper, we investigate the robustness of operationalization choices for few-shot stance detection, with special attention to modelling stance across different topics. Our experiments test pre-registered hypotheses on stance detection. Specifically, we compare two stance task definitions (Pro/Con versus Same Side Stance), two LLM architectures (bi-encoding versus cross-encoding), and adding Natural Language Inference knowledge, with pre-trained RoBERTa models trained with shots of 100 examples from 7 different stance detection datasets. Some of our hypotheses and claims from earlier work can be confirmed, while others give more inconsistent results. The effect of the Same Side Stance definition on performance differs per dataset and is influenced by other modelling choices. We found no relationship between the number of training topics in the training shots and performance. In general, cross-encoding out-performs bi-encoding, and adding NLI training to our models gives considerable improvement, but these results are not consistent across all datasets. Our results indicate that it is essential to include multiple datasets and systematic modelling experiments when aiming to find robust modelling choices for the concept ‘stance’.</abstract>
      <url hash="aaf16ce4">2024.lrec-main.809</url>
      <bibkey>reuver-etal-2024-investigating-robustness</bibkey>
    </paper>
    <paper id="810">
      <title><fixed-case>IR</fixed-case>2: Information Regularization for Information Retrieval</title>
      <author><first>Jianyou</first><last>Wang</last></author>
      <author><first>Kaicheng</first><last>Wang</last></author>
      <author><first>Xiaoyue</first><last>Wang</last></author>
      <author><first>Weili</first><last>Cao</last></author>
      <author><first>Ramamohan</first><last>Paturi</last></author>
      <author><first>Leon</first><last>Bergen</last></author>
      <pages>9261–9284</pages>
      <abstract>Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline—input, prompt, and output—each offering varying degrees of performance improvement compared to models where no regularization is applied. This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios. All code, prompts and synthetic data are available at https://github.com/Info-Regularization/Information-Regularization.</abstract>
      <url hash="a96199c0">2024.lrec-main.810</url>
      <bibkey>wang-etal-2024-ir2-information</bibkey>
    </paper>
    <paper id="811">
      <title><fixed-case>I</fixed-case> Remember You!: <fixed-case>SUI</fixed-case> Corpus for Remembering and Utilizing Users’ Information in Chat-oriented Dialogue Systems</title>
      <author><first>Yuiko</first><last>Tsunomori</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>9285–9295</pages>
      <abstract>To construct a chat-oriented dialogue system that will be used for a long time by users, it is important to build a good relationship between the user and the system. To achieve a good relationship, several methods for remembering and utilizing information on users (preferences, experiences, jobs, etc.) in system utterances have been investigated. One way to do this is to utilize user information to fill in utterance templates for use in response generation, but the utterances do not always fit the context. Another way is to use neural-based generation, but in current methods, user information can be incorporated only when the current dialogue topic is similar to that of the user information. This paper tackled these problems by constructing a novel corpus to incorporate arbitrary user information into system utterances regardless of the current dialogue topic while retaining appropriateness for the context. We then fine-tuned a model for generating system utterances using the constructed corpus. The result of a subjective evaluation demonstrated the effectiveness of our model. Furthermore, we incorporated our fine-tuned model into a dialogue system and confirmed the effectiveness of the system through interactive dialogues with users.</abstract>
      <url hash="9418543c">2024.lrec-main.811</url>
      <bibkey>tsunomori-higashinaka-2024-remember-sui</bibkey>
    </paper>
    <paper id="812">
      <title><fixed-case>Ì</fixed-case>ròyìn<fixed-case>S</fixed-case>peech: A Multi-purpose <fixed-case>Y</fixed-case>orùbá Speech Corpus</title>
      <author><first>Tolulope</first><last>Ogunremi</last></author>
      <author><first>Kola</first><last>Tubosun</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Iroro</first><last>Orife</last></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last></author>
      <pages>9296–9303</pages>
      <abstract>We introduce ÌròyìnSpeech corpus—a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yorùbá speech data that can be used for both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) tasks. We curated about 23,000 text sentences from the news and creative writing domains with an open license i.e., CC-BY-4.0 and asked multiple speakers to record each sentence. To encourage more participatory approach to data creation, we provide 5 000 utterances from the curated sentences to the Mozilla Common Voice platform to crowd-source the recording and validation of Yorùbá speech data. In total, we created about 42 hours of speech data recorded by 80 volunteers in-house, and 6 hours validated recordings on Mozilla Common Voice platform. Our evaluation on TTS shows that we can create a good quality general domain single-speaker TTS model for Yorùbá with as little 5 hours of speech by leveraging an end-to-end VITS architecture. Similarly, for ASR, we obtained a WER of 21.5.</abstract>
      <url hash="962b32d4">2024.lrec-main.812</url>
      <bibkey>ogunremi-etal-2024-iroyinspeech-multi</bibkey>
    </paper>
    <paper id="813">
      <title>Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization</title>
      <author><first>Shuo</first><last>Yang</last></author>
      <author><first>Gjergji</first><last>Kasneci</last></author>
      <pages>9304–9314</pages>
      <abstract>Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.</abstract>
      <url hash="ad46a7b6">2024.lrec-main.813</url>
      <bibkey>yang-kasneci-2024-crowdsourcing-breaking</bibkey>
    </paper>
    <paper id="814">
      <title>Is Gender Reference Gender-specific? Studies in a Polar Domain</title>
      <author><first>Manfred</first><last>Klenner</last></author>
      <author><first>Dylan</first><last>Massey</last></author>
      <pages>9315–9324</pages>
      <abstract>We investigate how gender authorship influences polar, i.e. positive and negative gender reference. Given German-language newspaper texts where the full name of the authors are known and their gender can be inferred from the first names. And given that nouns in the text have gender reference, i.e. are labeled by a gender classifier as female or male denoting nouns. If these nouns carry a polar load, they count towards the gender-specific statistics we are interested in. A polar load is given either via phrase-level sentiment composition, or by a verb-based analysis of the polar role a noun (phrase) plays: is it framed by the verb as a positive or negative actor, or as receiving a positive or negative effect? Also, reported gender-gender relations (in favor, against) might be gender-specific. Statistical hypothesis testing is carried out in order to find out whether significant gender-wise correlations exist. We found that, in fact, gender reference is gender-specific: each gender significantly more often focuses on their own gender than the other one and e.g. positive actorship supremacy is claimed (intra-) gender-wise.</abstract>
      <url hash="fef44c49">2024.lrec-main.814</url>
      <bibkey>klenner-massey-2024-gender-reference</bibkey>
    </paper>
    <paper id="815">
      <title>Is It Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models</title>
      <author><first>Asma</first><last>Farajidizaji</last></author>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <pages>9325–9339</pages>
      <abstract>Text simplification is a common task where the text is adapted to make it easier to understand. Similarly, text elaboration can make a passage more sophisticated, offering a method to control the complexity of reading comprehension tests. However, text simplification and elaboration tasks are limited to only relatively alter the readability of texts. It is useful to directly modify the readability of any text to an absolute target readability level to cater to a diverse audience. Ideally, the readability of readability-controlled generated text should be independent of the source text. Therefore, we propose a novel readability-controlled text modification task. The task requires the generation of 8 versions at various target readability levels for each input text. We introduce novel readability-controlled text modification metrics. The baselines for this task use ChatGPT and Llama-2, with an extension approach introducing a two-step process (generating paraphrases by passing through the language model twice). The zero-shot approaches are able to push the readability of the paraphrases in the desired direction but the final readability remains correlated with the original text’s readability. We also find greater drops in semantic and lexical similarity between the source and target texts with greater shifts in the readability.</abstract>
      <url hash="2bd6cb1e">2024.lrec-main.815</url>
      <bibkey>farajidizaji-etal-2024-possible-modify</bibkey>
    </paper>
    <paper id="816">
      <title>Is <fixed-case>LLM</fixed-case> a Reliable Reviewer? A Comprehensive Evaluation of <fixed-case>LLM</fixed-case> on Automatic Paper Reviewing Tasks</title>
      <author><first>Ruiyang</first><last>Zhou</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>9340–9351</pages>
      <abstract>The use of large language models (LLM), especially ChatGPT, to help with research has come into practice. Researchers use it for timely advice and hope to obtain in-depth feedback. However, can LLM be a qualified and reliable reviewer? Although there already exist several review-related datasets, few works have carefully and thoroughly inspected model’s capability as a reviewer, especially the correctness of generated reviews. In this paper, we first evaluate GPT-3.5 and GPT-4 (the current top-performing LLM) on 2 types of tasks under different settings: the score prediction task and the review generation task. In addition, we propose a dataset containing 197 review-revision multiple-choice questions (RR-MCQ) with detailed labels from the review-rebuttal forum in ICLR-2023. By asking questions from technical details to the overall presentation and quality, our RR-MCQ data provides a more complete model ability assessment. The results show that LLM is generally helpful, but great caution is needed as it always makes mistakes. Although it can give passable decisions (&gt; 60% accuracy) on single options, completely correct answers are still rare (about 20%); models are still weak on long paper processing, zero-shot scoring, and giving critical feedback like human reviewers.</abstract>
      <url hash="15d58ed0">2024.lrec-main.816</url>
      <bibkey>zhou-etal-2024-llm-reliable</bibkey>
    </paper>
    <paper id="817">
      <title>Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation</title>
      <author><first>Mateusz</first><last>Klimaszewski</last></author>
      <author><first>Piotr</first><last>Andruszkiewicz</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>9352–9360</pages>
      <abstract>The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we propose a method that allows the transfer of modules between incompatible PLMs without any change in the inference complexity. The experiments on Named Entity Recognition, Natural Language Inference, and Paraphrase Identification tasks over multiple languages and PEFT methods showcase the initial potential of transferable modularity.</abstract>
      <url hash="7e8a161d">2024.lrec-main.817</url>
      <bibkey>klimaszewski-etal-2024-modularity-transferable</bibkey>
    </paper>
    <paper id="818">
      <title><fixed-case>ISO</fixed-case> 24617-12: A New Standard for Semantic Annotation</title>
      <author><first>Harry</first><last>Bunt</last></author>
      <pages>9361–9371</pages>
      <abstract>This paper presents ISO 24617-12, an annotation scheme for quantification phenomena in natural language., as part of the ISO Semantic Annotation Framework (ISO 24617). This scheme combines ideas from the theory of generalised quantifiers, from neo-Davidsonian event semantics, and from Discourse Representation Theory. The scheme consists of (1) an abstract syntax which defines ‘annotation structures’ as triples and other set-theoretic constructs of quantification-related concepts; (2) a reference representation of annotation structures (‘concrete syntax’); and (3) a compositional semantics of annotation structures. Together, these components define the markup language QuantML. This paper focuses on the identification and structuring of the semantic information useful for the characterisation of quantification in natural language and the interoperable representation of these information structures in QuantML.</abstract>
      <url hash="b23385b5">2024.lrec-main.818</url>
      <bibkey>bunt-2024-iso-24617</bibkey>
    </paper>
    <paper id="819">
      <title><fixed-case>I</fixed-case>sra<fixed-case>P</fixed-case>arl<fixed-case>T</fixed-case>weet: The Israeli Parliamentary and <fixed-case>T</fixed-case>witter Resource</title>
      <author><first>Guy</first><last>Mor-Lan</last></author>
      <author><first>Effi</first><last>Levi</last></author>
      <author><first>Tamir</first><last>Sheafer</last></author>
      <author><first>Shaul R.</first><last>Shenhav</last></author>
      <pages>9372–9381</pages>
      <abstract>We introduce IsraParlTweet, a new linked corpus of Hebrew-language parliamentary discussions from the Knesset (Israeli Parliament) between the years 1992-2023 and Twitter posts made by Members of the Knesset between the years 2008-2023, containing a total of 294.5 million Hebrew tokens. In addition to raw text, the corpus contains comprehensive metadata on speakers and Knesset sessions as well as several linguistic annotations. As a result, IsraParlTweet can be used to conduct a wide variety of quantitative and qualitative analyses and provide valuable insights into political discourse in Israel.</abstract>
      <url hash="499be90f">2024.lrec-main.819</url>
      <bibkey>mor-lan-etal-2024-israparltweet-israeli</bibkey>
    </paper>
    <paper id="820">
      <title>Is Spoken <fixed-case>H</fixed-case>ungarian Low-resource?: A Quantitative Survey of <fixed-case>H</fixed-case>ungarian Speech Data Sets</title>
      <author><first>Peter</first><last>Mihajlik</last></author>
      <author><first>Katalin</first><last>Mády</last></author>
      <author><first>Anna</first><last>Kohári</last></author>
      <author><first>Fruzsina Sára</first><last>Fruzsina</last></author>
      <author><first>Gábor</first><last>Kiss</last></author>
      <author><first>Tekla Etelka</first><last>Gráczi</last></author>
      <author><first>A. Seza</first><last>Doğruöz</last></author>
      <pages>9382–9388</pages>
      <abstract>Even though various speech data sets are available in Hungarian, there is a lack of a general overview about their types and sizes. To fill in this gap, we provide a survey of available data sets in spoken Hungarian in five categories (e.g., monolingual, Hungarian part of multilingual, pathological, child-related and dialectal collections). In total, the estimated size of available data is about 2800 hours (across 7500 speakers) and it represents a rich spoken language diversity. However, the distribution of the data and its alignment to real-life (e.g. speech recognition) tasks is far from optimal indicating the need for additional larger-scale natural language speech data sets. Our survey presents an overview of available data sets for Hungarian explaining their strengths and weaknesses which is useful for researchers working on Hungarian across disciplines. In addition, our survey serves as a starting point towards a unified foundational speech model specific to Hungarian.</abstract>
      <url hash="53257953">2024.lrec-main.820</url>
      <bibkey>mihajlik-etal-2024-spoken-hungarian</bibkey>
    </paper>
    <paper id="821">
      <title>Is Summary Useful or Not? An Extrinsic Human Evaluation of Text Summaries on Downstream Tasks</title>
      <author><first>Xiao</first><last>Pu</last></author>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>9389–9404</pages>
      <abstract>Research on automated text summarization typically uses human and automatic evaluation methods. While most recent studies focus on intrinsic evaluation, which assesses the general quality of summaries, e.g. coherence and informativeness, we concentrate on task-based extrinsic evaluation to determine the usefulness of summaries. We incorporate three downstream tasks, namely question answering, text classification, and text similarity assessment, and measure the usefulness of summaries for these tasks by several metrics. Our findings reveal that summaries are generally useful in tasks that require a comprehensive grasp of the text but are less useful in tasks requiring a more specific understanding of the text. We also analyze the usefulness and inherent properties of summaries from different models, and find that fine-tuned models consistently produce more useful summaries across all three tasks. In contrast, zero-shot models tend to lean towards text classification and similarity assessment, providing more general and less detailed summaries. Additionally, we assess the correlation between 14 intrinsic automatic metrics and human judgments. Intrinsic metrics perform well in evaluating summaries for question answering but are less effective in the other two tasks. This highlights the limitations of relying solely on intrinsic metrics for assessing summary performance and usefulness.</abstract>
      <url hash="323bade2">2024.lrec-main.821</url>
      <bibkey>pu-etal-2024-summary-useful</bibkey>
    </paper>
    <paper id="822">
      <title><fixed-case>IT</fixed-case>2<fixed-case>ACL</fixed-case> Learning Easy-to-Hard Instructions via 2-Phase Automated Curriculum Learning for Large Language Models</title>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>9405–9421</pages>
      <abstract>Instruction tuning has demonstrated its superiority in unlocking the abilities of pre-trained large language models (LLMs), including their capability to respond to diverse human instructions and conduct complex reasoning. In order to further enhance the continuous learning capabilities of pre-trained LLMs, we explore the training process of instruction tuning through the lens of task sequences. We propose a 2-phase automated curriculum learning guided instruction tuning framework, IT2ACL that learns easy-to-hard instructions for LLMs in a self-adjusting dynamic manner. To facilitate curriculum learning from instructions, we propose a loss-driven progress signal for two-phase strategies: instruction prediction gain that decides the instruction level syllabus. Through comprehensive experiments on 70 Chinese datasets which have been grouped into 16 distinct task clusters, we demonstrate the effectiveness of our approach in eliciting latent ability in pre-trained LLMs and achieving superior performance across diverse tasks.</abstract>
      <url hash="8404bcd3">2024.lrec-main.822</url>
      <bibkey>huang-xiong-2024-it2acl-learning</bibkey>
    </paper>
    <paper id="823">
      <title><fixed-case>IT</fixed-case>5: Text-to-text Pretraining for <fixed-case>I</fixed-case>talian Language Understanding and Generation</title>
      <author><first>Gabriele</first><last>Sarti</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>9422–9433</pages>
      <abstract>We introduce IT5, the first family of encoder-decoder transformer models pretrained specifically on Italian. We document and perform a thorough cleaning procedure for a large Italian corpus and use it to pretrain four IT5 model sizes. We then introduce the ItaGen benchmark, which includes a broad range of natural language understanding and generation tasks for Italian, and use it to evaluate the performance of IT5 models and multilingual baselines. We find monolingual IT5 models to provide the best scale-to-performance ratio across tested models, consistently outperforming their multilingual counterparts and setting a new state-of-the-art for Italian language generation.</abstract>
      <url hash="10a343f8">2024.lrec-main.823</url>
      <bibkey>sarti-nissim-2024-it5-text</bibkey>
    </paper>
    <paper id="824">
      <title><fixed-case>I</fixed-case>talian Word Embeddings for the Medical Domain</title>
      <author><first>Franco Alberto</first><last>Cardillo</last></author>
      <author><first>Franca</first><last>Debole</last></author>
      <pages>9434–9440</pages>
      <abstract>Neural word embeddings have proven valuable in the development of medical applications. However, for the Italian language, there are no publicly available corpora, embeddings, or evaluation resources tailored to this domain. In this paper, we introduce an Italian corpus for the medical domain, that includes texts from Wikipedia, medical journals, drug leaflets, and specialized websites. Using this corpus, we generate neural word embeddings from scratch. These embeddings are then evaluated using standard evaluation resources, that we translated into Italian exploiting the concept graph in the UMLS Metathesaurus. Despite the relatively small size of the corpus, our experimental results indicate that the new embeddings correlate well with human judgments regarding the similarity and the relatedness of medical concepts. Moreover, these medical-specific embeddings outperform a baseline model trained on the full Wikipedia corpus, which includes the medical pages we used. We believe that our embeddings and the newly introduced textual resources will foster further advancements in the field of Italian medical Natural Language Processing.</abstract>
      <url hash="c7cc7477">2024.lrec-main.824</url>
      <bibkey>cardillo-debole-2024-italian-word</bibkey>
    </paper>
    <paper id="825">
      <title>It’s Not under the Lamppost: Expanding the Reach of Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Christy</first><last>Doran</last></author>
      <author><first>Deborah A.</first><last>Dahl</last></author>
      <pages>9441–9451</pages>
      <abstract>Generic commercial language-based assistants have become ubiquitously available, originally in the form of smart speakers and mobile apps, and more recently in the form of systems based on generative AI. At first glance, their capabilities seem remarkable. Speech recognition works well, NLU mostly works, and access to back-end information sources is usually quite good. However, there is still a lot of work to be done. In the area of NLU in particular, focused probes into the capabilities of language-based assistants easily reveal significant areas of brittleness that demonstrate large gaps in their coverage. For example, the straightforward disjunctive query <i>is this monday or tuesday</i> elicited the nonsensical response <i>it’s 2:50 p.m. many consider it to be the afternoon</i>. These gaps are difficult to identify if the development process relies on training the system with an ongoing supply of natural user data, because this natural data can become distorted by a self-reinforcing feedback loop where the system ‘trains’ the user to produce data that works. This paper describes a process for collecting specific kinds of data to uncover these gaps and an annotation scheme for system responses, and includes examples of simple utterances that nonetheless fail to be correctly processed. The systems tested include both Conventional assistants, such as Amazon Alexa and Google Assistant, as well as GenAI systems, including ChatGPT and Bard/Gemini. We claim that these failures are due to a lack of attention to the full spectrum of input possibilities, and argue that systems would benefit from the inclusion of focused manual assessment to directly target likely gaps.</abstract>
      <url hash="5107524c">2024.lrec-main.825</url>
      <bibkey>doran-dahl-2024-lamppost-expanding</bibkey>
    </paper>
    <paper id="826">
      <title><fixed-case>J</fixed-case>a<fixed-case>P</fixed-case>ara<fixed-case>P</fixed-case>at: A Large-Scale <fixed-case>J</fixed-case>apanese-<fixed-case>E</fixed-case>nglish Parallel Patent Application Corpus</title>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Katsuki</first><last>Chousa</last></author>
      <author><first>Norihito</first><last>Yasuda</last></author>
      <pages>9452–9462</pages>
      <abstract>We constructed JaParaPat (Japanese-English Parallel Patent Application Corpus), a bilingual corpus of more than 300 million Japanese-English sentence pairs from patent applications published in Japan and the United States from 2000 to 2021. We obtained the publication of unexamined patent applications from the Japan Patent Office (JPO) and the United States Patent and Trademark Office (USPTO). We also obtained patent family information from the DOCDB, that is a bibliographic database maintained by the European Patent Office (EPO). We extracted approximately 1.4M Japanese-English document pairs, which are translations of each other based on the patent families, and extracted about 350M sentence pairs from the document pairs using a translation-based sentence alignment method whose initial translation model is bootstrapped from a dictionary-based sentence alignment. We experimentally improved the accuracy of the patent translations by 20 bleu points by adding more than 300M sentence pairs obtained from patent applications to 22M sentence pairs obtained from the web.</abstract>
      <url hash="fda979b6">2024.lrec-main.826</url>
      <bibkey>nagata-etal-2024-japarapat-large</bibkey>
    </paper>
    <paper id="827">
      <title>Jargon: A Suite of Language Models and Evaluation Tasks for <fixed-case>F</fixed-case>rench Specialized Domains</title>
      <author><first>Vincent</first><last>Segonne</last></author>
      <author><first>Aidan</first><last>Mannion</last></author>
      <author><first>Laura Cristina</first><last>Alonzo Canul</last></author>
      <author><first>Alexandre Daniel</first><last>Audibert</last></author>
      <author><first>Xingyu</first><last>Liu</last></author>
      <author><first>Cécile</first><last>Macaire</last></author>
      <author><first>Adrien</first><last>Pupier</last></author>
      <author><first>Yongxin</first><last>Zhou</last></author>
      <author><first>Mathilde</first><last>Aguiar</last></author>
      <author><first>Felix E.</first><last>Herron</last></author>
      <author><first>Magali</first><last>Norré</last></author>
      <author><first>Massih R</first><last>Amini</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Iris</first><last>Eshkol-Taravella</last></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <author><first>Lorraine</first><last>Goeuriot</last></author>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>François</first><last>Portet</last></author>
      <author><first>Fabien</first><last>Ringeval</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>9463–9476</pages>
      <abstract>Pretrained Language Models (PLMs) are the de facto backbone of most state-of-the-art NLP systems. In this paper, we introduce a family of domain-specific pretrained PLMs for French, focusing on three important domains: transcribed speech, medicine, and law. We use a transformer architecture based on efficient methods (LinFormer) to maximise their utility, since these domains often involve processing long documents. We evaluate and compare our models to state-of-the-art models on a diverse set of tasks and datasets, some of which are introduced in this paper. We gather the datasets into a new French-language evaluation benchmark for these three domains. We also compare various training configurations: continued pretraining, pretraining from scratch, as well as single- and multi-domain pretraining. Extensive domain-specific experiments show that it is possible to attain competitive downstream performance even when pre-training with the approximative LinFormer attention mechanism. For full reproducibility, we release the models and pretraining data, as well as contributed datasets.</abstract>
      <url hash="d1e2e81c">2024.lrec-main.827</url>
      <bibkey>segonne-etal-2024-jargon-suite</bibkey>
    </paper>
    <paper id="828">
      <title><fixed-case>JC</fixed-case>o<fixed-case>LA</fixed-case>: <fixed-case>J</fixed-case>apanese Corpus of Linguistic Acceptability</title>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Yushi</first><last>Sugimoto</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <pages>9477–9488</pages>
      <abstract>Neural language models have exhibited outstanding performance in a range of downstream tasks. However, there is limited understanding regarding the extent to which these models internalize syntactic knowledge, so that various datasets have recently been constructed to facilitate syntactic evaluation of language models across languages. In this paper, we introduce JCoLA (Japanese Corpus of Linguistic Acceptability), which consists of 10,020 sentences annotated with binary acceptability judgments. Specifically, those sentences are manually extracted from linguistics textbooks, handbooks and journal articles, and split into in-domain data (86 %; relatively simple acceptability judgments extracted from textbooks and handbooks) and out-of-domain data (14 %; theoretically significant acceptability judgments extracted from journal articles), the latter of which is categorized by 12 linguistic phenomena. We then evaluate the syntactic knowledge of 9 different types of Japanese and multilingual language models on JCoLA. The results demonstrated that several models could surpass human performance for the in-domain data, while no models were able to exceed human performance for the out-of-domain data. Error analyses by linguistic phenomena further revealed that although neural language models are adept at handling local syntactic dependencies like argument structure, their performance wanes when confronted with long-distance syntactic dependencies like verbal agreement and NPI licensing.</abstract>
      <url hash="423c1969">2024.lrec-main.828</url>
      <bibkey>someya-etal-2024-jcola-japanese</bibkey>
    </paper>
    <paper id="829">
      <title><fixed-case>J</fixed-case>-<fixed-case>CR</fixed-case>e3: A <fixed-case>J</fixed-case>apanese Conversation Dataset for Real-world Reference Resolution</title>
      <author><first>Nobuhiro</first><last>Ueda</last></author>
      <author><first>Hideko</first><last>Habe</last></author>
      <author><first>Akishige</first><last>Yuguchi</last></author>
      <author><first>Seiya</first><last>Kawano</last></author>
      <author><first>Yasutomo</first><last>Kawanishi</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <pages>9489–9502</pages>
      <abstract>Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We also constructed an experimental model and clarified the challenges in multimodal reference resolution tasks.</abstract>
      <url hash="9b9dc3e9">2024.lrec-main.829</url>
      <bibkey>ueda-etal-2024-j-cre3</bibkey>
    </paper>
    <paper id="830">
      <title><fixed-case>JD</fixed-case>oc<fixed-case>QA</fixed-case>: <fixed-case>J</fixed-case>apanese Document Question Answering Dataset for Generative Language Models</title>
      <author><first>Eri</first><last>Onami</last></author>
      <author><first>Shuhei</first><last>Kurita</last></author>
      <author><first>Taiki</first><last>Miyanishi</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>9503–9514</pages>
      <abstract>Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables, and hence visual question answering (VQA) methods are often examined in addition to textual approaches. We introduce Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese. Each QA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of questions and <i>unanswerable</i> questions from the document for realistic question-answering applications. We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models. Incorporating <i>unanswerable</i> questions in finetuning may contribute to harnessing the so-called hallucination generation.</abstract>
      <url hash="ac244366">2024.lrec-main.830</url>
      <bibkey>onami-etal-2024-jdocqa-japanese</bibkey>
    </paper>
    <paper id="831">
      <title><fixed-case>JEMH</fixed-case>op<fixed-case>QA</fixed-case>: Dataset for <fixed-case>J</fixed-case>apanese Explainable Multi-Hop Question Answering</title>
      <author><first>Ai</first><last>Ishii</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Hisami</first><last>Suzuki</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <pages>9515–9525</pages>
      <abstract>We present JEMHopQA, a multi-hop QA dataset for the development of explainable QA systems. The dataset consists not only of question-answer pairs, but also of supporting evidence in the form of derivation triples, which contributes to making the QA task more realistic and difficult. It is created based on Japanese Wikipedia using both crowd-sourced human annotation as well as prompting a large language model (LLM), and contains a diverse set of question, answer and topic categories as compared with similar datasets released previously. We describe the details of how we built the dataset as well as the evaluation of the QA task presented by this dataset using GPT-4, and show that the dataset is sufficiently challenging for the state-of-the-art LLM while showing promise for combining such a model with existing knowledge resources to achieve better performance.</abstract>
      <url hash="88a14954">2024.lrec-main.831</url>
      <bibkey>ishii-etal-2024-jemhopqa-dataset</bibkey>
    </paper>
    <paper id="832">
      <title><fixed-case>JFLD</fixed-case>: A <fixed-case>J</fixed-case>apanese Benchmark for Deductive Reasoning Based on Formal Logic</title>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Atsuki</first><last>Yamaguchi</last></author>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Hikaru</first><last>Tomonari</last></author>
      <author><first>Osamu</first><last>Imaichi</last></author>
      <author><first>Yasuhiro</first><last>Sogawa</last></author>
      <pages>9526–9535</pages>
      <abstract>Large language models (LLMs) have proficiently solved a broad range of tasks with their rich knowledge but often struggle with logical reasoning. To foster the research on logical reasoning, many benchmarks have been proposed so far. However, most of these benchmarks are limited to English, hindering the evaluation of LLMs specialized for each language. To address this, we propose **JFLD** (**J**apanese **F**ormal **L**ogic **D**eduction), a deductive reasoning benchmark for Japanese. JFLD assess whether LLMs can generate logical steps to (dis-)prove a given hypothesis based on a given set of facts. Its key features are assessing pure logical reasoning abilities isolated from knowledge and assessing various reasoning rules. We evaluate various Japanese LLMs and see that they are still poor at logical reasoning, thus highlighting a substantial need for future research.</abstract>
      <url hash="543ffd66">2024.lrec-main.832</url>
      <bibkey>morishita-etal-2024-jfld-japanese</bibkey>
    </paper>
    <paper id="833">
      <title><fixed-case>JLB</fixed-case>ert: <fixed-case>J</fixed-case>apanese Light <fixed-case>BERT</fixed-case> for Cross-Domain Short Text Classification</title>
      <author><first>Chandrai</first><last>Kayal</last></author>
      <author><first>Sayantan</first><last>Chattopadhyay</last></author>
      <author><first>Aryan</first><last>Gupta</last></author>
      <author><first>Satyen</first><last>Abrol</last></author>
      <author><first>Archie</first><last>Gugol</last></author>
      <pages>9536–9542</pages>
      <abstract>Models, such as BERT, have made a significant breakthrough in the Natural Language Processing (NLP) domain solving 11+ tasks. This is achieved by training on a large scale of unlabelled text resources and leveraging Transformers architecture making it the “Jack of all NLP trades”. However, one of the popular and challenging tasks in Sequence Classification is Short Text Classification (STC). Short Texts face the problem of being short, equivocal, and non-standard. In this paper, we address two major problems: 1. Improving STC tasks performance in Japanese language which consists of many varieties and dialects. 2. Building a light-weight Japanese BERT model with cross-domain functionality and comparable accuracy with State of the Art (SOTA) BERT models. To solve this, we propose a novel cross-domain scalable model called JLBert, which is pre-trained on a rich, diverse and less explored Japanese e-commerce corpus. We present results from extensive experiments to show that JLBert is outperforming SOTA Multilingual and Japanese specialized BERT models on three Short Text datasets by approx 1.5% across various domain.</abstract>
      <url hash="576f92da">2024.lrec-main.833</url>
      <bibkey>kayal-etal-2024-jlbert-japanese</bibkey>
    </paper>
    <paper id="834">
      <title><fixed-case>JL</fixed-case>-Hate: An Annotated Dataset for Joint Learning of Hate Speech and Target Detection</title>
      <author><first>Kaan</first><last>Büyükdemirci</last></author>
      <author><first>Izzet Emre</first><last>Kucukkaya</last></author>
      <author><first>Eren</first><last>Ölmez</last></author>
      <author><first>Cagri</first><last>Toraman</last></author>
      <pages>9543–9553</pages>
      <abstract>The detection of hate speech is a subject extensively explored by researchers, and machine learning algorithms play a crucial role in this domain. The existing resources mostly focus on text sequence classification for the task of hate speech detection. However, the target of hateful content is another dimension that has not been studied in details due to the lack of data resources. In this study, we address this gap by introducing a novel tweet dataset for the task of joint learning of hate speech detection and target detection, called JL-Hate, for the tasks of sequential text classification and token classification, respectively. The JL-Hate dataset consists of 1,530 tweets divided equally in English and Turkish languages. Leveraging this dataset, we conduct a series of benchmark experiments. We utilize a joint learning model to concurrently perform sequence and token classification tasks on our data. Our experimental results demonstrate consistent performance with the prevalent studies, both in sequence and token classification tasks.</abstract>
      <url hash="3bd11296">2024.lrec-main.834</url>
      <bibkey>buyukdemirci-etal-2024-jl-hate</bibkey>
    </paper>
    <paper id="835">
      <title><fixed-case>JM</fixed-case>ulti<fixed-case>WOZ</fixed-case>: A Large-Scale <fixed-case>J</fixed-case>apanese Multi-Domain Task-Oriented Dialogue Dataset</title>
      <author><first>Atsumoto</first><last>Ohashi</last></author>
      <author><first>Ryu</first><last>Hirai</last></author>
      <author><first>Shinya</first><last>Iizuka</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>9554–9567</pages>
      <abstract>Dialogue datasets are crucial for deep learning-based task-oriented dialogue system research. While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2.2. In addition, through evaluation experiments of interactive dialogues with the models and human participants, we identified limitations in the task completion capabilities of LLMs in Japanese.</abstract>
      <url hash="3b5ca709">2024.lrec-main.835</url>
      <bibkey>ohashi-etal-2024-jmultiwoz-large</bibkey>
    </paper>
    <paper id="836">
      <title>Joint Annotation of Morphology and Syntax in Dependency Treebanks</title>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Kirian</first><last>Guiller</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Yixuan</first><last>Li</last></author>
      <pages>9568–9577</pages>
      <abstract>In this paper, we compare different ways to annotate both syntactic and morphological relations in a dependency treebank and we propose new formats we call mSUD and mUD, compatible with the Universal Dependencies (UD) schema for syntactic treebanks. We emphasize mSUD rather than mUD, the former being based on distributional criteria for the choice of the head of any combination, which allow us to clearly encode the internal structure of a word, that is, the derivational path. We investigate different problems posed by a morph-based annotation, concerning tokenization, choice of the head of a morph combination, relations between morphs, additional features needed, such as the token type differentiating roots and derivational and inflectional affixes. We show how our annotation schema can be applied to different languages from polysynthetic languages such as Yupik to isolating languages such as Chinese.</abstract>
      <url hash="11c62e56">2024.lrec-main.836</url>
      <bibkey>guillaume-etal-2024-joint-annotation</bibkey>
    </paper>
    <paper id="837">
      <title><fixed-case>J</fixed-case>o<fixed-case>TR</fixed-case>: A Joint Transformer and Reinforcement Learning Framework for Dialogue Policy Learning</title>
      <author><first>Wai-Chung</first><last>Kwan</last></author>
      <author><first>Huimin</first><last>Wang</last></author>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>9578–9588</pages>
      <abstract>Dialogue policy learning (DPL) aims to determine an abstract representation (also known as action) to guide what the response should be. Typically, DPL is cast as a sequential decision problem across a series of predefined action candidates. However, such static and narrow actions can limit response diversity and impede the dialogue agent’s adaptability to new scenarios and edge cases. To overcome these challenges, we introduce a novel <b>Jo</b>int <b>T</b>ransformer <b>R</b>einforcement Learning framework, coined as <b>JoTR</b>, where a text-to-text Transformer-based model is employed to directly generate dialogue actions. More concretely, JoTR formulates a token-grained policy, facilitating more dynamic and adaptable dialogue action generation without the need for predefined action candidates. This method not only enhances the diversity of responses but also significantly improves the system’s capability to manage unfamiliar scenarios. Furthermore, JoTR utilizes Reinforcement Learning with a reward-shaping mechanism to efficiently fine-tune the token-grained policy. This allows the model to evolve through interactions, thereby enhancing its performance over time. Our extensive evaluation demonstrates that JoTR surpasses previous state-of-the-art models, showing improvements of 9% and 13% in success rate, and 34% and 37% in the diversity of dialogue actions across two benchmark dialogue modeling tasks respectively. These results have been validated by both user simulators and human evaluators. Code and data are available at ://github.com/KwanWaiChung/JoTR.</abstract>
      <url hash="ff9890d3">2024.lrec-main.837</url>
      <bibkey>kwan-etal-2024-jotr-joint</bibkey>
    </paper>
    <paper id="838">
      <title><fixed-case>JRC</fixed-case>-Names-Retrieval: A Standardized Benchmark for Name Search</title>
      <author><first>Philip</first><last>Blair</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>9589–9603</pages>
      <abstract>Many systems rely on the ability to effectively search through databases of personal and organization entity names in multiple writing scripts. Despite this, there is a relative lack of research studying this problem in isolation. In this work, we discuss this problem in detail and support future research by publishing what we believe is the first comprehensive dataset designed for this task. Additionally, we present a number of baselines against which future work can be compared; among which, we describe a neural solution based on ByT5 (Xue et al. 2022) which demonstrates up to a 12% performance gain over preexisting baselines, indicating that there remains much room for improvement in this space.</abstract>
      <url hash="85140e28">2024.lrec-main.838</url>
      <bibkey>blair-bar-2024-jrc-names</bibkey>
    </paper>
    <paper id="839">
      <title><fixed-case>J</fixed-case>-<fixed-case>SNACS</fixed-case>: Adposition and Case Supersenses for <fixed-case>J</fixed-case>apanese Joshi</title>
      <author><first>Tatsuya</first><last>Aoyama</last></author>
      <author><first>Chihiro</first><last>Taguchi</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>9604–9614</pages>
      <abstract>Many languages use adpositions (prepositions or postpositions) to mark a variety of semantic relations, with different languages exhibiting both commonalities and idiosyncrasies in the relations grouped under the same lexeme. We present the first Japanese extension of the SNACS framework (Schneider et al., 2018), which has served as the basis for annotating adpositions in corpora from several languages. After establishing which of the set of particles (joshi) in Japanese qualify as case markers and adpositions as defined in SNACS, we annotate 10 chapters (≈10k tokens) of the Japanese translation of Le Petit Prince (The Little Prince), achieving high inter-annotator agreement. We find that, while a majority of the particles and their uses are captured by the existing and extended SNACS annotation guidelines from the previous work, some unique cases were observed. We also conduct experiments investigating the cross-lingual similarity of adposition and case marker supersenses, showing that the language-agnostic SNACS framework captures similarities not clearly observed in multilingual embedding space.</abstract>
      <url hash="ff280c2c">2024.lrec-main.839</url>
      <bibkey>aoyama-etal-2024-j-snacs</bibkey>
    </paper>
    <paper id="840">
      <title>Jump to Conclusions: Short-Cutting Transformers with Linear Transformations</title>
      <author><first>Alexander</first><last>Yom Din</last></author>
      <author><first>Taelin</first><last>Karidi</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <pages>9615–9625</pages>
      <abstract>Transformer-based language models create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, using linear transformations. This approximation far exceeds the prevailing practice of inspecting hidden representations from all layers, in the space of the final layer. Moreover, in the context of language modeling, our method produces more accurate predictions from hidden layers, across various model scales, architectures, and data distributions. This allows “peeking” into intermediate representations, showing that GPT-2 and BERT often predict the final output already in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for example, at retention of 95% accuracy, our approach saves additional 7.9% layers for GPT-2 and 5.4% layers for BERT. Last, we extend our method to linearly approximate sub-modules, finding that attention is most tolerant to this change. Our code and learned mappings are publicly available at https://github.com/sashayd/mat.</abstract>
      <url hash="e4f63074">2024.lrec-main.840</url>
      <bibkey>yom-din-etal-2024-jump-conclusions</bibkey>
    </paper>
    <paper id="841">
      <title><fixed-case>K</fixed-case>az<fixed-case>E</fixed-case>mo<fixed-case>TTS</fixed-case>: A Dataset for <fixed-case>K</fixed-case>azakh Emotional Text-to-Speech Synthesis</title>
      <author><first>Adal</first><last>Abilbekov</last></author>
      <author><first>Saida</first><last>Mussakhojayeva</last></author>
      <author><first>Rustem</first><last>Yeshpanov</last></author>
      <author><first>Huseyin Atakan</first><last>Varol</last></author>
      <pages>9626–9632</pages>
      <abstract>This study focuses on the creation of the KazEmoTTS dataset, designed for emotional Kazakh text-to-speech (TTS) applications. KazEmoTTS is a collection of 54,760 audio-text pairs, with a total duration of 74.85 hours, featuring 34.23 hours delivered by a female narrator and 40.62 hours by two male narrators. The list of the emotions considered include “neutral”, “angry”, “happy”, “sad”, “scared”, and “surprised”. We also developed a TTS model trained on the KazEmoTTS dataset. Objective and subjective evaluations were employed to assess the quality of synthesized speech, yielding an MCD score within the range of 6.02 to 7.67, alongside a MOS that spanned from 3.51 to 3.57. To facilitate reproducibility and inspire further research, we have made our code, pre-trained model, and dataset accessible in our GitHub repository.</abstract>
      <url hash="1e7e673f">2024.lrec-main.841</url>
      <bibkey>abilbekov-etal-2024-kazemotts-dataset</bibkey>
    </paper>
    <paper id="842">
      <title><fixed-case>K</fixed-case>az<fixed-case>P</fixed-case>ar<fixed-case>C</fixed-case>: <fixed-case>K</fixed-case>azakh Parallel Corpus for Machine Translation</title>
      <author><first>Rustem</first><last>Yeshpanov</last></author>
      <author><first>Alina</first><last>Polonskaya</last></author>
      <author><first>Huseyin Atakan</first><last>Varol</last></author>
      <pages>9633–9644</pages>
      <abstract>We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.</abstract>
      <url hash="760ef57a">2024.lrec-main.842</url>
      <bibkey>yeshpanov-etal-2024-kazparc-kazakh</bibkey>
    </paper>
    <paper id="843">
      <title><fixed-case>K</fixed-case>az<fixed-case>QAD</fixed-case>: <fixed-case>K</fixed-case>azakh Open-Domain Question Answering Dataset</title>
      <author><first>Rustem</first><last>Yeshpanov</last></author>
      <author><first>Pavel</first><last>Efimov</last></author>
      <author><first>Leonid</first><last>Boytsov</last></author>
      <author><first>Ardak</first><last>Shalkarbayuli</last></author>
      <author><first>Pavel</first><last>Braslavski</last></author>
      <pages>9645–9656</pages>
      <abstract>We introduce KazQAD—a Kazakh open-domain question answering (ODQA) dataset—that can be used in both reading comprehension and full ODQA settings, as well as for information retrieval experiments. KazQAD contains just under 6,000 unique questions with extracted short answers and nearly 12,000 passage-level relevance judgements. We use a combination of machine translation, Wikipedia search, and in-house manual annotation to ensure annotation efficiency and data quality. The questions come from two sources: translated items from the Natural Questions (NQ) dataset (only for training) and the original Kazakh Unified National Testing (UNT) exam (for development and testing). The accompanying text corpus contains more than 800,000 passages from the Kazakh Wikipedia. As a supplementary dataset, we release around 61,000 question-passage-answer triples from the NQ dataset that have been machine-translated into Kazakh. We develop baseline retrievers and readers that achieve reasonable scores in retrieval (NDCG10 = 0.389 MRR = 0.382), reading comprehension (EM = 38.5 F1 = 54.2), and full ODQA (EM = 17.8 F1 = 28.7) settings. Nevertheless, these results are substantially lower than state-of-the-art results for English QA collections, and we think that there should still be ample room for improvement. We also show that the current OpenAI’s ChatGPTv3.5 is not able to answer KazQAD test questions in the closed-book setting with acceptable quality. The dataset is freely available under the Creative Commons licence (CC BY-SA) at url https://github.com/IS2AI/KazQAD</abstract>
      <url hash="b5ad388e">2024.lrec-main.843</url>
      <bibkey>yeshpanov-etal-2024-kazqad-kazakh</bibkey>
    </paper>
    <paper id="844">
      <title><fixed-case>K</fixed-case>az<fixed-case>SA</fixed-case>n<fixed-case>DRA</fixed-case>: <fixed-case>K</fixed-case>azakh Sentiment Analysis Dataset of Reviews and Attitudes</title>
      <author><first>Rustem</first><last>Yeshpanov</last></author>
      <author><first>Huseyin Atakan</first><last>Varol</last></author>
      <pages>9657–9667</pages>
      <abstract>This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment analysis that is the first and largest publicly available dataset of its kind. KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes. The study also pursued the automation of Kazakh sentiment classification through the development and evaluation of four machine learning models trained for both polarity classification and score classification. Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios. The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets. The dataset and fine-tuned models are open access and available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.</abstract>
      <url hash="087c0f1e">2024.lrec-main.844</url>
      <bibkey>yeshpanov-varol-2024-kazsandra-kazakh</bibkey>
    </paper>
    <paper id="845">
      <title><fixed-case>KC</fixed-case>-<fixed-case>G</fixed-case>en<fixed-case>R</fixed-case>e: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion</title>
      <author><first>Yilin</first><last>Wang</last></author>
      <author><first>Minghao</first><last>Hu</last></author>
      <author><first>Zhen</first><last>Huang</last></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <author><first>Dong</first><last>Yang</last></author>
      <author><first>Xicheng</first><last>Lu</last></author>
      <pages>9668–9680</pages>
      <abstract>The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs. To tackle the misordering issue, we develop a knowledge-guided interactive training method that enhances the identification and ranking of candidates. To address the omission issue, we design a knowledge-augmented constrained inference method that enables contextual prompting and controlled generation, so as to obtain valid rankings. Experimental results show that KG-GenRe achieves state-of-the-art performance on four datasets, with gains of up to 6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and 9.0% and 11.1% compared to that without re-ranking. Extensive analysis demonstrates the effectiveness of components in KG-GenRe.</abstract>
      <url hash="3eb9ccf0">2024.lrec-main.845</url>
      <bibkey>wang-etal-2024-kc-genre</bibkey>
    </paper>
    <paper id="846">
      <title><fixed-case>KCL</fixed-case>: Few-shot Named Entity Recognition with Knowledge Graph and Contrastive Learning</title>
      <author><first>Shan</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Cao</last></author>
      <author><first>Jing</first><last>Fan</last></author>
      <pages>9681–9692</pages>
      <abstract>Named Entity Recognition(NER), as a crucial subtask in natural language processing(NLP), is limited to a few labeled samples(a.k.a. few-shot). Metric-based meta-learning methods aim to learn the semantic space and assign the entity to its nearest label based on the similarity of their representations. However, these methods have trouble with semantic space learning and result in suboptimal performance. Specifically, the label name or its description is widely used for label semantic representation learning, but the label information extracted from the existing label description is limited. In addition, these methods focus on reducing the distance between the entity and the corresponding label, which may also reduce the distance between the labels and thus cause misclassification. In this paper, we propose a few-shot NER method that harnesses the power of Knowledge Graph and Contrastive Learning to improve the prototypical semantic space learning. First, KCL leverages knowledge graphs to provide rich and structured label information for label semantic representation learning. Then, KCL introduces the idea of contrastive learning to learn the label semantic representation. The label semantic representation is used to help distance the label clusters in the prototypical semantic space to reduce misclassification. Extensive experiments show that KCL achieves significant improvement over the state-of-the-art methods.</abstract>
      <url hash="7c9a5489">2024.lrec-main.846</url>
      <bibkey>zhang-etal-2024-kcl-shot</bibkey>
    </paper>
    <paper id="847">
      <title><fixed-case>KEHRL</fixed-case>: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning</title>
      <author><first>Dongyang</first><last>Li</last></author>
      <author><first>Taolin</first><last>Zhang</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Xiaofeng</first><last>He</last></author>
      <author><first>Hui</first><last>Xue</last></author>
      <pages>9693–9704</pages>
      <abstract>Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation triples from knowledge graphs (KGs) and integrate these external data sources into language models via self-supervised learning. Previous works treat knowledge enhancement as two independent operations, i.e., knowledge injection and knowledge integration. In this paper, we propose to learn Knowledge-Enhanced language representations with Hierarchical Reinforcement Learning (KEHRL), which jointly addresses the problems of detecting positions for knowledge injection and integrating external knowledge into the model in order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a high-level reinforcement learning (RL) agent utilizes both internal and prior knowledge to iteratively detect essential positions in texts for knowledge injection, which filters out less meaningful entities to avoid diverting the knowledge learning direction. Once the entity positions are selected, a relevant triple filtration module is triggered to perform low-level RL to dynamically refine the triples associated with polysemic entities through binary-valued actions. Experiments validate KEHRL’s effectiveness in probing factual knowledge and enhancing the model’s performance on various natural language understanding tasks.</abstract>
      <url hash="6578227d">2024.lrec-main.847</url>
      <bibkey>li-etal-2024-kehrl-learning</bibkey>
    </paper>
    <paper id="848">
      <title><fixed-case>KET</fixed-case>-<fixed-case>QA</fixed-case>: A Dataset for Knowledge Enhanced Table Question Answering</title>
      <author><first>Mengkang</first><last>Hu</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Ping</first><last>Luo</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>9705–9719</pages>
      <abstract>Due to the concise and structured nature of tables, the knowledge contained therein may be incomplete or missing, posing a significant challenge for table question answering (TableQA) systems. However, most existing datasets either overlook the challenge of missing knowledge in TableQA or only utilize unstructured text as supplementary information for tables. In this paper, we propose to use a knowledge base (KB) as the external knowledge source for TableQA and construct a dataset KET-QA with fine-grained gold evidence annotation. Each table in the dataset corresponds to a sub-graph of the entire KB, and every question requires the integration of information from both the table and the sub-graph to be answered. To extract pertinent information from the vast knowledge sub-graph and apply it to TableQA, we design a retriever-reasoner structured pipeline model. Experimental results demonstrate that our model consistently achieves remarkable relative performance improvements ranging from 1.9 to 6.5 times on EM scores across three distinct settings (fine-tuning, zero-shot, and few-shot), in comparison with solely relying on table information. However, even the best model achieves a 60.23% EM score, which still lags behind the human-level performance, highlighting the challenging nature of KET-QA for the question-answering community.</abstract>
      <url hash="798e32a8">2024.lrec-main.848</url>
      <bibkey>hu-etal-2024-ket-qa</bibkey>
    </paper>
    <paper id="849">
      <title>Keyphrase Generation: Lessons from a Reproducibility Study</title>
      <author><first>Edwin</first><last>Thomas</last></author>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <pages>9720–9731</pages>
      <abstract>Reproducibility studies are treated as means to verify the validity of a scientific method, but what else can we learn from such experiments? We addressed this question taking Keyphrase Generation (KPG) as the use case in this paper, by studying three state-of-the-art KPG models in terms of reproducibility under either the same (same data/model/code) or varied (different training data/model, but same code) conditions, and exploring different ways of comparing KPG models beyond the most commonly used evaluation measures. We drew some conclusions on the state of the art in KPG based on these experiments, and provided guidelines for researchers working on the topic about reporting experimental results in a more comprehensive manner.</abstract>
      <url hash="251ebef9">2024.lrec-main.849</url>
      <bibkey>thomas-vajjala-2024-keyphrase-generation</bibkey>
    </paper>
    <paper id="850">
      <title><fixed-case>KGC</fixed-case>onv, a Conversational Corpus Grounded in <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Quentin</first><last>Brabant</last></author>
      <author><first>Lina M.</first><last>Rojas Barahona</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>9732–9742</pages>
      <abstract>We present KGConv, a large corpus of 71k English conversations where each question-answer pair is grounded in a Wikidata fact. Conversations contain on average 8.6 questions and for each Wikidata fact, we provide multiple variants (12 on average) of the corresponding question using templates, human annotations, hand-crafted rules and a question rewriting neural model. We provide baselines for the task of Knowledge-Based, Conversational Question Generation. KGConv can further be used for other generation and analysis tasks such as single-turn question generation from Wikidata triples, question rewriting, question answering from conversation or from knowledge graphs and quiz generation.</abstract>
      <url hash="c07be78a">2024.lrec-main.850</url>
      <bibkey>brabant-etal-2024-kgconv-conversational</bibkey>
    </paper>
    <paper id="851">
      <title>Khan Academy Corpus: A Multilingual Corpus of Khan Academy Lectures</title>
      <author><first>Dominika</first><last>Ďurišková</last></author>
      <author><first>Daniela</first><last>Jurášová</last></author>
      <author><first>Matúš</first><last>Žilinec</last></author>
      <author><first>Eduard</first><last>Šubert</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>9743–9752</pages>
      <abstract>We present the Khan Academy Corpus totalling 10122 hours in 87394 recordings across 29 languages, where 43% of recordings (4252 hours) are equipped with human-written subtitles. The subtitle texts cover a total of 137 languages. The dataset was collected from open access Khan Academy lectures, benefiting from their manual transcripts and manual translations of the transcripts. The dataset can serve in creation or evaluation of multilingual speech recognition or translation systems, featuring a diverse set of subject domains.</abstract>
      <url hash="fd082de2">2024.lrec-main.851</url>
      <bibkey>duriskova-etal-2024-khan-academy</bibkey>
    </paper>
    <paper id="852">
      <title>Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic Information</title>
      <author><first>Chihiro</first><last>Taguchi</last></author>
      <author><first>Jefferson</first><last>Saransig</last></author>
      <author><first>Dayana</first><last>Velásquez</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>9753–9763</pages>
      <abstract>This paper presents Killkan, the first dataset for automatic speech recognition (ASR) in the Kichwa language, an indigenous language of Ecuador. Kichwa is an extremely low-resource endangered language, and there have been no resources before Killkan for Kichwa to be incorporated in applications of natural language processing. The dataset contains approximately 4 hours of audio with transcription, translation into Spanish, and morphosyntactic annotation in the format of Universal Dependencies, all done in ELAN, the annotation software. The audio data was retrieved from a publicly available radio program in Kichwa. This paper also provides corpus-linguistic analyses of the dataset with a special focus on the agglutinative morphology of Kichwa and frequent code-switching with Spanish. The experiments show that the dataset makes it possible to develop the first ASR system for Kichwa with reliable quality despite its small dataset size. This dataset, the ASR model, and the code used to develop them will be publicly available. Thus, our study positively showcases resource building and its applications for low-resource languages and their community.</abstract>
      <url hash="b11ac7ba">2024.lrec-main.852</url>
      <bibkey>taguchi-etal-2024-killkan-automatic</bibkey>
    </paper>
    <paper id="853">
      <title><fixed-case>KIT</fixed-case>-19: A Comprehensive <fixed-case>K</fixed-case>orean Instruction Toolkit on 19 Tasks for Fine-Tuning <fixed-case>K</fixed-case>orean Large Language Models</title>
      <author><first>Dongjun</first><last>Jang</last></author>
      <author><first>Sungjoo</first><last>Byun</last></author>
      <author><first>Hyemi</first><last>Jo</last></author>
      <author><first>Hyopil</first><last>Shin</last></author>
      <pages>9764–9776</pages>
      <abstract>Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in the specific tasks. Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper, We introduce <i>KIT-19</i> as an instruction dataset for the development of LLM in Korean. <i>KIT-19</i> is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained LLM using <i>KIT-19</i> to demonstrate its effectiveness. The experimental results show that the model trained on <i>KIT-19</i> significantly outperforms existing Korean LLMs. Based on the its quality and empirical results, this paper proposes that <i>KIT-19</i> has the potential to make a substantial contribution to the future improvement of Korean LLMs’ performance.</abstract>
      <url hash="3deb5827">2024.lrec-main.853</url>
      <bibkey>jang-etal-2024-kit-19</bibkey>
    </paper>
    <paper id="854">
      <title>Know-Adapter: Towards Knowledge-Aware Parameter-Efficient Transfer Learning for Few-shot Named Entity Recognition</title>
      <author><first>Binling</first><last>Nie</last></author>
      <author><first>Yiming</first><last>Shao</last></author>
      <author><first>Yigang</first><last>Wang</last></author>
      <pages>9777–9786</pages>
      <abstract>Parameter-Efficient Fine-Tuning (PEFT) is a promising approach to mitigate the challenges about the model adaptation of pretrained language models (PLMs) for the named entity recognition (NER) task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding explicit knowledge from external source like KGs to otherwise naive PEFTs. In this paper, we propose a novel knowledgeable adapter, Know-adapter, to incorporate structure and semantic knowledge of knowledge graphs into PLMs for few-shot NER. First, we construct a related KG entity type sequence for each sentence using a knowledge retriever. However, the type system of a domain-specific NER task is typically independent of that of current KGs and thus exhibits heterogeneity issue inevitably, which makes matching between the original NER and KG types (e.g. Person in NER potentially matches President in KBs) less likely, or introduces unintended noises. Thus, then we design a unified taxonomy based on KG ontology for KG entity types and NER labels. This taxonomy is used to build a learnable shared representation module, which provides shared representations for both KG entity type sequences and NER labels. Based on these shared representations, our Know-adapter introduces high semantic relevance knowledge and structure knowledge from KGs as inductive bias to guide the updating process of the adapter. Additionally, the shared representations guide the learnable representation module to reduce noise in the unsupervised expansion of label words. Extensive experiments on multiple NER datasets show the superiority of Know-Adapter over other state-of-the-art methods in both full-resource and low-resource settings.</abstract>
      <url hash="636ac531">2024.lrec-main.854</url>
      <bibkey>nie-etal-2024-know-adapter</bibkey>
    </paper>
    <paper id="855">
      <title>Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection</title>
      <author><first>Ya</first><last>Gao</last></author>
      <author><first>Shaoxiong</first><last>Ji</last></author>
      <author><first>Pekka</first><last>Marttinen</last></author>
      <pages>9787–9798</pages>
      <abstract>Adverse drug events (ADEs) are an important aspect of drug safety. Various texts such as biomedical literature, drug reviews, and user posts on social media and medical forums contain a wealth of information about ADEs. Recent studies have applied word embedding and deep learning-based natural language processing to automate ADE detection from text. However, they did not explore incorporating explicit medical knowledge about drugs and adverse reactions or the corresponding feature learning. This paper adopts the heterogeneous text graph, which describes relationships between documents, words, and concepts, augments it with medical knowledge from the Unified Medical Language System, and proposes a concept-aware attention mechanism that learns features differently for the different types of nodes in the graph. We further utilize contextualized embeddings from pretrained language models and convolutional graph neural networks for effective feature representation and relational learning. Experiments on four public datasets show that our model performs competitively to the recent advances, and the concept-aware attention consistently outperforms other attention mechanisms.</abstract>
      <url hash="72980da8">2024.lrec-main.855</url>
      <bibkey>gao-etal-2024-knowledge-augmented</bibkey>
      <revision id="1" href="2024.lrec-main.855v1" hash="a3814096"/>
      <revision id="2" href="2024.lrec-main.855v2" hash="72980da8" date="2024-05-21">Minor update.</revision>
    </paper>
    <paper id="856">
      <title>Knowledge-aware Attention Network for Medication Effectiveness Prediction</title>
      <author><first>Yingying</first><last>Zhang</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>9799–9809</pages>
      <abstract>The first 24 hours’ medication plan is critical to patients with serious or life-threatening illnesses and injuries. An appropriate medication can result in a lower mortality, a shorter length stay and a higher APACHE score. However, in clinical practice, the medication plan is often error-prone, especially when a decision must be made quickly for life-threatening situations in Intensive Care Unit (ICU). Therefore, predicting the effectiveness of the first 24 hours’ medication plan is of great importance in assisting doctors to make proper decisions. Existing effectiveness prediction works usually focus on one specific medicine, one specific disease, or one specific lab test, making it hard to extend to general medicines and diseases in hospital/ICU scenarios. In this paper, we propose to predict medication effectiveness of the first 24 hours in hospital/ICU based on patients’ information. Specifically, we use a knowledge enhanced module to incorporate external knowledge about medications and a medical feature learning module to determine the interaction between diagnosis and medications. To handle the data imbalance problem, we further optimize the proposed model with a contrastive loss. Extensive experimental results on a public dataset show that our model can significantly outperform state-of-the-art methods.</abstract>
      <url hash="e79160b4">2024.lrec-main.856</url>
      <bibkey>zhang-etal-2024-knowledge-aware-attention</bibkey>
    </paper>
    <paper id="857">
      <title>Knowledge Enhanced Pre-training for Cross-lingual Dense Retrieval</title>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Shunyu</first><last>Zhang</last></author>
      <author><first>Xingwei</first><last>He</last></author>
      <author><first>Jiancheng</first><last>Lv</last></author>
      <author><first>Jian</first><last>Guo</last></author>
      <pages>9810–9821</pages>
      <abstract>In recent years, multilingual pre-trained language models (mPLMs) have achieved significant progress in cross-lingual dense retrieval. However, most mPLMs neglect the importance of knowledge. Knowledge always conveys similar semantic concepts in a language-agnostic manner, while query-passage pairs in cross-lingual retrieval also share common factual information. Motivated by this observation, we introduce KEPT, a novel mPLM that effectively leverages knowledge to learn language-agnostic semantic representations. To achieve this, we construct a multilingual knowledge base using hyperlinks and cross-language page alignment data annotated by Wiki. From this knowledge base, we mine intra- and cross-language pairs by extracting symmetrically linked segments and multilingual entity descriptions. Subsequently, we adopt contrastive learning with the mined pairs to pre-train KEPT. We evaluate KEPT on three widely-used benchmarks, considering both zero-shot cross-lingual transfer and supervised multilingual fine-tuning scenarios. Extensive experimental results demonstrate that KEPT achieves strong multilingual and cross-lingual retrieval performance with significant improvements over existing mPLMs.</abstract>
      <url hash="bc1a52f8">2024.lrec-main.857</url>
      <bibkey>zhang-etal-2024-knowledge-enhanced-pre</bibkey>
    </paper>
    <paper id="858">
      <title>Knowledge-enhanced Prompt Tuning for Dialogue-based Relation Extraction with Trigger and Label Semantic</title>
      <author><first>Hao</first><last>An</last></author>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Zhiqi</first><last>Huang</last></author>
      <author><first>Yuexian</first><last>Zou</last></author>
      <pages>9822–9831</pages>
      <abstract>Dialogue-based relation extraction (DRE) aims to determine the semantic relation of a given pair of arguments from a piece of dialogue, which has received increasing attention. Due to the low information density of dialogue text, it is difficult for the model to focus on key information. To this end, in this paper, we propose a Knowledge-Enhanced Prompt-Tuning (KEPT) method to effectively enhance DRE model by exploiting trigger and label semantic. Specifically, we propose two beneficial tasks, masked trigger prediction, and verbalizer representation learning, to effectively inject trigger knowledge and label semantic knowledge respectively. Furthermore, we convert the DRE task to a masked language modeling task to unify the format of knowledge injection and utilization, aiming to better promote DRE performance. Experimental results on the DialogRE dataset show that our KEPT achieves state-of-the-art performance in F1 and F1c scores. Detailed analyses demonstrate the effectiveness and efficiency of our proposed approach. Code is available at https://github.com/blackbookay/KEPT.</abstract>
      <url hash="b12a1a80">2024.lrec-main.858</url>
      <bibkey>an-etal-2024-knowledge-enhanced</bibkey>
    </paper>
    <paper id="859">
      <title>Knowledge <fixed-case>G</fixed-case>eo<fixed-case>G</fixed-case>ebra: Leveraging Geometry of Relation Embeddings in Knowledge Graph Completion</title>
      <author><first>Kossi</first><last>Amouzouvi</last></author>
      <author><first>Bowen</first><last>Song</last></author>
      <author><first>Sahar</first><last>Vahdati</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <pages>9832–9842</pages>
      <abstract>Knowledge graph embedding (KGE) models provide a low-dimensional representation of knowledge graphs in continuous vector spaces. This representation learning enables different downstream AI tasks such as link prediction for graph completion. However, most embedding models are only designed considering the algebra and geometry of the entity embedding space, the algebra of the relation embedding space, and the interaction between relation and entity embeddings. Neglecting the geometry of relation embedding limits the optimization of entity and relation distribution leading to suboptimal performance of knowledge graph completion. To address this issue, we propose a new perspective in the design of KGEs by looking into the geometry of relation embedding space. The proposed method and its variants are developed on top of an existing framework, RotatE, from which we leverage the geometry of the relation embeddings by mutating the unit circle to an ellipse, and further generalize it with the concept of a butterfly curve, consecutively. Besides the theoretical abilities of the model in preserving topological and relational patterns, the experiments on the WN18RR, FB15K-237 and YouTube benchmarks showed that this new family of KGEs can challenge or outperform state-of-the-art models.</abstract>
      <url hash="171e584e">2024.lrec-main.859</url>
      <bibkey>amouzouvi-etal-2024-knowledge-geogebra</bibkey>
    </paper>
    <paper id="860">
      <title>Knowledge Graphs for Real-World Rumour Verification</title>
      <author><first>John</first><last>Dougrez-Lewis</last></author>
      <author><first>Elena</first><last>Kochkina</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>9843–9853</pages>
      <abstract>Despite recent progress in automated rumour verification, little has been done on evaluating rumours in a real-world setting. We advance the state-of-the-art on the PHEME dataset, which consists of Twitter response threads collected as a rumour was unfolding. We automatically collect evidence relevant to PHEME and use it to construct knowledge graphs in a time-sensitive manner, excluding information post-dating rumour emergence. We identify discrepancies between the evidence retrieved and PHEME’s labels, which are discussed in detail and amended to release an updated dataset. We develop a novel knowledge graph approach which finds paths linking disjoint fragments of evidence. Our rumour verification model which combines evidence from the graph outperforms the state-of-the-art on PHEME and has superior generisability when evaluated on a temporally distant rumour verification dataset.</abstract>
      <url hash="09e678f2">2024.lrec-main.860</url>
      <bibkey>dougrez-lewis-etal-2024-knowledge-graphs</bibkey>
    </paper>
    <paper id="861">
      <title>Knowledge-Guided Cross-Topic Visual Question Generation</title>
      <author><first>Hongfei</first><last>Liu</last></author>
      <author><first>Guohua</first><last>Wang</last></author>
      <author><first>Jiayuan</first><last>Xie</last></author>
      <author><first>Jiali</first><last>Chen</last></author>
      <author><first>Wenhao</first><last>Fang</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <pages>9854–9864</pages>
      <abstract>Visual question generation (VQG) task aims to generate high-quality questions based on the input image. Current methods primarily focus on generating questions containing specified content utilizing answers or question types as constraints. However, these constraints make it challenging to control the topic of generated questions (e.g., conversation or test subject topics) for various applications. Thus, it is necessary to utilize topics as constraints to guide question generation. Considering that there are many topics and it is almost impossible for human annotations to cover them, we propose the cross-topic learning VQG (CTL-VQG) task, which aims to generate questions related to unseen topics in cross-topic scenarios. In this paper, we propose a knowledge-guided cross-topic visual question generation (KC-VQG) model to extract unseen topic-related information for question generation. Specifically, an image-topic feature extractor is introduced in our model to extract topic-related intuitive visual features; an image-topic knowledge extractor is used to extract and select the most appropriate topic-related implicit knowledge from large language models for generating questions. Extensive experiments show that our model outperforms baselines and can effectively generate unseen topic-related questions in cross-topic scenarios.</abstract>
      <url hash="8cd3873e">2024.lrec-main.861</url>
      <bibkey>liu-etal-2024-knowledge-guided</bibkey>
    </paper>
    <paper id="862">
      <title>Knowledge Triplets Derivation from Scientific Publications via Dual-Graph Resonance</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Pengcheng</first><last>Li</last></author>
      <author><first>Kaisong</first><last>Song</last></author>
      <author><first>Xurui</first><last>Li</last></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Xuhong</first><last>Zhang</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <pages>9865–9877</pages>
      <abstract>Scientific Information Extraction (SciIE) is a vital task and is increasingly being adopted in biomedical data mining to conceptualize and epitomize knowledge triplets from the scientific literature. Existing relation extraction methods aim to extract explicit triplet knowledge from documents, however, they can hardly perceive unobserved factual relations. Recent generative methods have more flexibility, but their generated relations will encounter trustworthiness problems. In this paper, we first propose a novel Extraction-Contextualization-Derivation (ECD) strategy to generate a document-specific and entity-expanded dynamic graph from a shared static knowledge graph. Then, we propose a novel Dual-Graph Resonance Network (DGRN) which can generate richer explicit and implicit relations under the guidance of static and dynamic knowledge topologies. Experiments conducted on a public PubMed corpus validate the superiority of our method against several state-of-the-art baselines.</abstract>
      <url hash="68ad5a97">2024.lrec-main.862</url>
      <bibkey>zhang-etal-2024-knowledge-triplets-derivation</bibkey>
    </paper>
    <paper id="863">
      <title><fixed-case>K</fixed-case>now<fixed-case>V</fixed-case>r<fixed-case>DU</fixed-case>: A Unified Knowledge-aware Prompt-Tuning Framework for Visually-rich Document Understanding</title>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Jingzhe</first><last>Zhu</last></author>
      <author><first>Jinyu</first><last>Xu</last></author>
      <author><first>Shuai</first><last>Yang</last></author>
      <author><first>Zhaoliang</first><last>Wu</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <author><first>Shuai</first><last>Chen</last></author>
      <pages>9878–9889</pages>
      <abstract>In Visually-rich Document Understanding (VrDU), recent advances of incorporating layout and image features into the pre-training language models have achieved significant progress. Existing methods usually developed complicated dedicated architectures based on pre-trained models and fine-tuned them with costly high-quality data to eliminate the inconsistency of knowledge distribution between the pre-training task and specialized downstream tasks. However, due to their huge data demands, these methods are not suitable for few-shot settings, which are essential for quick applications with limited resources but few previous works are presented. To solve these problems, we propose a unified Knowledge-aware prompt-tuning framework for Visual-rich Document Understanding (KnowVrDU) to enable broad utilization for diverse concrete applications and reduce data requirements. To model heterogeneous VrDU structures without designing task-specific architectures, we propose to reformulate various VrDU tasks into a single question-answering format with task-specific prompts and train the pre-trained model with the parameter-efficient prompt tuning method. To bridge the knowledge gap between the pre-training task and specialized VrDU tasks without additional annotations, we propose a prompt knowledge integration mechanism to leverage external open-source knowledge bases. We conduct experiments on several benchmark datasets in few-shot settings and the results validate the effectiveness of our method.</abstract>
      <url hash="eec045eb">2024.lrec-main.863</url>
      <bibkey>zhang-etal-2024-knowvrdu-unified</bibkey>
    </paper>
    <paper id="864">
      <title><fixed-case>K</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>S</fixed-case>a: <fixed-case>K</fixed-case>orean Context-aware Sarcasm Detection Dataset</title>
      <author><first>Yumin</first><last>Kim</last></author>
      <author><first>Heejae</first><last>Suh</last></author>
      <author><first>Mingi</first><last>Kim</last></author>
      <author><first>Dongyeon</first><last>Won</last></author>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <pages>9890–9904</pages>
      <abstract>Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on the dataset show that our baseline system outperforms strong baselines like large language models, such as GPT-3.5, in the Korean sarcasm detection task. We show that the sarcasm detection task relies deeply on the existence of sufficient context. We will release the dataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.</abstract>
      <url hash="de587f5b">2024.lrec-main.864</url>
      <bibkey>kim-etal-2024-kocosa-korean</bibkey>
    </paper>
    <paper id="865">
      <title><fixed-case>K</fixed-case>o<fixed-case>D</fixed-case>ialog<fixed-case>B</fixed-case>ench: Evaluating Conversational Understanding of Language Models with <fixed-case>K</fixed-case>orean Dialogue Benchmark</title>
      <author><first>Seongbo</first><last>Jang</last></author>
      <author><first>Seonghyeon</first><last>Lee</last></author>
      <author><first>Hwanjo</first><last>Yu</last></author>
      <pages>9905–9925</pages>
      <abstract>As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user’s first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models’ conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improvement in models’ conversation skills. Furthermore, our in-depth comparisons across different language models highlight the effectiveness of recent training techniques in enhancing conversational proficiency. We anticipate that KoDialogBench will promote the progress towards conversation-aware Korean language models.</abstract>
      <url hash="e9718a52">2024.lrec-main.865</url>
      <bibkey>jang-etal-2024-kodialogbench-evaluating</bibkey>
    </paper>
    <paper id="866">
      <title><fixed-case>K</fixed-case>o<fixed-case>FREN</fixed-case>: Comprehensive <fixed-case>K</fixed-case>orean Word Frequency Norms Derived from Large Scale Free Speech Corpora</title>
      <author><first>Jin-seo</first><last>Kim</last></author>
      <author><first>Anna Seo Gyeong</first><last>Choi</last></author>
      <author><first>Sunghye</first><last>Cho</last></author>
      <pages>9926–9931</pages>
      <abstract>Word frequencies are integral in linguistic studies, showing strong correlations with speakers’ cognitive abilities and other important linguistic parameters including the Age of Acquisition (AoA). However, the formulation of credible Korean word frequency norms has been obstructed by the lack of expansive speech data and a reliable part-ofspeech (POS) tagger. In this study, we unveil Korean word frequency norms (KoFREN), derived from large-scale spontaneous speech corpora (41 million words) that include a balanced representation of gender and age. We employed a machine learning-powered POS tagger, showcasing accuracy on par with human annotators. Our frequency norms correlate significantly with external studies’ lexical decision time (LDT) and AoA measures. KoFREN also aligns with English counterparts sourced from SUBTLEX_US - an English word frequency measure that has been frequently used in the literature. KoFREN is poised to facilitate research in spontaneous Contemporary Korean and can be utilized in many fields, including clinical studies of Korean patients.</abstract>
      <url hash="ab053196">2024.lrec-main.866</url>
      <bibkey>kim-etal-2024-kofren-comprehensive</bibkey>
    </paper>
    <paper id="867">
      <title>Konidioms Corpus: A Dataset of Idioms in <fixed-case>K</fixed-case>onkani Language</title>
      <author><first>Naziya Mahamdul</first><last>Shaikh</last></author>
      <author><first>Jyoti D.</first><last>Pawar</last></author>
      <author><first>Mubarak Banu</first><last>Sayed</last></author>
      <pages>9932–9940</pages>
      <abstract>Konkani is a language spoken by a large number of people from the states located in the west coast of India. It is the official language of Goa state from the Indian subcontinent. Currently there is a lack of idioms corpus in the low-resource Konkani language. This paper aims to improve the progress in idiomatic sentence identification in order to enhance linguistic processing by creating the first corpus for idioms in the Konkani language. We select a unique list of 1597 idioms from multiple sources and proceed with a strictly controlled sentence creation procedure through crowdsourcing. This is followed by quality check of the sentences and annotation procedure by the experts in the Konkani language. We were able to build a good quality corpus comprising of 6520 sentences written in the Devanagari script of Konkani language. Analysis of the collected idioms and their usage in the created sentences revealed the dominance of selective domains like ‘human body’ in the creation and occurrences of idiomatic expressions in the Konkani language. This corpus is made publicly available.</abstract>
      <url hash="25af815f">2024.lrec-main.867</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1bfe6813">2024.lrec-main.867.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>shaikh-etal-2024-konidioms-corpus</bibkey>
    </paper>
    <paper id="868">
      <title><fixed-case>K</fixed-case>orean Bio-Medical Corpus (<fixed-case>KBMC</fixed-case>) for Medical Named Entity Recognition</title>
      <author><first>Sungjoo</first><last>Byun</last></author>
      <author><first>Jiseung</first><last>Hong</last></author>
      <author><first>Sumin</first><last>Park</last></author>
      <author><first>Dongjun</first><last>Jang</last></author>
      <author><first>Jean</first><last>Seo</last></author>
      <author><first>Minseok</first><last>Kim</last></author>
      <author><first>Chaeyoung</first><last>Oh</last></author>
      <author><first>Hyopil</first><last>Shin</last></author>
      <pages>9941–9947</pages>
      <abstract>Named Entity Recognition (NER) plays a pivotal role in medical Natural Language Processing (NLP). Yet, there has not been an open-source medical NER dataset specifically for the Korean language. To address this, we utilized ChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which we are now presenting to the public. With the KBMC dataset, we noticed an impressive 20% increase in medical NER performance compared to models trained on general Korean NER datasets. This research underscores the significant benefits and importance of using specialized tools and datasets, like ChatGPT, to enhance language processing in specialized fields such as healthcare.</abstract>
      <url hash="7ba7a81e">2024.lrec-main.868</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0ba5a93e">2024.lrec-main.868.OptionalSupplementaryMaterial.tsv</attachment>
      <bibkey>byun-etal-2024-korean-bio</bibkey>
    </paper>
    <paper id="869">
      <title><fixed-case>K</fixed-case>orean Disaster Safety Information Sign Language Translation Benchmark Dataset</title>
      <author><first>Wooyoung</first><last>Kim</last></author>
      <author><first>TaeYong</first><last>Kim</last></author>
      <author><first>Byeongjin</first><last>Kim</last></author>
      <author><first>Myeong Jin MJ</first><last>Lee</last></author>
      <author><first>Gitaek</first><last>Lee</last></author>
      <author><first>Kirok</first><last>Kim</last></author>
      <author><first>Jisoo</first><last>Cha</last></author>
      <author><first>Wooju</first><last>Kim</last></author>
      <pages>9948–9953</pages>
      <abstract>Sign language is a crucial means of communication for deaf communities. However, those outside deaf communities often lack understanding of sign language, leading to inadequate communication accessibility for the deaf. Therefore, sign language translation is a significantly important research area. In this context, we present a new benchmark dataset for Korean sign language translation named SSL:korean disaster Safety information Sign Language translation benchmark dataset. Korean sign language translation datasets provided by the National Information Society Agency in South Korea have faced challenges related to computational resources, heterogeneity between train and test sets, and unrefined data. To alleviate the aforementioned issue, we refine the origin data and release them. Additionally, we report experimental results of baseline using a transformer architecture. We empirically demonstrate that the baseline performance varies depending on the tokenization method applied to gloss sequences. In particular, tokenization based on characteristics of sign language outperforms tokenization considering characteristics of spoken language and tokenization utilizing statistical techniques. We release materials at our https://github.com/SSL-Sign-Language/Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset</abstract>
      <url hash="99b4d660">2024.lrec-main.869</url>
      <bibkey>kim-etal-2024-korean-disaster</bibkey>
    </paper>
    <paper id="870">
      <title>Kosmic: <fixed-case>K</fixed-case>orean Text Similarity Metric Reflecting Honorific Distinctions</title>
      <author><first>Yerin</first><last>Hwang</last></author>
      <author><first>Yongil</first><last>Kim</last></author>
      <author><first>Hyunkyung</first><last>Bae</last></author>
      <author><first>Jeesoo</first><last>Bang</last></author>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>9954–9960</pages>
      <abstract>Existing English-based text similarity measurements primarily focus on the semantic dimension, neglecting the unique linguistic attributes found in languages like Korean, where honorific expressions are explicitly integrated. To address this limitation, this study proposes Kosmic, a novel Korean text-similarity metric that encompasses the semantic and tonal facets of a given text pair. For the evaluation, we introduce a novel benchmark annotated by human experts, empirically showing that Kosmic outperforms the existing method. Moreover, by leveraging Kosmic, we assess various Korean paraphrasing methods to determine which techniques are most effective in preserving semantics and tone.</abstract>
      <url hash="a167a51c">2024.lrec-main.870</url>
      <bibkey>hwang-etal-2024-kosmic-korean</bibkey>
    </paper>
    <paper id="871">
      <title><fixed-case>KP</fixed-case>atch: Knowledge Patch to Pre-trained Language Model for Zero-Shot Stance Detection on Social Media</title>
      <author><first>Shuohao</first><last>Lin</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Yunpeng</first><last>Gao</last></author>
      <author><first>Zhishu</first><last>Jiang</last></author>
      <author><first>Mengqi</first><last>Liao</last></author>
      <author><first>Zhiyu</first><last>Zhang</last></author>
      <author><first>Shuyuan</first><last>Zhao</last></author>
      <author><first>Huaiyu</first><last>Wan</last></author>
      <pages>9961–9973</pages>
      <abstract>Zero-shot stance detection on social media (ZSSD-SM) aims to distinguish the attitude in tweets towards an unseen target. Previous work capture latent variables between source and target domains to perform this task, but the lack of context knowledge hinders the detection performance. Recent studies have been devoted to obtaining the accurate representation of tweets by bringing additional facts from Knowledge Graph (KG), showing promising performance. However, these knowledge injection methods still suffer from two challenges: (i) The pipeline of knowledge injection causes error accumulation and (ii) irrelevant knowledge makes them fail to understand the semantics. In this paper, we propose a novel knowledge injection method for ZSSD-SM, which adopts two training stages, namely knowledge compression and task guidance, to flexibly inject knowledge into the pre-trained language model (PLM) and adaptively expand tweets context. Specifically, in the knowledge compression stage, the latent representation of KG is reconstructed by the triplet denoising task and compressed into external matrices; while in the task guidance stage, the frozen matrices are employed to guide the PLM to adaptively extract its own context-related knowledge, and then complete the fine-tuning of the ZSSD-SM task. Extensive experiments on multiple datasets show the effectiveness of our proposed method. The code is available at: https://github.com/ShuohaoLin/KPatch.</abstract>
      <url hash="9e04442d">2024.lrec-main.871</url>
      <bibkey>lin-etal-2024-kpatch-knowledge</bibkey>
    </paper>
    <paper id="872">
      <title>K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling</title>
      <author><first>Haven</first><last>Kim</last></author>
      <author><first>Jongmin</first><last>Jung</last></author>
      <author><first>Dasaem</first><last>Jeong</last></author>
      <author><first>Juhan</first><last>Nam</last></author>
      <pages>9974–9987</pages>
      <abstract>Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations.</abstract>
      <url hash="581ba859">2024.lrec-main.872</url>
      <bibkey>kim-etal-2024-k-pop</bibkey>
      <revision id="1" href="2024.lrec-main.872v1" hash="141fb956"/>
      <revision id="2" href="2024.lrec-main.872v2" hash="581ba859" date="2024-06-13">Citation error corrected.</revision>
    </paper>
    <paper id="873">
      <title><fixed-case>L</fixed-case>ˆ2<fixed-case>GC</fixed-case>:Lorentzian Linear Graph Convolutional Networks for Node Classification</title>
      <author><first>Qiuyu</first><last>Liang</last></author>
      <author><first>Weihua</first><last>Wang</last></author>
      <author><first>Feilong</first><last>Bao</last></author>
      <author><first>Guanglai</first><last>Gao</last></author>
      <pages>9988–9998</pages>
      <abstract>Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7% on Citeseer and 81.3% on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitude faster than other nonlinear GCN models on PubMed dataset. Our code is publicly available at https://github.com/llqy123/LLGC-master.</abstract>
      <url hash="0f4945de">2024.lrec-main.873</url>
      <bibkey>liang-etal-2024-l-2gc</bibkey>
    </paper>
    <paper id="874">
      <title>Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model</title>
      <author><first>Elaheh</first><last>Baharlouei</last></author>
      <author><first>Mahsa</first><last>Shafaei</last></author>
      <author><first>Yigeng</first><last>Zhang</last></author>
      <author><first>Hugo Jair</first><last>Escalante</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>9999–10013</pages>
      <abstract>We address the challenge of detecting questionable content in online media, specifically the subcategory of comic mischief. This type of content combines elements such as violence, adult content, or sarcasm with humor, making it difficult to detect. Employing a multimodal approach is vital to capture the subtle details inherent in comic mischief content. To tackle this problem, we propose a novel end-to-end multimodal system for the task of comic mischief detection. As part of this contribution, we release a novel dataset for the targeted task consisting of three modalities: video, text (video captions and subtitles), and audio. We also design a HIerarchical Cross-attention model with CAPtions (HICCAP) to capture the intricate relationships among these modalities. The results show that the proposed approach makes a significant improvement over robust baselines and state-of-the-art models for comic mischief detection and its type classification. This emphasizes the potential of our system to empower users, to make informed decisions about the online content they choose to see.</abstract>
      <url hash="65fcf5b4">2024.lrec-main.874</url>
      <bibkey>baharlouei-etal-2024-labeling-comic</bibkey>
    </paper>
    <paper id="875">
      <title>Labeling Results of Topic Models: Word Sense Disambiguation as Key Method for Automatic Topic Labeling with <fixed-case>G</fixed-case>erma<fixed-case>N</fixed-case>et</title>
      <author><first>Jennifer</first><last>Ecker</last></author>
      <pages>10014–10022</pages>
      <abstract>The combination of topic modeling and automatic topic labeling sheds light on understanding large corpora of text. It can be used to add semantic information for existing metadata. In addition, one can use the documents and the corresponding topic labels for topic classification. While there are existing algorithms for topic modeling readily accessible for processing texts, there is a need to postprocess the result to make the topics more interpretable and self-explanatory. The topic words from the topic model are ranked and the first/top word could easily be considered as a label. However, it is imperative to use automatic topic labeling, because the highest scored word is not the word that sums up the topic in the best way. Using the lexical-semantic word net GermaNet, the first step is to disambiguate words that are represented in GermaNet with more than one sense. We show how to find the correct sense in the context of a topic with the method of word sense disambiguation. To enhance accuracy, we present a similarity measure based on vectors of topic words that considers semantic relations of the senses demonstrating superior performance of the investigated cases compared to existing methods.</abstract>
      <url hash="b3995c92">2024.lrec-main.875</url>
      <bibkey>ecker-2024-labeling-results</bibkey>
    </paper>
    <paper id="876">
      <title>Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization</title>
      <author><first>Linzhi</first><last>Wu</last></author>
      <author><first>Xingyu</first><last>Zhang</last></author>
      <author><first>Yakun</first><last>Zhang</last></author>
      <author><first>Changyan</first><last>Zheng</last></author>
      <author><first>Tiejun</first><last>Liu</last></author>
      <author><first>Liang</first><last>Xie</last></author>
      <author><first>Ye</first><last>Yan</last></author>
      <author><first>Erwei</first><last>Yin</last></author>
      <pages>10023–10033</pages>
      <abstract>Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, a max-min mutual information regularization approach is proposed to capture speaker-insensitive latent representations. Experimental evaluations on public lip reading datasets demonstrate the effectiveness of the proposed approach under the intra-speaker and inter-speaker conditions.</abstract>
      <url hash="0c5666ba">2024.lrec-main.876</url>
      <attachment type="OptionalSupplementaryMaterial" hash="397786a0">2024.lrec-main.876.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>wu-etal-2024-landmark-guided</bibkey>
    </paper>
    <paper id="877">
      <title>Language and Speech Technology for <fixed-case>C</fixed-case>entral <fixed-case>K</fixed-case>urdish Varieties</title>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Daban</first><last>Jaff</last></author>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>10034–10045</pages>
      <abstract>Kurdish, an Indo-European language spoken by over 30 million speakers, is considered a dialect continuum and known for its diversity in language varieties. Previous studies addressing language and speech technology for Kurdish handle it in a monolithic way as a macro-language, resulting in disparities for dialects and varieties for which there are few resources and tools available. In this paper, we take a step towards developing resources for language and speech technology for varieties of Central Kurdish, creating a corpus by transcribing movies and TV series as an alternative to fieldwork. Additionally, we report the performance of machine translation, automatic speech recognition, and language identification as downstream tasks evaluated on Central Kurdish subdialects. Data and models are publicly available under an open license at https://github.com/sinaahmadi/CORDI.</abstract>
      <url hash="8894a0bc">2024.lrec-main.877</url>
      <bibkey>ahmadi-etal-2024-language-speech</bibkey>
    </paper>
    <paper id="878">
      <title>Language Models and Semantic Relations: A Dual Relationship</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>10046–10057</pages>
      <abstract>Since they rely on the distributional hypothesis, static and contextual language models are closely linked to lexical semantic relations. In this paper, we exploit this link for enhancing a BERT model. More precisely, we propose to extract lexical semantic relations with two unsupervised methods, one based on a static language model, the other on a contextual model, and to inject the extracted relations into a BERT model for improving its semantic capabilities. Through various evaluations performed for English and focusing on semantic similarity at the word and sentence levels, we show the interest of this approach, allowing us to semantically enrich a BERT model without using any external semantic resource.</abstract>
      <url hash="dfba3512">2024.lrec-main.878</url>
      <bibkey>ferret-2024-language-models</bibkey>
    </paper>
    <paper id="879">
      <title>Language Models for Text Classification: Is In-Context Learning Enough?</title>
      <author><first>Aleksandra</first><last>Edwards</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>10058–10072</pages>
      <abstract>Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.</abstract>
      <url hash="485b10e3">2024.lrec-main.879</url>
      <bibkey>edwards-camacho-collados-2024-language-models</bibkey>
    </paper>
    <paper id="880">
      <title>Language Pivoting from Parallel Corpora for Word Sense Disambiguation of Historical Languages: A Case Study on <fixed-case>L</fixed-case>atin</title>
      <author><first>Iacopo</first><last>Ghinassi</last></author>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Paola</first><last>Marongiu</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <author><first>Barbara</first><last>McGillivray</last></author>
      <pages>10073–10084</pages>
      <abstract>Word Sense Disambiguation (WSD) is an important task in NLP, which serves the purpose of automatically disambiguating a polysemous word with its most likely sense in context. Recent studies have advanced the state of the art in this task, but most of the work has been carried out on contemporary English or other modern languages, leaving challenges posed by low-resource languages and diachronic change open. Although the problem with low-resource languages has recently been mitigated by using existing multilingual resources to propagate otherwise expensive annotations from English to other languages, such techniques have hitherto not been applied to historical languages such as Latin. In this work, we make the following two major contributions. First, we test such a strategy on a historical language and propose a new approach in this framework which makes use of existing bilingual corpora instead of native English datasets. Second, we fine-tune a Latin WSD model on the data produced and achieve state-of-the-art results on a standard benchmark for the task. Finally, we release the dataset generated with our approach, which is the largest dataset for Latin WSD to date. This work opens the door to further research, as our approach can be used for different historical and, generally, under-resourced languages.</abstract>
      <url hash="54549ab5">2024.lrec-main.880</url>
      <bibkey>ghinassi-etal-2024-language-pivoting</bibkey>
    </paper>
    <paper id="881">
      <title>Language Technologies as If People Mattered: Centering Communities in Language Technology Development</title>
      <author><first>Nina</first><last>Markl</last></author>
      <author><first>Lauren</first><last>Hall-Lew</last></author>
      <author><first>Catherine</first><last>Lai</last></author>
      <pages>10085–10099</pages>
      <abstract>In this position paper we argue that researchers interested in language and/or language technologies should attend to challenges of linguistic and algorithmic injustice together with language communities. We put forward that this can be done by drawing together diverse scholarly and experiential insights, building strong interdisciplinary teams, and paying close attention to the wider social, cultural and historical contexts of both language communities and the technologies we aim to develop.</abstract>
      <url hash="d88387cc">2024.lrec-main.881</url>
      <bibkey>markl-etal-2024-language-technologies</bibkey>
    </paper>
    <paper id="882">
      <title>Language Variety Identification with True Labels</title>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Kai</first><last>North</last></author>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Neha</first><last>Kumari</last></author>
      <author><first>Nishant</first><last>Nair</last></author>
      <author><first>Yash Mahesh</first><last>Bangera</last></author>
      <pages>10100–10109</pages>
      <abstract>Language identification is an important first step in many NLP applications. Most publicly available language identification datasets, however, are compiled under the assumption that the gold label of each instance is determined by where texts are retrieved from. Research has shown that this is a problematic assumption, particularly in the case of very similar languages (e.g., Croatian and Serbian) and national language varieties (e.g., Brazilian and European Portuguese), where texts may contain no distinctive marker of the particular language or variety. To overcome this important limitation, this paper presents DSL True Labels (DSL-TL), the first human-annotated multilingual dataset for language variety identification. DSL-TL contains a total of 12,900 instances in Portuguese, split between European Portuguese and Brazilian Portuguese; Spanish, split between Argentine Spanish and Castilian Spanish; and English, split between American English and British English. We trained multiple models to discriminate between these language varieties, and we present the results in detail. The data and models presented in this paper provide a reliable benchmark toward the development of robust and fairer language variety identification systems. We make DSL-TL freely available to the research community.</abstract>
      <url hash="cc85e75e">2024.lrec-main.882</url>
      <bibkey>zampieri-etal-2024-language-variety</bibkey>
    </paper>
    <paper id="883">
      <title><fixed-case>LANID</fixed-case>: <fixed-case>LLM</fixed-case>-assisted New Intent Discovery</title>
      <author><first>Lu</first><last>Fan</last></author>
      <author><first>Jiashu</first><last>Pu</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>Xiao-Ming</first><last>Wu</last></author>
      <pages>10110–10116</pages>
      <abstract>Data annotation is expensive in Task-Oriented Dialogue (TOD) systems. New Intent Discovery (NID) is a task aims to identify novel intents while retaining the ability to recognize known intents. It is essential for expanding the intent base of task-based dialogue systems. Previous works relying on external datasets are hardly extendable. Meanwhile, the effective ones are generally depends on the power of the Large Language Models (LLMs). To address the limitation of model extensibility and take advantages of LLMs for the NID task, we propose LANID, a framework that leverages LLM’s zero-shot capability to enhance the performance of a smaller text encoder on the NID task. LANID employs KNN and DBSCAN algorithms to select appropriate pairs of utterances from the training set. The LLM is then asked to determine the relationships between them. The collected data are then used to construct finetuning task and the small text encoder is optimized with a triplet loss. Our experimental results demonstrate the efficacy of the proposed method on three distinct NID datasets, surpassing all strong baselines in both unsupervised and semi-supervised settings. Our code can be found in https://github.com/floatSDSDS/LANID.</abstract>
      <url hash="745381c3">2024.lrec-main.883</url>
      <bibkey>fan-etal-2024-lanid-llm</bibkey>
    </paper>
    <paper id="884">
      <title>Large Language Models Are Echo Chambers</title>
      <author><first>Jan</first><last>Nehring</last></author>
      <author><first>Aleksandra</first><last>Gabryszak</last></author>
      <author><first>Pascal</first><last>Jürgens</last></author>
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Stefan</first><last>Schaffer</last></author>
      <author><first>Matthias</first><last>Spielkamp</last></author>
      <author><first>Birgit</first><last>Stark</last></author>
      <pages>10117–10123</pages>
      <abstract>Modern large language models and chatbots based on them show impressive results in text generation and dialog tasks. At the same time, these models are subject to criticism in many aspects, e.g., they can generate hate speech and untrue and biased content. In this work, we show another problematic feature of such chatbots: they are echo chambers in the sense that they tend to agree with the opinions of their users. Social media, such as Facebook, was criticized for a similar problem and called an echo chamber. We experimentally test five LLM-based chatbots, which we feed with opinionated inputs. We annotate the chatbot answers whether they agree or disagree with the input. All chatbots tend to agree. However, the echo chamber effect is not equally strong. We discuss the differences between the chatbots and make the dataset publicly available.</abstract>
      <url hash="c7eeac71">2024.lrec-main.884</url>
      <bibkey>nehring-etal-2024-large-language</bibkey>
    </paper>
    <paper id="885">
      <title>Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency</title>
      <author><first>Toyin D.</first><last>Aguda</last></author>
      <author><first>Suchetha</first><last>Siddagangappa</last></author>
      <author><first>Elena</first><last>Kochkina</last></author>
      <author><first>Simerjot</first><last>Kaur</last></author>
      <author><first>Dongsheng</first><last>Wang</last></author>
      <author><first>Charese</first><last>Smiley</last></author>
      <pages>10124–10145</pages>
      <abstract>Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains under-explored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.</abstract>
      <url hash="4171e193">2024.lrec-main.885</url>
      <bibkey>aguda-etal-2024-large-language</bibkey>
    </paper>
    <paper id="886">
      <title>Large Language Models for Generative Recommendation: A Survey and Visionary Discussions</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yongfeng</first><last>Zhang</last></author>
      <author><first>Dugang</first><last>Liu</last></author>
      <author><first>Li</first><last>Chen</last></author>
      <pages>10146–10159</pages>
      <abstract>Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that this survey can provide the context and guidance needed to explore this interesting and emerging topic.</abstract>
      <url hash="c1f42562">2024.lrec-main.886</url>
      <bibkey>li-etal-2024-large-language</bibkey>
    </paper>
    <paper id="887">
      <title>Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling</title>
      <author><first>Yida</first><last>Mu</last></author>
      <author><first>Chun</first><last>Dong</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <pages>10160–10171</pages>
      <abstract>Topic modelling, as a well-established unsupervised technique, has found extensive use in automatically detecting significant topics within a corpus of documents. However, classic topic modelling approaches (e.g., LDA) have certain drawbacks, such as the lack of semantic understanding and the presence of overlapping topics. In this work, we investigate the untapped potential of large language models (LLMs) as an alternative for uncovering the underlying topics within extensive text corpora. To this end, we introduce a framework that prompts LLMs to generate topics from a given set of documents and establish evaluation protocols to assess the clustering efficacy of LLMs. Our findings indicate that LLMs with appropriate prompts can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics. Through in-depth experiments and evaluation, we summarise the advantages and constraints of employing LLMs in topic extraction.</abstract>
      <url hash="8efe21ca">2024.lrec-main.887</url>
      <bibkey>mu-etal-2024-large-language</bibkey>
    </paper>
    <paper id="888">
      <title>Latent vs Explicit Knowledge Representation: How <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Answers Questions about Low-Frequency Entities</title>
      <author><first>Arianna</first><last>Graciotti</last></author>
      <author><first>Valentina</first><last>Presutti</last></author>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <pages>10172–10185</pages>
      <abstract>In this paper, we present an evaluation of two different approaches to the free-form Question Answering (QA) task. The main difference between the two approaches is that one is based on latent representations of knowledge, and the other uses explicit knowledge representation. For the evaluation, we developed DynaKnowledge, a new benchmark composed of questions concerning Wikipedia low-frequency entities. We wanted to ensure, on the one hand, that the questions are answerable and, on the other, that the models can provide information about very specific facts. The evaluation that we conducted highlights that the proposed benchmark is particularly challenging. The best model answers correctly only on 50% of the questions. Analysing the results, we also found that ChatGPT shows low reliance on low-frequency entity questions, manifesting a popularity bias. On the other hand, a simpler model based on explicit knowledge is less affected by this bias. With this paper, we want to provide a living benchmark for open-form QA to test knowledge and latent representation models on a dynamic benchmark.</abstract>
      <url hash="f5f717df">2024.lrec-main.888</url>
      <attachment type="OptionalSupplementaryMaterial" hash="00723f7e">2024.lrec-main.888.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>graciotti-etal-2024-latent-vs</bibkey>
    </paper>
    <paper id="889">
      <title><fixed-case>L</fixed-case>at<fixed-case>E</fixed-case>val: An Interactive <fixed-case>LLM</fixed-case>s Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles</title>
      <author><first>Shulin</first><last>Huang</last></author>
      <author><first>Shirong</first><last>Ma</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Mengzuo</first><last>Huang</last></author>
      <author><first>Wuhe</first><last>Zou</last></author>
      <author><first>Weidong</first><last>Zhang</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <pages>10186–10197</pages>
      <abstract>With the evolution of LLMs, they are endowed with impressive logical reasoning, or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model’s lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: (1) posing high-quality questions that break out of conventional norms but are beneficial for puzzle-solving. (2) integrating existing information to gradually deduce the truth through reasoning. We observe that it is hard for most LLMs to accomplish lateral thinking during interactions. Even the most powerful LLM, GPT-4, faces challenges in achieving satisfactory performance, and for most open-source models, simply completing this task is quite difficult. This evaluation benchmark provides LLMs with a highly challenging and differentiating task that is crucial to an effective AI assistant. Our dataset and source codes are available at https://github.com/THUKElab/LatEval.</abstract>
      <url hash="1acafcd0">2024.lrec-main.889</url>
      <bibkey>huang-etal-2024-lateval-interactive</bibkey>
    </paper>
    <paper id="890">
      <title><fixed-case>LA</fixed-case>-<fixed-case>UCL</fixed-case>: <fixed-case>LLM</fixed-case>-Augmented Unsupervised Contrastive Learning Framework for Few-Shot Text Classification</title>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Gao</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Boda</first><last>Feng</last></author>
      <author><first>Wenmin</first><last>Deng</last></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <pages>10198–10207</pages>
      <abstract>The few-shot tasks require the model to have the ability to generalize from a few samples. However, due to the lack of cognitive ability, the current works cannot fully utilize limited samples to expand the sample space and still suffer from overfitting issues. To address the problems, we propose a LLM-Augmented Unsupervised Contrastive Learning Framework (LA-UCL), which introduces a cognition-enabled Large Language Model (LLM) for efficient data augmentation, and presents corresponding contrastive learning strategies. Specifically, in the self-augmented contrastive learning module, we construct a retrieval-based in-context prompt scheme by retrieving similar but different category data from the original samples, guiding the LLM to generate more discriminative augmented data. Then, by designing group-level contrastive loss to enhance the model’s discriminative ability. In the external-augmented contrastive learning module, we utilize web knowledge retrieval to expand the sample space and leverage LLM to generate more diverse data, and introduce sample-level contrastive loss for unlabeled data to improve the model’s generalization. Experimental results on six datasets show that our model exceeds the baseline models.</abstract>
      <url hash="400c4e11">2024.lrec-main.890</url>
      <bibkey>zhang-etal-2024-la-ucl</bibkey>
    </paper>
    <paper id="891">
      <title>Layer-wise Regularized Dropout for Neural Language Models</title>
      <author><first>Shiwen</first><last>Ni</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Chengming</first><last>Li</last></author>
      <author><first>Xiping</first><last>Xiping Hu</last></author>
      <pages>10208–10218</pages>
      <abstract>Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a “self-distillation” framework, in which each sub-model generated by dropout is the other’s “teacher” model and “student” model. Through extensive experiments on 8 natural language understanding datasets, 6 neural machine translation datasets, and 1 abstractive summarization dataset (a total of 15 datasets), we show that LR-Drop achieves superior performances, including state-of-the-art results.</abstract>
      <url hash="14ac39b5">2024.lrec-main.891</url>
      <bibkey>ni-etal-2024-layer-wise</bibkey>
    </paper>
    <paper id="892">
      <title><fixed-case>L</fixed-case>ayout<fixed-case>LLM</fixed-case>: Large Language Model Instruction Tuning for Visually Rich Document Understanding</title>
      <author><first>Masato</first><last>Fujitake</last></author>
      <pages>10219–10224</pages>
      <abstract>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs’ superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</abstract>
      <url hash="230d5f13">2024.lrec-main.892</url>
      <bibkey>fujitake-2024-layoutllm-large</bibkey>
    </paper>
    <paper id="893">
      <title><fixed-case>LCG</fixed-case>bank: A Corpus of Syntactic Analyses Based on Proof Nets</title>
      <author><first>Aditya</first><last>Bhargava</last></author>
      <author><first>Timothy A. D.</first><last>Fowler</last></author>
      <author><first>Gerald</first><last>Penn</last></author>
      <pages>10225–10236</pages>
      <abstract>In syntactic parsing, *proof nets* are graphical structures that have the advantageous property of invariance to spurious ambiguities. Semantically-equivalent derivations correspond to a single proof net. Recent years have seen fresh interest in statistical syntactic parsing with proof nets, including the development of methods based on neural networks. However, training of statistical parsers requires corpora that provide ground-truth syntactic analyses. Unfortunately, there has been a paucity of corpora in formalisms for which proof nets are applicable, such as Lambek categorial grammar (LCG), a formalism related to combinatory categorial grammar (CCG). To address this, we leverage CCGbank and the relationship between LCG and CCG to develop LCGbank, an English-language corpus of syntactic analyses based on LCG proof nets. In contrast to CCGbank, LCGbank eschews type-changing and uses only categorial rules; the syntactic analyses thus provide fully compositional semantics, exploiting the transparency between syntax and semantics that so characterizes categorial grammars.</abstract>
      <url hash="eb9db4da">2024.lrec-main.893</url>
      <bibkey>bhargava-etal-2024-lcgbank-corpus</bibkey>
    </paper>
    <paper id="894">
      <title><fixed-case>L</fixed-case>ead<fixed-case>E</fixed-case>mpathy: An Expert Annotated <fixed-case>G</fixed-case>erman Dataset of Empathy in Written Leadership Communication</title>
      <author><first>Didem</first><last>Sedefoglu</last></author>
      <author><first>Allison Claire</first><last>Lahnala</last></author>
      <author><first>Jasmin</first><last>Wagner</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <author><first>Sandra</first><last>Ohly</last></author>
      <pages>10237–10248</pages>
      <abstract>Empathetic leadership communication plays a pivotal role in modern workplaces as it is associated with a wide range of positive individual and organizational outcomes. This paper introduces LeadEmpathy, an innovative expert-annotated German dataset for modeling empathy in written leadership communication. It features a novel theory-based coding scheme to model cognitive and affective empathy in asynchronous communication. The final dataset comprises 770 annotated emails from 385 participants who were allowed to rewrite their emails after receiving recommendations for increasing empathy in an online experiment. Two independent annotators achieved substantial inter-annotator agreement of &gt;= .79 for all categories, indicating that the annotation scheme can be applied to produce high-quality, multidimensional empathy ratings in current and future applications. Beyond outlining the dataset’s development procedures, we present a case study on automatic empathy detection, establishing baseline models for predicting empathy scores in a range of ten possible scores that achieve a Pearson correlation of 0.816 and a mean squared error of 0.883. Our dataset is available at https://github.com/caisa-lab/LEAD-empathy-dataset.</abstract>
      <url hash="ae77df81">2024.lrec-main.894</url>
      <bibkey>sedefoglu-etal-2024-leadempathy-expert</bibkey>
    </paper>
    <paper id="895">
      <title>Learning Bidirectional Morphological Inflection like Humans</title>
      <author><first>Akiyo</first><last>Fukatsu</last></author>
      <author><first>Yuto</first><last>Harada</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <pages>10249–10262</pages>
      <abstract>For nearly the past forty years, there has been discussion regarding whether symbolic representations are involved in morphological inflection, a debate commonly known as the Past Tense Debate. The previous literature has extensively explored whether neural models, which do not use symbolic representations can process morphological inflection like humans. However, current research interest has shifted towards whether neural models can acquire morphological inflection like humans. In this paper, we trained neural models, the recurrent neural network (RNN) with attention and the transformer, and a symbolic model, the Minimal Generalization Learner (MGL), under a human-like learning environment. Evaluating the models from the perspective of language acquisition, we found that while the transformer and the MGL exhibited some human-like characteristics, the RNN with attention did not demonstrate human-like behavior across all the evaluation metrics considered in this study. Furthermore, none of the models accurately inflected verbs in the same manner as humans in terms of morphological inflection direction. These results suggest that these models fall short as cognitive models of morphological inflection.</abstract>
      <url hash="6766bed2">2024.lrec-main.895</url>
      <bibkey>fukatsu-etal-2024-learning-bidirectional</bibkey>
    </paper>
    <paper id="896">
      <title>Learning from Wrong Predictions in Low-Resource Neural Machine Translation</title>
      <author><first>Jia Cheng</first><last>Hu</last></author>
      <author><first>Roberto</first><last>Cavicchioli</last></author>
      <author><first>Giulia</first><last>Berardinelli</last></author>
      <author><first>Alessandro</first><last>Capotondi</last></author>
      <pages>10263–10273</pages>
      <abstract>Resource scarcity in Neural Machine Translation is a challenging problem in both industry applications and in the support of less-spoken languages represented, in the worst case, by endangered and low-resource languages. Many Data Augmentation methods rely on additional linguistic sources and software tools but these are often not available in less favoured language. For this reason, we present USKI (Unaligned Sentences Keytokens pre-traIning), a pre-training strategy that leverages the relationships and similarities that exist between unaligned sentences. By doing so, we increase the dataset size of endangered and low-resource languages by the square of the initial quantity, matching the typical size of high-resource language datasets such as WMT14 En-Fr. Results showcase the effectiveness of our approach with an increase on average of 0.9 BLEU across the benchmarks using a small fraction of the entire unaligned corpus, suggesting the importance of the research topic and the potential of a currently under-utilized resource and under-explored approach.</abstract>
      <url hash="c08a57d5">2024.lrec-main.896</url>
      <bibkey>hu-etal-2024-learning-wrong</bibkey>
    </paper>
    <paper id="897">
      <title>Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis</title>
      <author><first>Zhenxiao</first><last>Cheng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Wen</first><last>Wu</last></author>
      <author><first>Qin</first><last>Chen</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>10274–10285</pages>
      <abstract>Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity. Such methods determine word-level importance using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA), our preliminary research suggests that only specific dimensions are pertinent. To address this, we propose the Information Bottleneck-based Gradient (IBG) explanation framework for ABSA. This framework leverages an information bottleneck to refine word embeddings into a concise intrinsic dimension, maintaining essential features and omitting unrelated information. Comprehensive tests show that our IBG approach considerably improves both the models’ performance and the explanations’ clarity by identifying sentiment-aware features.</abstract>
      <url hash="f23d8e8e">2024.lrec-main.897</url>
      <bibkey>cheng-etal-2024-learning-intrinsic</bibkey>
    </paper>
    <paper id="898">
      <title>Learning Strategies for Robust Argument Mining: An Analysis of Variations in Language and Domain</title>
      <author><first>Ramon</first><last>Ruiz-Dolz</last></author>
      <author><first>Chr-Jr</first><last>Chiu</last></author>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Noriko</first><last>Kando</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>10286–10292</pages>
      <abstract>Argument mining has typically been researched for specific corpora belonging to concrete languages and domains independently in each research work. Human argumentation, however, has domain- and language-dependent linguistic features that determine the content and structure of arguments. Also, when deploying argument mining systems <i>in the wild</i>, we might not be able to control some of these features. Therefore, an important aspect that has not been thoroughly investigated in the argument mining literature is the robustness of such systems to variations in language and domain. In this paper, we present a complete analysis across three different languages and three different domains that allow us to have a better understanding on how to leverage the scarce available corpora to design argument mining systems that are more robust to natural language variations.</abstract>
      <url hash="c23efd70">2024.lrec-main.898</url>
      <bibkey>ruiz-dolz-etal-2024-learning-strategies</bibkey>
    </paper>
    <paper id="899">
      <title>Lemmatisation of Medieval <fixed-case>G</fixed-case>reek: Against the Limits of Transformer’s Capabilities?</title>
      <author><first>Colin</first><last>Swaelens</last></author>
      <author><first>Pranaydeep</first><last>Singh</last></author>
      <author><first>Ilse</first><last>de Vos</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>10293–10302</pages>
      <abstract>This paper presents preliminary experiments for the lemmatisation of unedited, Byzantine Greek epigrams. This type of Greek is quite different from its classical ancestor, mostly because of its orthographic inconsistencies. Existing lemmatisation algorithms display an accuracy drop of around 30pp when tested on these Byzantine book epigrams. We conducted seven different lemmatisation experiments, which were either transformer-based or based on neural edit-trees. The best performing lemmatiser was a hybrid method combining transformer-based embeddings with a dictionary look-up. We compare our results with existing lemmatisers, and provide a detailed error analysis revealing why unedited, Byzantine Greek is so challenging for lemmatisation.</abstract>
      <url hash="3b69bc85">2024.lrec-main.899</url>
      <bibkey>swaelens-etal-2024-lemmatisation-medieval</bibkey>
    </paper>
    <paper id="900">
      <title>Leros: Learning Explicit Reasoning on Synthesized Data for Commonsense Question Answering</title>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Jiachun</first><last>Li</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Xiaojian</first><last>Jiang</last></author>
      <author><first>Jiexin</first><last>Xu</last></author>
      <author><first>Li</first><last>Qiuxia</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>10303–10315</pages>
      <abstract>Recent work shows large language models can be prompted to generate useful rationales for commonsense question answering (CQA), which can improve the performance of both themselves and other models. However, the cost of deployment and further tuning is relatively expensive for the large models. Some work explores to distill the the rationale-generation ability to convenient small-sized models, yet it typically requires human-authored QA instances during the distillation. In this paper, we propose a novel framework that leverages both knowledge graphs and large language models to synthesize rationale-augmented CQA data. Based on it, we train Leros, a model that can generate helpful rationales to assist generic QA models to accomplish unseen CQA tasks. Empirical results demonstrate Leros can substantially enhance the performance of QA models on five unseen CQA benchmarks, providing better gains than both same-sized counterpart models trained with downstream data and 10x larger language models. Our work reveals a novel way to integrate knowledge from both knowledge graphs and large language models into smaller models. The codes and synthesized resources are publicly available at https://github.com/wchrepo/leros.</abstract>
      <url hash="4720efd2">2024.lrec-main.900</url>
      <bibkey>wang-etal-2024-leros-learning</bibkey>
    </paper>
    <paper id="901">
      <title>Lessons from Deploying the First Bilingual <fixed-case>P</fixed-case>eruvian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage - <fixed-case>S</fixed-case>panish Online Dictionary</title>
      <author><first>Joe</first><last>Huamani-Malca</last></author>
      <author><first>Miguel</first><last>Rodriguez Mondoñedo</last></author>
      <author><first>Francisco</first><last>Cerna-Herrera</last></author>
      <author><first>Gissella</first><last>Bejarano</last></author>
      <author><first>Carlos</first><last>Vásquez Roque</last></author>
      <author><first>Cesar Augusto</first><last>Ramos Cantu</last></author>
      <author><first>Sabina</first><last>Oporto Pérez</last></author>
      <pages>10316–10323</pages>
      <abstract>Bilingual dictionaries present several challenges, especially for sign languages and oral languages, where multimodality plays a role. We deployed and tested the first bilingual Peruvian Sign Language (LSP) - Spanish Online Dictionary. The first feature allows the user to introduce a text and receive as a result a list of videos whose glosses are related to the input text or Spanish word. The second feature allows the user to sign in front of the camera and shows the five most probable Spanish translations based on the similarity between the input sign and gloss-labeled sign videos used to train a machine learning model. These features are constructed in a design and architecture that differentiates among the coincidence for the Spanish text searched, the sign gloss, and Spanish translation. We explain in depth how these concepts or database columns impact the search. Similarly, we share the challenges of deploying a real-world machine learning model for isolated sign language recognition through Amazon Web Services (AWS).</abstract>
      <url hash="b940e354">2024.lrec-main.901</url>
      <bibkey>huamani-malca-etal-2024-lessons-deploying</bibkey>
    </paper>
    <paper id="902">
      <title>Let’s Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models</title>
      <author><first>Shunyu</first><last>Liu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Qunxi</first><last>Zhu</last></author>
      <author><first>Qin</first><last>Chen</last></author>
      <author><first>Qingchun</first><last>Bai</last></author>
      <author><first>Jun</first><last>Xiao</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>10324–10335</pages>
      <abstract>Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects’ boundaries (start and end indices), especially for long ones, due to users’ colloquial expressions. We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages offered by DiffusionABSA when compared against robust baseline models. Our code is publicly available at https://github.com/Qlb6x/DiffusionABSA.</abstract>
      <url hash="bc41143c">2024.lrec-main.902</url>
      <attachment type="OptionalSupplementaryMaterial" hash="94b7f61d">2024.lrec-main.902.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>liu-etal-2024-lets-rectify</bibkey>
    </paper>
    <paper id="903">
      <title>Leveraging <fixed-case>AMR</fixed-case> Graph Structure for Better Sequence-to-Sequence <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Linyu</first><last>Fan</last></author>
      <author><first>Wu Wu</first><last>Yiheng</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Fang</first><last>Kong</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>10336–10346</pages>
      <abstract>Thanks to the development of pre-trained sequence-to-sequence (seq2seq) models (e.g., BART), recent studies on AMR parsing often regard this task as a seq2seq translation problem by linearizing AMR graphs into AMR token sequences in pre-processing and recovering AMR graphs from sequences in post-processing. Seq2seq AMR parsing is a relatively simple paradigm but it unavoidably loses structural information among AMR tokens. To compensate for the loss of structural information, in this paper we explicitly leverage AMR structure in the decoding phase. Given an AMR graph, we first project the structure in the graph into an AMR token graph, i.e., structure among AMR tokens in the linearized sequence. The structures for an AMR token could be divided into two parts: structure in prediction history and structure in future. Then we propose to model structure in prediction history via a graph attention network (GAT) and learn structure in future via a multi-task scheme, respectively. Experimental results show that our approach significantly outperforms a strong baseline and achieves performance with 85.5 ±0.1 and 84.2 ±0.1 Smatch scores on AMR 2.0 and AMR 3.0, respectively</abstract>
      <url hash="ddb1bbe4">2024.lrec-main.903</url>
      <bibkey>fan-etal-2024-leveraging-amr</bibkey>
    </paper>
    <paper id="904">
      <title>Leveraging Domain Corpora for Enhanced Terminology: The Case of <fixed-case>E</fixed-case>stonian-<fixed-case>E</fixed-case>nglish Remote <fixed-case>S</fixed-case>ensing Termbase</title>
      <author><first>Liisi</first><last>Jakobson</last></author>
      <author><first>Jelena</first><last>Kallas</last></author>
      <author><first>Erko</first><last>Jakobson</last></author>
      <pages>10347–10351</pages>
      <abstract>This article addresses methodological issues related to developing domain corpora and a terminological database from scratch. We present an ongoing project focused on creating an Estonian-English Remote Sensing Termbase. First, we describe the compilation process of the Estonian Remote Sensing Corpus 2022 , which served as the primary data source for the termbase. The corpus was compiled by crawling the web and adding files using the Corpus Query System Sketch Engine (Kilgarriff et al., 2004). In the next step, we employed the Term Extraction module (Kilgarriff et al., 2014; Fišer et al., 2016; Blahuš et al., 2023) to identify terms, which were subsequently registered in the Estonian Remote Sensing Termbase using the Dictionary Writing System Ekilex (Tavast et al., 2018). For each term, we provided definitions, variants, and usage contexts. In the final stage, remote sensing experts reviewed and edited the terms, their variants, and usage contexts. Finally, we provide insights and outline directions for future work in this area.</abstract>
      <url hash="bd5224c8">2024.lrec-main.904</url>
      <bibkey>jakobson-etal-2024-leveraging-domain</bibkey>
    </paper>
    <paper id="905">
      <title>Leveraging Information Redundancy of Real-World Data through Distant Supervision</title>
      <author><first>Ariel</first><last>Cohen</last></author>
      <author><first>Alexandrine</first><last>Lanson</last></author>
      <author><first>Emmanuelle</first><last>Kempf</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <pages>10352–10364</pages>
      <abstract>We explore the task of event extraction and classification by harnessing the power of distant supervision. We present a novel text labeling method that leverages the redundancy of temporal information in a data lake. This method enables the creation of a large programmatically annotated corpus, allowing the training of transformer models using distant supervision. This aims to reduce expert annotation time, a scarce and expensive resource. Our approach utilizes temporal redundancy between structured sources and text, enabling the design of a replicable framework applicable to diverse real-world databases and use cases. We employ this method to create multiple silver datasets to reconstruct key events in cancer patients’ pathways, using clinical notes from a cohort of 380,000 oncological patients. By employing various noise label management techniques, we validate our end-to-end approach and compare it with a baseline classifier built on expert-annotated data. The implications of our work extend to accelerating downstream applications, such as patient recruitment for clinical trials, treatment effectiveness studies, survival analysis, and epidemiology research. While our study showcases the potential of the method, there remain avenues for further exploration, including advanced noise management techniques, semi-supervised approaches, and a deeper understanding of biases in the generated datasets and models.</abstract>
      <url hash="edc3151f">2024.lrec-main.905</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0060ad8a">2024.lrec-main.905.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>cohen-etal-2024-leveraging-information</bibkey>
    </paper>
    <paper id="906">
      <title>Leveraging Linguistically Enhanced Embeddings for Open Information Extraction</title>
      <author><first>Fauzan Nayeem</first><last>Farooqui</last></author>
      <author><first>Thanmay</first><last>Jayakumar</last></author>
      <author><first>Pulkit</first><last>Mathur</last></author>
      <author><first>Mansi A.</first><last>Radke</last></author>
      <pages>10365–10379</pages>
      <abstract>Open Information Extraction (OIE) is a structure prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured n-ary tuples - usually subject-relation-object triples - from free text. The word embeddings in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However, past enhancement techniques cannot leverage the power of pre-trained language models (PLMs), which themselves have been hardly used for OIE. To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq PLM for OIE. We do so by introducing two methods - Weighted Addition and Linearized Concatenation. Our work gives any neural OIE architecture the key performance boost from both PLMs and linguistic features in one go. In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively over the baseline. Beyond this, we address other important challenges in the field: to reduce compute overheads with the features, we are the first ones to exploit Semantic Dependency Parse (SemDP) tags; to address flaws in current datasets, we create a clean synthetic dataset; finally, we contribute the first known study of OIE behaviour in SP models.</abstract>
      <url hash="e5d8f3a8">2024.lrec-main.906</url>
      <bibkey>farooqui-etal-2024-leveraging-linguistically</bibkey>
    </paper>
    <paper id="907">
      <title>Leveraging Pre-existing Resources for Data-Efficient Counter-Narrative Generation in <fixed-case>K</fixed-case>orean</title>
      <author><first>Seungyoon</first><last>Lee</last></author>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>DaHyun</first><last>Jung</last></author>
      <author><first>Hyeonseok</first><last>Moon</last></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Sugyeong</first><last>Eo</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>10380–10392</pages>
      <abstract>Counter-narrative generation, i.e., the generation of fact-based responses to hate speech with the aim of correcting discriminatory beliefs, has been demonstrated to be an effective method to combat hate speech. However, its effectiveness is limited by the resource-intensive nature of dataset construction processes and only focuses on the primary language. To alleviate this problem, we propose a Korean Hate Speech Counter Punch (KHSCP), a cost-effective counter-narrative generation method in the Korean language. To this end, we release the first counter-narrative generation dataset in Korean and pose two research questions. Under the questions, we propose an effective augmentation method and investigate the reasonability of a large language model to overcome data scarcity in low-resource environments by leveraging existing resources. In this regard, we conduct several experiments to verify the effectiveness of the proposed method. Our results reveal that applying pre-existing resources can improve the generation performance by a significant margin. Through deep analysis on these experiments, this work proposes the possibility of overcoming the challenges of generating counter-narratives in low-resource environments.</abstract>
      <url hash="a6833acc">2024.lrec-main.907</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4aa72662">2024.lrec-main.907.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lee-etal-2024-leveraging-pre</bibkey>
    </paper>
    <paper id="908">
      <title>Leveraging Social Context for Humor Recognition and Sense of Humor Evaluation in Social Media with a New <fixed-case>C</fixed-case>hinese Humor Corpus - <fixed-case>H</fixed-case>umor<fixed-case>WB</fixed-case></title>
      <author><first>Zeyuan</first><last>Zeng</last></author>
      <author><first>Zefeng</first><last>Li</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>10393–10402</pages>
      <abstract>With the development of the Internet, social media has produced a large amount of user-generated data, which brings new challenges for humor computing. Traditional humor computing research mainly focuses on the content, while neglecting the information of interaction relationships in social media. In addition, both content and users are important in social media, while existing humor computing research mainly focuses on content rather than people. To address these problems, we model the information transfer and entity interactions in social media as a heterogeneous graph, and create the first dataset which introduces the social context information - HumorWB, which is collected from Chinese social media - Weibo. Two humor-related tasks are designed in the dataset. One is a content-oriented humor recognition task, and the other is a novel humor evaluation task. For the above tasks, we purpose a graph-based model called SCOG, which uses heterogeneous graph neural networks to optimize node representation for downstream tasks. Experimental results demonstrate the effectiveness of feature extraction and graph representation learning methods in the model, as well as the necessity of introducing social context information.</abstract>
      <url hash="eff3111d">2024.lrec-main.908</url>
      <bibkey>zeng-etal-2024-leveraging-social</bibkey>
    </paper>
    <paper id="909">
      <title>Leveraging Syntactic Dependencies in Disambiguation: The Case of <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican <fixed-case>E</fixed-case>nglish</title>
      <author><first>Wilermine</first><last>Previlon</last></author>
      <author><first>Alice</first><last>Rozet</last></author>
      <author><first>Jotsna</first><last>Gowda</last></author>
      <author><first>Bill</first><last>Dyer</last></author>
      <author><first>Kevin</first><last>Tang</last></author>
      <author><first>Sarah</first><last>Moeller</last></author>
      <pages>10403–10415</pages>
      <abstract>African American English (AAE) has received recent attention in the field of natural language processing (NLP). Efforts to address bias against AAE in NLP systems tend to focus on lexical differences. When the unique structures of AAE are considered, the solution is often to remove or neutralize the differences. This work leverages knowledge about the unique linguistic structures to improve automatic disambiguation of habitual and non-habitual meanings of “be” in naturally produced AAE transcribed speech. Both meanings are employed in AAE but examples of Habitual be are rare in already limited AAE data. Generally, representing additional syntactic information improves semantic disambiguation of habituality. Using an ensemble of classical machine learning models with a representation of the unique POS and dependency patterns of Habitual be, we show that integrating syntactic information improves the identification of habitual uses of “be” by about 65 F1 points over a simple baseline model of n-grams, and as much as 74 points. The success of this approach demonstrates the potential impact when we embrace, rather than neutralize, the structural uniqueness of African American English.</abstract>
      <url hash="03cb5439">2024.lrec-main.909</url>
      <bibkey>previlon-etal-2024-leveraging-syntactic</bibkey>
    </paper>
    <paper id="910">
      <title>Leveraging the Interplay between Syntactic and Acoustic Cues for Optimizing <fixed-case>K</fixed-case>orean <fixed-case>TTS</fixed-case> Pause Formation</title>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Gary Geunbae</first><last>Lee</last></author>
      <pages>10416–10421</pages>
      <abstract>Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech. Nevertheless, it is important to note that these achievements have predominantly been verified within the context of high-resource languages such as English. Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness. In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns. Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate out-of-domain (OOD) sentences, despite its training on short audio clips. Architectural design choices are validated through comparisons with baseline models and ablation studies using subjective and objective metrics, thus confirming model performance.</abstract>
      <url hash="2bab5d50">2024.lrec-main.910</url>
      <bibkey>jeon-etal-2024-leveraging-interplay</bibkey>
    </paper>
    <paper id="911">
      <title><fixed-case>L</fixed-case>ex<fixed-case>A</fixed-case>b<fixed-case>S</fixed-case>umm: Aspect-based Summarization of Legal Decisions</title>
      <author><first>Santosh</first><last>T.y.s.s.</last></author>
      <author><first>Mahmoud</first><last>Aly</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>10422–10431</pages>
      <abstract>Legal professionals frequently encounter long legal judgments that hold critical insights for their work. While recent advances have led to automated summarization solutions for legal documents, they typically provide generic summaries, which may not meet the diverse information needs of users. To address this gap, we introduce LexAbSumm, a novel dataset designed for aspect-based summarization of legal case decisions, sourced from the European Court of Human Rights jurisdiction. We evaluate several abstractive summarization models tailored for longer documents on LexAbSumm, revealing a challenge in conditioning these models to produce aspect-specific summaries. We release LexAbSum to facilitate research in aspect-based summarization for legal domain.</abstract>
      <url hash="ab60d0fd">2024.lrec-main.911</url>
      <bibkey>t-y-s-s-etal-2024-lexabsumm-aspect</bibkey>
    </paper>
    <paper id="912">
      <title><fixed-case>L</fixed-case>ex<fixed-case>C</fixed-case>om<fixed-case>S</fixed-case>pa<fixed-case>L</fixed-case>2: A Lexical Complexity Corpus for <fixed-case>S</fixed-case>panish as a Foreign Language</title>
      <author><first>Jasper</first><last>Degraeuwe</last></author>
      <author><first>Patrick</first><last>Goethals</last></author>
      <pages>10432–10447</pages>
      <abstract>We present LexComSpaL2, a novel corpus which can be employed to train personalised word-level difficulty classifiers for learners of Spanish as a foreign/second language (L2). The dataset contains 2,240 in-context target words with the corresponding difficulty judgements of 26 Dutch-speaking students who are learning Spanish as an L2, resulting in a total of 58,240 annotations. The target words are divided over 200 sentences from 4 different domains (economics, health, law, and migration) and have been selected based on their suitability to be included in L2 learning materials. As our annotation scheme, we use a customised version of the 5-point lexical complexity prediction scale (Shardlow et al., 2020), tailored to the vocabulary knowledge continuum (which ranges from no knowledge over receptive mastery to productive mastery; Schmitt, 2019). With LexComSpaL2, we aim to address the lack of relevant data for multi-category difficult prediction at word level for L2 learners of other languages than English.</abstract>
      <url hash="c63c9e8e">2024.lrec-main.912</url>
      <bibkey>degraeuwe-goethals-2024-lexcomspal2-lexical</bibkey>
    </paper>
    <paper id="913">
      <title><fixed-case>L</fixed-case>ex<fixed-case>D</fixed-case>rafter: Terminology Drafting for Legislative Documents Using Retrieval Augmented Generation</title>
      <author><first>Ashish</first><last>Chouhan</last></author>
      <author><first>Michael</first><last>Gertz</last></author>
      <pages>10448–10458</pages>
      <abstract>With the increase in legislative documents at the EU, the number of new terms and their definitions is increasing as well. As per the Joint Practical Guide of the European Parliament, the Council and the Commission, terms used in legal documents shall be consistent, and identical concepts shall be expressed without departing from their meaning in ordinary, legal, or technical language. Thus, while drafting a new legislative document, having a framework that provides insights about existing definitions and helps define new terms based on a document’s context will support such harmonized legal definitions across different regulations and thus avoid ambiguities. In this paper, we present LexDrafter, a framework that assists in drafting Definitions articles for legislative documents using retrieval augmented generation (RAG) and existing term definitions present in different legislative documents. For this, definition elements are built by extracting definitions from existing documents. Using definition elements and RAG, a Definitions article can be suggested on demand for a legislative document that is being drafted. We demonstrate and evaluate the functionality of LexDrafter using a collection of EU documents from the energy domain. The code for LexDrafter framework is available at https://github.com/achouhan93/LexDrafter.</abstract>
      <url hash="b0f36014">2024.lrec-main.913</url>
      <bibkey>chouhan-gertz-2024-lexdrafter-terminology</bibkey>
    </paper>
    <paper id="914">
      <title><fixed-case>L</fixed-case>exi<fixed-case>V</fixed-case>ault: A Repository for Psycholinguistic Lexicons of Lesser-studied Languages</title>
      <author><first>Hind</first><last>Saddiki</last></author>
      <author><first>Samantha</first><last>Wray</last></author>
      <author><first>Daisy</first><last>Li</last></author>
      <pages>10459–10465</pages>
      <abstract>This paper presents LexiVault, an open-source web tool with annotated lexicons and rich retrieval capabilities primarily developed for, but not restricted to, the support of psycholinguistic research with key measures to design stimuli for low-resource languages. Psycholinguistic research relies on human responses to carefully crafted stimuli for a better understanding of the mechanisms by which we learn, store and process language. Stimuli design captures specific language properties such as frequency, morphological complexity, or stem likelihood in a part of speech, typically derived from a corpus that is representative of the average speaker’s linguistic experience. These measures are more readily available for well-resourced languages, whereas efforts for lesser-studied languages come with substantial overhead for the researcher to build corpora and calculate these measures from scratch. This stumbling block widens the gap, further skewing our modeling of the mental architecture of linguistic processing towards a small, over-represented set of the world’s languages. To lessen this burden, we designed LexiVault to be user friendly and accommodate incremental growth of new and existing low-resource language lexicons in the system through moderated community contributions while abstracting programming complexity to foster more interest from the psycholinguistics community in exploring low-resource languages.</abstract>
      <url hash="bca67d67">2024.lrec-main.914</url>
      <bibkey>saddiki-etal-2024-lexivault-repository</bibkey>
    </paper>
    <paper id="915">
      <title><fixed-case>LFED</fixed-case>: A Literary Fiction Evaluation Dataset for Large Language Models</title>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>10466–10475</pages>
      <abstract>The rapid evolution of large language models (LLMs) has ushered in the need for comprehensive assessments of their performance across various dimensions. In this paper, we propose LFED, a Literary Fiction Evaluation Dataset, which aims to evaluate the capability of LLMs on the long fiction comprehension and reasoning. We collect 95 literary fictions that are either originally written in Chinese or translated into Chinese, covering a wide range of topics across several centuries. We define a question taxonomy with 8 question categories to guide the creation of 1,304 questions. Additionally, we conduct an in-depth analysis to ascertain how specific attributes of literary fictions (e.g., novel types, character numbers, the year of publication) impact LLM performance in evaluations. Through a series of experiments involving various state-of-the-art LLMs, our findings reveal that these models face considerable challenges in effectively addressing questions related to literary fictions, with ChatGPT reaching only 57.08% under the zero-shot setting. The dataset will be publicly available at https://github.com/tjunlp-lab/LFED.git.</abstract>
      <url hash="6bf3507a">2024.lrec-main.915</url>
      <bibkey>yu-etal-2024-lfed-literary</bibkey>
    </paper>
    <paper id="916">
      <title><fixed-case>LHMKE</fixed-case>: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for <fixed-case>C</fixed-case>hinese Large Language Models</title>
      <author><first>Chuang</first><last>Liu</last></author>
      <author><first>Renren</first><last>Jin</last></author>
      <author><first>Yuqi</first><last>Ren</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>10476–10487</pages>
      <abstract>Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams. Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs under the zero-shot setting, which aligns with real examinations, and compared their performance across different subjects. We also conduct an in-depth analysis to check whether GPT-4 can automatically score subjective predictions. Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese LLMs.</abstract>
      <url hash="41033068">2024.lrec-main.916</url>
      <bibkey>liu-etal-2024-lhmke-large</bibkey>
    </paper>
    <paper id="917">
      <title><fixed-case>LI</fixed-case>4: Label-Infused Iterative Information Interacting Based Fact Verification in Question-answering Dialogue</title>
      <author><first>Xiaocheng</first><last>Zhang</last></author>
      <author><first>Chang</first><last>Wang</last></author>
      <author><first>Guoping</first><last>Zhao</last></author>
      <author><first>Xiaohong</first><last>Su</last></author>
      <pages>10488–10498</pages>
      <abstract>Fact verification constitutes a pivotal application in the effort to combat the dissemination of disinformation, a concern that has recently garnered considerable attention. However, previous studies in the field of fact verification, particularly those focused on question-answering dialogue, have exhibited limitations, such as failing to fully exploit the potential of question structures and ignoring relevant label information during the verification process. In this paper, we introduce Label-Infused Iterative Information Interacting (LI4), a novel approach designed for the task of question-answering dialogue based fact verification. LI4 consists of two meticulously designed components, namely the Iterative Information Refining and Filtering Module (IIRF) and the Fact Label Embedding Module (FLEM). The IIRF uses the Interactive Gating Mechanism to iteratively filter out the noise of question and evidence, concurrently refining the claim information. The FLEM is conceived to strengthen the understanding ability of the model towards labels by injecting label knowledge. We evaluate the performance of the proposed LI4 on HEALTHVER, FAVIQ, and COLLOQUIAL. The experimental results confirm that our LI4 model attains remarkable progress, manifesting as a new state-of-the-art performance.</abstract>
      <url hash="d8b58acb">2024.lrec-main.917</url>
      <bibkey>zhang-etal-2024-li4-label</bibkey>
    </paper>
    <paper id="918">
      <title><fixed-case>L</fixed-case>ight<fixed-case>VLP</fixed-case>: A Lightweight Vision-Language Pre-training via Gated Interactive Masked <fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>ncoders</title>
      <author><first>Xingwu</first><last>Sun</last></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Fengzong</first><last>Lian</last></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <author><first>Chengzhong</first><last>Xu</last></author>
      <pages>10499–10510</pages>
      <abstract>This paper studies vision-language (V&amp;L) pre-training for deep cross-modal representations. Recently, pre-trained V&amp;L models have shown great success in V&amp;L tasks. However, most existing models apply multi-modal encoders to encode the image and text, at the cost of high training complexity because of the input sequence length. In addition, they suffer from noisy training corpora caused by V&amp;L mismatching. In this work, we propose a lightweight vision-language pre-training (LightVLP) for efficient and effective V&amp;L pre-training. First, we design a new V&amp;L framework with two autoencoders. Each autoencoder involves an encoder, which only takes in unmasked tokens (removes masked ones), as well as a lightweight decoder that reconstructs the masked tokens. Besides, we mask and remove large portions of input tokens to accelerate the training. Moreover, we propose a gated interaction mechanism to cope with noise in aligned image-text pairs. As for a matched image-text pair, the model tends to apply cross-modal representations for reconstructions. By contrast, for an unmatched pair, the model conducts reconstructions mainly using uni-modal representations. Benefiting from the above-mentioned designs, our base model shows competitive results compared to ALBEF while saving 44% FLOPs. Further, we compare our large model with ALBEF under the setting of similar FLOPs on six datasets and show the superiority of LightVLP. In particular, our model achieves 2.2% R@1 gains on COCO Text Retrieval and 1.1% on refCOCO+.</abstract>
      <url hash="4ace451a">2024.lrec-main.918</url>
      <bibkey>sun-etal-2024-lightvlp-lightweight</bibkey>
    </paper>
    <paper id="919">
      <title>Limitations of Human Identification of Automatically Generated Text</title>
      <author><first>Nadège</first><last>Alavoine</last></author>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last></author>
      <author><first>Romane</first><last>Gallienne</last></author>
      <author><first>Carlos-Emiliano</first><last>González-Gallardo</last></author>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <author><first>Jose G.</first><last>Moreno</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Vincent</first><last>Segonne</last></author>
      <author><first>Johanna</first><last>Simoens</last></author>
      <pages>10511–10516</pages>
      <abstract>Neural text generation is receiving broad attention with the publication of new tools such as ChatGPT. The main reason for that is that the achieved quality of the generated text may be attributed to a human writer by the naked eye of a human evaluator. In this paper, we propose a new corpus in French and English for the task of recognising automatically generated texts and we conduct a study of how humans perceive the text. Our results show, as previous work before the ChatGPT era, that the generated texts by tools such as ChatGPT share some common characteristics but they are not clearly identifiable which generates different perceptions of these texts.</abstract>
      <url hash="7676db4a">2024.lrec-main.919</url>
      <bibkey>alavoine-etal-2024-limitations-human</bibkey>
    </paper>
    <paper id="920">
      <title>Linear Cross-document Event Coreference Resolution with <fixed-case>X</fixed-case>-<fixed-case>AMR</fixed-case></title>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last></author>
      <author><first>George Arthur</first><last>Baker</last></author>
      <author><first>Evi</first><last>Judge</last></author>
      <author><first>Michael</first><last>Reagan</last></author>
      <author><first>Kristin</first><last>Wright-Bettner</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>James H.</first><last>Martin</last></author>
      <pages>10517–10529</pages>
      <abstract>Event Coreference Resolution (ECR) as a pairwise mention classification task is expensive both for automated systems and manual annotations. The task’s quadratic difficulty is exacerbated when using Large Language Models (LLMs), making prompt engineering for ECR prohibitively costly. In this work, we propose a graphical representation of events, X-AMR, anchored around individual mentions using a cross-document version of Abstract Meaning Representation. We then linearize the ECR with a novel multi-hop coreference algorithm over the event graphs. The event graphs simplify ECR, making it a) LLM cost-effective, b) compositional and interpretable, and c) easily annotated. For a fair assessment, we first enrich an existing ECR benchmark dataset with these event graphs using an annotator-friendly tool we introduce. Then, we employ GPT-4, the newest LLM by OpenAI, for these annotations. Finally, using the ECR algorithm, we assess GPT-4 against humans and analyze its limitations. Through this research, we aim to advance the state-of-the-art for efficient ECR and shed light on the potential shortcomings of current LLMs at this task. Code and annotations: <url>https://github.com/ahmeshaf/gpt_coref</url></abstract>
      <url hash="97b4e58b">2024.lrec-main.920</url>
      <bibkey>ahmed-etal-2024-linear-cross</bibkey>
    </paper>
    <paper id="921">
      <title><fixed-case>L</fixed-case>ingua<fixed-case>M</fixed-case>eta: Unified Metadata for Thousands of Languages</title>
      <author><first>Sandy</first><last>Ritchie</last></author>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Uche</first><last>Okonkwo</last></author>
      <author><first>Shikhar</first><last>Vashishth</last></author>
      <author><first>Emily</first><last>Drummond</last></author>
      <pages>10530–10538</pages>
      <abstract>We introduce LinguaMeta, a unified resource for language metadata for thousands of languages, including language codes, names, number of speakers, writing systems, countries, official status, coordinates, and language varieties. The resources are drawn from various existing repositories and supplemented with our own research. Each data point is tagged for its origin, allowing us to easily trace back to and improve existing resources with more up-to-date and complete metadata. The resource is intended for use by researchers and organizations who aim to extend technology to thousands of languages.</abstract>
      <url hash="e98de3ba">2024.lrec-main.921</url>
      <bibkey>ritchie-etal-2024-linguameta-unified</bibkey>
      <revision id="1" href="2024.lrec-main.921v1" hash="ebab2fdd"/>
      <revision id="2" href="2024.lrec-main.921v2" hash="e98de3ba" date="2024-05-23">Fix URL to point directly to LinguaMeta directory and update section on language varieties.</revision>
    </paper>
    <paper id="922">
      <title>Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)</title>
      <author><first>Alessio</first><last>Miaschi</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Giulia</first><last>Venturi</last></author>
      <pages>10539–10554</pages>
      <abstract>In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.</abstract>
      <url hash="cae51eeb">2024.lrec-main.922</url>
      <bibkey>miaschi-etal-2024-linguistic-knowledge</bibkey>
    </paper>
    <paper id="923">
      <title>Linguistic Nudges and Verbal Interaction with Robots, Smart-Speakers, and Humans</title>
      <author><first>Natalia</first><last>Kalashnikova</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Laurence</first><last>Devillers</last></author>
      <pages>10555–10564</pages>
      <abstract>This paper describes a data collection methodology and emotion annotation of dyadic interactions between a human, a Pepper robot, a Google Home smart-speaker, or another human. The collected 16 hours of audio recordings were used to analyze the propensity to change someone’s opinions about ecological behavior regarding the type of conversational agent, the kind of nudges, and the speaker’s emotional state. We describe the statistics of data collection and annotation. We also report the first results, which showed that humans change their opinions on more questions with a human than with a device, even against mainstream ideas. We observe a correlation between a certain emotional state and the interlocutor and a human’s propensity to be influenced. We also reported the results of the studies that investigated the effect of human likeness on speech using our data.</abstract>
      <url hash="108bd10a">2024.lrec-main.923</url>
      <bibkey>kalashnikova-etal-2024-linguistic-nudges</bibkey>
    </paper>
    <paper id="924">
      <title>Linguistic Rule Induction Improves Adversarial and <fixed-case>OOD</fixed-case> Robustness in Large Language Models</title>
      <author><first>Shuoran</first><last>Jiang</last></author>
      <author><first>Qingcai</first><last>Chen</last></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Youcheng</first><last>Pan</last></author>
      <author><first>Yukang</first><last>Lin</last></author>
      <pages>10565–10577</pages>
      <abstract>Ensuring robustness is especially important when AI is deployed in responsible or safety-critical environments. ChatGPT can perform brilliantly in both adversarial and out-of-distribution (OOD) robustness, while other popular large language models (LLMs), like LLaMA-2, ERNIE and ChatGLM, do not perform satisfactorily in this regard. Therefore, it is valuable to study what efforts play essential roles in ChatGPT, and how to transfer these efforts to other LLMs. This paper experimentally finds that linguistic rule induction is the foundation for identifying the cause-effect relationships in LLMs. For LLMs, accurately processing the cause-effect relationships improves its adversarial and OOD robustness. Furthermore, we explore a low-cost way for aligning LLMs with linguistic rules. Specifically, we constructed a linguistic rule instruction dataset to fine-tune LLMs. To further energize LLMs for reasoning step-by-step with the linguistic rule, we construct the task-relevant LingR-based chain-of-thoughts. Experiments showed that LingR-induced LLaMA-13B achieves comparable or better results with GPT-3.5 and GPT-4 on various adversarial and OOD robustness evaluations.</abstract>
      <url hash="1ac9aecb">2024.lrec-main.924</url>
      <bibkey>jiang-etal-2024-linguistic-rule</bibkey>
    </paper>
    <paper id="925">
      <title>Linguistic Survey of <fixed-case>I</fixed-case>ndia and Polyglotta Africana: Two Retrostandardized Digital Editions of Large Historical Collections of Multilingual Wordlists</title>
      <author><first>Robert</first><last>Forkel</last></author>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <author><first>Christoph</first><last>Rzymski</last></author>
      <author><first>Guillaume</first><last>Segerer</last></author>
      <pages>10578–10583</pages>
      <abstract>The Linguistic Survey of India (LSI) and the Polyglotta Africana (PA) are two of the largest historical collections of multilingual wordlists. While the originally printed editions have long since been digitized and shared in various forms, no editions in which the original data is presented in standardized form, comparable with contemporary wordlist collections, have been produced so far. Here we present digital retro-standardized editions of both sources. For maximal interoperability with datasets such as Lexibank the two datasets have been converted to CLDF, the standard proposed by the Cross-Linguistic Data Formats initiative. In this way, an unambiguous identification of the three main constituents of wordlist data – language, concept and segments used for transcription – is ensured through links to the respective reference catalogs, Glottolog, Concepticon and CLTS. At this level of interoperability, legacy material such as LSI and PA may provide a reasonable complementary source for language documentation, filling in gaps where original documentation is not possible anymore.</abstract>
      <url hash="872a2c07">2024.lrec-main.925</url>
      <bibkey>forkel-etal-2024-linguistic-survey</bibkey>
    </paper>
    <paper id="926">
      <title>Linking Adaptive Structure Induction and Neuron Filtering: A Spectral Perspective for Aspect-based Sentiment Analysis</title>
      <author><first>Hao</first><last>Niu</last></author>
      <author><first>Maoyi</first><last>Wang</last></author>
      <author><first>Yun</first><last>Xiong</last></author>
      <author><first>Biao</first><last>Yang</last></author>
      <author><first>Xing</first><last>Jia</last></author>
      <author><first>Zhonglei</first><last>Guo</last></author>
      <pages>10584–10597</pages>
      <abstract>Recently, it has been discovered that incorporating structure information (e.g., dependency trees) can improve the performance of aspect-based sentiment analysis (ABSA). The structure information is often obtained from off-the-shelf parsers, which are sub-optimal and unwieldy. Therefore, adaptively inducing task-specific structures is helpful in resolving this issue. In this work, we concentrate on adaptive graph structure induction for ABSA and explore the impact of neuron-level manipulation from a spectral perspective on structure induction. Specifically, we consider word representations from PLMs (pre-trained language models) as node features and employ a graph learning module to adaptively generate adjacency matrices, followed by graph neural networks (GNNs) to capture both node features and structural information. Meanwhile, we propose the Neuron Filtering (NeuLT), a method to conduct neuron-level manipulations on word representations in the frequency domain. We conduct extensive experiments on three public datasets to observe the impact of NeuLT on structure induction and ABSA. The results and further analysis demonstrate that performing neuron-level manipulation through NeuLT can shorten Aspects-sentiment Distance of induced structures and be beneficial to improve the performance of ABSA. The effects of our method can achieve or come close to SOTA (state-of-the-art) performance.</abstract>
      <url hash="b4fd8121">2024.lrec-main.926</url>
      <bibkey>niu-etal-2024-linking-adaptive</bibkey>
    </paper>
    <paper id="927">
      <title>Linking Judgement Text to Court Hearing Videos: <fixed-case>UK</fixed-case> <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt as a Case Study</title>
      <author><first>Hadeel</first><last>Saadany</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Sophie</first><last>Walker</last></author>
      <author><first>Catherine</first><last>Breslin</last></author>
      <pages>10598–10609</pages>
      <abstract>One the most important archived legal material in the UK is the video recordings of Supreme Court hearings and their corresponding judgements. The impact of Supreme Court published material extends far beyond the parties involved in any given case as it provides landmark rulings on points of law of the greatest public and constitutional importance. Typically, transcripts of legal hearings are lengthy, making it time-consuming for legal professionals to analyse crucial arguments. This study focuses on summarising the second phase of a collaborative research-industrial project aimed at creating an automatic tool designed to connect sections of written judgements with relevant moments in Supreme Court hearing videos, streamlining access to critical information. Acting as a User-Interface (UI) platform, the tool enhances access to justice by pinpointing significant moments in the videos, aiding in comprehension of the final judgement. We make available the initial dataset of judgement-hearing pairs for legal Information Retrieval research, and elucidate our use of AI generative technology to enhance it. Additionally, we demonstrate how fine-tuning GPT text embeddings to our dataset optimises accuracy for an automated linking system tailored to the legal domain.</abstract>
      <url hash="bef83a27">2024.lrec-main.927</url>
      <bibkey>saadany-etal-2024-linking-judgement</bibkey>
    </paper>
    <paper id="928">
      <title>Linking Named Entities in Diderot’s Encyclopédie to <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Pierre</first><last>Nugues</last></author>
      <pages>10610–10615</pages>
      <abstract>Diderot’s Encyclopédie is a reference work from XVIIIth century in Europe that aimed at collecting the knowledge of its era. Wikipedia has the same ambition with a much greater scope. However, the lack of digital connection between the two encyclopedias may hinder their comparison and the study of how knowledge has evolved. A key element of Wikipedia is Wikidata that backs the articles with a graph of structured data. In this paper, we describe the annotation of more than 9,100 of the Encyclopédie entries with Wikidata identifiers enabling us to connect these entries to the graph. We considered geographic and human entities. The Encyclopédie does not contain biographic entries as they mostly appear as subentries of locations. We extracted all the geographic entries and we completely annotated all the entries containing a description of human entities. This represents more than 2,600 links referring to locations or human entities. In addition, we annotated more than 8,300 entries having a geographic content only. We describe the annotation process as well as application examples. This resource is available at https://github.com/pnugues/encyclopedie_1751.</abstract>
      <url hash="1e6c72df">2024.lrec-main.928</url>
      <bibkey>nugues-2024-linking-named</bibkey>
    </paper>
    <paper id="929">
      <title>Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models</title>
      <author><first>Evgeniia</first><last>Razumovskaia</last></author>
      <author><first>Joshua</first><last>Maynez</last></author>
      <author><first>Annie</first><last>Louis</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Shashi</first><last>Narayan</last></author>
      <pages>10616–10631</pages>
      <abstract>Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.</abstract>
      <url hash="fb700b87">2024.lrec-main.929</url>
      <bibkey>razumovskaia-etal-2024-little-red</bibkey>
    </paper>
    <paper id="930">
      <title><fixed-case>L</fixed-case>lama<fixed-case>C</fixed-case>are: An Instruction Fine-Tuned Large Language Model for Clinical <fixed-case>NLP</fixed-case></title>
      <author><first>Rumeng</first><last>Li</last></author>
      <author><first>Xun</first><last>Wang</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>10632–10641</pages>
      <abstract>Large language models (LLMs) have shown remarkable abilities in generating natural texts for various tasks across different domains. However, applying LLMs to clinical settings still poses significant challenges, as it requires specialized knowledge, vocabulary, as well as reliability. In this work, we propose a novel method of instruction fine-tuning for adapting LLMs to the clinical domain, which leverages the instruction-following capabilities of LLMs and the availability of diverse real-world data sources. We generate instructions, inputs, and outputs covering a wide spectrum of clinical services, from primary cares to nursing, radiology, physician, and social work, and use them to fine-tune LLMs. We evaluated the fine-tuned LLM, LlamaCare, on various clinical tasks, such as generating discharge summaries, predicting mortality and length of stay, and more. Using both automatic and human metrics, we demonstrated that LlamaCare surpasses other LLM baselines in predicting clinical outcomes and producing more accurate and coherent clinical texts. We also discuss the challenges and limitations of LLMs that need to be addressed before they can be widely adopted in clinical settings.</abstract>
      <url hash="d6816d5d">2024.lrec-main.930</url>
      <bibkey>li-etal-2024-llamacare-instruction</bibkey>
    </paper>
    <paper id="931">
      <title>Llama-<fixed-case>VITS</fixed-case>: Enhancing <fixed-case>TTS</fixed-case> Synthesis with Semantic Awareness</title>
      <author><first>Xincan</first><last>Feng</last></author>
      <author><first>Akifumi</first><last>Yoshimoto</last></author>
      <pages>10642–10656</pages>
      <abstract>Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited. This research introduces an innovative approach, Llama-VITS, which enhances TTS synthesis by enriching the semantic content of text using LLM. Llama-VITS integrates semantic embeddings from Llama2 with the VITS model, a leading end-to-end TTS framework. By leveraging Llama2 for the primary speech synthesis process, our experiments demonstrate that Llama-VITS matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.</abstract>
      <url hash="4d019f98">2024.lrec-main.931</url>
      <bibkey>feng-yoshimoto-2024-llama-vits</bibkey>
    </paper>
    <paper id="932">
      <title><fixed-case>LLMR</fixed-case>: Knowledge Distillation with a Large Language Model-Induced Reward</title>
      <author><first>Dongheng</first><last>Li</last></author>
      <author><first>Yongchang</first><last>Hao</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <pages>10657–10664</pages>
      <abstract>Large language models have become increasingly popular and demonstrated remarkable performance in various natural language processing (NLP) tasks. However, these models are typically computationally expensive and difficult to be deployed in resource-constrained environments. In this paper, we propose LLMR, a novel knowledge distillation (KD) method based on a reward function induced from large language models. We conducted experiments on multiple datasets in the dialogue generation and summarization tasks. Empirical results demonstrate that our LLMR approach consistently outperforms traditional KD methods in different tasks and datasets.</abstract>
      <url hash="f119f1c3">2024.lrec-main.932</url>
      <bibkey>li-etal-2024-llmr-knowledge</bibkey>
    </paper>
    <paper id="933">
      <title><fixed-case>LLMS</fixed-case>egm: Surface-level Morphological Segmentation Using Large Language Model</title>
      <author><first>Marko</first><last>Pranjić</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>10665–10674</pages>
      <abstract>Morphological word segmentation splits a given word into its morphemes (roots and affixes), the smallest meaning-bearing units of language. We introduce a novel approach, called LLMSegm, to surface-level morphological segmentation leveraging large language models (LLMs). The proposed approach is applicable in low-data settings as well as for low-resourced languages. We show how to transform the surface-level morphological segmentation task to a binary classification problem and train LLMs to solve it efficiently. For input, we leverage the information from the default LLM subword tokenisation, and a custom morphological segmentation using novel encoding. The evaluation of LLMSegm across seven morphologically diverse languages demonstrates substantial gains in minimally-supervised settings as well as for low-resourced languages, compared to several existing competitive approaches. In terms of F1-scores and accuracy, we achieve improved results compared to the competing methods in six out of seven datasets. Keywords: morphological segmentation, surface-level segmentation, large language models, low-resource settings</abstract>
      <url hash="01a8c6b7">2024.lrec-main.933</url>
      <bibkey>pranjic-etal-2024-llmsegm-surface</bibkey>
    </paper>
    <paper id="934">
      <title><fixed-case>LM</fixed-case>-Combiner: A Contextual Rewriting Model for <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Yixuan</first><last>Wang</last></author>
      <author><first>Baoxin</first><last>Wang</last></author>
      <author><first>Yijun</first><last>Liu</last></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>10675–10685</pages>
      <abstract>Over-correction is a critical problem in Chinese grammatical error correction (CGEC) task. Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the GEC system. However, these methods still require the output of several GEC systems and inevitably lead to reduced error recall. In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of GEC system outputs without a model ensemble. Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text. In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner. Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged. Besides, we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT).</abstract>
      <url hash="c26b9c7f">2024.lrec-main.934</url>
      <bibkey>wang-etal-2024-lm-combiner</bibkey>
    </paper>
    <paper id="935">
      <title>Locally Differentially Private In-Context Learning</title>
      <author><first>Chunyan</first><last>Zheng</last></author>
      <author><first>Keke</first><last>Sun</last></author>
      <author><first>Wenhao</first><last>Zhao</last></author>
      <author><first>Haibo</first><last>Zhou</last></author>
      <author><first>Lixing</first><last>Jiang</last></author>
      <author><first>Shaoyang</first><last>Song</last></author>
      <author><first>Chunlai</first><last>Zhou</last></author>
      <pages>10686–10697</pages>
      <abstract>Large pretrained language models (LLMs) have shown surprising In-Context Learning (ICL) ability. An important application in deploying large language models is to augment LLMs with a private database for some specific task.The main problem with this promising commercial use is that LLMs have been shown to memorize their training data and their prompt data are vulnerable to membership inference attacks (MIA) and prompt leaking attacks. In order to deal with this problem, we treat LLMs as untrusted in privacy and propose a locally differentially private framework of in-context learning (LDP-ICL) in the settings where labels are sensitive. Considering the mechanisms of in-context learning in Transformers by gradient descent, we provide an analysis of the trade-off between privacy and utility in such LDP-ICL for classification. Moreover, we apply LDP-ICL to the discrete distribution estimation problem. In the end, we perform several experiments to demonstrate our analysis results</abstract>
      <url hash="65d2e07e">2024.lrec-main.935</url>
      <bibkey>zheng-etal-2024-locally-differentially</bibkey>
    </paper>
    <paper id="936">
      <title><fixed-case>L</fixed-case>ocal<fixed-case>T</fixed-case>weets to <fixed-case>L</fixed-case>ocal<fixed-case>H</fixed-case>ealth: A Mental Health Surveillance Framework Based on <fixed-case>T</fixed-case>witter Data</title>
      <author><first>Vijeta</first><last>Deshpande</last></author>
      <author><first>Minhwa</first><last>Lee</last></author>
      <author><first>Zonghai</first><last>Yao</last></author>
      <author><first>Zihao</first><last>Zhang</last></author>
      <author><first>Jason Brian</first><last>Gibbons</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>10698–10715</pages>
      <abstract>Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78%, respectively, a 59% improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize LocalHealth to extrapolate CDC’s estimates to proxy unreported neighborhoods, achieving an F1-score of 0.7291. Our work suggests that Twitter data can be effectively leveraged to simulate neighborhood-level MH outcomes.</abstract>
      <url hash="321f962a">2024.lrec-main.936</url>
      <bibkey>deshpande-etal-2024-localtweets-localhealth</bibkey>
    </paper>
    <paper id="937">
      <title>Loflòc: A Morphological Lexicon for <fixed-case>O</fixed-case>ccitan using <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Marianne</first><last>Vergez-Couret</last></author>
      <author><first>Myriam</first><last>Bras</last></author>
      <author><first>Aleksandra</first><last>Miletić</last></author>
      <author><first>Clamença</first><last>Poujade</last></author>
      <pages>10716–10724</pages>
      <abstract>This paper presents Loflòc (Lexic obèrt flechit Occitan – Open Inflected Lexicon of Occitan), a morphological lexicon for Occitan. Even though the lexicon no longer occupies the same place in the NLP pipeline since the advent of large language models, it remains a crucial resource for low-resourced languages. Occitan is a Romance language spoken in the south of France and in parts of Italy and Spain. It is not recognized as an official language in France and no standard variety is shared across the area. To the best of our knowledge, Loflòc is the first publicly available lexicon for Occitan. It contains 650 thousand entries for 57 thousand lemmas. Each entry is accompanied by the corresponding Universal Dependencies Part-of-Speech tag. We show that the lexicon has solid coverage on the existing freely available corpora of Occitan in four major dialects. Coverage gaps on multi-dialect corpora are overwhelmingly driven by dialectal variation, which affects both open and closed classes. Based on this analysis we propose directions for future improvements.</abstract>
      <url hash="b05ce00f">2024.lrec-main.937</url>
      <bibkey>vergez-couret-etal-2024-lofloc-morphological</bibkey>
    </paper>
    <paper id="938">
      <title>Logging Keystrokes in Writing by <fixed-case>E</fixed-case>nglish Learners</title>
      <author><first>Georgios</first><last>Velentzas</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Rita</first><last>Borgo</last></author>
      <author><first>Erin</first><last>Pacquetet</last></author>
      <author><first>Clive</first><last>Hamilton</last></author>
      <author><first>Taylor</first><last>Arnold</last></author>
      <author><first>Diane</first><last>Nicholls</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <author><first>Thomas</first><last>Gaillat</last></author>
      <author><first>Nicolas</first><last>Ballier</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last></author>
      <pages>10725–10746</pages>
      <abstract>Essay writing is a skill commonly taught and practised in schools. The ability to write a fluent and persuasive essay is often a major component of formal assessment. In natural language processing and education technology we may work with essays in their final form, for example to carry out automated assessment or grammatical error correction. In this work we collect and analyse data representing the essay writing process from start to finish, by recording every key stroke from multiple writers participating in our study. We describe our data collection methodology, the characteristics of the resulting dataset, and the assignment of proficiency levels to the texts. We discuss the ways the keystroke data can be used – for instance seeking to identify patterns in the keystrokes which might act as features in automated assessment or may enable further advancements in writing assistance – and the writing support technology which could be built with such information, if we can detect when writers are struggling to compose a section of their essay and offer appropriate intervention. We frame this work in the context of English language learning, but we note that keystroke logging is relevant more broadly to text authoring scenarios as well as cognitive or linguistic analyses of the writing process.</abstract>
      <url hash="eeba6286">2024.lrec-main.938</url>
      <bibkey>velentzas-etal-2024-logging-keystrokes</bibkey>
    </paper>
    <paper id="939">
      <title>Logic Rules as Explanations for Legal Case Retrieval</title>
      <author><first>ZhongXiang</first><last>Sun</last></author>
      <author><first>Kepu</first><last>Zhang</last></author>
      <author><first>Weijie</first><last>Yu</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Jun</first><last>Xu</last></author>
      <pages>10747–10759</pages>
      <abstract>In this paper, we address the issue of using logic rules to explain the results from legal case retrieval. The task is critical to legal case retrieval because the users (e.g., lawyers or judges) are highly specialized and require the system to provide logic, faithful, and interpretable explanations before making legal decisions. Recently, research efforts have been made to learn explainable legal case retrieval models. However, these methods usually select rationales (key sentences) from the legal cases as explanations, failing to provide faithful and logicly correct explanations. In this paper, we propose Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that explicitly conducts reasoning on the matching of legal cases through learning case-level and law-level logic rules. The learned rules are then integrated into the retrieval process in a neuro-symbolic manner. Benefiting from the logic and interpretable nature of the logic rules, NS-LCR is equipped with built-in faithful explainability. We also show that NS-LCR is a model-agnostic framework that can be plug-in for multiple legal retrieval models. To demonstrate the superiority of NS-LCR, we extend the benchmarks of LeCaRD and ELAM with manually annotated logic rules and propose a new explainability measure based on Large Language Models (LLMs). Extensive experiments show that NS-LCR can achieve state-of-the-art ranking performances, and the empirical analysis also showed that NS-LCR is capable of providing faithful explanations for legal case retrieval.</abstract>
      <url hash="0d259ea1">2024.lrec-main.939</url>
      <bibkey>sun-etal-2024-logic-rules</bibkey>
    </paper>
    <paper id="940">
      <title><fixed-case>L</fixed-case>o<fixed-case>NAS</fixed-case>: Elastic Low-Rank Adapters for Efficient Large Language Models</title>
      <author><first>Juan Pablo</first><last>Munoz</last></author>
      <author><first>Jinjie</first><last>Yuan</last></author>
      <author><first>Yi</first><last>Zheng</last></author>
      <author><first>Nilesh</first><last>Jain</last></author>
      <pages>10760–10776</pages>
      <abstract>Large Language Models (LLMs) continue to grow, reaching hundreds of billions of parameters and making it challenging for Deep Learning practitioners with resource-constrained systems to use them, e.g., fine-tuning these models for a downstream task of their interest. Adapters, such as low-rank adapters (LoRA), have been proposed to reduce the number of trainable parameters in a model, reducing memory requirements and enabling smaller systems to fine-tune these models. Orthogonal to this work, Neural Architecture Search (NAS) has been used to discover compressed and more efficient architectures without sacrificing performance compared to similar base models. This paper introduces a novel approach, LoNAS, to use NAS on language models by exploring a search space of elastic low-rank adapters while reducing memory and compute requirements of full-scale NAS, resulting in high-performing compressed models obtained from weight-sharing super-networks. Compared to models fine-tuned with LoRA, these models contain fewer total parameters, reducing the inference time with only minor decreases in accuracy and, in some cases, even improving accuracy. We discuss the limitations of LoNAS and share observations for the research community regarding its generalization capabilities, which have motivated our follow-up work.</abstract>
      <url hash="d41b687c">2024.lrec-main.940</url>
      <bibkey>munoz-etal-2024-lonas-elastic</bibkey>
    </paper>
    <paper id="941">
      <title><fixed-case>L</fixed-case>ong<fixed-case>D</fixed-case>oc<fixed-case>FACTS</fixed-case>core: Evaluating the Factuality of Long Document Abstractive Summarisation</title>
      <author><first>Jennifer A.</first><last>Bishop</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <pages>10777–10789</pages>
      <abstract>Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research and resources available for evaluating whether existing automatic evaluation metrics are fit for purpose when applied in long document settings. In this work, we evaluate the efficacy of automatic metrics for assessing the factual consistency of long document text summarisation. We create a human-annotated data set for evaluating automatic factuality metrics, LongSciVerify, which contains fine-grained factual consistency annotations for long document summaries from the scientific domain. We also propose a new evaluation framework, LongDocFACTScore, which is suitable for evaluating long document summarisation. This framework allows metrics to be efficiently extended to any length document and outperforms existing state-of-the-art metrics in its ability to correlate with human measures of factuality when used to evaluate long document summarisation data sets. We make our code and LongSciVerify data set publicly available: https://github.com/jbshp/LongDocFACTScore.</abstract>
      <url hash="ff9df430">2024.lrec-main.941</url>
      <bibkey>bishop-etal-2024-longdocfactscore-evaluating</bibkey>
    </paper>
    <paper id="942">
      <title>Longform Multimodal Lay Summarization of Scientific Papers: Towards Automatically Generating Science Blogs from Research Articles</title>
      <author><first>Sandeep</first><last>Kumar</last></author>
      <author><first>Guneet Singh</first><last>Kohli</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>10790–10801</pages>
      <abstract>Science communication, in layperson’s terms, is essential to reach the general population and also maximize the impact of underlying scientific research. Hence, good science blogs and journalistic reviews of research articles are so well-read and critical to conveying science. Scientific blogging goes beyond traditional research summaries, offering experts a platform to articulate findings in layperson’s terms. It bridges the gap between intricate research and its comprehension by the general public, policymakers, and other researchers. Amid the rapid expansion of scientific data and the accelerating pace of research, credible science blogs serve as vital artifacts for evidence-based information to the general non-expert audience. However, writing a scientific blog or even a short lay summary requires significant time and effort. Here, we are intrigued <i>what if the process of writing a scientific blog based on a given paper could be semi-automated to produce the first draft?</i> In this paper, we introduce a novel task of Artificial Intelligence (AI)-based science blog generation from a research article. We leverage the idea that presentations and science blogs share a symbiotic relationship in their aim to clarify and elucidate complex scientific concepts. Both rely on visuals, such as figures, to aid comprehension. With this motivation, we <i>create a new dataset of science blogs</i> using the presentation transcript and the corresponding slides. We create a dataset containing a paper’s presentation transcript and figures annotated from nearly 3000 papers. We then propose a multimodal attention model to generate a blog text and select the most relevant figures to explain a research article in layperson’s terms, essentially a science blog. Our experimental results with respect to both automatic and human evaluation metrics show the effectiveness of our proposed approach and the usefulness of our proposed dataset.</abstract>
      <url hash="8c434fad">2024.lrec-main.942</url>
      <bibkey>kumar-etal-2024-longform-multimodal</bibkey>
    </paper>
    <paper id="943">
      <title>Look before You Leap: Dual Logical Verification for Knowledge-based Visual Question Generation</title>
      <author><first>Xumeng</first><last>Liu</last></author>
      <author><first>Wenya</first><last>Guo</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Xubo</first><last>Liu</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Shenglong</first><last>Yu</last></author>
      <author><first>Xiaojie</first><last>Yuan</last></author>
      <pages>10802–10812</pages>
      <abstract>Knowledge-based Visual Question Generation aims to generate visual questions with outside knowledge other than the image. Existing approaches are answer-aware, which incorporate answers into the question-generation process. However, these methods just focus on leveraging the semantics of inputs to propose questions, ignoring the logical coherence among generated questions (Q), images (V), answers (A), and corresponding acquired outside knowledge (K). It results in generating many non-expected questions with low quality, lacking insight and diversity, and some of them are even without any corresponding answer. To address this issue, we inject logical verification into the processes of knowledge acquisition and question generation, which is defined as LVˆ2-Net. Through checking the logical structure among V, A, K, ground-truth and generated Q twice in the whole KB-VQG procedure, LVˆ2-Net can propose diverse and insightful knowledge-based visual questions. And experimental results on two commonly used datasets demonstrate the superiority of LVˆ2-Net. Our code will be released to the public soon.</abstract>
      <url hash="6370354c">2024.lrec-main.943</url>
      <bibkey>liu-etal-2024-look-leap</bibkey>
    </paper>
    <paper id="944">
      <title><fixed-case>L</fixed-case>o<fixed-case>SST</fixed-case>-<fixed-case>AD</fixed-case>: A Longitudinal Corpus for Tracking <fixed-case>A</fixed-case>lzheimer’s Disease Related Changes in Spontaneous Speech</title>
      <author><first>Ulla</first><last>Petti</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>10813–10821</pages>
      <abstract>Language-based biomarkers have shown promising results in differentiating those with Alzheimer’s disease (AD) diagnosis from healthy individuals, but the earliest changes in language are thought to start years or even decades before the diagnosis. Detecting these changes is critical to allow early interventions, but research into the earliest signs is challenging, as it requires large longitudinal datasets that are time-consuming and expensive to collect. There is a need for alternative methods for tracking longitudinal language change, including Natural Language Processing (NLP) and speech recognition technologies. We present a novel corpus that can enable this: a corpus of transcripts of public interviews with 20 famous figures, half of whom will eventually be diagnosed with AD, recorded over several decades. We evaluate the corpus by validating patterns of vocabulary richness changes known from literature, such as decline in noun frequency, word length, and several other features. We show that public data could be used to collect longitudinal datasets without causing extra stress for the participant, and that these data can adequately reflect longitudinal AD-related changes in vocabulary richness. Our corpus can provide a valuable starting point for the development of early detection tools and enhance our understanding of how AD affects language over time.</abstract>
      <url hash="dc3c5722">2024.lrec-main.944</url>
      <bibkey>petti-korhonen-2024-losst-ad</bibkey>
    </paper>
    <paper id="945">
      <title>Low-Rank Prune-And-Factorize for Language Model Compression</title>
      <author><first>Siyu</first><last>Ren</last></author>
      <author><first>Kenny Q.</first><last>Zhu</last></author>
      <pages>10822–10832</pages>
      <abstract>The components underpinning PLMs—large weight matrices—were shown to bear considerable redundancy. Matrix factorization, a well-established technique from matrix theory, has been utilized to reduce the number of parameters in PLM. However, it fails to retain satisfactory performance under moderate to high compression rates. In this paper, we identify the full-rankness of fine-tuned PLM as the fundamental bottleneck for the failure of matrix factorization and explore the use of network pruning to extract low-rank sparsity pattern desirable to matrix factorization. We find such a low-rank sparsity pattern exclusively exists in models generated by first-order pruning, which motivates us to unite the two approaches and achieve more effective model compression. We further propose two techniques: sparsity-aware SVD and mixed-rank fine-tuning, which improve the initialization and training of the compression procedure, respectively. Experiments on GLUE and question-answering tasks show that the proposed method has a superior compression-performance trade-off compared to existing approaches.</abstract>
      <url hash="ce851bd4">2024.lrec-main.945</url>
      <bibkey>ren-zhu-2024-low-rank</bibkey>
    </paper>
    <paper id="946">
      <title><fixed-case>M</fixed-case>2<fixed-case>SA</fixed-case>: Multimodal and Multilingual Model for Sentiment Analysis of Tweets</title>
      <author><first>Gaurish</first><last>Thakkar</last></author>
      <author><first>Sherzod</first><last>Hakimov</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <pages>10833–10845</pages>
      <abstract>In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.</abstract>
      <url hash="68d209ef">2024.lrec-main.946</url>
      <bibkey>thakkar-etal-2024-m2sa-multimodal</bibkey>
    </paper>
    <paper id="947">
      <title><fixed-case>M</fixed-case>3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval</title>
      <author><first>Yang</first><last>Bai</last></author>
      <author><first>Anthony</first><last>Colas</last></author>
      <author><first>Christan</first><last>Grant</last></author>
      <author><first>Zhe</first><last>Wang</last></author>
      <pages>10846–10857</pages>
      <abstract>In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval. However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER.</abstract>
      <url hash="800d869b">2024.lrec-main.947</url>
      <bibkey>bai-etal-2024-m3-multi</bibkey>
    </paper>
    <paper id="948">
      <title>m3<fixed-case>P</fixed-case>: Towards Multimodal Multilingual Translation with Multimodal Prompt</title>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Yuwei</first><last>Yin</last></author>
      <author><first>Jiaqi</first><last>Bai</last></author>
      <author><first>Bing</first><last>Wang</last></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Xinnian</first><last>Liang</last></author>
      <author><first>LinZheng</first><last>Chai</last></author>
      <author><first>Liqun</first><last>Yang</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>10858–10871</pages>
      <abstract>Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual Neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results show that m3P outperforms previous text-only baselines and multilingual multimodal methods by a large margin. Furthermore, the probing experiments validate the effectiveness of our method in enhancing translation under the low-resource and massively multilingual scenario.</abstract>
      <url hash="9bd7448f">2024.lrec-main.948</url>
      <bibkey>yang-etal-2024-m3p-towards</bibkey>
    </paper>
    <paper id="949">
      <title><fixed-case>M</fixed-case>3<fixed-case>TCM</fixed-case>: Multi-modal Multi-task Context Model for Utterance Classification in Motivational Interviews</title>
      <author><first>Sayed Muddashir</first><last>Hossain</last></author>
      <author><first>Jan</first><last>Alexandersson</last></author>
      <author><first>Philipp</first><last>Müller</last></author>
      <pages>10872–10879</pages>
      <abstract>Accurate utterance classification in motivational interviews is crucial to automatically understand the quality and dynamics of client-therapist interaction, and it can serve as a key input for systems mediating such interactions. Motivational interviews exhibit three important characteristics. First, there are two distinct roles, namely client and therapist. Second, they are often highly emotionally charged, which can be expressed both in text and in prosody. Finally, context is of central importance to classify any given utterance. Previous works did not adequately incorporate all of these characteristics into utterance classification approaches for mental health dialogues. In contrast, we present M3TCM, a Multi-modal, Multi-task Context Model for utterance classification. Our approach for the first time employs multi-task learning to effectively model both joint and individual components of therapist and client behaviour. Furthermore, M3TCM integrates information from the text and speech modality as well as the conversation context. With our novel approach, we outperform the state of the art for utterance classification on the recently introduced AnnoMI dataset with a relative improvement of 20% for the client- and by 15% for therapist utterance classification. In extensive ablation studies, we quantify the improvement resulting from each contribution.</abstract>
      <url hash="3f994ca5">2024.lrec-main.949</url>
      <bibkey>hossain-etal-2024-m3tcm-multi</bibkey>
    </paper>
    <paper id="950">
      <title><fixed-case>M</fixed-case>a<fixed-case>C</fixed-case>m<fixed-case>S</fixed-case>: <fixed-case>M</fixed-case>agahi Code-mixed Dataset for Sentiment Analysis</title>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Theodorus</first><last>Fransen</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Gaurav</first><last>Negi</last></author>
      <pages>10880–10890</pages>
      <abstract>The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset’s quality.</abstract>
      <url hash="34684630">2024.lrec-main.950</url>
      <bibkey>rani-etal-2024-macms-magahi</bibkey>
    </paper>
    <paper id="951">
      <title><fixed-case>MAGIC</fixed-case>: Multi-Argument Generation with Self-Refinement for Domain Generalization in Automatic Fact-Checking</title>
      <author><first>Wei-Yu</first><last>Kao</last></author>
      <author><first>An-Zi</first><last>Yen</last></author>
      <pages>10891–10902</pages>
      <abstract>Numerous studies have been conducted on automatic fact-checking, driven by its importance in real-world applications. However, two challenges persist: (1) extracting pivotal evidence from extensive documents, and (2) verifying claims across diverse domains. On one hand, current retrieval methods are limited in their ability to concisely retrieve evidence, which results in poor performance. On the other hand, retrieved evidence derived from different sources strains the generalization capabilities of classifiers. This paper explores the task of cross-domain fact-checking and presents the XClaimCheck dataset, which consists of claims from multiple domains. We propose a framework featuring a multi-argument generation technique. We leverage multi-argument generation to reconstruct concise evidence from large amounts of evidence retrieved from different sources. In addition, a self-refinement mechanism is introduced to confirm that the generated arguments are consistent with the content of the evidence. Experimental results show that our proposed framework is effective in identifying the veracity of out-of-domain claims, particularly those that are partially true or false.</abstract>
      <url hash="be3bb549">2024.lrec-main.951</url>
      <bibkey>kao-yen-2024-magic-multi</bibkey>
    </paper>
    <paper id="952">
      <title><fixed-case>MAGPIE</fixed-case>: Multi-Task Analysis of Media-Bias Generalization with Pre-Trained Identification of Expressions</title>
      <author><first>Tomáš</first><last>Horych</last></author>
      <author><first>Martin Paul</first><last>Wessel</last></author>
      <author><first>Jan Philip</first><last>Wahle</last></author>
      <author><first>Terry</first><last>Ruas</last></author>
      <author><first>Jerome</first><last>Waßmuth</last></author>
      <author><first>André</first><last>Greiner-Petter</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <author><first>Bela</first><last>Gipp</last></author>
      <author><first>Timo</first><last>Spinde</last></author>
      <pages>10903–10920</pages>
      <abstract>Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability. To address this, we introduce MAGPIE, a large-scale multi-task pre-training approach explicitly tailored for media bias detection. To enable large-scale pre-training, we construct Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score. Furthermore, using a RoBERTa encoder, we show that MAGPIE needs only 15% of fine-tuning steps compared to single-task approaches. We provide insight into task learning interference and show that sentiment analysis and emotion detection help learning of all other tasks, and scaling the number of tasks leads to the best results. MAGPIE confirms that MTL is a promising approach for addressing media bias detection, enhancing the accuracy and efficiency of existing models. Furthermore, LBM is the first available resource collection focused on media bias MTL.</abstract>
      <url hash="3b3a2810">2024.lrec-main.952</url>
      <bibkey>horych-etal-2024-magpie-multi</bibkey>
    </paper>
    <paper id="953">
      <title><fixed-case>M</fixed-case>ai<fixed-case>B</fixed-case>aam: A Multi-Dialectal <fixed-case>B</fixed-case>avarian <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Treebank</title>
      <author><first>Verena</first><last>Blaschke</last></author>
      <author><first>Barbara</first><last>Kovačić</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>10921–10938</pages>
      <abstract>Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in ‘within-language breadth’: most treebanks focus on standard languages. Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian. To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction). We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers’ orthographies. Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries. We provide baseline parsing and POS tagging results, which are lower than results obtained on German and vary substantially between different graph-based parsers. To support further research on Bavarian syntax, we make our dataset, language-specific guidelines and code publicly available.</abstract>
      <url hash="95b8e864">2024.lrec-main.953</url>
      <bibkey>blaschke-etal-2024-maibaam-multi</bibkey>
    </paper>
    <paper id="954">
      <title><fixed-case>M</fixed-case>aint<fixed-case>IE</fixed-case>: A Fine-Grained Annotation Schema and Benchmark for Information Extraction from Maintenance Short Texts</title>
      <author><first>Tyler K.</first><last>Bikaun</last></author>
      <author><first>Tim</first><last>French</last></author>
      <author><first>Michael</first><last>Stewart</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Melinda</first><last>Hodkiewicz</last></author>
      <pages>10939–10951</pages>
      <abstract>Maintenance short texts (MST), derived from maintenance work order records, encapsulate crucial information in a concise yet information-rich format. These user-generated technical texts provide critical insights into the state and maintenance activities of machines, infrastructure, and other engineered assets–pillars of the modern economy. Despite their importance for asset management decision-making, extracting and leveraging this information at scale remains a significant challenge. This paper presents MaintIE, a multi-level fine-grained annotation scheme for entity recognition and relation extraction, consisting of 5 top-level classes: PhysicalObject, State, Process, Activity and Property and 224 leaf entities, along with 6 relations tailored to MSTs. Using MaintIE, we have curated a multi-annotator, high-quality, fine-grained corpus of 1,076 annotated texts. Additionally, we present a coarse-grained corpus of 7,000 texts and consider its performance for bootstrapping and enhancing fine-grained information extraction. Using these corpora, we provide model performance measures for benchmarking automated entity recognition and relation extraction. The MaintIE scheme, corpus, and model are publicly available at https://github.com/nlp-tlp/maintie under the MIT license, encouraging further community exploration and innovation in extracting valuable insights from MSTs.</abstract>
      <url hash="9fbd267b">2024.lrec-main.954</url>
      <bibkey>bikaun-etal-2024-maintie-fine</bibkey>
    </paper>
    <paper id="955">
      <title>Majority Rules Guided Aspect-Category Based Sentiment Analysis via Label Prior Knowledge</title>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>Shaopeng</first><last>Tang</last></author>
      <author><first>Renwei</first><last>Wu</last></author>
      <pages>10952–10957</pages>
      <abstract>As an important fine-grained task of sentiment analysis, Aspect-Category based Sentiment Analysis (ACSA) aims to identify the sentiment polarities of pre-defined categories in text. However, due to subjectivity, the highly semantically similar text has polysemous sentiments to different people, leading to annotation difference. To this end, we propose a MAjority Rules Guided (MARG) for the profound understanding of this difference. Specifically, we firstly design a rule-based prompt generation, and then label word distribution is generated through an autoregression model for token-wise semantic consistency. Last but not least, the impact to the model caused by this commonly prevailing annotation difference can be mitigated by majority rules. 1) Our local majority rule is the ensemble of label word distributions, which alleviates the influence of the difference at the distribution generation stage. And 2) our global majority rule is the refinement based on the label prior knowledge of aspect categories, which further reduces the interference of the difference at the global data level. Conducted on four benchmark datasets, our MARG outperforms the state-of-the-art models by 2.43% to 67.68% in terms of F1-score and by 1.16% to 10.22% in terms of Accuracy.</abstract>
      <url hash="66d2c7da">2024.lrec-main.955</url>
      <bibkey>li-etal-2024-majority-rules</bibkey>
    </paper>
    <paper id="956">
      <title>Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives</title>
      <author><first>Qiushi</first><last>Sun</last></author>
      <author><first>Chengcheng</first><last>Han</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Renyu</first><last>Zhu</last></author>
      <author><first>Jingyang</first><last>Gong</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>10958–10969</pages>
      <abstract>Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initialization policy based on instruction search and auto-selected demonstration. Extensive experiments across various tasks on natural language understanding and inference demonstrate the effectiveness of our method. Our codes are available at https://github.com/QiushiSun/BBT-RGB.</abstract>
      <url hash="a91bd850">2024.lrec-main.956</url>
      <bibkey>sun-etal-2024-make-prompt</bibkey>
    </paper>
    <paper id="957">
      <title>Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors</title>
      <author><first>Shengkun</first><last>Ma</last></author>
      <author><first>Jiale</first><last>Han</last></author>
      <author><first>Yi</first><last>Liang</last></author>
      <author><first>Bo</first><last>Cheng</last></author>
      <pages>10970–10983</pages>
      <abstract>Continual Few-shot Relation Extraction (CFRE) is a practical problem that requires the model to continuously learn novel relations while avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses prompt learning to explore the implicit capabilities of pre-trained language models to address the above two challenges, thereby making language models better continual few-shot relation extractors. Specifically, we propose a Contrastive Prompt Learning framework, which designs prompt representation to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based contrastive learning to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in low-resource scenarios, we introduce an effective memory augmentation strategy that employs well-crafted prompts to guide ChatGPT in generating diverse samples. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin and significantly mitigates catastrophic forgetting and overfitting in low-resource scenarios.</abstract>
      <url hash="8545c1eb">2024.lrec-main.957</url>
      <bibkey>ma-etal-2024-making-pre</bibkey>
    </paper>
    <paper id="958">
      <title>Making Sentence Embeddings Robust to User-Generated Content</title>
      <author><first>Lydia</first><last>Nishimwe</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <pages>10984–10998</pages>
      <abstract>NLP models have been known to perform poorly on user-generated content (UGC), mainly because it presents a lot of lexical variations and deviates from the standard texts on which most of these models were trained. In this work, we focus on the robustness of LASER, a sentence embedding model, to UGC data. We evaluate this robustness by LASER’s ability to represent non-standard sentences and their standard counterparts close to each other in the embedding space. Inspired by previous works extending LASER to other languages and modalities, we propose RoLASER, a robust English encoder trained using a teacher-student approach to reduce the distances between the representations of standard and UGC sentences. We show that with training only on standard and synthetic UGC-like data, RoLASER significantly improves LASER’s robustness to both natural and artificial UGC data by achieving up to 2x and 11x better scores. We also perform a fine-grained analysis on artificial UGC data and find that our model greatly outperforms LASER on its most challenging UGC phenomena such as keyboard typos and social media abbreviations. Evaluation on downstream tasks shows that RoLASER performs comparably to or better than LASER on standard data, while consistently outperforming it on UGC data.</abstract>
      <url hash="5f0fa98b">2024.lrec-main.958</url>
      <bibkey>nishimwe-etal-2024-making-sentence</bibkey>
    </paper>
    <paper id="959">
      <title><fixed-case>M</fixed-case>alaysian <fixed-case>E</fixed-case>nglish News Decoded: A Linguistic Resource for Named Entity and Relation Extraction</title>
      <author><first>MohanRaj</first><last>Chanthran</last></author>
      <author><first>Lay-Ki</first><last>Soon</last></author>
      <author><first>Huey Fang</first><last>Ong</last></author>
      <author><first>Bhawani</first><last>Selvaretnam</last></author>
      <pages>10999–11022</pages>
      <abstract>Standard English and Malaysian English exhibit notable differences, posing challenges for natural language processing (NLP) tasks on Malaysian English. An experiment using state-of-the-art Named Entity Recognition (NER) solutions in Malaysian English news articles highlights that they cannot handle morphosyntactic variations in Malaysian English. Unfortunately, most of the existing datasets are mainly based on Standard English, which is not sufficient to enhance NLP tasks in Malaysian English. To the best of our knowledge, there is no annotated dataset that can be used to improve the model. To address this issue, we have constructed a Malaysian English News (MEN) dataset, which contains 200 news articles that are manually annotated with entities and relations. We then fine-tuned the spaCy NER tool and validated that having a dataset tailor-made for Malaysian English could significantly improve the performance of NER in Malaysian English. This paper presents our efforts to acquire data, the annotation methodology, and a detailed analysis of the annotated dataset. To ensure the quality of the annotation, we have measured the Inter-Annotator Agreement (IAA), and any disagreements were resolved by a subject matter expert through adjudication. After a rigorous quality check, we have developed a dataset with 6,061 entities and 3,268 relation instances. Finally, we discuss spaCy fine-tuning setup and analysis of NER performance. This unique dataset will contribute significantly to the advancement of NLP research in Malaysian English, allowing researchers to accelerate their progress, particularly in NER and relation extraction.</abstract>
      <url hash="f01b2e74">2024.lrec-main.959</url>
      <bibkey>chanthran-etal-2024-malaysian-english</bibkey>
    </paper>
    <paper id="960">
      <title>m<fixed-case>ALBERT</fixed-case>: Is a Compact Multilingual <fixed-case>BERT</fixed-case> Model Still Worth It?</title>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Sahar</first><last>Ghannay</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>11023–11029</pages>
      <abstract>Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical and ecological impact of such models. In this article, considering these critical remarks, we propose to focus on smaller models, such as compact models like ALBERT, which are more ecologically virtuous than these PLM. However, PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural Language Understanding, classification, Question–Answering tasks. PLMs also have the advantage of being multilingual, and, as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, we propose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipedia data, which complies with the ethical aspect of such a language model. We also evaluate the model against classical multilingual PLMs in classical NLP tasks. Finally, this paper proposes a rare study on the subword tokenization impact on language performances.</abstract>
      <url hash="2329ec68">2024.lrec-main.960</url>
      <bibkey>servan-etal-2024-malbert-compact</bibkey>
    </paper>
    <paper id="961">
      <title><fixed-case>M</fixed-case>an<fixed-case>NER</fixed-case> &amp; <fixed-case>M</fixed-case>an<fixed-case>POS</fixed-case>: Pioneering <fixed-case>NLP</fixed-case> for Endangered <fixed-case>M</fixed-case>anchu Language</title>
      <author><first>Sangah</first><last>Lee</last></author>
      <author><first>Sungjoo</first><last>Byun</last></author>
      <author><first>Jean</first><last>Seo</last></author>
      <author><first>Minha</first><last>Kang</last></author>
      <pages>11030–11039</pages>
      <abstract>We present pioneering research in the realm of Natural Language Processing (NLP) for the endangered Manchu language. Recognizing the critical importance of linguistic preservation, we experiment with three language models – BiLSTM-CRF, BERT, and mBERT – for Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. Given the limited digitized Manchu text available, we augment the data using GloVe embeddings for the pre-training of BERT-based models. Remarkably, all models demonstrated outstanding performance, achieving over 90% F1 score in both NER and POS tagging tasks. Our research not only marks the first application of NLP on Manchu and the inaugural use of BERT-based models for the language but also stands as the first endeavor to employ Manchu for NER and POS tagging. To foster further exploration and applications in the field, we make our fine-tuning dataset and models available to the public. Through this research, we aim to underscore the significance of NLP in the protection and revitalization of low-resource languages.</abstract>
      <url hash="519a3fff">2024.lrec-main.961</url>
      <bibkey>lee-etal-2024-manner-manpos</bibkey>
    </paper>
    <paper id="962">
      <title>Mapping the Past: Geographically Linking an Early 20th Century <fixed-case>S</fixed-case>wedish Encyclopedia with <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Axel</first><last>Ahlin</last></author>
      <author><first>Alfred</first><last>Myrne Blåder</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <pages>11040–11048</pages>
      <abstract>In this paper, we describe the extraction of all the location entries from a prominent Swedish encyclopedia from the early 20th century, the Nordisk Familjebok ‘Nordic Family Book’, focusing on the second edition called Uggleupplagan. This edition comprises 38 volumes and over 182,000 articles, making it one of the most extensive Swedish encyclopedia editions. Using a classifier, we first determined the category of the entities. We found that approximately 22 percent of the encyclopedia entries were locations. We applied a named entity recognition to these entries and we linked them to Wikidata. Wikidata enabled us to extract their precise geographic locations resulting in almost 18,000 valid coordinates. We then analyzed the distribution of these locations and the entry selection process. It showed a concentration within Sweden, Germany, and the United Kingdom. The paper sheds light on the selection and representation of geographic information in the Nordisk Familjebok, providing insights into historical and societal perspectives. It also paves the way for future investigations into entry selection in different time periods and comparative analyses among various encyclopedias.</abstract>
      <url hash="5ad612d9">2024.lrec-main.962</url>
      <bibkey>ahlin-etal-2024-mapping-past</bibkey>
    </paper>
    <paper id="963">
      <title>Mapping Work Task Descriptions from <fixed-case>G</fixed-case>erman Job Ads on the <fixed-case>O</fixed-case>*<fixed-case>NET</fixed-case> Work Activities Ontology</title>
      <author><first>Ann-Sophie</first><last>Gnehm</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <pages>11049–11059</pages>
      <abstract>This work addresses the challenge of extracting job tasks from German job postings and mapping them to the fine-grained work activities classification in the O*NET labor market ontology. By utilizing ontological data with a Multiple Negatives Ranking loss and integrating a modest volume of labeled job advertisement data into the training process, our top configuration achieved a notable precision of 70% for the best mapping on the test set, representing a substantial improvement compared to the 33% baseline delivered by a general-domain SBERT. In our experiments the following factors proved to be most effective for improving SBERT models: First, the incorporation of subspan markup, both during training and inference, supports accurate classification, by streamlining varied job ad task formats with structured, uniform ontological work activities. Second, the inclusion of additional occupational information from O*NET into training supported learning by contextualizing hierarchical ontological relationships. Third, the most significant performance improvement was achieved by updating SBERT models with labeled job ad data specifically addressing challenging cases encountered during pre-finetuning, effectively bridging the semantic gap between O*NET and job ad data.</abstract>
      <url hash="512ae676">2024.lrec-main.963</url>
      <bibkey>gnehm-clematide-2024-mapping-work</bibkey>
    </paper>
    <paper id="964">
      <title><fixed-case>MARASTA</fixed-case>: A Multi-dialectal <fixed-case>A</fixed-case>rabic Cross-domain Stance Corpus</title>
      <author><first>Anis</first><last>Charfi</last></author>
      <author><first>Mabrouka</first><last>Ben-Sghaier</last></author>
      <author><first>Andria Samy Raouf</first><last>Atalla</last></author>
      <author><first>Raghda</first><last>Akasheh</last></author>
      <author><first>Sara</first><last>Al-Emadi</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <pages>11060–11069</pages>
      <abstract>This paper introduces a cross-domain and multi-dialectal stance corpus for Arabic that includes four regions in the Arab World and covers the main Arabic dialect groups. Our corpus consists of 4657 sentences manually annotated with each sentence’s stance towards a specific topic. For each region, we collected sentences related to two controversial topics. We annotated each sentence by at least two annotators to indicate if its stance favors the topic, is against it, or is neutral. Our corpus is well-balanced concerning dialect and stance. Approximately half of the sentences are in Modern Standard Arabic (MSA) for each region, and the other half is in the region’s respective dialect. We conducted several machine-learning experiments for stance detection using our new corpus. Our most successful model is the Multi-Layer Perceptron (MLP), using Unigram or TF-IDF extracted features, which yielded an F1-score of 0.66 and an accuracy score of 0.66. Compared with the most similar state-of-the-art dataset, our dataset outperformed in specific stance classes, particularly “neutral” and “against”.</abstract>
      <url hash="6e9c1d62">2024.lrec-main.964</url>
      <bibkey>charfi-etal-2024-marasta-multi</bibkey>
    </paper>
    <paper id="965">
      <title>Massively Multilingual Token-Based Typology Using the Parallel <fixed-case>B</fixed-case>ible Corpus</title>
      <author><first>Amanda</first><last>Kann</last></author>
      <pages>11070–11079</pages>
      <abstract>The parallel Bible corpus is a uniquely broad multilingual resource, covering over 1400 languages. While this data is potentially highly useful for extending language coverage in both token-based typology research and various low-resource NLP applications, the restricted register and translational nature of the Bible texts has raised concerns as to whether they are sufficiently representative of language use outside of their specific context. In this paper, we analyze the reliability and generalisability of word order statistics extracted from the Bible corpus from two angles: stability across different translations in the same language, and comparability with Universal Dependencies corpora and typological database classifications from URIEL and Grambank. We find that variation between same-language translations is generally low and that agreement with other data sources and previous work is generally high, suggesting that the impact of issues specific to massively parallel texts is smaller than previously posited.</abstract>
      <url hash="3eeee2ff">2024.lrec-main.965</url>
      <attachment type="OptionalSupplementaryMaterial" hash="24acc7a9">2024.lrec-main.965.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kann-2024-massively-multilingual</bibkey>
    </paper>
    <paper id="966">
      <title>Mathematical Entities: Corpora and Benchmarks</title>
      <author><first>Jacob</first><last>Collard</last></author>
      <author><first>Valeria</first><last>de Paiva</last></author>
      <author><first>Eswaran</first><last>Subrahmanian</last></author>
      <pages>11080–11089</pages>
      <abstract>Mathematics is a highly specialized domain with its own unique set of challenges. Despite this, there has been relatively little research on natural language processing for mathematical texts, and there are few mathematical language resources aimed at NLP. In this paper, we aim to provide annotated corpora that can be used to study the language of mathematics in different contexts, ranging from fundamental concepts found in textbooks to advanced research mathematics. We preprocess the corpora with a neural parsing model and some manual intervention to provide part-of-speech tags, lemmas, and dependency trees. In total, we provide 182397 sentences across three corpora. We then aim to test and evaluate several noteworthy natural language processing models using these corpora, to show how well they can adapt to the domain of mathematics and provide useful tools for exploring mathematical language. We evaluate several neural and symbolic models against benchmarks that we extract from the corpus metadata to show that terminology extraction and definition extraction do not easily generalize to mathematics, and that additional work is needed to achieve good performance on these metrics. Finally, we provide a learning assistant that grants access to the content of these corpora in a context-sensitive manner, utilizing text search and entity linking. Though our corpora and benchmarks provide useful metrics for evaluating mathematical language processing, further work is necessary to adapt models to mathematics in order to provide more effective learning assistants and apply NLP methods to different mathematical domains.</abstract>
      <url hash="6e1f3691">2024.lrec-main.966</url>
      <bibkey>collard-etal-2024-mathematical-entities</bibkey>
    </paper>
    <paper id="967">
      <title><fixed-case>M</fixed-case>cc<fixed-case>STN</fixed-case>: Multi-Scale Contrast and Fine-Grained Feature Fusion Networks for Subject-driven Style Transfer</title>
      <author><first>Honggang</first><last>Zhao</last></author>
      <author><first>Chunling</first><last>Xiao</last></author>
      <author><first>Jiayi</first><last>Yang</last></author>
      <author><first>Guozhu</first><last>Jin</last></author>
      <author><first>Mingyong</first><last>Li</last></author>
      <pages>11090–11100</pages>
      <abstract>Stylistic transformation of artistic images is an important part of the current image processing field. In order to access the aesthetic artistic expression of style images, recent research has applied attention mechanisms to the field of style transfer. This approach transforms style images into tokens by calculating attention and then migrating the artistic style of the image through a decoder. Due to the very low semantic similarity between the original image and the style image, this results in many fine-grained style features being discarded. This can lead to discordant artifacts or obvious artifacts. To address this problem, we propose MccSTN, a novel style representation and transfer framework that can be adapted to existing arbitrary image style transfers. Specifically, we first introduce a feature fusion module (Mccformer) to fuse aesthetic features in style images with fine-grained features in content images. Feature maps are obtained through Mccformer. The feature map is then fed into the decoder to get the image we want. In order to lighten the model and train it quickly, we consider the relationship between specific styles and the overall style distribution. We introduce a multi-scale augmented contrast module that learns style representations from a large number of image pairs.</abstract>
      <url hash="c4c29e81">2024.lrec-main.967</url>
      <bibkey>zhao-etal-2024-mccstn-multi</bibkey>
    </paper>
    <paper id="968">
      <title><fixed-case>MCIL</fixed-case>: Multimodal Counterfactual Instance Learning for Low-resource Entity-based Multimodal Information Extraction</title>
      <author><first>Baohang</first><last>Zhou</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Kehui</first><last>Song</last></author>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Xuhui</first><last>Sui</last></author>
      <author><first>Xiaojie</first><last>Yuan</last></author>
      <pages>11101–11110</pages>
      <abstract>Multimodal information extraction (MIE) is a challenging task which aims to extract the structural information in free text coupled with the image for constructing the multimodal knowledge graph. The entity-based MIE tasks are based on the entity information to complete the specific tasks. However, the existing methods only investigated the entity-based MIE tasks under supervised learning with adequate labeled data. In the real-world scenario, collecting enough data and annotating the entity-based samples are time-consuming, and impractical. Therefore, we propose to investigate the entity-based MIE tasks under the low-resource settings. The conventional models are prone to overfitting on limited labeled data, which can result in poor performance. This is because the models tend to learn the bias existing in the limited samples, which can lead them to model the spurious correlations between multimodal features and task labels. To provide a more comprehensive understanding of the bias inherent in multimodal features of MIE samples, we decompose the features into image, entity, and context factors. Furthermore, we investigate the causal relationships between these factors and model performance, leveraging the structural causal model to delve into the correlations between the input features and output labels. Based on this, we propose the multimodal counterfactual instance learning framework to generate the counterfactual instances by the interventions on the limited observational samples. In the framework, we analyze the causal effect of the counterfactual instances and exploit it as a supervisory signal to maximize the effect for reducing the bias and improving the generalization of the model. Empirically, we evaluate the proposed method on the two public MIE benchmark datasets and the experimental results verify the effectiveness of it.</abstract>
      <url hash="83325fe8">2024.lrec-main.968</url>
      <bibkey>zhou-etal-2024-mcil-multimodal</bibkey>
    </paper>
    <paper id="969">
      <title><fixed-case>MCTS</fixed-case>: A Multi-Reference <fixed-case>C</fixed-case>hinese Text Simplification Dataset</title>
      <author><first>Ruining</first><last>Chong</last></author>
      <author><first>Luming</first><last>Lu</last></author>
      <author><first>Liner</first><last>Yang</last></author>
      <author><first>Jinran</first><last>Nie</last></author>
      <author><first>Zhenghao</first><last>Liu</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Shuhan</first><last>Zhou</last></author>
      <author><first>Yaoxin</first><last>Li</last></author>
      <author><first>Erhong</first><last>Yang</last></author>
      <pages>11111–11122</pages>
      <abstract>Text simplification aims to make the text easier to understand by applying rewriting transformations. There has been very little research on Chinese text simplification for a long time. The lack of generic evaluation data is an essential reason for this phenomenon. In this paper, we introduce MCTS, a multi-reference Chinese text simplification dataset. We describe the annotation process of the dataset and provide a detailed analysis. Furthermore, we evaluate the performance of several unsupervised methods and advanced large language models. We additionally provide Chinese text simplification parallel data that can be used for training, acquired by utilizing machine translation and English text simplification. We hope to build a basic understanding of Chinese text simplification through the foundational work and provide references for future research. All of the code and data are released at https://github.com/blcuicall/mcts/.</abstract>
      <url hash="f6610ecf">2024.lrec-main.969</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2ee01f21">2024.lrec-main.969.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>chong-etal-2024-mcts-multi</bibkey>
    </paper>
    <paper id="970">
      <title><fixed-case>MDS</fixed-case>: A Fine-Grained Dataset for Multi-Modal Dialogue Summarization</title>
      <author><first>Zhipeng</first><last>Liu</last></author>
      <author><first>Xiaoming</first><last>Zhang</last></author>
      <author><first>Litian</first><last>Zhang</last></author>
      <author><first>Zelong</first><last>Yu</last></author>
      <pages>11123–11137</pages>
      <abstract>Due to the explosion of various dialogue scenes, summarizing the dialogue into a short message has drawn much attention recently. In the multi-modal dialogue scene, people tend to use tone and body language to illustrate their intentions. While traditional dialogue summarization has predominantly focused on textual content, this approach may overlook vital visual and audio information essential for understanding multi-modal interactions. Recognizing the established field of multi-modal dialogue summarization, we develop a new multi-modal dialogue summarization dataset (MDS), which aims to enhance the variety and scope of data available for this research area. MDS provides a demanding testbed for multi-modal dialogue summarization. Subsequently, we conducted a comparative analysis of various summarization techniques on MDS and found that the existing methods tend to produce redundant and incoherent summaries. All of the models generate unfaithful facts to some degree, suggesting future research directions. MDS is available at https://github.com/R00kkie/MDS.</abstract>
      <url hash="625c59e0">2024.lrec-main.970</url>
      <attachment type="OptionalSupplementaryMaterial" hash="38ada17c">2024.lrec-main.970.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>liu-etal-2024-mds-fine</bibkey>
    </paper>
    <paper id="971">
      <title>Measuring Cross-Text Cohesion for Segmentation Similarity Scoring</title>
      <author><first>Gerardo</first><last>Ocampo Diaz</last></author>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <pages>11138–11147</pages>
      <abstract>Text segmentation is the task of dividing a sequence of text elements (eg. words, sentences, or paragraphs) into meaningful chunks. Although exciting advances are being made in modern segmentation-based tasks, such as automatically generating podcast chapters, current segmentation similarity metrics share a critical weakness: they are content-agnostic. In this paper, we present a word-embedding-based metric of cross-textual cohesion based on the formal linguistic definition of cohesion and incorporate it into a new segmentation similarity metric, SED. Our similarity metric, SED, is capable of providing fine-grained segmentation similarity scoring for the 3 basic segmentation errors: transposition, insertion, and deletion, as well as mixtures of them, avoiding the limitations of traditional metrics. We discuss the benefits of SED and evaluate its alignment with human judgement for each of the 3 basic error types. We show that our metric aligns with human evaluations significantly more than traditional metrics. We briefly discuss future work, such as the integration of anaphora resolution into our cohesion-based metric, and make our code publicly available.</abstract>
      <url hash="9352fd17">2024.lrec-main.971</url>
      <bibkey>ocampo-diaz-ouyang-2024-measuring-cross</bibkey>
    </paper>
    <paper id="972">
      <title>Medical Entity Disambiguation with Medical Mention Relation and Fine-grained Entity Knowledge</title>
      <author><first>Wenpeng</first><last>Lu</last></author>
      <author><first>Guobiao</first><last>Zhang</last></author>
      <author><first>Xueping</first><last>Peng</last></author>
      <author><first>Hongjiao</first><last>Guan</last></author>
      <author><first>Shoujin</first><last>Wang</last></author>
      <pages>11148–11158</pages>
      <abstract>Medical entity disambiguation (MED) plays a crucial role in natural language processing and biomedical domains, which is the task of mapping ambiguous medical mentions to structured candidate medical entities from knowledge bases (KBs). However, existing methods for MED often fail to fully utilize the knowledge within medical KBs and overlook essential interactions between medical mentions and candidate entities, resulting in knowledge- and interaction-inefficient modeling and suboptimal disambiguation performance. To address these limitations, this paper proposes a novel approach, MED with Medical Mention Relation and Fine-grained Entity Knowledge (MMR-FEK). Specifically, MMR-FEK incorporates a mention relation fusion module and an entity knowledge fusion module, followed by an interaction module. The former employs a relation graph convolutional network to fuse mention relation information between medical mentions to enhance mention representations, while the latter leverages an attention mechanism to fuse synonym and type information of candidate entities to enhance entity representations. Afterwards, an interaction module is designed to employ a bidirectional attention mechanism to capture interactions between mentions and entities to generate the matching representation. Extensive experiments on two publicly available real-world datasets demonstrate MMR-FEK’s superiority over state-of-the-art(SOTA) MED baselines across all metrics. Our source code is publicly available.</abstract>
      <url hash="f62f8417">2024.lrec-main.972</url>
      <bibkey>lu-etal-2024-medical-entity</bibkey>
    </paper>
    <paper id="973">
      <title>Medical Vision-Language Pre-Training for Brain Abnormalities</title>
      <author><first>Masoud</first><last>Monajatipoor</last></author>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Aichi</first><last>Chien</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>11159–11164</pages>
      <abstract>Vision-language models have become increasingly powerful for tasks that require an understanding of both visual and linguistic elements, bridging the gap between these modalities. In the context of multimodal clinical AI, there is a growing need for models that possess domain-specific knowledge, as existing models often lack the expertise required for medical applications. In this paper, we take <i>brain abnormalities</i> as an example to demonstrate how to automatically collect medical image-text aligned data for pretraining from public resources such as PubMed. In particular, we present a pipeline that streamlines the pre-training process by initially collecting a large brain image-text dataset from case reports and published journals and subsequently constructing a high-performance vision-language model tailored to specific medical tasks. We also investigate the unique challenge of mapping subfigures to subcaptions in the medical domain. We evaluated the resulting model with quantitative and qualitative intrinsic evaluations. The resulting dataset will be released to the community.</abstract>
      <url hash="4c15ecb8">2024.lrec-main.973</url>
      <bibkey>monajatipoor-etal-2024-medical-vision</bibkey>
    </paper>
    <paper id="974">
      <title><fixed-case>M</fixed-case>ed<fixed-case>MT</fixed-case>5: An Open-Source Multilingual Text-to-Text <fixed-case>LLM</fixed-case> for the Medical Domain</title>
      <author><first>Iker</first><last>García-Ferrero</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>Aitziber</first><last>Atutxa Salazar</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Iker</first><last>de la Iglesia</last></author>
      <author><first>Alberto</first><last>Lavelli</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <author><first>Benjamin</first><last>Molinet</last></author>
      <author><first>Johana</first><last>Ramirez-Romero</last></author>
      <author><first>German</first><last>Rigau</last></author>
      <author><first>Jose Maria</first><last>Villa-Gonzalez</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <author><first>Andrea</first><last>Zaninello</last></author>
      <pages>11165–11177</pages>
      <abstract>Research on language technology for the development of medical applications is currently a hot topic in Natural Language Understanding and Generation. Thus, a number of large language models (LLMs) have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction. While these LLMs display competitive performance on automated medical texts benchmarks, they have been pre-trained and evaluated with a focus on a single language (English mostly). This is particularly true of text-to-text models, which typically require large amounts of domain-specific pre-training data, often not easily accessible for many languages. In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish. This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we present two new evaluation benchmarks for all four languages with the aim of facilitating multilingual research in this domain. A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian benchmarks, while being competitive with current state-of-the-art LLMs in English.</abstract>
      <url hash="50993942">2024.lrec-main.974</url>
      <bibkey>garcia-ferrero-etal-2024-medmt5-open</bibkey>
    </paper>
    <paper id="975">
      <title><fixed-case>M</fixed-case>ed<fixed-case>QA</fixed-case>-<fixed-case>SWE</fixed-case> - a Clinical Question &amp; Answer Dataset for <fixed-case>S</fixed-case>wedish</title>
      <author><first>Niclas</first><last>Hertzberg</last></author>
      <author><first>Anna</first><last>Lokrantz</last></author>
      <pages>11178–11186</pages>
      <abstract>Considering the rapid improvement of large generative language models, it is important to measure their ability to encode clinical domain knowledge in order to help determine their potential utility in a clinical setting. To this end we present MedQA-SWE – a novel multiple choice, clinical question &amp; answering (Q&amp;A) dataset in Swedish consisting of 3,180 questions. The dataset was created from a series of exams aimed at evaluating doctors’ clinical understanding and decision making and is the first open-source clinical Q&amp;A dataset in Swedish. The exams – originally in PDF format – were parsed and each question manually checked and curated in order to limit errors in the dataset. We provide dataset statistics along with benchmark accuracy scores of seven large generative language models on a representative sample of questions in a zero-shot setting, with some models showing impressive performance given the difficulty of the exam the dataset is based on.</abstract>
      <url hash="71422743">2024.lrec-main.975</url>
      <bibkey>hertzberg-lokrantz-2024-medqa-swe</bibkey>
    </paper>
    <paper id="976">
      <title><fixed-case>M</fixed-case>emory<fixed-case>P</fixed-case>rompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models</title>
      <author><first>Nathanael Carraz</first><last>Rakotonirina</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <pages>11187–11195</pages>
      <abstract>Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM’s ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the underlying LM.</abstract>
      <url hash="39eda84f">2024.lrec-main.976</url>
      <bibkey>rakotonirina-baroni-2024-memoryprompt-light</bibkey>
    </paper>
    <paper id="977">
      <title><fixed-case>M</fixed-case>ental<fixed-case>H</fixed-case>elp: A Multi-Task Dataset for Mental Health in Social Media</title>
      <author><first>Nishat</first><last>Raihan</last></author>
      <author><first>Sadiya Sayara Chowdhury</first><last>Puspo</last></author>
      <author><first>Shafkat</first><last>Farabi</last></author>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>11196–11203</pages>
      <abstract>Early detection of mental health disorders is an essential step in treating and preventing mental health conditions. Computational approaches have been applied to users’ social media profiles in an attempt to identify various mental health conditions such as depression, PTSD, schizophrenia, and eating disorders. The interest in this topic has motivated the creation of various depression detection datasets. However, annotating such datasets is expensive and time-consuming, limiting their size and scope. To overcome this limitation, we present MentalHelp, a large-scale semi-supervised mental disorder detection dataset containing 14 million instances. The corpus was collected from Reddit and labeled in a semi-supervised way using an ensemble of three separate models - flan-T5, Disor-BERT, and Mental-BERT.</abstract>
      <url hash="d106c399">2024.lrec-main.977</url>
      <bibkey>raihan-etal-2024-mentalhelp-multi</bibkey>
    </paper>
    <paper id="978">
      <title><fixed-case>M</fixed-case>ental<fixed-case>R</fixed-case>isk<fixed-case>ES</fixed-case>: A New Corpus for Early Detection of Mental Disorders in <fixed-case>S</fixed-case>panish</title>
      <author><first>Alba M.</first><last>Mármol Romero</last></author>
      <author><first>Adrián</first><last>Moreno Muñoz</last></author>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>M. Dolores</first><last>Molina González</last></author>
      <author><first>María Teresa</first><last>Martín Valdivia</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <author><first>Arturo</first><last>Montejo Ráez</last></author>
      <pages>11204–11214</pages>
      <abstract>With mental health issues on the rise on the Web, especially among young people, there is a growing need for effective identification and intervention. In this paper, we introduce a new open-sourced corpus for the early detection of mental disorders in Spanish, focusing on eating disorders, depression, and anxiety. It consists of user messages posted on groups within the Telegram message platform and contains over 1,300 subjects with more than 45,000 messages posted in different public Telegram groups. This corpus has been manually annotated via crowdsourcing and is prepared for its use in several Natural Language Processing tasks including text classification and regression tasks. The samples in the corpus include both text and time data. To provide a benchmark for future research, we conduct experiments on text classification and regression by using state-of-the-art transformer-based models.</abstract>
      <url hash="33bb66a8">2024.lrec-main.978</url>
      <bibkey>marmol-romero-etal-2024-mentalriskes-new</bibkey>
    </paper>
    <paper id="979">
      <title>Meta-Adapter for Self-Supervised Speech Models: A Solution to Low-Resource Speech Recognition Challenges</title>
      <author><first>Yaqi</first><last>Chen</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Xukui</first><last>Yang</last></author>
      <author><first>Wenlin</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Qu</last></author>
      <pages>11215–11221</pages>
      <abstract>Self-supervised models have demonstrated remarkable performance in speech processing by learning latent representations from large amounts of unlabeled data. Although these models yield promising results on low-resource languages, the computational expense of fine-tuning all model parameters is prohibitively high. Adapters offer a solution by incorporating lightweight bottleneck structures into pre-trained models, enabling efficient parameter adaptation for downstream tasks. However, randomly initialized adapters often underperform in low-resource scenarios, limiting their applicability in low-resource languages. To address this issue, we develop the Meta-Adapter for self-supervised models to obtain meta-initialized parameters that facilitate quick adaptation to low-resource languages. Extensive experiments on the Common Voice and FLEURS datasets demonstrate the superior performance of Meta-Adapters on 12 low-resource languages spanning four different language families. Moreover, Meta-adapters show better generalization and extensibility than traditional pretraining methods.</abstract>
      <url hash="88be5652">2024.lrec-main.979</url>
      <bibkey>chen-etal-2024-meta-adapter</bibkey>
    </paper>
    <paper id="980">
      <title>Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models</title>
      <author><first>Zhuoqun</first><last>Li</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Yaojie</first><last>Lu</last></author>
      <author><first>Hao</first><last>Xiang</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>11222–11228</pages>
      <abstract>Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis for the findings and this can provide primary guidance for evaluation and enhancement of large language models.</abstract>
      <url hash="ea0f1d92">2024.lrec-main.980</url>
      <bibkey>li-etal-2024-meta-cognitive</bibkey>
    </paper>
    <paper id="981">
      <title>Meta-Evaluation of Sentence Simplification Metrics</title>
      <author><first>Noof Abdullah</first><last>Alfear</last></author>
      <author><first>Dimitar</first><last>Kazakov</last></author>
      <author><first>Hend</first><last>Al-Khalifa</last></author>
      <pages>11229–11235</pages>
      <abstract>Automatic Text Simplification (ATS) is one of the major Natural Language Processing (NLP) tasks, which aims to help people understand text that is above their reading abilities and comprehension. ATS models reconstruct the text into a simpler format by deletion, substitution, addition or splitting, while preserving the original meaning and maintaining correct grammar. Simplified sentences are usually evaluated by human experts based on three main factors: simplicity, adequacy and fluency or by calculating automatic evaluation metrics. In this paper, we conduct a meta-evaluation of reference-based automatic metrics for English sentence simplification using high-quality, human-annotated dataset, NEWSELA-LIKERT. We study the behavior of several evaluation metrics at sentence level across four different sentence simplification models. All the models were trained on the NEWSELA-AUTO dataset. The correlation between the metrics’ scores and human judgements was analyzed and the results used to recommend the most appropriate metrics for this task.</abstract>
      <url hash="6775178d">2024.lrec-main.981</url>
      <bibkey>alfear-etal-2024-meta-evaluation</bibkey>
    </paper>
    <paper id="982">
      <title>Metaphors in Online Religious Communication: A Detailed Dataset and Cross-Genre Metaphor Detection</title>
      <author><first>Sebastian</first><last>Reimann</last></author>
      <author><first>Tatjana</first><last>Scheffler</last></author>
      <pages>11236–11246</pages>
      <abstract>We present the first dataset of fine-grained metaphor annotations for texts from online religious communication, where figurative language plays a particularly important role. In addition to binary labels, metaphors are annotated for deliberateness, that is, whether they are communicated explicitly as metaphors, and we provide indicators for such deliberate use. We further show that cross-genre transfer metaphor detection (from the widely used VUA corpus to our Reddit data) leads to a drop in performance due to the shift in topic and metaphors from source domains that did not occur in the training data. We solve this issue by adding a small amount of in-genre data in fine-tuning, leading to notable performance increases of more than 5 points in F1. Moreover, religious communication has the tendency for extended metaphorical comparisons, which are problematic for current metaphor detection systems. Adding in-genre data had slightly positive effects but we argue that to solve this, architectures that consider larger spans of context are necessary.</abstract>
      <url hash="370faef1">2024.lrec-main.982</url>
      <bibkey>reimann-scheffler-2024-metaphors-online</bibkey>
    </paper>
    <paper id="983">
      <title><fixed-case>MEVTR</fixed-case>: A Multilingual Model Enhanced with Visual Text Representations</title>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Wenlong</first><last>Fei</last></author>
      <author><first>Min</first><last>Hu</last></author>
      <author><first>Qingyu</first><last>Zhang</last></author>
      <author><first>Aoqiang</first><last>Zhu</last></author>
      <pages>11247–11261</pages>
      <abstract>The goal of multilingual modelling is to generate multilingual text representations for various downstream tasks in different languages. However, some state-of-the-art pre-trained multilingual models perform poorly on many low-resource languages due to the lack of representation space and model capacity. To alleviate this issue, we propose a Multilingual model Enhanced with Visual Text Representations (MEVTR), which complements textual representations and extends the multilingual representation space with visual text representations. First, the visual encoder focuses on the glyphs and structure of the text to obtain visual text representations, and the textual encoder obtains textual representations. Then, multilingual representations are enhanced by aligning and fusing visual text representations and textual representations. Moreover, we propose similarity constraint, a self-supervised task to prompt the visual encoder to focus on more additional information. Prefix alignment and multi-head bilinear module are designed to acquire an improved integration effect of visual text representations and textual representations. Experimental results indicate that MEVTR benefits from visual text representations and achieves significant performance gains in downstream tasks. In particular, in the zero-shot cross-lingual transfer task, MEVTR achieves results that outperform the state-of-the-art adapter-based framework without the target language adapter.</abstract>
      <url hash="af0c34d1">2024.lrec-main.983</url>
      <bibkey>wang-etal-2024-mevtr-multilingual</bibkey>
    </paper>
    <paper id="984">
      <title>m<fixed-case>F</fixed-case>orms : Multimodal Form Filling with Question Answering</title>
      <author><first>Larry</first><last>Heck</last></author>
      <author><first>Simon</first><last>Heck</last></author>
      <author><first>Anirudh S.</first><last>Sundar</last></author>
      <pages>11262–11271</pages>
      <abstract>This paper presents a new approach to form-filling by reformulating the task as multimodal natural language Question Answering (QA). The reformulation is achieved by first translating the elements on the GUI form (text fields, buttons, icons, etc.) to natural language questions, where these questions capture the element’s multimodal semantics. After a match is determined between the form element (Question) and the user utterance (Answer), the form element is filled through a pre-trained extractive QA system. By leveraging pre-trained QA models and not requiring form-specific training, this approach to form-filling is zero-shot. The paper also presents an approach to further refine the form-filling by using multi-task training to incorporate a potentially large number of successive tasks. Finally, the paper introduces a multimodal natural language form-filling dataset Multimodal Forms (mForms), as well as a multimodal extension of the popular ATIS dataset to support future research and experimentation. Results show the new approach not only maintains robust accuracy for sparse training conditions but achieves state-of-the-art F1 of 0.97 on ATIS with approximately 1/10th the training data.</abstract>
      <url hash="eb282e7a">2024.lrec-main.984</url>
      <bibkey>heck-etal-2024-mforms-multimodal</bibkey>
    </paper>
    <paper id="985">
      <title><fixed-case>MHGRL</fixed-case>: An Effective Representation Learning Model for Electronic Health Records</title>
      <author><first>Feiyan</first><last>Liu</last></author>
      <author><first>Liangzhi</first><last>Li</last></author>
      <author><first>Xiaoli</first><last>Wang</last></author>
      <author><first>Feng</first><last>Luo</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Yiming</first><last>Qian</last></author>
      <pages>11272–11282</pages>
      <abstract>Electronic health records (EHRs) serve as a digital repository storing comprehensive medical information about patients. Representation learning for EHRs plays a crucial role in healthcare applications. In this paper, we propose a Multimodal Heterogeneous Graph-enhanced Representation Learning, denoted as MHGRL, aimed at learning effective EHR representations. To address the challenge posed by data insufficiency of EHRs, MHGRL utilizes a multimodal heterogeneous graph to model an EHR. Specifically, we construct a heterogeneous graph for each EHR and enrich it by incorporating multimodal information with medical ontology and textual notes. With the integration of pre-trained model, graph neural network, and attention mechanism, MHGRL effectively incorporates both node attributes and structural information across a multimodal heterogeneous graph. Moreover, we employ contrastive learning to ensure the consistency of representations for similar EHRs and improve the model robustness. The experimental results show that MHGRL outperforms all baselines on two real clinical datasets in downstream tasks, including EHR clustering and disease prediction. The code is available at https://github.com/emmali808/MHGRL.</abstract>
      <url hash="09bf2b0c">2024.lrec-main.985</url>
      <bibkey>liu-etal-2024-mhgrl-effective</bibkey>
    </paper>
    <paper id="986">
      <title><fixed-case>M</fixed-case>i<fixed-case>D</fixed-case>e22: An Annotated Multi-Event Tweet Dataset for Misinformation Detection</title>
      <author><first>Cagri</first><last>Toraman</last></author>
      <author><first>Oguzhan</first><last>Ozcelik</last></author>
      <author><first>Furkan</first><last>Sahinuc</last></author>
      <author><first>Fazli</first><last>Can</last></author>
      <pages>11283–11295</pages>
      <abstract>The rapid dissemination of misinformation through online social networks poses a pressing issue with harmful consequences jeopardizing human health, public safety, democracy, and the economy; therefore, urgent action is required to address this problem. In this study, we construct a new human-annotated dataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with their misinformation labels for several recent events between 2020 and 2022, including the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset includes user engagements with the tweets in terms of likes, replies, retweets, and quotes. We also provide a detailed data analysis with descriptive statistics and the experimental results of a benchmark evaluation for misinformation detection.</abstract>
      <url hash="e86d75ed">2024.lrec-main.986</url>
      <bibkey>toraman-etal-2024-mide22-annotated</bibkey>
    </paper>
    <paper id="987">
      <title>Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role Labeling for Legal Documents</title>
      <author><first>Santosh</first><last>T.y.s.s.</last></author>
      <author><first>Hassan</first><last>Sarwat</last></author>
      <author><first>Ahmed Mohamed Abdelaal</first><last>Abdou</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>11296–11306</pages>
      <abstract>Rhetorical Role Labeling (RRL) of legal judgments is essential for various tasks, such as case summarization, semantic search and argument mining. However, it presents challenges such as inferring sentence roles from context, interrelated roles, limited annotated data, and label imbalance. This study introduces novel techniques to enhance RRL performance by leveraging knowledge from semantically similar instances (neighbours). We explore inference-based and training-based approaches, achieving remarkable improvements in challenging macro-F1 scores. For inference-based methods, we explore interpolation techniques that bolster label predictions without re-training. While in training-based methods, we integrate prototypical learning with our novel discourse-aware contrastive method that work directly on embedding spaces. Additionally, we assess the cross-domain applicability of our methods, demonstrating their effectiveness in transferring knowledge across diverse legal domains.</abstract>
      <url hash="c20e1f01">2024.lrec-main.987</url>
      <bibkey>t-y-s-s-etal-2024-mind-neighbours</bibkey>
    </paper>
    <paper id="988">
      <title><fixed-case>M</fixed-case>in<fixed-case>T</fixed-case>: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning</title>
      <author><first>Zhenwen</first><last>Liang</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Xiaoman</first><last>Pan</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Qingkai</first><last>Zeng</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>11307–11318</pages>
      <abstract>Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on distilling knowledge from powerful yet inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different “views” that may help each other and leverage them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables relatively small LMs to outperform prior approaches that heavily rely on knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains.</abstract>
      <url hash="1b0d3154">2024.lrec-main.988</url>
      <bibkey>liang-etal-2024-mint-boosting</bibkey>
    </paper>
    <paper id="989">
      <title>Mitigating Linguistic Artifacts in Emotion Recognition for Conversations from <fixed-case>TV</fixed-case> Scripts to Daily Conversations</title>
      <author><first>Donovan</first><last>Ong</last></author>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Jian</first><last>Su</last></author>
      <author><first>Bin</first><last>Chen</last></author>
      <pages>11319–11324</pages>
      <abstract>Emotion Recognition in Conversations (ERC) is a well-studied task with numerous potential real-world applications. However, existing ERC models trained on the MELD dataset derived from TV series, struggle when applied to daily conversation datasets. A closer examination of the datasets unveils the prevalence of linguistic artifacts such as repetitions and interjections in TV scripts, which ERC models may exploit when making predictions. To address this issue, we explore two techniques aimed at reducing the reliance of ERC models on these artifacts: 1) using contrastive learning to prioritize emotional features over dataset-specific linguistic style and 2) refining emotion predictions with pseudo-emotion intensity score. Our experiment results show that reducing reliance on the linguistic style found in TV transcripts could enhance model’s robustness and accuracy in diverse conversational contexts.</abstract>
      <url hash="44ac6acc">2024.lrec-main.989</url>
      <bibkey>ong-etal-2024-mitigating-linguistic</bibkey>
    </paper>
    <paper id="990">
      <title>Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering</title>
      <author><first>Yexin</first><last>Wu</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>11325–11340</pages>
      <abstract>Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. We proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at Anonymous.</abstract>
      <url hash="12445c1c">2024.lrec-main.990</url>
      <bibkey>wu-etal-2024-mitigating-misleading</bibkey>
    </paper>
    <paper id="991">
      <title>Mitigating Shortcuts in Language Models with Soft Label Encoding</title>
      <author><first>Zirui</first><last>He</last></author>
      <author><first>Huiqi</first><last>Deng</last></author>
      <author><first>Haiyan</first><last>Zhao</last></author>
      <author><first>Ninghao</first><last>Liu</last></author>
      <author><first>Mengnan</first><last>Du</last></author>
      <pages>11341–11348</pages>
      <abstract>Recent research has shown that large language models rely on spurious correlations in the data for natural language understanding (NLU) tasks. In this work, we aim to answer the following research question: Can we reduce spurious correlations by modifying the ground truth labels of the training data? Specifically, we propose a simple yet effective debiasing framework, named Soft Label Encoding (SoftLE). First, we train a teacher model to quantify each sample’s degree of relying on shortcuts. Then, we encode this shortcut degree into a dummy class and use it to smooth the original ground truth labels, generating soft labels. These soft labels are used to train a more robust student model that reduces spurious correlations between shortcut features and certain classes. Extensive experiments on two NLU benchmark tasks via two language models demonstrate that SoftLE significantly improves out-of-distribution generalization while maintaining satisfactory in-distribution accuracy. Our code is available at https://github.com/ZiruiHE99/sle</abstract>
      <url hash="b7f786c8">2024.lrec-main.991</url>
      <bibkey>he-etal-2024-mitigating-shortcuts</bibkey>
    </paper>
    <paper id="992">
      <title>Mitigating Translationese in Low-resource Languages: The Storyboard Approach</title>
      <author><first>Garry</first><last>Kuwanto</last></author>
      <author><first>Eno-Abasi E.</first><last>Urua</last></author>
      <author><first>Priscilla Amondi</first><last>Amuok</last></author>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Verrah</first><last>Otiende</last></author>
      <author><first>Loice Emma</first><last>Nanyanga</last></author>
      <author><first>Teresiah W.</first><last>Nyoike</last></author>
      <author><first>Aniefon D.</first><last>Akpan</last></author>
      <author><first>Nsima Ab</first><last>Udouboh</last></author>
      <author><first>Idongesit Udeme</first><last>Archibong</last></author>
      <author><first>Idara Effiong</first><last>Moses</last></author>
      <author><first>Ifeoluwatayo A.</first><last>Ige</last></author>
      <author><first>Benjamin</first><last>Ajibade</last></author>
      <author><first>Olumide Benjamin</first><last>Awokoya</last></author>
      <author><first>Idris</first><last>Abdulmumin</last></author>
      <author><first>Saminu Mohammad</first><last>Aliyu</last></author>
      <author><first>Ruqayya Nasir</first><last>Iro</last></author>
      <author><first>Ibrahim Said</first><last>Ahmad</last></author>
      <author><first>Deontae</first><last>Smith</last></author>
      <author><first>Praise-EL</first><last>Michaels</last></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <author><first>Anietie</first><last>Andy</last></author>
      <pages>11349–11360</pages>
      <abstract>Low-resource languages often face challenges in acquiring high-quality language data due to the reliance on translation-based methods, which can introduce the translationese effect. This phenomenon results in translated sentences that lack fluency and naturalness in the target language. In this paper, we propose a novel approach for data collection by leveraging storyboards to elicit more fluent and natural sentences. Our method involves presenting native speakers with visual stimuli in the form of storyboards and collecting their descriptions without direct exposure to the source text. We conducted a comprehensive evaluation comparing our storyboard-based approach with traditional text translation-based methods in terms of accuracy and fluency. Human annotators and quantitative metrics were used to assess translation quality. The results indicate a preference for text translation in terms of accuracy, while our method demonstrates worse accuracy but better fluency in the language focused.</abstract>
      <url hash="a8ea820a">2024.lrec-main.992</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4a388198">2024.lrec-main.992.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kuwanto-etal-2024-mitigating-translationese</bibkey>
    </paper>
    <paper id="993">
      <title><fixed-case>M</fixed-case>ix<fixed-case>RED</fixed-case>: A Mix-lingual Relation Extraction Dataset</title>
      <author><first>Lingxing</first><last>Kong</last></author>
      <author><first>Yougang</first><last>Chu</last></author>
      <author><first>Zheng</first><last>Ma</last></author>
      <author><first>Jianbing</first><last>Zhang</last></author>
      <author><first>Liang</first><last>He</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>11361–11370</pages>
      <abstract>Relation extraction is a critical task in the field of natural language processing with numerous real-world applications. Existing research primarily focuses on monolingual relation extraction or cross-lingual enhancement for relation extraction. Yet, there remains a significant gap in understanding relation extraction in the mix-lingual (or code-switching) scenario, where individuals intermix contents from different languages within sentences, generating mix-lingual content. Due to the lack of a dedicated dataset, the effectiveness of existing relation extraction models in such a scenario is largely unexplored. To address this issue, we introduce a novel task of considering relation extraction in the mix-lingual scenario called MixRE and constructing the human-annotated dataset MixRED to support this task. In addition to constructing the MixRED dataset, we evaluate both state-of-the-art supervised models and large language models (LLMs) on MixRED, revealing their respective advantages and limitations in the mix-lingual scenario. Furthermore, we delve into factors influencing model performance within the MixRE task and uncover promising directions for enhancing the performance of both supervised models and LLMs in this novel task.</abstract>
      <url hash="53ef310c">2024.lrec-main.993</url>
      <bibkey>kong-etal-2024-mixred-mix</bibkey>
    </paper>
    <paper id="994">
      <title>Mixture-of-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>s: An Efficient Multitask Tuning Method for Large Language Models</title>
      <author><first>Wenfeng</first><last>Feng</last></author>
      <author><first>Chuzhan</first><last>Hao</last></author>
      <author><first>Yuewei</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Han</last></author>
      <author><first>Hao</first><last>Wang</last></author>
      <pages>11371–11380</pages>
      <abstract>Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific LLMs.</abstract>
      <url hash="f1221418">2024.lrec-main.994</url>
      <bibkey>feng-etal-2024-mixture-loras</bibkey>
    </paper>
    <paper id="995">
      <title>Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding</title>
      <author><first>Zichen</first><last>Wu</last></author>
      <author><first>Hsiu-Yuan</first><last>Huang</last></author>
      <author><first>Fanyi</first><last>Qu</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>11381–11393</pages>
      <abstract>Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot setting, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.</abstract>
      <url hash="da3abd59">2024.lrec-main.995</url>
      <bibkey>wu-etal-2024-mixture-prompt</bibkey>
    </paper>
    <paper id="996">
      <title><fixed-case>MK</fixed-case>e<fixed-case>CL</fixed-case>: Medical Knowledge-Enhanced Contrastive Learning for Few-shot Disease Diagnosis</title>
      <author><first>Yutian</first><last>Zhao</last></author>
      <author><first>Huimin</first><last>Wang</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>11394–11404</pages>
      <abstract>Artificial intelligence (AI)-aided disease prediction has gained extensive research interest due to its capability to support clinical decision-making. Existing works mainly formulate disease prediction as a multi-label classification problem and use historical Electronic Medical Records (EMR) to train supervised models. However, in real-world clinics, such purely data-driven approaches pose two main challenges: 1) long tail problem: there are excessive EMRs for common diseases and insufficient EMRs for rare diseases, thus training over an imbalanced data set could result in a biased model that ignores rare diseases in diagnosis; 2) easily misdiagnosed diseases: some diseases can be easily distinguished while others sharing analogous conditions are much more difficult. General classification models without emphasizing easily misdiagnosed diseases may generate incorrect predictions. To tackle these two problems, we propose a Medical Knowledge-Enhanced Contrastive Learning (<b>MKeCL</b>) approach to disease diagnosis in this paper. MKeCL incorporates medical knowledge graphs and medical licensing exams in modeling in order to compensate for the insufficient information on rare diseases; To handle hard-to-diagnose diseases, MKeCL introduces a contrastive learning strategy to separate diseases that are easily misdiagnosed. Moreover, we establish a new benchmark, named <b>Jarvis-D</b>, which contains clinical EMRs collected from various hospitals. Experiments on real clinical EMRs show that the proposed MKeCL outperforms existing disease prediction approaches, especially in the setting of few-shot and zero-shot scenarios.</abstract>
      <url hash="a766389c">2024.lrec-main.996</url>
      <bibkey>zhao-etal-2024-mkecl-medical</bibkey>
    </paper>
    <paper id="997">
      <title><fixed-case>MLDSP</fixed-case>-<fixed-case>MA</fixed-case>: Multidimensional Attention for Multi-Round Long Dialogue Sentiment Prediction</title>
      <author><first>Yunfei</first><last>Yin</last></author>
      <author><first>Congrui</first><last>Zou</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Xianjian</first><last>Bao</last></author>
      <pages>11405–11414</pages>
      <abstract>The intelligent chatbot takes dialogue sentiment prediction as the core, and it has to tackle long dialogue sentiment prediction problems in many real-world applications. Current state-of-the-art methods usually employ attention-based dialogue sentiment prediction models. However, as the conversation progresses, more topics are involved and the changes in sentiments become more frequent, which leads to a sharp decline in the accuracy and efficiency of the current methods. Therefore, we propose a Multi-round Long Dialogue Sentiment Prediction based on Multidimensional Attention (MLDSP-MA), which can focus on different topics. In particular, MLSDP-MA leverages a sliding window to capture different topics and traverses all historical dialogues. In each sliding window, the contextual dependency, sentiment persistence, and sentiment infectivity are characterized, and local attention cross fusion is performed. To learn dialogue sentiment globally, global attention is proposed to iteratively learn comprehensive sentiments from historical dialogues, and finally integrate with local attention. We conducted extensive experimental research on publicly available dialogue datasets. The experimental results show that, compared to the current state-of-the-art methods, our model improves by 3.5% in accuracy and 5.7% in Micro-F1 score.</abstract>
      <url hash="e13e06aa">2024.lrec-main.997</url>
      <bibkey>yin-etal-2024-mldsp-ma</bibkey>
    </paper>
    <paper id="998">
      <title><fixed-case>MMAD</fixed-case>:Multi-modal Movie Audio Description</title>
      <author><first>Xiaojun</first><last>Ye</last></author>
      <author><first>Junhao</first><last>Chen</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Haidong</first><last>Xin</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Sheng</first><last>Zhou</last></author>
      <author><first>Jiajun</first><last>Bu</last></author>
      <pages>11415–11428</pages>
      <abstract>Audio Description (AD) aims to generate narrations of information that is not accessible through unimodal hearing in movies to aid the visually impaired in following film narratives. Current solutions rely heavily on manual work, resulting in high costs and limited scalability. While automatic methods have been introduced, they often yield descriptions that are sparse and omit key details. ddressing these challenges, we propose a novel automated pipeline, the Multi-modal Movie Audio Description (MMAD). MMAD harnesses the capabilities of three key modules as well as the power of Llama2 to augment the depth and breadth of the generated descriptions. Specifically, first, we propose an Audio-aware Feature Enhancing Module to provide the model with multi-modal perception capabilities, enriching the background descriptions with a more comprehensive understanding of the environmental features. Second, we propose an Actor-tracking-aware Story Linking Module to aid in the generation of contextual and character-centric descriptions, thereby enhancing the richness of character depictions. Third, we incorporate a Subtitled Movie Clip Contextual Alignment Module, supplying semantic information about various time periods throughout the movie, which facilitates the consideration of the full movie narrative context when describing silent segments, thereby enhancing the richness of the descriptions. Experiments on widely used datasets convincingly demonstrates that MMAD significantly surpasses existing strong baselines in performance, establishing a new state-of-the-art in the field. Our code will be released at https://github.com/Daria8976/MMAD.</abstract>
      <url hash="7ac6f85d">2024.lrec-main.998</url>
      <bibkey>ye-etal-2024-mmad-multi</bibkey>
    </paper>
    <paper id="999">
      <title><fixed-case>MMAPS</fixed-case>: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization</title>
      <author><first>Tao</first><last>Chen</last></author>
      <author><first>Ze</first><last>Lin</last></author>
      <author><first>Hui</first><last>Li</last></author>
      <author><first>Jiayi</first><last>Ji</last></author>
      <author><first>Yiyi</first><last>Zhou</last></author>
      <author><first>Guanbin</first><last>Li</last></author>
      <author><first>Rongrong</first><last>Ji</last></author>
      <pages>11429–11439</pages>
      <abstract>Given the long textual product information and the product image, Multi-modal Product Summarization (MPS) aims to increase customers’ desire to purchase by highlighting product characteristics with a short textual summary. Existing MPS methods can produce promising results. Nevertheless, they still 1) lack end-to-end product summarization, 2) lack multi-grained multi-modal modeling, and 3) lack multi-modal attribute modeling. To improve MPS, we propose an end-to-end multi-grained multi-modal attribute-aware product summarization method (MMAPS) for generating high-quality product summaries in e-commerce. MMAPS jointly models product attributes and generates product summaries. We design several multi-grained multi-modal tasks to better guide the multi-modal learning of MMAPS. Furthermore, we model product attributes based on both text and image modalities so that multi-modal product characteristics can be manifested in the generated summaries. Extensive experiments on a real large-scale Chinese e-commence dataset demonstrate that our model outperforms state-of-the-art product summarization methods w.r.t. several summarization metrics. Our code is publicly available at: https://github.com/KDEGroup/MMAPS.</abstract>
      <url hash="ff505f35">2024.lrec-main.999</url>
      <bibkey>chen-etal-2024-mmaps-end</bibkey>
    </paper>
    <paper id="1000">
      <title><fixed-case>MM</fixed-case>-<fixed-case>IGLU</fixed-case>: Multi-Modal Interactive Grounded Language Understanding</title>
      <author><first>Claudiu Daniel</first><last>Hromei</last></author>
      <author><first>Daniele</first><last>Margiotta</last></author>
      <author><first>Danilo</first><last>Croce</last></author>
      <author><first>Roberto</first><last>Basili</last></author>
      <pages>11440–11451</pages>
      <abstract>This paper explores Interactive Grounded Language Understanding (IGLU) challenges within Human-Robot Interaction (HRI). In this setting, a robot interprets user commands related to its environment, aiming to discern whether a specific command can be executed. If faced with ambiguities or incomplete data, the robot poses relevant clarification questions. Drawing from the NeurIPS 2022 IGLU competition, we enrich the dataset by introducing our multi-modal data and natural language descriptions in <i>MM-IGLU: Multi-Modal Interactive Grounded Language Understanding</i>. Utilizing a BART-based model that integrates the user’s statement with the environment’s description, and a cutting-edge Multi-Modal Large Language Model that merges both visual and textual data, we offer a valuable resource for ongoing research in the domain. Additionally, we discuss the evaluation methods for such tasks, highlighting potential limitations imposed by traditional string-match-based evaluations on this intricate multi-modal challenge. Moreover, we provide an evaluation benchmark based on human judgment to address the limits and capabilities of such baseline models. This resource is released on a dedicated GitHub repository at https://github.com/crux82/MM-IGLU.</abstract>
      <url hash="0afaa5a1">2024.lrec-main.1000</url>
      <bibkey>hromei-etal-2024-mm-iglu</bibkey>
    </paper>
    <paper id="1001">
      <title><fixed-case>MNER</fixed-case>-<fixed-case>MI</fixed-case>: A Multi-image Dataset for Multimodal Named Entity Recognition in Social Media</title>
      <author><first>Shizhou</first><last>Huang</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <author><first>Changqun</first><last>Li</last></author>
      <author><first>Jiabo</first><last>Ye</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <pages>11452–11462</pages>
      <abstract>Recently, multimodal named entity recognition (MNER) has emerged as a vital research area within named entity recognition. However, current MNER datasets and methods are predominantly based on text and a single accompanying image, leaving a significant research gap in MNER scenarios involving multiple images. To address the critical research gap and enhance the scope of MNER for real-world applications, we propose a novel human-annotated MNER dataset with multiple images called MNER-MI. Additionally, we construct a dataset named MNER-MI-Plus, derived from MNER-MI, to ensure its generality and applicability. Based on these datasets, we establish a comprehensive set of strong and representative baselines and we further propose a simple temporal prompt model with multiple images to address the new challenges in multi-image scenarios. We have conducted extensive experiments to demonstrate that considering multiple images provides a significant improvement over a single image and can offer substantial benefits for MNER. Furthermore, our proposed method achieves state-of-the-art results on both MNER-MI and MNER-MI-Plus, demonstrating its effectiveness. The datasets and source code can be found at https://github.com/JinFish/MNER-MI.</abstract>
      <url hash="65150616">2024.lrec-main.1001</url>
      <bibkey>huang-etal-2024-mner-mi</bibkey>
    </paper>
    <paper id="1002">
      <title>Modalities Should Be Appropriately Leveraged: Uncertainty Guidance for Multimodal <fixed-case>C</fixed-case>hinese Spelling Correction</title>
      <author><first>Yongliang</first><last>Lin</last></author>
      <author><first>Zhen</first><last>Zhang</last></author>
      <author><first>Mengting</first><last>Hu</last></author>
      <author><first>Yufei</first><last>Sun</last></author>
      <author><first>Yuzhi</first><last>Zhang</last></author>
      <pages>11463–11474</pages>
      <abstract>Chinese spelling correction (CSC) aims to detect and correct spelling errors in Chinese texts. Most spelling errors are phonetically or graphically similar to the correct ones. Thus, recent works introduce multimodal features to obtain achievements. In this paper, we found that different spelling errors have various biases to each modality, highlighting the importance of appropriately exploiting multimodal features. To achieve this goal, we propose the UGMSC framework, which incorporates uncertainty into both the feature learning and correction stages. Specifically, the UGMSC framework makes predictions with multimodal features and estimates the uncertainty of the corresponding modalities. Then it dynamically fuses the features of all modalities for model learning, and performs spelling correction under the uncertainty-guided strategy. Experimental results on three public datasets demonstrate that the proposed approach provides a significant improvement compared with previous strong multimodal models. The proposed framework is model-agnostic and can be easily applied to other multimodal models.</abstract>
      <url hash="341fd9bd">2024.lrec-main.1002</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1aedb818">2024.lrec-main.1002.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lin-etal-2024-modalities-appropriately</bibkey>
    </paper>
    <paper id="1003">
      <title><fixed-case>M</fixed-case>o<fixed-case>DE</fixed-case>-<fixed-case>C</fixed-case>o<fixed-case>TD</fixed-case>: Chain-of-Thought Distillation for Complex Reasoning Tasks with Mixture of Decoupled <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-Experts</title>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Jiayu</first><last>Wu</last></author>
      <author><first>Zhao</first><last>Yang</last></author>
      <author><first>Yao</first><last>Xu</last></author>
      <author><first>Yang jun</first><last>Jun</last></author>
      <author><first>Haifeng</first><last>Liu</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>11475–11485</pages>
      <abstract>Chain-of-thought Distillation (CoTD) aims at distilling Chain-of-thought (CoT) reasoning ability of large language models (LLMs) to much smaller student models. The core of CoTD is using a large teacher model to generate rationales and fine-tune smaller student models. However, current Chain-of-thought Distillation works have the following limitations: 1) Student models are separately distilled from specific reasoning tasks and lack a collaboration mechanism, hindering the enhancement of reasoning performance through collaboration among various reasoning tasks. 2) The parameter update of student models severely harms the CoT reasoning ability on other unseen reasoning tasks not included in the distillation process. In this work, we introduce a novel CoT Distillation method, MoDE-CoTD, which decouples the CoT reasoning abilities out of the student model by distilling multiple LoRA-Experts and freezing the parameters of the student model. Sequentially, LoRA-Experts are combined and adapted to handle both seen and unseen reasoning tasks, enabling collaboration among diverse reasoning tasks to further enhance CoT reasoning performance. Experimental results on 14 datasets (including 4 unseen datasets) demonstrate the strength of MoDE-CoTD, with an average accuracy gain of 6.3% on seen datasets and 7.8% on unseen datasets.</abstract>
      <url hash="cce39167">2024.lrec-main.1003</url>
      <bibkey>li-etal-2024-mode-cotd</bibkey>
    </paper>
    <paper id="1004">
      <title>Model-Agnostic Cross-Lingual Training for Discourse Representation Structure Parsing</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <pages>11486–11497</pages>
      <abstract>Discourse Representation Structure (DRS) is an innovative semantic representation designed to capture the meaning of texts with arbitrary lengths across languages. The semantic representation parsing is essential for achieving natural language understanding through logical forms. Nevertheless, the performance of DRS parsing models remains constrained when trained exclusively on monolingual data. To tackle this issue, we introduce a cross-lingual training strategy. The proposed method is model-agnostic yet highly effective. It leverages cross-lingual training data and fully exploits the alignments between languages encoded in pre-trained language models. The experiments conducted on the standard benchmarks demonstrate that models trained using the cross-lingual training method exhibit significant improvements in DRS clause and graph parsing in English, German, Italian and Dutch. Comparing our final models to previous works, we achieve state-of-the-art results in the standard benchmarks. Furthermore, the detailed analysis provides deep insights into the performance of the parsers, offering inspiration for future research in DRS parsing.</abstract>
      <url hash="ef536ed9">2024.lrec-main.1004</url>
      <bibkey>liu-2024-model-agnostic</bibkey>
    </paper>
    <paper id="1005">
      <title>Modeling Low-Resource Health Coaching Dialogues via Neuro-Symbolic Goal Summarization and Text-Units-Text Generation</title>
      <author><first>Yue</first><last>Zhou</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <author><first>Brian</first><last>Ziebart</last></author>
      <author><first>Lisa</first><last>Sharp</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Nikolaos</first><last>Agadakos</last></author>
      <pages>11498–11509</pages>
      <abstract>Health coaching helps patients achieve personalized and lifestyle-related goals, effectively managing chronic conditions and alleviating mental health issues. It is particularly beneficial, however cost-prohibitive, for low-socioeconomic status populations due to its highly personalized and labor-intensive nature. In this paper, we propose a neuro-symbolic goal summarizer to support health coaches in keeping track of the goals and a text-units-text dialogue generation model that converses with patients and helps them create and accomplish specific goals for physical activities. Our models outperform previous state-of-the-art while eliminating the need for predefined schema and corresponding annotation. We also propose a new health coaching dataset extending previous work and a metric to measure the unconventionality of the patient’s response based on data difficulty, facilitating potential coach alerts during deployment.</abstract>
      <url hash="5c842321">2024.lrec-main.1005</url>
      <bibkey>zhou-etal-2024-modeling-low</bibkey>
    </paper>
    <paper id="1006">
      <title>Modeling Orthographic Variation Improves <fixed-case>NLP</fixed-case> Performance for <fixed-case>N</fixed-case>igerian <fixed-case>P</fixed-case>idgin</title>
      <author><first>Pin-Jie</first><last>Lin</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Muhammed</first><last>Saeed</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>11510–11522</pages>
      <abstract>Nigerian Pidgin is an English-derived contact language and is traditionally an oral language, spoken by approximately 100 million people. No orthographic standard has yet been adopted, and thus the few available Pidgin datasets that exist are characterised by noise in the form of orthographic variations. This contributes to under-performance of models in critical NLP tasks. The current work is the first to describe various types of orthographic variations commonly found in Nigerian Pidgin texts, and model this orthographic variation. The variations identified in the dataset form the basis of a phonetic-theoretic framework for word editing, which is used to generate orthographic variations to augment training data. We test the effect of this data augmentation on two critical NLP tasks: machine translation and sentiment analysis. The proposed variation generation framework augments the training data with new orthographic variants which are relevant for the test set but did not occur in the training set originally. Our results demonstrate the positive effect of augmenting the training data with a combination of real texts from other corpora as well as synthesized orthographic variation, resulting in performance improvements of 2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.</abstract>
      <url hash="a0b2a9ca">2024.lrec-main.1006</url>
      <bibkey>lin-etal-2024-modeling-orthographic</bibkey>
    </paper>
    <paper id="1007">
      <title>Modeling the Quality of Dialogical Explanations</title>
      <author><first>Milad</first><last>Alshomary</last></author>
      <author><first>Felix</first><last>Lange</last></author>
      <author><first>Meisam</first><last>Booshehri</last></author>
      <author><first>Meghdut</first><last>Sengupta</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>11523–11536</pages>
      <abstract>Explanations are pervasive in our lives. Mostly, they occur in dialogical form where an <i>explainer</i> discusses a concept or phenomenon of interest with an <i>explainee</i>. Leaving the explainee with a clear understanding is not straightforward due to the knowledge gap between the two participants. Previous research looked at the interaction of explanation moves, dialogue acts, and topics in successful dialogues with expert explainers. However, daily-life explanations often fail, raising the question of what makes a dialogue successful. In this work, we study explanation dialogues in terms of the interactions between the explainer and explainee and how they correlate with the quality of explanations in terms of a successful understanding on the explainee’s side. In particular, we first construct a corpus of 399 dialogues from the Reddit forum <i>Explain Like I am Five</i> and annotate it for interaction flows and explanation quality. We then analyze the interaction flows, comparing them to those appearing in expert dialogues. Finally, we encode the interaction flows using two language models that can handle long inputs, and we provide empirical evidence for the effectiveness boost gained through the encoding in predicting the success of explanation dialogues.</abstract>
      <url hash="58fd3c9c">2024.lrec-main.1007</url>
      <bibkey>alshomary-etal-2024-modeling-quality</bibkey>
    </paper>
    <paper id="1008">
      <title>Modelling and Linking an Old <fixed-case>L</fixed-case>atin-<fixed-case>P</fixed-case>ortuguese Dictionary to the <fixed-case>L</fixed-case>i<fixed-case>L</fixed-case>a Knowledge Base</title>
      <author><first>Lucas Consolin</first><last>Dezotti</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Francesco</first><last>Mambrini</last></author>
      <pages>11537–11547</pages>
      <abstract>This paper describes the steps undertaken to include data from Antonio Velez’s bilingual Latin-Portuguese dictionary (Index Totius Artis, 1744) into the LiLa Knowledge Base of interoperable linguistic resources for Latin. The paper focuses on how the lexical and lexicographic information of the source dictionary was modelled by using respectively the Lexicon Model for Ontologies (OntoLex-lemon) and its lexicog module. The linking process of the dictionary entries with those of the LiLa collection of Latin lemmas is detailed, discussing issues in dealing with ambiguities and typographical errors found in the source. The result is the first Latin-Portuguese lexical resource made interoperable with the (meta)data of the other linguistic resources for Latin interlinked in the LiLa Knowledge Base, providing new ways of assessing the dictionary information or using its content as starting point to explore the connections with other interlinked linguistic resources. A couple of use case scenarios illustrate those possibilities.</abstract>
      <url hash="b8b19262">2024.lrec-main.1008</url>
      <bibkey>dezotti-etal-2024-modelling-linking</bibkey>
    </paper>
    <paper id="1009">
      <title>Modelling Argumentation for an User Opinion Aggregation Tool</title>
      <author><first>Pablo</first><last>Weingart</last></author>
      <author><first>Thiemo</first><last>Wambsganss</last></author>
      <author><first>Matthias</first><last>Soellner</last></author>
      <pages>11548–11559</pages>
      <abstract>We introduce an argumentation annotation scheme that models basic argumentative structure and additional contextual details across diverse user opinion domains. Drawing from established argumentation modeling approaches and related theory on user opinions, the scheme integrates the concepts of argumentative components, specificity, sentiment and aspects of the user opinion domain. Our freely available dataset includes 1,016 user opinions with 7,266 sentences, spanning products from 19 e-commerce categories, restaurants, hotels, local services, and mobile applications. Utilizing the dataset, we trained three transformer-based models, demonstrating their efficacy in predicting the annotated classes for identifying argumentative statements and contextual details from user opinion documents. Finally, we evaluate a prototypical dashboard that integrates the model inferences to aggregate information and rank exemplary products based on a vast array of user opinions. Early results from an experimental evaluation with eighteen users include positive user perceptions but also highlight challenges when condensing detailed argumentative information to users.</abstract>
      <url hash="ea53bcc2">2024.lrec-main.1009</url>
      <bibkey>weingart-etal-2024-modelling-argumentation</bibkey>
    </paper>
    <paper id="1010">
      <title><fixed-case>M</fixed-case>o<fixed-case>NMT</fixed-case>: Modularly Leveraging Monolingual and Bilingual Knowledge for Neural Machine Translation</title>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <pages>11560–11573</pages>
      <abstract>The effective use of monolingual and bilingual knowledge represents a critical challenge within the neural machine translation (NMT) community. In this paper, we propose a modular strategy that facilitates the cooperation of these two types of knowledge in translation tasks, while avoiding the issue of catastrophic forgetting and exhibiting superior model generalization and robustness. Our model is comprised of three functionally independent modules: an encoding module, a decoding module, and a transferring module. The former two acquire large-scale monolingual knowledge via self-supervised learning, while the latter is trained on parallel data and responsible for transferring latent features between the encoding and decoding modules. Extensive experiments in multi-domain translation tasks indicate our model yields remarkable performance, with up to 7 BLEU improvements in out-of-domain tests over the conventional pretrain-and-finetune approach. Our codes are available at https://github.com/NLP2CT/MoNMT.</abstract>
      <url hash="d9f6b818">2024.lrec-main.1010</url>
      <bibkey>pang-etal-2024-monmt-modularly</bibkey>
    </paper>
    <paper id="1011">
      <title>Monolingual Paraphrase Detection Corpus for Low Resource <fixed-case>P</fixed-case>ashto Language at Sentence Level</title>
      <author><first>Iqra</first><last>Ali</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>11574–11581</pages>
      <abstract>Paraphrase detection is a task to identify if two sentences are semantically similar or not. It plays an important role in maintaining the integrity of written work such as plagiarism detection and text reuse detection. Formerly, researchers focused on developing large corpora for English. However, no research has been conducted on sentence-level paraphrase detection in low-resource Pashto language. To bridge this gap, we introduce the first fully manually annotated Pashto sentential paraphrase detection corpus collected from authentic cases in journalism covering 10 different domains, including Sports, Health, Environment, and more. Our proposed corpus contains 6,727 sentences, encompassing 3,687 paraphrased and 3,040 non-paraphrased. Experimental findings reveal that our proposed corpus is sufficient to train XLM-RoBERTa to accurately detect paraphrased sentence pairs in Pashto with an F1 score of 84%. To compare our corpus with those in other languages, we also applied our fine-tuned model to the Indonesian and English paraphrase datasets in a zero-shot manner, achieving F1 scores of 82% and 78%, respectively. This result indicates that the quality of our corpus is not less than commonly used datasets. It‘s a pioneering contribution to the field. We will publicize a subset of 1,800 instances from our corpus, free from any licensing issues.</abstract>
      <url hash="1ae82584">2024.lrec-main.1011</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f3c0db9d">2024.lrec-main.1011.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>ali-etal-2024-monolingual-paraphrase</bibkey>
    </paper>
    <paper id="1012">
      <title><fixed-case>M</fixed-case>o<fixed-case>PE</fixed-case>: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking</title>
      <author><first>Tianwen</first><last>Tang</last></author>
      <author><first>Tong</first><last>Zhu</last></author>
      <author><first>Haodong</first><last>Liu</last></author>
      <author><first>Yin</first><last>Bai</last></author>
      <author><first>Jia</first><last>Cheng</last></author>
      <author><first>Wenliang</first><last>Chen</last></author>
      <pages>11582–11592</pages>
      <abstract>Zero-shot dialogue state tracking (DST) transfers knowledge to unseen domains, reducing the cost of annotating new datasets. Previous zero-shot DST models mainly suffer from domain transferring and partial prediction problems. To address these challenges, we propose Mixture of Prefix Experts (MoPE) to establish connections between similar slots in different domains, which strengthens the model transfer performance in unseen domains. Empirical results demonstrate that MoPE-DST achieves the joint goal accuracy of 57.13% on MultiWOZ2.1 and 55.4.</abstract>
      <url hash="e7153e4b">2024.lrec-main.1012</url>
      <bibkey>tang-etal-2024-mope-mixture</bibkey>
    </paper>
    <paper id="1013">
      <title><fixed-case>MORE</fixed-case>-3<fixed-case>S</fixed-case>:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces</title>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Xingwei</first><last>Qu</last></author>
      <author><first>Ming</first><last>Kuang</last></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Zhaofeng</first><last>He</last></author>
      <pages>11593–11604</pages>
      <abstract>Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily, we propose a new perspective to the offline reinforcement learning (RL) challenge. More concretely, we transform it into a supervised learning task by integrating multimodal and pre-trained language models. Our approach incorporates state information derived from images and action-related data obtained from text, thereby bolstering RL training performance and promoting long-term strategic thinking. We emphasize the contextual understanding of language and demonstrate how decision-making in RL can benefit from aligning states’ and actions’ representation with languages’ representation. Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments. This contributes to advancing offline RL performance and efficiency while providing a novel perspective on offline RL.</abstract>
      <url hash="9518306b">2024.lrec-main.1013</url>
      <bibkey>zheng-etal-2024-3s-multimodal</bibkey>
    </paper>
    <paper id="1014">
      <title>Morpheme Sense Disambiguation: A New Task Aiming for Understanding the Language at Character Level</title>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Hua</first><last>Zheng</last></author>
      <author><first>Yaqi</first><last>Yin</last></author>
      <author><first>Hansi</first><last>Wang</last></author>
      <author><first>Qiliang</first><last>Liang</last></author>
      <author id="yang-liu-pk"><first>Yang</first><last>Liu</last></author>
      <pages>11605–11618</pages>
      <abstract>Morphemes serve as a strong linguistic feature to capture lexical semantics, with higher coverage than words and more natural than sememes. However, due to the lack of morpheme-informed resources and the expense of manual annotation, morpheme-enhanced methods remain largely unexplored in Computational Linguistics. To address this issue, we propose the task of Morpheme Sense Disambiguation (MSD), with two subtasks in-text and in-word, similar to Word Sense Disambiguation (WSD) and Sememe Prediction (SP), to generalize morpheme features on more tasks. We first build the MorDis resource for Chinese, including MorInv as a morpheme inventory, MorTxt and MorWrd as two types of morpheme-annotated datasets. Next, we provide two baselines in each evaluation; the best model yields a promising precision of 77.66% on in-text MSD and 88.19% on in-word MSD, indicating its comparability with WSD and superiority over SP. Finally, we demonstrate that predicted morphemes achieve comparable performance with the ground-truth ones on a downstream application of Definition Generation (DG). This validates the feasibility and applicability of our proposed tasks. The resources and workflow of MSD will provide new insights and solutions for downstream tasks, including DG as well as WSD, training pre-trained models, etc.</abstract>
      <url hash="bb654e84">2024.lrec-main.1014</url>
      <bibkey>wang-etal-2024-morpheme-sense</bibkey>
    </paper>
    <paper id="1015">
      <title>Motion Capture Analysis of Verb and Adjective Types in <fixed-case>A</fixed-case>ustrian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage (<fixed-case>ÖGS</fixed-case>)</title>
      <author><first>Julia</first><last>Krebs</last></author>
      <author><first>Evguenia A.</first><last>Malaia</last></author>
      <author><first>Isabella</first><last>Fessl</last></author>
      <author><first>Hans-Peter</first><last>Wiesinger</last></author>
      <author><first>Dietmar</first><last>Roehm</last></author>
      <author><first>Ronnie</first><last>Wilbur</last></author>
      <author><first>Hermann</first><last>Schwameder</last></author>
      <pages>11619–11624</pages>
      <abstract>Across a number of sign languages, temporal and spatial characteristics of dominant hand articulation are used to express semantic and grammatical features. In this study of Austrian Sign Language (Österreichische Gebärdensprache, or ÖGS), motion capture data of four Deaf signers is used to quantitatively characterize the kinematic parameters of sign production in verbs and adjectives. We investigate (1) the difference in production between verbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking an endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in intensified vs. non-intensified (plain) forms. Motion capture data analysis using linear-mixed effects models (LME) indicates that both the endpoint marking in verbs, as well as marking of intensification in adjectives, are expressed by movement modulation in ÖGS. While the semantic distinction between verb types (telic/atelic) is marked by higher peak velocity and shorter duration for telic signs compared to atelic ones, the grammatical distinction (intensification) in adjectives is expressed by longer duration for intensified compared to non-intensified adjectives. The observed individual differences of signers might be interpreted as personal signing style.</abstract>
      <url hash="ae6bc8e0">2024.lrec-main.1015</url>
      <bibkey>krebs-etal-2024-motion-capture</bibkey>
    </paper>
    <paper id="1016">
      <title>Motion Generation from Fine-grained Textual Descriptions</title>
      <author><first>Kunhang</first><last>Li</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <pages>11625–11641</pages>
      <abstract>The task of **text2motion** is to generate human motion sequences from given textual descriptions, where the model explores diverse mappings from natural language instructions to human body movements. While most existing works are confined to coarse-grained motion descriptions, e.g., _”A man squats.”_, fine-grained descriptions specifying movements of relevant body parts are barely explored. Models trained with coarse-grained texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure to generate motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset specializing in fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with step-by-step instructions with pseudo-code compulsory checks. Accordingly, we design a new text2motion model, FineMotionDiffuse, making full use of fine-grained textual information. Our quantitative evaluation shows that FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of 0.38, compared with competitive baselines. According to the qualitative evaluation and case study, our model outperforms MotionDiffuse in generating spatially or chronologically composite motions, by learning the implicit mappings from fine-grained descriptions to the corresponding basic motions. We release our data at [https://github.com/KunhangL/finemotiondiffuse](https://github.com/KunhangL/finemotiondiffuse).</abstract>
      <url hash="f259727e">2024.lrec-main.1016</url>
      <bibkey>li-feng-2024-motion-generation</bibkey>
    </paper>
    <paper id="1017">
      <title>Motivational Interviewing Transcripts Annotated with Global Scores</title>
      <author><first>Ben</first><last>Cohen</last></author>
      <author><first>Moreah</first><last>Zisquit</last></author>
      <author><first>Stav</first><last>Yosef</last></author>
      <author><first>Doron</first><last>Friedman</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>11642–11657</pages>
      <abstract>Motivational interviewing (MI) is a counseling approach that aims to increase intrinsic motivation and commitment to change. Despite its effectiveness in various disorders such as addiction, weight loss, and smoking cessation, publicly available annotated MI datasets are scarce, limiting the development and evaluation of MI language generation models. We present MI-TAGS, a new annotated dataset of MI therapy sessions written in English collected from video recordings available on public sources. The dataset includes 242 MI demonstration transcripts annotated with the MI Treatment Integrity (MITI) 4.2 therapist behavioral codes and global scores, and Client Language EAsy Rating (CLEAR) 1.0 tags for client speech. In this paper we describe the process of data collection, transcription, and annotation, and provide an analysis of the new dataset. Additionally, we explore the potential use of the dataset for training language models to perform several MITI classification tasks; our results suggest that models may be able to automatically provide utterance-level annotation as well as global scores, with performance comparable to human annotators.</abstract>
      <url hash="c17853b9">2024.lrec-main.1017</url>
      <bibkey>cohen-etal-2024-motivational-interviewing</bibkey>
    </paper>
    <paper id="1018">
      <title><fixed-case>M</fixed-case>o<fixed-case>ZIP</fixed-case>: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property</title>
      <author><first>Shiwen</first><last>Ni</last></author>
      <author><first>Minghuan</first><last>Tan</last></author>
      <author><first>Yuelin</first><last>Bai</last></author>
      <author><first>Fuqiang</first><last>Niu</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Bowen</first><last>Zhang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Xiaojun</first><last>Chen</last></author>
      <author><first>Chengming</first><last>Li</last></author>
      <author><first>Xiping</first><last>Hu</last></author>
      <pages>11658–11668</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with ChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level. Our source code, data, and models are available at <url>https://github.com/AI-for-Science/MoZi</url>.</abstract>
      <url hash="8d098ce9">2024.lrec-main.1018</url>
      <bibkey>ni-etal-2024-mozip-multilingual</bibkey>
    </paper>
    <paper id="1019">
      <title><fixed-case>MRC</fixed-case>-based Nested Medical <fixed-case>NER</fixed-case> with Co-prediction and Adaptive Pre-training</title>
      <author><first>Xiaojing</first><last>Du</last></author>
      <author><first>Hanjie</first><last>Zhao</last></author>
      <author><first>Danyan</first><last>Xing</last></author>
      <author><first>Yuxiang</first><last>Jia</last></author>
      <author><first>Hongying</first><last>Zan</last></author>
      <pages>11669–11679</pages>
      <abstract>In medical information extraction, medical Named Entity Recognition (NER) is indispensable, playing a crucial role in developing medical knowledge graphs, enhancing medical question-answering systems, and analyzing electronic medical records. The challenge in medical NER arises from the complex nested structures and sophisticated medical terminologies, distinguishing it from its counterparts in traditional domains. In response to these complexities, we propose a medical NER model based on Machine Reading Comprehension (MRC), which uses a task-adaptive pre-training strategy to improve the model’s capability in the medical field. Meanwhile, our model introduces multiple word-pair embeddings and multi-granularity dilated convolution to enhance the model’s representation ability and uses a combined predictor of Biaffine and MLP to improve the model’s recognition performance. Experimental evaluations conducted on the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our proposed model outperforms the compared state-of-the-art (SOTA) models.</abstract>
      <url hash="ee14af5e">2024.lrec-main.1019</url>
      <bibkey>du-etal-2024-mrc-based</bibkey>
    </paper>
    <paper id="1020">
      <title><fixed-case>MRT</fixed-case>: Multi-modal Short- and Long-range Temporal Convolutional Network for Time-sync Comment Video Behavior Prediction</title>
      <author><first>Weihao</first><last>Zhao</last></author>
      <author><first>Weidong</first><last>He</last></author>
      <author><first>Hao</first><last>Wang</last></author>
      <author><first>Haoyang</first><last>Bi</last></author>
      <author><first>Han</first><last>Wu</last></author>
      <author><first>Chen</first><last>Zhu</last></author>
      <author><first>Tong</first><last>Xu</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>11680–11691</pages>
      <abstract>As a fresh way to improve the user viewing experience, videos of time-sync comments have attracted a lot of interest. Many efforts have been made to explore the effectiveness of time-sync comments for various applications. However, due to the complexity of interactions among users, videos, and comments, it still remains challenging to understand users’ behavior on time-sync comments. Along this line, we study the problem of time-sync comment behavior prediction with considerations of both historical behaviors and multi-modal information of visual frames and textual comments. Specifically, we propose a novel Multi-modal short- and long-Range Temporal Convolutional Network model, namely MRT. Firstly, we design two amplified Temporal Convolutional Networks with different sizes of receptive fields, to capture both short- and long-range surrounding contexts for each frame and time-sync comments. Then, we design a bottle-neck fusion module to obtain the multi-modal enhanced representation. Furthermore, we take the user preferences into consideration to generate the personalized multi-model semantic representation at each timestamp. Finally, we utilize the binary cross-entropy loss to optimize MRT on the basis of users’ historical records. Through comparing with representative baselines, we demonstrate the effectiveness of MRT and qualitatively verify the necessity and utility of short- and long-range contextual and multi-modal information through extensive experiments.</abstract>
      <url hash="1eb9d5f4">2024.lrec-main.1020</url>
      <bibkey>zhao-etal-2024-mrt-multi</bibkey>
    </paper>
    <paper id="1021">
      <title><fixed-case>MUCH</fixed-case>: A Multimodal Corpus Construction for Conversational Humor Recognition Based on <fixed-case>C</fixed-case>hinese Sitcom</title>
      <author><first>Hongyu</first><last>Guo</last></author>
      <author><first>Wenbo</first><last>Shang</last></author>
      <author><first>Xueyao</first><last>Zhang</last></author>
      <author><first>Shubo</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Binyang</first><last>Li</last></author>
      <pages>11692–11698</pages>
      <abstract>Conversational humor is the key to capturing dialogue semantics and dialogue comprehension, which is usually generated in multiple modalities, such as linguistic rhetoric (textual modality), exaggerated facial expressions or movements (visual modality), and quirky intonation (acoustic modality). However, existing multimodal corpora for conversation humor are coarse-grained, and the modality is insufficient to support the conversational humor recognition task. This paper designed an annotation scheme for multimodal humor datasets, and constructed a corpus based on a Chinese sitcom for conversational humor recognition, named MUCH. The MUCH corpus consists of 34,804 utterances in total, and 7,079 of them are humorous. We employed both unimodal and multimodal methods to test our MUCH corpus. Experimental results showed that the multimodal approach could achieve 75.94% in terms of F1-score and surpassed the performance of most unimodal methods, which demonstrated that the MUCH corpus was effective for multimodal humor recognition tasks.</abstract>
      <url hash="8c7ccb83">2024.lrec-main.1021</url>
      <bibkey>guo-etal-2024-much-multimodal</bibkey>
      <revision id="1" href="2024.lrec-main.1021v1" hash="22183877"/>
      <revision id="2" href="2024.lrec-main.1021v2" hash="8c7ccb83" date="2024-06-10">This revision corrected the author list.</revision>
    </paper>
    <paper id="1022">
      <title>Multi-Channel Spatio-Temporal Transformer for Sign Language Production</title>
      <author><first>Xiaohan</first><last>Ma</last></author>
      <author><first>Rize</first><last>Jin</last></author>
      <author><first>Tae-Sun</first><last>Chung</last></author>
      <pages>11699–11712</pages>
      <abstract>The task of Sign Language Production (SLP) in machine learning involves converting text-based spoken language into corresponding sign language expressions. Sign language conveys meaning through the continuous movement of multiple articulators, including manual and non-manual channels. However, most current Transformer-based SLP models convert these multi-channel sign poses into a unified feature representation, ignoring the inherent structural correlations between channels. This paper introduces a novel approach called MCST-Transformer for skeletal sign language production. It employs multi-channel spatial attention to capture correlations across various channels within each frame, and temporal attention to learn sequential dependencies for each channel over time. Additionally, the paper explores and experiments with multiple fusion techniques to combine the spatial and temporal representations into naturalistic sign sequences. To validate the effectiveness of the proposed MCST-Transformer model and its constituent components, extensive experiments were conducted on two benchmark sign language datasets from diverse cultures. The results demonstrate that this new approach outperforms state-of-the-art models on both datasets.</abstract>
      <url hash="9489cfe9">2024.lrec-main.1022</url>
      <bibkey>ma-etal-2024-multi-channel</bibkey>
    </paper>
    <paper id="1023">
      <title><fixed-case>MULTICOLLAB</fixed-case>: A Multimodal Corpus of Dialogues for Analyzing Collaboration and Frustration in Language</title>
      <author><first>Michael</first><last>Peechatt</last></author>
      <author><first>Cecilia Ovesdotter</first><last>Alm</last></author>
      <author><first>Reynold</first><last>Bailey</last></author>
      <pages>11713–11722</pages>
      <abstract>This paper addresses an existing resource gap for studying complex emotional states when a speaker collaborates with a partner to solve a task. We present a novel dialogue resource — the MULTICOLLAB corpus — where two interlocutors, an instructor and builder, communicated through a Zoom call while sensors recorded eye gaze, facial action units, and galvanic skin response, with transcribed speech signals, resulting in a unique, heavily multimodal corpus. The builder received instructions from the instructor. Half of the builders were privately told to disobey the instructor’s directions. After the task, participants watched the Zoom recording and annotated their instances of frustration. In this study, we introduce this new corpus and perform computational experiments with time series transformers, using early fusion through time for sensor data and late fusion for speech transcripts. We then average predictions from both methods to recognize instructor frustration. Using sensor and speech data in a 4.5 second time window, we find that the fusion of both models yields 21% improvement in classification accuracy (with a precision of 79% and F1 of 63%) over a comparison baseline, demonstrating that complex emotions can be recognized when rich multimodal data from transcribed spoken dialogue and biophysical sensor data are fused.</abstract>
      <url hash="8744f1c4">2024.lrec-main.1023</url>
      <bibkey>peechatt-etal-2024-multicollab-multimodal</bibkey>
    </paper>
    <paper id="1024">
      <title>Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for <fixed-case>K</fixed-case>orean</title>
      <author><first>Dojun</first><last>Park</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>11723–11744</pages>
      <abstract>Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy. Overall, RemBERT emerges as the most promising model. Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner.</abstract>
      <url hash="72994a7a">2024.lrec-main.1024</url>
      <bibkey>park-pado-2024-multi-dimensional</bibkey>
    </paper>
    <paper id="1025">
      <title>Multi-domain Hate Speech Detection Using Dual Contrastive Learning and Paralinguistic Features</title>
      <author><first>Somaiyeh</first><last>Dehghan</last></author>
      <author><first>Berrin</first><last>Yanıkoğlu</last></author>
      <pages>11745–11755</pages>
      <abstract>Social networks have become venues where people can share and spread hate speech, especially when the platforms allow users to remain anonymous. Hate speech can have significant social and cultural effects, especially when it targets specific groups of people in terms of religion, race, ethnicity, culture or a specific social situation such as immigrants and refugees. In this study, we propose a hate speech detection model, BERTurk-DualCL, using a mixed objective with contrastive learning loss that is combined with the traditional cross-entropy loss used for classification. In addition, we study the effects of paralinguistic features, namely emojis and hashtags, on the performance of our model. We trained and evaluated our model on tweets in four different topics with heated discussions from two separate datasets, ranging from discussions about migrants to the Israel-Palestine conflict. Our multi-domain model outperforms comparable results in literature and the average results of four domain-specific models, achieving a macro-F1 score of 81.04% and 58.89% on two- and five-class tasks respectively.</abstract>
      <url hash="373f9472">2024.lrec-main.1025</url>
      <bibkey>dehghan-yanikoglu-2024-multi-domain</bibkey>
    </paper>
    <paper id="1026">
      <title>Multi-Grained Conversational Graph Network for Retrieval-based Dialogue Systems</title>
      <author><first>Quan</first><last>Tu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>11756–11765</pages>
      <abstract>Retrieval-based dialogue agents aim at selecting a proper response according to multi-turn conversational history. Existing methods have achieved great progress in terms of retrieval accuracy on benchmarks with pre-trained language models. However, these methods simply concatenate all turns in the dialogue history as the input, ignoring the dialogue dependency and structural information between the utterances. Besides, they usually reason the relationship of the context-response pair at a single level of abstraction (e.g., utterance level), which can not comprehensively capture the fine-grained relation between the context and response. In this paper, we present the multi-grained conversational graph network (MCGN) that considers multiple levels of abstraction from dialogue histories and semantic dependencies within multi-turn dialogues for addressing. Evaluation results on two benchmarks indicate that the proposed multi-grained conversational graph network is helpful for dialogue context understanding and can bring consistent and significant improvement over the state-of-the-art methods.</abstract>
      <url hash="c6eebd37">2024.lrec-main.1026</url>
      <bibkey>tu-etal-2024-multi-grained</bibkey>
    </paper>
    <paper id="1027">
      <title>Multi-Granularity Fusion Text Semantic Matching Based on <fixed-case>W</fixed-case>o<fixed-case>BERT</fixed-case></title>
      <author><first>Hongchun</first><last>Yu</last></author>
      <author><first>Wei</first><last>Pan</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Hanqi</first><last>Li</last></author>
      <pages>11766–11775</pages>
      <abstract>Text semantic matching is crucial in natural language processing, applied in information retrieval, question answering, and recommendation systems. Traditional text-matching methods struggle with semantic nuances in short text. Recent advancements in multi-granularity representation learning have led to increased interest in improving text semantic matching models. We propose a novel multi-granularity fusion model that harnesses WoBERT, a pre-trained language model, to enhance the accuracy of text semantic information capture. Initially, we process text using WoBERT to acquire semantic representations, effectively capturing individual text semantic nuances. Next, we employ a soft attention alignment mechanism, enabling multi-granularity fusions among characters, words, and sentences, thus further improving matching performance. Our approach was evaluated through experiments on common Chinese short text matching datasets, BQ and LCQMC. Results reveal a significant improvement in performance compared to traditional methods, particularly in terms of accuracy.</abstract>
      <url hash="3ae10599">2024.lrec-main.1027</url>
      <bibkey>yu-etal-2024-multi-granularity</bibkey>
    </paper>
    <paper id="1028">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>eg: Dataset for Text Sanitisation in Less-resourced Languages</title>
      <author><first>Rinalds</first><last>Vīksna</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <pages>11776–11782</pages>
      <abstract>Text sanitization is the task of detecting and removing personal information from the text. While it has been well-studied in monolingual settings, today, there is also a need for multilingual text sanitization. In this paper, we introduce MultiLeg: a parallel, multilingual named entity (NE) dataset consisting of documents from the Court of Justice of the European Union annotated with semantic categories suitable for text sanitization. The dataset is available in 8 languages, and it contains 3082 parallel text segments for each language. We also show that the pseudonymized dataset remains useful for downstream tasks.</abstract>
      <url hash="6b15e2b3">2024.lrec-main.1028</url>
      <bibkey>viksna-skadina-2024-multileg-dataset</bibkey>
    </paper>
    <paper id="1029">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>ex<fixed-case>BATS</fixed-case>: Multilingual Dataset of Lexical Semantic Relations</title>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <author><first>Hugo</first><last>Goncalo Oliveira</last></author>
      <author><first>Lucia</first><last>Pitarch</last></author>
      <author><first>Elena-Simona</first><last>Apostol</last></author>
      <author><first>Jordi</first><last>Bernad</last></author>
      <author><first>Eliot</first><last>Bytyçi</last></author>
      <author><first>Chiara</first><last>Cantone</last></author>
      <author><first>Sara</first><last>Carvalho</last></author>
      <author><first>Francesca</first><last>Frontini</last></author>
      <author><first>Radovan</first><last>Garabik</last></author>
      <author><first>Jorge</first><last>Gracia</last></author>
      <author><first>Letizia</first><last>Granata</last></author>
      <author><first>Fahad</first><last>Khan</last></author>
      <author><first>Timotej</first><last>Knez</last></author>
      <author><first>Penny</first><last>Labropoulou</last></author>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Maria Pia</first><last>Di Buono</last></author>
      <author><first>Ana</first><last>Ostroški Anić</last></author>
      <author><first>Sigita</first><last>Rackevičienė</last></author>
      <author><first>Ricardo</first><last>Rodrigues</last></author>
      <author><first>Gilles</first><last>Sérasset</last></author>
      <author><first>Linas</first><last>Selmistraitis</last></author>
      <author><first>Mahammadou</first><last>Sidibé</last></author>
      <author><first>Purificação</first><last>Silvano</last></author>
      <author><first>Blerina</first><last>Spahiu</last></author>
      <author><first>Enriketa</first><last>Sogutlu</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Ciprian-Octavian</first><last>Truică</last></author>
      <author><first>Giedre</first><last>Valunaite Oleskeviciene</last></author>
      <author><first>Slavko</first><last>Zitnik</last></author>
      <author><first>Katerina</first><last>Zdravkova</last></author>
      <pages>11783–11793</pages>
      <abstract>Understanding the relation between the meanings of words is an important part of comprehending natural language. Prior work has either focused on analysing lexical semantic relations in word embeddings or probing pretrained language models (PLMs), with some exceptions. Given the rarity of highly multilingual benchmarks, it is unclear to what extent PLMs capture relational knowledge and are able to transfer it across languages. To start addressing this question, we propose MultiLexBATS, a multilingual parallel dataset of lexical semantic relations adapted from BATS in 15 languages including low-resource languages, such as Bambara, Lithuanian, and Albanian. As experiment on cross-lingual transfer of relational knowledge, we test the PLMs’ ability to (1) capture analogies across languages, and (2) predict translation targets. We find considerable differences across relation types and languages with a clear preference for hypernymy and antonymy as well as romance languages.</abstract>
      <url hash="42168bf3">2024.lrec-main.1029</url>
      <bibkey>gromann-etal-2024-multilexbats-multilingual</bibkey>
    </paper>
    <paper id="1030">
      <title>Multilingual Brain Surgeon: Large Language Models Can Be Compressed Leaving No Language behind</title>
      <author><first>Hongchuan</first><last>Zeng</last></author>
      <author><first>Hongshen</first><last>Xu</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>11794–11812</pages>
      <abstract>Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques. Keywords: Large Language Model, Multilingual Model Compression</abstract>
      <url hash="01d3efc6">2024.lrec-main.1030</url>
      <bibkey>zeng-etal-2024-multilingual-brain</bibkey>
    </paper>
    <paper id="1031">
      <title>Multilingual Coreference Resolution in Low-resource <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Ritwik</first><last>Mishra</last></author>
      <author><first>Pooja</first><last>Desur</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>11813–11826</pages>
      <abstract>Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on our test-split of Hindi golden set. This study is the first to evaluate an end-to-end coreference resolution model on a Hindi golden set. Furthermore, this work underscores the limitations of current coreference evaluation metrics when applied to datasets with split antecedents, advocating for the development of more suitable evaluation metrics.</abstract>
      <url hash="81ae76de">2024.lrec-main.1031</url>
      <bibkey>mishra-etal-2024-multilingual-coreference</bibkey>
    </paper>
    <paper id="1032">
      <title>Multilingual Generation in Abstractive Summarization: A Comparative Study</title>
      <author><first>Jinpeng</first><last>Li</last></author>
      <author><first>Jiaze</first><last>Chen</last></author>
      <author><first>Huadong</first><last>Chen</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>11827–11837</pages>
      <abstract>The emergence of pre-trained models marks a significant juncture for the multilingual generation, offering unprecedented capabilities to comprehend and produce text across multiple languages. These models display commendable efficiency in high-resource languages. However, their performance notably falters in low-resource languages due to the extensive linguistic diversity encountered. Moreover, the existing works lack thorough analysis impairs the discovery of effective multilingual strategies, further complicating the advancement of current multilingual generation systems. This paper aims to appraise the efficacy of multilingual generation tasks, with a focus on summarization, through three resource availability scenarios: high-resource, low-resource, and zero-shot. We classify multilingual generation methodologies into three foundational categories based on their underlying modeling principles: Fine-tuning, Parameter-isolation, and Constraint-based approaches. Following this classification, we conduct a comprehensive comparative study of these methodologies across different resource contexts using two datasets that span six languages. This analysis provides insights into the unique advantages and limitations of each method. In addition, we introduce an innovative yet simple automatic metric LANGM designed to mitigate the prevalent problem of spurious correlations associated with language mixing. LANGM accurately measures the degree of code-mixing at the language level. Finally, we highlight several challenges and suggest potential avenues for future inquiry, aiming to spur further advancements within the field of multilingual text generation.</abstract>
      <url hash="129ea0ec">2024.lrec-main.1032</url>
      <bibkey>li-etal-2024-multilingual-generation</bibkey>
    </paper>
    <paper id="1033">
      <title>Multilinguality or Back-translation? A Case Study with <fixed-case>E</fixed-case>stonian</title>
      <author><first>Elizaveta</first><last>Korotkova</last></author>
      <author><first>Taido</first><last>Purason</last></author>
      <author><first>Agnes</first><last>Luhtaru</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>11838–11848</pages>
      <abstract>Machine translation quality is highly reliant on large amounts of training data, and, when a limited amount of parallel data is available, synthetic back-translated or multilingual data can be used in addition. In this work, we introduce SynEst, a synthetic corpus of translations from 11 languages into Estonian which totals over 1 billion sentence pairs. Using this corpus, we investigate whether adding synthetic or English-centric additional data yields better translation quality for translation directions that do not include English. Our results show that while both strategies are effective, synthetic data gives better results. Our final models improve the performance of the baseline No Language Left Behind model while retaining its source-side multilinguality.</abstract>
      <url hash="b3d6c28b">2024.lrec-main.1033</url>
      <bibkey>korotkova-etal-2024-multilinguality-back</bibkey>
    </paper>
    <paper id="1034">
      <title>Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications</title>
      <author><first>Chihiro</first><last>Yano</last></author>
      <author><first>Akihiko</first><last>Fukuchi</last></author>
      <author><first>Shoko</first><last>Fukasawa</last></author>
      <author><first>Hideyuki</first><last>Tachibana</last></author>
      <author><first>Yotaro</first><last>Watanabe</last></author>
      <pages>11849–11858</pages>
      <abstract>Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent “exponential” growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model’s size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase. Our model is available at https://huggingface.co/pkshatech/m-ST5.</abstract>
      <url hash="e0aa6a2d">2024.lrec-main.1034</url>
      <bibkey>yano-etal-2024-multilingual-sentence</bibkey>
    </paper>
    <paper id="1035">
      <title>Multilingual Substitution-based Word Sense Induction</title>
      <author><first>Denis</first><last>Kokosinskii</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <pages>11859–11872</pages>
      <abstract>Word Sense Induction (WSI) is the task of discovering senses of an ambiguous word by grouping usages of this word into clusters corresponding to these senses. Many approaches were proposed to solve WSI in English and a few other languages, but these approaches are not easily adaptable to new languages. We present multilingual substitution-based WSI methods that support any of 100 languages covered by the underlying multilingual language model with minimal to no adaptation required. Despite the multilingual capabilities, our methods perform on par with the existing monolingual approaches on popular English WSI datasets. At the same time, they will be most useful for lower-resourced languages which miss lexical resources available for English, thus, have higher demand for unsupervised methods like WSI.</abstract>
      <url hash="0c7af297">2024.lrec-main.1035</url>
      <bibkey>kokosinskii-arefyev-2024-multilingual-substitution</bibkey>
    </paper>
    <paper id="1036">
      <title>Multilingual Turn-taking Prediction Using Voice Activity Projection</title>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Bing’er</first><last>Jiang</last></author>
      <author><first>Erik</first><last>Ekstedt</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <author><first>Gabriel</first><last>Skantze</last></author>
      <pages>11873–11883</pages>
      <abstract>This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).</abstract>
      <url hash="9659afc5">2024.lrec-main.1036</url>
      <bibkey>inoue-etal-2024-multilingual-turn</bibkey>
    </paper>
    <paper id="1037">
      <title>Multimodal and Multilingual Laughter Detection in Stand-Up Comedy Videos</title>
      <author><first>Anna</first><last>Kuznetsova</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>11884–11889</pages>
      <abstract>This paper presents the development of a novel multimodal multilingual dataset in Russian and English, with a particular emphasis on the exploration of laughter detection techniques. Data was collected from YouTube stand-up comedy videos with manually annotated subtitles, and our research covers data preparation and laughter labeling. We explore two laughter detection approaches presented in the literature: peak detection using preprocessed voiceless audio with an energy-based algorithm and machine learning approach with pretrained models to identify laughter presence and duration. While the machine learning approach currently outperforms peak detection in accuracy and generalization, the latter shows promise and warrants further study. Additionally, we explore unimodal and multimodal humor detection on the new dataset, showing the effectiveness of neural models in capturing humor in both languages, even with textual data. Multimodal experiments indicate that even basic models benefit from visual data, improving detection results. However, further research is needed to enhance laughter detection labeling quality and fully understand the impact of different modalities in a multimodal and multilingual context.</abstract>
      <url hash="37d0cd2b">2024.lrec-main.1037</url>
      <bibkey>kuznetsova-strapparava-2024-multimodal-multilingual</bibkey>
    </paper>
    <paper id="1038">
      <title>Multimodal Behaviour in an Online Environment: The <fixed-case>GEHM</fixed-case> Zoom Corpus Collection</title>
      <author><first>Patrizia</first><last>Paggio</last></author>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <author><first>Costanza</first><last>Navarretta</last></author>
      <author><first>Leo</first><last>Vitasovic</last></author>
      <pages>11890–11900</pages>
      <abstract>This paper introduces a novel multimodal corpus consisting of 12 video recordings of Zoom meetings held in English by an international group of researchers from September 2021 to March 2023. The meetings have an average duration of about 40 minutes each, for a total of 8 hours. The number of participants varies from 5 to 9 per meeting. The participants’ speech was transcribed automatically using WhisperX, while visual coordinates of several keypoints of the participants’ head, their shoulders and wrists, were extracted using OpenPose. The audio-visual recordings will be distributed together with the orthographic transcription as well as the visual coordinates. In the paper we describe the way the corpus was collected, transcribed and enriched with the visual coordinates, we give descriptive statistics concerning both the speech transcription and the visual keypoint values and we present and discuss visualisations of these values. Finally, we carry out a short preliminary analysis of the role of feedback in the meetings, and show how visualising the coordinates extracted via OpenPose can be used to see how gestural behaviour supports the use of feedback words during the interaction.</abstract>
      <url hash="7039db82">2024.lrec-main.1038</url>
      <bibkey>paggio-etal-2024-multimodal-behaviour</bibkey>
    </paper>
    <paper id="1039">
      <title>Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles</title>
      <author><first>Abhijnan</first><last>Nath</last></author>
      <author><first>Huma</first><last>Jamil</last></author>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last></author>
      <author><first>George Arthur</first><last>Baker</last></author>
      <author><first>Rahul</first><last>Ghosh</last></author>
      <author><first>James H.</first><last>Martin</last></author>
      <author><first>Nathaniel</first><last>Blanchard</last></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <pages>11901–11916</pages>
      <abstract>Event coreference resolution (ECR) is the task of determining whether distinct mentions of events within a multi-document corpus are actually linked to the same underlying occurrence. Images of the events can help facilitate resolution when language is ambiguous. Here, we propose a multimodal cross-document event coreference resolution method that integrates visual and textual cues with a simple linear map between vision and language models. As existing ECR benchmark datasets rarely provide images for all event mentions, we augment the popular ECB+ dataset with event-centric images scraped from the internet and generated using image diffusion models. We establish three methods that incorporate images and text for coreference: 1) a standard fused model with finetuning, 2) a novel linear mapping method without finetuning and 3) an ensembling approach based on splitting mention pairs by semantic and discourse-level difficulty. We evaluate on 2 datasets: the augmented ECB+, and AIDA Phase 1. Our ensemble systems using cross-modal linear mapping establish an upper limit (91.9 CoNLL F1) on ECB+ ECR performance given the preprocessing assumptions used, and establish a novel baseline on AIDA Phase 1. Our results demonstrate the utility of multimodal information in ECR for certain challenging coreference problems, and highlight a need for more multimodal resources in the coreference resolution space.</abstract>
      <url hash="e6d79682">2024.lrec-main.1039</url>
      <bibkey>nath-etal-2024-multimodal-cross</bibkey>
    </paper>
    <paper id="1040">
      <title>Multimodal Cross-lingual Phrase Retrieval</title>
      <author><first>Chuanqi</first><last>Dong</last></author>
      <author><first>Wenjie</first><last>Zhou</last></author>
      <author><first>Xiangyu</first><last>Duan</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>11917–11927</pages>
      <abstract>Cross-lingual phrase retrieval aims to retrieve parallel phrases among languages. Current approaches only deals with textual modality. There lacks multimodal data resources and explorations for multimodal cross-lingual phrase retrieval (MXPR). In this paper, we create the first MXPR data resource and propose a novel approach for MXPR to explore the effectiveness of multi-modality. The MXPR data resource is built by marrying the benchmark dataset for textual cross-lingual phrase retrieval with Wikimedia Commons, which is a media store containing tremendous texts and related images. In the built resource, the phrase pairs of the textual benchmark dataset are equipped with their related images. Based on this novel data resource, we introduce a strategy to bridge the gap between different modalities by multimodal relation generation with a large multimodal pre-trained model and consistency training. Experiments on benchmarked dataset covering eight language pairs show that our MXPR approach, which deals with multimodal phrases, performs significantly better than pure textual cross-lingual phrase retrieval.</abstract>
      <url hash="81ace146">2024.lrec-main.1040</url>
      <bibkey>dong-etal-2024-multimodal-cross</bibkey>
    </paper>
    <paper id="1041">
      <title>Multimodal Language Models Show Evidence of Embodied Simulation</title>
      <author><first>Cameron R.</first><last>Jones</last></author>
      <author><first>Sean</first><last>Trott</last></author>
      <pages>11928–11933</pages>
      <abstract>Multimodal large language models (MLLMs) are gaining popularity as partial solutions to the “symbol grounding problem” faced by language models trained on text alone. However, little is known about whether and how these multiple modalities are integrated. We draw inspiration from analogous work in human psycholinguistics on embodied simulation, i.e., the hypothesis that language comprehension is grounded in sensorimotor representations. We show that MLLMs are sensitive to implicit visual features like object shape (e.g., “The egg was in the skillet” implies a frying egg rather than one in a shell). This suggests that MLLMs activate implicit information about object shape when it is implied by a verbal description of an event. We find mixed results for color and orientation, and rule out the possibility that this is due to models’ insensitivity to those features in our dataset overall. We suggest that both human psycholinguistics and computational models of language could benefit from cross-pollination, e.g., with the potential to establish whether grounded representations play a functional role in language processing.</abstract>
      <url hash="1e35a1a8">2024.lrec-main.1041</url>
      <bibkey>jones-trott-2024-multimodal-language</bibkey>
    </paper>
    <paper id="1042">
      <title>Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment</title>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Ke</first><last>Chang</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>11934–11943</pages>
      <abstract>Multi-modal semantic understanding requires integrating information from different modalities to extract users’ real intention behind words. Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction. This paper proposes a novel CLIP-guided contrastive-learning-based architecture to perform multi-modal feature alignment, which projects the features derived from different modalities into a unified deep space. On multi-modal sarcasm detection (MMSD) and multi-modal sentiment analysis (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge. More importantly, our model is simple to implement without using task-specific external knowledge, and thus can easily migrate to other multi-modal tasks. Our source codes are available at https://github.com/ChangKe123/CLFA.</abstract>
      <url hash="38c791a4">2024.lrec-main.1042</url>
      <bibkey>zhang-etal-2024-multi-modal</bibkey>
    </paper>
    <paper id="1043">
      <title>Multi-Objective Forward Reasoning and Multi-Reward Backward Refinement for Product Review Summarization</title>
      <author><first>Libo</first><last>Sun</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Meng</first><last>Han</last></author>
      <author><first>Ruofei</first><last>Lai</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>11944–11955</pages>
      <abstract>Product review summarization aims to generate a concise summary based on product reviews to facilitate purchasing decisions. This intricate task gives rise to three challenges in existing work: factual accuracy, aspect comprehensiveness, and content relevance. In this paper, we first propose an FB-Thinker framework to improve the summarization ability of LLMs with multi-objective forward reasoning and multi-reward backward refinement. To enable LLM with these dual capabilities, we present two Chinese product review summarization datasets, Product-CSum and Product-CSum-Cross, for both instruction-tuning and cross-domain evaluation. Specifically, these datasets are collected via GPT-assisted manual annotations from an online forum and public datasets. We further design an evaluation mechanism Product-Eval, integrating both automatic and human evaluation across multiple dimensions for product summarization. Experimental results show the competitiveness and generalizability of our proposed framework in the product review summarization tasks.</abstract>
      <url hash="6754e0a8">2024.lrec-main.1043</url>
      <bibkey>sun-etal-2024-multi-objective</bibkey>
    </paper>
    <paper id="1044">
      <title>Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models</title>
      <author><first>Derong</first><last>Xu</last></author>
      <author><first>Ziheng</first><last>Zhang</last></author>
      <author><first>Zhenxi</first><last>Lin</last></author>
      <author><first>Xian</first><last>Wu</last></author>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Tong</first><last>Xu</last></author>
      <author><first>Xiangyu</first><last>Zhao</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>11956–11968</pages>
      <abstract>Knowledge graph completion (KGC) is a widely used method to tackle incompleteness in knowledge graphs (KGs) by making predictions for missing links. Description-based KGC leverages pre-trained language models to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. We conducted extensive evaluation of the effectiveness and improvement of our framework based on four description-based KGC models, for both link prediction and triplet classification tasks. All codes and generated data will be publicly available after review.</abstract>
      <url hash="a4ed2459">2024.lrec-main.1044</url>
      <bibkey>xu-etal-2024-multi-perspective</bibkey>
    </paper>
    <paper id="1045">
      <title>Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition</title>
      <author><first>Yash</first><last>Jain</last></author>
      <author><first>David M.</first><last>Chan</last></author>
      <author><first>Pranav</first><last>Dheram</last></author>
      <author><first>Aparna</first><last>Khare</last></author>
      <author><first>Olabanji</first><last>Shonibare</last></author>
      <author><first>Venkatesh</first><last>Ravichandran</last></author>
      <author><first>Shalini</first><last>Ghosh</last></author>
      <pages>11969–11980</pages>
      <abstract>Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.</abstract>
      <url hash="e71c12f1">2024.lrec-main.1045</url>
      <bibkey>jain-etal-2024-multi-stage</bibkey>
    </paper>
    <paper id="1046">
      <title>Multi-stream Information Fusion Framework for Emotional Support Conversation</title>
      <author><first>Yinan</first><last>Bao</last></author>
      <author><first>Dou</first><last>Hu</last></author>
      <author><first>Lingwei</first><last>Wei</last></author>
      <author><first>Shuchong</first><last>Wei</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>11981–11992</pages>
      <abstract>Emotional support conversation (ESC) task aims to relieve the emotional distress of users who have high-intensity of negative emotions. However, due to the ignorance of emotion intensity modelling which is essential for ESC, previous methods fail to capture the transition of emotion intensity effectively. To this end, we propose a Multi-stream information Fusion Framework (MFF-ESC) to thoroughly fuse three streams (text semantics stream, emotion intensity stream, and feedback stream) for the modelling of emotion intensity, based on a designed multi-stream fusion unit. As the difficulty of modelling subtle transitions of emotion intensity and the strong emotion intensity-feedback correlations, we use the KL divergence between feedback distribution and emotion intensity distribution to further guide the learning of emotion intensities. Experimental results on automatic and human evaluations indicate the effectiveness of our method.</abstract>
      <url hash="8676179a">2024.lrec-main.1046</url>
      <bibkey>bao-etal-2024-multi-stream</bibkey>
    </paper>
    <paper id="1047">
      <title>Multi-Tiered <fixed-case>C</fixed-case>antonese Word Segmentation</title>
      <author><first>Charles</first><last>Lam</last></author>
      <author><first>Chaak-ming</first><last>Lau</last></author>
      <author><first>Jackson L.</first><last>Lee</last></author>
      <pages>11993–12002</pages>
      <abstract>Word segmentation for Chinese text data is essential for compiling corpora and any other tasks where the notion of “word” is assumed, since Chinese orthography does not have conventional word boundaries as languages such as English do. A perennial issue, however, is that there is no consensus about the definition of “word” in Chinese, which makes word segmentation challenging. Recent work in Chinese word segmentation has begun to embrace the idea of multiple word segmentation possibilities. In a similar spirit, this paper focuses on Cantonese, another major Chinese variety. We propose a linguistically motivated, multi-tiered word segmentation system for Cantonese, and release a Cantonese corpus of 150,000 characters word-segmented by this proposal. Our work will be of interest to researchers whose work involves Cantonese corpus data.</abstract>
      <url hash="2ccf72e3">2024.lrec-main.1047</url>
      <bibkey>lam-etal-2024-multi-tiered</bibkey>
    </paper>
    <paper id="1048">
      <title>Murre24: Dialect Identification of <fixed-case>F</fixed-case>innish <fixed-case>I</fixed-case>nternet Forum Messages</title>
      <author><first>Olli</first><last>Kuparinen</last></author>
      <pages>12003–12015</pages>
      <abstract>This paper presents Murre24, a collection of dialectal messages posted on the largest Finnish internet forum, Suomi24. The messages posted in Finnish on the forum between 2001 and 2020 are classified to present either the standard language, one of the seven traditional dialects, a colloquial style or the Helsinki slang. We present a manually annotated dataset used to train dialect identification models as well as the automatic annotation of almost 94 million messages in total. We experiment with five different dialect identification methods and evaluate them on dialectally balanced and random test samples. The best performing method for differentiating standard Finnish from non-standard Finnish is a character n-gram based support vector machine (SVM), while fine-tuning a BERT-based model achieves best scores in the final dialect identification task. According to the automatic classification, most of the messages written on the forum are in standard Finnish, and most of the non-standard messages are in a colloquial variety used typically by young speakers in Finland. We moreover show that the proportion of non-standard messages declines over time, but the proportion of the traditional dialects stays relatively steady.</abstract>
      <url hash="0dd93ebe">2024.lrec-main.1048</url>
      <bibkey>kuparinen-2024-murre24-dialect</bibkey>
    </paper>
    <paper id="1049">
      <title><fixed-case>MVP</fixed-case>: Minimal Viable Phrase for Long Text Understanding</title>
      <author><first>Louis</first><last>Clouatre</last></author>
      <author><first>Amal</first><last>Zouaq</last></author>
      <author><first>Sarath</first><last>Chandar</last></author>
      <pages>12016–12026</pages>
      <abstract>A recent renewal in interest in long text understanding has sparked the emergence of high-quality long text benchmarks, as well as new models demonstrating significant performance improvements on these benchmarks. However, gauging the implication of these advancements based solely on the length of the input text offers limited insight. Such benchmarks may require models to parse long-range dependencies or merely to locate and comprehend the relevant paragraph within a longer text. This work introduces the Minimal Viable Phrase (MVP), a novel metric that determines, through perturbations to the input text, the shortest average text length that needs to be preserved to execute the task with limited performance degradation. Our evaluation of the popular SCROLLS benchmark reveals that only one of its seven tasks necessitates an MVP of over 512 tokens–the maximum text length manageable by the previous generation of pre-trained models. We highlight the limited need for understanding long-range dependencies in resolving these tasks, discuss the specific design decisions that seem to have led to the QuALITY task requiring reliance on long-range dependencies to be solved, and point out specific modeling choices that seem to outperform on the QuALITY task.</abstract>
      <url hash="b5cd6076">2024.lrec-main.1049</url>
      <bibkey>clouatre-etal-2024-mvp-minimal</bibkey>
    </paper>
    <paper id="1050">
      <title><fixed-case>MWE</fixed-case>-Finder: A Demonstration</title>
      <author><first>Jan</first><last>Odijk</last></author>
      <author><first>Martin</first><last>Kroon</last></author>
      <author><first>Tijmen</first><last>Baarda</last></author>
      <author><first>Ben</first><last>Bonfil</last></author>
      <author><first>Sheean</first><last>Spoel</last></author>
      <pages>12027–12031</pages>
      <abstract>This paper introduces and demonstrates MWE Finder, an application to search for flexible multiword expressions (MWEs) in Dutch text corpora, starting from an example. If the example is in canonical form, the application automatically generates three queries to search for sentences that contain an occurrence of the MWE and thus enables efficient analysis of its properties. Searching is done in treebanks, so the grammatical structure of the sentences is taken into account.</abstract>
      <url hash="c71a316d">2024.lrec-main.1050</url>
      <attachment type="OptionalSupplementaryMaterial" hash="204a98b7">2024.lrec-main.1050.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>odijk-etal-2024-mwe-finder</bibkey>
    </paper>
    <paper id="1051">
      <title>my<fixed-case>M</fixed-case>edi<fixed-case>C</fixed-case>on: End-to-End <fixed-case>B</fixed-case>urmese Automatic Speech Recognition for Medical Conversations</title>
      <author><first>Hay Man</first><last>Htun</last></author>
      <author><first>Ye</first><last>Kyaw Thu</last></author>
      <author><first>Hutchatai</first><last>Chanlekha</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <author><first>Thepchai</first><last>Supnithi</last></author>
      <pages>12032–12039</pages>
      <abstract>End-to-End Automatic Speech Recognition (ASR) models have significantly advanced the field of speech processing by streamlining traditionally complex ASR system pipelines, promising enhanced accuracy and efficiency. Despite these advancements, there is a notable absence of freely available medical conversation speech corpora for Burmese, which is one of the low-resource languages. Addressing this gap, we present a manually curated Burmese Medical Speech Conversations (myMediCon) corpus, encapsulating conversations among medical doctors, nurses, and patients. Utilizing the ESPnet speech processing toolkit, we explore End-to-End ASR models for the Burmese language, focus on Transformer and Recurrent Neural Network (RNN) architectures. Our corpus comprises 12 speakers, including three males and nine females, with a total speech duration of nearly 11 hours within the medical domain. To assess the ASR performance, we applied word and syllable segmentation to the text corpus. ASR models were evaluated using Character Error Rate (CER), Word Error Rate (WER), and Translation Error Rate (TER). The experimental results indicate that the RNN-based Burmese speech recognition with syllable-level segmentation achieved the best performance, yielding a CER of 9.7%. Moreover, the RNN approach significantly outperformed the Transformer model.</abstract>
      <url hash="b3e79975">2024.lrec-main.1051</url>
      <bibkey>htun-etal-2024-mymedicon-end</bibkey>
    </paper>
    <paper id="1052">
      <title>My Science Tutor (<fixed-case>M</fixed-case>y<fixed-case>ST</fixed-case>)–a Large Corpus of Children’s Conversational Speech</title>
      <author><first>Sameer</first><last>Pradhan</last></author>
      <author><first>Ronald A.</first><last>Cole</last></author>
      <author><first>Wayne H.</first><last>Ward</last></author>
      <pages>12040–12045</pages>
      <abstract>This article describes the [corpus-name] corpus developed as part of the [project-name] project. To the best of our knowledge, this is one of the largest collections of children’s conversational speech that is freely available for non-commercial use under the creative commons license (CC BY-NC-SA 4.0). It comprises approximately 400 hours of speech, spanning some 230K utterances spread across about 10,500 virtual tutor sessions. Roughly 1,300 third, fourth and fifth grade students contributed to this corpus. The current release contains roughly 100K transcribed utterances. It is our hope that the corpus can be used to improve automatic speech recognition models and algorithms. We report the word error rate achieved on the test set using a model trained on the training and development portion of the corpus. The git repository of the corpus contains the complete training and evaluation setup in order to facilitate a fair and consistent evaluation. It is our hope that this corpus will contribute to the creation and evaluation of conversational AI agents having a better understanding of children’s speech, potentially opening doors to novel, effective, learning and therapeutic interventions.</abstract>
      <url hash="d5ef7e5a">2024.lrec-main.1052</url>
      <bibkey>pradhan-etal-2024-science-tutor</bibkey>
    </paper>
    <paper id="1053">
      <title><fixed-case>NAIST</fixed-case>-<fixed-case>SIC</fixed-case>-Aligned: An Aligned <fixed-case>E</fixed-case>nglish-<fixed-case>J</fixed-case>apanese Simultaneous Interpretation Corpus</title>
      <author><first>Jinming</first><last>Zhao</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <author><first>Yuka</first><last>Ko</last></author>
      <author><first>Kosuke</first><last>Doi</last></author>
      <author><first>Ryo</first><last>Fukuda</last></author>
      <pages>12046–12052</pages>
      <abstract>It remains a question that how simultaneous interpretation (SI) data affects simultaneous machine translation (SiMT). Research has been limited due to the lack of a large-scale training corpus. In this work, we aim to fill in the gap by introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel English-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we propose a two-stage alignment approach to make the corpus parallel and thus suitable for model training. The first stage is coarse alignment where we perform a many-to-many mapping between source and target sentences, and the second stage is fine-grained alignment where we perform intra- and inter-sentence filtering to improve the quality of aligned pairs. To ensure the quality of the corpus, each step has been validated either quantitatively or qualitatively. This is the first open-sourced large-scale parallel SI dataset in the literature. We also manually curated a small test set for evaluation purposes. Our results show that models trained with SI data lead to significant improvement in translation quality and latency over baselines. We hope our work advances research on SI corpora construction and SiMT. Our data will be released upon the paper’s acceptance.</abstract>
      <url hash="0e1ca144">2024.lrec-main.1053</url>
      <bibkey>zhao-etal-2024-naist-sic</bibkey>
    </paper>
    <paper id="1054">
      <title><fixed-case>N</fixed-case>arrative<fixed-case>T</fixed-case>ime: Dense Temporal Annotation on a Timeline</title>
      <author><first>Anna</first><last>Rogers</last></author>
      <author><first>Marzena</first><last>Karpinska</last></author>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Vladislav</first><last>Lialin</last></author>
      <author><first>Gregory</first><last>Smelkov</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <pages>12053–12073</pages>
      <abstract>For the past decade, temporal annotation has been sparse: only a small portion of event pairs in a text was annotated. We present NarrativeTime, the first timeline-based annotation framework that achieves full coverage of all possible TLINKs. To compare with the previous SOTA in dense temporal annotation, we perform full re-annotation of the classic TimeBankDense corpus (American English), which shows comparable agreement with a signigicant increase in density. We contribute TimeBankNT corpus (with each text fully annotated by two expert annotators), extensive annotation guidelines, open-source tools for annotation and conversion to TimeML format, and baseline results.</abstract>
      <url hash="312ba40d">2024.lrec-main.1054</url>
      <bibkey>rogers-etal-2024-narrativetime-dense</bibkey>
    </paper>
    <paper id="1055">
      <title>Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science</title>
      <author><first>Yida</first><last>Mu</last></author>
      <author><first>Ben P.</first><last>Wu</last></author>
      <author><first>William</first><last>Thorne</last></author>
      <author><first>Ambrose</first><last>Robinson</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <author><first>Xingyi</first><last>Song</last></author>
      <pages>12074–12086</pages>
      <abstract>Instruction-tuned Large Language Models (LLMs) have exhibited impressive language understanding and the capacity to generate responses that follow specific prompts. However, due to the computational demands associated with training these models, their applications often adopt a zero-shot setting. In this paper, we evaluate the zero-shot performance of two publicly accessible LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social Science classification tasks, while also investigating the effects of various prompting strategies. Our experiments investigate the impact of prompt complexity, including the effect of incorporating label definitions into the prompt; use of synonyms for label names; and the influence of integrating past memories during foundation model training. The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large). Additionally, we find that different prompting strategies can significantly affect classification accuracy, with variations in accuracy and F1 scores exceeding 10%.</abstract>
      <url hash="236ca156">2024.lrec-main.1055</url>
      <bibkey>mu-etal-2024-navigating-prompt</bibkey>
    </paper>
    <paper id="1056">
      <title><fixed-case>NB</fixed-case> Uttale: A <fixed-case>N</fixed-case>orwegian Pronunciation Lexicon with Dialect Variation</title>
      <author><first>Marie Iversdatter</first><last>Røsok</last></author>
      <author><first>Ingerid Løyning</first><last>Dale</last></author>
      <pages>12087–12092</pages>
      <abstract>We present a Norwegian pronunciation lexicon with Bokmål orthographic word forms and up to eight alternate phonological transcriptions per word form. The lexicon covers dialectal variations for five geographical areas, as well as pronunciation variations for spontaneous and manuscript-read speech. It is based on the NST Bokmål lexicon for East Norwegian, whose original phonological transcriptions have been corrected, before they were converted with dialect specific regular expression rules. To evaluate the quality and consistency of the new, rule-generated transcriptions, we trained grapheme-to phoneme (G2P) models and report our results with word- (WER) and phoneme-error-rate (PER) metrics. We found that the G2P models trained on lexica for Southwest and West Norwegian close-to written transcriptions have the lowest WER scores, and that all error-corrected, close-to-written lexica yield better WER scores than the original NST lexicon. The lexicon is available under an open license, and can be used for various language technology applications and in linguistic research.</abstract>
      <url hash="36f62523">2024.lrec-main.1056</url>
      <bibkey>rosok-dale-2024-nb-uttale</bibkey>
    </paper>
    <paper id="1057">
      <title>Negation Scope Conversion: Towards a Unified Negation-Annotated Dataset</title>
      <author><first>Asahi</first><last>Yoshida</last></author>
      <author><first>Yoshihide</first><last>Kato</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <pages>12093–12099</pages>
      <abstract>Negation scope resolution is the task that identifies the part of a sentence affected by the negation cue. The three major corpora used for this task, the BioScope corpus, the SFU review corpus and the Sherlock dataset, have different annotation schemes for negation scope. Due to the different annotations, the negation scope resolution models based on pre-trained language models (PLMs) perform worse when fine-tuned on the simply combined dataset consisting of the three corpora. To address this issue, we propose a method for automatically converting the scopes of BioScope and SFU to those of Sherlock and merge them into a unified dataset. To verify the effectiveness of the proposed method, we conducted experiments using the unified dataset for fine-tuning PLM-based models. The experimental results demonstrate that the performances of the models increase when fine-tuned on the unified dataset unlike the simply combined one. In the token-level metric, the model fine-tuned on the unified dataset archived the state-of-the-art performance on the Sherlock dataset.</abstract>
      <url hash="e068c7c3">2024.lrec-main.1057</url>
      <bibkey>yoshida-etal-2024-negation-scope</bibkey>
    </paper>
    <paper id="1058">
      <title>Negation Triplet Extraction with Syntactic Dependency and Semantic Consistency</title>
      <author><first>Yuchen</first><last>Shi</last></author>
      <author><first>Deqing</first><last>Yang</last></author>
      <author><first>Jingping</first><last>Liu</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <author><first>Zongyu</first><last>Wang</last></author>
      <author><first>Huimin</first><last>Xu</last></author>
      <pages>12100–12110</pages>
      <abstract>Previous works of negation understanding mainly focus on negation cue detection and scope resolution, without identifying negation subject which is also significant to the downstream tasks. In this paper, we propose a new negation triplet extraction (NTE) task which aims to extract negation subject along with negation cue and scope. To achieve NTE, we devise a novel Syntax&amp;Semantic-Enhanced Negation Extraction model, namely SSENE, which is built based on a generative pretrained language model (PLM) of Encoder-Decoder architecture with a multi-task learning framework. Specifically, the given sentence’s syntactic dependency tree is incorporated into the PLM’s encoder to discover the correlations between the negation subject, cue and scope. Moreover, the semantic consistency between the sentence and the extracted triplet is ensured by an auxiliary task learning. Furthermore, we have constructed a high-quality Chinese dataset NegComment based on the users’ reviews from the real-world platform of Meituan, upon which our evaluations show that SSENE achieves the best NTE performance compared to the baselines. Our ablation and case studies also demonstrate that incorporating the syntactic information helps the PLM’s recognize the distant dependency between the subject and cue, and the auxiliary task learning is helpful to extract the negation triplets with more semantic consistency. We further demonstrate that SSENE is also competitive on the traditional CDSR task.</abstract>
      <url hash="fd99a984">2024.lrec-main.1058</url>
      <bibkey>shi-etal-2024-negation-triplet</bibkey>
    </paper>
    <paper id="1059">
      <title>n<fixed-case>EMO</fixed-case>: Dataset of Emotional Speech in <fixed-case>P</fixed-case>olish</title>
      <author><first>Iwona</first><last>Christop</last></author>
      <pages>12111–12116</pages>
      <abstract>Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional speech in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).</abstract>
      <url hash="483fdcb8">2024.lrec-main.1059</url>
      <bibkey>christop-2024-nemo-dataset</bibkey>
    </paper>
    <paper id="1060">
      <title><fixed-case>NER</fixed-case>-guided Comprehensive Hierarchy-aware Prompt Tuning for Hierarchical Text Classification</title>
      <author><first>Fuhan</first><last>Cai</last></author>
      <author><first>Duo</first><last>Liu</last></author>
      <author><first>Zhongqiang</first><last>Zhang</last></author>
      <author><first>Ge</first><last>Liu</last></author>
      <author><first>Xiaozhe</first><last>Yang</last></author>
      <author><first>Xiangzhong</first><last>Fang</last></author>
      <pages>12117–12126</pages>
      <abstract>Hierarchical text classification (HTC) is a significant but challenging task in natural language processing (NLP) due to its complex taxonomic label hierarchy. Recently, there have been a number of approaches that applied prompt learning to HTC problems, demonstrating impressive efficacy. The majority of prompt-based studies emphasize global hierarchical features by employing graph networks to represent the hierarchical structure as a whole, with limited research on maintaining path consistency within the internal hierarchy of the structure. In this paper, we formulate prompt-based HTC as a named entity recognition (NER) task and introduce conditional random fields (CRF) and Global Pointer to establish hierarchical dependencies. Specifically, we approach single- and multi-path HTC as flat and nested entity recognition tasks and model them using span- and token-based methods. By narrowing the gap between HTC and NER, we maintain the consistency of internal paths within the hierarchical structure through a simple and effective way. Extensive experiments on three public datasets show that our method achieves state-of-the-art (SoTA) performance.</abstract>
      <url hash="684e76ee">2024.lrec-main.1060</url>
      <bibkey>cai-etal-2024-ner-guided</bibkey>
    </paper>
    <paper id="1061">
      <title>Nested Event Extraction upon Pivot Element Recognition</title>
      <author><first>Weicheng</first><last>Ren</last></author>
      <author><first>Zixuan</first><last>Li</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Miao</first><last>Su</last></author>
      <author><first>Yantao</first><last>Liu</last></author>
      <author><first>Saiping</first><last>Guan</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>12127–12137</pages>
      <abstract>Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer-nest events and as triggers of inner-nest events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner-nest and outer-nest events and further recognizes the PEs via classifying the relation type between trigger pairs. The model uses prompt learning to incorporate information from both event types and argument roles for better trigger and argument representations to improve NEE performance. Since existing NEE datasets (e.g., Genia11) are limited to specific domains and contain a narrow range of event types with nested structures, we systematically categorize nested events in the generic domain and construct a new NEE dataset, called ACE2005-Nest. Experimental results demonstrate that PerNee consistently achieves state-of-the-art performance on ACE2005-Nest, Genia11, and Genia13. The ACE2005-Nest dataset and the code of the PerNee model are available at https://github.com/waysonren/PerNee.</abstract>
      <url hash="1c22a59c">2024.lrec-main.1061</url>
      <bibkey>ren-etal-2024-nested-event</bibkey>
    </paper>
    <paper id="1062">
      <title>Nested Noun Phrase Identification Using <fixed-case>BERT</fixed-case></title>
      <author><first>Shweta</first><last>Misra</last></author>
      <author><first>Johan</first><last>Boye</last></author>
      <pages>12138–12143</pages>
      <abstract>For several NLP tasks, an important substep is the identification of noun phrases in running text. This has typically been done by “chunking” – a way of finding minimal noun phrases by token classification. However, chunking-like methods do not represent the fact that noun phrases can be nested. This paper presents a novel method of finding all noun phrases in a sentence, nested to an arbitrary depth, using the BERT model for token classification. We show that our proposed method achieves very good results for both Swedish and English.</abstract>
      <url hash="ef8a851a">2024.lrec-main.1062</url>
      <bibkey>misra-boye-2024-nested-noun</bibkey>
    </paper>
    <paper id="1063">
      <title>Neural Machine Translation between Low-Resource Languages with Synthetic Pivoting</title>
      <author><first>Khalid</first><last>Ahmed</last></author>
      <author><first>Jan</first><last>Buys</last></author>
      <pages>12144–12158</pages>
      <abstract>Training neural models for translating between low-resource languages is challenging due to the scarcity of direct parallel data between such languages. Pivot-based neural machine translation (NMT) systems overcome data scarcity by including a high-resource pivot language in the process of translating between low-resource languages. We propose synthetic pivoting, a novel approach to pivot-based translation in which the pivot sentences are generated synthetically from both the source and target languages. Synthetic pivot sentences are generated through sequence-level knowledge distillation, with the aim of changing the structure of pivot sentences to be closer to that of the source or target languages, thereby reducing pivot translation complexity. We incorporate synthetic pivoting into two paradigms for pivoting: cascading and direct translation using synthetic source and target sentences. We find that the performance of pivot-based systems highly depends on the quality of the NMT model used for sentence regeneration. Furthermore, training back-translation models on these sentences can make the models more robust to input-side noise. The results show that synthetic data generation improves pivot-based systems translating between low-resource Southern African languages by up to 5.6 BLEU points after fine-tuning.</abstract>
      <url hash="017ea911">2024.lrec-main.1063</url>
      <bibkey>ahmed-buys-2024-neural-machine</bibkey>
    </paper>
    <paper id="1064">
      <title>Neural Multimodal Topic Modeling: A Comprehensive Evaluation</title>
      <author><first>Felipe</first><last>Gonzalez-Pizarro</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>12159–12172</pages>
      <abstract>Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the potential for their application in guiding future multimodal topic modeling endeavors.</abstract>
      <url hash="dc3ca0e6">2024.lrec-main.1064</url>
      <bibkey>gonzalez-pizarro-carenini-2024-neural-multimodal</bibkey>
    </paper>
    <paper id="1065">
      <title>New Datasets for Automatic Detection of Textual Entailment and of Contradictions between Sentences in <fixed-case>F</fixed-case>rench</title>
      <author><first>Maximos</first><last>Skandalis</last></author>
      <author><first>Richard</first><last>Moot</last></author>
      <author><first>Christian</first><last>Retoré</last></author>
      <author><first>Simon</first><last>Robillard</last></author>
      <pages>12173–12186</pages>
      <abstract>This paper introduces DACCORD, an original dataset in French for automatic detection of contradictions between sentences. It also presents new, manually translated versions of two datasets, namely the well known dataset RTE3 and the recent dataset GQNLI, from English to French, for the task of natural language inference / recognising textual entailment, which is a sentence-pair classification task. These datasets help increase the admittedly limited number of datasets in French available for these tasks. DACCORD consists of 1034 pairs of sentences and is the first dataset exclusively dedicated to this task and covering among others the topic of the Russian invasion in Ukraine. RTE3-FR contains 800 examples for each of its validation and test subsets, while GQNLI-FR is composed of 300 pairs of sentences and focuses specifically on the use of generalised quantifiers. Our experiments on these datasets show that they are more challenging than the two already existing datasets for the mainstream NLI task in French (XNLI, FraCaS). For languages other than English, most deep learning models for NLI tasks currently have only XNLI available as a training set. Additional datasets, such as ours for French, could permit different training and evaluation strategies, producing more robust results and reducing the inevitable biases present in any single dataset.</abstract>
      <url hash="37846283">2024.lrec-main.1065</url>
      <bibkey>skandalis-etal-2024-new-datasets</bibkey>
    </paper>
    <paper id="1066">
      <title>New Evaluation Methodology for Qualitatively Comparing Classification Models</title>
      <author><first>Ahmad</first><last>Aljanaideh</last></author>
      <pages>12187–12192</pages>
      <abstract>Text Classification is one of the most common tasks in Natural Language Processing. When proposing new classification models, practitioners select a sample of items the proposed model classified correctly while the baseline did not, and then try to observe patterns across those items to understand the proposed model’s strengths. However, this approach is not comprehensive and requires the effort of observing patterns across text items. In this work, we propose a new evaluation methodology for performing qualitative assessment over multiple classification models. The proposed methodology is driven to discover clusters of text items where each cluster’s items 1) exhibit a linguistic pattern and 2) the proposed model significantly outperforms the baseline when classifying such items. This helps practitioners in learning what their proposed model is powerful at capturing in comparison with the baseline model without having to perform this process manually. We use a fine-tuned BERT and Logistic Regression as the two models to compare with Sentiment Analysis as the downstream task. We show how our proposed evaluation methodology discovers various clusters of text items which BERT classifies significantly more accurately than the Logistic Regression baseline, thus providing insight into what BERT is powerful at capturing.</abstract>
      <url hash="e4d03e91">2024.lrec-main.1066</url>
      <bibkey>aljanaideh-2024-new-evaluation</bibkey>
    </paper>
    <paper id="1067">
      <title>New Intent Discovery with Attracting and Dispersing Prototype</title>
      <author><first>Shun</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Jiaqi</first><last>Bai</last></author>
      <author><first>Chaoran</first><last>Yan</last></author>
      <author><first>Tongliang</first><last>Li</last></author>
      <author><first>Zhao</first><last>Yan</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>12193–12206</pages>
      <abstract>New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data. The task is addressed as a feature-clustering problem and recent studies augment instance representation. However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances. Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories. Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness. To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective. Experimental results evaluated on three challenging benchmarks (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even large language model) by a large margin (average 5.5% improvement).</abstract>
      <url hash="c0aadb93">2024.lrec-main.1067</url>
      <bibkey>zhang-etal-2024-new-intent</bibkey>
    </paper>
    <paper id="1068">
      <title>New Methods for Exploring Intonosyntax: Introducing an Intonosyntactic Treebank for <fixed-case>N</fixed-case>igerian <fixed-case>P</fixed-case>idgin</title>
      <author><first>Emmett</first><last>Strickland</last></author>
      <author><first>Anne</first><last>Lacheret-Dujour</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Marc</first><last>Evrard</last></author>
      <author><first>Perrine</first><last>Quennehen</last></author>
      <author><first>Bernard</first><last>Caron</last></author>
      <author><first>Francis</first><last>Egbokhare</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <pages>12207–12216</pages>
      <abstract>This paper presents a new phonetic resource for Nigerian Pidgin, a low-resource language of West Africa. Aiming to provide a new tool for research on intonosyntax, we have augmented an existing syntactic treebank of Nigerian Pidgin, associating each orthographically transcribed token with a series of syllable-level alignments and phonetizations. Syllables are further described using a set of continuous and discrete prosodic features. This new approach provides a simple tool for researchers to explore the prosodic characteristics of various syntactic phenomena. In this paper, we present the format of the corpus, the various features added, and several explorations that can be performed using an online interface. We also present a prosodically specified lexicon extracted using this resource. In it, each orthographic form is accompanied by the frequency of its phoneme-level variants, as well as the suprasegmental features that most frequently accompany each syllable. Finally, we present several additional case studies on how this corpus can used in the study of the language’s prosody.</abstract>
      <url hash="551ec99c">2024.lrec-main.1068</url>
      <bibkey>strickland-etal-2024-new-methods</bibkey>
    </paper>
    <paper id="1069">
      <title>New Proposal of Greenberg’s Universal 14 from Typometrics</title>
      <author><first>Antoni</first><last>Brosa-Rodríguez</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>12217–12226</pages>
      <abstract>In his Universal 14, Greenberg stated that the normal and dominant order in all world languages was to place the condition before the conclusion in conditional sentences. We take this claim to review it quantitatively and based on occurrences in real texts in more than 50 languages. We can see that Greenberg’s proposal is correct but that it needs a reformulation to be true at all. We propose a quantitatively based and updated Universal 14, which gives a better account of the representation of the different languages analyzed and which is fulfilled in 100% of the cases (as opposed to Greenberg’s 60% in our sample). In addition, we also analyze adverbial sentences. Once we obtain the occurrence data in their direction (before or after the main verb), we plot a new Universal in a typometrical way: 100% of the languages show a higher proportion of preceding conditional clauses than of adverbial clauses, regardless of their type or the direction preference for adverbial clauses. The relationship between the SOV type and a stricter initial conditional location is also proposed.</abstract>
      <url hash="2e539aed">2024.lrec-main.1069</url>
      <bibkey>brosa-rodriguez-kahane-2024-new-proposal</bibkey>
    </paper>
    <paper id="1070">
      <title>New Semantic Task for the <fixed-case>F</fixed-case>rench Spoken Language Understanding <fixed-case>MEDIA</fixed-case> Benchmark</title>
      <author><first>Nadège</first><last>Alavoine</last></author>
      <author><first>Gaëlle</first><last>Laperrière</last></author>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Sahar</first><last>Ghannay</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>12227–12246</pages>
      <abstract>Intent classification and slot-filling are essential tasks of Spoken Language Understanding (SLU). In most SLU systems, those tasks are realized by independent modules, but for about fifteen years, models achieving both of them jointly and exploiting their mutual enhancement have been proposed. A multilingual module using a joint model was envisioned to create a touristic dialogue system for a European project, HumanE-AI-Net. A combination of multiple datasets, including the MEDIA dataset, was suggested for training this joint model. The MEDIA SLU dataset is a French dataset distributed since 2005 by ELRA, mainly used by the French research community and free for academic research since 2020. Unfortunately, it is annotated only in slots but not intents. An enhanced version of MEDIA annotated with intents has been built to extend its use to more tasks and use cases. This paper presents the semi-automatic methodology used to obtain this enhanced version. In addition, we present the first results of SLU experiments on this enhanced dataset using joint models for intent classification and slot-filling.</abstract>
      <url hash="6ae9f2de">2024.lrec-main.1070</url>
      <bibkey>alavoine-etal-2024-new-semantic</bibkey>
    </paper>
    <paper id="1071">
      <title><fixed-case>NGLUE</fixed-case>ni: Benchmarking and Adapting Pretrained Language Models for Nguni Languages</title>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Haiyue</first><last>Song</last></author>
      <author><first>Abhisek</first><last>Chakrabarty</last></author>
      <author><first>Jan</first><last>Buys</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Hideki</first><last>Tanaka</last></author>
      <pages>12247–12258</pages>
      <abstract>The Nguni languages have over 20 million home language speakers in South Africa. There has been considerable growth in the datasets for Nguni languages, but so far no analysis of the performance of NLP models for these languages has been reported across languages and tasks. In this paper we study pretrained language models for the 4 Nguni languages - isiXhosa, isiZulu, isiNdebele, and Siswati. We compile publicly available datasets for natural language understanding and generation, spanning 6 tasks and 11 datasets. This benchmark, which we call NGLUEni, is the first centralised evaluation suite for the Nguni languages, allowing us to systematically evaluate the Nguni-language capabilities of pretrained language models (PLMs). Besides evaluating existing PLMs, we develop new PLMs for the Nguni languages through multilingual adaptive finetuning. Our models, Nguni-XLMR and Nguni-ByT5, outperform their base models and large-scale adapted models, showing that performance gains are obtainable through limited language group-based adaptation. We also perform experiments on cross-lingual transfer and machine translation. Our models achieve notable cross-lingual transfer improvements in the lower resourced Nguni languages (isiNdebele and Siswati). To facilitate future use of NGLUEni as a standardised evaluation suite for the Nguni languages, we create a web portal to access the collection of datasets and publicly release our models.</abstract>
      <url hash="8a3deb91">2024.lrec-main.1071</url>
      <bibkey>meyer-etal-2024-nglueni-benchmarking</bibkey>
    </paper>
    <paper id="1072">
      <title><fixed-case>NL</fixed-case>o<fixed-case>PT</fixed-case>: N-gram Enhanced Low-Rank Task Adaptive Pre-training for Efficient Language Model Adaption</title>
      <author><first>Hao</first><last>Gu</last></author>
      <author><first>Jiangyan</first><last>Yi</last></author>
      <author><first>Zheng</first><last>Lian</last></author>
      <author><first>Jianhua</first><last>Tao</last></author>
      <author><first>Xinrui</first><last>Yan</last></author>
      <pages>12259–12270</pages>
      <abstract>Pre-trained Language Models (PLMs) like BERT have achieved superior performance on different downstream tasks, even when such a model is trained on a general domain. Moreover, recent studies have shown that continued pre-training on task-specific data, known as task adaptive pre-training (TAPT), can further improve downstream task performance. However, conventional TAPT adjusts all the parameters of the PLMs, which distorts the learned generic knowledge embedded in the original PLMs weights, and it is expensive to store a whole model copy for each downstream task. In this paper, we propose NLoPT, a two-step n-gram enhanced low-rank task adaptive pre-training method, to effectively and efficiently customize a PLM to the downstream task. Specifically, we first apply low-rank adaption (LoRA), a prevalent parameter-efficient technique, for efficient TAPT. We further explicitly incorporate the task-specific multi-granularity n-gram information via the cross-attention mechanism. Experimental results on six datasets from four domains illustrate the effectiveness of NLoPT, demonstrating the superiority of LoRA based TAPT and the necessity of incorporating task-specific n-gram information.</abstract>
      <url hash="ff26ae2b">2024.lrec-main.1072</url>
      <bibkey>gu-etal-2024-nlopt-n</bibkey>
    </paper>
    <paper id="1073">
      <title><fixed-case>NLP</fixed-case>re: A Revised Approach towards Language-centric Benchmarking of Natural Language Preprocessing Systems</title>
      <author><first>Martyna</first><last>Wiącek</last></author>
      <author><first>Piotr</first><last>Rybak</last></author>
      <author><first>Łukasz</first><last>Pszenny</last></author>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <pages>12271–12287</pages>
      <abstract>With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an extensive evaluation of a variety of Polish NLPre systems. To facilitate the construction of benchmarking environments for other languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full customization of the publicly released source code of the benchmarking system. The links to all the resources (deployed platforms, source code, trained models, datasets etc.) can be found on the project website: https://sites.google.com/view/nlpre-benchmark.</abstract>
      <url hash="992187d6">2024.lrec-main.1073</url>
      <bibkey>wiacek-etal-2024-nlpre-revised</bibkey>
    </paper>
    <paper id="1074">
      <title>No Need for Large-Scale Search: Exploring Large Language Models in Complex Knowledge Base Question Answering</title>
      <author><first>Shouhui</first><last>Wang</last></author>
      <author><first>Biao</first><last>Qin</last></author>
      <pages>12288–12299</pages>
      <abstract>Knowledge Base Question Answering (KBQA) systems play a pivotal role in the domain of natural language processing and information retrieval. Its primary objective is to bridge the gap between natural language questions and structured knowledge representations, especially for complex KBQA. Despite the significant progress in developing effective and interconnected KBQA technologies, the recent emergence of large language models (LLMs) offers an opportunity to address the challenges faced by KBQA systems more efficiently. This study adopts the LLMs, such as Large Language Model Meta AI (LLaMA), as a channel to connect natural language questions with structured knowledge representations and proposes a Three-step Fine-tune Strategy based on large language model to implement the KBQA system (TFS-KBQA). This method achieves direct conversion from natural language questions to structured knowledge representations, thereby overcoming the limitations of existing KBQA methods, such as addressing large search and reasoning spaces and ranking massive candidates. To evaluate the effectiveness of the proposed method, we conduct experiments using three popular complex KBQA datasets. The results achieve state-of-the-art performance across all three datasets, with particularly notable results for the WebQuestionSP dataset, which achieves an <tex-math>F1</tex-math> value of 79.9%.</abstract>
      <url hash="75db5ba6">2024.lrec-main.1074</url>
      <bibkey>wang-qin-2024-need-large</bibkey>
    </paper>
    <paper id="1075">
      <title>Non-Essential Is <fixed-case>NE</fixed-case>cessary: Order-agnostic Multi-hop Question Generation</title>
      <author><first>Kyungho</first><last>Kim</last></author>
      <author><first>Seongmin</first><last>Park</last></author>
      <author><first>Junseo</first><last>Lee</last></author>
      <author><first>Jihwa</first><last>Lee</last></author>
      <pages>12300–12306</pages>
      <abstract>Existing multi-hop question generation (QG) methods treat answer-irrelevant documents as non-essential and remove them as impurities. However, this approach can create a training-inference discrepancy when impurities cannot be completely removed, which can lead to a decrease in model performance. To overcome this problem, we propose an auxiliary task, called order-agnostic, which leverages non-essential data in the training phase to create a robust model and extract the consistent embeddings in real-world inference environments. Additionally, we use a single LM to perform both ranker and generator through a prompt-based approach without applying additional external modules. Furthermore, we discover that appropriate utilization of the non-essential components can achieve a significant performance increase. Finally, experiments conducted on HotpotQA dataset achieve state-of-the-art.</abstract>
      <url hash="7c97b9c6">2024.lrec-main.1075</url>
      <bibkey>kim-etal-2024-non-essential</bibkey>
    </paper>
    <paper id="1076">
      <title><fixed-case>NS</fixed-case>ina: A News Corpus for <fixed-case>S</fixed-case>inhala</title>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <author><first>Damith</first><last>Premasiri</last></author>
      <author><first>Lasitha Randunu Chandrakantha</first><last>Uyangodage</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>12307–12312</pages>
      <abstract>The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSina, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSina aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSina is the largest news corpus for Sinhala, available up to date.</abstract>
      <url hash="bc00bf22">2024.lrec-main.1076</url>
      <bibkey>hettiarachchi-etal-2024-nsina-news</bibkey>
    </paper>
    <paper id="1077">
      <title>Null Subjects in <fixed-case>S</fixed-case>panish as a Machine Translation Problem</title>
      <author><first>Jose Diego</first><last>Suarez</last></author>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <pages>12313–12322</pages>
      <abstract>In this study we approach the detection of null subjects and impersonal constructions in Spanish using a machine translation methodology. We repurpose the Spanish AnCora corpus, converting it to a parallel set that transforms Spanish sentences into a format that allows us to detect and classify verbs, and train LSTM-based neural machine translation systems to perform this task. Various models differing on output format and hyperparameters were evaluated. Experimental results proved this approach to be highly resource-effective, obtaining results comparable to or surpassing the state of the art found in existing literature, while employing modest computational resources. Additionally, an improved dataset for training and evaluating Spanish null-subject detection tools was elaborated for this project, that could aid in the creation and serve as a benchmark for further developments in the area.</abstract>
      <url hash="a56b23e3">2024.lrec-main.1077</url>
      <bibkey>suarez-chiruzzo-2024-null-subjects</bibkey>
    </paper>
    <paper id="1078">
      <title><fixed-case>N</fixed-case>um<fixed-case>HG</fixed-case>: A Dataset for Number-Focused Headline Generation</title>
      <author><first>Jian-Tao</first><last>Huang</last></author>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>12323–12329</pages>
      <abstract>Headline generation, a key task in abstractive summarization, strives to condense a full-length article into a succinct, single line of text. Notably, while contemporary encoder-decoder models excel based on the ROUGE metric, they often falter when it comes to the precise generation of numerals in headlines. We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock. To address this, we introduce a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news articles for detailed investigation. Further, we evaluate five well-performing models from previous headline-generation tasks using human evaluation in terms of numerical accuracy, reasonableness, and readability. Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation and stimulate further discussions in numeral-focused text generation.</abstract>
      <url hash="e8ed7469">2024.lrec-main.1078</url>
      <bibkey>huang-etal-2024-numhg-dataset</bibkey>
    </paper>
    <paper id="1079">
      <title><fixed-case>N</fixed-case>ut<fixed-case>F</fixed-case>rame: Frame-based Conceptual Structure Induction with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shaoru</first><last>Guo</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Ru</first><last>Li</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>12330–12335</pages>
      <abstract>Conceptual structure is fundamental to human cognition and natural language understanding. It is significant to explore whether Large Language Models (LLMs) understand such knowledge. Since FrameNet serves as a well-defined conceptual structure knowledge resource, with meaningful frames, fine-grained frame elements, and rich frame relations, we construct a benchmark for coNceptual structure induction based on FrameNet, called NutFrame. It contains three sub-tasks: Frame Induction, Frame Element Induction, and Frame Relation Induction. In addition, we utilize prompts to induce conceptual structure of Framenet with LLMs. Furthermore, we conduct extensive experiments on NutFrame to evaluate various widely-used LLMs. Experimental results demonstrate that FrameNet induction remains a challenge for LLMs.</abstract>
      <url hash="0a314e64">2024.lrec-main.1079</url>
      <bibkey>guo-etal-2024-nutframe-frame</bibkey>
    </paper>
    <paper id="1080">
      <title><fixed-case>OATS</fixed-case>: A Challenge Dataset for Opinion Aspect Target Sentiment Joint Detection for Aspect-Based Sentiment Analysis</title>
      <author><first>Siva Uday Sampreeth</first><last>Chebolu</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>12336–12347</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) delves into understanding sentiments specific to distinct elements within a user-generated review. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 27,470 sentence-level quadruples and 17,092 review-level tuples. Our initiative seeks to bridge specific observed gaps in existing datasets: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS’s potential and shed light on various ABSA subtasks that OATS can solve, we conducted experiments, establishing initial baselines. We hope the OATS dataset augments current resources, paving the way for an encompassing exploration of ABSA (https://github.com/RiTUAL-UH/OATS-ABSA).</abstract>
      <url hash="372eff33">2024.lrec-main.1080</url>
      <bibkey>chebolu-etal-2024-oats-challenge</bibkey>
    </paper>
    <paper id="1081">
      <title><fixed-case>OLV</fixed-case>i<fixed-case>T</fixed-case>: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</title>
      <author><first>Adnen</first><last>Abdessaied</last></author>
      <author><first>Manuel</first><last>Hochmeister</last></author>
      <author><first>Andreas</first><last>Bulling</last></author>
      <pages>12348–12358</pages>
      <abstract>We present the Object Language Video Transformer (OLViT) – a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.</abstract>
      <url hash="1b5daeec">2024.lrec-main.1081</url>
      <bibkey>abdessaied-etal-2024-olvit-multi</bibkey>
    </paper>
    <paper id="1082">
      <title>On an Intermediate Task for Classifying <fixed-case>URL</fixed-case> Citations on Scholarly Papers</title>
      <author><first>Kazuhiro</first><last>Wada</last></author>
      <author><first>Masaya</first><last>Tsunokake</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <pages>12359–12369</pages>
      <abstract>Citations using URL (URL citations) that appear in scholarly papers can be used as an information source for the research resource search engines. In particular, the information about the types of cited resources and reasons for their citation is crucial to describe the resources and their relations in the search services. To obtain this information, previous studies proposed some methods for classifying URL citations. However, their methods trained the model using a simple fine-tuning strategy and exhibited insufficient performance. We propose a classification method using a novel intermediate task. Our method trains the model on our intermediate task of identifying whether sample pairs belong to the same class before being fine-tuned on the target task. In the experiment, our method outperformed previous methods using the simple fine-tuning strategy with higher macro F-scores for different model sizes and architectures. Our analysis results indicate that the model learns the class boundaries of the target task by training our intermediate task. Our intermediate task also demonstrated higher performance and computational efficiency than an alternative intermediate task using triplet loss. Finally, we applied our method to other text classification tasks and confirmed the effectiveness when a simple fine-tuning strategy does not stably work.</abstract>
      <url hash="ae70356e">2024.lrec-main.1082</url>
      <bibkey>wada-etal-2024-intermediate-task</bibkey>
    </paper>
    <paper id="1083">
      <title>On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation</title>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Wasi</first><last>Ahmad</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>12370–12384</pages>
      <abstract>This study addresses the application of encoder-only Pre-trained Language Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG, (2) optimal architectural decisions for employing encoder-only PLMs in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder PLMs across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only PLMs, although keyphrase extraction with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq PLMs. We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only PLMs. The study sheds light on the potential of utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork for future KPG methods. Our code and pre-trained checkpoints are released at https://github.com/uclanlp/DeepKPG.</abstract>
      <url hash="97d7ceca">2024.lrec-main.1083</url>
      <bibkey>wu-etal-2024-leveraging-encoder</bibkey>
    </paper>
    <paper id="1084">
      <title>On Modelling Corpus Citations in Computational Lexical Resources</title>
      <author><first>Fahad</first><last>Khan</last></author>
      <author><first>Maxim</first><last>Ionov</last></author>
      <author><first>Christian</first><last>Chiarcos</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Gilles</first><last>Sérasset</last></author>
      <author><first>Besim</first><last>Kabashi</last></author>
      <pages>12385–12394</pages>
      <abstract>In this article we look at how two different standards for lexical resources, TEI and OntoLex, deal with corpus citations in lexicons. We will focus on how corpus citations in retrodigitised dictionaries can be modelled using each of the two standards since this provides us with a suitably challenging use case. After looking at the structure of an example entry from a legacy dictionary, we examine the two approaches offered by the two different standards by outlining an encoding for the example entry using both of them (note that this article features the first extended discussion of how the Frequency Attestation and Corpus (FrAC) module of OntoLex deals with citations). After comparing the two approaches and looking at the advantages and disadvantages of both, we argue for a combination of both. In the last part of the article we discuss different ways of doing this, giving our preference for a strategy which makes use of RDFa.</abstract>
      <url hash="74855268">2024.lrec-main.1084</url>
      <bibkey>khan-etal-2024-modelling-corpus</bibkey>
    </paper>
    <paper id="1085">
      <title>On the Adaptation of Unlimiformer for Decoder-Only Transformers</title>
      <author><first>Kian</first><last>Ahrabian</last></author>
      <author><first>Alon</first><last>Benhaim</last></author>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <pages>12395–12402</pages>
      <abstract>One of the prominent issues stifling the current generation of large language models is their limited context length. Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less. Unlimiformer (Bertsch et al., 2023) is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index. However, its main limitation is incompatibility with decoder-only transformers out of the box. In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation. Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&amp;A) and an instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length. Moreover, we discuss limitations and future directions for free-form Q&amp;A and instruction-tuned models.</abstract>
      <url hash="a00a5b38">2024.lrec-main.1085</url>
      <bibkey>ahrabian-etal-2024-adaptation-unlimiformer</bibkey>
    </paper>
    <paper id="1086">
      <title>On the Relationship between Skill Neurons and Robustness in Prompt Tuning</title>
      <author><first>Leon</first><last>Ackermann</last></author>
      <author><first>Xenia Isabel</first><last>Ohmer</last></author>
      <pages>12403–12415</pages>
      <abstract>Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer’s feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these “skill neurons”, using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data. While prompts tuned for RoBERTa yield below-chance performance on adversarial data, prompts tuned for T5 are slightly more robust and retain above-chance performance in two out of three cases. At the same time, we replicate the finding that skill neurons exist in RoBERTa and further show that skill neurons also exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to a model’s ability to consistently activate the relevant skill neurons on adversarial data.</abstract>
      <url hash="689749f5">2024.lrec-main.1086</url>
      <bibkey>ackermann-ohmer-2024-relationship-skill</bibkey>
    </paper>
    <paper id="1087">
      <title>On the Scaling Laws of Geographical Representation in Language Models</title>
      <author><first>Nathan</first><last>Godey</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>12416–12422</pages>
      <abstract>Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.</abstract>
      <url hash="833ce947">2024.lrec-main.1087</url>
      <bibkey>godey-etal-2024-scaling-laws</bibkey>
    </paper>
    <paper id="1088">
      <title>On the Use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction</title>
      <author><first>Jianwei</first><last>Wang</last></author>
      <author><first>Tianyin</first><last>Wang</last></author>
      <author><first>Ziqian</first><last>Zeng</last></author>
      <pages>12423–12434</pages>
      <abstract>The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clean data; (4) Inference on the test data. The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation classification task, and by 3% ~7 % on Smile (Korean and Polish) in the zero-shot cross-lingual relation classification task, and by 8% on ACE05-E+ in the zero-shot event argument classification task.</abstract>
      <url hash="d1de877e">2024.lrec-main.1088</url>
      <attachment type="OptionalSupplementaryMaterial" hash="adbc38d4">2024.lrec-main.1088.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>wang-etal-2024-use-silver</bibkey>
    </paper>
    <paper id="1089">
      <title>On the Way to Lossless Compression of Language Transformers: Exploring Cross-Domain Properties of Quantization</title>
      <author><first>Nikita</first><last>Martynov</last></author>
      <author><first>Aleksei</first><last>Goncharov</last></author>
      <author><first>Gleb</first><last>Kumichev</last></author>
      <author><first>Evgeniy</first><last>Egorov</last></author>
      <author><first>Stanislav Vladimirovich</first><last>Pavlov</last></author>
      <author><first>Mikhail Sergeevich</first><last>Durinov</last></author>
      <author><first>Aleksandr Sergeevich</first><last>Zuev</last></author>
      <author><first>Egor Anatolievich</first><last>Filimonov</last></author>
      <pages>12435–12442</pages>
      <abstract>Modern Transformers achieved impressive results on various Natural Language Processing tasks over the last few years. The one downside of this success is the size of these models. Huge capacity, which sometimes surpasses billions of parameters, improves generalization abilities, but makes it difficult to employ. Developing field of model compression seeks to reduce the model size and inference latency. This research focuses on one of the compression techniques — Post-Training Quantization. We present a methodology to effectively quantize at least 95% of Transformer weights and corresponding activations to INT8 without any access to task-specific data so the drop in performance does not exceed 0.02%. Furthermore, we provide intriguing observations that reflect cross-domain nature of some of the quantization properties.</abstract>
      <url hash="711738e9">2024.lrec-main.1089</url>
      <bibkey>martynov-etal-2024-way-lossless</bibkey>
    </paper>
    <paper id="1090">
      <title>On Zero-Shot Counterspeech Generation by <fixed-case>LLM</fixed-case>s</title>
      <author><first>Punyajoy</first><last>Saha</last></author>
      <author><first>Aalok</first><last>Agrawal</last></author>
      <author><first>Abhik</first><last>Jana</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Animesh</first><last>Mukherjee</last></author>
      <pages>12443–12454</pages>
      <abstract>With the emergence of numerous Large Language Models (LLM), the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings. In this work, we present a comprehensive analysis of the performances of four LLMs namely GPT-2, DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech generation, which is the first of its kind. For GPT-2 and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, large) of the models. On the other hand, we propose three different prompting strategies for generating different types of counterspeech and analyse the impact of such strategies on the performance of the models. Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size. Considering type of model, GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT. ChatGPT are much better at generating counter speech than other models across all metrics. In terms of prompting, we find that our proposed strategies help in improving counter speech generation across all the models.</abstract>
      <url hash="14325187">2024.lrec-main.1090</url>
      <bibkey>saha-etal-2024-zero-shot</bibkey>
    </paper>
    <paper id="1091">
      <title><fixed-case>OOV</fixed-case>s in the Spotlight: How to Inflect Them?</title>
      <author><first>Tomáš</first><last>Sourada</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <pages>12455–12466</pages>
      <abstract>We focus on morphological inflection in out-of-vocabulary (OOV) conditions, an under-researched subtask in which state-of-the-art systems usually are less effective. We developed three systems: a retrograde model and two sequence-to-sequence (seq2seq) models based on LSTM and Transformer. For testing in OOV conditions, we automatically extracted a large dataset of nouns in the morphologically rich Czech language, with lemma-disjoint data splits, and we further manually annotated a real-world OOV dataset of neologisms. In the standard OOV conditions, Transformer achieves the best results, with increasing performance in ensemble with LSTM, the retrograde model and SIGMORPHON baselines. On the real-world OOV dataset of neologisms, the retrograde model outperforms all neural models. Finally, our seq2seq models achieve state-of-the-art results in 9 out of 16 languages from SIGMORPHON 2022 shared task data in the OOV evaluation (feature overlap) in the large data condition. We release the Czech OOV Inflection Dataset for rigorous evaluation in OOV conditions. Further, we release the inflection system with the seq2seq models as a ready-to-use Python library.</abstract>
      <url hash="735e7511">2024.lrec-main.1091</url>
      <attachment type="OptionalSupplementaryMaterial" hash="db752997">2024.lrec-main.1091.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>sourada-etal-2024-oovs-spotlight</bibkey>
    </paper>
    <paper id="1092">
      <title><fixed-case>O</fixed-case>pen<fixed-case>MSD</fixed-case>: Towards Multilingual Scientific Documents Similarity Measurement</title>
      <author><first>Yang</first><last>Gao</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Ivan</first><last>Korotkov</last></author>
      <author><first>Keith</first><last>Hall</last></author>
      <author><first>Dana</first><last>Alon</last></author>
      <author><first>Donald</first><last>Metzler</last></author>
      <pages>12467–12480</pages>
      <abstract>We develop and evaluate multilingual scientific documents similarity measurement models in this work. Such models can be used to find related papers in different languages, which can help multilingual researchers find and explore papers more efficiently. We propose the first multilingual scientific documents dataset, Open-access Multilingual Scientific Documents (OpenMSD), which has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we develop multilingual SDSM models by adjusting and extending the state-of-the-art methods designed for English SDSM tasks. We find that: (i)Some highly successful methods in English SDSM yield significantly worse performance in multilingual SDSM. (ii)Our best model, which enriches the non-English papers with English summaries, outperforms strong baselines by 7% (in mean average precision) on multilingual SDSM tasks, without compromising the performance on English SDSM tasks.</abstract>
      <url hash="34a0ffca">2024.lrec-main.1092</url>
      <bibkey>gao-etal-2024-openmsd-towards</bibkey>
    </paper>
    <paper id="1093">
      <title>Opinion Mining Using Pre-Trained Large Language Models: Identifying the Type, Polarity, Intensity, Expression, and Source of Private States</title>
      <author><first>Saeed</first><last>Ahmadnia</last></author>
      <author><first>Arash</first><last>Yousefi Jordehi</last></author>
      <author><first>Mahsa</first><last>Hosseini Khasheh Heyran</last></author>
      <author><first>SeyedAbolghasem</first><last>Mirroshandel</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <pages>12481–12495</pages>
      <abstract>Opinion mining is an important task in natural language processing. The MPQA Opinion Corpus is a fine-grained and comprehensive dataset of private states (i.e., the condition of a source who has an attitude which may be directed toward a target) based on context. Although this dataset was released years ago, because of its complex definition of annotations and hard-to-read data format, almost all existing research works have only focused on a small subset of the dataset. In this paper, we present a comprehensive study of the entire MPQA 2.0 dataset. In order to achieve this goal, we first provide a clean version of MPQA 2.0 in a more interpretable format. Then, we propose two novel approaches for opinion mining, establishing new high baselines for future work. We use two pre-trained large language models, BERT and T5, to automatically identify the type, polarity, and intensity of private states expressed in phrases, and we use T5 to detect opinion expressions and their agents (i.e., sources).</abstract>
      <url hash="9a037d6b">2024.lrec-main.1093</url>
      <bibkey>ahmadnia-etal-2024-opinion-mining</bibkey>
    </paper>
    <paper id="1094">
      <title>Opinions Are Not Always Positive: Debiasing Opinion Summarization with Model-Specific and Model-Agnostic Methods</title>
      <author><first>Yanyue</first><last>Zhang</last></author>
      <author><first>Yilong</first><last>Lai</last></author>
      <author><first>Zhenglin</first><last>Wang</last></author>
      <author><first>Pengfei</first><last>Li</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>12496–12513</pages>
      <abstract>As in the existing opinion summary data set, more than 70% are positive texts, the current opinion summarization approaches are reluctant to generate the negative opinion summary given the input of negative opinions. To address such sentiment bias, two approaches are proposed through two perspectives: model-specific and model-agnostic. For the model-specific approach, a variational autoencoder is proposed to disentangle the input representation into sentiment-relevant and sentiment-irrelevant components through adversarial loss. Therefore, the sentiment information in the input is kept and employed for the following decoding which avoids interference of content information with emotional signals. To further avoid relying on some specific opinion summarization frameworks, a model-agnostic approach based on counterfactual data augmentation is proposed. A dataset with a more balanced emotional polarity distribution is constructed using a large pre-trained language model based on some pairwise and mini-edited principles. Experimental results show that the sentiment consistency of the generated summaries is significantly improved using the proposed approaches, while their semantics quality is unaffected.</abstract>
      <url hash="9a581718">2024.lrec-main.1094</url>
      <bibkey>zhang-etal-2024-opinions-always</bibkey>
    </paper>
    <paper id="1095">
      <title>Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on <fixed-case>K</fixed-case>orean</title>
      <author><first>ChangSu</first><last>Choi</last></author>
      <author><first>Yongbin</first><last>Jeong</last></author>
      <author><first>Seoyoon</first><last>Park</last></author>
      <author><first>Inho</first><last>Won</last></author>
      <author><first>HyeonSeok</first><last>Lim</last></author>
      <author><first>SangMin</first><last>Kim</last></author>
      <author><first>Yejee</first><last>Kang</last></author>
      <author><first>Chanhyuk</first><last>Yoon</last></author>
      <author><first>Jaewan</first><last>Park</last></author>
      <author><first>Yiseul</first><last>Lee</last></author>
      <author><first>HyeJin</first><last>Lee</last></author>
      <author><first>Younggyun</first><last>Hahm</last></author>
      <author><first>Hansaem</first><last>Kim</last></author>
      <author><first>KyungTae</first><last>Lim</last></author>
      <pages>12514–12526</pages>
      <abstract>Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human evaluation and GPT4. Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models.</abstract>
      <url hash="8b0bab23">2024.lrec-main.1095</url>
      <bibkey>choi-etal-2024-optimizing-language</bibkey>
    </paper>
    <paper id="1096">
      <title><fixed-case>ORT</fixed-case>icket: Let One Robust <fixed-case>BERT</fixed-case> Ticket Transfer across Different Tasks</title>
      <author><first>Yuhao</first><last>Zhou</last></author>
      <author><first>Wenxiang</first><last>Chen</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>12527–12538</pages>
      <abstract>Pretrained language models can be applied for various downstream tasks but are susceptible to subtle perturbations. Most adversarial defense methods often introduce adversarial training during the fine-tuning phase to enhance empirical robustness. However, the repeated execution of adversarial training hinders training efficiency when transitioning to different tasks. In this paper, we explore the transferability of robustness within subnetworks and leverage this insight to introduce a novel adversarial defense method ORTicket, eliminating the need for separate adversarial training across diverse downstream tasks. Specifically, (i) pruning the full model using the MLM task (the same task employed for BERT pretraining) yields a task-agnostic robust subnetwork(i.e., winning ticket in Lottery Ticket Hypothesis); and (ii) fine-tuning this subnetwork for downstream tasks. Extensive experiments demonstrate that our approach achieves comparable robustness to other defense methods while retaining the efficiency of traditional fine-tuning.This also confirms the significance of selecting MLM task for identifying the transferable robust subnetwork. Furthermore, our method is orthogonal to other adversarial training approaches, indicating the potential for further enhancement of model robustness.</abstract>
      <url hash="ab231117">2024.lrec-main.1096</url>
      <bibkey>zhou-etal-2024-orticket-one</bibkey>
    </paper>
    <paper id="1097">
      <title>Out-of-Domain Intent Detection Considering Multi-Turn Dialogue Contexts</title>
      <author><first>Hao</first><last>Lang</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Binyuan</first><last>Hui</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>12539–12552</pages>
      <abstract>Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-the-art performances on multi-turn OOD detection tasks by improving the F1-OOD score of over 29% compared to the previous best method.</abstract>
      <url hash="525e11c8">2024.lrec-main.1097</url>
      <bibkey>lang-etal-2024-domain-intent</bibkey>
    </paper>
    <paper id="1098">
      <title>Out of the Mouths of <fixed-case>MP</fixed-case>s: Speaker Attribution in Parliamentary Debates</title>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Annelen</first><last>Brunner</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>12553–12563</pages>
      <abstract>This paper presents GePaDe_SpkAtt , a new corpus for speaker attribution in German parliamentary debates, with more than 7,700 manually annotated events of speech, thought and writing. Our role inventory includes the sources, addressees, messages and topics of the speech event and also two additional roles, medium and evidence. We report baseline results for the automatic prediction of speech events and their roles, with high scores for both, event triggers and roles. Then we apply our model to predict speech events in 20 years of parliamentary debates and investigate the use of factives in the rhetoric of MPs.</abstract>
      <url hash="33e6e98c">2024.lrec-main.1098</url>
      <bibkey>rehbein-etal-2024-mouths-mps</bibkey>
    </paper>
    <paper id="1099">
      <title><fixed-case>PACAR</fixed-case>: Automated Fact-Checking with Planning and Customized Action Reasoning Using Large Language Models</title>
      <author><first>Xiaoyan</first><last>Zhao</last></author>
      <author><first>Lingzhi</first><last>Wang</last></author>
      <author><first>Zhanghao</first><last>Wang</last></author>
      <author><first>Hong</first><last>Cheng</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>12564–12573</pages>
      <abstract>In an era characterized by the rapid proliferation of information, the pervasive issues of misinformation and disinformation have significantly impacted numerous individuals. Consequently, the evaluation of information’s truthfulness and accuracy has garnered substantial attention among researchers. In this work, we present a novel fact-checking framework called PACAR, fact-checking based on planning and customized action reasoning using LLMs. It comprises four modules: a claim decomposer with self-reflection, an LLM-centric planner module, an executor for carrying out planned actions, and a verifier module that assesses veracity and generates explanations based on the overall reasoning process. Unlike previous work that employs single-path decision-making and single-step verdict prediction, PACAR focuses on the use of LLMs in dynamic planning and execution of actions. Furthermore, in contrast to previous work that relied primarily on general reasoning, we introduce tailored actions such as numerical reasoning and entity disambiguation to effectively address potential challenges in fact-checking. Our PACAR framework, incorporating LLM-centric planning along with customized action reasoning, significantly outperforms baseline methods across three datasets from different domains and with varying complexity levels. Additional experiments, including multidimensional and sliced observations, demonstrate the effectiveness of PACAR and offer valuable insights for the advancement of automated fact-checking.</abstract>
      <url hash="8a8fc6f2">2024.lrec-main.1099</url>
      <bibkey>zhao-etal-2024-pacar-automated</bibkey>
    </paper>
    <paper id="1100">
      <title><fixed-case>PAD</fixed-case>: A Robustness Enhancement Ensemble Method via Promoting Attention Diversity</title>
      <author><first>Yuting</first><last>Yang</last></author>
      <author><first>Pei</first><last>Huang</last></author>
      <author><first>Feifei</first><last>Ma</last></author>
      <author><first>Juan</first><last>Cao</last></author>
      <author><first>Jintao</first><last>Li</last></author>
      <pages>12574–12584</pages>
      <abstract>Deep neural networks can be vulnerable to adversarial attacks, even for the mainstream Transformer-based models. Although several robustness enhancement approaches have been proposed, they usually focus on some certain type of perturbation. As the types of attack can be various and unpredictable in practical scenarios, a general and strong defense method is urgently in require. We notice that most well-trained models can be weakly robust in the perturbation space, i.e., only a small ratio of adversarial examples exist. Inspired by the weak robust property, this paper presents a novel ensemble method for enhancing robustness. We propose a lightweight framework PAD to save computational resources in realizing an ensemble. Instead of training multiple models, a plugin module is designed to perturb the parameters of a base model which can achieve the effect of multiple models. Then, to diversify adversarial example distributions among different models, we promote each model to have different attention patterns via optimizing a diversity measure we defined. Experiments on various widely-used datasets and target models show that PAD can consistently improve the defense ability against many types of adversarial attacks while maintaining accuracy on clean data. Besides, PAD also presents good interpretability via visualizing diverse attention patterns.</abstract>
      <url hash="7e00d070">2024.lrec-main.1100</url>
      <bibkey>yang-etal-2024-pad-robustness</bibkey>
    </paper>
    <paper id="1101">
      <title>Palmyra 3.0: A User-Friendly Cloud-Based Platform for Morphology and Dependency Syntax Annotation</title>
      <author><first>Muhammed</first><last>AbuOdeh</last></author>
      <author><first>Long</first><last>Phan</last></author>
      <author><first>Ahmed</first><last>Farouk Zakaria Elshabrawy</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>12585–12591</pages>
      <abstract>We present Palmyra 3.0, a cloud-based, configurable, and user-friendly platform for morphology and syntax annotation through dependency-tree visualization. Palmyra 3.0 implements a robust system that stores data on the cloud. By default, Palmyra 3.0 comes with an Arabic dependency parser that generates highly accurate trees, but it is easily configurable to support dependency parsers in other languages. Palmyra 3.0 provides default configuration files for a number of predefined formalisms, such as UD and CATiB, and a number of user-friendly features to support annotators.</abstract>
      <url hash="7828e93c">2024.lrec-main.1101</url>
      <bibkey>abuodeh-etal-2024-palmyra-3</bibkey>
    </paper>
    <paper id="1102">
      <title>Parameter-Efficient Transfer Learning for End-to-end Speech Translation</title>
      <author><first>Yunlong</first><last>Zhao</last></author>
      <author><first>Kexin</first><last>Wang</last></author>
      <author><first>Qianqian</first><last>Dong</last></author>
      <author><first>Tom</first><last>Ko</last></author>
      <pages>12592–12598</pages>
      <abstract>Recently, end-to-end speech translation (ST) has gained significant attention in research, but its progress is hindered by the limited availability of labeled data. To overcome this challenge, leveraging pre-trained models for knowledge transfer in ST has emerged as a promising direction. In this paper, we propose PETL-ST, which investigates parameter-efficient transfer learning for end-to-end speech translation. Our method utilizes two lightweight adaptation techniques, namely prefix and adapter, to modulate Attention and the Feed-Forward Network, respectively, while preserving the capabilities of pre-trained models. We conduct experiments on MuST-C En-De, Es, Fr, Ru datasets to evaluate the performance of our approach. The results demonstrate that PETL-ST outperforms strong baselines, achieving superior translation quality with high parameter efficiency. Moreover, our method exhibits remarkable data efficiency and significantly improves performance in low-resource settings.</abstract>
      <url hash="e6eec925">2024.lrec-main.1102</url>
      <bibkey>zhao-etal-2024-parameter-efficient</bibkey>
    </paper>
    <paper id="1103">
      <title><fixed-case>P</fixed-case>ara<fixed-case>N</fixed-case>ames 1.0: Creating an Entity Name Corpus for 400+ Languages Using <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Jonne</first><last>Sälevä</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <pages>12599–12610</pages>
      <abstract>We introduce ParaNames, a massively multilingual parallel name resource consisting of 140 million names spanning over 400 languages. Names are provided for 16.8 million entities, and each entity is mapped from a complex type hierarchy to a standard type (PER/LOC/ORG). Using Wikidata as a source, we create the largest resource of this type to date. We describe our approach to filtering and standardizing the data to provide the best quality possible. ParaNames is useful for multilingual language processing, both in defining tasks for name translation/transliteration and as supplementary data for tasks such as named entity recognition and linking. We demonstrate the usefulness of ParaNames on two tasks. First, we perform canonical name translation between English and 17 other languages. Second, we use it as a gazetteer for multilingual named entity recognition, obtaining performance improvements on all 10 languages evaluated.</abstract>
      <url hash="bbe23551">2024.lrec-main.1103</url>
      <bibkey>saleva-lignos-2024-paranames-1</bibkey>
    </paper>
    <paper id="1104">
      <title><fixed-case>P</fixed-case>a<fixed-case>R</fixed-case>e<fixed-case>NT</fixed-case> (Parent Retrieval Neural Tool): A Deep Dive into Word Formation across Languages</title>
      <author><first>Emil</first><last>Svoboda</last></author>
      <author><first>Magda</first><last>Sevcikova</last></author>
      <pages>12611–12621</pages>
      <abstract>We present PaReNT (Parent Retrieval Neural Tool), a deep-learning-based multilingual tool performing retrieval and word formation classification in English, German, Dutch, Spanish, French, Russian, and Czech. Parent retrieval refers to determining the lexeme or lexemes the input lexeme was based on (e.g. “darkness” is traced back to “dark”; “waterfall” decomposes into “water” and “fall”). Additionally, PaReNT performs word formation classification, which determines the input lexeme as a compound e.g. “proofread”, a derivative (e.g. “deescalate”) or as an unmotivated word (e.g. “dog”). These seven languages are selected from three major branches of the Indo-European language family (Germanic, Romance, Slavic). Data is aggregated from a range of word-formation resources, as well as Wiktionary, to train and test the tool. The tool is based on a custom-architecture hybrid transformer block-enriched sequence-to-sequence neural network utilizing both a character-based and semantic representation of the input lexemes, with two output modules - one decoder-based dedicated to parent retrieval, and one classifier-based for word formation classification. PaReNT achieves a mean accuracy of 0.62 in parent retrieval and a mean balanced accuracy of 0.74 in word formation classification.</abstract>
      <url hash="c674c840">2024.lrec-main.1104</url>
      <bibkey>svoboda-sevcikova-2024-parent-parent</bibkey>
    </paper>
    <paper id="1105">
      <title>Parsing for Mauritian Creole Using <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Neha</first><last>Ramsurrun</last></author>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <author><first>Michael</first><last>Gonzalez</last></author>
      <pages>12622–12632</pages>
      <abstract>This paper presents a first attempt to apply Universal Dependencies (De Marneffe et al., 2021) to train a parser for Mauritian Creole (MC), a French-based Creole language spoken on the island of Mauritius. This paper demonstrates the construction of a 161-sentence (1007-token) treebank for MC and evaluates the performance of a part-of-speech tagger and Universal Dependencies parser trained on this data. The sentences were collected from publicly available grammar books (Syea, 2013) and online resources (Baker and Kriegel, 2013), as well as from government-produced school textbooks (Antonio-Françoise et al., 2021; Natchoo et al., 2017). The parser, trained with UDPipe 2 (Straka, 2018), reached F1 scores of UPOS=86.2, UAS=80.8 and LAS=69.8. This fares favorably when compared to models of similar size for other under-resourced Indigenous and Creole languages. We then address some of the challenges faced when applying UD to Creole languages in general and to Mauritian Creole in particular. The main challenge was the handling of spelling variation in the input. Other issues include the tagging of modal verbs, middle voice sentences, and parts of the tense-aspect-mood system (such as the particle fek).</abstract>
      <url hash="78eaa8ee">2024.lrec-main.1105</url>
      <bibkey>ramsurrun-etal-2024-parsing-mauritian</bibkey>
    </paper>
    <paper id="1106">
      <title>Parsing Headed Constituencies</title>
      <author><first>Katarzyna</first><last>Krasnowska-Kieraś</last></author>
      <author><first>Marcin</first><last>Woliński</last></author>
      <pages>12633–12643</pages>
      <abstract>In the paper, we present a parsing technique that generates headed constituency trees, which combine information typically contained in constituency and dependency trees. We advocate for using such structures for syntactic representation. The parsing method combines prediction of dependency links with prediction of constituency spines in a ‘parsing as tagging’ approach and outputs a hybrid structure. An interesting feature is that the method can generate constituency trees with discontinuities. The parser is built on top of a BERT model for the given language and uses a specially crafted classifier for predicting dependency links. With suitable training data the method can be applied to arbitrary language; we report evaluation results for Polish and German.</abstract>
      <url hash="898375c7">2024.lrec-main.1106</url>
      <bibkey>krasnowska-kieras-wolinski-2024-parsing-headed</bibkey>
    </paper>
    <paper id="1107">
      <title><fixed-case>PASUM</fixed-case>: A Pre-training Architecture for Social Media User Modeling Based on Text Graph</title>
      <author><first>Kun</first><last>Wu</last></author>
      <author><first>Xinyi</first><last>Mou</last></author>
      <author><first>Lanqing</first><last>Xue</last></author>
      <author><first>Zhenzhe</first><last>Ying</last></author>
      <author><first>Weiqiang</first><last>Wang</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>12644–12656</pages>
      <abstract>Modeling social media users is the core of social governance in the digital society. Existing works have incorporated different digital traces to better learn the representations of social media users, including text information encoded by pre-trained language models and social network information encoded by graph models. However, limited by overloaded text information and hard-to-collect social network information, they cannot utilize global text information and cannot be generalized without social relationships. In this paper, we propose a Pre-training Architecture for Social Media User Modeling based on Text Graph(PASUM). We aggregate all microblogs to represent social media users based on the text graph model and learn the mapping from microblogs to user representation. We further design inter-user and intra-user contrastive learning tasks to inject general structural information into the mapping. In different scenarios, we can represent users based on text, even without social network information. Experimental results on various downstream tasks demonstrate the effectiveness and superiority of our framework.</abstract>
      <url hash="4b7edc43">2024.lrec-main.1107</url>
      <bibkey>wu-etal-2024-pasum-pre</bibkey>
    </paper>
    <paper id="1108">
      <title>Pater Incertus? There Is a Solution: Automatic Discrimination between Cognates and Borrowings for <fixed-case>R</fixed-case>omance Languages</title>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Ioan-Bogdan</first><last>Iordache</last></author>
      <author><first>Alina Maria</first><last>Cristea</last></author>
      <author><first>Simona</first><last>Georgescu</last></author>
      <author><first>Laurentiu</first><last>Zoicas</last></author>
      <pages>12657–12667</pages>
      <abstract>Identifying the type of relationship between words (cognates, borrowings, inherited) provides a deeper insight into the history of a language and allows for a better characterization of language relatedness. In this paper, we propose a computational approach for discriminating between cognates and borrowings, one of the most difficult tasks in historical linguistics. We compare the discriminative power of graphic and phonetic features and we analyze the underlying linguistic factors that prove relevant in the classification task. We perform experiments for pairs of languages in the Romance language family (French, Italian, Spanish, Portuguese, and Romanian), based on a comprehensive database of Romance cognates and borrowings. To our knowledge, this is one of the first attempts of this kind and the most comprehensive in terms of covered languages.</abstract>
      <url hash="4a03b2d5">2024.lrec-main.1108</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ccf1f784">2024.lrec-main.1108.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>dinu-etal-2024-pater-incertus</bibkey>
    </paper>
    <paper id="1109">
      <title><fixed-case>PDAM</fixed-case>eta: Meta-Learning Framework with Progressive Data Augmentation for Few-Shot Text Classification</title>
      <author><first>Xurui</first><last>Li</last></author>
      <author><first>Kaisong</first><last>Song</last></author>
      <author><first>Tianqianjin</first><last>Lin</last></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Fubang</first><last>Zhao</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <pages>12668–12678</pages>
      <abstract>Recently, we have witnessed the breakthroughs of meta-learning for few-shot learning scenario. Data augmentation is essential for meta-learning, particularly in situations where data is extremely scarce. However, existing text data augmentation methods can not ensure the diversity and quality of the generated data, which leads to sub-optimal performance. Inspired by the recent success of large language models (LLMs) which demonstrate improved language comprehension abilities, we propose a Meta-learning framework with Progressive Data Augmentation (PDAMeta) for few-shot text classification, which contains a two-stage data augmentation strategy. First, the prompt-based data augmentation enriches the diversity of the training instances from a global perspective. Second, the attention-based data augmentation further improves the data quality from a local perspective. Last, we propose a dual-stream contrastive meta-learning strategy to learn discriminative text representations from both original and augmented instances. Extensive experiments conducted on four public few-shot text classification datasets show that PDAMeta significantly outperforms several state-of-the-art models and shows better robustness.</abstract>
      <url hash="62258c83">2024.lrec-main.1109</url>
      <attachment type="OptionalSupplementaryMaterial" hash="66727c35">2024.lrec-main.1109.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2024-pdameta-meta</bibkey>
    </paper>
    <paper id="1110">
      <title><fixed-case>PE</fixed-case>a<fixed-case>CE</fixed-case>: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents</title>
      <author><first>Nan</first><last>Zhang</last></author>
      <author><first>Connor</first><last>Heaton</last></author>
      <author><first>Sean Timothy</first><last>Okonsky</last></author>
      <author><first>Prasenjit</first><last>Mitra</last></author>
      <author><first>Hilal Ezgi</first><last>Toraman</last></author>
      <pages>12679–12689</pages>
      <abstract>Optical Character Recognition (OCR) is an established task with the objective of identifying the text present in an image. While many off-the-shelf OCR models exist, they are often trained for either scientific (e.g., formulae) or generic printed English text. Extracting text from chemistry publications requires an OCR model that is capable in both realms. Nougat, a recent tool, exhibits strong ability to parse academic documents, but is unable to parse tables in PubMed articles, which comprises a significant part of the academic community and is the focus of this work. To mitigate this gap, we present the Printed English and Chemical Equations (PEaCE) dataset, containing both synthetic and real-world records, and evaluate the efficacy of transformer-based OCR models when trained on this resource. Given that real-world records contain artifacts not present in synthetic records, we propose transformations that mimic such qualities. We perform a suite of experiments to explore the impact of patch size, multi-domain training, and our proposed transformations, ultimately finding that models with a small patch size trained on multiple domains using the proposed transformations yield the best performance. Our dataset and code is available at https://github.com/ZN1010/PEaCE.</abstract>
      <url hash="6c90f333">2024.lrec-main.1110</url>
      <bibkey>zhang-etal-2024-peace-chemistry</bibkey>
    </paper>
    <paper id="1111">
      <title><fixed-case>PECC</fixed-case>: Problem Extraction and Coding Challenges</title>
      <author><first>Patrick</first><last>Haller</last></author>
      <author><first>Jonas</first><last>Golde</last></author>
      <author><first>Alan</first><last>Akbik</last></author>
      <pages>12690–12699</pages>
      <abstract>Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMs’ capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver.</abstract>
      <url hash="9eae1f69">2024.lrec-main.1111</url>
      <bibkey>haller-etal-2024-pecc-problem</bibkey>
    </paper>
    <paper id="1112">
      <title><fixed-case>P</fixed-case>ejorativ<fixed-case>IT</fixed-case>y: Disambiguating Pejorative Epithets to Improve Misogyny Detection in <fixed-case>I</fixed-case>talian Tweets</title>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Federico</first><last>Ruggeri</last></author>
      <author><first>Cagri</first><last>Toraman</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <author><first>Samuel</first><last>Algherini</last></author>
      <author><first>Lorenzo</first><last>Musetti</last></author>
      <author><first>Silvia</first><last>Ronchi</last></author>
      <author><first>Gianmarco</first><last>Saretto</last></author>
      <author><first>Caterina</first><last>Zapparoli</last></author>
      <pages>12700–12711</pages>
      <abstract>Misogyny is often expressed through figurative language. Some neutral words can assume a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the word level and misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated words into a model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous words with univocal terms. Our experimental results, both on our corpus and on two popular benchmarks on Italian tweets, show that both approaches lead to a major classification improvement, indicating that word sense disambiguation is a promising preliminary step for misogyny detection. Furthermore, we investigate LLMs’ understanding of pejorative epithets by means of contextual word embeddings analysis and prompting.</abstract>
      <url hash="e746ebc6">2024.lrec-main.1112</url>
      <attachment type="OptionalSupplementaryMaterial" hash="38ad9a77">2024.lrec-main.1112.OptionalSupplementaryMaterial.xlsx</attachment>
      <bibkey>muti-etal-2024-pejorativity-disambiguating</bibkey>
    </paper>
    <paper id="1113">
      <title>Persona-aware Multi-party Conversation Response Generation</title>
      <author><first>Khyati</first><last>Mahajan</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>12712–12723</pages>
      <abstract>Modeling interlocutor information is essential towards modeling multi-party conversations to account for the presence of multiple participants. We investigate the role of including the persona attributes of both the speaker and addressee relevant to each utterance, collected via 3 distinct mock social media experiments. The participants were recruited via MTurk, and were unaware of the persona attributes of the other users they interacted with on the platform. Our main contributions include 1) a multi-party conversation dataset with rich associated metadata (including persona), and 2) a persona-aware heterogeneous graph transformer response generation model. We find that PersonaHeterMPC provides a good baseline towards persona-aware generation for multi-party conversation modeling, generating responses which are relevant and consistent with the interlocutor personas relevant to the conversation.</abstract>
      <url hash="98494916">2024.lrec-main.1113</url>
      <bibkey>mahajan-shaikh-2024-persona-aware</bibkey>
    </paper>
    <paper id="1114">
      <title>Phonetic Segmentation of the <fixed-case>UCLA</fixed-case> Phonetics Lab Archive</title>
      <author><first>Eleanor</first><last>Chodroff</last></author>
      <author><first>Blaž</first><last>Pažon</last></author>
      <author><first>Annie</first><last>Baker</last></author>
      <author><first>Steven</first><last>Moran</last></author>
      <pages>12724–12733</pages>
      <abstract>Research in speech technologies and comparative linguistics depends on access to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU re-release as our starting point. VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0. This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel intrinsic f0. We also discuss the utility of the VoxAngeles corpus for general research and pedagogy in crosslinguistic phonetics, as well as for low-resource and multilingual speech technologies. VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.</abstract>
      <url hash="60f0d781">2024.lrec-main.1114</url>
      <bibkey>chodroff-etal-2024-phonetic-segmentation</bibkey>
    </paper>
    <paper id="1115">
      <title>Phonotactic Complexity across Dialects</title>
      <author><first>Ryan Soh-Eun</first><last>Shim</last></author>
      <author><first>Kalvin</first><last>Chang</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <pages>12734–12748</pages>
      <abstract>Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model—a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the strength of the negative correlation observed.</abstract>
      <url hash="0cf814c8">2024.lrec-main.1115</url>
      <bibkey>shim-etal-2024-phonotactic-complexity</bibkey>
    </paper>
    <paper id="1116">
      <title><fixed-case>PILA</fixed-case>: A Historical-Linguistic Dataset of Proto-Italic and <fixed-case>L</fixed-case>atin</title>
      <author><first>Stephen</first><last>Bothwell</last></author>
      <author><first>Brian</first><last>DuSell</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <author><first>Brian</first><last>Krostenko</last></author>
      <pages>12749–12760</pages>
      <abstract>Computational historical linguistics seeks to systematically understand processes of sound change, including during periods at which little to no formal recording of language is attested. At the same time, few computational resources exist which deeply explore phonological and morphological connections between proto-languages and their descendants. This is particularly true for the family of Italic languages. To assist historical linguists in the study of Italic sound change, we introduce the Proto-Italic to Latin (PILA) dataset, which consists of roughly 3,000 pairs of forms from Proto-Italic and Latin. We provide a detailed description of how our dataset was created and organized. Then, we exhibit PILA’s value in two ways. First, we present baseline results for PILA on a pair of traditional computational historical linguistics tasks. Second, we demonstrate PILA’s capability for enhancing other historical-linguistic datasets through a dataset compatibility study.</abstract>
      <url hash="74f6eb73">2024.lrec-main.1116</url>
      <bibkey>bothwell-etal-2024-pila-historical</bibkey>
    </paper>
    <paper id="1117">
      <title><fixed-case>PIRB</fixed-case>: A Comprehensive Benchmark of <fixed-case>P</fixed-case>olish Dense and Hybrid Text Retrieval Methods</title>
      <author><first>Slawomir</first><last>Dadas</last></author>
      <author><first>Michał</first><last>Perełkiewicz</last></author>
      <author><first>Rafał</first><last>Poświata</last></author>
      <pages>12761–12774</pages>
      <abstract>We present Polish Information Retrieval Benchmark (PIRB), a comprehensive evaluation framework encompassing 41 text information retrieval tasks for Polish. The benchmark incorporates existing datasets as well as 10 new, previously unpublished datasets covering diverse topics such as medicine, law, business, physics, and linguistics. We conduct an extensive evaluation of over 20 dense and sparse retrieval models, including the baseline models trained by us as well as other available Polish and multilingual methods. Finally, we introduce a three-step process for training highly effective language-specific retrievers, consisting of knowledge distillation, supervised fine-tuning, and building sparse-dense hybrid retrievers using a lightweight rescoring model. In order to validate our approach, we train new text encoders for Polish and compare their results with previously evaluated methods. Our dense models outperform the best solutions available to date, and the use of hybrid methods further improves their performance.</abstract>
      <url hash="ffd041c5">2024.lrec-main.1117</url>
      <bibkey>dadas-etal-2024-pirb-comprehensive</bibkey>
    </paper>
    <paper id="1118">
      <title><fixed-case>PLAES</fixed-case>: Prompt-generalized and Level-aware Learning Framework for Cross-prompt Automated Essay Scoring</title>
      <author><first>Yuan</first><last>Chen</last></author>
      <author><first>Xia</first><last>Li</last></author>
      <pages>12775–12786</pages>
      <abstract>Current cross-prompt automatic essay scoring (AES) systems are primarily concerned with obtaining shared knowledge specific to the target prompt by using the source and target prompt essays. However, it may not be feasible in practical situations because the target prompt essays may not be available as training data. When constructing a model solely from source prompt essays, its capacity to generalize to the target prompt may be hindered by significant discrepancies among different prompt essays. In this study, a novel learning framework for cross-prompt AES is proposed in order to capture more general knowledge across prompts and improve the model’s capacity to distinguish between writing levels. To acquire generic knowledge across different prompts, a primary model is trained via meta learning with all source prompt essays. To improve the model’s ability to differentiate writing levels, we present a level-aware learning strategy consisting of a general scorer and three level scorers for low-, middle-, and high-level essays. Then, we introduce a contrastive learning strategy to bring the essay representation of the general scorer closer to its corresponding level representation and far away from the other two levels, thereby improving the system’s ability to differentiate writing levels as well as boosting scoring performance. Experimental results on public datasets illustrate the efficacy of our method.</abstract>
      <url hash="30e3e9d9">2024.lrec-main.1118</url>
      <bibkey>chen-li-2024-plaes-prompt</bibkey>
    </paper>
    <paper id="1119">
      <title>Plots Made Quickly: An Efficient Approach for Generating Visualizations from Natural Language Queries</title>
      <author><first>Henrik</first><last>Voigt</last></author>
      <author><first>Kai</first><last>Lawonn</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>12787–12793</pages>
      <abstract>Generating visualizations from natural language queries is a useful extension to visualization libraries such as Vega-Lite. The goal of the NL2VIS task is to generate a valid Vega-Lite specification from a data frame and a natural language query as input, which can then be rendered as a visualization. To enable real-time interaction with the data, small model sizes and fast inferences are required. Previous work has introduced custom neural network solutions with custom visualization specifications and has not systematically tested pre-trained LMs to solve this problem. In this work, we opt for a more generic approach that (i) evaluates pre-trained LMs of different sizes and (ii) uses string encodings of data frames and visualization specifications instead of custom specifications. In our experiments, we show that these representations, in combination with pre-trained LMs, scale better than current state-of-the-art models. In addition, the small and base versions of the T5 architecture achieve real-time interaction, while LLMs far exceed latency thresholds suitable for visual exploration tasks. In summary, our models generate visualization specifications in real-time on a CPU and establish a new state of the art on the NL2VIS benchmark nvBench.</abstract>
      <url hash="e2b0af30">2024.lrec-main.1119</url>
      <bibkey>voigt-etal-2024-plots-made</bibkey>
    </paper>
    <paper id="1120">
      <title>Pluggable Neural Machine Translation Models via Memory-augmented Adapters</title>
      <author><first>Yuzhuang</first><last>Xu</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Xuebo</first><last>Liu</last></author>
      <author><first>Xiaolong</first><last>Wang</last></author>
      <author><first>Weidong</first><last>Liu</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>12794–12808</pages>
      <abstract>Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.</abstract>
      <url hash="35d54429">2024.lrec-main.1120</url>
      <bibkey>xu-etal-2024-pluggable-neural</bibkey>
    </paper>
    <paper id="1121">
      <title>Pointing Out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials</title>
      <author><first>Gennaro</first><last>Nolano</last></author>
      <author><first>Moritz</first><last>Blum</last></author>
      <author><first>Basil</first><last>Ell</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>12809–12820</pages>
      <abstract>In recent years, large language models have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples. For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it. For example, consider the sentence “Leonardo da Vinci painted the Mona Lisa” expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute “Leonardo da Vinci” with “Barack Obama”, then the sentence still expresses the created relation. A robust model is supposed to detect the same relation in both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigate how state-of-the-art RE models perform under pressure. Our analyses show that the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent on shortcuts, such as surface forms (or patterns therein) of entities, without making full use of the information present in the sentences.</abstract>
      <url hash="c5c1f2a4">2024.lrec-main.1121</url>
      <bibkey>nolano-etal-2024-pointing-shortcomings</bibkey>
    </paper>
    <paper id="1122">
      <title><fixed-case>P</fixed-case>olish-<fixed-case>ASTE</fixed-case>: Aspect-Sentiment Triplet Extraction Datasets for <fixed-case>P</fixed-case>olish</title>
      <author><first>Marta</first><last>Lango</last></author>
      <author><first>Borys</first><last>Naglik</last></author>
      <author><first>Mateusz</first><last>Lango</last></author>
      <author><first>Iwo</first><last>Naglik</last></author>
      <pages>12821–12828</pages>
      <abstract>Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and complex tasks in sentiment analysis. It concerns the construction of triplets that contain an aspect, its associated sentiment polarity, and an opinion phrase that serves as a rationale for the assigned polarity. Despite the growing popularity of the task and the many machine learning methods being proposed to address it, the number of datasets for ASTE is very limited. In particular, no dataset is available for any of the Slavic languages. In this paper, we present two new datasets for ASTE containing customer opinions about hotels and purchased products expressed in Polish. We also perform experiments with two ASTE techniques combined with two large language models for Polish to investigate their performance and the difficulty of the assembled datasets. The new datasets are available under a permissive licence and have the same file format as the English datasets, facilitating their use in future research.</abstract>
      <url hash="bb3633d1">2024.lrec-main.1122</url>
      <bibkey>lango-etal-2024-polish-aste</bibkey>
    </paper>
    <paper id="1123">
      <title><fixed-case>P</fixed-case>olish Discourse Corpus (<fixed-case>PDC</fixed-case>): Corpus Design, <fixed-case>ISO</fixed-case>-Compliant Annotation, Data Highlights, and Parser Development</title>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Aleksandra</first><last>Tomaszewska</last></author>
      <author><first>Daniel</first><last>Ziembicki</last></author>
      <author><first>Sebastian</first><last>Żurowski</last></author>
      <author><first>Ryszard</first><last>Tuora</last></author>
      <author><first>Aleksandra</first><last>Zwierzchowska</last></author>
      <pages>12829–12835</pages>
      <abstract>This paper presents the Polish Discourse Corpus, a pioneering resource of this kind for Polish and the first corpus in Poland to employ the ISO standard for discourse relation annotation. The Polish Discourse Corpus adopts ISO 24617-8, a segment of the Language Resource Management – Semantic Annotation Framework (SemAF), which outlines a set of core discourse relations adaptable for diverse languages and genres. The paper overviews the corpus architecture, annotation procedures, the challenges that the annotators have encountered, as well as key statistical data concerning discourse relations and connectives in the corpus. It further discusses the initial phases of the discourse parser tailored for the ISO 24617-8 framework. Evaluations on the efficacy and potential refinement areas of the corpus annotation and parsing strategies are also presented. The final part of the paper touches upon anticipated research plans to improve discourse analysis techniques in the project and to conduct discourse studies involving multiple languages.</abstract>
      <url hash="b1450525">2024.lrec-main.1123</url>
      <bibkey>ogrodniczuk-etal-2024-polish-discourse</bibkey>
    </paper>
    <paper id="1124">
      <title><fixed-case>P</fixed-case>oliti<fixed-case>C</fixed-case>ause: An Annotation Scheme and Corpus for Causality in Political Texts</title>
      <author><first>Paulina</first><last>Garcia Corral</last></author>
      <author><first>Hanna</first><last>Bechara</last></author>
      <author><first>Ran</first><last>Zhang</last></author>
      <author><first>Slava</first><last>Jankin</last></author>
      <pages>12836–12845</pages>
      <abstract>In this paper, we present PolitiCAUSE, a new corpus of political texts annotated for causality. We provide a detailed and robust annotation scheme for annotating two types of information: (1) whether a sentence contains a causal relation or not, and (2) the spans of text that correspond to the cause and effect components of the causal relation. We also provide statistics and analysis of the corpus, and outline the difficulties and limitations of the task. Finally, we test out two transformer-based classification models on our dataset as a form of evaluation. The models achieve a moderate performance on the dataset, with a MCC score of 0.62. Our results show that PolitiCAUSE is a valuable resource for studying causality in texts, especially in the domain of political discourse, and that there is still room for improvement in developing more accurate and robust methods for this problem.</abstract>
      <url hash="a3efe70b">2024.lrec-main.1124</url>
      <bibkey>garcia-corral-etal-2024-politicause-annotation</bibkey>
    </paper>
    <paper id="1125">
      <title><fixed-case>P</fixed-case>ol<fixed-case>QA</fixed-case>: <fixed-case>P</fixed-case>olish Question Answering Dataset</title>
      <author><first>Piotr</first><last>Rybak</last></author>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <pages>12846–12855</pages>
      <abstract>Recently proposed systems for open-domain question answering (OpenQA) require large amounts of training data to achieve state-of-the-art performance. However, data annotation is known to be time-consuming and therefore expensive to acquire. As a result, the appropriate datasets are available only for a handful of languages (mainly English and Chinese). In this work, we introduce and publicly release PolQA, the first Polish dataset for OpenQA. It consists of 7,000 questions, 87,525 manually labeled evidence passages, and a corpus of over 7,097,322 candidate passages. Each question is classified according to its formulation, type, as well as entity type of the answer. This resource allows us to evaluate the impact of different annotation choices on the performance of the QA system and propose an efficient annotation strategy that increases the passage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation cost by 82%.</abstract>
      <url hash="8c52eda6">2024.lrec-main.1125</url>
      <bibkey>rybak-etal-2024-polqa-polish</bibkey>
    </paper>
    <paper id="1126">
      <title><fixed-case>P</fixed-case>oly<fixed-case>NERE</fixed-case>: A Novel Ontology and Corpus for Named Entity Recognition and Relation Extraction in Polymer Science Domain</title>
      <author><first>Van-Thuy</first><last>Phi</last></author>
      <author><first>Hiroki</first><last>Teranishi</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <author><first>Hiroyuki</first><last>Oka</last></author>
      <author><first>Masashi</first><last>Ishii</last></author>
      <pages>12856–12866</pages>
      <abstract>Polymers are widely used in diverse fields, and the demand for efficient methods to extract and organize information about them is increasing. An automated approach that utilizes machine learning can accurately extract relevant information from scientific papers, providing a promising solution for automating information extraction using annotated training data. In this paper, we introduce a polymer-relevant ontology featuring crucial entities and relations to enhance information extraction in the polymer science field. Our ontology is customizable to adapt to specific research needs. We present PolyNERE, a high-quality named entity recognition (NER) and relation extraction (RE) corpus comprising 750 polymer abstracts annotated using our ontology. Distinctive features of PolyNERE include multiple entity types, relation categories, support for various NER settings, and the ability to assert entities and relations at different levels. PolyNERE also facilitates reasoning in the RE task through supporting evidence. While our experiments with recent advanced methods achieved promising results, challenges persist in adapting NER and RE from abstracts to full-text paragraphs. This emphasizes the need for robust information extraction systems in the polymer domain, making our corpus a valuable benchmark for future developments.</abstract>
      <url hash="b5b7679c">2024.lrec-main.1126</url>
      <bibkey>phi-etal-2024-polynere-novel</bibkey>
    </paper>
    <paper id="1127">
      <title><fixed-case>P</fixed-case>op<fixed-case>ALM</fixed-case>: Popularity-Aligned Language Models for Social Media Trendy Response Prediction</title>
      <author><first>Erxin</first><last>Yu</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <pages>12867–12878</pages>
      <abstract>Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning. Recognizing the noisy labels from user “likes”, we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.</abstract>
      <url hash="580f5faf">2024.lrec-main.1127</url>
      <bibkey>yu-etal-2024-popalm-popularity</bibkey>
    </paper>
    <paper id="1128">
      <title><fixed-case>P</fixed-case>op<fixed-case>A</fixed-case>ut: An Annotated Corpus for Populism Detection in <fixed-case>A</fixed-case>ustrian News Comments</title>
      <author><first>Ahmadou</first><last>Wagne</last></author>
      <author><first>Julia</first><last>Neidhardt</last></author>
      <author><first>Thomas Elmar</first><last>Kolb</last></author>
      <pages>12879–12892</pages>
      <abstract>Populism is a phenomenon that is noticeably present in the political landscape of various countries over the past decades. While populism expressed by politicians has been thoroughly examined in the literature, populism expressed by citizens is still underresearched, especially when it comes to its automated detection in text. This work presents the PopAut corpus, which is the first annotated corpus of news comments for populism in the German language. It features 1,200 comments collected between 2019-2021 that are annotated for populist motives anti-elitism, people-centrism and people-sovereignty. Following the definition of Cas Mudde, populism is seen as a thin ideology. This work shows that annotators reach a high agreement when labeling news comments for these motives. The data set is collected to serve as the basis for automated populism detection using machine-learning methods. By using transformer-based models, we can outperform existing dictionaries tailored for automated populism detection in German social media content. Therefore our work provides a rich resource for future work on the classification of populist user comments in the German language.</abstract>
      <url hash="de0462f4">2024.lrec-main.1128</url>
      <bibkey>wagne-etal-2024-popaut-annotated</bibkey>
    </paper>
    <paper id="1129">
      <title>Positive and Risky Message Assessment for Music Products</title>
      <author><first>Yigeng</first><last>Zhang</last></author>
      <author><first>Mahsa</first><last>Shafaei</last></author>
      <author><first>Fabio</first><last>Gonzalez</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>12893–12905</pages>
      <abstract>In this work, we introduce a pioneering research challenge: evaluating positive and potentially harmful messages within music products. We initiate by setting a multi-faceted, multi-task benchmark for music content assessment. Subsequently, we introduce an efficient multi-task predictive model fortified with ordinality-enforcement to address this challenge. Our findings reveal that the proposed method not only significantly outperforms robust task-specific alternatives but also possesses the capability to assess multiple aspects simultaneously. Furthermore, through detailed case studies, where we employed Large Language Models (LLMs) as surrogates for content assessment, we provide valuable insights to inform and guide future research on this topic. The code for dataset creation and model implementation is publicly available at https://github.com/RiTUAL-UH/music-message-assessment.</abstract>
      <url hash="1e4cffd8">2024.lrec-main.1129</url>
      <bibkey>zhang-etal-2024-positive-risky</bibkey>
    </paper>
    <paper id="1130">
      <title><fixed-case>POS</fixed-case> Tagging for the Endangered Dagur Language</title>
      <author><first>Joanna</first><last>Dolińska</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <pages>12906–12916</pages>
      <abstract>The application of natural language processing tools opens new ways for the documentation and revitalization of under-resourced languages. In this article we aim to investigate the feasibility of automatic part-of-speech (POS) tagging for Dagur, which is an endangered Mongolic language spoken mainly in northeast China, with no official written standard for all Dagur dialects. We present a new manually annotated corpus for Dagur, which includes about 1,200 tokens, and detail the decisions made during the annotation process. This corpus is used to test transfer of models from other languages, especially from Buryat, which is currently the only Mongolic language included in the Universal Dependencies corpora. We applied the models trained by de Vries et al. (2022) to the Dagur corpus and continued training these models on Buryat. We analyse the results with respect to language families, script and POS distribution, in three different zero-shot settings: (1) unrelated, (2) related and (3) unrelated+related language.</abstract>
      <url hash="a5a355b2">2024.lrec-main.1130</url>
      <bibkey>dolinska-bernhard-2024-pos-tagging</bibkey>
    </paper>
    <paper id="1131">
      <title>Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview</title>
      <author><first>Heyang</first><last>Liu</last></author>
      <author><first>Yanfeng</first><last>Wang</last></author>
      <author><first>Yu</first><last>Wang</last></author>
      <pages>12917–12926</pages>
      <abstract>End-to-end (E2E) approach is gradually replacing hybrid models for automatic speech recognition (ASR) tasks. However, the optimization of E2E models lacks an intuitive method for handling decoding shifts, especially in scenarios with a large number of domain-specific rare words that hold specific important meanings. Furthermore, the absence of knowledge-intensive speech datasets in academia has been a significant limiting factor, and the commonly used speech corpora exhibit significant disparities with realistic conversation. To address these challenges, we present Medical Interview (MED-IT), a multi-turn consultation speech dataset that contains a substantial number of knowledge-intensive named entities. We also explore methods to enhance the recognition performance of rare words for E2E models. We propose a novel approach, post-decoder biasing, which constructs a transform probability matrix based on the distribution of training transcriptions. This guides the model to prioritize recognizing words in the biasing list. In our experiments, for subsets of rare words appearing in the training speech between 10 and 20 times, and between 1 and 5 times, the proposed method achieves a relative improvement of 9.3% and 5.1%, respectively.</abstract>
      <url hash="aed0acf2">2024.lrec-main.1131</url>
      <bibkey>liu-etal-2024-post-decoder</bibkey>
    </paper>
    <paper id="1132">
      <title><fixed-case>PPORTAL</fixed-case>_ner: An Annotated Corpus of <fixed-case>P</fixed-case>ortuguese Literary Entities</title>
      <author><first>Mariana O.</first><last>Silva</last></author>
      <author><first>Mirella M.</first><last>Moro</last></author>
      <pages>12927–12937</pages>
      <abstract>The intersection of natural language processing (NLP) and literary analysis has yielded valuable insights and applications across various languages. However, the scarcity of labeled data tailored for Portuguese literary texts poses a notable challenge. To address this gap, we present the PPORTAL_ner corpus, an annotated dataset that simplifies the development of Named Entity Recognition (NER) models specifically adapted for Portuguese literary works. Our corpus includes annotations of PER, LOC, GPE, ORG, and DATE entities within a diverse set of 25 literary texts. Annotation of the corpus involved a two-step process: initial pre-annotation using a pre-trained spaCy model followed by correction and refinement using the Prodigy annotation tool. With a total of 125,059 tokens and 5,266 annotated entities, PPORTAL_ner corpus significantly enriches the landscape of resources available for computational literary analysis in Portuguese. This paper details the annotation methodology, guidelines, and dataset statistics while also evaluating four NER models over the PPORTAL_ner corpus. Our evaluation analysis reveals that fine-tuning on domain-specific data significantly improves NER model performance, demonstrating the value of the PPORTAL_ner corpus for developing domain-specific language models.</abstract>
      <url hash="8c3b048a">2024.lrec-main.1132</url>
      <attachment type="OptionalSupplementaryMaterial" hash="18d851db">2024.lrec-main.1132.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>silva-moro-2024-pportal-ner</bibkey>
    </paper>
    <paper id="1133">
      <title>Predictive and Distinctive Linguistic Features in Schizophrenia-Bipolar Spectrum Disorders</title>
      <author><first>Martina Katalin</first><last>Szabó</last></author>
      <author><first>Veronika</first><last>Vincze</last></author>
      <author><first>Bernadett</first><last>Dam</last></author>
      <author><first>Csenge</first><last>Guba</last></author>
      <author><first>Anita</first><last>Bagi</last></author>
      <author><first>István</first><last>Szendi</last></author>
      <pages>12938–12953</pages>
      <abstract>In this study, we analyze spontaneous speech transcripts from Hungarian patients with schizophrenia, schizoaffective, and bipolar disorders. Our goal is to identify distinctive linguistic features in these patient groups and controls. To our knowledge, no prior study has systematically examined the linguistic features of these disorders or explored their use in distinguishing between these patient groups. We collected recordings from 77 participants during three directed spontaneous speech tasks in a clinical setting, resulting in 458 texts. Our research group manually transcribed the recordings. We processed the written corpus texts using Natural Language Processing methods and tools. The final corpus consists of 179,515 tokens, excluding punctuation. Using this data, we analyze different linguistic features’ predictive power by computing and comparing their frequency distributions. We then attempt to automatically differentiate between patient groups and controls using our extensive set of linguistic features, employing the random forest algorithm in these experiments. Our results indicate that applying machine learning techniques based on distinctive features can effectively distinguish SZ, SAD, BD, and controls, surpassing baseline results.</abstract>
      <url hash="3230c09f">2024.lrec-main.1133</url>
      <bibkey>szabo-etal-2024-predictive-distinctive</bibkey>
    </paper>
    <paper id="1134">
      <title>Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning</title>
      <author><first>Guisheng</first><last>Liu</last></author>
      <author><first>Yi</first><last>Li</last></author>
      <author><first>Zhengcong</first><last>Fei</last></author>
      <author><first>Haiyan</first><last>Fu</last></author>
      <author><first>Xiangyang</first><last>Luo</last></author>
      <author><first>Yanqing</first><last>Guo</last></author>
      <pages>12954–12965</pages>
      <abstract>While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.</abstract>
      <url hash="9e95e67a">2024.lrec-main.1134</url>
      <bibkey>liu-etal-2024-prefix-diffusion</bibkey>
    </paper>
    <paper id="1135">
      <title>Pre-Trained Language Models Represent Some Geographic Populations Better than Others</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <author><first>Benjamin</first><last>Adams</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <pages>12966–12976</pages>
      <abstract>This paper measures the skew in how well two families of LLMs represent diverse geographic populations. A spatial probing task is used with geo-referenced corpora to measure the degree to which pre-trained language models from the OPT and BLOOM series represent diverse populations around the world. Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented. Analysis shows that both families of models largely share the same skew across populations. At the same time, this skew cannot be fully explained by sociolinguistic factors, economic factors, or geographic factors. The basic conclusion from this analysis is that pre-trained models do not equally represent the world’s population: there is a strong skew towards specific geographic populations. This finding challenges the idea that a single model can be used for all populations.</abstract>
      <url hash="41f488a8">2024.lrec-main.1135</url>
      <bibkey>dunn-etal-2024-pre-trained</bibkey>
    </paper>
    <paper id="1136">
      <title>Pre-training Cross-Modal Retrieval by Expansive Lexicon-Patch Alignment</title>
      <author><first>Yang</first><last>Yiyuan</last></author>
      <author><first>Guodong</first><last>Long</last></author>
      <author><first>Michael</first><last>Blumenstein</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>12977–12987</pages>
      <abstract>Recent large-scale vision-language pre-training depends on image-text global alignment by contrastive learning and is further boosted by fine-grained alignment in a weakly contrastive manner for cross-modal retrieval. Nonetheless, besides semantic matching learned by contrastive learning, cross-modal retrieval also largely relies on object matching between modalities. This necessitates fine-grained categorical discriminative learning, which however suffers from scarce data in full-supervised scenarios and information asymmetry in weakly-supervised scenarios when applied to cross-modal retrieval. To address these issues, we propose expansive lexicon-patch alignment (ELA) to align image patches with a vocabulary rather than only the words explicitly in the text for annotation-free alignment and information augmentation, thus enabling more effective fine-grained categorical discriminative learning for cross-modal retrieval. Experimental results show that ELA could effectively learn representative fine-grained information and outperform state-of-the-art methods on cross-modal retrieval.</abstract>
      <url hash="85098151">2024.lrec-main.1136</url>
      <bibkey>yiyuan-etal-2024-pre-training</bibkey>
    </paper>
    <paper id="1137">
      <title><fixed-case>PRIMO</fixed-case>: Progressive Induction for Multi-hop Open Rule Generation</title>
      <author><first>Jianyu</first><last>Liu</last></author>
      <author><first>Sheng</first><last>Bi</last></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <pages>12988–12998</pages>
      <abstract>Open rules refer to the implication from premise atoms to hypothesis atoms, which captures various relationships between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring scenarios involving multiple hops, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and rank modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model’s understanding of commonsense knowledge. Experimental results demonstrate that compared to baseline models, PRIMO significantly enhances rule quality and diversity while reducing the repetition rate of rule atoms.</abstract>
      <url hash="307ac1cd">2024.lrec-main.1137</url>
      <bibkey>liu-etal-2024-primo-progressive</bibkey>
    </paper>
    <paper id="1138">
      <title>Principal Component Analysis as a Sanity Check for <fixed-case>B</fixed-case>ayesian Phylolinguistic Reconstruction</title>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <pages>12999–13013</pages>
      <abstract>Bayesian approaches to reconstructing the evolutionary history of languages rely on the tree model, which assumes that these languages descended from a common ancestor and underwent modifications over time. However, this assumption can be violated to different extents due to contact and other factors. Understanding the degree to which this assumption is violated is crucial for validating the accuracy of phylolinguistic inference. In this paper, we propose a simple sanity check: projecting a reconstructed tree onto a space generated by principal component analysis. By using both synthetic and real data, we demonstrate that our method effectively visualizes anomalies, particularly in the form of jogging.</abstract>
      <url hash="975a9cb5">2024.lrec-main.1138</url>
      <bibkey>murawaki-2024-principal-component</bibkey>
    </paper>
    <paper id="1139">
      <title>Prior Relational Schema Assists Effective Contrastive Learning for Inductive Knowledge Graph Completion</title>
      <author><first>Ruilin</first><last>Luo</last></author>
      <author><first>Jiayi</first><last>Li</last></author>
      <author><first>Jianghangfan</first><last>Zhang</last></author>
      <author><first>Jing</first><last>Xiao</last></author>
      <author><first>Yujiu</first><last>Yang</last></author>
      <pages>13014–13025</pages>
      <abstract>Knowledge Graph Completion (KGC) is a task aimed at uncovering the inherent relationships among known knowledge triplets in a Knowledge Graph (KG) and subsequently predicting missing links. Presently, there is a rising interest in inductive knowledge graph completion, where missing links may pertain to previously unobserved entities. Previous inductive KGC methods mainly rely on descriptive information of entities to improve the representation of unseen entities, neglecting to provide effective prior knowledge for relation modeling. To tackle this challenge, we capture prior schema-level interactions related to relations by leveraging entity type information, thereby furnishing effective prior constraints when reasoning with newly introduced entities. Moreover, We employ normal in-batch negatives and introduce schema-guided negatives to bolster the efficiency of normal contrastive representation learning. Experimental results demonstrate that our approach consistently achieves state-of-the-art performance on various established metrics across multiple benchmark datasets for link prediction. Notably, our method achieves a 20.5% relative increase in Hits@1 on the HumanWiki-Ind dataset.</abstract>
      <url hash="93c540db">2024.lrec-main.1139</url>
      <bibkey>luo-etal-2024-prior-relational</bibkey>
    </paper>
    <paper id="1140">
      <title>Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models</title>
      <author><first>Yichun</first><last>Zhao</last></author>
      <author><first>Shuheng</first><last>Zhou</last></author>
      <author><first>Huijia</first><last>Zhu</last></author>
      <pages>13026–13032</pages>
      <abstract>Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into Small Language Models (SLMs), with a significant focus on transferring the reasoning capabilities of LLMs to SLMs via CoT. However, the outcomes of CoT distillation are inadequate for knowledge-intensive reasoning tasks. This is because generating accurate rationales requires crucial factual knowledge, which SLMs struggle to retain due to their parameter constraints. We propose a retrieval-based CoT distillation framework, named Probe then Retrieve and Reason (PRR), which distills the question probing and reasoning capabilities from LLMs into SLMs. We train two complementary distilled SLMs, a probing model and a reasoning model, in tandem. When presented with a new question, the probing model first identifies the necessary knowledge to answer it, generating queries for retrieval. Subsequently, the reasoning model uses the retrieved knowledge to construct a step-by-step rationale for the answer. In knowledge-intensive reasoning tasks, such as StrategyQA and OpenbookQA, our distillation framework yields superior performance for SLMs compared to conventional methods, including simple CoT distillation and knowledge-augmented distillation using raw questions.</abstract>
      <url hash="2c932d32">2024.lrec-main.1140</url>
      <bibkey>zhao-etal-2024-probe-retrieve</bibkey>
    </paper>
    <paper id="1141">
      <title>Probing Large Language Models for Scalar Adjective Lexical Semantics and Scalar Diversity Pragmatics</title>
      <author><first>Fangru</first><last>Lin</last></author>
      <author><first>Daniel</first><last>Altshuler</last></author>
      <author><first>Janet B.</first><last>Pierrehumbert</last></author>
      <pages>13033–13049</pages>
      <abstract>Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.</abstract>
      <url hash="fcbe605a">2024.lrec-main.1141</url>
      <bibkey>lin-etal-2024-probing-large</bibkey>
    </paper>
    <paper id="1142">
      <title>Probing Multimodal Large Language Models for Global and Local Semantic Representations</title>
      <author><first>Mingxu</first><last>Tao</last></author>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Liwei</first><last>Chen</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>13050–13056</pages>
      <abstract>The advancement of Multimodal Large Language Models (MLLMs) has greatly accelerated the development of applications in understanding integrated texts and images. Recent works leverage image-caption datasets to train MLLMs, achieving state-of-the-art performance on image-to-text tasks. However, there are few studies exploring which layers of MLLMs make the most effort to the global image information, which plays vital roles in multimodal comprehension and generation. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models regarding local semantic representations through object recognition tasks. We find that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information. Our code and data are released via https://github.com/kobayashikanna01/probing_MLLM_rep.</abstract>
      <url hash="4204028b">2024.lrec-main.1142</url>
      <bibkey>tao-etal-2024-probing-multimodal</bibkey>
    </paper>
    <paper id="1143">
      <title><fixed-case>P</fixed-case>ro<fixed-case>CQA</fixed-case>: A Large-scale Community-based Programming Question Answering Dataset for Code Search</title>
      <author><first>Zehan</first><last>Li</last></author>
      <author><first>Jianfei</first><last>Zhang</last></author>
      <author><first>Chuantao</first><last>Yin</last></author>
      <author><first>Yuanxin</first><last>Ouyang</last></author>
      <author><first>Wenge</first><last>Rong</last></author>
      <pages>13057–13067</pages>
      <abstract>Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.</abstract>
      <url hash="27cf7776">2024.lrec-main.1143</url>
      <bibkey>li-etal-2024-procqa-large</bibkey>
    </paper>
    <paper id="1144">
      <title><fixed-case>PRODIS</fixed-case> - a Speech Database and a Phoneme-based Language Model for the Study of Predictability Effects in <fixed-case>P</fixed-case>olish</title>
      <author><first>Zofia</first><last>Malisz</last></author>
      <author><first>Jan</first><last>Foremski</last></author>
      <author><first>Małgorzata</first><last>Kul</last></author>
      <pages>13068–13073</pages>
      <abstract>We present a speech database and a phoneme-level language model of Polish. The database and model are designed for the analysis of prosodic and discourse factors interacting with predictability effects on acoustic parameters. The database is also the first large, publicly available Polish speech corpus of excellent acoustic quality that can be used for phonetic analysis and training of multi-speaker speech technology systems. The speech in the database is processed in a pipeline that achieves a 90% degree of automation. It incorporates state-of-the-art, freely available tools enabling database expansion or adaptation to additional languages.</abstract>
      <url hash="5a26e478">2024.lrec-main.1144</url>
      <bibkey>malisz-etal-2024-prodis-speech</bibkey>
    </paper>
    <paper id="1145">
      <title>Producing a Parallel <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank of <fixed-case>A</fixed-case>ncient <fixed-case>H</fixed-case>ebrew and <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek via Cross-Lingual Projection</title>
      <author><first>Daniel G.</first><last>Swanson</last></author>
      <author><first>Bryce D.</first><last>Bussert</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>13074–13078</pages>
      <abstract>In this paper we present the initial construction of a treebank of Ancient Greek containing portions of the Septuagint, a translation of the Hebrew Scriptures (1576 sentences, 39K tokens, roughly 7% of the total corpus). We construct the treebank by word-aligning and projecting from the parallel text in Ancient Hebrew before automatically correcting systematic syntactic mismatches and manually correcting other errors.</abstract>
      <url hash="dead6ee2">2024.lrec-main.1145</url>
      <bibkey>swanson-etal-2024-producing-parallel</bibkey>
    </paper>
    <paper id="1146">
      <title>Projective Methods for Mitigating Gender Bias in Pre-trained Language Models</title>
      <author><first>Hillary</first><last>Dawkins</last></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Daniel</first><last>Gillis</last></author>
      <author><first>Judi</first><last>McCuaig</last></author>
      <pages>13079–13091</pages>
      <abstract>Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT’s internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT’s next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model.</abstract>
      <url hash="b0ca5d82">2024.lrec-main.1146</url>
      <bibkey>dawkins-etal-2024-projective-methods</bibkey>
    </paper>
    <paper id="1147">
      <title>Project <fixed-case>MOSLA</fixed-case>: Recording Every Moment of Second Language Acquisition</title>
      <author><first>Masato</first><last>Hagiwara</last></author>
      <author><first>Joshua B.</first><last>Tanner</last></author>
      <pages>13092–13102</pages>
      <abstract>Second language acquisition (SLA) is a complex and dynamic process. Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process). In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom. The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and fine-tuned state-of-the-art speech models. Our experiments reveal linguistic insights into learners’ proficiency development over time, as well as the potential for automatically detecting the areas of focus on the screen purely from the unannotated multimodal data. Our dataset is freely available for research purposes and can serve as a valuable resource for a wide range of applications, including but not limited to SLA, proficiency assessment, language and speech processing, pedagogy, and multimodal learning analytics.</abstract>
      <url hash="56baee9c">2024.lrec-main.1147</url>
      <bibkey>hagiwara-tanner-2024-project-mosla</bibkey>
    </paper>
    <paper id="1148">
      <title><fixed-case>PROM</fixed-case>: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization</title>
      <author><first>Xinbei</first><last>Ma</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Pengcheng</first><last>He</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>13103–13119</pages>
      <abstract>Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In the zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness. Our code is publicly available at https://github.com/xbmxb/PROM.</abstract>
      <url hash="3e76d892">2024.lrec-main.1148</url>
      <bibkey>ma-etal-2024-prom-phrase</bibkey>
    </paper>
    <paper id="1149">
      <title><fixed-case>P</fixed-case>rom<fixed-case>IS</fixed-case>e: Releasing the Capabilities of <fixed-case>LLM</fixed-case>s with Prompt Introspective Search</title>
      <author><first>Minzheng</first><last>Wang</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Jiahao</first><last>Zhao</last></author>
      <author><first>Yin</first><last>Luo</last></author>
      <author><first>Wenji</first><last>Mao</last></author>
      <pages>13120–13130</pages>
      <abstract>The development of large language models (LLMs) raises the importance of assessing the fairness and completeness of various evaluation benchmarks. Regrettably, these benchmarks predominantly utilize uniform manual prompts, which may not fully capture the expansive capabilities of LLMs—potentially leading to an underestimation of their performance. To unlock the potential of LLMs, researchers pay attention to automated prompt search methods, which employ LLMs as optimizers to discover optimal prompts. However, previous methods generate the solutions implicitly, which overlook the underlying thought process and lack explicit feedback. In this paper, we propose a novel prompt introspective search framework, namely PromISe, to better release the capabilities of LLMs. It converts the process of optimizing prompts into an explicit chain of thought, through a step-by-step procedure that integrates self-introspect and self-refine. Extensive experiments, conducted over 73 tasks on two major benchmarks, demonstrate that our proposed PromISe significantly boosts the performance of 12 well-known LLMs compared to the baseline approach. Moreover, our study offers enhanced insights into the interaction between humans and LLMs, potentially serving as a foundation for future designs and implementations. Keywords: large language models, prompt search, self-introspect, self-refine</abstract>
      <url hash="79e23a03">2024.lrec-main.1149</url>
      <attachment type="OptionalSupplementaryMaterial" hash="8d457c9d">2024.lrec-main.1149.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>wang-etal-2024-promise-releasing</bibkey>
    </paper>
    <paper id="1150">
      <title>Prompt-based Generation of Natural Language Explanations of Synthetic Lethality for Cancer Drug Discovery</title>
      <author><first>Ke</first><last>Zhang</last></author>
      <author><first>Yimiao</first><last>Feng</last></author>
      <author><first>Jie</first><last>Zheng</last></author>
      <pages>13131–13142</pages>
      <abstract>Synthetic lethality (SL) offers a promising approach for targeted anti-cancer therapy. Deeply understanding SL gene pair mechanisms is vital for anti-cancer drug discovery. However, current wet-lab and machine learning-based SL prediction methods lack user-friendly and quantitatively evaluable explanations. To address these problems, we propose a prompt-based pipeline for generating natural language explanations. We first construct a natural language dataset named NexLeth. This dataset is derived from New Bing through prompt-based queries and expert annotations and contains 707 instances. NexLeth enhances the understanding of SL mechanisms and it is a benchmark for evaluating SL explanation methods. For the task of natural language generation for SL explanations, we combine subgraph explanations from an SL knowledge graph (KG) with instructions to construct novel personalized prompts, so as to inject the domain knowledge into the generation process. We then leverage the prompts to fine-tune pre-trained biomedical language models on our dataset. Experimental results show that the fine-tuned model equipped with designed prompts performs better than existing biomedical language models in terms of text quality and explainability, suggesting the potential of our dataset and the fine-tuned model for generating understandable and reliable explanations of SL mechanisms.</abstract>
      <url hash="0f721918">2024.lrec-main.1150</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5c6057d1">2024.lrec-main.1150.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>zhang-etal-2024-prompt-based</bibkey>
    </paper>
    <paper id="1151">
      <title>Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation</title>
      <author><first>Jiaying</first><last>Gong</last></author>
      <author><first>Hoda</first><last>Eldardiry</last></author>
      <pages>13143–13156</pages>
      <abstract>In relation triplet extraction (RTE), recognizing unseen relations for which there are no training instances is a challenging task. Efforts have been made to recognize unseen relations based on question-answering models or relation descriptions. However, these approaches miss the semantic information about connections between seen and unseen relations. In this paper, We propose a prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize unseen relations under the zero-shot setting. We present a new word-level analogy-based sentence translation rule and generate augmented instances with unseen relations from instances with seen relations using that new rule. We design prompts with weighted virtual label construction based on an external knowledge graph to integrate semantic knowledge information learned from seen relations. Instead of using the actual label sets in the prompt template, we construct weighted virtual label words. We learn the representations of both seen and unseen relations with augmented instances and prompts. We then calculate the distance between the generated representations using prototypical networks to predict unseen relations. Extensive experiments conducted on three public datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperforms other methods under zero-shot setting. Results also demonstrate the effectiveness and robustness of ZS-SKA.</abstract>
      <url hash="86305bcc">2024.lrec-main.1151</url>
      <bibkey>gong-eldardiry-2024-prompt-based</bibkey>
    </paper>
    <paper id="1152">
      <title>Prompt-fused Framework for Inductive Logical Query Answering</title>
      <author><first>Zezhong</first><last>Xu</last></author>
      <author><first>Wen</first><last>Zhang</last></author>
      <author><first>Peng</first><last>Ye</last></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>13157–13167</pages>
      <abstract>Answering logical queries on knowledge graphs (KG) poses a significant challenge for machine reasoning. The primary obstacle in this task stems from the inherent incompleteness of KGs. Existing research has predominantly focused on addressing the issue of missing edges in KGs, thereby neglecting another aspect of incompleteness: the emergence of new entities. Furthermore, most of the existing methods tend to reason over each logical operator separately, rather than comprehensively analyzing the query as a whole during the reasoning process. In this paper, we propose a query-aware prompt-fused framework named Pro-QE, which could incorporate existing query embedding methods and address the embedding of emerging entities through contextual information aggregation. Additionally, a query prompt, which is generated by encoding the symbolic query, is introduced to gather information relevant to the query from a holistic perspective. To evaluate the efficacy of our model in the inductive setting, we introduce two new challenging benchmarks. Experimental results demonstrate that our model successfully handles the issue of unseen entities in logical queries. Furthermore, the ablation study confirms the efficacy of the aggregator and prompt components.</abstract>
      <url hash="38f242a5">2024.lrec-main.1152</url>
      <bibkey>xu-etal-2024-prompt-fused</bibkey>
    </paper>
    <paper id="1153">
      <title>Prompting-based Synthetic Data Generation for Few-Shot Question Answering</title>
      <author><first>Maximilian</first><last>Schmidt</last></author>
      <author><first>Andrea</first><last>Bartezzaghi</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>13168–13178</pages>
      <abstract>Although language models (LMs) have boosted the performance of Question Answering, they still need plenty of data. Data annotation, in contrast, is a time-consuming process. This especially applies to Question Answering, where possibly large documents have to be parsed and annotated with questions and their corresponding answers. Furthermore, Question Answering models often only work well for the domain they were trained on. Since annotation is costly, we argue that domain-agnostic knowledge from LMs, such as linguistic understanding, is sufficient to create a well-curated dataset. With this motivation, we show that using large language models can improve Question Answering performance on various datasets in the few-shot setting compared to state-of-the-art approaches. For this, we perform data generation leveraging the Prompting framework, suggesting that language models contain valuable task-agnostic knowledge that can be used beyond the common pre-training/fine-tuning scheme. As a result, we consistently outperform previous approaches on few-shot Question Answering.</abstract>
      <url hash="f25453ae">2024.lrec-main.1153</url>
      <bibkey>schmidt-etal-2024-prompting-based</bibkey>
    </paper>
    <paper id="1154">
      <title>Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process</title>
      <author><first>Guangming</first><last>Huang</last></author>
      <author><first>Yunfei</first><last>Long</last></author>
      <author><first>Cunjin</first><last>Luo</last></author>
      <author><first>Jiaxing</first><last>Shen</last></author>
      <author><first>Xia</first><last>Sun</last></author>
      <pages>13179–13189</pages>
      <abstract>Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs’ reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs’ pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a Prompting Explicit and Implicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reasoning. Furthermore, our model incorporates type-specific reasoning via prompts, a form of implicit knowledge. Experimental results show that PEI performs comparably to the state-of-the-art on HotpotQA. Ablation studies confirm the efficacy of our model in bridging and integrating explicit and implicit knowledge.</abstract>
      <url hash="a57ba33a">2024.lrec-main.1154</url>
      <bibkey>huang-etal-2024-prompting-explicit</bibkey>
    </paper>
    <paper id="1155">
      <title>Prompting for Numerical Sequences: A Case Study on Market Comment Generation</title>
      <author><first>Masayuki</first><last>Kawarada</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>13190–13200</pages>
      <abstract>Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer insights into creating effective prompts for tasks that generate text from numerical sequences.</abstract>
      <url hash="fbbd8015">2024.lrec-main.1155</url>
      <bibkey>kawarada-etal-2024-prompting-numerical</bibkey>
    </paper>
    <paper id="1156">
      <title>Prompting Large Language Models for Counterfactual Generation: An Empirical Study</title>
      <author id="yongqi-li-wuhan"><first>Yongqi</first><last>Li</last></author>
      <author><first>Mayi</first><last>Xu</last></author>
      <author><first>Xin</first><last>Miao</last></author>
      <author><first>Shen</first><last>Zhou</last></author>
      <author><first>Tieyun</first><last>Qian</last></author>
      <pages>13201–13221</pages>
      <abstract>Large language models (LLMs) have made remarkable progress in a wide range of natural language understanding and generation tasks. However, their ability to generate counterfactuals has not been examined systematically. To bridge this gap, we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs’ capability of generating counterfactuals. Based on this framework, we 1) investigate the strengths and weaknesses of LLMs as the counterfactual generator, and 2) disclose the factors that affect LLMs when generating counterfactuals, including both the intrinsic properties of LLMs and prompt designing. The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias. We also find that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs. On the contrary, simply increasing the parameter size does not yield the desired improvements. Besides, from the perspective of prompt designing, task guidelines unsurprisingly play an important role. However, the chain-of-thought approach does not always help due to inconsistency issues.</abstract>
      <url hash="323010c1">2024.lrec-main.1156</url>
      <bibkey>li-etal-2024-prompting-large</bibkey>
    </paper>
    <paper id="1157">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>S</fixed-case>tream: Self-Supervised News Story Discovery Using Topic-Aware Article Representations</title>
      <author><first>Arezoo</first><last>Hatefi</last></author>
      <author><first>Anton</first><last>Eklund</last></author>
      <author><first>Mona</first><last>Forsman</last></author>
      <pages>13222–13232</pages>
      <abstract>Given the importance of identifying and monitoring news stories within the continuous flow of news articles, this paper presents PromptStream, a novel method for unsupervised news story discovery. In order to identify coherent and comprehensive stories across the stream, it is crucial to create article representations that incorporate as much topic-related information from the articles as possible. PromptStream constructs these article embeddings using cloze-style prompting. These representations continually adjust to the evolving context of the news stream through self-supervised learning, employing a contrastive loss and a memory of the most confident article-story assignments from the most recent days. Extensive experiments with real news datasets highlight the notable performance of our model, establishing a new state of the art. Additionally, we delve into selected news stories to reveal how the model’s structuring of the article stream aligns with story progression.</abstract>
      <url hash="a785a4e6">2024.lrec-main.1157</url>
      <bibkey>hatefi-etal-2024-promptstream-self</bibkey>
    </paper>
    <paper id="1158">
      <title>Prompt Tuning for Few-shot Relation Extraction via Modeling Global and Local Graphs</title>
      <author><first>Zirui</first><last>Zhang</last></author>
      <author><first>Yiyu</first><last>Yang</last></author>
      <author><first>Benhui</first><last>Chen</last></author>
      <pages>13233–13242</pages>
      <abstract>Recently, prompt-tuning has achieved very significant results for few-shot tasks. The core idea of prompt-tuning is to insert prompt templates into the input, thus converting the classification task into a masked language modeling problem. However, for few-shot relation extraction tasks, how to mine more information from limited resources becomes particularly important. In this paper, we first construct a global relation graph based on label consistency to optimize the feature representation of samples between different relations. Then the global relation graph is further divided to form a local relation subgraph for each relation type to optimize the feature representation of samples within the same relation. This fully uses the limited supervised information and improves the tuning efficiency. In addition, the existence of rich semantic knowledge in relation labels cannot be ignored. For this reason, this paper incorporates the knowledge in relation labels into prompt-tuning. Specifically, the potential knowledge implicit in relation labels is injected into constructing learnable prompt templates. In this paper, we conduct extensive experiments on four datasets under low-resource settings, showing that this method achieves significant results.</abstract>
      <url hash="fb7314cc">2024.lrec-main.1158</url>
      <bibkey>zhang-etal-2024-prompt-tuning-shot</bibkey>
    </paper>
    <paper id="1159">
      <title><fixed-case>P</fixed-case>r<fixed-case>O</fixed-case>nto: Language Model Evaluations for 859 Languages</title>
      <author><first>Luke</first><last>Gessler</last></author>
      <pages>13243–13256</pages>
      <abstract>Evaluation datasets are critical resources for measuring the quality of pretrained language models. However, due to the high cost of dataset annotation, these resources are scarce for most languages other than English, making it difficult to assess the quality of language models. In this work, we present a new method for evaluation dataset construction which enables any language with a New Testament translation to receive a suite of evaluation datasets suitable for pretrained language model evaluation. The method critically involves aligning verses with those in the New Testament portion of English OntoNotes, and then projecting annotations from English to the target language, with no manual annotation required. We apply this method to 1051 New Testament translations in 859 languages and make them publicly available. Additionally, we conduct experiments which demonstrate the efficacy of our method for creating evaluation tasks which can assess language model quality.</abstract>
      <url hash="377aa281">2024.lrec-main.1159</url>
      <bibkey>gessler-2024-pronto-language</bibkey>
    </paper>
    <paper id="1160">
      <title>Prophecy Distillation for Boosting Abstractive Summarization</title>
      <author><first>Jiaxin</first><last>Duan</last></author>
      <author><first>Fengyu</first><last>Lu</last></author>
      <author><first>Junfei</first><last>Liu</last></author>
      <pages>13257–13268</pages>
      <abstract>Abstractive summarization models learned with maximum likelihood estimation (MLE) have long been guilty of generating unfaithful facts alongside ambiguous focus. Improved paradigm under the guidance of reference-identified words, i.e., guided summarization, has exhibited remarkable advantages in overcoming this problem. However, it suffers limited real applications since the prophetic guidance is practically agnostic at inference. In this paper, we introduce a novel teacher-student framework, which learns a regular summarization model to mimic the behavior of being guided by prophecy for boosting abstractive summaries. Specifically, by training in probability spaces to follow and distinguish a guided teacher model, a student model learns the key to generating teacher-like quality summaries without any guidance. We refer to this process as prophecy distillation, and it breaks the limitations of both standard and guided summarization. Through extensive experiments, we show that our method achieves new or matched state-of-the-art on four well-known datasets, including ROUGE scores, faithfulness, and saliency awareness. Human evaluations are also carried out to evidence these merits. Furthermore, we conduct empirical studies to analyze how the hyperparameters setting and the guidance choice affect TPG performance.</abstract>
      <url hash="34d20ff9">2024.lrec-main.1160</url>
      <bibkey>duan-etal-2024-prophecy-distillation</bibkey>
    </paper>
    <paper id="1161">
      <title>Prototype-based Prompt-Instance Interaction with Causal Intervention for Few-shot Event Detection</title>
      <author><first>Jingyao</first><last>Tang</last></author>
      <author><first>Lishuang</first><last>Li</last></author>
      <author><first>Hongbin</first><last>Lu</last></author>
      <author><first>Xueyang</first><last>Qin</last></author>
      <author><first>Beibei</first><last>Zhang</last></author>
      <author><first>Haiming</first><last>Wu</last></author>
      <pages>13269–13278</pages>
      <abstract>Few-shot Event Detection (FSED) is a meaningful task due to the limited labeled data and expensive manual labeling. Some prompt-based methods are used in FSED. However, these methods require large GPU memory due to the increased length of input tokens caused by concatenating prompts, as well as additional human effort for designing verbalizers. Moreover, they ignore instance and prompt biases arising from the confounding effects between prompts and texts. In this paper, we propose a prototype-based prompt-instance Interaction with causal Intervention (2xInter) model to conveniently utilize both prompts and verbalizers and effectively eliminate all biases. Specifically, 2xInter first presents a Prototype-based Prompt-Instance Interaction (PPII) module that applies an interactive approach for texts and prompts to reduce memory and regards class prototypes as verbalizers to avoid design costs. Next, 2xInter constructs a Structural Causal Model (SCM) to explain instance and prompt biases and designs a Double-View Causal Intervention (DVCI) module to eliminate these biases. Due to limited supervised information, DVCI devises a generation-based prompt adjustment for instance intervention and a Siamese network-based instance contrasting for prompt intervention. Finally, the experimental results show that 2xInter achieves state-of-the-art performance on RAMS and ACE datasets.</abstract>
      <url hash="a23a03c6">2024.lrec-main.1161</url>
      <attachment type="OptionalSupplementaryMaterial" hash="010d9c56">2024.lrec-main.1161.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>tang-etal-2024-prototype-based</bibkey>
    </paper>
    <paper id="1162">
      <title>Pruning before Fine-tuning: A Retraining-free Compression Framework for Pre-trained Language Models</title>
      <author><first>Pingjie</first><last>Wang</last></author>
      <author><first>Hongcheng</first><last>Liu</last></author>
      <author><first>Yanfeng</first><last>Wang</last></author>
      <author><first>Yu</first><last>Wang</last></author>
      <pages>13279–13289</pages>
      <abstract>Structured pruning is an effective technique for compressing pre-trained language models (PLMs), reducing model size and improving inference speed for efficient deployment. However, most of existing pruning algorithms require retraining, leading to additional computational overhead. While some retraining-free approaches have been proposed for classification tasks, they still require a fully fine-tuned model for the task, and may cause catastrophic performance degradation on generative tasks. To address these challenges, we propose P-pruning (pre-pruning), an innovative task-specific compression framework. P-pruning prunes redundant modules of PLMs before fine-tuning, reducing the costs associated with fine-tuning. We also introduce a pruning algorithm for this framework, which includes two techniques: (1) module clustering, which clusters the outputs of all heads and neurons based on the task input; and (2) centroid selection, which identifies the most salient element in each cluster and prunes the others. We apply our method to BERT and GPT-2 and evaluate its effectiveness on GLUE, SQuAD, WikiText-2, WikiText-103, and PTB datasets. Experimental results demonstrate that our approach achieves higher performance in both classification and generative tasks, while also reducing the time required for fine-tuning.</abstract>
      <url hash="688f4b24">2024.lrec-main.1162</url>
      <bibkey>wang-etal-2024-pruning-fine</bibkey>
    </paper>
    <paper id="1163">
      <title><fixed-case>PS</fixed-case>ent<fixed-case>S</fixed-case>core: Evaluating Sentiment Polarity in Dialogue Summarization</title>
      <author><first>Yongxin</first><last>Zhou</last></author>
      <author><first>Fabien</first><last>Ringeval</last></author>
      <author><first>François</first><last>Portet</last></author>
      <pages>13290–13302</pages>
      <abstract>Automatic dialogue summarization is a well-established task with the goal of distilling the most crucial information from human conversations into concise textual summaries. However, most existing research has predominantly focused on summarizing factual information, neglecting the affective content, which can hold valuable insights for analyzing, monitoring, or facilitating human interactions. In this paper, we introduce and assess a set of measures PSentScore, aimed at quantifying the preservation of affective content in dialogue summaries. Our findings indicate that state-of-the-art summarization models do not preserve well the affective content within their summaries. Moreover, we demonstrate that a careful selection of the training set for dialogue samples can lead to improved preservation of affective content in the generated summaries, albeit with a minor reduction in content-related metrics.</abstract>
      <url hash="6f30727a">2024.lrec-main.1163</url>
      <bibkey>zhou-etal-2024-psentscore-evaluating</bibkey>
    </paper>
    <paper id="1164">
      <title>Pseudonymization Categories across Domain Boundaries</title>
      <author><first>Maria Irena</first><last>Szawerna</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>Therese</first><last>Lindström Tiedemann</last></author>
      <author><first>Ricardo</first><last>Muñoz Sánchez</last></author>
      <author><first>Xuan-Son</first><last>Vu</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <pages>13303–13314</pages>
      <abstract>Linguistic data, a component critical not only for research in a variety of fields but also for the development of various Natural Language Processing (NLP) applications, can contain personal information. As a result, its accessibility is limited, both from a legal and an ethical standpoint. One of the solutions is the pseudonymization of the data. Key stages of this process include the identification of sensitive elements and the generation of suitable surrogates in a way that the data is still useful for the intended task. Within this paper, we conduct an analysis of tagsets that have previously been utilized in anonymization and pseudonymization. We also investigate what kinds of Personally Identifiable Information (PII) appear in various domains. These reveal that none of the analyzed tagsets account for all of the PII types present cross-domain at the level of detailedness seemingly required for pseudonymization. We advocate for a universal system of tags for categorizing PIIs leading up to their replacement. Such categorization could facilitate the generation of grammatically, semantically, and sociolinguistically appropriate surrogates for the kinds of information that are considered sensitive in a given domain, resulting in a system that would enable dynamic pseudonymization while keeping the texts readable and useful for future research in various fields.</abstract>
      <url hash="d34e45dc">2024.lrec-main.1164</url>
      <bibkey>szawerna-etal-2024-pseudonymization-categories</bibkey>
    </paper>
    <paper id="1165">
      <title><fixed-case>PSE</fixed-case> v1.0: The First Open Access Corpus of Public Service Encounters</title>
      <author><first>Ingrid</first><last>Espinoza</last></author>
      <author><first>Steffen</first><last>Frenzel</last></author>
      <author><first>Laurin</first><last>Friedrich</last></author>
      <author><first>Wassiliki</first><last>Siskou</last></author>
      <author><first>Steffen</first><last>Eckhard</last></author>
      <author><first>Annette</first><last>Hautli-Janisz</last></author>
      <pages>13315–13320</pages>
      <abstract>Face-to-face interactions between representatives of the state and citizens are a key intercept in public service delivery, for instance when providing social benefits to vulnerable groups. Despite the relevance of these encounters for the individual, but also for society at large, there is a significant research gap in the systematic empirical study of the communication taking place. This is mainly due to the high institutional and data protection barriers for collecting data in a very sensitive and private setting in which citizens request support from the state. In this paper, we describe the procedure of compiling the first open access dataset of transcribed recordings of so-called Public Service Encounters in Germany, i.e., meetings between state officials and citizens in which there is direct communication in order to allocate state services. This dataset sets a new research directive in the social sciences, because it allows the community to open up the black box of direct state-citizen interaction. With data of this kind it becomes possible to directly and systematically investigate bias, bureaucratic discrimination and other power-driven dynamics in the actual communication and ideally propose guidelines as to alleviate these issues.</abstract>
      <url hash="8ddd39cb">2024.lrec-main.1165</url>
      <bibkey>espinoza-etal-2024-pse-v1</bibkey>
    </paper>
    <paper id="1166">
      <title><fixed-case>PSYDIAL</fixed-case>: Personality-based Synthetic Dialogue Generation Using Large Language Models</title>
      <author><first>Ji-Eun</first><last>Han</last></author>
      <author><first>Jun-Seok</first><last>Koh</last></author>
      <author><first>Hyeon-Tae</first><last>Seo</last></author>
      <author><first>Du-Seong</first><last>Chang</last></author>
      <author><first>Kyung-Ah</first><last>Sohn</last></author>
      <pages>13321–13331</pages>
      <abstract>We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages.</abstract>
      <url hash="e542239d">2024.lrec-main.1166</url>
      <bibkey>han-etal-2024-psydial-personality</bibkey>
    </paper>
    <paper id="1167">
      <title>Puntuguese: A Corpus of Puns in <fixed-case>P</fixed-case>ortuguese with Micro-edits</title>
      <author><first>Marcio Lima</first><last>Inacio</last></author>
      <author><first>Gabriela</first><last>Wick-Pedro</last></author>
      <author><first>Renata</first><last>Ramisch</last></author>
      <author><first>Luís</first><last>Espírito Santo</last></author>
      <author><first>Xiomara S. Q.</first><last>Chacon</last></author>
      <author><first>Roney</first><last>Santos</last></author>
      <author><first>Rogério</first><last>Sousa</last></author>
      <author><first>Rafael</first><last>Anchiêta</last></author>
      <author><first>Hugo</first><last>Goncalo Oliveira</last></author>
      <pages>13332–13343</pages>
      <abstract>Humor is an intricate part of verbal communication and dealing with this kind of phenomenon is essential to building systems that can process language at large with all of its complexities. In this paper, we introduce Puntuguese, a new corpus of punning humor in Portuguese, motivated by previous works showing that currently available corpora for this language are still unfit for Machine Learning due to data leakage. Puntuguese comprises 4,903 manually-gathered punning one-liners in Brazilian and European Portuguese. To create negative examples that differ exclusively in terms of funniness, we carried out a micro-editing process, in which all jokes were edited by fluent Portuguese speakers to make the texts unfunny. Finally, we did some experiments on Humor Recognition, showing that Puntuguese is considerably more difficult than the previous corpus, achieving an F1-Score of 68.9%. With this new dataset, we hope to enable research not only in NLP but also in other fields that are interested in studying humor; thus, the data is publicly available.</abstract>
      <url hash="03f1341e">2024.lrec-main.1167</url>
      <bibkey>inacio-etal-2024-puntuguese-corpus</bibkey>
    </paper>
    <paper id="1168">
      <title><fixed-case>PWES</fixed-case>uite: Phonetic Word Embeddings and Tasks They Facilitate</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Kalvin</first><last>Chang</last></author>
      <author><first>Chenxuan</first><last>Cui</last></author>
      <author><first>Nate B.</first><last>Carlson</last></author>
      <author><first>Nathaniel Romney</first><last>Robinson</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <pages>13344–13355</pages>
      <abstract>Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.</abstract>
      <url hash="9b8fb9a1">2024.lrec-main.1168</url>
      <bibkey>zouhar-etal-2024-pwesuite-phonetic</bibkey>
    </paper>
    <paper id="1169">
      <title><fixed-case>P</fixed-case>y<fixed-case>R</fixed-case>ater: A Python Toolkit for Annotation Analysis</title>
      <author><first>Angelo</first><last>Basile</last></author>
      <author><first>Marc</first><last>Franco-Salvador</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>13356–13362</pages>
      <abstract>We introduce PyRater, an open-source Python toolkit designed for analysing corpora annotations. When creating new annotated language resources, probabilistic models of annotation are the state-of-the-art solution for identifying the best annotators, retrieving the gold standard, and more generally separating annotation signal from noise. PyRater offers a unified interface for several such models and includes an API for the addition of new ones. Additionally, the toolkit has built-in functions to read datasets with multiple annotations and plot the analysis outcomes. In this work, we also demonstrate a novel application of PyRater to zero-shot classifiers, where it effectively selects the best-performing prompt. We make PyRater available to the research community.</abstract>
      <url hash="9d48d4ec">2024.lrec-main.1169</url>
      <bibkey>basile-etal-2024-pyrater-python</bibkey>
    </paper>
    <paper id="1170">
      <title>Qabas: An Open-Source <fixed-case>A</fixed-case>rabic Lexicographic Database</title>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <author><first>Tymaa Hasanain</first><last>Hammouda</last></author>
      <pages>13363–13370</pages>
      <abstract>We present Qabas, a novel open-source Arabic lexicon designed for NLP applications. The novelty of Qabas lies in its synthesis of 110 lexicons. Specifically, Qabas lexical entries (lemmas) are assembled by linking lemmas from 110 lexicons. Furthermore, Qabas lemmas are also linked to 12 morphologically annotated corpora (about 2M tokens), making it the first Arabic lexicon to be linked to lexicons and corpora. Qabas was developed semi-automatically, utilizing a mapping framework and a web-based tool. Compared with other lexicons, Qabas stands as the most extensive Arabic lexicon, encompassing about 58K lemmas (45K nominal lemmas, 12.5K verbal lemmas, and 473 functional-word lemmas). Qabas is open-source and accessible online at https://sina.birzeit.edu/qabas</abstract>
      <url hash="a17d7b38">2024.lrec-main.1170</url>
      <bibkey>jarrar-hammouda-2024-qabas-open</bibkey>
    </paper>
    <paper id="1171">
      <title><fixed-case>QA</fixed-case>-based Event Start-Points Ordering for Clinical Temporal Relation Annotation</title>
      <author><first>Seiji</first><last>Shimizu</last></author>
      <author><first>Lis</first><last>Pereira</last></author>
      <author><first>Shuntaro</first><last>Yada</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <pages>13371–13381</pages>
      <abstract>Temporal relation annotation in the clinical domain is crucial yet challenging due to its workload and the medical expertise required. In this paper, we propose a novel annotation method that integrates event start-points ordering and question-answering (QA) as the annotation format. By focusing only on two points on a timeline, start-points ordering reduces ambiguity and simplifies the relation set to be considered during annotation. QA as annotation recasts temporal relation annotation into a reading comprehension task, allowing annotators to use natural language instead of the formalisms commonly adopted in temporal relation annotation. Based on our method, most of the relations in a document are inferable from a significantly smaller number of explicitly annotated relations, showing the efficiency of our proposed method. Using these inferred relations, we develop a temporal relation classification model that achieves a 0.72 F1 score. Also, by decomposing the annotation process into QA generation and QA validation, our method enables collaboration among medical experts and non-experts. We obtained high inter-annotator agreement (IAA) scores, which indicate the positive prospect of such collaboration in the annotation process. Our annotated corpus, annotation tool, and trained model are publicly available: https://github.com/seiji-shimizu/qa-start-ordering.</abstract>
      <url hash="8d8d07d2">2024.lrec-main.1171</url>
      <bibkey>shimizu-etal-2024-qa-based</bibkey>
    </paper>
    <paper id="1172">
      <title><fixed-case>QCAW</fixed-case> 1.0: Building a Qatari Corpus of Student Argumentative Writing</title>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Abdelhamid</first><last>Ahmed</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Lameya</first><last>Rezk</last></author>
      <pages>13382–13394</pages>
      <abstract>This paper presents the creation of the Qatari Corpus of Argumentative Writing (QCAW) as an annotated L1 Arabic and L2 English bilingual writer corpus. It comprises 200,000 tokens of argumentative writing by Qatari university students in L1 Arabic and L2 English. The corpus includes 195 essays written by 195 students, 159 females and 36 males. The students were native Arabic speakers proficient in English as a second language. The corpus is divided into Arabic and English sections, accompanied by part-of-speech annotated files. The Metadata contains information about the students (gender, major, first and second languages) and the essays (text serial numbers, word limits, genre, writing date, time spent, and location). The paper outlines the steps for collecting and analysing the corpus, including details on essay writers, topic selection, pre-analysis text modifications, proficiency level, gender, and major ratings. Statistical analyses were applied to examine the corpus. The QCAW offers a valuable bilingual data source authored by the same students in Arabic and English, with implications for further research</abstract>
      <url hash="65bdeada">2024.lrec-main.1172</url>
      <bibkey>zaghouani-etal-2024-qcaw-1</bibkey>
    </paper>
    <paper id="1173">
      <title><fixed-case>QDMR</fixed-case>-based Planning-and-Solving Prompting for Complex Reasoning Tasks</title>
      <author><first>Jinfeng</first><last>Huang</last></author>
      <author><first>Qiaoqiao</first><last>She</last></author>
      <author><first>Wenbin</first><last>Jiang</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Yang</first><last>Hao</last></author>
      <author><first>Tong</first><last>Xu</last></author>
      <author><first>Feng</first><last>Wu</last></author>
      <pages>13395–13406</pages>
      <abstract>Chain-of-Thought prompting has improved reasoning capability of large language models (LLM). However, it still is challenging to guarantee the effectiveness and stability for questions requiring complicated reasoning. Recently, Plan-and-Solve prompting enhances the reasoning capability for complex questions by planning the solution steps firstly and then solving them step by step, but it suffers the difficulty to represent and execute the problem-solving logic of complex questions. To deal with these challenges, in this work, we propose a novel Plan-and-Solve prompting method based on Question Decomposition Meaning Representation (QDMR). Specifically, this method first allows the LLM to generate a QDMR graph to represent the problem-solving logic, which is a directed acyclic graph composed of sub-questions. Then, the LLM generates a specific solving process based on the QDMR graph. When solving each sub-question, it can locate the preceding sub-questions and their answers according to the QDMR graph, and then utilize this information for solution. Compared with existing Plan-and-Solve prompting techniques, our method can not only represent the problem-solving logic of complicated questions more accurately with the aid of QDMR graph, but also deliver the dependence information accurately for different solution steps according to the QDMR graph. In addition, with the supervised fine-tuning on the Allen Institute dataset, the decomposing capability of LLM for complicated questions can be considerably enhanced. Extensive experiments show that our method has achieve a great significance in arithmetic reasoning and commonsense reasoning task by comparing the classical Chain-of-Thought prompting and Plan-and-Solve prompting techniques, and the improvements achieved are even greater for problems with more reasoning steps.</abstract>
      <url hash="d0381086">2024.lrec-main.1173</url>
      <bibkey>huang-etal-2024-qdmr-based</bibkey>
    </paper>
    <paper id="1174">
      <title>Qsnail: A Questionnaire Dataset for Sequential Question Generation</title>
      <author><first>Yan</first><last>Lei</last></author>
      <author><first>Liang</first><last>Pang</last></author>
      <author><first>Yuanzhuo</first><last>Wang</last></author>
      <author><first>Huawei</first><last>Shen</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>13407–13418</pages>
      <abstract>The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address these issues, we present Qsnail, the first dataset specifically constructed for the questionnaire generation task, which comprises 13,168 human-written questionnaires gathered from online platforms. We further conduct experiments on Qsnail, and the results reveal that retrieval models and traditional generative models do not fully align with the given research topic and intents. Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires. Therefore, questionnaire generation is challenging and needs to be further explored. The dataset will be published in the future.</abstract>
      <url hash="8d4b6b1d">2024.lrec-main.1174</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c1347583">2024.lrec-main.1174.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lei-etal-2024-qsnail-questionnaire</bibkey>
    </paper>
    <paper id="1175">
      <title>Quantifying the Impact of Disfluency on Spoken Content Summarization</title>
      <author><first>Maria</first><last>Teleki</last></author>
      <author><first>Xiangjue</first><last>Dong</last></author>
      <author><first>James</first><last>Caverlee</last></author>
      <pages>13419–13428</pages>
      <abstract>Spoken content is abundant – including podcasts, meeting transcripts, and TikTok-like short videos. And yet, many important tasks like summarization are often designed for written content rather than the looser, noiser, and more disfluent style of spoken content. Hence, we aim in this paper to quantify the impact of disfluency on spoken content summarization. Do disfluencies negatively impact the quality of summaries generated by existing approaches? And if so, to what degree? Coupled with these goals, we also investigate two methods towards improving summarization in the presence of such disfluencies. We find that summarization quality does degrade with an increase in these disfluencies and that a combination of multiple disfluency types leads to even greater degradation. Further, our experimental results show that naively removing disfluencies and augmenting with special tags can worsen the summarization when used for testing, but that removing disfluencies for fine-tuning yields the best results. We make the code available at https://github.com/mariateleki/Quantifying-Impact-Disfluency.</abstract>
      <url hash="05f557c9">2024.lrec-main.1175</url>
      <bibkey>teleki-etal-2024-quantifying-impact</bibkey>
    </paper>
    <paper id="1176">
      <title><fixed-case>QUEEREOTYPES</fixed-case>: A Multi-Source <fixed-case>I</fixed-case>talian Corpus of Stereotypes towards <fixed-case>LGBTQIA</fixed-case>+ Community Members</title>
      <author><first>Alessandra Teresa</first><last>Cignarella</last></author>
      <author><first>Manuela</first><last>Sanguinetti</last></author>
      <author><first>Simona</first><last>Frenda</last></author>
      <author><first>Andrea</first><last>Marra</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <pages>13429–13441</pages>
      <abstract>The paper describes a dataset composed of two sub-corpora from two different sources in Italian. The QUEEREOTYPES corpus includes social media texts regarding LGBTQIA+ individuals, behaviors, ideology and events. The texts were collected from Facebook and Twitter in 2018 and were annotated for the presence of stereotypes, and orthogonal dimensions (such as hate speech, aggressiveness, offensiveness, and irony in one sub-corpus, and stance in the other). The resource was developed by Natural Language Processing researchers together with activists from an Italian LGBTQIA+ not-for-profit organization. The creation of the dataset allows the NLP community to study stereotypes against marginalized groups, individuals and, ultimately, to develop proper tools and measures to reduce the online spread of such stereotypes. A test for the robustness of the language resource has been performed by means of 5-fold cross-validation experiments. Finally, text classification experiments have been carried out with a fine-tuned version of AlBERTo (a BERT-based model pre-trained on Italian tweets) and mBERT, obtaining good results on the task of stereotype detection, suggesting that stereotypes towards different targets might share common traits.</abstract>
      <url hash="a8836657">2024.lrec-main.1176</url>
      <bibkey>cignarella-etal-2024-queereotypes-multi</bibkey>
    </paper>
    <paper id="1177">
      <title>Query-driven Relevant Paragraph Extraction from Legal Judgments</title>
      <author><first>Santosh</first><last>T.y.s.s.</last></author>
      <author><first>Elvin A.</first><last>Quero Hernandez</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>13442–13454</pages>
      <abstract>Legal professionals often grapple with navigating lengthy legal judgements to pinpoint information that directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a zero-shot way and also establish fine-tuning benchmarks using various models. The results highlight the significant gap between fine-tuned and zero-shot performance, emphasizing the challenge of handling distribution shift in the legal domain. We notice that the legal pre-training handles distribution shift on the corpus side but still struggles on query side distribution shift, with unseen legal queries. We also explore various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their practicality within the context of information retrieval, shedding light on the effectiveness of different PEFT methods across diverse configurations with pre-training and model architectures influencing the choice of PEFT method.</abstract>
      <url hash="7761390f">2024.lrec-main.1177</url>
      <bibkey>t-y-s-s-etal-2024-query-driven</bibkey>
    </paper>
    <paper id="1178">
      <title><fixed-case>Q</fixed-case>uery<fixed-case>NER</fixed-case>: Segmentation of <fixed-case>E</fixed-case>-commerce Queries</title>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <author><first>Lizzie</first><last>Liang</last></author>
      <author><first>Zhe</first><last>Wu</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <pages>13455–13470</pages>
      <abstract>We present QueryNER, a manually-annotated dataset and accompanying model for e-commerce query segmentation. Prior work in sequence labeling for e-commerce has largely addressed aspect-value extraction which focuses on extracting portions of a product title or query for narrowly defined aspects. Our work instead focuses on the goal of dividing a query into meaningful chunks with broadly applicable types. We report baseline tagging results and conduct experiments comparing token and entity dropping for null and low recall query recovery. Challenging test sets are created using automatic transformations and show how simple data augmentation techniques can make the models more robust to noise. We make the QueryNER dataset publicly available.</abstract>
      <url hash="5f8ffe82">2024.lrec-main.1178</url>
      <bibkey>palen-michel-etal-2024-queryner-segmentation</bibkey>
    </paper>
    <paper id="1179">
      <title>Question Answering over Tabular Data with <fixed-case>D</fixed-case>ata<fixed-case>B</fixed-case>ench: A Large-Scale Empirical Evaluation of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jorge</first><last>Osés Grijalba</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <author><first>Eugenio</first><last>Martínez Cámara</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>13471–13488</pages>
      <abstract>Large Language Models (LLMs) are showing emerging abilities, and one of the latest recognized ones deals with their ability to reason and answer questions from tabular data. Although there are some available datasets to assess question answering systems on tabular data, they are not large and diverse enough to properly assess the capabilities of LLMs. To this end, we propose DataBench, a benchmark composed of 65 real-world datasets over several domains, including 20 human-generated questions per dataset, totaling 1300 questions and answers overall. Using this benchmark, we perform a large-scale empirical comparison of several open and closed source models, including both code-generating and in-context learning models. The results highlight the current gap between open-source and closed-source models, with all types of model having room for improvement even in simple boolean questions or involving a single column.</abstract>
      <url hash="7174b48c">2024.lrec-main.1179</url>
      <bibkey>oses-grijalba-etal-2024-question-answering</bibkey>
    </paper>
    <paper id="1180">
      <title>Quite Good, but Not Enough: Nationality Bias in Large Language Models - a Case Study of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>Shucheng</first><last>Zhu</last></author>
      <author><first>Weikang</first><last>Wang</last></author>
      <author><first>Ying</first><last>Liu</last></author>
      <pages>13489–13502</pages>
      <abstract>While nationality is a pivotal demographic element that enhances the performance of language models, it has received far less scrutiny regarding inherent biases. This study investigates nationality bias in ChatGPT (GPT-3.5), a large language model (LLM) designed for text generation. The research covers 195 countries, 4 temperature settings, and 3 distinct prompt types, generating 4,680 discourses about nationality descriptions in Chinese and English. Automated metrics were used to analyze the nationality bias, and expert annotators alongside ChatGPT itself evaluated the perceived bias. The results show that ChatGPT’s generated discourses are predominantly positive, especially compared to its predecessor, GPT-2. However, when prompted with negative inclinations, it occasionally produces negative content. Despite ChatGPT considering its generated text as neutral, it shows consistent self-awareness about nationality bias when subjected to the same pair-wise comparison annotation framework used by human annotators. In conclusion, while ChatGPT’s generated texts seem friendly and positive, they reflect the inherent nationality biases in the real world. This bias may vary across different language versions of ChatGPT, indicating diverse cultural perspectives. The study highlights the subtle and pervasive nature of biases within LLMs, emphasizing the need for further scrutiny.</abstract>
      <url hash="d6d39b22">2024.lrec-main.1180</url>
      <bibkey>zhu-etal-2024-quite-good</bibkey>
    </paper>
    <paper id="1181">
      <title><fixed-case>RAAM</fixed-case>ove: A Corpus for Analyzing Moves in Research Article Abstracts</title>
      <author><first>Hongzheng</first><last>Li</last></author>
      <author><first>Ruojin</first><last>Wang</last></author>
      <author><first>Ge</first><last>Shi</last></author>
      <author><first>Xing</first><last>Lv</last></author>
      <author><first>Lei</first><last>Lei</last></author>
      <author><first>Chong</first><last>Feng</last></author>
      <author><first>Fang</first><last>Liu</last></author>
      <author><first>Jinkun</first><last>Lin</last></author>
      <author><first>Yangguang</first><last>Mei</last></author>
      <author><first>Linnan</first><last>Xu</last></author>
      <pages>13503–13513</pages>
      <abstract>Move structures have been studied in English for Specific Purposes (ESP) and English for Academic Purposes (EAP) for decades. However, there are few move annotation corpora for Research Article (RA) abstracts. In this paper, we introduce RAAMove, a comprehensive multi-domain corpus dedicated to the annotation of move structures in RA abstracts. The primary objective of RAAMove is to facilitate move analysis and automatic move identification. This paper provides a thorough discussion of the corpus construction process, including the scheme, data collection, annotation guidelines, and annotation procedures. The corpus is constructed through two stages: initially, expert annotators manually annotate high-quality data; subsequently, based on the human-annotated data, a BERT-based model is employed for automatic annotation with the help of experts’ modification. The result is a large-scale and high-quality corpus comprising 33,988 annotated instances. We also conduct preliminary move identification experiments using the BERT-based model to verify the effectiveness of the proposed corpus and model. The annotated corpus is available for academic research purposes and can serve as essential resources for move analysis, English language teaching and writing, as well as move/discourse-related tasks in Natural Language Processing (NLP).</abstract>
      <url hash="23d5e035">2024.lrec-main.1181</url>
      <bibkey>li-etal-2024-raamove-corpus</bibkey>
    </paper>
    <paper id="1182">
      <title><fixed-case>RADC</fixed-case>o<fixed-case>T</fixed-case>: Retrieval-Augmented Distillation to Specialization Models for Generating Chain-of-Thoughts in Query Expansion</title>
      <author><first>Sung-Min</first><last>Lee</last></author>
      <author><first>Eunhwan</first><last>Park</last></author>
      <author><first>DongHyeon</first><last>Jeon</last></author>
      <author><first>Inho</first><last>Kang</last></author>
      <author><first>Seung-Hoon</first><last>Na</last></author>
      <pages>13514–13523</pages>
      <abstract>Large language models (LLMs) have demonstrated superior performance to that of small language models (SLM) in information retrieval for various subtasks including dense retrieval, reranking, query expansion, and pseudo-document generation. However, the parameter sizes of LLMs are extremely large, making it expensive to operate LLMs stably for providing LLM-based retrieval services. Recently, retrieval-augmented language models have been widely employed to significantly reduce the parameter size by retrieving relevant knowledge from large-scale corpora and exploiting the resulting “in-context” knowledge as additional model input, thereby substantially reducing the burden of internalizing and retaining world knowledge in model parameters. Armed by the retrieval-augmented language models, we present a retrieval-augmented model specialization that distills the capability of LLMs to generate the chain-of-thoughts (CoT) for query expansion – that is, injects the LLM’s capability to generate CoT into a retrieval-augmented SLM – referred to as <b>RADCoT</b>. Experimental results on the MS-MARCO, TREC DL 19, 20 datasets show that RADCoT yields consistent improvements over distillation without retrieval, achieving comparable performance to that of the query expansion method using LLM-based CoTs. Our code is publicly available at <url>https://github.com/ZIZUN/RADCoT</url>.</abstract>
      <url hash="66bd48ca">2024.lrec-main.1182</url>
      <bibkey>lee-etal-2024-radcot-retrieval</bibkey>
    </paper>
    <paper id="1183">
      <title><fixed-case>R</fixed-case>ank<fixed-case>P</fixed-case>rompt: Step-by-Step Comparisons Make Language Models Better Reasoners</title>
      <author><first>Chi</first><last>Hu</last></author>
      <author><first>Yuan</first><last>Ge</last></author>
      <author><first>Xiangnan</first><last>Ma</last></author>
      <author><first>Hang</first><last>Cao</last></author>
      <author><first>Qiang</first><last>Li</last></author>
      <author><first>Yonghua</first><last>Yang</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>13524–13536</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, such as deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover, RankPrompt excels in LLM-based automatic evaluations for open-ended tasks, aligning with human judgments 74% of the time in the AlpacaEval dataset. It also exhibits robustness to variations in response order and consistency. Collectively, our results validate RankPrompt as an effective method for eliciting high-quality feedback from language models.</abstract>
      <url hash="a7738e6d">2024.lrec-main.1183</url>
      <bibkey>hu-etal-2024-rankprompt-step</bibkey>
    </paper>
    <paper id="1184">
      <title>Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on <fixed-case>J</fixed-case>apanese</title>
      <author><first>Yikun</first><last>Sun</last></author>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Nobuhiro</first><last>Ueda</last></author>
      <author><first>Sakiko</first><last>Yahata</last></author>
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>13537–13547</pages>
      <abstract>The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across all three base pre-trained models. Our GPT-4 self-instruct data allowed the LLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37% win-rate. The human evaluation exhibits the consistency between GPT-4’s assessments and human preference. Our high-quality instruction data and evaluation benchmark are released here.</abstract>
      <url hash="9c5e48be">2024.lrec-main.1184</url>
      <bibkey>sun-etal-2024-rapidly-developing</bibkey>
    </paper>
    <paper id="1185">
      <title>Rapidly Piloting Real-time Linguistic Assistance for Simultaneous Interpreters with Untrained Bilingual Surrogates</title>
      <author><first>Alvin C.</first><last>Grissom II</last></author>
      <author><first>Jo</first><last>Shoemaker</last></author>
      <author><first>Benjamin</first><last>Goldman</last></author>
      <author><first>Ruikang</first><last>Shi</last></author>
      <author><first>Craig</first><last>Stewart</last></author>
      <author><first>C. Anton</first><last>Rytting</last></author>
      <author><first>Leah</first><last>Findlater</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>13548–13556</pages>
      <abstract>Simultaneous interpretation is a cognitively taxing task, and even seasoned professionals benefit from real-time assistance. However, both recruiting professional interpreters and evaluating new assistance techniques are difficult. We present a novel, realistic simultaneous interpretation task that mimics the cognitive load of interpretation with crowdworker surrogates. Our task tests different real-time assistance methods in a Wizard-of-Oz experiment with a large pool of proxy users and compares against professional interpreters. Both professional and proxy participants respond similarly to changes in interpreting conditions, including improvement with two assistance interventions—translation of specific terms and of numbers—compared to a no-assistance control.</abstract>
      <url hash="79d1e87a">2024.lrec-main.1185</url>
      <bibkey>grissom-ii-etal-2024-rapidly-piloting</bibkey>
    </paper>
    <paper id="1186">
      <title>Rationale-based Learning Using Self-Supervised Narrative Events for Text Summarisation of Interactive Digital Narratives</title>
      <author><first>Ashwathy</first><last>T Revi</last></author>
      <author><first>Stuart E.</first><last>Middleton</last></author>
      <author><first>David E.</first><last>Millard</last></author>
      <pages>13557–13585</pages>
      <abstract>This paper explores using rationale-based learning with supervised attention to focus the training of text summarisation models on words and sentences surrounding choice points for Interactive Digital Narratives (IDNs). IDNs allow players to interact with the story via choice points, making choices central to these narratives. Exploiting such knowledge about narrative structure during model training can help ensure key narrative information appears in generated summaries of narrative-based text and thus improve the quality of these summaries. We experiment with using word-level and sentence-level rationales indicating the proximity of words and sentences to self-supervised choice points. Our results indicate that rationale-based learning can improve the ability of attention-based text summarisation models to create higher quality summaries that encode key narrative information better for different playthroughs of the same interactive narrative. These results suggest a promising new direction for narrative-based text summarisation models.</abstract>
      <url hash="90448626">2024.lrec-main.1186</url>
      <bibkey>t-revi-etal-2024-rationale-based</bibkey>
    </paper>
    <paper id="1187">
      <title>Reading Does Not Equal Reading: Comparing, Simulating and Exploiting Reading Behavior across Populations</title>
      <author><first>David R.</first><last>Reich</last></author>
      <author><first>Shuwen</first><last>Deng</last></author>
      <author><first>Marina</first><last>Björnsdóttir</last></author>
      <author><first>Lena</first><last>Jäger</last></author>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <pages>13586–13594</pages>
      <abstract>Eye-tracking-while-reading corpora play a crucial role in the study of human language processing, and, more recently, have been leveraged for cognitively enhancing neural language models. A critical limitation of existing corpora is that they often lack diversity, comprising primarily native speakers. In this study, we expand the eye-tracking-while-reading dataset CopCo, which initially included only Danish L1 readers with and without dyslexia, by incorporating a new dataset of L2 readers with diverse L1 backgrounds. Thus, the extended CopCo corpus constitutes the first eye-tracking-while-reading dataset encompassing neurotypical L1 and L1 readers with dyslexia as well as L2 readers, all reading the same materials. We first provide extensive descriptive statistics of the extended CopCo corpus. Second, we investigate how different degrees of diversity of the training data affect a state-of-the-art generative model of eye movements in reading. Finally, we use this scanpath generation model for gaze-augmented language modeling and investigate the impact of diversity in the training data on the model’s performance on a range of NLP downstream tasks. The code can be found here: https://github.com/norahollenstein/copco-processing.</abstract>
      <url hash="b9456be1">2024.lrec-main.1187</url>
      <bibkey>reich-etal-2024-reading-equal</bibkey>
    </paper>
    <paper id="1188">
      <title><fixed-case>R</fixed-case>ead<fixed-case>L</fixed-case>et: A Dataset for Oral, Visual and Tactile Text Reading Data of Early and Mature Readers</title>
      <author><first>Marcello</first><last>Ferro</last></author>
      <author><first>Claudia</first><last>Marzi</last></author>
      <author><first>Andrea</first><last>Nadalini</last></author>
      <author><first>Loukia</first><last>Taxitari</last></author>
      <author><first>Alessandro</first><last>Lento</last></author>
      <author><first>Vito</first><last>Pirrelli</last></author>
      <pages>13595–13609</pages>
      <abstract>The paper presents the design and construction of a time-stamped multimodal dataset for reading research, including multiple time-aligned temporal signals elicited with four experimental trials of connected text reading by both child and adult readers. We present the experimental protocols, as well as the data acquisition process and the post-processing phase of data annotation/augmentation. To evaluate the potential and usefulness of a time-aligned multimodal dataset for reading research, we present a few statistical analyses showing the correlation and complementarity of multimodal time-series of reading data, as well as some results of modelling adults’ reading data by integrating different modalities. The total dataset size amounts to about 2.5 GByte in compressed format.</abstract>
      <url hash="1385445b">2024.lrec-main.1188</url>
      <bibkey>ferro-etal-2024-readlet-dataset</bibkey>
    </paper>
    <paper id="1189">
      <title>Reassessing Semantic Knowledge Encoded in Large Language Models through the Word-in-Context Task</title>
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <pages>13610–13620</pages>
      <abstract>Despite the remarkable recent advancements in large language models (LLMs), a comprehensive understanding of their inner workings and the depth of their knowledge remains elusive. This study aims to reassess the semantic knowledge encoded in LLMs by utilizing the Word-in-Context (WiC) task, which involves predicting the semantic equivalence of a target word across different contexts, as a probing task. To address this challenge, we start by prompting LLMs, specifically GPT-3 and GPT-4, to generate natural language descriptions that contrast the meanings of the target word in two contextual sentences given in the WiC dataset. Subsequently, we conduct a manual analysis to examine their linguistic attributes. In parallel, we train a text classification model that utilizes the generated descriptions as supervision and assesses their practical effectiveness in the WiC task. The linguistic and empirical findings reveal a consistent provision of valid and valuable descriptions by LLMs, with LLM-generated descriptions significantly improving classification accuracy. Notably, the highest classification result achieved with GPT-3-generated descriptions largely surpassed GPT-3’s zero-shot baseline. However, the GPT-4-generated descriptions performed slightly below GPT-4’s zero-shot baseline, suggesting that the full potential of the most advanced large language models, such as GPT-4, is yet to be fully revealed.</abstract>
      <url hash="fb065d49">2024.lrec-main.1189</url>
      <bibkey>hayashi-2024-reassessing-semantic</bibkey>
    </paper>
    <paper id="1190">
      <title>Rebalancing Label Distribution While Eliminating Inherent Waiting Time in Multi Label Active Learning Applied to Transformers</title>
      <author><first>Maxime</first><last>Arens</last></author>
      <author><first>Lucile</first><last>Callebert</last></author>
      <author><first>Mohand</first><last>Boughanem</last></author>
      <author><first>Jose G.</first><last>Moreno</last></author>
      <pages>13621–13632</pages>
      <abstract>Data annotation is crucial for machine learning, notably in technical domains, where the quality and quantity of annotated data, significantly affect effectiveness of trained models. Employing humans is costly, especially when annotating for multi-label classification, as instances may bear multiple labels. Active Learning (AL) aims to alleviate annotation costs by intelligently selecting instances for annotation, rather than randomly annotating. Recent attention on transformers has spotlighted the potential of AL in this context. However, in practical settings, implementing AL faces challenges beyond theory. Notably, the gap between AL cycles presents idle time for annotators. To address this issue, we investigate alternative instance selection methods, aiming to maximize annotation efficiency by seamlessly integrating with the AL process. We begin by evaluating two existing methods in our transformer setting, employing respectively random sampling and outdated information. Following this we propose our novel method based on annotating instances to rebalance label distribution. Our approach mitigates biases, enhances model performance (up to 23% improvement on f1score), reduces strategy-dependent disparities (decrease of nearly 50% on standard deviation) and reduces label imbalance (decrease of 30% on Mean Imbalance Ratio).</abstract>
      <url hash="2fc9ef79">2024.lrec-main.1190</url>
      <bibkey>arens-etal-2024-rebalancing-label</bibkey>
    </paper>
    <paper id="1191">
      <title><fixed-case>R</fixed-case>e<fixed-case>CAP</fixed-case>: Semantic Role Enhanced Caption Generation</title>
      <author><first>Abhidip</first><last>Bhattacharyya</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Christoffer</first><last>Heckman</last></author>
      <pages>13633–13649</pages>
      <abstract>Even though current vision language (V+L) models have achieved success in generating image captions, they often lack specificity and overlook various aspects of the image. Additionally, the attention learned through weak supervision operates opaquely and is difficult to control. To address these limitations, we propose the use of semantic roles as control signals in caption generation. Our hypothesis is that, by incorporating semantic roles as signals, the generated captions can be guided to follow specific predicate argument structures. To validate the effectiveness of our approach, we conducted experiments using data and compared the results with a baseline model VL-BART(CITATION). The experiments showed a significant improvement, with a gain of 45% in Smatch score (Standard NLP evaluation metric for semantic representations), demonstrating the efficacy of our approach. By focusing on specific objects and their associated semantic roles instead of providing a general description, our framework produces captions that exhibit enhanced quality, diversity, and controllability.</abstract>
      <url hash="fc7f3d03">2024.lrec-main.1191</url>
      <bibkey>bhattacharyya-etal-2024-recap-semantic</bibkey>
    </paper>
    <paper id="1192">
      <title>Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations</title>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>13650–13665</pages>
      <abstract>Enhancing user engagement through personalization in conversational agents has gained significance, especially with the advent of large language models that generate fluent responses. Personalized dialogue generation, however, is multifaceted and varies in its definition – ranging from instilling a persona in the agent to capturing users’ explicit and implicit cues. This paper seeks to systemically survey the recent landscape of personalized dialogue generation, including the datasets employed, methodologies developed, and evaluation metrics applied. Covering 22 datasets, we highlight benchmark datasets and newer ones enriched with additional features. We further analyze 17 seminal works from top conferences between 2021-2023 and identify five distinct types of problems. We also shed light on recent progress by LLMs in personalized dialogue generation. Our evaluation section offers a comprehensive summary of assessment facets and metrics utilized in these works. In conclusion, we discuss prevailing challenges and envision prospect directions for future research in personalized dialogue generation.</abstract>
      <url hash="c18d803d">2024.lrec-main.1192</url>
      <bibkey>chen-etal-2024-recent-trends</bibkey>
    </paper>
    <paper id="1193">
      <title><fixed-case>RECIPE</fixed-case>4<fixed-case>U</fixed-case>: Student-<fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Interaction Dataset in <fixed-case>EFL</fixed-case> Writing Education</title>
      <author><first>Jieun</first><last>Han</last></author>
      <author><first>Haneul</first><last>Yoo</last></author>
      <author><first>Junho</first><last>Myung</last></author>
      <author><first>Minsun</first><last>Kim</last></author>
      <author><first>Tak Yeon</first><last>Lee</last></author>
      <author><first>So-Yeon</first><last>Ahn</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>13666–13676</pages>
      <abstract>The integration of generative AI in education is expanding, yet empirical analyses of large-scale and real-world interactions between students and AI systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE for University), a dataset sourced from a semester-long experiment with 212 college students in English as Foreign Language (EFL) writing courses. During the study, students engaged in dialogues with ChatGPT to revise their essays. RECIPE4U includes comprehensive records of these interactions, including conversation logs, students’ intent, students’ self-rated satisfaction, and students’ essay edit histories. In particular, we annotate the students’ utterances in RECIPE4U with 13 intention labels based on our coding schemes. We establish baseline results for two subtasks in task-oriented dialogue systems within educational contexts: intent detection and satisfaction estimation. As a foundational step, we explore student-ChatGPT interaction patterns through RECIPE4U and analyze them by focusing on students’ dialogue, essay data statistics, and students’ essay edits. We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of LLMs in educational frameworks. RECIPE4U is publicly available at https://zeunie.github.io/RECIPE4U/.</abstract>
      <url hash="284ed69f">2024.lrec-main.1193</url>
      <bibkey>han-etal-2024-recipe4u-student</bibkey>
    </paper>
    <paper id="1194">
      <title>Recognizing Social Cues in Crisis Situations</title>
      <author><first>Di</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Zhuang</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <author><first>Marina</first><last>Kogan</last></author>
      <pages>13677–13687</pages>
      <abstract>During crisis situations, observations of other people’s behaviors often play an essential role in a person’s decision-making. For example, a person might evacuate before a hurricane only if everyone else in the neighborhood does so. Conversely, a person might stay if no one else is leaving. Such observations are called social cues. Social cues are important for understanding people’s response to crises, so recognizing them can help inform the decisions of government officials and emergency responders. In this paper, we propose the first NLP task to categorize social cues in social media posts during crisis situations. We introduce a manually annotated dataset of 6,000 tweets, labeled with respect to eight social cue categories. We also present experimental results of several classification models, which show that some types of social cues can be recognized reasonably well, but overall this task is challenging for NLP systems. We further present error analyses to identify specific types of mistakes and promising directions for future research on this task.</abstract>
      <url hash="a6dbf603">2024.lrec-main.1194</url>
      <bibkey>wang-etal-2024-recognizing-social</bibkey>
    </paper>
    <paper id="1195">
      <title>Recognizing Value Resonance with Resonance-Tuned <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Task Definition, Experimental Validation, and Robust Modeling</title>
      <author><first>Noam K.</first><last>Benkler</last></author>
      <author><first>Scott</first><last>Friedman</last></author>
      <author><first>Sonja</first><last>Schmer-Galunder</last></author>
      <author><first>Drisana Marissa</first><last>Mosaphir</last></author>
      <author><first>Robert P.</first><last>Goldman</last></author>
      <author><first>Ruta</first><last>Wheelock</last></author>
      <author><first>Vasanth</first><last>Sarathy</last></author>
      <author><first>Pavan</first><last>Kantharaju</last></author>
      <author><first>Matthew D.</first><last>McLure</last></author>
      <pages>13688–13698</pages>
      <abstract>Understanding the implicit values and beliefs of diverse groups and cultures using qualitative texts – such as long-form narratives – and domain-expert interviews is a fundamental goal of social anthropology. This paper builds upon a 2022 study that introduced the NLP task of Recognizing Value Resonance (RVR) for gauging perspective – positive, negative, or neutral – on implicit values and beliefs in textual pairs. This study included a novel hand-annotated dataset, the World Values Corpus (WVC), designed to simulate the task of RVR, and a transformer-based model, Resonance-Tuned RoBERTa, designed to model the task. We extend existing work by refining the task definition and releasing the World Values Corpus (WVC) dataset. We further conduct several validation experiments designed to robustly evaluate the need for task specific modeling, even in the world of LLMs. Finally, we present two additional Resonance-Tuned models trained over extended RVR datasets, designed to improve RVR model versatility and robustness. Our results demonstrate that the Resonance-Tuned models outperform top-performing Recognizing Textual Entailment (RTE) models in recognizing value resonance as well as zero-shot GPT-3.5 under several different prompt structures, emphasizing its practical applicability. Our findings highlight the potential of RVR in capturing cultural values within texts and the importance of task-specific modeling.</abstract>
      <url hash="5296ac33">2024.lrec-main.1195</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d0156836">2024.lrec-main.1195.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>benkler-etal-2024-recognizing-value</bibkey>
    </paper>
    <paper id="1196">
      <title>Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines</title>
      <author><first>Kehan</first><last>Long</last></author>
      <author><first>Shasha</first><last>Li</last></author>
      <author><first>Pancheng</first><last>Wang</last></author>
      <author><first>Chenlong</first><last>Bao</last></author>
      <author><first>Jintao</first><last>Tang</last></author>
      <author><first>Ting</first><last>Wang</last></author>
      <pages>13699–13711</pages>
      <abstract>Citing comprehensively and appropriately has become a challenging task with the explosive growth of scientific publications. Current citation recommendation systems aim to recommend a list of scientific papers for a given text context or a draft paper. However, none of the existing work focuses on already included citations of full papers, which are imperfect and still have much room for improvement. In the scenario of peer reviewing, it is a common phenomenon that submissions are identified as missing vital citations by reviewers. This may lead to a negative impact on the credibility and validity of the research presented. To help improve citations of full papers, we first define a novel task of Recommending Missed Citations Identified by Reviewers (RMC) and construct a corresponding expert-labeled dataset called CitationR. We conduct an extensive evaluation of several state-of-the-art methods on CitationR. Furthermore, we propose a new framework RMCNet with an Attentive Reference Encoder module mining the relevance between papers, already-made citations, and missed citations. Empirical results prove that RMC is challenging, with the proposed architecture outperforming previous methods in all metrics. We release our dataset and benchmark models to motivate future research on this challenging new task.</abstract>
      <url hash="3a9d3412">2024.lrec-main.1196</url>
      <bibkey>long-etal-2024-recommending-missed</bibkey>
    </paper>
    <paper id="1197">
      <title>Reconstruction of Cuneiform Literary Texts as Text Matching</title>
      <author><first>Fabian</first><last>Simonjetz</last></author>
      <author><first>Jussi</first><last>Laasonen</last></author>
      <author><first>Yunus</first><last>Cobanoglu</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Enrique</first><last>Jiménez</last></author>
      <pages>13712–13721</pages>
      <abstract>Ancient Mesopotamian literature is riddled with gaps, caused by the decay and fragmentation of its writing material, clay tablets. The discovery of overlaps between fragments allows reconstruction to advance, but it is a slow and unsystematic process. Since new pieces are found and digitized constantly, NLP techniques can help to identify fragments and match them with existing text collections to restore complete literary works. We compare a number of approaches and determine that a character-level n-gram-based similarity matching approach works well for this problem, leading to a large speed-up for researchers in Assyriology.</abstract>
      <url hash="5dae1cf3">2024.lrec-main.1197</url>
      <bibkey>simonjetz-etal-2024-reconstruction-cuneiform</bibkey>
    </paper>
    <paper id="1198">
      <title>Reduce Redundancy Then Rerank: Enhancing Code Summarization with a Novel Pipeline Framework</title>
      <author><first>Xiaoyu</first><last>Hu</last></author>
      <author><first>Xu</first><last>Zhang</last></author>
      <author><first>Zexu</first><last>Lin</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <pages>13722–13733</pages>
      <abstract>Code summarization is the task of automatically generating natural language descriptions from source code. Recently, pre-trained language models have gained significant popularity in code summarization due to their capacity to capture richer semantic representations of both code and natural language. Nonetheless, contemporary code summarization models grapple with two fundamental limitations. (1) Some tokens in the code are irrelevant to the natural language description and damage the alignment of the representation spaces for code and language. (2) Most approaches are based on the encoder-decoder framework, which is often plagued by the exposure bias problem, hampering the effectiveness of their decoding sampling strategies. To address the two challenges, we propose a novel pipeline framework named Reduce Redundancy then Rerank (Reˆ3). Specifically, a redundancy reduction component is introduced to eliminate redundant information in code representation space. Moreover, a re-ranking model is incorporated to select more suitable summary candidates, alleviating the exposure bias problem. The experimental results show the effectiveness of Reˆ3 over some state-of-the-art approaches across six different datasets from the CodeSearchNet benchmark.</abstract>
      <url hash="53f0700c">2024.lrec-main.1198</url>
      <bibkey>hu-etal-2024-reduce-redundancy</bibkey>
    </paper>
    <paper id="1199">
      <title>Re-evaluating the Tomes for the Times</title>
      <author><first>Ryan</first><last>Brate</last></author>
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <pages>13734–13739</pages>
      <abstract>Literature is to some degree a snapshot of the time it was written in and the societal attitudes of the time. Not all depictions are pleasant or in-line with modern-day sensibilities; this becomes problematic when the prevalent depictions over a large body of work are negatively biased, leading to their normalisation. Many much-loved and much-read classics are set in periods of heightened social inequality: slavery, pre-womens’ rights movements, colonialism, etc. In this paper, we exploit known text co-occurrence metrics with respect to token-level level contexts to identify prevailing themes associated with known problematic descriptors. We see that prevalent, negative depictions are perpetuated by classic literature. We propose that such a methodology could form the basis of a system for making explicit such problematic associations, for interested parties: such as, sensitivity coordinators of publishing houses, library curators, or organisations concerned with social justice</abstract>
      <url hash="fdb967e8">2024.lrec-main.1199</url>
      <bibkey>brate-etal-2024-evaluating-tomes</bibkey>
    </paper>
    <paper id="1200">
      <title><fixed-case>REF</fixed-case>e<fixed-case>REE</fixed-case>: A <fixed-case>RE</fixed-case>ference-<fixed-case>FREE</fixed-case> Model-Based Metric for Text Simplification</title>
      <author><first>Yichen</first><last>Huang</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last></author>
      <pages>13740–13753</pages>
      <abstract>Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly. We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available. Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time.</abstract>
      <url hash="7fd1198e">2024.lrec-main.1200</url>
      <bibkey>huang-kochmar-2024-referee-reference</bibkey>
    </paper>
    <paper id="1201">
      <title>Reference-guided Style-Consistent Content Transfer</title>
      <author><first>Wei-Fan</first><last>Chen</last></author>
      <author><first>Milad</first><last>Alshomary</last></author>
      <author><first>Maja</first><last>Stahl</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>13754–13768</pages>
      <abstract>In this paper, we introduce the task of style-consistent content transfer, which concerns modifying a text’s content based on a provided reference statement while preserving its original style. We approach the task by employing multi-task learning to ensure that the modified text meets three important conditions: reference faithfulness, style adherence, and coherence. In particular, we train three independent classifiers for each condition. During inference, these classifiers are used to determine the best modified text variant. Our evaluation, conducted on hotel reviews and news articles, compares our approach with sequence-to-sequence and error correction baselines. The results demonstrate that our approach reasonably generates text satisfying all three conditions. In subsequent analyses, we highlight the strengths and limitations of our approach, providing valuable insights for future research directions.</abstract>
      <url hash="45233310">2024.lrec-main.1201</url>
      <bibkey>chen-etal-2024-reference-guided</bibkey>
    </paper>
    <paper id="1202">
      <title>Reference-less Analysis of Context Specificity in Translation with Personalised Language Models</title>
      <author><first>Sebastian</first><last>Vincent</last></author>
      <author><first>Rowanne</first><last>Sumner</last></author>
      <author><first>Alice</first><last>Dowek</last></author>
      <author><first>Charlotte</first><last>Prescott</last></author>
      <author><first>Emily</first><last>Preston</last></author>
      <author><first>Chris</first><last>Bayliss</last></author>
      <author><first>Chris</first><last>Oakley</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <pages>13769–13784</pages>
      <abstract>Sensitising language models (LMs) to external context helps them to more effectively capture the speaking patterns of individuals with specific characteristics or in particular environments. This work investigates to what extent detailed character and film annotations can be leveraged to personalise LMs in a scalable manner. We then explore the use of such models in evaluating context specificity in machine translation. We build LMs which leverage rich contextual information to reduce perplexity by up to 6.5% compared to a non-contextual model, and generalise well to a scenario with no speaker-specific data, relying on combinations of demographic characteristics expressed via metadata. Our findings are consistent across two corpora, one of which (Cornell-rich) is also a contribution of this paper. We then use our personalised LMs to measure the co-occurrence of extra-textual context and translation hypotheses in a machine translation setting. Our results suggest that the degree to which professional translations in our domain are context-specific can be preserved to a better extent by a contextual machine translation model than a non-contextual model, which is also reflected in the contextual model’s superior reference-based scores.</abstract>
      <url hash="2e15f2d8">2024.lrec-main.1202</url>
      <bibkey>vincent-etal-2024-reference-less</bibkey>
    </paper>
    <paper id="1203">
      <title>Refining Idioms Semantics Comprehension via Contrastive Learning and Cross-Attention</title>
      <author><first>Mingmin</first><last>Wu</last></author>
      <author><first>Guixin</first><last>Su</last></author>
      <author><first>Yongcheng</first><last>Zhang</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Ying</first><last>Sha</last></author>
      <pages>13785–13795</pages>
      <abstract>Chinese idioms on social media demand a nuanced understanding for correct usage. The Chinese idiom cloze test poses a unique challenge for machine reading comprehension due to the figurative meanings of idioms deviating from their literal interpretations, resulting in a semantic bias in models’ comprehension of idioms. Furthermore, given that the figurative meanings of many idioms are similar, their use as suboptimal options can interfere with optimal selection. Despite achieving some success in the Chinese idiom cloze test, existing methods based on deep learning still struggle to comprehensively grasp idiom semantics due to the aforementioned issues. To tackle these challenges, we introduce a Refining Idioms Semantics Comprehension Framework (RISCF) to capture the comprehensive idioms semantics. Specifically, we propose a semantic sense contrastive learning module to enhance the representation of idiom semantics, diminishing the semantic bias between figurative and literal meanings of idioms. Meanwhile, we propose an interference-resistant cross-attention module to attenuate the interference of suboptimal options, which considers the interaction between the candidate idioms and the blank space in the context. Experimental results on the benchmark datasets demonstrate the effectiveness of our RISCF model, which outperforms state-of-the-art methods significantly.</abstract>
      <url hash="cfcb37f0">2024.lrec-main.1203</url>
      <bibkey>wu-etal-2024-refining-idioms</bibkey>
    </paper>
    <paper id="1204">
      <title>Refining rt<fixed-case>MRI</fixed-case> Landmark-Based Vocal Tract Contour Labels with <fixed-case>FCN</fixed-case>-Based Smoothing and Point-to-Curve Projection</title>
      <author><first>Mushaffa Rasyid</first><last>Ridha</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <pages>13796–13802</pages>
      <abstract>Advanced real-time Magnetic Resonance Imaging (rtMRI) enables researchers to study dynamic articulatory movements during speech production with high temporal resolution. However, accurately outlining articulator contours in high-frame-rate rtMRI presents challenges due to data scalability and image quality issues, making manual and automatic labeling difficult. The widely used publicly available USC-TIMIT dataset offers rtMRI data with landmark-based contour labels derived from unsupervised region segmentation using spatial frequency domain representation and gradient descent optimization. Unfortunately, occasional labeling errors exist, and many contour detection methods were trained and tested based on this ground truth, which is not purely a gold label, with the resulting contour data largely remaining undisclosed to the public. This paper offers a refinement of landmark-based vocal-tract contour labels by employing outlier removal, full convolutional network (FCN)-based smoothing, and a landmark point-to-edge curve projection technique. Since there is no established ground truth label, we evaluate the quality of the new labels through subjective assessments of several contour areas, comparing them to the existing data labels.</abstract>
      <url hash="673693a2">2024.lrec-main.1204</url>
      <bibkey>ridha-sakti-2024-refining-rtmri</bibkey>
    </paper>
    <paper id="1205">
      <title>Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels</title>
      <author><first>Kexin</first><last>Luo</last></author>
      <author><first>Yue</first><last>Mao</last></author>
      <author><first>Bei</first><last>Zhang</last></author>
      <author><first>Sophie</first><last>Hao</last></author>
      <pages>13803–13812</pages>
      <abstract>Inspired by the concept of the male gaze (Mulvey, 1975) in literature and media studies, this paper proposes a framework for analyzing gender bias in terms of female objectification—the extent to which a text portrays female individuals as objects of visual pleasure. Our framework measures female objectification along two axes. First, we compute an agency bias score that indicates whether male entities are more likely to appear in the text as grammatical agents than female entities. Next, by analyzing the word embedding space induced by a text (Caliskan et al., 2017), we compute an appearance bias score that indicates whether female entities are more closely associated with appearance-related words than male entities. Applying our framework to 19th and 20th century novels reveals evidence of female objectification in literature: we find that novels written from a male perspective systematically objectify female characters, while novels written from a female perspective do not exhibit statistically significant objectification of any gender.</abstract>
      <url hash="2f4a1a2e">2024.lrec-main.1205</url>
      <bibkey>luo-etal-2024-reflecting-male</bibkey>
    </paper>
    <paper id="1206">
      <title>Reflections &amp; Resonance: Two-Agent Partnership for Advancing <fixed-case>LLM</fixed-case>-based Story Annotation</title>
      <author><first>Yuetian</first><last>Chen</last></author>
      <author><first>Mei</first><last>Si</last></author>
      <pages>13813–13818</pages>
      <abstract>We introduce a novel multi-agent system for automating story annotation through the generation of tailored prompts for a large language model (LLM). This system utilizes two agents: Agent A is responsible for generating prompts that identify the key information necessary for reconstructing the story, while Agent B reconstructs the story from these annotations and provides feedback to refine the initial prompts. Human evaluations and perplexity scores revealed that optimized prompts significantly enhance the model’s narrative reconstruction accuracy and confidence, demonstrating that dynamic interaction between agents substantially boosts the annotation process’s precision and efficiency. Utilizing this innovative approach, we created the “StorySense” corpus, containing 615 stories, meticulously annotated to facilitate comprehensive story analysis. The paper also demonstrates the practical application of our annotated dataset by drawing the story arcs of two distinct stories, showcasing the utility of the annotated information in story structure analysis and understanding.</abstract>
      <url hash="b819a2de">2024.lrec-main.1206</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e634474a">2024.lrec-main.1206.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>chen-si-2024-reflections-resonance</bibkey>
    </paper>
    <paper id="1207">
      <title><fixed-case>R</fixed-case>eflect<fixed-case>S</fixed-case>umm: A Benchmark for Course Reflection Summarization</title>
      <author><first>Mohamed</first><last>Elaraby</last></author>
      <author><first>Yang</first><last>Zhong</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <author><first>Ahmed Ashraf</first><last>Butt</last></author>
      <author><first>Muhsin</first><last>Menekse</last></author>
      <pages>13819–13846</pages>
      <abstract>This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students’ reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, with potential implications in the opinion summarization domain in general and the educational domain in particular. The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide benchmarks for facilitating further research in this area.</abstract>
      <url hash="c6381c30">2024.lrec-main.1207</url>
      <bibkey>elaraby-etal-2024-reflectsumm-benchmark</bibkey>
    </paper>
    <paper id="1208">
      <title>Reimagining Intent Prediction: Insights from Graph-Based Dialogue Modeling and Sentence Encoders</title>
      <author><first>Daria Romanovna</first><last>Ledneva</last></author>
      <author><first>Denis Pavlovich</first><last>Kuznetsov</last></author>
      <pages>13847–13860</pages>
      <abstract>This paper presents a innovative approach tailored to the specific characteristics of closed-domain dialogue systems. Leveraging scenario dialog graphs, our method effectively addresses the challenges posed by highly specialized fields, where context comprehension is of paramount importance. By modeling dialogues as sequences of transitions between intents, representing distinct goals or requests, our approach focuses on accurate intent prediction for generating contextually relevant responses. The study conducts a thorough evaluation, comparing the performance of state-of-the-art sentence encoders in conjunction with graph-based models across diverse datasets encompassing both open and closed domains. The results highlight the superiority of our methodology, offering fresh perspectives on the integration of advanced sentence encoders and graph models for precise and contextually-driven intent prediction in dialogue systems. Additionally, the use of this approach enhances the transparency of generated output, enabling a deeper understanding of the reasoning behind system responses. This study significantly advances the field of dialogue systems, providing valuable insights into the effectiveness and potential limitations of the proposed approaches.</abstract>
      <url hash="58aaefa0">2024.lrec-main.1208</url>
      <bibkey>ledneva-kuznetsov-2024-reimagining-intent</bibkey>
    </paper>
    <paper id="1209">
      <title>Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box <fixed-case>LLM</fixed-case></title>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Gao</last></author>
      <pages>13861–13873</pages>
      <abstract>Retrieval-augmented language models have exhibited promising performance across various areas of natural language processing (NLP), including fact-critical tasks. However, due to the black-box nature of advanced large language models (LLMs) and the non-retrieval-oriented supervision signal of specific tasks, the training of retrieval model faces significant challenges under the setting of black-box LLM. We propose an approach leveraging Fine-grained Feedback with Reinforcement Retrieval (FFRR) to enhance fact-checking on news claims by using black-box LLM. FFRR adopts a two-level strategy to gather fine-grained feedback from the LLM, which serves as a reward for optimizing the retrieval policy, by rating the retrieved documents based on the non-retrieval ground truth of the task. We evaluate our model on two public datasets for real-world news claim verification, and the results demonstrate that FFRR achieves significant improvements over strong LLM-enabled and non-LLM baselines.</abstract>
      <url hash="b61bd10a">2024.lrec-main.1209</url>
      <bibkey>zhang-gao-2024-reinforcement-retrieval</bibkey>
    </paper>
    <paper id="1210">
      <title>Related Work Is All You Need</title>
      <author><first>Rodolfo Joel</first><last>Zevallos</last></author>
      <author><first>John E.</first><last>Ortega</last></author>
      <author><first>Benjamin</first><last>Irving</last></author>
      <pages>13874–13878</pages>
      <abstract>In modern times, generational artificial intelligence is used in several industries and by many people. One use case that can be considered important but somewhat redundant is the act of searching for related work and other references to cite. As an avenue to better ascertain the value of citations and their corresponding locations, we focus on the common “related work” section as a focus of experimentation with the overall objective to generate the section. In this article, we present a corpus with 400k annotations of that distinguish related work from the rest of the references. Additionally, we show that for the papers in our experiments, the related work section represents the paper just as good, and in many cases, better than the rest of the references. We show that this is the case for more than 74% of the articles when using cosine similarity to measure the distance between two common graph neural network algorithms: Prone and Specter.</abstract>
      <url hash="e99fb05a">2024.lrec-main.1210</url>
      <bibkey>zevallos-etal-2024-related-work</bibkey>
    </paper>
    <paper id="1211">
      <title>Relation between Cross-Genre and Cross-Topic Transfer in Dependency Parsing</title>
      <author><first>Vera</first><last>Danilova</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>13879–13884</pages>
      <abstract>Matching genre in training and test data has been shown to improve dependency parsing. However, it is not clear whether the used methods capture only the genre feature. We hypothesize that successful transfer may also depend on topic similarity. Using topic modelling, we assess whether cross-genre transfer in dependency parsing is stable with respect to topic distribution. We show that LAS scores in cross-genre transfer within and across treebanks typically align with topic distances. This indicates that topic is an important explanatory factor for genre transfer.</abstract>
      <url hash="b69d14b0">2024.lrec-main.1211</url>
      <bibkey>danilova-stymne-2024-relation-cross</bibkey>
    </paper>
    <paper id="1212">
      <title>Relation Classification via Bidirectional Prompt Learning with Data Augmentation by Large Language Model</title>
      <author><first>Yizhi</first><last>Jiang</last></author>
      <author><first>Jinlong</first><last>Li</last></author>
      <author><first>Huanhuan</first><last>Chen</last></author>
      <pages>13885–13897</pages>
      <abstract>The Relation Extraction (RE) task aims to extract the relation between two entities in a sentence. As the performance of methods on RE task depends on datasets’ quantity and quality, in this paper, we propose to use the Large Language Model (LLM) to do data augmentation. Moreover, compared to traditional fine-tuning methods, more research focuses on prompt learning. However, all of their prompt templates ignore the relative order of entities, which we believe will affect the prediction error. Due to that, we propose novel bidirectional prompt templates for prompt learning and design a training strategy for utilizing the templates. Then we try to fit the probability distributions of both prompt learning and fine-tuning methods into our model. To this end, we propose Relation Classification via Bidirectional Prompt learning with data augmentation by LLM (RCBP) and conduct experiments on four datasets: TACRED, RETACRED, TACREV and Semeval. The results show that RCBP performs well on these datasets and outperforms the state-of-the-art in the TACREV, RETACRED datasets.</abstract>
      <url hash="86ee31ec">2024.lrec-main.1212</url>
      <attachment type="OptionalSupplementaryMaterial" hash="d598d9d2">2024.lrec-main.1212.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>jiang-etal-2024-relation-classification</bibkey>
    </paper>
    <paper id="1213">
      <title>Release of Pre-Trained Models for the <fixed-case>J</fixed-case>apanese Language</title>
      <author><first>Kei</first><last>Sawada</last></author>
      <author><first>Tianyu</first><last>Zhao</last></author>
      <author><first>Makoto</first><last>Shing</last></author>
      <author><first>Kentaro</first><last>Mitsui</last></author>
      <author><first>Akio</first><last>Kaga</last></author>
      <author><first>Yukiya</first><last>Hono</last></author>
      <author><first>Toshiaki</first><last>Wakatsuki</last></author>
      <author><first>Koh</first><last>Mitsuda</last></author>
      <pages>13898–13905</pages>
      <abstract>AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.</abstract>
      <url hash="234edccd">2024.lrec-main.1213</url>
      <bibkey>sawada-etal-2024-release-pre</bibkey>
    </paper>
    <paper id="1214">
      <title>Releasing the Capacity of <fixed-case>GAN</fixed-case>s in Non-Autoregressive Image Captioning</title>
      <author><first>Da</first><last>Ren</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <pages>13906–13918</pages>
      <abstract>Building Non-autoregressive (NAR) models in image captioning can fundamentally tackle the high inference latency of autoregressive models. However, existing NAR image captioning models are trained on maximum likelihood estimation, and suffer from their inherent multi-modality problem. Although constructing NAR models based on GANs can theoretically tackle this problem, existing GAN-based NAR models obtain poor performance when transferred to image captioning due to their incapacity of modeling complicated relations between images and text. To tackle this problem, we propose an Adversarial Non-autoregressive Transformer for Image Captioning (CaptionANT) by improving performance from two aspects: 1) modifying the model structure so as to be compatible with contrastive learning to effectively make use of unpaired samples; 2) integrating a reconstruction process to better utilize paired samples. By further combining with other effective techniques and our proposed lightweight structure, CaptionANT can better align input images and output text, and thus achieves new state-of-the-art performance for fully NAR models on the challenging MSCOCO dataset. More importantly, CaptionANT achieves a 26.72 times speedup compared to the autoregressive baseline with only 36.3% the number of parameters of the existing best fully NAR model for image captioning.</abstract>
      <url hash="36d4889f">2024.lrec-main.1214</url>
      <bibkey>ren-li-2024-releasing-capacity</bibkey>
    </paper>
    <paper id="1215">
      <title><fixed-case>RENN</fixed-case>: A Rule Embedding Enhanced Neural Network Framework for Temporal Knowledge Graph Completion</title>
      <author><first>Linlin</first><last>Zong</last></author>
      <author><first>Zhenrong</first><last>Xie</last></author>
      <author><first>Chi</first><last>Ma</last></author>
      <author><first>Xinyue</first><last>Liu</last></author>
      <author><first>Xianchao</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <pages>13919–13928</pages>
      <abstract>Temporal knowledge graph completion is a critical task within the knowledge graph domain. Existing approaches encompass deep neural network-based methods for temporal knowledge graph embedding and rule-based logical symbolic reasoning. However, the former may not adequately account for structural dependencies between relations.Conversely, the latter methods relies heavily on strict logical rule reasoning and lacks robustness in the face of fuzzy or noisy data. In response to these challenges, we present RENN, a groundbreaking framework that enhances temporal knowledge graph completion through rule embedding. RENN employs a three-step approach. First, it utilizes temporary random walk to extract temporal logic rules. Then, it pre-trains by learning embeddings for each logical rule and its associated relations, thereby enhancing the likelihood of existing quadruples and logical rules. Finally, it incorporates the embeddings of logical rules into the deep neural network. Our methodology has been validated through experiments conducted on various temporal knowledge graph models and datasets, consistently demonstrating its effectiveness and potential in improving temporal knowledge graph completion.</abstract>
      <url hash="5c8ee0d7">2024.lrec-main.1215</url>
      <bibkey>zong-etal-2024-renn-rule</bibkey>
    </paper>
    <paper id="1216">
      <title>Replace, Paraphrase or Fine-tune? Evaluating Automatic Simplification for Medical Texts in <fixed-case>S</fixed-case>panish</title>
      <author><first>Leonardo</first><last>Campillos-Llanos</last></author>
      <author><first>Ana Rosa</first><last>Terroba</last></author>
      <author><first>Rocío</first><last>Bartolomé</last></author>
      <author><first>Ana</first><last>Valverde-Mateos</last></author>
      <author><first>Cristina</first><last>González</last></author>
      <author><first>Adrián</first><last>Capllonch-Carrión</last></author>
      <author><first>Jonathan</first><last>Heras</last></author>
      <pages>13929–13945</pages>
      <abstract>Patients can not always completely understand medical documents given the myriad of technical terms they contain. Automatic text simplification techniques can help, but they must guarantee that the content is transmitted rigorously and not creating wrong information. In this work, we tested: 1) lexicon-based simplification approaches, using a Spanish lexicon of technical and laymen terms collected for this task (SimpMedLexSp); 2) deep-learning (DL) based methods, with BART-based and prompt-learning-based models; and 3) a combination of both techniques. As a test set, we used 5000 parallel (technical and laymen) sentence pairs: 3800 manually aligned sentences from the CLARA-MeD corpus; and 1200 sentences from clinical trials simplified by linguists. We conducted a quantitative evaluation with standard measures (BLEU, ROUGE and SARI) and a human evaluation, in which eleven subjects scored the simplification output of several methods. In our experiments, the lexicon improved the quantitative results when combined with the DL models. The simplified sentences using only the lexicon were assessed with the highest scores regarding semantic adequacy; however, their fluency needs to be improved. The prompt-method had similar ratings in this aspect and in simplification. We make available the models and the data to reproduce our results.</abstract>
      <url hash="697c8a0a">2024.lrec-main.1216</url>
      <bibkey>campillos-llanos-etal-2024-replace-paraphrase</bibkey>
    </paper>
    <paper id="1217">
      <title>Representation Degeneration Problem in Prompt-based Models for Natural Language Understanding</title>
      <author><first>Qingyan</first><last>Zhao</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Jinpeng</first><last>Zhang</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <pages>13946–13957</pages>
      <abstract>Prompt-based fine-tuning (PF), by aligning with the training objective of pre-trained language models (PLMs), has shown improved performance on many few-shot natural language understanding (NLU) benchmarks. However, the word embedding space of PLMs exhibits anisotropy, which is called the representation degeneration problem. In this paper, we explore the self-similarity within the same context and identify the anisotropy of the feature embedding space in PF model. Given that the performance of PF models is dependent on feature embeddings, we inevitably pose the hypothesis: this anisotropy limits the performance of the PF models. Based on our experimental findings, we propose CLMA, a Contrastive Learning framework based on the [MASK] token and Answers, to alleviate the anisotropy in the embedding space. By combining our proposed counter-intuitive SSD, a Supervised Signal based on embedding Distance, our approach outperforms mainstream methods on the many NLU benchmarks in the few-shot experimental settings. In subsequent experiments, we analyze the capability of our method to capture deep semantic cues and the impact of the anisotropy in the feature embedding space on the performance of the PF model.</abstract>
      <url hash="27fd537c">2024.lrec-main.1217</url>
      <bibkey>zhao-etal-2024-representation-degeneration</bibkey>
    </paper>
    <paper id="1218">
      <title>Representing Compounding with <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex. An Evaluation of Vocabularies for Word Formation Resources</title>
      <author><first>Elena</first><last>Benzoni</last></author>
      <author><first>Matteo</first><last>Pellegrini</last></author>
      <author><first>Francesco</first><last>Dedè</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <pages>13958–13969</pages>
      <abstract>This paper explores how compounds are represented in resources documenting word formation, and proposes ways to convert them into Linked Open Data using the OntoLex model. The ultimate purpose is to offer a broad empirical evaluation of which of the two OntoLex modules allowing for the representation of compounds – Decomp and Morph – fits best the different formats and theoretical approaches of the resources we examine. We show that the vocabulary of Decomp alone is rarely sufficient to account for all relevant facts; in almost all cases, it is necessary to resort to the vocabulary of Morph, either to reify the relation between compounds and their constituents or to represent specifically morphological information or other aspects. Special attention is devoted to the format of the Universal Derivations project: the modelling strategy that we propose can be applied to all resources harmonized in that format, potentially allowing for the conversion into Linked Open Data of a large amount of structured data.</abstract>
      <url hash="8b1cb25b">2024.lrec-main.1218</url>
      <bibkey>benzoni-etal-2024-representing-compounding</bibkey>
    </paper>
    <paper id="1219">
      <title>Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue Systems</title>
      <author><first>Songbo</first><last>Hu</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>13970–13991</pages>
      <abstract>End-to-end task-oriented dialogue systems are prone to fall into the so-called ‘likelihood trap’, resulting in generated responses which are dull, repetitive, and often inconsistent with dialogue history. Comparing ranked lists of multiple generated responses against the ‘gold response’ reveals a wide diversity in quality, with many good responses placed lower in the ranked list. The main challenge addressed in this work is how to reach beyond greedily generated system responses, that is, how to obtain and select high-quality responses from the list of overgenerated responses at inference without the availability of the gold response. To this end, we propose a simple yet effective reranking method to select high-quality items from the lists of initially overgenerated responses. The idea is to use any sequence-level scoring function to divide the semantic space of responses into high-scoring versus low-scoring partitions. At training, the high-scoring partition comprises all generated responses whose similarity to the gold response is higher than the similarity of the greedy response to the gold response. At inference, the aim is to estimate the probability that each overgenerated response belongs to the high-scoring partition. We evaluate our proposed method on the standard MultiWOZ dataset, the BiTOD dataset, and with human evaluation.</abstract>
      <url hash="dba41e6b">2024.lrec-main.1219</url>
      <bibkey>hu-etal-2024-reranking-overgenerated</bibkey>
    </paper>
    <paper id="1220">
      <title>Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents</title>
      <author><first>Ramona</first><last>Christen</last></author>
      <author><first>Anastassia</first><last>Shaitarova</last></author>
      <author><first>Matthias</first><last>Stürmer</last></author>
      <author><first>Joel</first><last>Niklaus</last></author>
      <pages>13992–14004</pages>
      <abstract>Resolving the scope of a negation within a sentence is a challenging NLP task. The complexity of legal texts and the lack of annotated in-domain negation corpora pose challenges for state-of-the-art (SotA) models when performing negation scope resolution on multilingual legal data. Our experiments demonstrate that models pre-trained without legal data underperform in the task of negation scope resolution. We release a new set of annotated court decisions in German, French, and Italian and use it to improve negation scope resolution in both zero-shot and multilingual settings. We achieve token-level F1-scores of up to 86.7% in our zero-shot cross-lingual experiments, where the models are trained on two languages of our legal datasets and evaluated on the third. Our multilingual experiments, where the models were trained on all available negation data and evaluated on our legal datasets, resulted in F1-scores of up to 91.1%.</abstract>
      <url hash="d7cfed3a">2024.lrec-main.1220</url>
      <bibkey>christen-etal-2024-resolving-legalese</bibkey>
    </paper>
    <paper id="1221">
      <title>Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach</title>
      <author><first>Siyu</first><last>Duan</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <pages>14005–14015</pages>
      <abstract>Cultural heritage serves as the enduring record of human thought and history. Despite significant efforts dedicated to the preservation of cultural relics, many ancient artefacts have been ravaged irreversibly by natural deterioration and human actions. Deep learning technology has emerged as a valuable tool for restoring various kinds of cultural heritages, including ancient text restoration. Previous research has approached ancient text restoration from either visual or textual perspectives, often overlooking the potential of synergizing multimodal information. This paper proposes a novel Multimodal Multitask Restoring Model (MMRM) to restore ancient texts, particularly emphasising the ideograph. This model combines context understanding with residual visual information from damaged ancient artefacts, enabling it to predict damaged characters and generate restored images simultaneously. We tested the MMRM model through experiments conducted on both simulated datasets and authentic ancient inscriptions. The results show that the proposed method gives insightful restoration suggestions in both simulation experiments and real-world scenarios. To the best of our knowledge, this work represents the pioneering application of multimodal deep learning in ancient text restoration, which will contribute to the understanding of ancient society and culture in digital humanities fields.</abstract>
      <url hash="c641e9e9">2024.lrec-main.1221</url>
      <bibkey>duan-etal-2024-restoring-ancient</bibkey>
    </paper>
    <paper id="1222">
      <title>Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models</title>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Qiaoyu</first><last>Tang</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Bin</first><last>Dong</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Tianshu</first><last>Wang</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>14016–14036</pages>
      <abstract>Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memorizing dynamic mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models without pre-training are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.</abstract>
      <url hash="58d3c491">2024.lrec-main.1222</url>
      <bibkey>cao-etal-2024-retentive-forgetful</bibkey>
    </paper>
    <paper id="1223">
      <title>Rethinking Word-level Adversarial Attack: The Trade-off between Efficiency, Effectiveness, and Imperceptibility</title>
      <author><first>Pengwei</first><last>Zhan</last></author>
      <author><first>Jing</first><last>Yang</last></author>
      <author><first>He</first><last>Wang</last></author>
      <author><first>Chao</first><last>Zheng</last></author>
      <author><first>Liming</first><last>Wang</last></author>
      <pages>14037–14052</pages>
      <abstract>Neural language models have demonstrated impressive performance in various tasks but remain vulnerable to word-level adversarial attacks. Word-level adversarial attacks can be formulated as a combinatorial optimization problem, and thus, an attack method can be decomposed into search space and search method. Despite the significance of these two components, previous works inadequately distinguish them, which may lead to unfair comparisons and insufficient evaluations. In this paper, to address the inappropriate practices in previous works, we perform thorough ablation studies on the search space, illustrating the substantial influence of search space on attack efficiency, effectiveness, and imperceptibility. Based on the ablation study, we propose two standardized search spaces: the Search Space for ImPerceptibility (SSIP) and Search Space for EffecTiveness (SSET). The reevaluation of eight previous attack methods demonstrates the success of SSIP and SSET in achieving better trade-offs between efficiency, effectiveness, and imperceptibility in different scenarios, offering fair and comprehensive evaluations of previous attack methods and providing potential guidance for future works.</abstract>
      <url hash="70cceac7">2024.lrec-main.1223</url>
      <bibkey>zhan-etal-2024-rethinking-word</bibkey>
    </paper>
    <paper id="1224">
      <title>Retrieval-Augmented Modular Prompt Tuning for Low-Resource Data-to-Text Generation</title>
      <author><first>Ruitao</first><last>Feng</last></author>
      <author><first>Xudong</first><last>Hong</last></author>
      <author><first>Mayank</first><last>Jobanputra</last></author>
      <author><first>Mattes</first><last>Warning</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>14053–14062</pages>
      <abstract>Data-to-text (D2T) generation describes the task of verbalizing data, often given as attribute-value pairs. While this task is relevant for many different data domains beyond the traditionally well-explored tasks of weather forecasting, restaurant recommendations, and sports reporting, a major challenge to the applicability of data-to-text generation methods is typically data sparsity. For many applications, there is extremely little training data in terms of attribute-value inputs and target language outputs available for training a model. Given the sparse data setting, recently developed prompting methods seem most suitable for addressing D2T tasks since they do not require substantial amounts of training data, unlike finetuning approaches. However, prompt-based approaches are also challenging, as a) the design and search of prompts are non-trivial; and b) hallucination problems may occur because of the strong inductive bias of these models. In this paper, we propose a retrieval-augmented modular prompt tuning () method, which constructs prompts that fit the input data closely, thereby bridging the domain gap between the large-scale language model and the structured input data. Experiments show that our method generates texts with few hallucinations and achieves state-of-the-art performance on a dataset for drone handover message generation.</abstract>
      <url hash="805426af">2024.lrec-main.1224</url>
      <bibkey>feng-etal-2024-retrieval-augmented</bibkey>
    </paper>
    <paper id="1225">
      <title>Retrieval-based Question Answering with Passage Expansion Using a Knowledge Graph</title>
      <author><first>Benno</first><last>Kruit</last></author>
      <author><first>Yiming</first><last>Xu</last></author>
      <author><first>Jan-Christoph</first><last>Kalo</last></author>
      <pages>14063–14072</pages>
      <abstract>Recent advancements in dense neural retrievers and language models have led to large improvements in state-of-the-art approaches to open-domain Question Answering (QA) based on retriever-reader architectures. However, issues stemming from data quality and imbalances in the use of dense embeddings have hindered performance, particularly for less common entities and facts. To tackle these problems, this study explores a multi-modal passage retrieval model’s potential to bolster QA system performance. This study poses three key questions: (1) Can a distantly supervised question-relation extraction model enhance retrieval using a knowledge graph (KG), compensating for dense neural retrievers’ shortcomings with rare entities? (2) How does this multi-modal approach compare to existing QA systems based on textual features? (3) Can this QA system alleviate poor performance on less common entities on common benchmarks? We devise a multi-modal retriever combining entity features and textual data, leading to improved retrieval precision in some situations, particularly for less common entities. Experiments across different datasets confirm enhanced performance for entity-centric questions, but challenges remain in handling complex generalized questions.</abstract>
      <url hash="3f8b206d">2024.lrec-main.1225</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7e1896f7">2024.lrec-main.1225.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kruit-etal-2024-retrieval-based</bibkey>
    </paper>
    <paper id="1226">
      <title>Revisiting Context Choices for Context-aware Machine Translation</title>
      <author><first>Matiss</first><last>Rikters</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <pages>14073–14079</pages>
      <abstract>One of the most popular methods for context-aware machine translation (MT) is to use separate encoders for the source sentence and context as multiple sources for one target sentence. Recent work has cast doubt on whether these models actually learn useful signals from the context or are improvements in automatic evaluation metrics just a side-effect. We show that multi-source transformer models improve MT over standard transformer-base models even with empty lines provided as context, but the translation quality improves significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is provided. We also show that even though randomly shuffling in-domain context can also improve over baselines, the correct context further improves translation quality and random out-of-domain context further degrades it.</abstract>
      <url hash="4a962f46">2024.lrec-main.1226</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a030bfed">2024.lrec-main.1226.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>rikters-nakazawa-2024-revisiting-context</bibkey>
    </paper>
    <paper id="1227">
      <title>Revisiting Data Reconstruction Attacks on Real-world Dataset for Federated Natural Language Understanding</title>
      <author><first>Zhuo</first><last>Zhang</last></author>
      <author><first>Jintao</first><last>Huang</last></author>
      <author><first>Xiangjing</first><last>Hu</last></author>
      <author><first>Jingyuan</first><last>Zhang</last></author>
      <author><first>Yating</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Qifan</first><last>Wang</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <pages>14080–14091</pages>
      <abstract>With the growing privacy concerns surrounding natural language understanding (NLU) applications, the need to train high-quality models while safeguarding data privacy has reached unprecedented importance. Federated learning (FL) offers a promising approach to collaborative model training by exchanging model gradients. However, many studies show that eavesdroppers in FL could develop sophisticated data reconstruction attack (DRA) to accurately reconstruct clients’ data from the shared gradients. Regrettably, current DRA methods in federated NLU have been mostly conducted on public datasets, lacking a comprehensive evaluation of real-world privacy datasets. To address this limitation, this paper presents a pioneering study that reexamines the performance of these DRA methods as well as corresponding defense methods. Specifically, we introduce a novel real-world privacy dataset called FedAttack which leads to a significant discovery: existing DRA methods usually fail to accurately recover the original text of real-world privacy data. In detail, the tokens within a recovery sentence are disordered and intertwined with tokens from other sentences in the same training batch. Moreover, our experiments demonstrate that the performance of DRA is also influenced by different languages and domains. By discovering these findings, our work lays a solid foundation for further research into the development of more practical DRA methods and corresponding defenses.</abstract>
      <url hash="5d7c8a3f">2024.lrec-main.1227</url>
      <bibkey>zhang-etal-2024-revisiting-data</bibkey>
    </paper>
    <paper id="1228">
      <title>Revisiting the Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems</title>
      <author><first>Aditya Narayan</first><last>Sankaran</last></author>
      <author><first>Vigneshwaran</first><last>Shankaran</last></author>
      <author><first>Sampath</first><last>Lonka</last></author>
      <author><first>Rajesh</first><last>Sharma</last></author>
      <pages>14092–14102</pages>
      <abstract>Rhymes and poems are a powerful medium for transmitting cultural norms and societal roles. However, the pervasive existence of gender stereotypes in these works perpetuates biased perceptions and limits the scope of individuals’ identities. Past works have shown that stereotyping and prejudice emerge in early childhood, and developmental research on causal mechanisms is critical for understanding and controlling stereotyping and prejudice. This work contributes by gathering a dataset of rhymes and poems to identify gender stereotypes and propose a model with 97% accuracy to identify gender bias. Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications. To summarize, this work highlights the pervasive nature of gender stereotypes in literary works and reveal the potential of LLMs to rectify gender stereotypes. This study raises awareness and promotes inclusivity within artistic expressions, making a significant contribution to the discourse on gender equality.</abstract>
      <url hash="b574d870">2024.lrec-main.1228</url>
      <bibkey>sankaran-etal-2024-revisiting-classics</bibkey>
    </paper>
    <paper id="1229">
      <title>Revisiting the Self-Consistency Challenges in Multi-Choice Question Formats for Large Language Model Evaluation</title>
      <author><first>Wenjie</first><last>Zhou</last></author>
      <author><first>Qiang</first><last>Wang</last></author>
      <author><first>Mingzhou</first><last>Xu</last></author>
      <author><first>Ming</first><last>Chen</last></author>
      <author><first>Xiangyu</first><last>Duan</last></author>
      <pages>14103–14110</pages>
      <abstract>Multi-choice questions (MCQ) are a common method for assessing the world knowledge of large language models (LLMs), demonstrated by benchmarks such as MMLU and C-Eval. However, recent findings indicate that even top-tier LLMs, such as ChatGPT and GPT4, might display inconsistencies when faced with slightly varied inputs. This raises concerns about the credibility of MCQ-based evaluations. To address this issue, we introduced three knowledge-equivalent question variants: option position shuffle, option label replacement, and conversion to a True/False format. We rigorously tested a range of LLMs, varying in model size (from 6B to 70B) and types—pretrained language model (PLM), supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Our findings from MMLU and C-Eval revealed that accuracy for individual questions lacks robustness, particularly in smaller models (&lt;30B) and PLMs. Consequently, we advocate that consistent accuracy may serve as a more reliable metric for evaluating and ranking LLMs.</abstract>
      <url hash="16b576c5">2024.lrec-main.1229</url>
      <bibkey>zhou-etal-2024-revisiting-self</bibkey>
    </paper>
    <paper id="1230">
      <title>Revisiting Three Text-to-Speech Synthesis Experiments with a Web-Based Audience Response System</title>
      <author><first>Christina</first><last>Tånnander</last></author>
      <author><first>Jens</first><last>Edlund</last></author>
      <author><first>Joakim</first><last>Gustafson</last></author>
      <pages>14111–14121</pages>
      <abstract>In order to investigate the strengths and weaknesses of Audience Response System (ARS) in text-to-speech synthesis (TTS) evaluations, we revisit three previously published TTS studies and perform an ARS-based evaluation on the stimuli used in each study. The experiments are performed with a participant pool of 39 respondents, using a web-based tool that emulates an ARS experiment. The results of the first experiment confirms that ARS is highly useful for evaluating long and continuous stimuli, particularly if we wish for a diagnostic result rather than a single overall metric, while the second and third experiments highlight weaknesses in ARS with unsuitable materials as well as the importance of framing and instruction when conducting ARS-based evaluation.</abstract>
      <url hash="d72fda8f">2024.lrec-main.1230</url>
      <bibkey>tannander-etal-2024-revisiting-three</bibkey>
    </paper>
    <paper id="1231">
      <title>Rewiring the Transformer with Depth-Wise <fixed-case>LSTM</fixed-case>s</title>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Yang</first><last>Song</last></author>
      <author><first>Qiuhui</first><last>Liu</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>14122–14133</pages>
      <abstract>Stacking non-linear layers allows deep neural networks to model complicated functions, and including residual connections in Transformer layers is beneficial for convergence and performance. However, residual connections may make the model “forget” distant layers and fail to fuse information from previous layers effectively. Selectively managing the representation aggregation of Transformer layers may lead to better performance. In this paper, we present a Transformer with depth-wise LSTMs connecting cascading Transformer layers and sub-layers. We show that layer normalization and feed-forward computation within a Transformer layer can be absorbed into depth-wise LSTMs connecting pure Transformer attention layers. Our experiments with the 6-layer Transformer show significant BLEU improvements in both WMT 14 English-German / French tasks and the OPUS-100 many-to-many multilingual NMT task, and our deep Transformer experiments demonstrate the effectiveness of depth-wise LSTM on the convergence and performance of deep Transformers.</abstract>
      <url hash="4fcf2eb1">2024.lrec-main.1231</url>
      <bibkey>xu-etal-2024-rewiring-transformer</bibkey>
    </paper>
    <paper id="1232">
      <title><fixed-case>RISE</fixed-case>: Robust Early-exiting Internal Classifiers for Suicide Risk Evaluation</title>
      <author><first>Ritesh Singh</first><last>Soun</last></author>
      <author><first>Atula Tejaswi</first><last>Neerkaje</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>14134–14145</pages>
      <abstract>Suicide is a serious public health issue, but it is preventable with timely intervention. Emerging studies have suggested there is a noticeable increase in the number of individuals sharing suicidal thoughts online. As a result, utilising advance Natural Language Processing techniques to build automated systems for risk assessment is a viable alternative. However, existing systems are prone to incorrectly predicting risk severity and have no early detection mechanisms. Therefore, we propose RISE, a novel robust mechanism for accurate early detection of suicide risk by ensembling Hyperbolic Internal Classifiers equipped with an abstention mechanism and early-exit inference capabilities. Through quantitative, qualitative and ablative experiments, we demonstrate RISE as an efficient and robust human-in-the-loop approach for risk assessment over the Columbia Suicide Severity Risk Scale (C-SSRS) and CLPsych 2022 datasets. It is able to successfully abstain from 84% incorrect predictions on Reddit data while out-predicting state of the art models upto 3.5x earlier.</abstract>
      <url hash="3578caec">2024.lrec-main.1232</url>
      <bibkey>soun-etal-2024-rise-robust</bibkey>
    </paper>
    <paper id="1233">
      <title><fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a Low Resource Fine Tuning for Sentiment Analysis in <fixed-case>A</fixed-case>lbanian</title>
      <author><first>Krenare Pireva</first><last>Nuci</last></author>
      <author><first>Paul</first><last>Landes</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <pages>14146–14151</pages>
      <abstract>The education domain has been a popular area of collaboration with NLP researchers for decades. However, many recent breakthroughs, such as large transformer based language models, have provided new opportunities for solving interesting, but difficult problems. One such problem is assigning sentiment to reviews of educators’ performance. We present EduSenti: a corpus of 1,163 Albanian and 624 English reviews of educational instructor’s performance reviews annotated for sentiment, emotion and educational topic. In this work, we experiment with fine-tuning several language models on the EduSenti corpus and then compare with an Albanian masked language trained model from the last XLM-RoBERTa checkpoint. We show promising results baseline results, which include an F1 of 71.9 in Albanian and 73.8 in English. Our contributions are: (i) a sentiment analysis corpus in Albanian and English, (ii) a large Albanian corpus of crawled data useful for unsupervised training of language models, and (iii) the source code for our experiments.</abstract>
      <url hash="248bf96a">2024.lrec-main.1233</url>
      <bibkey>nuci-etal-2024-roberta-low</bibkey>
    </paper>
    <paper id="1234">
      <title><fixed-case>R</fixed-case>obo<fixed-case>V</fixed-case>ox: A Single/Multi-channel Far-field Speaker Recognition Benchmark for a Mobile Robot</title>
      <author><first>Mohammad</first><last>Mohammadamini</last></author>
      <author><first>Driss</first><last>Matrouf</last></author>
      <author><first>Michael</first><last>Rouvier</last></author>
      <author><first>Jean-Francois</first><last>Bonastre</last></author>
      <author><first>Romain</first><last>Serizel</last></author>
      <author><first>Theophile</first><last>Gonos</last></author>
      <pages>14152–14156</pages>
      <abstract>In this paper, we introduce a new far-field speaker recognition benchmark called RoboVox. RoboVox is a French corpus recorded by a mobile robot. The files are recorded from different distances under severe acoustical conditions with the presence of several types of noise and reverberation. In addition to noise and reverberation, the robot’s internal noise acts as an extra additive noise. RoboVox can be used for both single-channel and multi-channel speaker recognition. In the evaluation protocols, we are considering both cases. The obtained results demonstrate a significant decline in performance in far-filed speaker recognition and urge the community to further research in this domain</abstract>
      <url hash="d6db3e8c">2024.lrec-main.1234</url>
      <bibkey>mohammadamini-etal-2024-robovox-single</bibkey>
    </paper>
    <paper id="1235">
      <title>Robust and Scalable Model Editing for Large Language Models</title>
      <author><first>Yingfa</first><last>Chen</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Kuai</first><last>Li</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>14157–14172</pages>
      <abstract>Large language models (LLMs) can make predictions using *parametric knowledge* – knowledge encoded in the model weights – or *contextual knowledge* – knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model’s knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets. Empirical results show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa). The source code can be found at https://github.com/thunlp/EREN.</abstract>
      <url hash="b22d3876">2024.lrec-main.1235</url>
      <bibkey>chen-etal-2024-robust-scalable</bibkey>
    </paper>
    <paper id="1236">
      <title><fixed-case>R</fixed-case>o<fixed-case>C</fixed-case>ode: A Dataset for Measuring Code Intelligence from Problem Definitions in <fixed-case>R</fixed-case>omanian</title>
      <author><first>Adrian</first><last>Cosma</last></author>
      <author><first>Ioan-Bogdan</first><last>Iordache</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>14173–14185</pages>
      <abstract>Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.</abstract>
      <url hash="00aebf0d">2024.lrec-main.1236</url>
      <bibkey>cosma-etal-2024-rocode-dataset</bibkey>
    </paper>
    <paper id="1237">
      <title><fixed-case>R</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>I</fixed-case>ns: Enhancing Robustness of Large Language Models through Code-Style Instructions</title>
      <author><first>Yuansen</first><last>Zhang</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Han</first><last>Xia</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>14186–14203</pages>
      <abstract>Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs for natural language understanding (NLU) tasks when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (adversarial context method) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural language, for example, with gpt-3.5-turbo on average, our method achieves an improvement of 5.68% in test set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).</abstract>
      <url hash="a38bee75">2024.lrec-main.1237</url>
      <bibkey>zhang-etal-2024-rocoins-enhancing</bibkey>
    </paper>
    <paper id="1238">
      <title><fixed-case>RT</fixed-case>-<fixed-case>VQ</fixed-case>2<fixed-case>A</fixed-case>2: Real Time Vector Quantized Question Answering with <fixed-case>ASR</fixed-case></title>
      <author><first>Kyungho</first><last>Kim</last></author>
      <author><first>Seongmin</first><last>Park</last></author>
      <author><first>Jihwa</first><last>Lee</last></author>
      <pages>14204–14214</pages>
      <abstract>In Spoken Question Answering (SQA), automatic speech recognition (ASR) outputs are often relayed to language models for QA. However, constructing such a cascaded framework with large language models (LLMs) in a real-time SQA setting involves realistic challenges, such as noise in the ASR output, the limited context length of LLMs, and latency in processing large models. This paper proposes a novel model-agnostic framework, RT-VQ2A2, to address these challenges. RT-VQ2A2 consists of three steps: codebook preparation, quantized semantic vector extractor, and dual segment selector. We construct a codebook from clustering, removing outliers on a text corpus derived from ASR to mitigate the influence of ASR error. Extracting quantized semantic vectors through a pre-built codebook shows significant speed and performance improvements in relevant context retrieval. Dual segment selector considers both semantic and lexical aspects to deal with ASR error. The efficacy of RT-VQ2A2 is validated on the widely used Spoken-SQuAD dataset.</abstract>
      <url hash="2b00c28e">2024.lrec-main.1238</url>
      <bibkey>kim-etal-2024-rt-vq2a2</bibkey>
    </paper>
    <paper id="1239">
      <title><fixed-case>RU</fixed-case>22<fixed-case>F</fixed-case>act: Optimizing Evidence for Multilingual Explainable Fact-Checking on <fixed-case>R</fixed-case>ussia-<fixed-case>U</fixed-case>kraine Conflict</title>
      <author><first>Yirong</first><last>Zeng</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Yi</first><last>Zhao</last></author>
      <author><first>Xiangyu</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Yao</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>14215–14226</pages>
      <abstract>Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks.</abstract>
      <url hash="d8b3777f">2024.lrec-main.1239</url>
      <bibkey>zeng-etal-2024-ru22fact-optimizing</bibkey>
    </paper>
    <paper id="1240">
      <title><fixed-case>R</fixed-case>u<fixed-case>B</fixed-case>ia: A <fixed-case>R</fixed-case>ussian Language Bias Detection Dataset</title>
      <author><first>Veronika</first><last>Grigoreva</last></author>
      <author><first>Anastasiia</first><last>Ivanova</last></author>
      <author><first>Ilseyar</first><last>Alimova</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>14227–14239</pages>
      <abstract>Warning: this work contains upsetting or disturbing content. Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if an LLM’s behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To illustrate the dataset’s purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs’ predisposition to social biases.</abstract>
      <url hash="12a83a17">2024.lrec-main.1240</url>
      <bibkey>grigoreva-etal-2024-rubia-russian</bibkey>
    </paper>
    <paper id="1241">
      <title><fixed-case>R</fixed-case>ussian Learner Corpus: Towards Error-Cause Annotation for <fixed-case>L</fixed-case>2 <fixed-case>R</fixed-case>ussian</title>
      <author><first>Daniil</first><last>Kosakin</last></author>
      <author><first>Sergei</first><last>Obiedkov</last></author>
      <author><first>Ivan</first><last>Smirnov</last></author>
      <author><first>Ekaterina</first><last>Rakhilina</last></author>
      <author><first>Anastasia</first><last>Vyrenkova</last></author>
      <author><first>Ekaterina</first><last>Zalivina</last></author>
      <pages>14240–14258</pages>
      <abstract>Russian Learner Corpus (RLC) is a large collection of learner texts in Russian written by native speakers of over forty languages. Learner errors in part of the corpus are manually corrected and annotated. Diverging from conventional error classifications, which typically focus on isolated lexical and grammatical features, the RLC error classification intends to highlight learners’ strategies employed in the process of text production, such as derivational patterns and syntactic relations (including agreement and government). In this paper, we present two open datasets derived from RLC: a manually annotated full-text dataset and a dataset with crowdsourced corrections for individual sentences. In addition, we introduce an automatic error annotation tool that, given an original sentence and its correction, locates and labels errors according to a simplified version of the RLC error-type system. We evaluate the performance of the tool on manually annotated data from RLC.</abstract>
      <url hash="2e499bee">2024.lrec-main.1241</url>
      <bibkey>kosakin-etal-2024-russian-learner</bibkey>
    </paper>
    <paper id="1242">
      <title><fixed-case>S</fixed-case>3<fixed-case>P</fixed-case>rompt: Instructing the Model with Self-calibration, Self-recall and Self-aggregation to Improve In-context Learning</title>
      <author><first>Junda</first><last>Chen</last></author>
      <author><first>Jianting</first><last>Liu</last></author>
      <pages>14259–14271</pages>
      <abstract>Large language models achieve impressive results by inferring conditional probability distributions in the context of user input to generate responses. However, they still have the following limitations in practical applications: 1) User queries are often colloquial and do not conform to the conditional probability distribution of LLM. 2) Unsupervised generation and recall of in-context examples(compared to random sampling) remains an open problem. To alleviate the above problems, we propose a novel Self-calibration, Self-recall and Self-aggregation prompt pipeline (S 3Prompt). Specifically, we first design a question calibration prompt to align colloquial queries with LLM context. Secondly, we construct a candidate recall prompt that allows LLM to generate potential background information, which is different from traditional retrieval-based QA. Finally, we design an information aggregation prompt to generate the final answer by aggregating the recalled information. Notably, we find that the self-generated information by LLM has a smaller gap when fused with LLM. We conducted comprehensive experiments on various datasets, including numerical reasoning, common sense reasoning, logical reasoning, and reading comprehension. The results showed that the performance of LLM can be significantly improved by using question calibration, candidate recall, and information aggregation, without requiring annotated datasets and model parameter updates.</abstract>
      <url hash="7d2f0b50">2024.lrec-main.1242</url>
      <bibkey>chen-liu-2024-s3prompt-instructing</bibkey>
    </paper>
    <paper id="1243">
      <title><fixed-case>S</fixed-case>a<fixed-case>GE</fixed-case>: Evaluating Moral Consistency in Large Language Models</title>
      <author><first>Vamshi Krishna</first><last>Bonagiri</last></author>
      <author><first>Sreeram</first><last>Vennam</last></author>
      <author><first>Priyanshul</first><last>Govil</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <author><first>Manas</first><last>Gaur</last></author>
      <pages>14272–14284</pages>
      <abstract>Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of “Rules of Thumb” (RoTs) to measure a model’s moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets – TruthfulQA and HellaSwag. Our results reveal that task accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.</abstract>
      <url hash="b287c84e">2024.lrec-main.1243</url>
      <bibkey>bonagiri-etal-2024-sage-evaluating</bibkey>
    </paper>
    <paper id="1244">
      <title>Saliency-Aware Interpolative Augmentation for Multimodal Financial Prediction</title>
      <author><first>Samyak</first><last>Jain</last></author>
      <author><first>Parth</first><last>Chhabra</last></author>
      <author><first>Atula Tejaswi</first><last>Neerkaje</last></author>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Shivam</first><last>Agarwal</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Sudheer</first><last>Chava</last></author>
      <author><first>Dinesh</first><last>Manocha</last></author>
      <pages>14285–14297</pages>
      <abstract>Predicting price variations of financial instruments for risk modeling and stock trading is challenging due to the stochastic nature of the stock market. While recent advancements in the Financial AI realm have expanded the scope of data and methods they use, such as textual and audio cues from financial earnings calls, limitations exist. Most datasets are small, and show domain distribution shifts due to the nature of their source, suggesting the exploration for data augmentation for robust augmentation strategies such as Mixup. To tackle such challenges in the financial domain, we propose SH-Mix: Saliency-guided Hierarchical Mixup augmentation technique for multimodal financial prediction tasks. SH-Mix combines multi-level embedding mixup strategies based on the contribution of each modality and context subsequences. Through extensive quantitative and qualitative experiments on financial earnings and conference call datasets consisting of text and speech, we show that SH-Mix outperforms state-of-the-art methods by 3-7%. Additionally, we show that SH-Mix is generalizable across different modalities and models.</abstract>
      <url hash="2d1baa25">2024.lrec-main.1244</url>
      <attachment type="OptionalSupplementaryMaterial" hash="833fdcf8">2024.lrec-main.1244.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>jain-etal-2024-saliency-aware</bibkey>
    </paper>
    <paper id="1245">
      <title>Samayik: A Benchmark and Dataset for <fixed-case>E</fixed-case>nglish-<fixed-case>S</fixed-case>anskrit Translation</title>
      <author><first>Ayush</first><last>Maheshwari</last></author>
      <author><first>Ashim</first><last>Gupta</last></author>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Atul Kumar</first><last>Singh</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last></author>
      <author><first>Anil Kumar</first><last>Gourishetty</last></author>
      <author><first>Jitin</first><last>Singla</last></author>
      <pages>14298–14304</pages>
      <abstract>We release Saamayik, a dataset of around 53,000 parallel English-Sanskrit sentences, written in contemporary prose. Sanskrit is a classical language still in sustenance and has a rich documented heritage. However, due to the limited availability of digitized content, it still remains a low-resource language. Existing Sanskrit corpora, whether monolingual or bilingual, have predominantly focused on poetry and offer limited coverage of contemporary written materials. Saamayik is curated from a diverse range of domains, including language instruction material, textual teaching pedagogy, and online tutorials, among others. It stands out as a unique resource that specifically caters to the contemporary usage of Sanskrit, with a primary emphasis on prose writing. Translation models trained on our dataset demonstrate statistically significant improvements when translating out-of-domain contemporary corpora, outperforming models trained on older classical-era poetry datasets. Finally, we also release benchmark models by adapting four multilingual pre-trained models, three of them have not been previously exposed to Sanskrit for translating between English and Sanskrit while one of them is multi-lingual pre-trained translation model including English and Sanskrit. The dataset and source code can be found at https://github.com/ayushbits/saamayik.</abstract>
      <url hash="ba1c0d58">2024.lrec-main.1245</url>
      <bibkey>maheshwari-etal-2024-samayik-benchmark</bibkey>
    </paper>
    <paper id="1246">
      <title><fixed-case>S</fixed-case>amró<fixed-case>M</fixed-case>ur <fixed-case>M</fixed-case>illjó<fixed-case>N</fixed-case>: An <fixed-case>ASR</fixed-case> Corpus of One Million Verified Read Prompts in <fixed-case>I</fixed-case>celandic</title>
      <author><first>Carlos Daniel</first><last>Hernandez Mena</last></author>
      <author><first>Þorsteinn Daði</first><last>Gunnarsson</last></author>
      <author><first>Jon</first><last>Gudnason</last></author>
      <pages>14305–14312</pages>
      <abstract>The platform samromur.is, or “Samrómur” for short, is a crowdsourcing web application built on Mozilla’s Common Voice, designed to accumulate speech data for the advancement of language technologies in Icelandic. Over the years, Samrómur has proven to be remarkably successful in amassing a significant number of high-quality audio clips from thousands of users. However, the challenge of manually verifying the entirety of the collected data has hindered its effective exploitation, especially in the realm of Automatic Speech Recognition (ASR), its original purpose. In this paper, we introduce the “Samrómur Milljón” corpus, an ASR dataset comprising one million audio clips from Samrómur. These clips have been automatically verified using state-of-the-art speech recognition systems such as NeMo, Wav2Vec2, and Whisper. Additionally, we present the ASR results obtained from creating acoustic models based on Samrómur Milljón. These results demonstrate significant promise when compared to other acoustic models trained with a similar volume of Icelandic data from different sources.</abstract>
      <url hash="2c70c90c">2024.lrec-main.1246</url>
      <bibkey>hernandez-mena-etal-2024-samromur-milljon</bibkey>
    </paper>
    <paper id="1247">
      <title>Sarcasm Detection in a Disaster Context</title>
      <author><first>Tiberiu</first><last>Sosea</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>14313–14324</pages>
      <abstract>During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning</abstract>
      <url hash="4b59d15f">2024.lrec-main.1247</url>
      <bibkey>sosea-etal-2024-sarcasm-detection</bibkey>
    </paper>
    <paper id="1248">
      <title><fixed-case>S</fixed-case>arc<fixed-case>N</fixed-case>et: A Multilingual Multimodal Sarcasm Detection Dataset</title>
      <author><first>Tan</first><last>Yue</last></author>
      <author><first>Xuzhao</first><last>Shi</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Zonghai</first><last>Hu</last></author>
      <author><first>Erik</first><last>Cambria</last></author>
      <pages>14325–14335</pages>
      <abstract>Sarcasm poses a challenge in linguistic analysis due to its implicit nature, involving an intended meaning that contradicts the literal expression. The advent of social networks has propelled the utilization of multimodal data to enhance sarcasm detection performance. In prior multimodal sarcasm detection datasets, a single label is assigned to a multimodal instance. Subsequent experiments often highlight the superiority of multimodal models by demonstrating their improvements compared to unimodal models based on these unified labels across multiple modalities. However, our investigation revealed that numerous instances of sarcasm cannot be identified using a single modality. Humans employ the conflict between a statement and factual information as a cue to detect sarcasm, and these cues can stem from different modalities. Then, a unified label for a multimodal instance may be not suitable for the associated text or image. In this work, we introduce SarcNet, a multilingual and multimodal sarcasm detection dataset in English and Chinese, consisting of 3,335 image-text pair samples. We provide annotations for sarcasm in visual, textual, and multimodal data, respectively, resulting in over 10,000 labeled instances. The separated annotation schema for unimodal and multimodal data facilitates a more accurate and reasonable assessment of unimodal and multimodal models.</abstract>
      <url hash="94be96e7">2024.lrec-main.1248</url>
      <bibkey>yue-etal-2024-sarcnet-multilingual</bibkey>
    </paper>
    <paper id="1249">
      <title>Scalable Patent Classification with Aggregated Multi-View Ranking</title>
      <author><first>Dan</first><last>Li</last></author>
      <author><first>Vikrant</first><last>Yadav</last></author>
      <author><first>Zi Long</first><last>Zhu</last></author>
      <author><first>Maziar Moradi</first><last>Fard</last></author>
      <author><first>Zubair</first><last>Afzal</last></author>
      <author><first>George</first><last>Tsatsaronis</last></author>
      <pages>14336–14346</pages>
      <abstract>Automated patent classification typically involves assigning labels to a patent from a taxonomy, using multi-class multi-label classification models. However, classification-based models face challenges in scaling to large numbers of labels, struggle with generalizing to new labels, and fail to effectively utilize the rich information and multiple views of patents and labels. In this work, we propose a multi-view ranking-based method to address these limitations. Our method consists of four ranking-based models that incorporate different views of patents and a meta-model that aggregates and re-ranks the candidate labels given by the four ranking models. We compared our approach against the state-of-the-art baselines on two publicly available patent classification datasets, USPTO-2M and CLEF-IP-2011. We demonstrate that our approach can alleviate the aforementioned limitations and achieve a new state-of-the-art performance by a significant margin.</abstract>
      <url hash="7cc55ad1">2024.lrec-main.1249</url>
      <bibkey>li-etal-2024-scalable-patent</bibkey>
    </paper>
    <paper id="1250">
      <title>Scale-<fixed-case>VAE</fixed-case>: Preventing Posterior Collapse in Variational Autoencoder</title>
      <author><first>Tianbao</first><last>Song</last></author>
      <author><first>Jingbo</first><last>Sun</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Weiming</first><last>Peng</last></author>
      <pages>14347–14357</pages>
      <abstract>Variational autoencoder (VAE) is a widely used generative model that gains great popularity for its capability in density estimation and representation learning. However, when employing a strong autoregressive generation network, VAE tends to converge to a degenerate local optimum known as posterior collapse. In this paper, we propose a model named Scale-VAE to solve this problem. Scale-VAE does not force the KL term to be larger than a positive constant, but aims to make the latent variables easier to be exploited by the generation network. Specifically, each dimension of the mean for the approximate posterior distribution is multiplied by a factor to keep that dimension discriminative across data instances. The same factors are used for all data instances so as not to change the relative relationship between the posterior distributions. Latent variables from the scaled-up posteriors are fed into the generation network, but the original posteriors are still used to calculate the KL term. In this way, Scale-VAE can solve the posterior collapse problem with a training cost similar to or even lower than the basic VAE. Experimental results show that Scale-VAE outperforms state-of-the-art models in density estimation, representation learning, and consistency of the latent space, and is competitive with other models in generation.</abstract>
      <url hash="6c921536">2024.lrec-main.1250</url>
      <bibkey>song-etal-2024-scale-vae</bibkey>
      <revision id="1" href="2024.lrec-main.1250v1" hash="b2eb6c7b"/>
      <revision id="2" href="2024.lrec-main.1250v2" hash="6c921536" date="2024-06-19">This revision corrects the number of the first sponsored Fund in the Acknowledgments section (from 4222003 to 4234081). It also corrects a typo in the email address of the third author.</revision>
    </paper>
    <paper id="1251">
      <title>Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment</title>
      <author><first>Feifan</first><last>Song</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Hao</first><last>Lang</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>14358–14369</pages>
      <abstract>Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after fine-tuning. We also leverage it on data augmentation and conduct experiments to show its effect on different algorithms.</abstract>
      <url hash="a246954f">2024.lrec-main.1251</url>
      <bibkey>song-etal-2024-scaling-data</bibkey>
    </paper>
    <paper id="1252">
      <title>Scansion-based Lyrics Generation</title>
      <author><first>Yiwen</first><last>Chen</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>14370–14381</pages>
      <abstract>We aim to generate lyrics for Mandarin songs with a good match between the melody and the tonal contour of the lyrics. Our solution relies on mBart, treating lyrics generation as a translation problem, but rather than translating directly from the melody as is common, our novelty in this paper is that we generate from scansion as an intermediate contour representation that can fit a given melody. One of the advantages of our solution is that it does not require a parallel melody-lyrics dataset. We also present a thorough automatic evaluation of our system against competitors, using several new evaluation metrics. These measure intelligibility, fit to melody, and use proxies for quantifying creativity (variation to other songs created by the same system in different settings, semantic similarity to keywords given to the system, perplexity). When comparing different implementations of scansion to competitor systems, a varied picture emerges. Our best system outperforms all others in lyric-melody fit and is in the top group of systems for two of the creativity metrics (variation and perplexity), overshadowing two large language models (LLM) specialised to this task.</abstract>
      <url hash="86017560">2024.lrec-main.1252</url>
      <bibkey>chen-teufel-2024-scansion-based</bibkey>
    </paper>
    <paper id="1253">
      <title>Schema-based Data Augmentation for Event Extraction</title>
      <author><first>Xiaomeng</first><last>Jin</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>14382–14392</pages>
      <abstract>Event extraction is a crucial task for semantic understanding and structured knowledge construction. However, the expense of collecting and labeling data for training event extraction models is usually high. To address this issue, we propose a novel schema-based data augmentation method that utilizes event schemas to guide the data generation process. The event schemas depict the typical patterns of complex events and can be used to create new synthetic data for event extraction. Specifically, we sub-sample from the schema graph to obtain a subgraph, instantiate the schema subgraph, and then convert the instantiated subgraph to natural language texts. We conduct extensive experiments on event trigger detection, event trigger extraction, and event argument extraction tasks using two datasets (including five scenarios). The experimental results demonstrate that our proposed data-augmentation method produces high-quality generated data and significantly enhances the model performance, with up to 12% increase in F1 score compared to baseline methods.</abstract>
      <url hash="466cd777">2024.lrec-main.1253</url>
      <bibkey>jin-ji-2024-schema-based</bibkey>
    </paper>
    <paper id="1254">
      <title>Schema Learning Corpus: Data and Annotation Focused on Complex Events</title>
      <author><first>Song</first><last>Chen</last></author>
      <author><first>Jennifer</first><last>Tracey</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <pages>14393–14399</pages>
      <abstract>The Schema Learning Corpus (SLC) is a new linguistic resource designed to support research into the structure of complex events in multilingual, multimedia data. The SLC incorporates large volumes of background data in English, Spanish and Russian, and defines 100 complex events (CEs) across 12 domains, with CE profiles containing information about the typical steps and substeps and expected event categories for the CE. Multiple documents are labeled for each CE, with pointers to evidence in the document for each CE step, plus labeled events and relations along with their arguments across a large tag set. The SLC was designed to support development and evaluation of technology capable of understanding and reasoning about complex real-world events in multimedia, multilingual data streams in order to provide users with a deeper understanding of the potential relationships among seemingly disparate events and actors, and to allow users to make better predictions about how future events are likely to unfold. The Schema Learning Corpus will be made available to the research community through publication in Linguistic Data Consortium catalog.</abstract>
      <url hash="89f10b0d">2024.lrec-main.1254</url>
      <bibkey>chen-etal-2024-schema-learning</bibkey>
    </paper>
    <paper id="1255">
      <title>Schroedinger’s Threshold: When the <fixed-case>AUC</fixed-case> Doesn’t Predict Accuracy</title>
      <author><first>Juri</first><last>Opitz</last></author>
      <pages>14400–14406</pages>
      <abstract>The Area Under Curve measure (AUC) seems apt to evaluate and compare diverse models, possibly without calibration. An important example of AUC application is the evaluation and benchmarking of models that predict faithfulness of generated text. But we show that the AUC yields an academic and optimistic notion of accuracy that can misalign with the actual accuracy observed in application, yielding significant changes in benchmark rankings. To paint a more realistic picture of downstream model performance (and prepare it for actual application), we explore different calibration modes, testing calibration data and method.</abstract>
      <url hash="303d2ba0">2024.lrec-main.1255</url>
      <bibkey>opitz-2024-schroedingers-threshold</bibkey>
      <revision id="1" href="2024.lrec-main.1255v1" hash="88164497"/>
      <revision id="2" href="2024.lrec-main.1255v2" hash="303d2ba0" date="2024-05-21">Minor update.</revision>
    </paper>
    <paper id="1256">
      <title><fixed-case>S</fixed-case>ci<fixed-case>DMT</fixed-case>: A Large-Scale Corpus for Detecting Scientific Mentions</title>
      <author><first>Huitong</first><last>Pan</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <author><first>Eduard</first><last>Dragut</last></author>
      <author><first>Longin Jan</first><last>Latecki</last></author>
      <pages>14407–14417</pages>
      <abstract>We present SciDMT, an enhanced and expanded corpus for scientific mention detection, offering a significant advancement over existing related resources. SciDMT contains annotated scientific documents for datasets (D), methods (M), and tasks (T). The corpus consists of two components: 1) the SciDMT main corpus, which includes 48 thousand scientific articles with over 1.8 million weakly annotated mention annotations in the format of in-text span, and 2) an evaluation set, which comprises 100 scientific articles manually annotated for evaluation purposes. To the best of our knowledge, SciDMT is the largest corpus for scientific entity mention detection. The corpus’s scale and diversity are instrumental in developing and refining models for tasks such as indexing scientific papers, enhancing information retrieval, and improving the accessibility of scientific knowledge. We demonstrate the corpus’s utility through experiments with advanced deep learning architectures like SciBERT and GPT-3.5. Our findings establish performance baselines and highlight unresolved challenges in scientific mention detection. SciDMT serves as a robust benchmark for the research community, encouraging the development of innovative models to further the field of scientific information extraction.</abstract>
      <url hash="58ad2261">2024.lrec-main.1256</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1c4f6dec">2024.lrec-main.1256.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>pan-etal-2024-scidmt-large</bibkey>
    </paper>
    <paper id="1257">
      <title><fixed-case>S</fixed-case>ci<fixed-case>MRC</fixed-case>: Multi-perspective Scientific Machine Reading Comprehension</title>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Heqi</first><last>Zheng</last></author>
      <author><first>Yuxiang</first><last>Nie</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <pages>14418–14428</pages>
      <abstract>Scientific Machine Reading Comprehension (SMRC) aims to facilitate the understanding of scientific texts through human-machine interactions. While existing dataset has significantly contributed to this field, it predominantly focus on single-perspective question-answer pairs, thereby overlooking the inherent variation in comprehension levels among different readers. To address this limitation, we introduce a novel multi-perspective scientific machine reading comprehension dataset, SciMRC, which incorporates perspectives from beginners, students, and experts. Our dataset comprises 741 scientific papers and 6,057 question-answer pairs, with 3,306, 1,800, and 951 pairs corresponding to beginners, students, and experts respectively. Extensive experiments conducted on SciMRC using pre-trained models underscore the importance of considering diverse perspectives in SMRC and highlight the challenging nature of our scientific machine comprehension tasks.</abstract>
      <url hash="a299d9c9">2024.lrec-main.1257</url>
      <bibkey>zhang-etal-2024-scimrc-multi</bibkey>
    </paper>
    <paper id="1258">
      <title><fixed-case>S</fixed-case>ci<fixed-case>N</fixed-case>ews: From Scholarly Complexities to Public Narratives – a Dataset for Scientific News Report Generation</title>
      <author><first>Dongqi</first><last>Pu</last></author>
      <author><first>Yifan</first><last>Wang</last></author>
      <author><first>Jia E.</first><last>Loy</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>14429–14444</pages>
      <abstract>Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related to this work are available at https://dongqi.me/projects/SciNews.</abstract>
      <url hash="60926663">2024.lrec-main.1258</url>
      <bibkey>pu-etal-2024-scinews-scholarly</bibkey>
    </paper>
    <paper id="1259">
      <title><fixed-case>SCOUT</fixed-case>: A Situated and Multi-Modal Human-Robot Dialogue Corpus</title>
      <author><first>Stephanie M.</first><last>Lukin</last></author>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Matthew</first><last>Marge</last></author>
      <author><first>Taylor A.</first><last>Hudson</last></author>
      <author><first>Cory J.</first><last>Hayes</last></author>
      <author><first>Kimberly</first><last>Pollard</last></author>
      <author><first>Anthony</first><last>Baker</last></author>
      <author><first>Ashley N.</first><last>Foots</last></author>
      <author><first>Ron</first><last>Artstein</last></author>
      <author><first>Felix</first><last>Gervits</last></author>
      <author><first>Mitchell</first><last>Abrams</last></author>
      <author><first>Cassidy</first><last>Henry</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Anton</first><last>Leuski</last></author>
      <author><first>Susan G.</first><last>Hill</last></author>
      <author><first>David</first><last>Traum</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>14445–14458</pages>
      <abstract>We introduce the Situated Corpus Of Understanding Transactions (SCOUT), a multi-modal collection of human-robot dialogue in the task domain of collaborative exploration. The corpus was constructed from multiple Wizard-of-Oz experiments where human participants gave verbal instructions to a remotely-located robot to move and gather information about its surroundings. SCOUT contains 89,056 utterances and 310,095 words from 278 dialogues averaging 320 utterances per dialogue. The dialogues are aligned with the multi-modal data streams available during the experiments: 5,785 images and 30 maps. The corpus has been annotated with Abstract Meaning Representation and Dialogue-AMR to identify the speaker’s intent and meaning within an utterance, and with Transactional Units and Relations to track relationships between utterances to reveal patterns of the Dialogue Structure. We describe how the corpus and its annotations have been used to develop autonomous human-robot systems and enable research in open questions of how humans speak to robots. We release this corpus to accelerate progress in autonomous, situated, human-robot dialogue, especially in the context of navigation tasks where details about the environment need to be discovered.</abstract>
      <url hash="f3c78910">2024.lrec-main.1259</url>
      <attachment type="OptionalSupplementaryMaterial" hash="00000000">2024.lrec-main.1259.OptionalSupplementaryMaterial.txt</attachment>
      <bibkey>lukin-etal-2024-scout-situated</bibkey>
    </paper>
    <paper id="1260">
      <title><fixed-case>SDA</fixed-case>: Simple Discrete Augmentation for Contrastive Sentence Representation Learning</title>
      <author><first>Dongsheng</first><last>Zhu</last></author>
      <author><first>Zhenyu</first><last>Mao</last></author>
      <author><first>Jinghui</first><last>Lu</last></author>
      <author><first>Rui</first><last>Zhao</last></author>
      <author><first>Fei</first><last>Tan</last></author>
      <pages>14459–14471</pages>
      <abstract>Contrastive learning has recently achieved compelling performance in unsupervised sentence representation. As an essential element, data augmentation protocols, however, have not been well explored. The pioneering work SimCSE resorting to a simple dropout mechanism (viewed as continuous augmentation) surprisingly dominates discrete augmentations such as cropping, word deletion, and synonym replacement as reported. To understand the underlying rationales, we revisit existing approaches and attempt to hypothesize the desiderata of reasonable data augmentation methods: balance of semantic consistency and expression diversity. We then develop three simple yet effective discrete sentence augmentation schemes: punctuation insertion, modal verbs, and double negation. They act as minimal noises at lexical level to produce diverse forms of sentences. Furthermore, standard negation is capitalized on to generate negative samples for alleviating feature suppression involved in contrastive learning. We experimented extensively with semantic textual similarity on diverse datasets. The results support the superiority of the proposed methods consistently. Our key code is available at https://github.com/Zhudongsheng75/SDA</abstract>
      <url hash="c9064562">2024.lrec-main.1260</url>
      <bibkey>zhu-etal-2024-sda-simple</bibkey>
    </paper>
    <paper id="1261">
      <title>Searching by Code: A New <fixed-case>S</fixed-case>earch<fixed-case>B</fixed-case>y<fixed-case>S</fixed-case>nippet Dataset and <fixed-case>S</fixed-case>nippe<fixed-case>R</fixed-case> Retrieval Model for Searching by Code Snippets</title>
      <author><first>Ivan</first><last>Sedykh</last></author>
      <author><first>Nikita</first><last>Sorokin</last></author>
      <author><first>Dmitry</first><last>Abulkhanov</last></author>
      <author><first>Sergey I.</first><last>Nikolenko</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <pages>14472–14477</pages>
      <abstract>Code search is an important and well-studied task, but it usually means searching for code by a text query. We argue that using a code snippet (and possibly an error traceback) as a query while looking for bugfixing instructions and code samples is a natural use case not covered by prior art. Moreover, existing datasets use code comments rather than full-text descriptions as text, making them unsuitable for this use case. We present a new SearchBySnippet dataset implementing the search-by-code use case based on StackOverflow data; we show that on SearchBySnippet, existing architectures fall short of a simple BM25 baseline even after fine-tuning. We present a new single encoder model SnippeR that outperforms several strong baselines on SearchBySnippet with a result of 0.451 Recall@10; we propose the SearchBySnippet dataset and SnippeR as a new important benchmark for code search evaluation.</abstract>
      <url hash="34d0fe37">2024.lrec-main.1261</url>
      <bibkey>sedykh-etal-2024-searching-code</bibkey>
    </paper>
    <paper id="1262">
      <title>Sebastian, Basti, Wastl?! Recognizing Named Entities in <fixed-case>B</fixed-case>avarian Dialectal Data</title>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Zihang</first><last>Sun</last></author>
      <author><first>Huangyan</first><last>Shan</last></author>
      <author><first>Marie</first><last>Kolm</last></author>
      <author><first>Verena</first><last>Blaschke</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>14478–14493</pages>
      <abstract>Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki. We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.</abstract>
      <url hash="7b64f8d7">2024.lrec-main.1262</url>
      <attachment type="OptionalSupplementaryMaterial" hash="aea01a06">2024.lrec-main.1262.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>peng-etal-2024-sebastian-basti</bibkey>
    </paper>
    <paper id="1263">
      <title>Seeing Eye-to-Eye: Cross-Modal Coherence Relations Inform Eye-gaze Patterns During Comprehension &amp; Production</title>
      <author><first>Mert</first><last>Inan</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>14494–14512</pages>
      <abstract>Context influences how we engage with multimodal documents. Describing and processing the content of images is highly correlated with the goals of the discourse. It is known that these underlying cognitive processes can be tapped into by looking at eye movements, but the connection between discourse goals and eye movements is a missing link. In this study, we carry out both augmented reality and webcam-based eye-tracking experiments during comprehension and production tasks. We build on computational frameworks of coherence in text and images that study causal, logical, elaborative, and temporal inferences to understand how eye gaze patterns and coherence relations influence each other. No state-of-the-art techniques exist to analyze eye movements in multimodal language settings. So, we introduce a new eye gaze pattern ranking algorithm and a semantic gaze visualization technique to study this phenomenon better. Our results demonstrate that eye gaze durations are person-dependent, and during comprehension and production, ranked gaze patterns are significantly different for different types of coherence relations. We also present a case study of how Multimodal Large Language Models represent this connection of eye gaze patterns and coherence relations. We make all of our code and novel analysis tools available through https://github.com/Merterm/eye-gaze-coherence.</abstract>
      <url hash="d208515d">2024.lrec-main.1263</url>
      <bibkey>inan-alikhani-2024-seeing-eye</bibkey>
    </paper>
    <paper id="1264">
      <title>Seeing Is Believing! towards Knowledge-Infused Multi-modal Medical Dialogue Generation</title>
      <author><first>Abhisek</first><last>Tiwari</last></author>
      <author><first>Shreyangshu</first><last>Bera</last></author>
      <author><first>Preeti</first><last>Verma</last></author>
      <author><first>Jaithra Varma</first><last>Manthena</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Minakshi</first><last>Dhar</last></author>
      <author><first>Sarbajeet</first><last>Tiwari</last></author>
      <pages>14513–14523</pages>
      <abstract>Over the last few years, artificial intelligence-based clinical assistance has gained immense popularity and demand in telemedicine, including automatic disease diagnosis. Patients often describe their signs and symptoms to doctors using visual aids, which provide vital evidence for identifying a medical condition. In addition to learning from our experiences, we learn from well-established theories/ knowledge. With the motivation of leveraging visual cues and medical knowledge, we propose a transformer-based, knowledge-infused multi-modal medical dialogue generation (KI-MMDG) framework. In addition, we present a discourse-aware image identifier (DII) that recognizes signs and their severity by leveraging the current conversation context in addition to the image of the signs. We first curate an empathy and severity-aware multi-modal medical dialogue (ES-MMD) corpus in English, which is annotated with intent, symptoms, and visual signs with severity information. Experimental results show the superior performance of the proposed KI-MMDG model over uni-modal and non-knowledge infused generative models, demonstrating the importance of visual signs and knowledge infusion in symptom investigation and diagnosis. We also observed that the DII model surpasses the existing state-of-the-art model by 7.84%, indicating the crucial significance of dialogue context for identifying a sign image surfaced during conversations. The code and dataset are available at https://github.com/NLP-RL/KI-MMDG.</abstract>
      <url hash="22a5bc81">2024.lrec-main.1264</url>
      <bibkey>tiwari-etal-2024-seeing-believing</bibkey>
    </paper>
    <paper id="1265">
      <title>Segmentation of Complex Question Turns for Argument Mining: A Corpus-based Study in the Financial Domain</title>
      <author><first>Giulia</first><last>D’Agostino</last></author>
      <author><first>Chris A.</first><last>Reed</last></author>
      <author><first>Daniele</first><last>Puccinelli</last></author>
      <pages>14524–14530</pages>
      <abstract>Within the financial communication domain, Earnings Conference Calls (ECCs) play a pivotal role in tracing (a) the presentational strategies and trust-building devices used by company representatives and (b) the relevant hot-topics for stakeholders, from which they form an (e)valuation of the company. Due to their formally regulated nature, ECCs are a favoured domain for the study of argumentation in context and the extraction of Argumentative Discourse Units (ADUs). However, the idiosyncratic structure of dialogical exchanges in Q&amp;A sessions of ECCs, particularly at the level of question formulation, challenges existing models of argument mining, which assume adjacency of related question and answer turns in the dialogue. Maximal Interrogative Units (MIUs) are a novel approach to grouping together topically contiguous argumentative components within a question turn. MIU identification allows application of existing argument mining techniques to a less noisy unit of text, following removal of discourse regulators and splitting into sub-units of thematically related text. Evaluation of an automated method for MIU recognition is also presented with respect to gold-standard manual annotation.</abstract>
      <url hash="6b264d91">2024.lrec-main.1265</url>
      <bibkey>dagostino-etal-2024-segmentation-complex</bibkey>
    </paper>
    <paper id="1266">
      <title>Select and Reorder: A Novel Approach for Neural Sign Language Production</title>
      <author><first>Harry</first><last>Walsh</last></author>
      <author><first>Ben</first><last>Saunders</last></author>
      <author><first>Richard</first><last>Bowden</last></author>
      <pages>14531–14542</pages>
      <abstract>Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&amp;R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.</abstract>
      <url hash="9636cef8">2024.lrec-main.1266</url>
      <bibkey>walsh-etal-2024-select-reorder</bibkey>
    </paper>
    <paper id="1267">
      <title>Select High-quality Synthetic <fixed-case>QA</fixed-case> Pairs to Augment Training Data in <fixed-case>MRC</fixed-case> under the Reward Guidance of Generative Language Models</title>
      <author><first>Jing</first><last>Jin</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>14543–14554</pages>
      <abstract>Synthesizing QA pairs via question generator (QG) for data augmentation is widely used in Machine Reading Comprehension (MRC), especially in data-scarce scenarios like limited labeled data or domain adaptation. However, the quality of generated QA pairs varies, and it is necessary to select the ones with high quality from them. Existing approaches focus on downstream metrics to choose QA pairs, which lacks generalization across different metrics and datasets. In this paper, we propose a general selection method that employs a generative large pre-trained language model as a reward model in a Reinforcement Learning (RL) framework for the training of the selection agent. Our experiments on both generative and extractive datasets demonstrate that our selection method leads to better downstream performance. We also find that using the large language model (LLM) as a reward model is more beneficial than using it as a direct selector or QA model. Furthermore, we assess the selected QA pairs from multiple angles, not just downstream metrics, highlighting their superior quality compared to other methods. Our work has better flexibility across metrics, provides interpretability for the selected data, and expands the potential of leveraging generative large language models in the field of MRC and RL training. Our code is available at https://github.com/JulieJin-km/LLM_RL_Selection.</abstract>
      <url hash="523f7426">2024.lrec-main.1267</url>
      <bibkey>jin-wang-2024-select-high</bibkey>
    </paper>
    <paper id="1268">
      <title>Selective Temporal Knowledge Graph Reasoning</title>
      <author><first>Zhongni</first><last>Hou</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Zixuan</first><last>Li</last></author>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>14555–14566</pages>
      <abstract>Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently. TKG reasoning aims to predict future facts based on given historical ones. However, existing TKG reasoning models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications. Thus, in this paper, we propose an abstention mechanism for TKG reasoning, which helps the existing models make selective, instead of indiscriminate, predictions. Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing TKG reasoning models to first estimate their confidence in making predictions, and then abstain from those with low confidence. To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy of historical predictions. Experiments with representative TKG reasoning models on two benchmark datasets demonstrate the effectiveness of the proposed CEHis.</abstract>
      <url hash="2f99ad93">2024.lrec-main.1268</url>
      <bibkey>hou-etal-2024-selective-temporal</bibkey>
    </paper>
    <paper id="1269">
      <title>Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models</title>
      <author><first>Haoyu</first><last>Gao</last></author>
      <author><first>Ting-En</first><last>Lin</last></author>
      <author><first>Hangyu</first><last>Li</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Yuchuan</first><last>Wu</last></author>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>14567–14578</pages>
      <abstract>Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel “Self-Explanation” prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs’ comprehension in complex dialogue tasks.</abstract>
      <url hash="efbaba0f">2024.lrec-main.1269</url>
      <bibkey>gao-etal-2024-self-explanation</bibkey>
    </paper>
    <paper id="1270">
      <title>Self-Improvement Programming for Temporal Knowledge Graph Question Answering</title>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Zhao</first><last>Zhang</last></author>
      <author><first>Zixuan</first><last>Li</last></author>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Yutao</first><last>Zeng</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Yongjun</first><last>Xu</last></author>
      <pages>14579–14594</pages>
      <abstract>Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric.</abstract>
      <url hash="5e90d96e">2024.lrec-main.1270</url>
      <bibkey>chen-etal-2024-self-improvement</bibkey>
    </paper>
    <paper id="1271">
      <title>Self-Knowledge Distillation for Knowledge Graph Embedding</title>
      <author><first>Haotian</first><last>Xu</last></author>
      <author><first>Yuhua</first><last>Wang</last></author>
      <author><first>Jiahui</first><last>Fan</last></author>
      <pages>14595–14605</pages>
      <abstract>Knowledge graph embedding (KGE) is an important task and it can benefit lots of downstream applications. General KGE can increase the embedding dimension to improve model performance. High-dimensional KGE will significantly increase the number of model parameters and training time. Therefore, knowledge distillation is proposed for learning a low-dimensional model from a pre-trained high-dimensional model. To avoid introducing a complex teacher model, we use self-knowledge distillation. However, there are still some issues with the self-knowledge distillation model we mentioned later. One of them is misdirection from incorrect predictions during model training. Another is the loss of discrimination information caused by excessive distillation temperature. To address these issues, we apply self-knowledge distillation, knowledge adjustment and dynamic temperature distillation to KGE. Self-knowledge distillation uses the information from the latest iteration to guide the training in the current iteration. Knowledge adjustment fixes the predictions of misjudged training samples. Dynamic temperature distillation designs dynamic sample-wise temperatures to compute soft targets. Our model can not only improve model performance but also achieve a lightweight model. Experimental results demonstrate the effectiveness and generalization ability of our model in link prediction. The lightweight model can maintain good model performance while reducing the number of model parameters and training time.</abstract>
      <url hash="6c9b959a">2024.lrec-main.1271</url>
      <bibkey>xu-etal-2024-self-knowledge</bibkey>
    </paper>
    <paper id="1272">
      <title>Self-reported Demographics and Discourse Dynamics in a Persuasive Online Forum</title>
      <author><first>Agnieszka</first><last>Falenska</last></author>
      <author><first>Eva Maria</first><last>Vecchi</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <pages>14606–14621</pages>
      <abstract>Research on language as interactive discourse underscores the deliberate use of demographic parameters such as gender, ethnicity, and class to shape social identities. For example, by explicitly disclosing one’s information and enforcing one’s social identity to an online community, the reception by and interaction with the said community is impacted, e.g., strengthening one’s opinions by depicting the speaker as credible through their experience in the subject. Here, we present a first thorough study of the role and effects of self-disclosures on online discourse dynamics, focusing on a pervasive type of self-disclosure: author gender. Concretely, we investigate the contexts and properties of gender self-disclosures and their impact on interaction dynamics in an online persuasive forum, ChangeMyView. Our contribution is twofold. At the level of the target phenomenon, we fill a research gap in the understanding of the impact of these self-disclosures on the discourse by bringing together features related to forum activity (votes, number of comments), linguistic/stylistic features from the literature, and discourse topics. At the level of the contributed resource, we enrich and release a comprehensive dataset that will provide a further impulse for research on the interplay between gender disclosures, community interaction, and persuasion in online discourse.</abstract>
      <url hash="31ce13e5">2024.lrec-main.1272</url>
      <bibkey>falenska-etal-2024-self-reported</bibkey>
    </paper>
    <paper id="1273">
      <title>Semantic Frame Extraction in Multilingual Olfactory Events</title>
      <author><first>Stefano</first><last>Menini</last></author>
      <pages>14622–14627</pages>
      <abstract>In this work we present a system for multilingual olfactory information extraction covering six European languages, introducing new models to extract olfactory information from large amounts of text in a structured and scalable way. For the task we rely on a supervised multi-task approach to detect olfactory related text adopting a FrameNet-like structure, identifying the lexical units triggering the smell event and a related set of frame elements.</abstract>
      <url hash="c1cb7cd3">2024.lrec-main.1273</url>
      <bibkey>menini-2024-semantic-frame</bibkey>
    </paper>
    <paper id="1274">
      <title>Semantic Map-based Generation of Navigation Instructions</title>
      <author><first>Chengzu</first><last>Li</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <author><first>Rama Sanand</first><last>Doddipatla</last></author>
      <author><first>Svetlana</first><last>Stoyanchev</last></author>
      <pages>14628–14640</pages>
      <abstract>We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement. We release the code for data preparation and model training at https://github.com/chengzu-li/VLGen.</abstract>
      <url hash="6edb3202">2024.lrec-main.1274</url>
      <bibkey>li-etal-2024-semantic-map</bibkey>
    </paper>
    <paper id="1275">
      <title>Semantic Role Labeling Guided Out-of-distribution Detection</title>
      <author><first>Jinan</first><last>Zou</last></author>
      <author><first>Maihao</first><last>Guo</last></author>
      <author><first>Yu</first><last>Tian</last></author>
      <author><first>Yuhao</first><last>Lin</last></author>
      <author><first>Haiyao</first><last>Cao</last></author>
      <author><first>Lingqiao</first><last>Liu</last></author>
      <author><first>Ehsan</first><last>Abbasnejad</last></author>
      <author><first>Javen Qinfeng</first><last>Shi</last></author>
      <pages>14641–14651</pages>
      <abstract>Identifying unexpected domain-shifted instances in natural language processing is crucial in real-world applications. Previous works identify the out-of-distribution (OOD) instance by leveraging a single global feature embedding to represent the sentence, which cannot characterize subtle OOD patterns well. Another major challenge current OOD methods face is learning effective low-dimensional sentence representations to identify the hard OOD instances that are semantically similar to the in-distribution (ID) data. In this paper, we propose a new unsupervised OOD detection method, namely Semantic Role Labeling Guided Out-of-distribution Detection (SRLOOD), that separates, extracts, and learns the semantic role labeling (SRL) guided fine-grained local feature representations from different arguments of a sentence and the global feature representations of the full sentence using a margin-based contrastive loss. A novel self-supervised approach is also introduced to enhance such global-local feature learning by predicting the SRL extracted role. The resulting model achieves SOTA performance on four OOD benchmarks, indicating the effectiveness of our approach. The code is publicly accessible via https://github.com/cytai/SRLOOD.</abstract>
      <url hash="78eef98b">2024.lrec-main.1275</url>
      <bibkey>zou-etal-2024-semantic-role</bibkey>
    </paper>
    <paper id="1276">
      <title>Semantics-Aware Dual Graph Convolutional Networks for Argument Pair Extraction</title>
      <author><first>Minzhao</first><last>Guan</last></author>
      <author><first>Zhixun</first><last>Qiu</last></author>
      <author><first>Fenghuan</first><last>Li</last></author>
      <author><first>Yun</first><last>Xue</last></author>
      <pages>14652–14663</pages>
      <abstract>Argument pair extraction (APE) is a task that aims to extract interactive argument pairs from two argument passages. Generally, existing works focus on either simple argument interaction or task form conversion, instead of thorough deep-level feature exploitation of argument pairs. To address this issue, a Semantics-Aware Dual Graph Convolutional Networks (SADGCN) is proposed for APE. Specifically, the co-occurring word graph is designed to tackle the lexical and semantic relevance of arguments with a pre-trained Rouge-guided Transformer (ROT). Considering the topic relevance in argument pairs, a topic graph is constructed by the neural topic model to leverage the topic information of argument passages. The two graphs are fused via a gating mechanism, which contributes to the extraction of argument pairs. Experimental results indicate that our approach achieves the state-of-the-art performance. The performance on F1 score is significantly improved by 6.56% against the existing best alternative.</abstract>
      <url hash="b87dd393">2024.lrec-main.1276</url>
      <bibkey>guan-etal-2024-semantics-aware</bibkey>
    </paper>
    <paper id="1277">
      <title>Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training</title>
      <author><first>Haowei</first><last>Liu</last></author>
      <author><first>Yaya</first><last>Shi</last></author>
      <author><first>Haiyang</first><last>Xu</last></author>
      <author><first>Chunfeng</first><last>Yuan</last></author>
      <author><first>Qinghao</first><last>Ye</last></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Bing</first><last>Li</last></author>
      <author><first>Weiming</first><last>Hu</last></author>
      <pages>14664–14675</pages>
      <abstract>In vision-language pre-training (VLP), masked image modeling (MIM) has recently been introduced for fine-grained cross-modal alignment. However, in most existing methods, the reconstruction targets for MIM lack high-level semantics, and text is not sufficiently involved in masked modeling. These two drawbacks limit the effect of MIM in facilitating cross-modal semantic alignment. In this work, we propose a semantics-enhanced cross-modal MIM framework (SemMIM) for vision-language representation learning. Specifically, to provide more semantically meaningful supervision for MIM, we propose a local semantics enhancing approach, which harvest high-level semantics from global image features via self-supervised agreement learning and transfer them to local patch encodings by sharing the encoding space. Moreover, to achieve deep involvement of text during the entire MIM process, we propose a text-guided masking strategy and devise an efficient way of injecting textual information in both masked modeling and reconstruction target acquisition. Experimental results validate that our method improves the effectiveness of the MIM task in facilitating cross-modal semantic alignment. Compared to previous VLP models with similar model size and data scale, our SemMIM model achieves state-of-the-art or competitive performance on multiple downstream vision-language tasks.</abstract>
      <url hash="1d793308">2024.lrec-main.1277</url>
      <bibkey>liu-etal-2024-semantics-enhanced</bibkey>
    </paper>
    <paper id="1278">
      <title>Sense of the Day: Short Timeframe Temporal-Aware Word Sense Disambiguation</title>
      <author><first>Yuchen</first><last>Wei</last></author>
      <author><first>Milton</first><last>King</last></author>
      <pages>14676–14686</pages>
      <abstract>The predominant sense of a lemma can vary based on the timeframe (years, decades, centuries) that the text was written. In our work, we explore the predominant sense of shorter timeframes (days, months, seasons, etc.) and find that different short timeframes can have different predominant senses from each other and from the predominant sense of a corpus. Leveraging the predominant sense and sense distribution of a short timeframe, we design short timeframe temporal-aware word sense disambiguation (WSD) models that outperform a temporal agnostic model. Likewise, author-aware WSD models tend to outperform author agnostic models, therefore we augment our temporal-aware models to leverage knowledge of author-level predominant senses and sense distributions to create temporal and author-aware WSD models. In addition to this, we found that considering recent usages of a lemma by the same author can assist a WSD model. Our approach requires the use of only a small amount of text from authors and timeframes.</abstract>
      <url hash="f8aea3c3">2024.lrec-main.1278</url>
      <bibkey>wei-king-2024-sense-day</bibkey>
    </paper>
    <paper id="1279">
      <title><fixed-case>SENTA</fixed-case>: Sentence Simplification System for <fixed-case>S</fixed-case>lovene</title>
      <author><first>Aleš</first><last>Žagar</last></author>
      <author><first>Matej</first><last>Klemen</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <author><first>Iztok</first><last>Kosem</last></author>
      <pages>14687–14692</pages>
      <abstract>Ensuring universal access to written content, regardless of users’ language proficiency and cognitive abilities, is of paramount importance. Sentence simplification, which involves converting complex sentences into more accessible forms while preserving their meaning, plays a crucial role in enhancing text accessibility. This paper introduces SENTA, a system for sentence simplification in Slovene. The system consists of two components. First, a neural classifier identifies sentences that require simplification, and second, a large Slovene language model based on T5 architecture is fine-tuned to transform complex texts into a simpler form, achieving an excellent SARI score of 41. Both automatic and qualitative evaluations provide important insights into the problem, highlighting areas for future research in multilingual applications, and fluency maintenance. Finally, SENTA is integrated into a freely accessible, user-friendly user interface, offering a valuable service to less-fluent Slovene users.</abstract>
      <url hash="78198be0">2024.lrec-main.1279</url>
      <bibkey>zagar-etal-2024-senta-sentence</bibkey>
    </paper>
    <paper id="1280">
      <title><fixed-case>S</fixed-case>enti<fixed-case>CSE</fixed-case>: A Sentiment-aware Contrastive Sentence Embedding Framework with Sentiment-guided Textual Similarity</title>
      <author><first>Jaemin</first><last>Kim</last></author>
      <author><first>Yohan</first><last>Na</last></author>
      <author><first>Kangmin</first><last>Kim</last></author>
      <author><first>Sang-Rak</first><last>Lee</last></author>
      <author><first>Dong-Kyu</first><last>Chae</last></author>
      <pages>14693–14704</pages>
      <abstract>Recently, sentiment-aware pre-trained language models (PLMs) demonstrate impressive results in downstream sentiment analysis tasks. However, they neglect to evaluate the quality of their constructed sentiment representations; they just focus on improving the fine-tuning performance, which overshadows the representation quality. We argue that without guaranteeing the representation quality, their downstream performance can be highly dependent on the supervision of the fine-tuning data rather than representation quality. This problem would make them difficult to foray into other sentiment-related domains, especially where labeled data is scarce. We first propose Sentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the quality of sentiment representations, which is designed based on the degree of equivalence in sentiment polarity between two sentences. We then propose SentiCSE, a novel Sentiment-aware Contrastive Sentence Embedding framework for constructing sentiment representations via combined word-level and sentence-level objectives, whose quality is guaranteed by SgTS. Qualitative and quantitative comparison with the previous sentiment-aware PLMs shows the superiority of our work. Our code is available at: https://github.com/nayohan/SentiCSE</abstract>
      <url hash="07a79159">2024.lrec-main.1280</url>
      <attachment type="OptionalSupplementaryMaterial" hash="79c570f9">2024.lrec-main.1280.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>kim-etal-2024-senticse-sentiment</bibkey>
    </paper>
    <paper id="1281">
      <title>Sequence Reducible Holdout Loss for Language Model Pretraining</title>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author><first>Nicholas</first><last>Monath</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Sam</first><last>Wiseman</last></author>
      <pages>14705–14716</pages>
      <abstract>Data selection techniques, which adaptively select datapoints inside the training loop, have demonstrated empirical benefits in reducing the number of gradient steps to train neural models. However, these techniques have so far largely been applied to classification. In this work, we study their applicability to language model pretraining, a highly time-intensive task. We propose a simple modification to an existing data selection technique (reducible hold-out loss training) in order to adapt it to the sequence losses typical in language modeling. We experiment on both autoregressive and masked language modelling, and show that applying data selection to pretraining offers notable benefits including a 4.3% reduction in total number of steps, a 21.5% steps reduction in average, to an intermediate target perplexity, over the course of pretraining an autoregressive language model. Further, data selection trained language models demonstrate significantly better generalization ability on out of domain datasets - 7.9% reduction in total number of steps and 23.2% average steps reduction to an intermediate target perplexity.</abstract>
      <url hash="2d631d1c">2024.lrec-main.1281</url>
      <bibkey>thirukovalluru-etal-2024-sequence-reducible</bibkey>
    </paper>
    <paper id="1282">
      <title>Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives</title>
      <author><first>Gustave</first><last>Cortal</last></author>
      <pages>14717–14728</pages>
      <abstract>The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.</abstract>
      <url hash="a9c57a45">2024.lrec-main.1282</url>
      <bibkey>cortal-2024-sequence-sequence</bibkey>
    </paper>
    <paper id="1283">
      <title>Sequence-to-Sequence <fixed-case>S</fixed-case>panish Pre-trained Language Models</title>
      <author><first>Vladimir</first><last>Araujo</last></author>
      <author><first>Maria Mihaela</first><last>Trusca</last></author>
      <author><first>Rodrigo</first><last>Tufiño</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <pages>14729–14743</pages>
      <abstract>In recent years, significant advancements in pre-trained language models have driven the creation of numerous non-English language variants, with a particular emphasis on encoder-only and decoder-only architectures. While Spanish language models based on BERT and GPT have demonstrated proficiency in natural language understanding and generation, there remains a noticeable scarcity of encoder-decoder models explicitly designed for sequence-to-sequence tasks, which aim to map input sequences to generate output sequences conditionally. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across various sequence-to-sequence tasks, including summarization, question answering, split-and-rephrase, dialogue, and translation. Our findings underscore the competitive performance of all models, with the BART- and T5-based models emerging as top performers across all tasks. We have made all models publicly available to the research community to foster future explorations and advancements in Spanish NLP: https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.</abstract>
      <url hash="4c512ad3">2024.lrec-main.1283</url>
      <bibkey>araujo-etal-2024-sequence-sequence</bibkey>
    </paper>
    <paper id="1284">
      <title>Sequential and Repetitive Pattern Learning for Temporal Knowledge Graph Reasoning</title>
      <author><first>Xuefei</first><last>Li</last></author>
      <author><first>Huiwei</first><last>Zhou</last></author>
      <author><first>Weihong</first><last>Yao</last></author>
      <author><first>Wenchu</first><last>Li</last></author>
      <author><first>Yingyu</first><last>Lin</last></author>
      <author><first>Lei</first><last>Du</last></author>
      <pages>14744–14754</pages>
      <abstract>Temporal Knowledge Graph (TKG) reasoning has received a growing interest recently, especially in forecasting the future facts based on the historical KG sequences. Existing studies typically utilize a recurrent neural network to learn the evolutional representations of entities for temporal reasoning. However, these methods are hard to capture the complex temporal evolutional patterns such as sequential and repetitive patterns accurately. To tackle this challenge, we propose a novel Sequential and Repetitive Pattern Learning (SRPL) method, which comprehensively captures both the sequential and repetitive patterns. Specifically, a Dependency-aware Sequential Pattern Learning (DSPL) component expresses the temporal dependencies of each historical timestamp as embeddings for accurately capturing the sequential patterns across temporally adjacent facts. A Time-interval guided Repetitive Pattern Learning (TRPL) component models the irregular time intervals between historical repetitive facts for capturing the repetitive patterns. Extensive experiments on four representative benchmarks demonstrate that our proposed method outperforms state-of-the-art methods in all metrics by an obvious margin, especially on GDELT dataset, where performance improvement of MRR reaches up to 18.84%.</abstract>
      <url hash="c9bd6302">2024.lrec-main.1284</url>
      <bibkey>li-etal-2024-sequential-repetitive</bibkey>
    </paper>
    <paper id="1285">
      <title><fixed-case>SGCM</fixed-case>: Salience-Guided Context Modeling for Question Generation</title>
      <author><first>Chuyao</first><last>Ding</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Jianmin</first><last>Yao</last></author>
      <pages>14755–14762</pages>
      <abstract>We tackle Paragraph-level Question Generation (abbr., PQG) in this paper. PQG is a task of automatically generating questions given paragraphs and answers. Identifying the relevant sentences to answers is crucial for reasoning the possible questions before generation. Accordingly, we propose a salience-guided approach to enhance PQG. Specifically, we construct an auxiliary task of identifying salient sentences that manifest relevance. Grounded on this auxiliary task and the main task of PQG, we strengthen the BART encoder during training within a multitask learning framework. In particular, we utilize the identified salient sentences as an explicit guidance to enable the salience-aware attention computation in the BART decoder. We experiment on the benchmark dataset FairytaleQA. The test results show that our approach yields substantial improvements compared to the BART baseline, achieving the Rouge-L, BLEU4, BERTScore, Q-BLUE-3 and F1-scores of about 56.56%, 19.78%, 61.19%, 54.33% and 43.55%, respectively. Both the source codes and models will be publicly available.</abstract>
      <url hash="59ebaf12">2024.lrec-main.1285</url>
      <bibkey>ding-etal-2024-sgcm-salience</bibkey>
    </paper>
    <paper id="1286">
      <title><fixed-case>S</fixed-case>hadow<fixed-case>S</fixed-case>ense: A Multi-annotated Dataset for Evaluating Word Sense Induction</title>
      <author><first>Ondřej</first><last>Herman</last></author>
      <author><first>Miloš</first><last>Jakubíček</last></author>
      <pages>14763–14769</pages>
      <abstract>In this paper we present a novel bilingual (Czech, English) dataset called ShadowSense developed for the purposes of word sense induction (WSI) evaluation. Unlike existing WSI datasets, ShadowSense is annotated by multiple annotators whose inter-annotator agreement represents key reliability score to be used for evaluation of systems automatically inducing word senses. In this paper we clarify the motivation for such an approach, describe the dataset in detail and provide evaluation of three neural WSI systems showing substantial differences compared to traditional evaluation paradigms.</abstract>
      <url hash="f56ee632">2024.lrec-main.1286</url>
      <bibkey>herman-jakubicek-2024-shadowsense-multi</bibkey>
    </paper>
    <paper id="1287">
      <title>Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies</title>
      <author><first>Philipp</first><last>Sadler</last></author>
      <author><first>Sherzod</first><last>Hakimov</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>14770–14783</pages>
      <abstract>In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players’ assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement—which invites further research in the direction of cost-sharing in collaborative interactions.</abstract>
      <url hash="459e1a9b">2024.lrec-main.1287</url>
      <bibkey>sadler-etal-2024-sharing-cost</bibkey>
    </paper>
    <paper id="1288">
      <title><fixed-case>SIGA</fixed-case>: A Naturalistic <fixed-case>NLI</fixed-case> Dataset of <fixed-case>E</fixed-case>nglish Scalar Implicatures with Gradable Adjectives</title>
      <author><first>Rashid</first><last>Nizamani</last></author>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>14784–14795</pages>
      <abstract>Many utterances convey meanings that go beyond the literal meaning of a sentence. One class of such meanings is scalar implicatures, a phenomenon by which a speaker conveys the negation of a more informative utterance by producing a less informative utterance. This paper introduces a Natural Language Inference (NLI) dataset designed to investigate the ability of language models to interpret utterances with scalar implicatures. Our dataset is comprised of text extracted from the C4 English text corpus and annotated with both crowd-sourced and expert annotations. We evaluate NLI models based on DeBERTa to investigate 1) whether NLI models can learn to predict pragmatic inferences involving gradable adjectives and 2) whether models generalize to utterances involving unseen adjectives. We find that fine-tuning NLI models on our dataset significantly improves their performance to derive scalar implicatures, both for in-domain and for out-of domain examples. At the same time, we find that the investigated models still perform considerably worse on examples with scalar implicatures than on other types of NLI examples, highlighting that pragmatic inferences still pose challenges for current models.</abstract>
      <url hash="b19e85b6">2024.lrec-main.1288</url>
      <bibkey>nizamani-etal-2024-siga-naturalistic</bibkey>
    </paper>
    <paper id="1289">
      <title><fixed-case>S</fixed-case>ign<fixed-case>BLEU</fixed-case>: Automatic Evaluation of Multi-channel Sign Language Translation</title>
      <author><first>Jung-Ho</first><last>Kim</last></author>
      <author><first>Mathew</first><last>Huerta-Enochian</last></author>
      <author><first>Changyong</first><last>Ko</last></author>
      <author><first>Du Hui</first><last>Lee</last></author>
      <pages>14796–14811</pages>
      <abstract>Sign languages are multi-channel languages that communicate information through not just the hands (manual signals) but also facial expressions and upper body movements (non-manual signals). However, since automatic sign language translation is usually performed by generating a single sequence of glosses, researchers eschew non-manual and co-occurring manual signals in favor of a simplified list of manual glosses. This can lead to significant information loss and ambiguity. In this paper, we introduce a new task named multi-channel sign language translation (MCSLT) and present a novel metric, SignBLEU, designed to capture multiple signal channels. We validated SignBLEU on a system-level task using three sign language corpora with varied linguistic structures and transcription methodologies and examined its correlation with human judgment through two segment-level tasks. We found that SignBLEU consistently correlates better with human judgment than competing metrics. To facilitate further MCSLT research, we report benchmark scores for the three sign language corpora and release the source code for SignBLEU at https://github.com/eq4all-projects/SignBLEU.</abstract>
      <url hash="1397dcb1">2024.lrec-main.1289</url>
      <bibkey>kim-etal-2024-signbleu-automatic</bibkey>
    </paper>
    <paper id="1290">
      <title><fixed-case>S</fixed-case>ilver<fixed-case>A</fixed-case>lign: <fixed-case>MT</fixed-case>-Based Silver Data Algorithm for Evaluating Word Alignment</title>
      <author><first>Abdullatif</first><last>Koksal</last></author>
      <author><first>Silvia</first><last>Severini</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>14812–14825</pages>
      <abstract>Word alignments are essential for a variety of NLP tasks. Therefore, choosing the best approaches for their creation is crucial. However, the scarce availability of gold evaluation data makes the choice difficult. We propose SilverAlign, a new method to automatically create silver data for the evaluation of word aligners by exploiting machine translation and minimal pairs. We show that performance on our silver data correlates well with gold benchmarks for 9 language pairs, making our approach a valid resource for evaluation of different languages and domains when gold data is not available. This addresses the important scenario of missing gold data alignments for low-resource languages.</abstract>
      <url hash="04dc5c84">2024.lrec-main.1290</url>
      <bibkey>koksal-etal-2024-silveralign-mt</bibkey>
    </paper>
    <paper id="1291">
      <title>Silver Retriever: Advancing Neural Passage Retrieval for <fixed-case>P</fixed-case>olish Question Answering</title>
      <author><first>Piotr</first><last>Rybak</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <pages>14826–14831</pages>
      <abstract>Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present Silver Retriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. Silver Retriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.</abstract>
      <url hash="54bd2cab">2024.lrec-main.1291</url>
      <bibkey>rybak-ogrodniczuk-2024-silver-retriever</bibkey>
    </paper>
    <paper id="1292">
      <title><fixed-case>S</fixed-case>im<fixed-case>L</fixed-case>ex-999 for <fixed-case>D</fixed-case>utch</title>
      <author><first>Lizzy</first><last>Brans</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <pages>14832–14845</pages>
      <abstract>Word embeddings revolutionised natural language processing by effectively representing words as dense vectors. Although many datasets exist to evaluate English embeddings, few cater to Dutch. We developed a Dutch variant of the SimLex-999 word similarity dataset by gathering similarity judgements from 235 native Dutch speakers. Subsequently, we evaluated two popular Dutch language models, Bertje and RobBERT, finding that Bertje showed superior alignment with human semantic similarity judgments compared to RobBERT. This study provides the first intrinsic Dutch word embedding evaluation dataset, which enables accurate assessment of these embeddings and fosters the development of effective Dutch language models.</abstract>
      <url hash="aa7eabfb">2024.lrec-main.1292</url>
      <bibkey>brans-bloem-2024-simlex-999</bibkey>
    </paper>
    <paper id="1293">
      <title><fixed-case>S</fixed-case>inkhorn Distance Minimization for Knowledge Distillation</title>
      <author><first>Xiao</first><last>Cui</last></author>
      <author><first>Yulei</first><last>Qin</last></author>
      <author><first>Yuting</first><last>Gao</last></author>
      <author><first>Enwei</first><last>Zhang</last></author>
      <author><first>Zihan</first><last>Xu</last></author>
      <author><first>Tong</first><last>Wu</last></author>
      <author><first>Ke</first><last>Li</last></author>
      <author><first>Xing</first><last>Sun</last></author>
      <author><first>Wengang</first><last>Zhou</last></author>
      <author><first>Houqiang</first><last>Li</last></author>
      <pages>14846–14858</pages>
      <abstract>Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception of divergence in each teacher-student sample pair. Instead, we propose a batch-wise reformulation to capture geometric intricacies of distributions across samples in the high-dimensional space. Comprehensive evaluation on GLUE and SuperGLUE, in terms of comparability, validity, and generalizability, highlights our superiority over state-of-the-art methods on all kinds of LLMs with encoder-only, encoder-decoder, and decoder-only architectures.</abstract>
      <url hash="4cd2ad09">2024.lrec-main.1293</url>
      <bibkey>cui-etal-2024-sinkhorn-distance</bibkey>
    </paper>
    <paper id="1294">
      <title><fixed-case>SI</fixed-case>-<fixed-case>NLI</fixed-case>: A <fixed-case>S</fixed-case>lovene Natural Language Inference Dataset and Its Evaluation</title>
      <author><first>Matej</first><last>Klemen</last></author>
      <author><first>Aleš</first><last>Žagar</last></author>
      <author><first>Jaka</first><last>Čibej</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <pages>14859–14870</pages>
      <abstract>Natural language inference (NLI) is an important language understanding benchmark. Two deficiencies of this benchmark are: i) most existing NLI datasets exist for English and a few other well-resourced languages, and ii) most NLI datasets are formed with a narrow set of annotators’ instructions, allowing the prediction models to capture linguistic clues instead of measuring true reasoning capability. We address both issues and introduce SI-NLI, the first dataset for Slovene natural language inference. The dataset is constructed from scratch using knowledgeable annotators with carefully crafted guidelines aiming to avoid commonly encountered problems in existing NLI datasets. We also manually translate the SI-NLI to English to enable cross-lingual model training and evaluation. Using the newly created dataset and its translation, we train and evaluate a variety of large transformer language models in a monolingual and cross-lingual setting. The results indicate that larger models, in general, achieve better performance. The qualitative analysis shows that the SI-NLI dataset is diverse and that there remains plenty of room for improvement even for the largest models.</abstract>
      <url hash="4cc5cdd3">2024.lrec-main.1294</url>
      <bibkey>klemen-etal-2024-si-nli</bibkey>
    </paper>
    <paper id="1295">
      <title><fixed-case>S</fixed-case>k<fixed-case>OT</fixed-case>a<fixed-case>PA</fixed-case>: A Dataset for Skepticism Detection in Online Text after Persuasion Attempt</title>
      <author><first>Smitha</first><last>Muthya Sudheendra</last></author>
      <author><first>Maral</first><last>Abdollahi</last></author>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Jisu</first><last>Huh</last></author>
      <author><first>Jaideep</first><last>Srivastava</last></author>
      <pages>14871–14876</pages>
      <abstract>Individuals often encounter persuasion attempts, during which a persuasion agent aims to persuade a target to change the target’s emotions, beliefs, and behaviors. These persuasion attempts can be observed in various social settings, such as advertising, public health, political campaigns, and personal relationships. During these persuasion attempts, targets generally like to preserve their autonomy, so their responses often manifest in some form of resistance, like a skeptical reaction. In order to detect such skepticism in response to persuasion attempts on social media, we developed a corpus based on consumer psychology. In this paper, we consider one of the most prominent areas in which persuasion attempts unfold: social media influencer marketing. In this paper, we introduce the skepticism detection corpus, SkOTaPA, which was developed using multiple independent human annotations, and inter-coder reliability was evaluated with Krippendorff’s alpha (0.709). We performed validity tests to show skepticism cannot be detected using other potential proxy variables like sentiment and sarcasm.</abstract>
      <url hash="f5dfd8f1">2024.lrec-main.1295</url>
      <bibkey>muthya-sudheendra-etal-2024-skotapa-dataset</bibkey>
    </paper>
    <paper id="1296">
      <title><fixed-case>SL</fixed-case>a<fixed-case>CAD</fixed-case>: A Spoken Language Corpus for Early <fixed-case>A</fixed-case>lzheimer’s Disease Detection</title>
      <author><first>Shahla</first><last>Farzana</last></author>
      <author><first>Edoardo</first><last>Stoppa</last></author>
      <author><first>Alex</first><last>Leow</last></author>
      <author><first>Tamar</first><last>Gollan</last></author>
      <author><first>Raeanne</first><last>Moore</last></author>
      <author><first>David</first><last>Salmon</last></author>
      <author><first>Douglas</first><last>Galasko</last></author>
      <author><first>Erin</first><last>Sundermann</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>14877–14897</pages>
      <abstract>Identifying early markers of Alzheimer’s disease (AD) trajectory enables intervention in early disease stages when our currently-available interventions are most likely to be beneficial. Research has shown that alterations in speech, as well as linguistic and semantic deviations in spontaneous conversation detected using natural language processing, manifest early in AD prior to some other observed cognitive deficits. Recent studies show that cerebrospinal fluid (CSF) levels serve as useful early biomarkers for identifying early AD, but CSF biomarkers are challenging to collect. A simpler alternative that has seen very rapid development is based on the use of plasma biomarkers as a blood draw is minimally invasive. Associating verbal and nonverbal characteristics from speech data with CSF and plasma biomarkers may open the door to less invasive, more efficient methods for early AD detection. We present SLaCAD, a new dataset to facilitate this process. We describe our data collection procedures, analyze the resulting corpus, and present preliminary findings that relate measures extracted from the audio and transcribed text to clinical diagnoses, CSF levels, and plasma biomarkers. Our findings demonstrate the feasibility of this and indicate that the collected data can be used to improve assessments of early AD.</abstract>
      <url hash="391a5d15">2024.lrec-main.1296</url>
      <bibkey>farzana-etal-2024-slacad-spoken</bibkey>
    </paper>
    <paper id="1297">
      <title>Slot and Intent Detection Resources for <fixed-case>B</fixed-case>avarian and <fixed-case>L</fixed-case>ithuanian: Assessing Translations vs Natural Queries to Digital Assistants</title>
      <author><first>Miriam</first><last>Winkler</last></author>
      <author><first>Virginija</first><last>Juozapaityte</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>14898–14915</pages>
      <abstract>Digital assistants perform well in high-resource languages like English, where tasks like slot and intent detection (SID) are well-supported. Many recent SID datasets start including multiple language varieties. However, it is unclear how realistic these translated datasets are. Therefore, we extend one such dataset, namely xSID-0.4, to include two underrepresented languages: Bavarian, a German dialect, and Lithuanian, a Baltic language. Both language variants have limited speaker populations and are often not included in multilingual projects. In addition to translations we provide “natural” queries to digital assistants generated by native speakers. We further include utterances from another dataset for Bavarian to build the richest SID dataset available today for a low-resource dialect without standard orthography. We then set out to evaluate models trained on English in a zero-shot scenario on our target language variants. Our evaluation reveals that translated data can produce overly optimistic scores. However, the error patterns in translated and natural datasets are highly similar. Cross-dataset experiments demonstrate that data collection methods influence performance, with scores lower than those achieved with single-dataset translations. This work contributes to enhancing SID datasets for underrepresented languages, yielding NaLiBaSID, a new evaluation dataset for Bavarian and Lithuanian.</abstract>
      <url hash="a8313e56">2024.lrec-main.1297</url>
      <bibkey>winkler-etal-2024-slot-intent</bibkey>
    </paper>
    <paper id="1298">
      <title><fixed-case>S</fixed-case>lovak<fixed-case>S</fixed-case>um: A Large Scale <fixed-case>S</fixed-case>lovak Summarization Dataset</title>
      <author><first>Viktoria</first><last>Ondrejova</last></author>
      <author><first>Marek</first><last>Suppa</last></author>
      <pages>14916–14922</pages>
      <abstract>The ability to automatically summarize news articles has become increasingly important due to the vast amount of information available online. Together with the rise of chatbots , Natural Language Processing (NLP) has recently experienced a tremendous amount of development. Despite these advancements, the majority of research is focused on established well-resourced languages, such as English. To contribute to development of the low resource Slovak language, we introduce SlovakSum, a Slovak news summarization dataset consisting of over 200 thousand news articles with titles and short abstracts obtained from multiple Slovak newspapers. The abstractive approach, including MBART and mT5 models, was used to evaluate various baselines. The code for the reproduction of our dataset and experiments can be found at https://github.com/NaiveNeuron/slovaksum</abstract>
      <url hash="cc969525">2024.lrec-main.1298</url>
      <bibkey>ondrejova-suppa-2024-slovaksum-large</bibkey>
    </paper>
    <paper id="1299">
      <title>Small Language Models Are Good Too: An Empirical Study of Zero-Shot Classification</title>
      <author><first>Pierre</first><last>Lepagnol</last></author>
      <author><first>Thomas</first><last>Gerald</last></author>
      <author><first>Sahar</first><last>Ghannay</last></author>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>14923–14936</pages>
      <abstract>This study is part of the debate on the efficiency of large versus small language models for text classification by prompting. We assess the performance of small language models in zero-shot text classification, challenging the prevailing dominance of large models. Across 15 datasets, our investigation benchmarks language models from 77M to 40B parameters using different architectures and scoring functions. Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts. We developed and shared a comprehensive open-source repository that encapsulates our methodologies. This research underscores the notion that bigger isn’t always better, suggesting that resource-efficient small models may offer viable solutions for specific data classification challenges.</abstract>
      <url hash="06871381">2024.lrec-main.1299</url>
      <bibkey>lepagnol-etal-2024-small-language</bibkey>
    </paper>
    <paper id="1300">
      <title><fixed-case>S</fixed-case>mart<fixed-case>T</fixed-case>rim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models</title>
      <author><first>Zekun</first><last>Wang</last></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Haichao</first><last>Zhu</last></author>
      <author><first>Jiafeng</first><last>Liang</last></author>
      <author><first>Liping</first><last>Shan</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Dongliang</first><last>Xu</last></author>
      <author><first>Qing</first><last>Yang</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>14937–14953</pages>
      <abstract>Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstrate that SmartTrim accelerates the original model by 2-3 times with minimal performance degradation, highlighting the effectiveness and efficiency compared to previous approaches. Code will be available at https://github.com/kugwzk/SmartTrim.</abstract>
      <url hash="170e81ed">2024.lrec-main.1300</url>
      <bibkey>wang-etal-2024-smarttrim-adaptive</bibkey>
    </paper>
    <paper id="1301">
      <title><fixed-case>SM</fixed-case>-<fixed-case>FEEL</fixed-case>-<fixed-case>BG</fixed-case> - the First <fixed-case>B</fixed-case>ulgarian Datasets and Classifiers for Detecting Feelings, Emotions, and Sentiments of <fixed-case>B</fixed-case>ulgarian Social Media Text</title>
      <author><first>Irina</first><last>Temnikova</last></author>
      <author><first>Iva</first><last>Marinova</last></author>
      <author><first>Silvia</first><last>Gargova</last></author>
      <author><first>Ruslana</first><last>Margova</last></author>
      <author><first>Alexander</first><last>Komarov</last></author>
      <author><first>Tsvetelina</first><last>Stefanova</last></author>
      <author><first>Veneta</first><last>Kireva</last></author>
      <author><first>Dimana</first><last>Vyatrova</last></author>
      <author><first>Nevena</first><last>Grigorova</last></author>
      <author><first>Yordan</first><last>Mandevski</last></author>
      <author><first>Stefan</first><last>Minkov</last></author>
      <pages>14954–14966</pages>
      <abstract>This article introduces SM-FEEL-BG – the first Bulgarian-language package, containing 6 datasets with Social Media (SM) texts with emotion, feeling, and sentiment labels and 4 classifiers trained on them. All but one dataset from these are freely accessible for research purposes. The largest dataset contains 6000 Twitter, Telegram, and Facebook texts, manually annotated with 21 fine-grained emotion/feeling categories. The fine-grained labels are automatically merged into three coarse-grained sentiment categories, producing a dataset with two parallel sets of labels. Several classification experiments are run on different subsets of the fine-grained categories and their respective sentiment labels with a Bulgarian fine-tuned BERT. The highest Acc. reached was 0.61 for 16 emotions and 0.70 for 11 emotions (incl. 310 ChatGPT 4-generated texts). The sentiments Acc. of the 11 emotions dataset was also the highest (0.79). As Facebook posts cannot be shared, we ran experiments on the Twitter and Telegram subset of the 11 emotions dataset, obtaining 0.73 Acc. for emotions and 0.80 for sentiments. The article describes the annotation procedures, guidelines, experiments, and results. We believe that this package will be of significant benefit to researchers working on emotion detection and sentiment analysis in Bulgarian.</abstract>
      <url hash="b1ebd3d5">2024.lrec-main.1301</url>
      <bibkey>temnikova-etal-2024-sm-feel</bibkey>
    </paper>
    <paper id="1302">
      <title><fixed-case>SOBR</fixed-case>: A Corpus for Stylometry, Obfuscation, and Bias on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Chris</first><last>Emmery</last></author>
      <author><first>Marilù</first><last>Miotto</last></author>
      <author><first>Sergey</first><last>Kramp</last></author>
      <author><first>Bennett</first><last>Kleinberg</last></author>
      <pages>14967–14983</pages>
      <abstract>Sharing textual content in the form of public posts on online platforms remains a significant part of the social web. Research on stylometric profiling suggests that despite users’ discreetness, and even under the guise of anonymity, the content and style of such posts may still reveal detailed author information. Studying how this might be inferred and obscured is relevant not only to the domain of cybersecurity, but also to those studying bias of classifiers drawing features from web corpora. While the collection of gold standard data is expensive, prior work shows that distant labels (i.e., those gathered via heuristics) offer an effective alternative. Currently, however, pre-existing corpora are limited in scope (e.g., variety of attributes and size). We present the SOBR corpus: 235M Reddit posts for which we used subreddits, flairs, and self-reports as distant labels for author attributes (age, gender, nationality, personality, and political leaning). In addition to detailing the data collection pipeline and sampling strategy, we report corpus statistics and provide a discussion on the various tasks and research avenues to be pursued using this resource. Along with the raw corpus, we provide sampled splits of the data, and suggest baselines for stylometric profiling. We close our work with a detailed set of ethical considerations relevant to the proposed lines of research.</abstract>
      <url hash="53070a2a">2024.lrec-main.1302</url>
      <bibkey>emmery-etal-2024-sobr-corpus</bibkey>
    </paper>
    <paper id="1303">
      <title>Social Convos: Capturing Agendas and Emotions on Social Media</title>
      <author><first>Ankita</first><last>Bhaumik</last></author>
      <author><first>Ning</first><last>Sa</last></author>
      <author><first>Gregorios</first><last>Katsios</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <pages>14984–14994</pages>
      <abstract>Social media platforms are popular tools for disseminating targeted information during major public events like elections or pandemics. Systematic analysis of the message traffic can provide valuable insights into prevailing opinions and social dynamics among different segments of the population. We are specifically interested in influence spread, and in particular whether more deliberate influence operations can be detected. However, filtering out the essential messages with telltale influence indicators from the extensive and often chaotic social media traffic is a major challenge.In this paper we present a novel approach to extract influence indicators from messages circulating among groups of users discussing particular topics. We build upon the the concept of a convo to identify influential authors who are actively promoting some particular agenda around that topic within the group. We focus on two influence indicators: the (control of) agenda and the use of emotional language.</abstract>
      <url hash="393baddd">2024.lrec-main.1303</url>
      <bibkey>bhaumik-etal-2024-social-convos</bibkey>
    </paper>
    <paper id="1304">
      <title>Social Orientation: A New Feature for Dialogue Analysis</title>
      <author><first>Todd</first><last>Morrill</last></author>
      <author><first>Zhaoyuan</first><last>Deng</last></author>
      <author><first>Yanda</first><last>Chen</last></author>
      <author><first>Amith</first><last>Ananthram</last></author>
      <author><first>Colin Wayne</first><last>Leach</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>14995–15011</pages>
      <abstract>There are many settings where it is useful to predict and explain the success or failure of a dialogue. Circumplex theory from psychology models the social orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation participants and can be used to predict and explain the outcome of social interactions. Our work is novel in its systematic application of social orientation tags to modeling conversation outcomes. In this paper, we introduce a new data set of dialogue utterances machine-labeled with social orientation tags. We show that social orientation tags improve task performance, especially in low-resource settings, on both English and Chinese language benchmarks. We also demonstrate how social orientation tags help explain the outcomes of social interactions when used in neural models. Based on these results showing the utility of social orientation tags for dialogue outcome prediction tasks, we release our data sets, code, and models that are fine-tuned to predict social orientation tags on dialogue utterances.</abstract>
      <url hash="a6d8067a">2024.lrec-main.1304</url>
      <bibkey>morrill-etal-2024-social-orientation</bibkey>
    </paper>
    <paper id="1305">
      <title><fixed-case>S</fixed-case>oft<fixed-case>MCL</fixed-case>: Soft Momentum Contrastive Learning for Fine-grained Sentiment-aware Pre-training</title>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Liang-Chih</first><last>Yu</last></author>
      <author><first>Xuejie</first><last>Zhang</last></author>
      <pages>15012–15023</pages>
      <abstract>The pre-training for language models captures general language understanding but fails to distinguish the affective impact of a particular context to a specific word. Recent works have sought to introduce contrastive learning (CL) for sentiment-aware pre-training in acquiring affective information. Nevertheless, these methods present two significant limitations. First, the compatibility of the GPU memory often limits the number of negative samples, hindering the opportunities to learn good representations. In addition, using only a few sentiment polarities as hard labels, e.g., positive, neutral, and negative, to supervise CL will force all representations to converge to a few points, leading to the issue of latent space collapse. This study proposes a soft momentum contrastive learning (SoftMCL) for fine-grained sentiment-aware pre-training. Instead of hard labels, we introduce valence ratings as soft-label supervision for CL to fine-grained measure the sentiment similarities between samples. The proposed SoftMCL conducts CL on both the word- and sentence-level to enhance the model’s ability to learn affective information. A momentum queue was introduced to expand the contrastive samples, allowing storing and involving more negatives to overcome the limitations of hardware platforms. Extensive experiments were conducted on four different sentiment-related tasks, which demonstrates the effectiveness of the proposed SoftMCL method. The code and data of the proposed SoftMCL is available at: https://www.github.com/wangjin0818/SoftMCL/.</abstract>
      <url hash="e08c3f1c">2024.lrec-main.1305</url>
      <bibkey>wang-etal-2024-softmcl-soft</bibkey>
    </paper>
    <paper id="1306">
      <title>Soft-Prompting with Graph-of-Thought for Multi-modal Representation Learning</title>
      <author><first>Jun Cheng</first><last>Yang</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Shuai</first><last>Xie</last></author>
      <author><first>Wei</first><last>Yu</last></author>
      <author><first>Shijun</first><last>Li</last></author>
      <author><first>Bo</first><last>Du</last></author>
      <pages>15024–15036</pages>
      <abstract>The chain-of-thought technique has been received well in multi-modal tasks. It is a step-by-step linear reasoning process that adjusts the length of the chain to improve the performance of generated prompts. However, human thought processes are predominantly non-linear, as they encompass multiple aspects simultaneously and employ dynamic adjustment and updating mechanisms. Therefore, we propose a novel Aggregation-Graph-of-Thought (AGoT) mechanism for soft-prompt tuning in multi-modal representation learning. The proposed AGoT models the human thought process not only as a chain but also models each step as a reasoning aggregation graph to cope with the overlooked multiple aspects of thinking in single-step reasoning. This turns the entire reasoning process into prompt aggregation and prompt flow operations. Experiments show that our multi-modal model enhanced with AGoT soft-prompting achieves good results in several tasks such as text-image retrieval, visual question answering, and image recognition. In addition, we demonstrate that it has good domain generalization performance due to better reasoning.</abstract>
      <url hash="58f32468">2024.lrec-main.1306</url>
      <bibkey>yang-etal-2024-soft-prompting</bibkey>
    </paper>
    <paper id="1307">
      <title>Soft Well-Formed Semantic Parsing with Score-Based Selection</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <pages>15037–15043</pages>
      <abstract>Semantic parsing is the task of translating natural language into a structured, formal semantic representation that can be interpreted by machines. These semantic representations are organized with complex structures. While various models have been developed for semantic parsing, there has been limited focus on generating semantic representations with well-formed structures. In this study, we introduce a score-based method to select well-formed outputs from candidates generated by beam search algorithms. Our experiments focus on parsing texts into discourse representation structures, which are innovative semantic representations designed to capture the meaning of texts with arbitrary lengths across languages. Our experimental results demonstrate that models utilizing the proposed method can reduce the number of ill-formed outputs and improve F1 scores in English. Furthermore, our final model achieves significant improvements in German, Italian and Dutch zero-shot DRS parsing by effectively preventing ill-formed outputs.</abstract>
      <url hash="72df3e02">2024.lrec-main.1307</url>
      <bibkey>liu-2024-soft-well</bibkey>
    </paper>
    <paper id="1308">
      <title>So Hateful! Building a Multi-Label Hate Speech Annotated <fixed-case>A</fixed-case>rabic Dataset</title>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Md. Rafiul</first><last>Biswas</last></author>
      <pages>15044–15055</pages>
      <abstract>Social media enables widespread propagation of hate speech targeting groups based on ethnicity, religion, or other characteristics. With manual content moderation being infeasible given the volume, automatic hate speech detection is essential. This paper analyzes 70,000 Arabic tweets, from which 15,965 tweets were selected and annotated, to identify hate speech patterns and train classification models. Annotators labeled the Arabic tweets for offensive content, hate speech, emotion intensity and type, effect on readers, humor, factuality, and spam. Key findings reveal 15% of tweets contain offensive language while 6% have hate speech, mostly targeted towards groups with common ideological or political affiliations. Annotations capture diverse emotions, and sarcasm is more prevalent than humor. Additionally, 10% of tweets provide verifiable factual claims, and 7% are deemed important. For hate speech detection, deep learning models like AraBERT outperform classical machine learning approaches. By providing insights into hate speech characteristics, this work enables improved content moderation and reduced exposure to online hate. The annotated dataset advances Arabic natural language processing research and resources.</abstract>
      <url hash="c3d1de7d">2024.lrec-main.1308</url>
      <bibkey>zaghouani-etal-2024-hateful-building</bibkey>
    </paper>
    <paper id="1309">
      <title>Sonos Voice Control Bias Assessment Dataset: A Methodology for Demographic Bias Assessment in Voice Assistants</title>
      <author><first>Chloe</first><last>Sekkat</last></author>
      <author><first>Fanny</first><last>Leroy</last></author>
      <author><first>Salima</first><last>Mdhaffar</last></author>
      <author><first>Blake Perry</first><last>Smith</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Joseph</first><last>Dureau</last></author>
      <author><first>Alice</first><last>Coucke</last></author>
      <pages>15056–15075</pages>
      <abstract>Recent works demonstrate that voice assistants do not perform equally well for everyone, but research on demographic robustness of speech technologies is still scarce. This is mainly due to the rarity of large datasets with controlled demographic tags. This paper introduces the Sonos Voice Control Bias Assessment Dataset, an open dataset composed of voice assistant requests for North American English in the music domain (1,038 speakers, 166 hours, 170k audio samples, with 9,040 unique labelled transcripts) with a controlled demographic diversity (gender, age, dialectal region and ethnicity). We also release a statistical demographic bias assessment methodology, at the univariate and multivariate levels, tailored to this specific use case and leveraging spoken language understanding metrics rather than transcription accuracy, which we believe is a better proxy for user experience. To demonstrate the capabilities of this dataset and statistical method to detect demographic bias, we consider a pair of state-of-the-art Automatic Speech Recognition and Spoken Language Understanding models. Results show statistically significant differences in performance across age, dialectal region and ethnicity. Multivariate tests are crucial to shed light on mixed effects between dialectal region, gender and age.</abstract>
      <url hash="8dda69c8">2024.lrec-main.1309</url>
      <bibkey>sekkat-etal-2024-sonos-voice</bibkey>
    </paper>
    <paper id="1310">
      <title>Source-free Domain Adaptation for Aspect-based Sentiment Analysis</title>
      <author><first>Zishuo</first><last>Zhao</last></author>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Zhenzhou</first><last>Lin</last></author>
      <author><first>Jingyou</first><last>Xie</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <pages>15076–15086</pages>
      <abstract>Unsupervised Domain Adaptation (UDA) of the Aspect-based Sentiment Analysis (ABSA) task aims to transfer knowledge learned from labeled source domain datasets to unlabeled target domains on the assumption that samples from the source domain are freely accessible during the training period. However, this assumption can easily lead to privacy invasion issues in real-world applications, especially when the source data involves privacy-preserving domains such as healthcare and finance. In this paper, we introduce the Source-Free Domain Adaptation Framework for ABSA (SF-ABSA), which only allows model parameter transfer, not data transfer, between different domains. Specifically, the proposed SF-ABSA framework consists of two parts, i.e., feature-based adaptation and pseudo-label-based adaptation. Experiment results on four benchmarks show that the proposed framework performs competitively with traditional unsupervised domain adaptation methods under the premise of insufficient information, which demonstrates the superiority of our method under privacy conditions.</abstract>
      <url hash="0608cd63">2024.lrec-main.1310</url>
      <bibkey>zhao-etal-2024-source-free</bibkey>
    </paper>
    <paper id="1311">
      <title><fixed-case>SPACE</fixed-case>-<fixed-case>IDEAS</fixed-case>: A Dataset for Salient Information Detection in Space Innovation</title>
      <author><first>Andres</first><last>Garcia-Silva</last></author>
      <author><first>Cristian</first><last>Berrio</last></author>
      <author><first>Jose Manuel</first><last>Gomez-Perez</last></author>
      <pages>15087–15092</pages>
      <abstract>Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow. Nevertheless, most of the datasets available for this task are derived mainly from academic publications. We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain. The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles. In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model. We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers.</abstract>
      <url hash="fe3667b0">2024.lrec-main.1311</url>
      <bibkey>garcia-silva-etal-2024-space-ideas</bibkey>
    </paper>
    <paper id="1312">
      <title><fixed-case>S</fixed-case>panish Resource Grammar Version 2023</title>
      <author><first>Olga</first><last>Zamaraeva</last></author>
      <author><first>Lorena</first><last>S. Allegue</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <pages>15093–15104</pages>
      <abstract>We present the latest version of the Spanish Resource Grammar (SRG), a grammar of Spanish implemented in the HPSG formalism. Such grammars encode a complex set of hypotheses about syntax making them a resource for empirical testing of linguistic theory. They also encode a strict notion of grammaticality which makes them a resource for natural language processing applications in computer-assisted language learning. This version of the SRG uses the recent version of the Freeling morphological analyzer and is released along with an automatically created, manually verified treebank of 2,291 sentences. We explain the treebanking process, emphasizing how it is different from treebanking with manual annotation and how it contributes to empirically-driven development of syntactic theory. The treebanks’ high level of consistency and detail makes them a resource for training high-quality semantic parsers and generally systems that benefit from precise and detailed semantics. Finally, we present the grammar’s coverage and overgeneration on 100 sentences from a learner corpus, a new research line related to developing methodologies for robust empirical evaluation of hypotheses in second language acquisition.</abstract>
      <url hash="304ba336">2024.lrec-main.1312</url>
      <bibkey>zamaraeva-etal-2024-spanish-resource</bibkey>
    </paper>
    <paper id="1313">
      <title>Spanless Event Annotation for Corpus-Wide Complex Event Understanding</title>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Jennifer</first><last>Tracey</last></author>
      <author><first>Ann</first><last>O’Brien</last></author>
      <author><first>Song</first><last>Chen</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <pages>15105–15113</pages>
      <abstract>We present a new approach to event annotation designed to promote whole-corpus understanding of complex events in multilingual, multimedia data as part of the DARPA Knowledge-directed Artificial Intelligence Reasoning Over Schemas (KAIROS) Program. KAIROS aims to build technology capable of reasoning about complex real-world events like a specific terrorist attack in order to provide actionable insights to end users. KAIROS systems extract events from a corpus, aggregate information into a coherent semantic representation, and instantiate observed events or predict unseen but expected events using a relevant event schema selected from a generalized schema library. To support development and testing for KAIROS Phase 2B we created a complex event annotation corpus that, instead of individual event mentions anchored in document spans with pre-defined event type labels, comprises a series of temporally ordered event frames populated with information aggregated from the whole corpus and labeled with an unconstrained tag set based on Wikidata Qnodes. The corpus makes a unique contribution to the resource landscape for information extraction, addressing gaps in the availability of multilingual, multimedia corpora for schema-based event representation. The corpus will be made available through publication in the Linguistic Data Consortium (LDC) catalog.</abstract>
      <url hash="fb930255">2024.lrec-main.1313</url>
      <bibkey>bies-etal-2024-spanless-event</bibkey>
    </paper>
    <paper id="1314">
      <title>Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks</title>
      <author><first>Santiago</first><last>Herrera</last></author>
      <author><first>Caio</first><last>Corro</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>15114–15125</pages>
      <abstract>Descriptive grammars are highly valuable, but writing them is time-consuming and difficult. Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data. As for formal grammars, they can be challenging to interpret. In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar. More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules. For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study. We associate statistical information to each rule, and we compare the ranking of the model’s results to those of other quantitative and statistical measures. Our method captures both well-known and less well-known significant grammar rules in Spanish, French, and Wolof.</abstract>
      <url hash="aa610f51">2024.lrec-main.1314</url>
      <bibkey>herrera-etal-2024-sparse-logistic</bibkey>
    </paper>
    <paper id="1315">
      <title>Specifying Genericity through Inclusiveness and Abstractness Continuous Scales</title>
      <author><first>Claudia</first><last>Collacciani</last></author>
      <author><first>Andrea Amelio</first><last>Ravelli</last></author>
      <author><first>Marianna</first><last>Bolognesi</last></author>
      <pages>15126–15136</pages>
      <abstract>This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases’ (NPs) genericity in natural language. The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks. Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory. Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research. To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework’s effectiveness in capturing nuanced aspects of genericity. Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on the semantics of genericity, and NLP practitioners, contributing to the development of commonsense knowledge repositories valuable in enhancing various NLP applications.</abstract>
      <url hash="84bc41bc">2024.lrec-main.1315</url>
      <bibkey>collacciani-etal-2024-specifying-genericity</bibkey>
    </paper>
    <paper id="1316">
      <title><fixed-case>S</fixed-case>peech<fixed-case>A</fixed-case>lign: A Framework for Speech Translation Alignment Evaluation</title>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Aleix</first><last>Sant</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>David</first><last>Dale</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>15137–15146</pages>
      <abstract>Speech-to-Speech and Speech-to-Text translation are currently dynamic areas of research. In our commitment to advance these fields, we present SpeechAlign, a framework designed to evaluate the underexplored field of source-target alignment in speech models. The SpeechAlign framework has two core components. First, to tackle the absence of suitable evaluation datasets, we introduce the Speech Gold Alignment dataset, built upon a English-German text translation gold alignment dataset. Secondly, we introduce two novel metrics, Speech Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate (TW-SAER), which enable the evaluation of alignment quality within speech models. While the former gives equal importance to each word, the latter assigns weights based on the length of the words in the speech signal. By publishing SpeechAlign we provide an accessible evaluation framework for model assessment, and we employ it to benchmark open-source Speech Translation models. In doing so, we contribute to the ongoing research progress within the fields of Speech-to-Speech and Speech-to-Text translation.</abstract>
      <url hash="b833a9a6">2024.lrec-main.1316</url>
      <bibkey>alastruey-etal-2024-speechalign-framework</bibkey>
    </paper>
    <paper id="1317">
      <title>Speech Analysis of Language Varieties in <fixed-case>I</fixed-case>taly</title>
      <author><first>Moreno</first><last>La Quatra</last></author>
      <author><first>Alkis</first><last>Koudounas</last></author>
      <author><first>Elena</first><last>Baralis</last></author>
      <author><first>Sabato Marco</first><last>Siniscalchi</last></author>
      <pages>15147–15159</pages>
      <abstract>Italy exhibits rich linguistic diversity across its territory due to the distinct regional languages spoken in different areas. Recent advances in self-supervised learning provide new opportunities to analyze Italy’s linguistic varieties using speech data alone. This includes the potential to leverage representations learned from large amounts of data to better examine nuances between closely related linguistic varieties. In this study, we focus on automatically identifying the geographic region of origin of speech samples drawn from Italy’s diverse language varieties. We leverage self-supervised learning models to tackle this task and analyze differences and similarities between Italy’s regional languages. In doing so, we also seek to uncover new insights into the relationships among these diverse yet closely related varieties, which may help linguists understand their interconnected evolution and regional development over time and space. To improve the discriminative ability of learned representations, we evaluate several supervised contrastive learning objectives, both as pre-training steps and additional fine-tuning objectives. Experimental evidence shows that pre-trained self-supervised models can effectively identify regions from speech recording. Additionally, incorporating contrastive objectives during fine-tuning improves classification accuracy and yields embeddings that distinctly separate regional varieties, demonstrating the value of combining self-supervised pre-training and contrastive learning for this task.</abstract>
      <url hash="ec4f20e9">2024.lrec-main.1317</url>
      <bibkey>la-quatra-etal-2024-speech-analysis</bibkey>
    </paper>
    <paper id="1318">
      <title>Speech Corpus for <fixed-case>K</fixed-case>orean Children with Autism Spectrum Disorder: Towards Automatic Assessment Systems</title>
      <author><first>Seonwoo</first><last>Lee</last></author>
      <author><first>Jihyun</first><last>Mun</last></author>
      <author><first>Sunhee</first><last>Kim</last></author>
      <author><first>Minhwa</first><last>Chung</last></author>
      <pages>15160–15170</pages>
      <abstract>Despite the growing demand for digital therapeutics for children with Autism Spectrum Disorder (ASD), there is currently no speech corpus available for Korean children with ASD. This paper introduces a speech corpus specifically designed for Korean children with ASD, aiming to advance speech technologies such as pronunciation and severity evaluation. Speech recordings from speech and language evaluation sessions were transcribed, and annotated for articulatory and linguistic characteristics. Three speech and language pathologists rated these recordings for social communication severity (SCS) and pronunciation proficiency (PP) using a 3-point Likert scale. The total number of participants will be 300 for children with ASD and 50 for typically developing (TD) children. The paper also analyzes acoustic and linguistic features extracted from speech data collected and completed for annotation from 73 children with ASD and 9 TD children to investigate the characteristics of children with ASD and identify significant features that correlate with the clinical scores. The results reveal some speech and linguistic characteristics in children with ASD that differ from those in TD children or another subgroup of ASD categorized by clinical scores, demonstrating the potential for developing automatic assessment systems for SCS and PP.</abstract>
      <url hash="64cd4520">2024.lrec-main.1318</url>
      <bibkey>lee-etal-2024-speech-corpus</bibkey>
    </paper>
    <paper id="1319">
      <title>Speech Recognition Corpus of the Khinalug Language for Documenting Endangered Languages</title>
      <author><first>Zhaolin</first><last>Li</last></author>
      <author><first>Monika</first><last>Rind-Pawlowski</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>15171–15180</pages>
      <abstract>Automatic Speech Recognition (ASR) can be a valuable tool to document endangered languages. However, building ASR tools for these languages poses several difficult research challenges, notably data scarcity. In this paper, we show the whole process of creating a useful ASR tool for language documentation scenarios. We publish the first speech corpus for Khinalug, an endangered language spoken in Northern Azerbaijan. The corpus consists of 2.67 hours of labeled data from recordings of spontaneous speech about various topics. As Khinalug is an extremely low-resource language, we investigate the benefits of multilingual models for self-supervised learning and supervised learning and achieve the performance of 6.65 Character Error Rate (CER) points and 25.53 Word Error Rate (WER) points. The benefits of multilingual models are further validated through experimentation with three additional under-resourced languages. Lastly, this work conducts quality assessments with linguists on new recordings to investigate the model’s usefulness in language documentation. We observe an evident degradation for new recordings, indicating the importance of enhancing model robustness. In addition, we find the inaudible content is the main cause of wrong ASR predictions, suggesting relating work on incorporating contextual information.</abstract>
      <url hash="db8078a3">2024.lrec-main.1319</url>
      <bibkey>li-etal-2024-speech-recognition</bibkey>
    </paper>
    <paper id="1320">
      <title><fixed-case>SPICED</fixed-case>: News Similarity Detection Dataset with Multiple Topics and Complexity Levels</title>
      <author><first>Elena</first><last>Shushkevich</last></author>
      <author><first>Long Thanh</first><last>Mai</last></author>
      <author><first>Manuel V.</first><last>Loureiro</last></author>
      <author><first>Steven</first><last>Derby</last></author>
      <author><first>Tri Kurniawan</first><last>Wijaya</last></author>
      <pages>15181–15190</pages>
      <abstract>The proliferation of news media outlets has increased the demand for intelligent systems capable of detecting redundant information in news articles in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a novel dataset of similar news, SPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment, Disasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp; Technology, and Sports. Futhermore, we present four different levels of complexity, specifically designed for news similarity detection task. We benchmarked the created datasets using MinHash, BERT, SBERT, and SimCSE models.</abstract>
      <url hash="998f5d08">2024.lrec-main.1320</url>
      <bibkey>shushkevich-etal-2024-spiced-news</bibkey>
    </paper>
    <paper id="1321">
      <title><fixed-case>SPLICE</fixed-case>: A Singleton-Enhanced <fixed-case>P</fixed-case>ipe<fixed-case>LI</fixed-case>ne for Coreference <fixed-case>RE</fixed-case>solution</title>
      <author><first>Yilun</first><last>Zhu</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Sameer</first><last>Pradhan</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <pages>15191–15201</pages>
      <abstract>Singleton mentions, i.e. entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective. However previous attempts to incorporate their detection in end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention spans in the OntoNotes benchmark. This paper addresses this limitation by combining predicted mentions from existing nested NER systems and features derived from OntoNotes syntax trees. With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons. We then propose a two-step neural mention and coreference resolution system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed singleton training yields results comparable to end-to-end systems for OntoNotes, while improving OOD stability (+1.1 avg. F1). We conduct error analysis for mention detection and delve into its impact on coreference clustering, revealing that precision improvements deliver more substantial benefits than increases in recall for resolving coreference chains.</abstract>
      <url hash="35a057e2">2024.lrec-main.1321</url>
      <bibkey>zhu-etal-2024-splice-singleton</bibkey>
    </paper>
    <paper id="1322">
      <title><fixed-case>SPOTTER</fixed-case>: A Framework for Investigating Convention Formation in a Visually Grounded Human-Robot Reference Task</title>
      <author><first>Jaap</first><last>Kruijt</last></author>
      <author><first>Peggy</first><last>van Minkelen</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Piek T.J.M.</first><last>Vossen</last></author>
      <author><first>Elly</first><last>Konijn</last></author>
      <author><first>Thomas</first><last>Baier</last></author>
      <pages>15202–15215</pages>
      <abstract>Linguistic conventions that arise in dialogue reflect common ground and can increase communicative efficiency. Social robots that can understand these conventions and the process by which they arise have the potential to become efficient communication partners. Nevertheless, it is unclear how robots can engage in convention formation when presented with both familiar and new information. We introduce an adaptable game platform, SPOTTER, to study the dynamics of convention formation for visually grounded referring expressions in both human-human and human-robot interaction. Specifically, we seek to elicit convention forming for members of an inner circle of well-known individuals in the common ground, as opposed to individuals from an outer circle, who are unfamiliar. We release an initial corpus of 5000 utterances from two exploratory pilot experiments in Dutch. Different from previous work focussing on human-human interaction, we find that referring expressions for both familiar and unfamiliar individuals maintain their length throughout human-robot interaction. Stable conventions are formed, although these conventions can be impacted by distracting outer circle individuals. With our distinction between familiar and unfamiliar, we create a contrastive operationalization of common ground, which aids research into convention formation.</abstract>
      <url hash="9339f95e">2024.lrec-main.1322</url>
      <bibkey>kruijt-etal-2024-spotter-framework</bibkey>
    </paper>
    <paper id="1323">
      <title><fixed-case>S</fixed-case>pread<fixed-case>N</fixed-case>a<fixed-case>L</fixed-case>a: A Naturalistic Code Generation Evaluation Dataset of Spreadsheet Formulas</title>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <author><first>Ayesha</first><last>Ansar</last></author>
      <author><first>Om</first><last>Agarwal</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>15216–15225</pages>
      <abstract>Automatic generation of code from natural language descriptions has emerged as one of the main use cases of large language models (LLMs). This has also led to a proliferation of datasets to track progress in the reliability of code generation models, including domains such as programming challenges and common data science tasks. However, existing datasets primarily target the use of code generation models to aid expert programmers in writing code. In this work, we consider a domain of code generation which is more frequently used by users without sophisticated programming skills: translating English descriptions to spreadsheet formulas that can be used to do everyday data processing tasks. We extract naturalistic instructions from StackOverflow posts and manually verify and standardize the corresponding spreadsheet formulas. We use this dataset to evaluate an off-the-shelf code generation model (GPT 3.5 text-davinci-003) as well as recently proposed pragmatic code generation procedures and find that Code Reviewer reranking (Zhang et al., 2022) performs best among the evaluated methods but still frequently generates formulas that differ from human-generated ones.</abstract>
      <url hash="1e9507e2">2024.lrec-main.1323</url>
      <bibkey>schuster-etal-2024-spreadnala-naturalistic</bibkey>
    </paper>
    <paper id="1324">
      <title><fixed-case>STAF</fixed-case>: Pushing the Boundaries of Test-Time Adaptation towards Practical Noise Scenarios</title>
      <author><first>Haoyu</first><last>Xiong</last></author>
      <author><first>Xinchun</first><last>Zhang</last></author>
      <author><first>Leixin</first><last>Yang</last></author>
      <author><first>Yu</first><last>Xiang</last></author>
      <author><first>Gang</first><last>Fang</last></author>
      <pages>15226–15237</pages>
      <abstract>Test-time adaptation (TTA) aims to adapt the neural network to the distribution of the target domain using only unlabeled test data. Most previous TTA methods have achieved success under mild conditions, such as considering only a single or multiple independent static domains. However, in real-world settings, the test data is sampled in a correlated manner and the test environments undergo continual changes over time, which may cause previous TTA methods to fail in practical noise scenarios, i.e., independent noise distribution shifts, continual noise distribution shifts, and continual mixed distribution shifts. To address these issues, we elaborate a Stable Test-time Adaptation Framework, called STAF, to stabilize the adaptation process. Specifically, to boost model robustness to noise distribution shifts, we present a multi-stream perturbation consistency method, enabling weak-to-strong views to be consistent, guided by the weak view from the original sample. Meanwhile, we develop a reliable memory-based corrector which utilizes reliable snapshots between the anchor model and the adapt model to correct prediction bias. Furthermore, we propose a dynamic parameter restoration strategy to alleviate error accumulation and catastrophic forgetting that takes into account both the distribution shift and sample adaptation degree. Extensive experiments demonstrate the robustness and effectiveness of STAF, which pushes the boundaries of test-time adaptation to more realistic scenarios and paves the way for stable deployment of real-world applications.</abstract>
      <url hash="8606b5e1">2024.lrec-main.1324</url>
      <attachment type="OptionalSupplementaryMaterial" hash="93cc041f">2024.lrec-main.1324.OptionalSupplementaryMaterial.txt</attachment>
      <bibkey>xiong-etal-2024-staf-pushing</bibkey>
    </paper>
    <paper id="1325">
      <title><fixed-case>STAGE</fixed-case>: Simple Text Data Augmentation by Graph Exploration</title>
      <author><first>Ho-Seung</first><last>Kim</last></author>
      <author><first>YongHoon</first><last>Kang</last></author>
      <author><first>Jee-Hyong</first><last>Lee</last></author>
      <pages>15238–15256</pages>
      <abstract>Pre-trained language models (PLMs) are widely used for various tasks, but fine-tuning them requires sufficient data. Data augmentation approaches have been proposed as alternatives, but they vary in complexity, cost, and performance. To address these challenges, we propose STAGE (Simple Text Data Augmentation by Graph Exploration), a highly effective method for data augmentation. STAGE utilizes simple modification operations such as insertion, deletion, replacement, and swap. However, what distinguishes STAGE lies in the selection of optimal words for each modification. This is achieved by leveraging a word-relation graph called the co-graph. The co-graph takes into account both word frequency and co-occurrence, providing valuable information for operand selection. To assess the performance of STAGE, we conduct evaluations using seven representative datasets and three different PLMs. Our results demonstrate the effectiveness of STAGE across diverse data domains, varying data sizes, and different PLMs. Also, STAGE demonstrates superior performance when compared to previous methods that use simple modification operations or large language models like GPT3.</abstract>
      <url hash="a2d2a34f">2024.lrec-main.1325</url>
      <bibkey>kim-etal-2024-stage-simple</bibkey>
    </paper>
    <paper id="1326">
      <title>Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning</title>
      <author><first>Maksym</first><last>Taranukhin</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Evangelos</first><last>Milios</last></author>
      <pages>15257–15272</pages>
      <abstract>Social media platforms are rich sources of opinionated content. Stance detection allows the automatic extraction of users’ opinions on various topics from such content. We focus on zero-shot stance detection, where the model’s success relies on (a) having knowledge about the target topic; and (b) learning general reasoning strategies that can be employed for new topics. We present Stance Reasoner, an approach to zero-shot stance detection on social media that leverages explicit reasoning over background knowledge to guide the model’s inference about the document’s stance on a target. Specifically, our method uses a pre-trained language model as a source of world knowledge, with the chain-of-thought in-context learning approach to generate intermediate reasoning steps. Stance Reasoner outperforms the current state-of-the-art models on 3 Twitter datasets, including fully supervised models. It can better generalize across targets, while at the same time providing explicit and interpretable explanations for its predictions.</abstract>
      <url hash="2b130b8c">2024.lrec-main.1326</url>
      <bibkey>taranukhin-etal-2024-stance-reasoner</bibkey>
    </paper>
    <paper id="1327">
      <title><fixed-case>STE</fixed-case>nt<fixed-case>C</fixed-case>onv: Predicting Disagreement between <fixed-case>R</fixed-case>eddit Users with Stance Detection and a Signed Graph Convolutional Network</title>
      <author><first>Isabelle</first><last>Lorge</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Xiaowen</first><last>Dong</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <pages>15273–15284</pages>
      <abstract>The rise of social media platforms has led to an increase in polarised online discussions, especially on political and socio-cultural topics such as elections and climate change. We propose a simple and entirely novel unsupervised method to better predict whether the authors of two posts agree or disagree, leveraging user stances about named entities obtained from their posts. We present STEntConv, a model which builds a graph of users and named entities weighted by stance and trains a Signed Graph Convolutional Network (SGCN) to detect disagreement between comment and reply posts. We run experiments and ablation studies and show that including this information improves disagreement detection performance on a dataset of Reddit posts for a range of controversial subreddit topics, without the need for platform-specific features or user history</abstract>
      <url hash="bd0559fd">2024.lrec-main.1327</url>
      <bibkey>lorge-etal-2024-stentconv-predicting</bibkey>
    </paper>
    <paper id="1328">
      <title>Step-by-Step: Controlling Arbitrary Style in Text with Large Language Models</title>
      <author><first>Pusheng</first><last>Liu</last></author>
      <author><first>Lianwei</first><last>Wu</last></author>
      <author><first>Linyong</first><last>Wang</last></author>
      <author><first>Sensen</first><last>Guo</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>15285–15295</pages>
      <abstract>Recently, the autoregressive framework based on large language models (LLMs) has achieved excellent performance in controlling the generated text to adhere to the required style. These methods guide LLMs through prompt learning to generate target text in an autoregressive manner. However, this manner possesses lower controllability and suffers from the challenge of accumulating errors, where early prediction inaccuracies might influence subsequent word generation. Furthermore, existing prompt-based methods overlook specific region editing, resulting in a deficiency of localized control over input text. To overcome these challenges, we propose a novel three-stage prompt-based approach for specific region editing. To alleviate the issue of accumulating errors, we transform the text style transfer task into a text infilling task, guiding the LLMs to modify only a small portion of text within the editing region to achieve style transfer, thus reducing the number of autoregressive iterations. To achieve an effective specific editing region, we adopt both prompt-based and word frequency-based strategies for region selection, subsequently employing a discriminator to validate the efficacy of the selected region. Experiments conducted on several publicly competitive datasets for text style transfer task confirm that our proposed approach achieves state-of-the-art performance. Keywords: text style transfer, natural language generation, large language models</abstract>
      <url hash="8b6e81a8">2024.lrec-main.1328</url>
      <bibkey>liu-etal-2024-step-step</bibkey>
    </paper>
    <paper id="1329">
      <title>Step Feasibility-Aware and Error-Correctable Entailment Tree Generation</title>
      <author><first>Junyue</first><last>Song</last></author>
      <author><first>Xin</first><last>Wu</last></author>
      <author><first>Yi</first><last>Cai</last></author>
      <pages>15296–15308</pages>
      <abstract>An entailment tree is a structured reasoning path that clearly demonstrates the process of deriving hypotheses through multiple steps of inference from known premises. It enhances the interpretability of QA systems. Existing methods for generating entailment trees typically employ iterative frameworks to ensure reasoning faithfulness. However, they often suffer from the issue of false feasible steps, where selected steps appear feasible but actually lead to incorrect intermediate conclusions. Moreover, the existing iterative frameworks do not consider error-prone search branches, resulting in error propagation. In this work, we propose SPEH: an iterative entailment tree generation framework with Step feasibility Perception and state Error Handling mechanisms. Step Feasibility Perception enables the model to learn how to choose steps that are not false feasible. State Error Handling includes error detection and backtracking, allowing the model to correct errors when entering incorrect search branches. Experimental results demonstrate the effectiveness of our approach in improving the generation of entailment trees.</abstract>
      <url hash="cb39569a">2024.lrec-main.1329</url>
      <bibkey>song-etal-2024-step-feasibility</bibkey>
    </paper>
    <paper id="1330">
      <title>Still All <fixed-case>G</fixed-case>reeklish to Me: <fixed-case>G</fixed-case>reeklish to <fixed-case>G</fixed-case>reek Transliteration</title>
      <author><first>Anastasios</first><last>Toumazatos</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Stavros</first><last>Vassos</last></author>
      <pages>15309–15319</pages>
      <abstract>Modern Greek is normally written in the Greek alphabet. In informal online messages, however, Greek is often written using characters available on Latin-character keyboards, a form known as Greeklish. Originally used to bypass the lack of support for the Greek alphabet in older computers, Greeklish is now also used to avoid switching languages on multilingual keyboards, hide spelling mistakes, or as a form of slang. There is no consensus mapping, hence the same Greek word can be written in numerous different ways in Greeklish. Even native Greek speakers may struggle to understand (or be annoyed by) Greeklish, which requires paying careful attention to context to decipher. Greeklish may also be a problem for NLP models trained on Greek datasets written in the Greek alphabet. Experimenting with a range of statistical and deep learning models on both artificial and real-life Greeklish data, we find that: (i) prompting large language models (e.g., GPT-4) performs impressively well with few- or even zero-shot training, outperforming several fine-tuned encoder-decoder models; however (ii) a twenty years old statistical Greeklish transliteration model is still very competitive; and (iii) the problem is still far from having been solved; (iv) nevertheless, downstream Greek NLP systems that need to cope with Greeklish, such as moderation classifiers, can benefit significantly even with the current non-perfect transliteration systems. We make all our code, models, and data available and suggest future improvements, based on an analysis of our experimental results.</abstract>
      <url hash="7d1d7771">2024.lrec-main.1330</url>
      <bibkey>toumazatos-etal-2024-still-greeklish</bibkey>
    </paper>
    <paper id="1331">
      <title>Stories and Personal Experiences in the <fixed-case>COVID</fixed-case>-19 Discourse</title>
      <author><first>Neele</first><last>Falk</last></author>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <pages>15320–15340</pages>
      <abstract>Storytelling, i.e., the use of of anecdotes and personal experiences, plays a crucial role in everyday argumentation. This is particularly true for the highly controversial debates that spark in times of crisis - where the focus of the discussion is on heterogeneous aspects of everyday life. For individuals, stories can have a strong persuasive power; for a larger collective, stories can help decision-makers to develop strategies for addressing the challenges people are facing, especially in times of crisis. In this paper, we analyse the use of storytelling in the COVID-19 discourse. We carry out our analysis on three publicly available Reddit datasets, for a total of 367K comments. We automatically annotate the Reddit datasets by detecting spans containing storytelling and classifying them into: a) personal vs. general – is the story experienced by the speaker? b) argumentative function (Does the story clarify a problem, potentially consisting in harm to a specific group? Does it exemplify a solution to a problem, or does it establish the credibility of the speaker?), and c) topic. We then carry out an analysis which establishes the relevance of storytelling in the COVID discourse and further uncovers interactions between topics and types of stories associated to them.</abstract>
      <url hash="58faf69b">2024.lrec-main.1331</url>
      <bibkey>falk-lapesa-2024-stories-personal</bibkey>
    </paper>
    <paper id="1332">
      <title>Strengthening the <fixed-case>W</fixed-case>i<fixed-case>C</fixed-case>: New Polysemy Dataset in <fixed-case>H</fixed-case>indi and Lack of Cross Lingual Transfer</title>
      <author><first>Haim</first><last>Dubossarsky</last></author>
      <author><first>Farheen</first><last>Dairkee</last></author>
      <pages>15341–15349</pages>
      <abstract>This study addresses the critical issue of Natural Language Processing in low-resource languages such as Hindi, which, despite having substantial number of speakers, is limited in linguistic resources. The paper focuses on Word Sense Disambiguation, a fundamental NLP task that deals with polysemous words. It introduces a novel Hindi WSD dataset in the modern WiC format, enabling the training and testing of contextualized models. The primary contributions of this work lie in testing the efficacy of multilingual models to transfer across languages and hence to handle polysemy in low-resource languages, and in providing insights into the minimum training data required for a viable solution. Experiments compare different contextualized models on the WiC task via transfer learning from English to Hindi. Models purely transferred from English yield poor 55% accuracy, while fine-tuning on Hindi dramatically improves performance to 90% accuracy. This demonstrates the need for language-specific tuning and resources like the introduced Hindi WiC dataset to drive advances in Hindi NLP. The findings offer valuable insights into addressing the NLP needs of widely spoken yet low-resourced languages, shedding light on the problem of transfer learning in these contexts.</abstract>
      <url hash="d98f7617">2024.lrec-main.1332</url>
      <bibkey>dubossarsky-dairkee-2024-strengthening-wic</bibkey>
    </paper>
    <paper id="1333">
      <title><fixed-case>S</fixed-case>truct<fixed-case>AM</fixed-case>: Enhancing Address Matching through Semantic Understanding of Structure-aware Information</title>
      <author><first>Zhaoqi</first><last>Zhang</last></author>
      <author><first>Pasquale</first><last>Balsebre</last></author>
      <author><first>Siqiang</first><last>Luo</last></author>
      <author><first>Zhen</first><last>Hai</last></author>
      <author><first>Jiangping</first><last>Huang</last></author>
      <pages>15350–15361</pages>
      <abstract>The task of address matching involves linking unstructured addresses to standard ones in a database. The challenges presented by this task are manifold: misspellings, incomplete information, and variations in address content are some examples. While there have been previous studies on entity matching in natural language processing, for the address matching solution, existing approaches still rely on string-based similarity matching or manually-designed rules. In this paper, we propose StructAM, a novel method based on pre-trained language models (LMs) and graph neural networks to extract the textual and structured information of the addresses. The proposed method leverages the knowledge acquired by large language models during the pre-training phase, and refines it during the fine-tuning process on the address domain, to obtain address-specific semantic features. Meanwhile, it also applies an attribute attention mechanism based on Graph Sampling and Aggregation (GraphSAGE) module to capture internal hierarchy information of the address text. To further enhance the accuracy of our algorithm in dirty settings, we incorporate spatial coordinates and contextual information from the surrounding area as auxiliary guidance. We conduct extensive experiments on real-world datasets from four different countries and the results show that StructAM outperforms state-of-the-art baseline approaches for address matching.</abstract>
      <url hash="c750dfa6">2024.lrec-main.1333</url>
      <bibkey>zhang-etal-2024-structam-enhancing</bibkey>
    </paper>
    <paper id="1334">
      <title>Structure-aware Fine-tuning for Code Pre-trained Models</title>
      <author><first>Jiayi</first><last>Wu</last></author>
      <author><first>Renyu</first><last>Zhu</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Qiushi</first><last>Sun</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>15362–15372</pages>
      <abstract>Over the past few years, we have witnessed remarkable advancements in Code Pre-trained Models (CodePTMs). These models achieved excellent representation capabilities by designing structure-based pre-training tasks for code. However, how to enhance the absorption of structural knowledge when fine-tuning CodePTMs still remains a significant challenge. To fill this gap, in this paper, we present SAT, a novel structure-enhanced and plug-and-play fine-tuning method for CodePTMs. We first propose a structure loss to quantify the difference between the information learned by CodePTMs and the knowledge extracted from code structure. Specifically, we use the attention scores from Transformer layer as the learned information, and the shortest path length between leaves in abstract syntax trees as the structural knowledge. Subsequently, multi-task learning is introduced to improve the performance of fine-tuning. Experiments conducted on four pre-trained models and two generation tasks demonstrate the effectiveness of our proposed method as a plug-and-play solution. Furthermore, we observed that SAT can benefit CodePTMs more with limited training data.</abstract>
      <url hash="37705da0">2024.lrec-main.1334</url>
      <bibkey>wu-etal-2024-structure-aware</bibkey>
    </paper>
    <paper id="1335">
      <title>Structure-aware Generation Model for Cross-Domain Aspect-based Sentiment Classification</title>
      <author><first>Shichen</first><last>Li</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <author><first>Yanzhi</first><last>Xu</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>15373–15383</pages>
      <abstract>Employing pre-trained generation models for cross-domain aspect-based sentiment classification has recently led to large improvements. However, they ignore the importance of syntactic structures, which have shown appealing effectiveness in classification based models. Different from previous studies, efficiently encoding the syntactic structure in generation model is challenging because such models are pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this study, we propose a novel structure-aware generation model to tackle this challenge. In particular, a prompt-driven strategy is designed to bridge the gap between different domains, by capturing implicit syntactic information from the input and output sides. Furthermore, the syntactic structure is explicitly encoded into the structure-aware generation model, which can effectively learn domain-irrelevant features based on syntactic pivot features. Empirical results demonstrate the effectiveness of the proposed structure-aware generation model over several strong baselines. The results also indicate the proposed model is capable of leveraging the input syntactic structure into the generation model.</abstract>
      <url hash="019c87f6">2024.lrec-main.1335</url>
      <bibkey>li-etal-2024-structure-aware</bibkey>
    </paper>
    <paper id="1336">
      <title><fixed-case>S</fixed-case>tyle<fixed-case>F</fixed-case>low: Disentangle Latent Representations via Normalizing Flow for Unsupervised Text Style Transfer</title>
      <author><first>Kangchen</first><last>Zhu</last></author>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Jingyu</first><last>Wei</last></author>
      <author><first>Ruifeng</first><last>Luo</last></author>
      <author><first>Yiping</first><last>Song</last></author>
      <author><first>Xiaoguang</first><last>Mao</last></author>
      <pages>15384–15397</pages>
      <abstract>Unsupervised text style transfer aims to modify the style of a sentence while preserving its content without parallel corpora. Existing approaches attempt to separate content from style, but some words contain both content and style information. It makes them difficult to disentangle, where unsatisfactory disentanglement results in the loss of the content information or the target style. To address this issue, researchers adopted a “cycle reconstruction” mechanism to maintain content information, but it is still hard to achieve satisfactory content preservation due to incomplete disentanglement. In this paper, we propose a new disentanglement-based method, StyleFlow, which effectively avoids the loss of contents through a better cycle reconstruction via a reversible encoder. The reversible encoder is a normalizing flow that can not only produce output given input but also infer the exact input given the output reversely. We design a stack of attention-aware coupling layers, where each layer is reversible and adopts the attention mechanism to improve the content-style disentanglement. Moreover, we propose a data augmentation method based on normalizing flow to enhance the training data. Our experiments on sentiment transfer and formality transfer tasks show that StyleFlow outperforms strong baselines on both content preservation and style transfer.</abstract>
      <url hash="e2fc6391">2024.lrec-main.1336</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0566cc32">2024.lrec-main.1336.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>zhu-etal-2024-styleflow-disentangle</bibkey>
    </paper>
    <paper id="1337">
      <title>Submodular-based In-context Example Selection for <fixed-case>LLM</fixed-case>s-based Machine Translation</title>
      <author><first>Baijun</first><last>Ji</last></author>
      <author><first>Xiangyu</first><last>Duan</last></author>
      <author><first>Zhenyu</first><last>Qiu</last></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>15398–15409</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive performances across various NLP tasks with just a few prompts via in-context learning. Previous studies have emphasized the pivotal role of well-chosen examples in in-context learning, as opposed to randomly selected instances that exhibits unstable results.A successful example selection scheme depends on multiple factors, while in the context of LLMs-based machine translation, the common selection algorithms only consider the single factor, i.e., the similarity between the example source sentence and the input sentence.In this paper, we introduce a novel approach to use multiple translational factors for in-context example selection by using monotone submodular function maximization.The factors include surface/semantic similarity between examples and inputs on both source and target sides, as well as the diversity within examples.Importantly, our framework mathematically guarantees the coordination between these factors, which are different and challenging to reconcile.Additionally, our research uncovers a previously unexamined dimension: unlike other NLP tasks, the translation part of an example is also crucial, a facet disregarded in prior studies.Experiments conducted on BLOOMZ-7.1B and LLAMA2-13B, demonstrate that our approach significantly outperforms random selection and robust single-factor baselines across various machine translation tasks.</abstract>
      <url hash="32dd9323">2024.lrec-main.1337</url>
      <bibkey>ji-etal-2024-submodular-based</bibkey>
    </paper>
    <paper id="1338">
      <title>Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals</title>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Yuhao</first><last>Zhou</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>15410–15421</pages>
      <abstract>Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is inevitable in subspace learning, we propose an independence criterion to disentangle clean signals from perturbations. Experimental results show that the proposed strategy enables the model to inherently suppress adversaries, which not only boosts model robustness but also motivates new directions of effective adversarial defense.</abstract>
      <url hash="3562dd69">2024.lrec-main.1338</url>
      <bibkey>zheng-etal-2024-subspace-defense</bibkey>
    </paper>
    <paper id="1339">
      <title>Sub-Table Rescorer for Table Question Answering</title>
      <author><first>Atsushi</first><last>Kojima</last></author>
      <pages>15422–15427</pages>
      <abstract>We propose a sub-table rescorer (STR) to improve the performance of an inner table retriever (ITR)-based inference for the table question answering. Tabular language model (TLM) truncates the sequence of a long table due to their input token limits. It leads to accuracy degradation. To solve this problem, ITR extracts sub-table candidates, which correspond to a part of an entire greater original table on the basis of relevance scores to the question for each of the columns and rows. Then, the topN longest sub-tables are selected. Our proposed STR estimates the relevance score between a question and each sub-table. In this work, we explored two different methods to integrate STR to the ITR-based inference. In the first method, STR rescores sub-table candidates, and the topN sub-tables are chosen. Then, TLM outputs the most confident answer. In the second method, the score calculated by STR is interpolated with the score calculated by TLM. Then, the most confident answer is chosen. In the experiment, we evaluate the performance on the WikiTableQuestions dataset. By applying STR to the ITR-based inference, we observed 4.4% and 6.3% relative reductions in error rate in the rescoring- and score-fusion-based methods, respectively.</abstract>
      <url hash="1c126168">2024.lrec-main.1339</url>
      <bibkey>kojima-2024-sub-table</bibkey>
    </paper>
    <paper id="1340">
      <title><fixed-case>SUK</fixed-case> 1.0: A New Training Corpus for Linguistic Annotation of Modern Standard <fixed-case>S</fixed-case>lovene</title>
      <author><first>Špela</first><last>Arhar Holdt</last></author>
      <author><first>Jaka</first><last>Čibej</last></author>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Polona</first><last>Gantar</last></author>
      <author><first>Simon</first><last>Krek</last></author>
      <author><first>Tina</first><last>Munda</last></author>
      <author><first>Nejc</first><last>Robida</last></author>
      <author><first>Luka</first><last>Terčon</last></author>
      <author><first>Slavko</first><last>Zitnik</last></author>
      <pages>15428–15435</pages>
      <abstract>This paper introduces the upgrade of a training corpus for linguistic annotation of modern standard Slovene. The enhancement spans both the size of the corpus and the depth of annotation layers. The revised SUK 1.0 corpus, building on its predecessor ssj500k 2.3, has doubled in size, containing over a million tokens. This expansion integrates three preexisting open-access datasets, all of which have undergone automatic tagging and meticulous manual review across multiple annotation layers, each represented in varying proportions. These layers span tokenization, segmentation, lemmatization, MULTEXT-East morphology, Universal Dependencies, JOS-SYN syntax, semantic role labeling, named entity recognition, and the newly incorporated coreferences. The paper illustrates the annotation processes for each layer while also presenting the results of the new CLASSLA-Stanza annotation tool, trained on the SUK corpus data. As one of the fundamental language resources of modern Slovene, the SUK corpus calls for constant development, as outlined in the concluding section.</abstract>
      <url hash="80b40b67">2024.lrec-main.1340</url>
      <bibkey>arhar-holdt-etal-2024-suk-1</bibkey>
    </paper>
    <paper id="1341">
      <title><fixed-case>S</fixed-case>uper<fixed-case>ST</fixed-case>: Superficial Self-Training for Few-Shot Text Classification</title>
      <author><first>Ju-Hyoung</first><last>Lee</last></author>
      <author><first>Joonghyuk</first><last>Hahn</last></author>
      <author><first>Hyeon-Tae</first><last>Seo</last></author>
      <author><first>Jiho</first><last>Park</last></author>
      <author><first>Yo-Sub</first><last>Han</last></author>
      <pages>15436–15447</pages>
      <abstract>In few-shot text classification, self-training is a popular tool in semi-supervised learning (SSL). It relies on pseudo-labels to expand data, which has demonstrated success. However, these pseudo-labels contain potential noise and provoke a risk of underfitting the decision boundary. While the pseudo-labeled data can indeed be noisy, fully acquiring this flawed data can result in the accumulation of further noise and eventually impacting the model performance. Consequently, self-training presents a challenge: mitigating the accumulation of noise in the pseudo-labels. Confronting this challenge, we introduce superficial learning, inspired by pedagogy’s focus on essential knowledge. Superficial learning in pedagogy is a learning scheme that only learns the material ‘at some extent’, not fully understanding the material. This approach is usually avoided in education but counter-intuitively in our context, we employ superficial learning to acquire only the necessary context from noisy data, effectively avoiding the noise. This concept serves as the foundation for SuperST, our self-training framework. SuperST applies superficial learning to the noisy data and fine-tuning to the less noisy data, creating an efficient learning cycle that prevents overfitting to the noise and spans the decision boundary effectively. Notably, SuperST improves the classifier accuracy for few-shot text classification by 18.5% at most and 8% in average, compared with the state-of-the-art SSL baselines. We substantiate our claim through empirical experiments and decision boundary analysis.</abstract>
      <url hash="39be7018">2024.lrec-main.1341</url>
      <attachment type="OptionalSupplementaryMaterial" hash="6223d052">2024.lrec-main.1341.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lee-etal-2024-superst-superficial</bibkey>
    </paper>
    <paper id="1342">
      <title><fixed-case>S</fixed-case>wiss<fixed-case>SL</fixed-case>i: The Multi-parallel Sign Language Corpus for <fixed-case>S</fixed-case>witzerland</title>
      <author><first>Zifan</first><last>Jiang</last></author>
      <author><first>Anne</first><last>Göhring</last></author>
      <author><first>Amit</first><last>Moryossef</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Sarah</first><last>Ebling</last></author>
      <pages>15448–15456</pages>
      <abstract>In this work, we introduce SwissSLi, the first sign language corpus that contains parallel data of all three Swiss sign languages, namely Swiss German Sign Language (DSGS), French Sign Language of Switzerland (LSF-CH), and Italian Sign Language of Switzerland (LIS-CH). The data underlying this corpus originates from television programs in three spoken languages: German, French, and Italian. The programs have for the most part been translated into sign language by deaf translators, resulting in a unique, up to six-way multi-parallel dataset between spoken and sign languages. We describe and release the sign language videos and spoken language subtitles as well as the overall statistics and some derivatives of the raw material. These derived components include cropped videos, pose estimation, phrase/sign-segmented videos, and sentence-segmented subtitles, all of which facilitate downstream tasks such as sign language transcription (glossing) and machine translation. The corpus is publicly available on the SWISSUbase data platform for research purposes only under a CC BY-NC-SA 4.0 license.</abstract>
      <url hash="a1041a22">2024.lrec-main.1342</url>
      <bibkey>jiang-etal-2024-swisssli-multi</bibkey>
    </paper>
    <paper id="1343">
      <title>Synergetic Interaction Network with Cross-task Attention for Joint Relational Triple Extraction</title>
      <author><first>Da</first><last>Luo</last></author>
      <author><first>Run</first><last>Lin</last></author>
      <author><first>Qiao</first><last>Liu</last></author>
      <author><first>Yuxiang</first><last>Cai</last></author>
      <author><first>Xueyi</first><last>Liu</last></author>
      <author><first>Yanglei</first><last>Gan</last></author>
      <author><first>Rui</first><last>Hou</last></author>
      <pages>15457–15468</pages>
      <abstract>Joint entity-relation extraction remains a challenging task in information retrieval, given the intrinsic difficulty in modelling the interdependence between named entity recognition (NER) and relation extraction (RE) sub-tasks. Most existing joint extraction models encode entity and relation features in a sequential or parallel manner, allowing for limited one-way interaction. However, it is not yet clear how to capture the interdependence between these two sub-tasks in a synergistic and mutually reinforcing fashion. With this in mind, we propose a novel approach for joint entity-relation extraction, named Synergetic Interaction Network (SINET) which utilizes a cross-task attention mechanism to effectively leverage contextual associations between NER and RE. Specifically, we construct two sets of distinct token representations for NER and RE sub-tasks respectively. Then, both sets of unique representation interact with one another via a cross-task attention mechanism, which exploits associated contextual information produced by concerted efforts of both NER and RE. Experiments on three benchmark datasets demonstrate that the proposed model achieves significantly better performance in joint entity-relation extraction. Moreover, extended analysis validates that the proposed mechanism can indeed leverage the semantic information produced by NER and RE sub-tasks to boost one another in a complementary way. The source code is available to the public online.</abstract>
      <url hash="80549538">2024.lrec-main.1343</url>
      <attachment type="OptionalSupplementaryMaterial" hash="6d9af5da">2024.lrec-main.1343.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>luo-etal-2024-synergetic-interaction</bibkey>
    </paper>
    <paper id="1344">
      <title><fixed-case>S</fixed-case>yn<fixed-case>P</fixed-case>rompt: Syntax-aware Enhanced Prompt Engineering for Aspect-based Sentiment Analysis</title>
      <author><first>Wen</first><last>Yin</last></author>
      <author><first>Cencen</first><last>Liu</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Ahmad Raza</first><last>Wahla</last></author>
      <author><first>Huang</first><last>Yiting</last></author>
      <author><first>Dezhang</first><last>Zheng</last></author>
      <pages>15469–15479</pages>
      <abstract>Although there have been some works using prompt learning for the Aspect-based Sentiment Analysis(ABSA) tasks, their methods of prompt-tuning are simple and crude. Compared with vanilla fine-tuning methods, prompt learning intuitively bridges the objective form gap between pre-training and fine-tuning. Concretely, simply constructing prompt related to aspect words fails to fully exploit the potential of Pre-trained Language Models, and conducting more robust and professional prompt engineering for downstream tasks is a challenging problem that needs to be solved urgently. Therefore, in this paper, we propose a novel Syntax-aware Enhanced Prompt method (SynPrompt), which sufficiently mines the key syntactic information related to aspect words from the syntactic dependency tree. Additionally, to effectively harness the domain-specific knowledge embedded within PLMs for the ABSA tasks, we construct two adaptive prompt frameworks to enhance the perception ability of the above method. After conducting extensive experiments on three benchmark datasets, we have found that our method consistently achieves favorable results. These findings not only demonstrate the effectiveness and rationality of our proposed methods but also provide a powerful alternative to traditional prompt-tuning.</abstract>
      <url hash="59487da4">2024.lrec-main.1344</url>
      <bibkey>yin-etal-2024-synprompt-syntax</bibkey>
    </paper>
    <paper id="1345">
      <title>Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation</title>
      <author><first>Kartik</first><last>Kartik</last></author>
      <author><first>Sanjana</first><last>Soni</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <author><first>Md. Shad</first><last>Akhtar</last></author>
      <pages>15480–15492</pages>
      <abstract>The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.</abstract>
      <url hash="7aa21b05">2024.lrec-main.1345</url>
      <bibkey>kartik-etal-2024-synthetic-data</bibkey>
    </paper>
    <paper id="1346">
      <title><fixed-case>S</fixed-case>yn<fixed-case>TOD</fixed-case>: Augmented Response Synthesis for Robust End-to-End Task-Oriented Dialogue System</title>
      <author><first>Nguyen Quang</first><last>Chieu</last></author>
      <author><first>Quang-Minh</first><last>Tran</last></author>
      <author><first>Khac-Hoai Nam</first><last>Bui</last></author>
      <pages>15493–15499</pages>
      <abstract>Task-oriented dialogue (TOD) systems are introduced to solve specific tasks, which focus on training multiple tasks such as language understanding, tracking states, and generating appropriate responses to help users achieve their specific goals. Currently, one of the remaining challenges in this emergent research field is the capability to produce more robust architectures fine-tuned for end-to-end TOD systems. In this study, we consider this issue by exploiting the ability of pre-trained models to provide synthesis responses, which are then used as the input for the fine-tuned process. The main idea is to overcome the gap between the training process and inference process during fine-tuning end-to-end TOD systems. The experiment on Multiwoz datasets shows the effectiveness of our model compared with strong baselines in this research field. The source code is available for further exploitation.</abstract>
      <url hash="50e4d447">2024.lrec-main.1346</url>
      <bibkey>chieu-etal-2024-syntod-augmented</bibkey>
    </paper>
    <paper id="1347">
      <title>Tackling Long Code Search with Splitting, Encoding, and Aggregating</title>
      <author><first>Fan</first><last>Hu</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <author><first>Lun</first><last>Du</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <author><first>Xirong</first><last>Li</last></author>
      <pages>15500–15510</pages>
      <abstract>Code search with natural language helps us reuse existing code snippets. Thanks to the Transformer-based pretraining models, the performance of code search has been improved significantly. However, due to the quadratic complexity of multi-head self-attention, there is a limit on the input token length. For efficient training on standard GPUs like V100, existing pretrained code models, including GraphCodeBERT, CodeBERT, RoBERTa (code), take the first 256 tokens by default, which makes them unable to represent the complete information of long code that is greater than 256 tokens. To tackle the long code problem, we propose a new baseline SEA (Split, Encode and Aggregate), which splits long code into code blocks, encodes these blocks into embeddings, and aggregates them to obtain a comprehensive long code representation. With SEA, we could directly use Transformer-based pretraining models to model long code without changing their internal structure and re-pretraining. We also compare SEA with sparse Trasnformer methods. With GraphCodeBERT as the encoder, SEA achieves an overall mean reciprocal ranking score of 0.785, which is 10.1% higher than GraphCodeBERT on the CodeSearchNet benchmark, justifying SEA as a strong baseline for long code search.</abstract>
      <url hash="3cd25d4f">2024.lrec-main.1347</url>
      <bibkey>hu-etal-2024-tackling-long</bibkey>
    </paper>
    <paper id="1348">
      <title><fixed-case>T</fixed-case>aco<fixed-case>ERE</fixed-case>: Cluster-aware Compression for Event Relation Extraction</title>
      <author><first>Yong</first><last>Guan</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Jeff Z.</first><last>Pan</last></author>
      <author><first>Jiaoyan</first><last>Chen</last></author>
      <author><first>Freddy</first><last>Lecue</last></author>
      <pages>15511–15521</pages>
      <abstract>Event relation extraction (ERE) is a critical and fundamental challenge for natural language processing. Existing work mainly focuses on directly modeling the entire document, which cannot effectively handle long-range dependencies and information redundancy. To address these issues, we propose a cluster-aware compression method for improving event relation extraction (TacoERE), which explores a compression-then-extraction paradigm. Specifically, we first introduce document clustering for modeling event dependencies. It splits the document into intra- and inter-clusters, where intra-clusters aim to enhance the relations within the same cluster, while inter-clusters attempt to model the related events at arbitrary distances. Secondly, we utilize cluster summarization to simplify and highlight important text content of clusters for mitigating information redundancy and event distance. We have conducted extensive experiments on both pre-trained language models, such as RoBERTa, and large language models, such as ChatGPT and GPT-4, on three ERE datasets, i.e., MAVEN-ERE, EventStoryLine and HiEve. Experimental results demonstrate that TacoERE is an effective method for ERE.</abstract>
      <url hash="1c7fb7b4">2024.lrec-main.1348</url>
      <bibkey>guan-etal-2024-tacoere-cluster</bibkey>
    </paper>
    <paper id="1349">
      <title><fixed-case>TACO</fixed-case> – <fixed-case>T</fixed-case>witter Arguments from <fixed-case>CO</fixed-case>nversations</title>
      <author><first>Marc</first><last>Feger</last></author>
      <author><first>Stefan</first><last>Dietze</last></author>
      <pages>15522–15529</pages>
      <abstract>Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire COnversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff’s α among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify argument components on Twitter. Our transformer-based classifier achieves an 85.06% macro F1 baseline score in detecting arguments. Moreover, our data reveals that Twitter users tend to engage in discussions involving informed inferences and information. TACO serves multiple purposes, such as training tweet classifiers to manage tweets based on inference and information elements, while also providing valuable insights into the conversational reply patterns of tweets.</abstract>
      <url hash="412b15a3">2024.lrec-main.1349</url>
      <bibkey>feger-dietze-2024-taco-twitter</bibkey>
    </paper>
    <paper id="1350">
      <title><fixed-case>TA</fixed-case>e<fixed-case>KD</fixed-case>: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation</title>
      <author><first>Bo</first><last>Lv</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Kaiwen</first><last>Wei</last></author>
      <author><first>Ping</first><last>Luo</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <pages>15530–15541</pages>
      <abstract>Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to apply vanilla KD methods to transfer knowledge from closed-source Multilingual Neural Machine Translation (MNMT) models based on LLMs. In this scenario, the soft labels and training data are not accessible, making it difficult to achieve effective knowledge transfer. To address this issue, this paper proposes a Teacher Assistant enhanced Knowledge Distillation (TAeKD) method to augment the knowledge transfer capacity from closed-source MNMT models. Specifically, TAeKD designs a fusion model that integrates translation outputs from multiple closed-source models to generate soft labels and training samples. Furthermore, a quality assessment learning mechanism is introduced to enhance the generalization of the fusion model and elevate the quality of the fusion data used to train the student model. To facilitate research on knowledge transfer from MNMT models, we also introduce FuseData, a benchmark consisting of a blend of translations from multiple closed-source systems. The experimental results show that TAeKD outperforms the previous state-of-the-art KD methods on both WMT22 and FLORES-101 test sets.</abstract>
      <url hash="00ef7e51">2024.lrec-main.1350</url>
      <bibkey>lv-etal-2024-taekd-teacher</bibkey>
    </paper>
    <paper id="1351">
      <title><fixed-case>T</fixed-case>ai<fixed-case>C</fixed-case>hi: Improving the Robustness of <fixed-case>NLP</fixed-case> Models by Seeking Common Ground While Reserving Differences</title>
      <author><first>Huimin</first><last>Chen</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Yanhao</first><last>Wang</last></author>
      <author><first>Cen</first><last>Chen</last></author>
      <author><first>Yinggui</first><last>Wang</last></author>
      <pages>15542–15551</pages>
      <abstract>Recent studies have shown that Pre-trained Language Models (PLMs) are vulnerable to adversarial examples, crafted by introducing human-imperceptible perturbations to clean examples to deceive the models. This vulnerability stems from the divergence in the data distributions of clean and adversarial examples. Therefore, addressing this issue involves teaching the model to diminish the differences between the two types of samples and to focus more on their similarities. To this end, we propose a novel approach named <i>TaiChi</i> that employs a Siamese network architecture. Specifically, it consists of two sub-networks sharing the same structure but trained on clean and adversarial samples, respectively, and uses a contrastive learning strategy to encourage the generation of similar language representations for both kinds of samples. Furthermore, it utilizes the Kullback-Leibler (KL) divergence loss to enhance the consistency in the predictive behavior of the two sub-networks. Extensive experiments across three widely used datasets demonstrate that <i>TaiChi</i> achieves superior trade-offs between robustness to adversarial attacks at token and character levels and accuracy on clean examples compared to previous defense methods. Our code and data are publicly available at <url>https://github.com/sai4july/TaiChi</url>.</abstract>
      <url hash="2af62811">2024.lrec-main.1351</url>
      <bibkey>chen-etal-2024-taichi-improving</bibkey>
    </paper>
    <paper id="1352">
      <title>Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction</title>
      <author><first>Ziyang</first><last>Xu</last></author>
      <author><first>Keqin</first><last>Peng</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <author><first>Xiliang</first><last>Lu</last></author>
      <pages>15552–15565</pages>
      <abstract>Recent research shows that pre-trained language models (PLMs) suffer from “prompt bias” in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. Prompt bias presents a significant challenge in assessing the factual knowledge within PLMs. Therefore, this paper aims to improve the reliability of existing benchmarks by thoroughly investigating and mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the model’s internal representations to generate the debiased representations, which are used to produce the final debiased outputs. Experiments across various prompts, PLMs, and benchmarks show that our approach can not only correct the overfitted performance caused by prompt bias, but also significantly improve the prompt retrieval capability (up to 10% absolute performance gain). These results indicate that our approach effectively alleviates prompt bias in knowledge evaluation, thereby enhancing the reliability of benchmark assessments. Hopefully, our plug-and-play approach can be a golden standard to strengthen PLMs toward reliable knowledge bases. Code and data are released in https://github.com/FelliYang/PromptBias.</abstract>
      <url hash="21c3140d">2024.lrec-main.1352</url>
      <bibkey>xu-etal-2024-take-care</bibkey>
    </paper>
    <paper id="1353">
      <title>Take Its Essence, Discard Its Dross! Debiasing for Toxic Language Detection via Counterfactual Causal Effect</title>
      <author><first>Junyu</first><last>Lu</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <author><first>Xiaokun</first><last>Zhang</last></author>
      <author><first>Kaiyuan</first><last>Liu</last></author>
      <author><first>Dongyu</first><last>Zhang</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>15566–15578</pages>
      <abstract>Researchers have attempted to mitigate lexical bias in toxic language detection (TLD). However, existing methods fail to disentangle the “useful” and “misleading” impact of lexical bias on model decisions. Therefore, they do not effectively exploit the positive effects of the bias and lead to a degradation in the detection performance of the debiased model. In this paper, we propose a Counterfactual Causal Debiasing Framework (CCDF) to mitigate lexical bias in TLD. It preserves the “useful impact” of lexical bias and eliminates the “misleading impact”. Specifically, we first represent the total effect of the original sentence and biased tokens on decisions from a causal view. We then conduct counterfactual inference to exclude the direct causal effect of lexical bias from the total effect. Empirical evaluations demonstrate that the debiased TLD model incorporating CCDF achieves state-of-the-art performance in both accuracy and fairness compared to competitive baselines applied on several vanilla models. The generalization capability of our model outperforms current debiased models for out-of-distribution data.</abstract>
      <url hash="ac19a524">2024.lrec-main.1353</url>
      <bibkey>lu-etal-2024-take-essence</bibkey>
    </paper>
    <paper id="1354">
      <title><fixed-case>TAPASGO</fixed-case>: Transfer Learning towards a <fixed-case>G</fixed-case>erman-Language Tabular Question Answering Model</title>
      <author><first>Dominik Andreas</first><last>Kowieski</last></author>
      <author><first>Michael</first><last>Hellwig</last></author>
      <author><first>Thomas</first><last>Feilhauer</last></author>
      <pages>15579–15584</pages>
      <abstract>Processing tabular data holds significant importance across various domains and applications. This study investigates the performance and limitations of fine-tuned models for tabular data analysis, specifically focusing on using fine-tuning mechanics on an English model towards a potential German model. The validation of the effectiveness of the transfer learning approach compares the performance of the fine-tuned German model and of the original English model on test data from the German training set. A potential shortcut that translates the German test data into English serves for comparison. Results reveal that the fine-tuned model outperforms the original model significantly, demonstrating the effectiveness of transfer learning even for a limited amount of training data. One also observes that the English model can effectively process translated German tabular data, albeit with a slight accuracy drop compared to fine-tuning. The model evaluation extends to real-world data extracted from the sustainability reports of a financial institution. The fine-tuned model proves superior in extracting knowledge from these training-unrelated tables, indicating its potential applicability in practical scenarios. This paper also releases the first manually annotated dataset for German Table Question Answering and the related annotation tool.</abstract>
      <url hash="e4eac5ce">2024.lrec-main.1354</url>
      <bibkey>kowieski-etal-2024-tapasgo-transfer</bibkey>
    </paper>
    <paper id="1355">
      <title>Target-Adaptive Consistency Enhanced Prompt-Tuning for Multi-Domain Stance Detection</title>
      <author><first>Shaokang</first><last>Wang</last></author>
      <author><first>Li</first><last>Pan</last></author>
      <pages>15585–15594</pages>
      <abstract>Stance detection is a fundamental task in Natural Language Processing (NLP). It is challenging due to diverse expressions and topics related to the targets from multiple domains. Recently, prompt-tuning has been introduced to convert the original task into a cloze-style prediction task, achieving impressive results. Many prompt-tuning-based methods focus on one or two classic scenarios with concrete external knowledge enhancement. However, when facing intricate information in multi-domain stance detection, these methods cannot be adaptive to multi-domain semantics. In this paper, we propose a novel target-adaptive consistency enhanced prompt-tuning method (TCP) for stance detection with multiple domains. TCP incorporates target knowledge and prior knowledge to construct target-adaptive verbalizers for diverse domains and employs pilot experiments distillation to enhance the consistency between verbalizers and model training. Specifically, to capture the knowledge from multiple domains, TCP uses a target-adaptive candidate mining strategy to obtain the domain-related candidates. Then, TCP refines them with prior attributes to ensure prediction consistency. The Pre-trained Language Models (PLMs) in prompt-tuning are with large-scale parameters, while only changing the verbalizer without corresponding tuning has a limited impact on the training process. Target-aware pilot experiments are conducted to enhance the consistency between the verbalizer and training by distilling the target-adaptive knowledge into prompt-tuning. Extensive experiments and ablation studies demonstrate that TCP outperforms the state-of-the-art methods on nine stance detection datasets from multiple domains.</abstract>
      <url hash="36a06deb">2024.lrec-main.1355</url>
      <bibkey>wang-pan-2024-target-adaptive</bibkey>
    </paper>
    <paper id="1356">
      <title>Targeted Syntactic Evaluation on the <fixed-case>C</fixed-case>homsky Hierarchy</title>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Ryo</first><last>Yoshida</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <pages>15595–15605</pages>
      <abstract>In this paper, we propose a novel evaluation paradigm for Targeted Syntactic Evaluations, where we assess how well language models can recognize linguistic phenomena situated at different levels of the Chomsky hierarchy. Specifically, we create formal languages that abstract four syntactic phenomena in natural languages, each identified at a different level of the Chomsky hierarchy, and use these to evaluate the capabilities of language models: (1) (Adj)ˆn NP type, (2) NPˆn VPˆn type, (3) Nested Dependency type, and (4) Cross Serial Dependency type. We first train three different language models (LSTM, Transformer LM, and Stack-RNN) on language modeling tasks and then evaluate them using pairs of a positive and a negative sentence by investigating whether they can assign a higher probability to the positive sentence than the negative one. Our result demonstrated that all language models have the ability to capture the structural patterns of the (Adj)ˆn NP type formal language. However, LSTM and Transformer LM failed to capture NPˆn VPˆn type language and no architectures can recognize nested dependency and Cross Serial dependency correctly. Neural language models, especially Transformer LMs, have exhibited high performance across a multitude of downstream tasks, leading to the perception that they possess an understanding of natural languages. However, our findings suggest that these models may not necessarily comprehend the syntactic structures that underlie natural language phenomena such as dependency. Rather, it appears that they may extend grammatical rules equivalent to regular grammars to approximate the rules governing dependencies.</abstract>
      <url hash="37081fef">2024.lrec-main.1356</url>
      <bibkey>someya-etal-2024-targeted-syntactic</bibkey>
    </paper>
    <paper id="1357">
      <title><fixed-case>TARIC</fixed-case>-<fixed-case>SLU</fixed-case>: A <fixed-case>T</fixed-case>unisian Benchmark Dataset for Spoken Language Understanding</title>
      <author><first>Salima</first><last>Mdhaffar</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Renato</first><last>de Mori</last></author>
      <author><first>Salah</first><last>Zaiem</last></author>
      <author><first>Mirco</first><last>Ravanelli</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <pages>15606–15616</pages>
      <abstract>In recent years, there has been a significant increase in interest in developing Spoken Language Understanding (SLU) systems. SLU involves extracting a list of semantic information from the speech signal. A major issue for SLU systems is the lack of sufficient amount of bi-modal (audio and textual semantic annotation) training data. Existing SLU resources are mainly available in high-resource languages such as English, Mandarin and French. However, one of the current challenges concerning low-resourced languages is data collection and annotation. In this work, we present a new freely available corpus, named TARIC-SLU, composed of railway transport conversations in Tunisian dialect that is continuously annotated in dialogue acts and slots. We describe the semantic model of the dataset, the data and experiments conducted to build ASR-based and SLU-based baseline models. To facilitate its use, a complete recipe, including data preparation, training and evaluation scripts, has been built and will be integrated to SpeechBrain, a popular open-source conversational AI toolkit based on PyTorch.</abstract>
      <url hash="bb4bde01">2024.lrec-main.1357</url>
      <bibkey>mdhaffar-etal-2024-taric-slu</bibkey>
    </paper>
    <paper id="1358">
      <title><fixed-case>TARN</fixed-case>-<fixed-case>VIST</fixed-case>: Topic Aware Reinforcement Network for Visual Storytelling</title>
      <author><first>Weiran</first><last>Chen</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Jiaqi</first><last>Su</last></author>
      <author><first>Guiqian</first><last>Zhu</last></author>
      <author><first>Ying</first><last>Li</last></author>
      <author><first>Yi</first><last>Ji</last></author>
      <author><first>Chunping</first><last>Liu</last></author>
      <pages>15617–15628</pages>
      <abstract>As a cross-modal task, visual storytelling aims to generate a story for an ordered image sequence automatically. Different from the image captioning task, visual storytelling requires not only modeling the relationships between objects in the image but also mining the connections between adjacent images. Recent approaches primarily utilize either end-to-end frameworks or multi-stage frameworks to generate relevant stories, but they usually overlook latent topic information. In this paper, in order to generate a more coherent and relevant story, we propose a novel method, Topic Aware Reinforcement Network for VIsual StoryTelling (TARN-VIST). In particular, we pre-extracted the topic information of stories from both visual and linguistic perspectives. Then we apply two topic-consistent reinforcement learning rewards to identify the discrepancy between the generated story and the human-labeled story so as to refine the whole generation process. Extensive experimental results on the VIST dataset and human evaluation demonstrate that our proposed model outperforms most of the competitive models across multiple evaluation metrics.</abstract>
      <url hash="b6c4b409">2024.lrec-main.1358</url>
      <bibkey>chen-etal-2024-tarn-vist</bibkey>
    </paper>
    <paper id="1359">
      <title>Task-agnostic Distillation of Encoder-Decoder Language Models</title>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Qiuchi</first><last>Li</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>15629–15639</pages>
      <abstract>Finetuning pretrained language models (LMs) have enabled appealing performance on a diverse array of tasks. The intriguing task-agnostic property has driven a shifted focus from task-specific to task-agnostic distillation of LMs. While task-agnostic, compute-efficient, performance-preserved LMs can be yielded by task-agnostic distillation, previous studies mainly sit in distillation of either encoder-only LMs (e.g., BERT) or decoder-only ones (e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g., T5) can posit very distinguished behaviors. Frustratingly, we discover that existing task-agnostic distillation methods can fail to handle the distillation of encoder-decoder LMs. To the demand, we explore a few paths and uncover a path named as MiniEnD that successfully tackles the distillation of encoder-decoder LMs in a task-agnostic fashion. We examine MiniEnD on language understanding and abstractive summarization. The results showcase that MiniEnD is generally effective and is competitive compared to other alternatives. We further scale MiniEnD up to distillation of 3B encoder-decoder language models with interpolated distillation. The results imply the opportunities and challenges in distilling large language models (e.g., LLaMA).</abstract>
      <url hash="21fb5018">2024.lrec-main.1359</url>
      <bibkey>zhang-etal-2024-task-agnostic</bibkey>
    </paper>
    <paper id="1360">
      <title>Task-Oriented Paraphrase Analytics</title>
      <author><first>Marcel</first><last>Gohsen</last></author>
      <author><first>Matthias</first><last>Hagen</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>15640–15654</pages>
      <abstract>Since paraphrasing is an ill-defined task, the term “paraphrasing” covers text transformation tasks with different characteristics. Consequently, existing paraphrasing studies have applied quite different (explicit and implicit) criteria as to when a pair of texts is to be considered a paraphrase, all of which amount to postulating a certain level of semantic or lexical similarity. In this paper, we conduct a literature review and propose a taxonomy to organize the 25 identified paraphrasing (sub-)tasks. Using classifiers trained to identify the tasks that a given paraphrasing instance fits, we find that the distributions of task-specific instances in the known paraphrase corpora vary substantially. This means that the use of these corpora, without the respective paraphrase conditions being clearly defined (which is the normal case), must lead to incomparable and misleading results.</abstract>
      <url hash="b022aefe">2024.lrec-main.1360</url>
      <bibkey>gohsen-etal-2024-task-oriented</bibkey>
    </paper>
    <paper id="1361">
      <title>tasksource: A Large Collection of <fixed-case>NLP</fixed-case> tasks with a Structured Dataset Preprocessing Framework</title>
      <author><first>Damien</first><last>Sileo</last></author>
      <pages>15655–15684</pages>
      <abstract>The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different structures, making harmonization challenging which prevents the interchangeable use of comparable datasets. As a result, multi-task training or evaluation necessitates manual work to fit data into task templates. Several initiatives independently tackle this issue by releasing harmonized datasets or providing harmonization codes to preprocess datasets into a consistent format. We identify patterns in such preprocessings, such as column renaming, or more complex patterns. We then propose an annotation framework that enables concise, readable, and reusable preprocessing annotations. tasksource annotates more than 600 task preprocessings and provides a backend to automate dataset alignment. We fine-tune a multi-task text encoder on all tasksource tasks, outperforming every publicly available text encoder of comparable parameter count according to an external evaluation.</abstract>
      <url hash="cf139db9">2024.lrec-main.1361</url>
      <bibkey>sileo-2024-tasksource-large</bibkey>
    </paper>
    <paper id="1362">
      <title>Teaching Large Language Models to Translate on Low-resource Languages with Textbook Prompting</title>
      <author><first>Ping</first><last>Guo</last></author>
      <author><first>Yubing</first><last>Ren</last></author>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Yunpeng</first><last>Li</last></author>
      <author><first>Jiarui</first><last>Zhang</last></author>
      <author><first>Xingsheng</first><last>Zhang</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <pages>15685–15697</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive results in Machine Translation by simply following instructions, even without training on parallel data. However, LLMs still face challenges on low-resource languages due to the lack of pre-training data. In real-world situations, humans can become proficient in their native languages through abundant and meaningful social interactions and can also learn foreign languages effectively using well-organized textbooks. Drawing inspiration from human learning patterns, we introduce the Translate After LEarNing Textbook (TALENT) approach, which aims to enhance LLMs’ ability to translate low-resource languages by learning from a textbook. TALENT follows a step-by-step process: (1) Creating a Textbook for low-resource languages. (2) Guiding LLMs to absorb the Textbook’s content for Syntax Patterns. (3) Enhancing translation by utilizing the Textbook and Syntax Patterns. We thoroughly assess TALENT’s performance using 112 low-resource languages from FLORES-200 with two LLMs: ChatGPT and BLOOMZ. Evaluation across three different metrics reveals that TALENT consistently enhances translation performance by 14.8% compared to zero-shot baselines. Further analysis demonstrates that TALENT not only improves LLMs’ comprehension of low-resource languages but also equips them with the knowledge needed to generate accurate and fluent sentences in these languages.</abstract>
      <url hash="620f31b9">2024.lrec-main.1362</url>
      <bibkey>guo-etal-2024-teaching-large</bibkey>
    </paper>
    <paper id="1363">
      <title><fixed-case>TECA</fixed-case>: A Two-stage Approach with Controllable Attention Soft Prompt for Few-shot Nested Named Entity Recognition</title>
      <author><first>Yuanyuan</first><last>Xu</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <pages>15698–15710</pages>
      <abstract>Few-shot nested named entity recognition (NER), identifying named entities that are nested with a small number of labeled data, has attracted much attention. Recently, a span-based method based on three stages ( focusing, bridging and prompting) has been proposed for few-shot nested NER. However, such a span-based approach for few-shot nested NER suffers from two challenges: 1) error propagation because of its 3-stage pipeline-based framework; 2) ignoring the relationship between inner and outer entities, which is crucial for few-shot nested NER. Therefore, in this work, we propose a two-stage approach with a controllable attention soft prompt for few-shot nested named entity recognition (TECA). It consists of two components: span part identification and entity mention recognition. The span part identification provides possible entity mentions without an extra filtering module. The entity mention recognition pays fine-grained attention to the inner and outer entities and the corresponding adjacent context through the controllable attention soft prompt to classify the candidate entity mentions. Experimental results show that the TECA approach achieves state-of-the-art performance consistently on the four benchmark datasets (ACE2004, ACE2005, GENIA, and KBP2017) and outperforms several competing baseline models on F1-score by 5.62% on ACE04, 5.11% on ACE05, 3.41% on KBP2017 and 0.7% on GENIA on the 10-shot setting.</abstract>
      <url hash="a30bde04">2024.lrec-main.1363</url>
      <bibkey>xu-etal-2024-teca-two</bibkey>
    </paper>
    <paper id="1364">
      <title><fixed-case>T</fixed-case>e<fixed-case>C</fixed-case>lass: A Human-Annotated Relevance-based Headline Classification and Generation Dataset for <fixed-case>T</fixed-case>elugu</title>
      <author><first>Gopichand</first><last>Kanumolu</last></author>
      <author><first>Lokesh</first><last>Madasu</last></author>
      <author><first>Nirmal</first><last>Surange</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>15711–15720</pages>
      <abstract>News headline generation is a crucial task in increasing productivity for both the readers and producers of news. This task can easily be aided by automated News headline-generation models. However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models. We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines. Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles. While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data. To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs. We experiment with various baseline models and provide a comprehensive analysis of their results. We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset. The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores. To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available.</abstract>
      <url hash="8ff96ba1">2024.lrec-main.1364</url>
      <bibkey>kanumolu-etal-2024-teclass-human</bibkey>
    </paper>
    <paper id="1365">
      <title><fixed-case>TED</fixed-case>-<fixed-case>EL</fixed-case>: A Corpus for Speech Entity Linking</title>
      <author><first>Silin</first><last>Li</last></author>
      <author><first>Ruoyu</first><last>Song</last></author>
      <author><first>Tianwei</first><last>Lan</last></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>15721–15731</pages>
      <abstract>Speech entity linking amis to recognize mentions from speech and link them to entities in knowledge bases. Previous work on entity linking mainly focuses on visual context and text context. In contrast, speech entity linking focuses on audio context. In this paper, we first propose the speech entity linking task. To facilitate the study of this task, we propose the first speech entity linking dataset, TED-EL. Our corpus is a high-quality, human-annotated, audio, text, and mention-entity pair parallel dataset derived from Technology, Entertainment, Design (TED) talks and includes a wide range of entity types (24 types). Based on TED-EL, we designed two types of models: ranking-based and generative speech entity linking models. We conducted experiments on the TED-EL dataset for both types of models. The results show that the ranking-based models outperform the generative models, achieving an F1 score of 60.68%.</abstract>
      <url hash="6bad698b">2024.lrec-main.1365</url>
      <bibkey>li-etal-2024-ted-el</bibkey>
    </paper>
    <paper id="1366">
      <title>Tell Me Again! a Large-Scale Dataset of Multiple Summaries for the Same Story</title>
      <author><first>Hans Ole</first><last>Hatzel</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>15732–15741</pages>
      <abstract>A wide body of research is concerned with the semantics of narratives, both in terms of understanding narratives and generating fictional narratives and stories. We provide a dataset of summaries to be used as a proxy for entire stories or for the analysis of the summaries themselves. Our dataset consists of a total of 96,831 individual summaries across 29,505 stories. We intend for the dataset to be used for training and evaluation of embedding representations for stories, specifically the stories’ narratives. The summary data is harvested from five different language versions of Wikipedia. Our dataset comes with rich metadata, which we extract from Wikidata, enabling a wide range of applications that operate on story summaries in conjunction with metadata. To set baseline results, we run retrieval experiments on the dataset, exploring the capability of similarity models in retrieving summaries of the same story. For this retrieval, a crucial element is to not place too much emphasis on the named entities, as this can enable retrieval of other summaries for the same work without taking the narrative into account.</abstract>
      <url hash="1100e10e">2024.lrec-main.1366</url>
      <bibkey>hatzel-biemann-2024-tell-large</bibkey>
    </paper>
    <paper id="1367">
      <title>Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding</title>
      <author><first>Xinyue</first><last>Liu</last></author>
      <author><first>Jianan</first><last>Zhang</last></author>
      <author><first>Chi</first><last>Ma</last></author>
      <author><first>Wenxin</first><last>Liang</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <author><first>Linlin</first><last>Zong</last></author>
      <pages>15742–15751</pages>
      <abstract>Reasoning over the Temporal Knowledge Graph (TKG) that predicts facts in the future has received much attention. Most previous works attempt to model temporal dynamics with knowledge graphs and graph convolution networks. However, these methods lack the consideration of high-order interactions between objects in TKG, which is an important factor to predict future facts. To address this problem, we introduce dynamic hypergraph embedding for temporal knowledge graph reasoning. Specifically, we obtain high-order interactions by constructing hypergraphs based on temporal knowledge graphs at different timestamps. Besides, we integrate the differences caused by time into the hypergraph representation in order to fit TKG. Then, we adapt dynamic meta-embedding for temporal hypergraph representation that allows our model to choose the appropriate high-order interactions for downstream reasoning. Experimental results on public TKG datasets show that our method outperforms the baselines. Furthermore, the analysis part demonstrates that the proposed method brings good interpretation for the predicted results.</abstract>
      <url hash="075228e7">2024.lrec-main.1367</url>
      <bibkey>liu-etal-2024-temporal-knowledge</bibkey>
    </paper>
    <paper id="1368">
      <title>Term-Driven Forward-Looking Claim Synthesis in Earnings Calls</title>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>15752–15760</pages>
      <abstract>Argument synthesis aims to generate rational claims, representing a fundamental objective in this field. While existing models excel in summarizing arguments and engaging in debates, we observe a critical gap in their ability to generate accurate arguments that incorporate forward-looking perspectives. In light of this observation, this paper introduces a novel task called “forward-looking claim planning.” We delve into this task by exploring the efficacy of well-performing classification and generation models. Furthermore, we propose several customized preprocessing methods that yield substantial performance improvements. Through comprehensive discussion and analysis, we also outline a future research agenda for the forward-looking claim planning task.</abstract>
      <url hash="b7937350">2024.lrec-main.1368</url>
      <bibkey>chen-takamura-2024-term-driven</bibkey>
    </paper>
    <paper id="1369">
      <title>text2story: A Python Toolkit to Extract and Visualize Story Components of Narrative Text</title>
      <author><first>Evelin</first><last>Amorim</last></author>
      <author><first>Ricardo</first><last>Campos</last></author>
      <author><first>Alipio</first><last>Jorge</last></author>
      <author><first>Pedro</first><last>Mota</last></author>
      <author><first>Rúben</first><last>Almeida</last></author>
      <pages>15761–15772</pages>
      <abstract>Story components, namely, events, time, participants, and their relations are present in narrative texts from different domains such as journalism, medicine, finance, and law. The automatic extraction of narrative elements encompasses several NLP tasks such as Named Entity Recognition, Semantic Role Labeling, Event Extraction, Coreference resolution, and Temporal Inference. The text2story python, an easy-to-use modular library, supports the narrative extraction and visualization pipeline. The package contains an array of narrative extraction tools that can be used separately or in sequence. With this toolkit, end users can process free text in English or Portuguese and obtain formal representations, like standard annotation files or a formal logical representation. The toolkit also enables narrative visualization as Message Sequence Charts (MSC), Knowledge Graphs, and Bubble Diagrams, making it useful to visualize and transform human-annotated narratives. The package combines the use of off-the-shelf and custom tools and is easily patched (replacing existing components) and extended (e.g. with new visualizations). It includes an experimental module for narrative element effectiveness assessment and being is therefore also a valuable asset for researchers developing solutions for narrative extraction. To evaluate the baseline components, we present some results of the main annotators embedded in our packages for datasets in English and Portuguese. We also compare the results with the extraction of narrative elements by GPT-3, a robust LLM model.</abstract>
      <url hash="e3e55af7">2024.lrec-main.1369</url>
      <bibkey>amorim-etal-2024-text2story-python</bibkey>
    </paper>
    <paper id="1370">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>S</fixed-case>tory Lusa: A Dataset for Narrative Analysis in <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese News Articles</title>
      <author><first>Sérgio</first><last>Nunes</last></author>
      <author><first>Alípio</first><last>Mario Jorge</last></author>
      <author><first>Evelin</first><last>Amorim</last></author>
      <author><first>Hugo</first><last>Sousa</last></author>
      <author><first>António</first><last>Leal</last></author>
      <author><first>Purificação Moura</first><last>Silvano</last></author>
      <author><first>Inês</first><last>Cantante</last></author>
      <author><first>Ricardo</first><last>Campos</last></author>
      <pages>15773–15782</pages>
      <abstract>Narratives have been the subject of extensive research across various scientific fields such as linguistics and computer science. However, the scarcity of freely available datasets, essential for studying this genre, remains a significant obstacle. Furthermore, datasets annotated with narratives components and their morphosyntactic and semantic information are even scarcer. To address this gap, we developed the Text2Story Lusa datasets, which consist of a collection of news articles in European Portuguese. The first datasets consists of 357 news articles and the second dataset comprises a subset of 117 manually densely annotated articles, totaling over 50 thousand individual annotations. By focusing on texts with substantial narrative elements, we aim to provide a valuable resource for studying narrative structures in European Portuguese news articles. On the one hand, the first dataset provides researchers with data to study narratives from various perspectives. On the other hand, the annotated dataset facilitates research in information extraction and related tasks, particularly in the context of narrative extraction pipelines. Both datasets are made available adhering to FAIR principles, thereby enhancing their utility within the research community.</abstract>
      <url hash="391e7ff9">2024.lrec-main.1370</url>
      <bibkey>nunes-etal-2024-text2story-lusa</bibkey>
    </paper>
    <paper id="1371">
      <title><fixed-case>T</fixed-case>ext360<fixed-case>N</fixed-case>av: 360-Degree Image Captioning Dataset for Urban Pedestrians Navigation</title>
      <author><first>Chieko</first><last>Nishimura</last></author>
      <author><first>Shuhei</first><last>Kurita</last></author>
      <author><first>Yohei</first><last>Seki</last></author>
      <pages>15783–15788</pages>
      <abstract>Text feedback from urban scenes is a crucial tool for pedestrians to understand surroundings, obstacles, and safe pathways. However, existing image captioning datasets often concentrate on the overall image description and lack detailed scene descriptions, overlooking features for pedestrians walking on urban streets. We developed a new dataset to assist pedestrians in urban scenes using 360-degree camera images. Through our dataset of Text360Nav, we aim to provide textual feedback from machinery visual perception such as 360-degree cameras to visually impaired individuals and distracted pedestrians navigating urban streets, including those engrossed in their smartphones while walking. In experiments, we combined our dataset with multimodal generative models and observed that models trained with our dataset can generate textual descriptions focusing on street objects and obstacles that are meaningful in urban scenes in both quantitative and qualitative analyses, thus supporting the effectiveness of our dataset for urban pedestrian navigation.</abstract>
      <url hash="4d8d9e5a">2024.lrec-main.1371</url>
      <bibkey>nishimura-etal-2024-text360nav-360</bibkey>
    </paper>
    <paper id="1372">
      <title>Text Filtering Classifiers for Medium-Resource Languages</title>
      <author><first>Jón</first><last>Daðason</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>15789–15801</pages>
      <abstract>Web-crawled corpora are essential resources for linguistic and NLP research, offering far more data than is available from curated corpora. However, they often contain a great deal of low-quality texts which can complicate research and degrade the quality of pre-trained language models. Therefore, they are typically filtered, e.g. by applying rules or classifiers. In this paper, we compare the effectiveness of various text filtering classifiers and measure their impact on language model performance for three medium-resource languages. We present TQ-IS, an Icelandic text quality dataset consisting of 2,000 web-crawled documents, in which spans of low-quality text have been manually identified and labeled. We then evaluate a perplexity-based classifier, a supervised classifier trained on TQ-IS, and a self-supervised classifier trained to discern between documents from curated and web-crawled corpora on Icelandic, Estonian and Basque. We find that these classifiers obtain F1 scores of 94.48%, 99.01% and 93.40%, respectively, when evaluated on the TQ-IS dataset. Furthermore, our results show that while adding filtered web-crawled text to a pre-training corpus can improve downstream performance for pre-trained language models, any improvement is likely to remain modest unless the web-crawled corpus is significantly larger in size.</abstract>
      <url hash="e246d554">2024.lrec-main.1372</url>
      <bibkey>dadason-loftsson-2024-text-filtering</bibkey>
    </paper>
    <paper id="1373">
      <title>Text Style Transfer Evaluation Using Large Language Models</title>
      <author><first>Phil Sidney</first><last>Ostheimer</last></author>
      <author><first>Mayank Kumar</first><last>Nagda</last></author>
      <author><first>Marius</first><last>Kloft</last></author>
      <author><first>Sophie</first><last>Fellenz</last></author>
      <pages>15802–15822</pages>
      <abstract>Evaluating Text Style Transfer (TST) is a complex task due to its multi-faceted nature. The quality of the generated text is measured based on challenging factors, such as style transfer accuracy, content preservation, and overall fluency. While human evaluation is considered to be the gold standard in TST assessment, it is costly and often hard to reproduce. Therefore, automated metrics are prevalent in these domains. Nonetheless, it is uncertain whether and to what extent these automated metrics correlate with human evaluations. Recent strides in Large Language Models (LLMs) have showcased their capacity to match and even exceed average human performance across diverse, unseen tasks. This suggests that LLMs could be a viable alternative to human evaluation and other automated metrics in TST evaluation. We compare the results of different LLMs in TST evaluation using multiple input prompts. Our findings highlight a strong correlation between (even zero-shot) prompting and human evaluation, showing that LLMs often outperform traditional automated metrics. Furthermore, we introduce the concept of prompt ensembling, demonstrating its ability to enhance the robustness of TST evaluation. This research contributes to the ongoing efforts for more robust and diverse evaluation methods by standardizing and validating TST evaluation with LLMs.</abstract>
      <url hash="a2d0f559">2024.lrec-main.1373</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fcf0e96c">2024.lrec-main.1373.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>ostheimer-etal-2024-text-style</bibkey>
    </paper>
    <paper id="1374">
      <title>Text-to-Multimodal Retrieval with Bimodal Input Fusion in Shared Cross-Modal Transformer</title>
      <author><first>Pranav</first><last>Arora</last></author>
      <author><first>Selen</first><last>Pehlivan</last></author>
      <author><first>Jorma</first><last>Laaksonen</last></author>
      <pages>15823–15834</pages>
      <abstract>The rapid proliferation of multimedia content has necessitated the development of effective multimodal video retrieval systems. Multimodal video retrieval is a non-trivial task involving retrieval of relevant information across different modalities, such as text, audio, and visual. This work aims to improve multimodal retrieval by guiding the creation of a shared embedding space with task-specific contrastive loss functions. An important aspect of our work is to propose a model that learns retrieval cues for the textual query from multiple modalities both separately and jointly within a hierarchical architecture that can be flexibly extended and fine-tuned for any number of modalities. To this end, the loss functions and the architectural design of the model are developed with a strong focus on increasing the mutual information between the textual and cross-modal representations. The proposed approach is quantitatively evaluated on the MSR-VTT and YouCook2 text-to-video retrieval benchmark datasets. The results showcase that the approach not only holds its own against state-of-the-art methods, but also outperforms them in a number of scenarios, with a notable relative improvements from baseline in R@1, R@5 and R@10 metrics.</abstract>
      <url hash="4ca2ea03">2024.lrec-main.1374</url>
      <bibkey>arora-etal-2024-text-multimodal</bibkey>
    </paper>
    <paper id="1375">
      <title>Textual Coverage of Eventive Entries in Lexical Semantic Resources</title>
      <author><first>Eva</first><last>Fučíková</last></author>
      <author><first>Cristina Fernández</first><last>Alcaina</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Zdeňka</first><last>Urešová</last></author>
      <pages>15835–15841</pages>
      <abstract>This short paper focuses on the coverage of eventive entries (verbs, predicates, etc.) of some well-known lexical semantic resources when applied to random running texts taken from the internet. While coverage gaps are often reported for manually created lexicons (which is the case of most semantically-oriented lexical ones), it was our aim to quantify these gaps, cross-lingually, on a new purely textual resource set produced by the HPLT Project from crawled internet data. Several English, German, Spanish and Czech lexical semantic resources (which, for the most part, focus on verbs and predicates) have been selected for this experiment. We also describe the challenges related to the fact that these resources are (to a varying extent) semantically oriented, meaning that the texts have to be preprocessed to obtain lemmas (base forms) and some types of MWEs before the coverage can be reasonably evaluated, and thus the results are necessarily only approximate. The coverage of these resources, with some exclusions as described in the paper, range from 41.00% to 97.33%, confirming the need to expand at least some - even well-known - resources to cover the prevailing source of today’s textual resources with regard to lexical units describing events or states (or possibly other eventive mentions).</abstract>
      <url hash="ff9b88de">2024.lrec-main.1375</url>
      <bibkey>fucikova-etal-2024-textual-coverage</bibkey>
    </paper>
    <paper id="1376">
      <title>The Challenges of Creating a Parallel Multilingual Hate Speech Corpus: An Exploration</title>
      <author><first>Katerina</first><last>Korre</last></author>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>15842–15853</pages>
      <abstract>Hate speech is infamously one of the most demanding topics in Natural Language Processing, as its multifacetedness is accompanied by a handful of challenges, such as multilinguality and cross-linguality. Hate speech has a subjective aspect that intensifies when referring to different cultures and different languages. In this respect, we design a pipeline that will help us explore the possibility of the creation of a parallel multilingual hate speech dataset, using machine translation. In this paper, we evaluate how/whether this is feasible by assessing the quality of the translations, calculating the toxicity levels of original and target texts, and calculating correlations between the newly obtained scores. Finally, we perform a qualitative analysis to gain further semantic and grammatical insights. With this pipeline we aim at exploring ways of filtering hate speech texts in order to parallelize sentences in multiple languages, examining the challenges of the task.</abstract>
      <url hash="6818c49c">2024.lrec-main.1376</url>
      <bibkey>korre-etal-2024-challenges-creating</bibkey>
    </paper>
    <paper id="1377">
      <title>The Contextual Variability of <fixed-case>E</fixed-case>nglish Nouns: The Impact of Categorical Specificity beyond Conceptual Concreteness</title>
      <author><first>Giulia</first><last>Rambelli</last></author>
      <author><first>Marianna</first><last>Bolognesi</last></author>
      <pages>15854–15860</pages>
      <abstract>Research on conceptual abstraction has investigated the differences in contextual distributions, or “contextual variability,” of abstract and concrete concept words (e.g., *love* vs. *cat*). Empirical studies on this topic show that abstract words tend to occur in diverse linguistic contexts, while concrete words are typically constrained within more homogeneous contexts. Nonetheless, these investigations have somewhat overlooked a factor that influences both abstract and concrete concepts: *Categorial Specificity*, which denotes the inclusiveness of a category (e.g., *ragdoll* vs. *mammal*). We argue that more specific words are tied to narrower domains, independently or whether they are concrete or abstract, thus resulting in a diminished degree of contextual variability when compared to generic terms. In this study, we used distributional models to investigate the interplay between contextual variability, concreteness, specificity, and their interaction. Analyzing 676 English nouns, we found that contextual variability is explained by both concreteness and specificity: more specific words have closer contexts, while generic words, whether abstract or concrete, exhibit less related contexts.</abstract>
      <url hash="74c57fef">2024.lrec-main.1377</url>
      <bibkey>rambelli-bolognesi-2024-contextual-variability</bibkey>
    </paper>
    <paper id="1378">
      <title>The Corpus <fixed-case>AIKIA</fixed-case>: Using Ranking Annotation for Offensive Language Detection in <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek</title>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <author><first>Vivian</first><last>Stamou</last></author>
      <author><first>Christina</first><last>Christodoulou</last></author>
      <author><first>Georgia</first><last>Apostolopoulou</last></author>
      <author><first>Antonis</first><last>Balas</last></author>
      <author><first>George</first><last>Ioannakis</last></author>
      <pages>15861–15871</pages>
      <abstract>We introduce a new corpus, named AIKIA, for Offensive Language Detection (OLD) in Modern Greek (EL). EL is a less-resourced language regarding OLD. AIKIA offers free access to annotated data leveraged from EL Twitter and fiction texts using the lexicon of offensive terms, ERIS, that originates from HurtLex. AIKIA has been annotated for offensive values with the Best Worst Scaling (BWS) method, which is designed to avoid problems of categorical and scalar annotation methods. BWS assigns continuous offensive scores in the form of floating point numbers instead of binary arithmetical or categorical values. AIKIA’s performance in OLD was tested by fine-tuning a variety of pre-trained language models in a binary classification task. Experimentation with a number of thresholds showed that the best mapping of the continuous values to binary labels should occur at the range [0.5-0.6] of BWS values and that the pre-trained models on EL data achieved the highest Macro-F1 scores. Greek-Media-BERT outperformed all models with a threshold of 0.6 by obtaining a Macro-F1 score of 0.92</abstract>
      <url hash="65e0636e">2024.lrec-main.1378</url>
      <bibkey>markantonatou-etal-2024-corpus-aikia</bibkey>
    </paper>
    <paper id="1379">
      <title>The Distracted Ear: How Listeners Shape Conversational Dynamics</title>
      <author><first>Auriane</first><last>Boudin</last></author>
      <author><first>Stéphane</first><last>Rauzy</last></author>
      <author><first>Roxane</first><last>Bertrand</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>15872–15887</pages>
      <abstract>In the realm of human communication, feedback plays a pivotal role in shaping the dynamics of conversations. This study delves into the multifaceted relationship between listener feedback, narration quality and distraction effects. We present an analysis conducted on the SMYLE corpus, specifically enriched for this study, where 30 dyads of participants engaged in 1) face-to-face storytelling (8.2 hours) followed by 2) a free conversation (7.8 hours). The storytelling task unfolds in two conditions, where a storyteller engages with either a “normal” or a “distracted” listener. Examining the feedback impact on storytellers, we discover a positive correlation between the frequency of specific feedback and the narration quality in normal conditions, providing an encouraging conclusion regarding the enhancement of interaction through specific feedback in distraction-free settings. In contrast, in distracted settings, a negative correlation emerges, suggesting that increased specific feedback may disrupt narration quality, underscoring the complexity of feedback dynamics in human communication. The contribution of this paper is twofold: first presenting a new and highly enriched resource for the analysis of discourse phenomena in controlled and normal conditions; second providing new results on feedback production, its form and its consequence on the discourse quality (with direct applications in human-machine interaction).</abstract>
      <url hash="81361277">2024.lrec-main.1379</url>
      <bibkey>boudin-etal-2024-distracted-ear</bibkey>
    </paper>
    <paper id="1380">
      <title>The Effects of Pretraining in Video-Guided Machine Translation</title>
      <author><first>Ammon</first><last>Shurtz</last></author>
      <author><first>Lawry</first><last>Sorenson</last></author>
      <author><first>Stephen D.</first><last>Richardson</last></author>
      <pages>15888–15898</pages>
      <abstract>We propose an approach that improves the performance of VMT (Video-guided Machine Translation) models, which integrate text and video modalities. We experiment with the MAD (Movie Audio Descriptions) dataset, a new dataset which contains transcribed audio descriptions of movies. We find that the MAD dataset is more lexically rich than the VATEX dataset (the current VMT baseline), and we experiment with MAD pretraining to improve performance on the VATEX dataset. We experiment with two different video encoder architectures: a Conformer (Convolution-augmented Transformer) and a Transformer. Additionally, we conduct experiments by masking the source sentences to assess the degree to which the performance of both architectures improves due to pretraining on additional video data. Finally, we conduct an analysis of the transfer learning potential of a video dataset and compare it to pretraining on a text-only dataset. Our findings demonstrate that pretraining with a lexically rich dataset leads to significant improvements in model performance when models use both text and video modalities.</abstract>
      <url hash="48442898">2024.lrec-main.1380</url>
      <bibkey>shurtz-etal-2024-effects-pretraining</bibkey>
    </paper>
    <paper id="1381">
      <title>The <fixed-case>ELC</fixed-case>o Dataset: Bridging Emoji and Lexical Composition</title>
      <author><first>Zi Yun</first><last>Yang</last></author>
      <author><first>Ziqing</first><last>Zhang</last></author>
      <author><first>Yisong</first><last>Miao</last></author>
      <pages>15899–15909</pages>
      <abstract>Can emojis be composed to convey intricate meanings like English phrases? As a pioneering study, we present the Emoji-Lexical Composition (ELCo) dataset, a new resource that offers parallel annotations of emoji sequences corresponding to English phrases. Our dataset contains 1,655 instances, spanning 209 diverse concepts from tangible ones like “right man” (✔️👨) to abstract ones such as “full attention” (🧐✍️, illustrating a metaphoric composition of a focusing face and writing hand). ELCo enables the analysis of the patterns shared between emoji and lexical composition. Through a corpus study, we discovered that simple strategies like direct representation and reduplication are sufficient for conveying certain concepts, but a richer, metaphorical strategy is essential for expressing more abstract ideas. We further introduce an evaluative task, Emoji-based Textual Entailment (EmoTE), to assess the proficiency of NLP models in comprehending emoji compositions. Our findings reveals the challenge of understanding emoji composition in a zero-shot setting for current models, including ChatGPT. Our analysis indicates that the intricacy of metaphorical compositions contributes to this challenge. Encouragingly, models show marked improvement when fine-tuned on the ELCo dataset, with larger models excelling in deciphering nuanced metaphorical compositions.</abstract>
      <url hash="0bffb860">2024.lrec-main.1381</url>
      <bibkey>yang-etal-2024-elco-dataset</bibkey>
    </paper>
    <paper id="1382">
      <title>The Emergence of Semantic Units in Massively Multilingual Models</title>
      <author><first>Andrea Gregor</first><last>de Varda</last></author>
      <author><first>Marco</first><last>Marelli</last></author>
      <pages>15910–15921</pages>
      <abstract>Massively multilingual models can process text in several languages relying on a shared set of parameters; however, little is known about the encoding of multilingual information in single network units. In this work, we study how two semantic variables, namely valence and arousal, are processed in the latent dimensions of mBERT and XLM-R across 13 languages. We report a significant cross-lingual overlap in the individual neurons processing affective information, which is more pronounced when considering XLM-R vis-à-vis mBERT. Furthermore, we uncover a positive relationship between cross-lingual alignment and performance, where the languages that rely more heavily on a shared cross-lingual neural substrate achieve higher performance scores in semantic encoding.</abstract>
      <url hash="c99c2c6d">2024.lrec-main.1382</url>
      <bibkey>de-varda-marelli-2024-emergence-semantic</bibkey>
    </paper>
    <paper id="1383">
      <title>The Ethical Question – Use of Indigenous Corpora for Large Language Models</title>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <author><first>Flammie A.</first><last>Pirinen</last></author>
      <author><first>Børre</first><last>Gaup</last></author>
      <author><first>Trond</first><last>Trosterud</last></author>
      <author><first>Maja Lisa</first><last>Kappfjell</last></author>
      <author><first>Sjur</first><last>Moshagen</last></author>
      <pages>15922–15931</pages>
      <abstract>Creating language technology based on language data has become very popular with the recent advances of large language models and neural network technologies. This makes language resources very valuable, and especially in case of indigenous languages, the scarce resources are even more precious. Given the good results of simply fetching everything you can from the internet and feeding it to neural networks in English, there has been more work on doing the same for all languages. However, indigenous language resources as they are on the web are not comparable in that they would encode the most recent normativised language in all its aspects. This problematic is further due to not understanding the texts input to models or output by models by the people who work on them. Corpora also have intelligent property rights and copyrights that are not respected. Furthermore, the web is filled with the result of language model -generated texts. In this article we describe an ethical and sustainable way to work with indigenous languages.</abstract>
      <url hash="8076593b">2024.lrec-main.1383</url>
      <bibkey>wiechetek-etal-2024-ethical-question</bibkey>
    </paper>
    <paper id="1384">
      <title>The <fixed-case>I</fixed-case>gbo<fixed-case>API</fixed-case> Dataset: Empowering <fixed-case>I</fixed-case>gbo Language Technologies through Multi-dialectal Enrichment</title>
      <author><first>Chris Chinenye</first><last>Emezue</last></author>
      <author><first>Ifeoma</first><last>Okoh</last></author>
      <author><first>Chinedu Emmanuel</first><last>Mbonu</last></author>
      <author><first>Chiamaka</first><last>Chukwuneke</last></author>
      <author><first>Daisy Monika</first><last>Lal</last></author>
      <author><first>Ignatius</first><last>Ezeani</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Ijemma</first><last>Onwuzulike</last></author>
      <author><first>Chukwuma Onyebuchi</first><last>Okeke</last></author>
      <author><first>Gerald Okey</first><last>Nweya</last></author>
      <author><first>Bright Ikechukwu</first><last>Ogbonna</last></author>
      <author><first>Chukwuebuka Uchenna</first><last>Oraegbunam</last></author>
      <author><first>Esther Chidinma</first><last>Awo-Ndubuisi</last></author>
      <author><first>Akudo Amarachukwu</first><last>Osuagwu</last></author>
      <pages>15932–15941</pages>
      <abstract>The Igbo language is facing a risk of becoming endangered, as indicated by a 2025 UNESCO study. This highlights the need to develop language technologies for Igbo to foster communication, learning and preservation. To create robust, impactful, and widely adopted language technologies for Igbo, it is essential to incorporate the multi-dialectal nature of the language. The primary obstacle in achieving dialectal-aware language technologies is the lack of comprehensive dialectal datasets. In response, we present the IgboAPI dataset, a multi-dialectal Igbo-English dictionary dataset, developed with the aim of enhancing the representation of Igbo dialects. Furthermore, we illustrate the practicality of the IgboAPI dataset through two distinct studies: one focusing on Igbo semantic lexicon and the other on machine translation. In the semantic lexicon project, we successfully establish an initial Igbo semantic lexicon for the Igbo semantic tagger, while in the machine translation study, we demonstrate that by finetuning existing machine translation systems using the IgboAPI dataset, we significantly improve their ability to handle dialectal variations in sentences.</abstract>
      <url hash="e781312c">2024.lrec-main.1384</url>
      <bibkey>emezue-etal-2024-igboapi-dataset</bibkey>
    </paper>
    <paper id="1385">
      <title>The Impact of Stance Object Type on the Quality of Stance Detection</title>
      <author><first>Maxwell A.</first><last>Weinzierl</last></author>
      <author><first>Sanda M.</first><last>Harabagiu</last></author>
      <pages>15942–15954</pages>
      <abstract>Stance as an expression of an author’s standpoint and as a means of communication has long been studied by computational linguists. Automatically identifying the stance of a subject toward an object is an active area of research in natural language processing. Significant work has employed topics and claims as the object of stance, with frames of communication becoming more recently considered as alternative objects of stance. However, little attention has been paid to finding what are the benefits and what are the drawbacks when inferring the stance of a text towards different possible stance objects. In this paper we seek to answer this question by analyzing the implied knowledge and the judgments required when deciding the stance of a text towards each stance object type. Our analysis informed experiments with models capable of inferring the stance of a text towards any of the stance object types considered, namely topics, claims, and frames of communication. Experiments clearly indicate that it is best to infer the stance of a text towards a frame of communication, rather than a claim or a topic. It is also better to infer the stance of a text towards a claim rather than a topic. Therefore we advocate that rather than continuing efforts to annotate the stance of texts towards topics, it is better to use those efforts to produce annotations towards frames of communication. These efforts will allow us to better capture the stance towards claims and topics as well.</abstract>
      <url hash="e34f58bc">2024.lrec-main.1385</url>
      <bibkey>weinzierl-harabagiu-2024-impact-stance</bibkey>
    </paper>
    <paper id="1386">
      <title>The Influence of Automatic Speech Recognition on Linguistic Features and Automatic <fixed-case>A</fixed-case>lzheimer’s Disease Detection from Spontaneous Speech</title>
      <author><first>Jonathan</first><last>Heitz</last></author>
      <author><first>Gerold</first><last>Schneider</last></author>
      <author><first>Nicolas</first><last>Langer</last></author>
      <pages>15955–15969</pages>
      <abstract>Alzheimer’s disease (AD) represents a major problem for society and a heavy burden for those affected. The study of changes in speech offers a potential means for large-scale AD screening that is non-invasive and inexpensive. Automatic Speech Recognition (ASR) is necessary for a fully automated system. We compare different ASR systems in terms of Word Error Rate (WER) using a publicly available benchmark dataset of speech recordings of AD patients and controls. Furthermore, this study is the first to quantify how popular linguistic features change when replacing manual transcriptions with ASR output. This contributes to the understanding of linguistic features in the context of AD detection. Moreover, we investigate how ASR affects AD classification performance by implementing two popular approaches: A fine-tuned BERT model, and Random Forest on popular linguistic features. Our results show best classification performance when using manual transcripts, but the degradation when using ASR is not dramatic. Performance stays strong, achieving an AUROC of 0.87. Our BERT-based approach is affected more strongly by ASR transcription errors than the simpler and more explainable approach based on linguistic features.</abstract>
      <url hash="f22663f1">2024.lrec-main.1386</url>
      <bibkey>heitz-etal-2024-influence-automatic</bibkey>
    </paper>
    <paper id="1387">
      <title>The Key Points: Using Feature Importance to Identify Shortcomings in Sign Language Recognition Models</title>
      <author><first>Ruth M.</first><last>Holmes</last></author>
      <author><first>Ellen</first><last>Rushe</last></author>
      <author><first>Anthony</first><last>Ventresque</last></author>
      <pages>15970–15975</pages>
      <abstract>Pose estimation keypoints are widely used in sign language recognition (SLR) as a means of generalising to unseen signers. Despite the advantages of keypoints, SLR models struggle to achieve high recognition accuracy for many signed languages due to the large degree of variability between occurrences of the same signs, the lack of large datasets and the imbalanced nature of the data therein. In this paper we seek to provide a deeper analysis into the ways that these keypoints are used by models in order to determine which are most informative to SLR, identify potentially redundant ones and investigate whether keypoints that are central to differentiating signs in practice are being effectively used as expected by models.</abstract>
      <url hash="1df61276">2024.lrec-main.1387</url>
      <bibkey>holmes-etal-2024-key-points</bibkey>
    </paper>
    <paper id="1388">
      <title>The <fixed-case>L</fixed-case>ow <fixed-case>S</fixed-case>axon <fixed-case>LSDC</fixed-case> Dataset at <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Janine</first><last>Siewert</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <pages>15976–15981</pages>
      <abstract>We present an extension of the Low Saxon Universal Dependencies dataset and discuss a few annotation-related challenges. Low Saxon is a West-Germanic low-resource language that lacks a common standard and therefore poses challenges for NLP. The 1,000 sentences in our dataset cover the last 200 years and 8 of the 9 major dialects. They are presented both in original and in normalised spelling and two lemmata are provided: A Modern Low Saxon lemma and a Middle Low Saxon lemma. Several annotation-related issues result from dialectal variation in morphological categories, and we explain differences in the pronoun, gender, case, and mood system. Furthermore, we take up three syntactic constructions that do not occur in Standard Dutch or Standard German: the possessive dative, pro-drop in pronominal adverbs, and complementiser doubling in subordinate interrogative clauses. These constructions are also rare in the other Germanic UD datasets and have not always been annotated consistently.</abstract>
      <url hash="f9bb92a7">2024.lrec-main.1388</url>
      <bibkey>siewert-rueter-2024-low-saxon</bibkey>
    </paper>
    <paper id="1389">
      <title>The Onomastic Repertoire of the <fixed-case>R</fixed-case>oman d’Alexandre (<fixed-case>ORNARE</fixed-case>). Designing an Integrated Digital Onomastic Tool for Medieval <fixed-case>F</fixed-case>rench <fixed-case>R</fixed-case>omance</title>
      <author><first>Marta</first><last>Milazzo</last></author>
      <author><first>Giorgio Maria</first><last>Di Nunzio</last></author>
      <pages>15982–15987</pages>
      <abstract>The paper reports on the first results of the design and implementation of a new digital tool for romance philology: the digital Onomastic Repertoire for the medieval French romance (12th-15th centuries). This tool, projected with a modular and integrable architecture, was implemented from a selection of romances, the corpus of the Medieval French Roman d’Alexandre. After introducing the peculiarities of the onomastic system in the Middle Ages (and, more generally, the peculiarities of medieval literary texts), the paper describes 1) the methodological challenges faced in the preparatory work, illustrates and comments on the first results achieved and 2) the design and implementation of the first integrated system for the interactive creation of the Onomastic Repertoire of the romaN d’AlexandRE (ORNARE), and 3) the current research output in terms of both a digital edition and the digital onomastic index of the corpus.</abstract>
      <url hash="558f2156">2024.lrec-main.1389</url>
      <bibkey>milazzo-di-nunzio-2024-onomastic-repertoire</bibkey>
    </paper>
    <paper id="1390">
      <title>The Open-World Lottery Ticket Hypothesis for <fixed-case>OOD</fixed-case> Intent Classification</title>
      <author><first>Yunhua</first><last>Zhou</last></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Peiju</first><last>Liu</last></author>
      <author><first>Yuxin</first><last>Wang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>15988–15999</pages>
      <abstract>Most existing methods of Out-of-Domain (OOD) intent classification rely on extensive auxiliary OOD corpora or specific training paradigms. However, they are underdeveloped in the underlying principle that the models should have differentiated confidence in In- and Out-of-domain intent. In this work, we shed light on the fundamental cause of model overconfidence on OOD and demonstrate that calibrated subnetworks can be uncovered by pruning the overparameterized model. Calibrated confidence provided by the subnetwork can better distinguish In- and Out-of-domain, which can be a benefit for almost all post hoc methods. In addition to bringing fundamental insights, we also extend the Lottery Ticket Hypothesis to open-world scenarios. We conduct extensive experiments on four real-world datasets to demonstrate our approach can establish consistent improvements compared with a suite of competitive baselines.</abstract>
      <url hash="fe943c61">2024.lrec-main.1390</url>
      <bibkey>zhou-etal-2024-open-world</bibkey>
    </paper>
    <paper id="1391">
      <title>Theoretical and Empirical Advantages of Dense-Vector to One-Hot Encoding of Intent Classes in Open-World Scenarios</title>
      <author><first>Paulo</first><last>Cavalin</last></author>
      <author><first>Claudio Santos</first><last>Pinhanez</last></author>
      <pages>16000–16013</pages>
      <abstract>This work explores the intrinsic limitations of the popular one-hot encoding method in classification of intents when detection of out-of-scope (OOS) inputs is required. Although recent work has shown that there can be significant improvements in OOS detection when the intent classes are represented as dense-vectors based on domain-specific knowledge, we argue in this paper that such gains are more likely due to advantages of the much richer topologies that can be created with dense vectors compared to the equidistant class representation assumed by one-hot encodings. We start by demonstrating how dense-vector encodings are able to create OOS spaces with much richer topologies. Then, we show empirically, using four standard intent classification datasets, that knowledge-free, randomly generated dense-vector encodings of intent classes can yield over 20% gains over one-hot encodings, producing better systems for open-world classification tasks, mostly from improvements in OOS detection.</abstract>
      <url hash="4ba7c383">2024.lrec-main.1391</url>
      <bibkey>cavalin-pinhanez-2024-theoretical-empirical</bibkey>
    </paper>
    <paper id="1392">
      <title>The <fixed-case>P</fixed-case>ar<fixed-case>C</fixed-case>o<fixed-case>L</fixed-case>ab Parallel Corpus and Its Extension to Four Regional Languages of <fixed-case>F</fixed-case>rance</title>
      <author><first>Dejan</first><last>Stosic</last></author>
      <author><first>Saša</first><last>Marjanović</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Xavier</first><last>Bach</last></author>
      <author><first>Myriam</first><last>Bras</last></author>
      <author><first>Laurent</first><last>Kevers</last></author>
      <author><first>Stella</first><last>Retali-Medori</last></author>
      <author><first>Marianne</first><last>Vergez-Couret</last></author>
      <author><first>Carole</first><last>Werner</last></author>
      <pages>16014–16023</pages>
      <abstract>Parallel corpora are still scarce for most of the world’s language pairs. The situation is by no means different for regional languages of France. In addition, adequate web interfaces facilitate and encourage the use of parallel corpora by target users, such as language learners and teachers, as well as linguists. In this paper, we describe ParCoLab, a parallel corpus and a web platform for querying the corpus. From its onset, ParCoLab has been geared towards lower-resource languages, with an initial corpus in Serbian, along with French and English (later Spanish). We focus here on the extension of ParCoLab with a parallel corpus for four regional languages of France: Alsatian, Corsican, Occitan and Poitevin-Saintongeais. In particular, we detail criteria for choosing texts and issues related to their collection. The new parallel corpus contains more than 20k tokens per regional language.</abstract>
      <url hash="8054c740">2024.lrec-main.1392</url>
      <bibkey>stosic-etal-2024-parcolab-parallel</bibkey>
    </paper>
    <paper id="1393">
      <title>The <fixed-case>P</fixed-case>arla<fixed-case>S</fixed-case>ent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings</title>
      <author><first>Michal</first><last>Mochtak</last></author>
      <author><first>Peter</first><last>Rupnik</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>16024–16036</pages>
      <abstract>The paper presents a new training dataset of sentences in 7 languages, manually annotated for sentiment, which are used in a series of experiments focused on training a robust sentiment identifier for parliamentary proceedings. The paper additionally introduces the first domain-specific multilingual transformer language model for political science applications, which was additionally pre-trained on 1.72 billion words from parliamentary proceedings of 27 European parliaments. We present experiments demonstrating how the additional pre-training on parliamentary data can significantly improve the model downstream performance, in our case, sentiment identification in parliamentary proceedings. We further show that our multilingual model performs very well on languages not seen during fine-tuning, and that additional fine-tuning data from other languages significantly improves the target parliament’s results. The paper makes an important contribution to multiple disciplines inside the social sciences, and bridges them with computer science and computational linguistics. Lastly, the resulting fine-tuned language model sets up a more robust approach to sentiment analysis of political texts across languages, which allows scholars to study political sentiment from a comparative perspective using standardized tools and techniques.</abstract>
      <url hash="02915884">2024.lrec-main.1393</url>
      <bibkey>mochtak-etal-2024-parlasent-multilingual</bibkey>
    </paper>
    <paper id="1394">
      <title>There’s Something New about the <fixed-case>I</fixed-case>talian Parliament: The <fixed-case>IPSA</fixed-case> Corpus</title>
      <author><first>Valentino</first><last>Frasnelli</last></author>
      <author><first>Alessio</first><last>Palmero Aprosio</last></author>
      <pages>16037–16046</pages>
      <abstract>Parliamentary debates constitute a substantial and somewhat underutilized reservoir of publicly available written content. Despite their potential, the Italian parliamentary documents remain largely unexplored and most importantly inaccessible in their original paper-based form. In this paper we attempt to transform these valuable historical documents into IPSA, a digitally readable structured corpus containing speeches, reports of the Standing Committees, and law proposals spanning 175 years of Italian history, from the issuing of the Statuto Albertino in 1848, up to the present day. At first, the PDF documents, available on the official websites of Senato della Repubblica and Camera dei Deputati, the two chambers that form the Italian Parliament, are digitized using Optical Character Recognition (OCR) techniques. Then, the speeches are tagged with the corresponding speakers. The final dataset is released both in textual and structured format.</abstract>
      <url hash="83f371fc">2024.lrec-main.1394</url>
      <bibkey>frasnelli-palmero-aprosio-2024-theres-something</bibkey>
    </paper>
    <paper id="1395">
      <title>The <fixed-case>RIP</fixed-case> Corpus of Collaborative Hypothesis-Making</title>
      <author><first>Ella</first><last>Schad</last></author>
      <author><first>Jacky</first><last>Visser</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <pages>16047–16057</pages>
      <abstract>The dearth of literature combining hypothesis-making and collaborative problem solving presents a problem in the investigation into how hypotheses are generated in group environments. A new dataset, the Resolving Investigative hyPotheses (RIP) corpus, is introduced to address this issue. The corpus uses the fictionalised environment of a murder investigation game. An artificial environment restricts the number of possible hypotheses compared to real-world situations, allowing a deeper dive into the data. In three groups of three, participants collaborated to solve the mystery: two groups came to the wrong conclusion in different ways, and one succeeded in solving the game. RIP is a 49k-word dialogical corpus, consisting of three sub-corpora, annotated for argumentation and discourse structure on the basis of Inference Anchoring Theory. The corpus shows the emergent roles individuals took on and the strategies the groups employed, showing what can be gained through a deeper exploration of this domain. The corpus bridges the gap between these two areas – hypothesis generation and collaborative problem solving – by using an environment rich with potential for hypothesising within a highly collaborative space.</abstract>
      <url hash="e52b02e1">2024.lrec-main.1395</url>
      <bibkey>schad-etal-2024-rip-corpus</bibkey>
    </paper>
    <paper id="1396">
      <title>The Role of Creaky Voice in Turn Taking and the Perception of Speaker Stance: Experiments Using Controllable <fixed-case>TTS</fixed-case></title>
      <author><first>Harm</first><last>Lameris</last></author>
      <author><first>Eva</first><last>Szekely</last></author>
      <author><first>Joakim</first><last>Gustafson</last></author>
      <pages>16058–16065</pages>
      <abstract>Recent advancements in spontaneous text-to-speech (TTS) have enabled the realistic synthesis of creaky voice, a voice quality known for its diverse pragmatic and paralinguistic functions. In this study, we used synthesized creaky voice in perceptual tests, to explore how listeners without formal training perceive two distinct types of creaky voice. We annotated a spontaneous speech corpus using creaky voice detection tools and modified a neural TTS engine with a creaky phonation embedding to control the presence of creaky phonation in the synthesized speech. We performed an objective analysis using a creak detection tool which revealed significant differences in creaky phonation levels between the two creaky voice types and modal voice. Two subjective listening experiments were performed to investigate the effect of creaky voice on perceived certainty, valence, sarcasm, and turn finality. Participants rated non-positional creak as less certain, less positive, and more indicative of turn finality, while positional creak was rated significantly more turn final compared to modal phonation.</abstract>
      <url hash="ee84f682">2024.lrec-main.1396</url>
      <bibkey>lameris-etal-2024-role-creaky</bibkey>
    </paper>
    <paper id="1397">
      <title>The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement</title>
      <author><first>Jonathan</first><last>Kamp</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>16066–16078</pages>
      <abstract>Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size k. We systematically investigate the interaction between k and spans and propose an improved configuration for selecting important tokens.</abstract>
      <url hash="7b776ed0">2024.lrec-main.1397</url>
      <bibkey>kamp-etal-2024-role-syntactic</bibkey>
    </paper>
    <paper id="1398">
      <title>The <fixed-case>SAMER</fixed-case> <fixed-case>A</fixed-case>rabic Text Simplification Corpus</title>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Reem</first><last>Hazim</last></author>
      <author><first>Juan David</first><last>Pineros Liberato</last></author>
      <author><first>Muhamed</first><last>Al Khalil</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>16079–16093</pages>
      <abstract>We present the SAMER Corpus, the first manually annotated Arabic parallel corpus for text simplification targeting school-aged learners. Our corpus comprises texts of 159K words selected from 15 publicly available Arabic fiction novels most of which were published between 1865 and 1955. Our corpus includes readability level annotations at both the document and word levels, as well as two simplified parallel versions for each text targeting learners at two different readability levels. We describe the corpus selection process, and outline the guidelines we followed to create the annotations and ensure their quality. Our corpus is publicly available to support and encourage research on Arabic text simplification, Arabic automatic readability assessment, and the development of Arabic pedagogical language technologies.</abstract>
      <url hash="faa80850">2024.lrec-main.1398</url>
      <bibkey>alhafni-etal-2024-samer-arabic</bibkey>
    </paper>
    <paper id="1399">
      <title>The <fixed-case>S</fixed-case>lovak Autistic and Non-Autistic Child Speech Corpus:Task-Oriented Child-Adult Interactions</title>
      <author><first>Joanna</first><last>Kruyt</last></author>
      <author><first>Róbert</first><last>Sabo</last></author>
      <author><first>Katarína</first><last>Polónyiová</last></author>
      <author><first>Daniela</first><last>Ostatníková</last></author>
      <author><first>Štefan</first><last>Beňuš</last></author>
      <pages>16094–16099</pages>
      <abstract>This paper presents the Slovak Autistic and Non-Autistic Child Speech Corpus, which consists of audio-recordings and transcripts of collaborative, task-oriented conversations between children (with or without autism spectrum disorder, ASD) and a non-autistic adult experimenter. The task used to elicit this corpus was the Maps task. This corpus was primarily recorded to investigate lexical alignment, but can also be used to study other conversation coordination strategies and behaviours. Scores on various standardised psychometric tests, such as those measuring IQ, executive functioning, and theory of mind, are included for each participant. In total, the corpus contains over 15 hours of speech. This relatively large database contains a non-Germanic language and can be shared with any qualified researcher, making it a valuable resource for replication of existing findings regarding communication and ASD as well as future research into communication between individuals with and without ASD.</abstract>
      <url hash="a7f6907c">2024.lrec-main.1399</url>
      <bibkey>kruyt-etal-2024-slovak-autistic</bibkey>
    </paper>
    <paper id="1400">
      <title>The <fixed-case>S</fixed-case>wedish Parliament Corpus 1867 – 2022</title>
      <author><first>Väinö Aleksi</first><last>Yrjänäinen</last></author>
      <author><first>Fredrik</first><last>Mohammadi Norén</last></author>
      <author><first>Robert</first><last>Borges</last></author>
      <author><first>Johan</first><last>Jarlbrink</last></author>
      <author><first>Lotta</first><last>Åberg Brorsson</last></author>
      <author><first>Anders P.</first><last>Olsson</last></author>
      <author><first>Pelle</first><last>Snickars</last></author>
      <author><first>Måns</first><last>Magnusson</last></author>
      <pages>16100–16112</pages>
      <abstract>The Swedish parliamentary records are an important source material for social science and humanities researchers. We introduce a new research corpus, the Swedish Parliament Corpus, which is larger and more developed than previously available research corpora for the Swedish parliament. The corpus contains annotated and structured parliamentary records over more than 150 years, through the bicameral parliament (1867–1970) and the unicameral parliament (1971–). In addition to the records, which contain all speeches in the parliament, we also provide a database of all members of parliament over the same period. Along with the corpus, we describe procedures to ensure data quality. The corpus facilitates detailed analysis of parliamentary speeches in several research fields.</abstract>
      <url hash="2cd83132">2024.lrec-main.1400</url>
      <bibkey>yrjanainen-etal-2024-swedish-parliament</bibkey>
    </paper>
    <paper id="1401">
      <title>The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of <fixed-case>E</fixed-case>nglish</title>
      <author><first>Tom S</first><last>Juzek</last></author>
      <pages>16113–16120</pages>
      <abstract>We present a preview of the Syntactic Acceptability Dataset, a resource being designed for both syntax and computational linguistics research. In its current form, the dataset comprises 1,000 English sequences from the syntactic discourse: Half from textbooks and half from the journal Linguistic Inquiry, the latter to ensure a representation of the contemporary discourse. Each entry is labeled with its grammatical status (“well-formedness” according to syntactic formalisms) extracted from the literature, as well as its acceptability status (“intuitive goodness” as determined by native speakers) obtained through crowdsourcing, with highest experimental standards. Even in its preliminary form, this dataset stands as the largest of its kind that is publicly accessible. We also offer preliminary analyses addressing three debates in linguistics and computational linguistics: We observe that grammaticality and acceptability judgments converge in about 83% of the cases and that “in-betweenness” occurs frequently. This corroborates existing research. We also find that while machine learning models struggle with predicting grammaticality, they perform considerably better in predicting acceptability. This is a novel finding. Future work will focus on expanding the dataset.</abstract>
      <url hash="c67bac7a">2024.lrec-main.1401</url>
      <bibkey>juzek-2024-syntactic-acceptability</bibkey>
    </paper>
    <paper id="1402">
      <title>The Touché23-<fixed-case>V</fixed-case>alue<fixed-case>E</fixed-case>val Dataset for Identifying Human Values behind Arguments</title>
      <author><first>Nailia</first><last>Mirzakhmedova</last></author>
      <author><first>Johannes</first><last>Kiesel</last></author>
      <author><first>Milad</first><last>Alshomary</last></author>
      <author><first>Maximilian</first><last>Heinrich</last></author>
      <author><first>Nicolas</first><last>Handke</last></author>
      <author><first>Xiaoni</first><last>Cai</last></author>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>Doratossadat</first><last>Dastgheib</last></author>
      <author><first>Omid</first><last>Ghahroodi</last></author>
      <author><first>MohammadAli</first><last>SadraeiJavaheri</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last></author>
      <author><first>Lea</first><last>Kawaletz</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>16121–16134</pages>
      <abstract>While human values play a crucial role in making arguments persuasive, we currently lack the necessary extensive datasets to develop methods for analyzing the values underlying these arguments on a large scale. To address this gap, we present the Touché23-ValueEval dataset, an expansion of the Webis-ArgValues-22 dataset. We collected and annotated an additional 4780 new arguments, doubling the dataset’s size to 9324 arguments. These arguments were sourced from six diverse sources, covering religious texts, community discussions, free-text arguments, newspaper editorials, and political debates. Each argument is annotated by three crowdworkers for 54 human values, following the methodology established in the original dataset. The Touché23-ValueEval dataset was utilized in the SemEval 2023 Task 4. ValueEval: Identification of Human Values behind Arguments, where an ensemble of transformer models demonstrated state-of-the-art performance. Furthermore, our experiments show that a fine-tuned large language model, Llama-2-7B, achieves comparable results.</abstract>
      <url hash="a9183da6">2024.lrec-main.1402</url>
      <bibkey>mirzakhmedova-etal-2024-touche23-valueeval</bibkey>
    </paper>
    <paper id="1403">
      <title><fixed-case>TIGER</fixed-case>: A Unified Generative Model Framework for Multimodal Dialogue Response Generation</title>
      <author><first>Fanheng</first><last>Kong</last></author>
      <author><first>Peidong</first><last>Wang</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>16135–16141</pages>
      <abstract>Responding with multimodal content has been recognized as one of the essential functionalities of intelligent conversational agents. However, existing research on multimodal dialogues primarily focuses on two topics: (1) textual response generation that ground the conversation on a given image; and (2) visual response selection based on the dialogue context. In light of the aforementioned gap, we propose mulTImodal GEnerator for dialogue Response (TIGER), a unified generative model framework for multimodal dialogue response generation. Through extensive experiments, TIGER has demonstrated new state-of-the-art results, providing users with an enhanced conversational experience. A multimodal dialogue system based on TIGER is available at https://github.com/friedrichor/TIGER. A video demonstrating the system is available at https://www.youtube.com/watch?v=Kd0CMwDs8Rk.</abstract>
      <url hash="30fab8d1">2024.lrec-main.1403</url>
      <bibkey>kong-etal-2024-tiger-unified</bibkey>
    </paper>
    <paper id="1404">
      <title><fixed-case>TIGQA</fixed-case>: An Expert-Annotated Question-Answering Dataset in <fixed-case>T</fixed-case>igrinya</title>
      <author><first>Hailay Kidu</first><last>Teklehaymanot</last></author>
      <author><first>Dren</first><last>Fazlija</last></author>
      <author><first>Niloy</first><last>Ganguly</last></author>
      <author><first>Gourab Kumar</first><last>Patro</last></author>
      <author><first>Wolfgang</first><last>Nejdl</last></author>
      <pages>16142–16161</pages>
      <abstract>The absence of explicitly tailored, accessible annotated datasets for educational purposes presents a notable obstacle for NLP tasks in languages with limited resources. This study initially explores the feasibility of using machine translation (MT) to convert an existing dataset into a Tigrinya dataset in SQuAD format. As a result, we present TIGQA, an expert-annotated dataset containing 2,685 question-answer pairs covering 122 diverse topics such as climate, water, and traffic. These pairs are from 537 context paragraphs in publicly accessible Tigrinya and Biology books. Through comprehensive analyses, we demonstrate that the TIGQA dataset requires skills beyond simple word matching, requiring both single-sentence and multiple-sentence inference abilities. We conduct experiments using state-of-the-art MRC methods, marking the first exploration of such models on TIGQA. Additionally, we estimate human performance on the dataset and juxtapose it with the results obtained from pre-trained models. The notable disparities between human performance and the best model performance underscore the potential for fu- ture enhancements to TIGQA through continued research. Our dataset is freely accessible via the provided link to encourage the research community to address the challenges in the Tigrinya MRC. Keywords: Tigrinya QA dataset, Low resource QA dataset, domain specific QA</abstract>
      <url hash="5c3ddbc0">2024.lrec-main.1404</url>
      <bibkey>teklehaymanot-etal-2024-tigqa-expert</bibkey>
    </paper>
    <paper id="1405">
      <title>Time-aware <fixed-case>COMET</fixed-case>: A Commonsense Knowledge Model with Temporal Knowledge</title>
      <author><first>Eiki</first><last>Murata</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>16162–16174</pages>
      <abstract>To better handle commonsense knowledge, which is difficult to acquire in ordinary training of language models, commonsense knowledge graphs and commonsense knowledge models have been constructed. The former manually and symbolically represents commonsense, and the latter stores these graphs’ knowledge in the models’ parameters. However, the existing commonsense knowledge models that deal with events do not consider granularity or time axes. In this paper, we propose a time-aware commonsense knowledge model, TaCOMET. The construction of TaCOMET consists of two steps. First, we create TimeATOMIC using ChatGPT, which is a commonsense knowledge graph with time. Second, TaCOMET is built by continually finetuning an existing commonsense knowledge model on TimeATOMIC. TimeATOMIC and continual finetuning let the model make more time-aware generations with rich commonsense than the existing commonsense models. We also verify the applicability of TaCOMET on a robotic decision-making task. TaCOMET outperformed the existing commonsense knowledge model when proper times are input. Our dataset and models will be made publicly available.</abstract>
      <url hash="d5604341">2024.lrec-main.1405</url>
      <bibkey>murata-kawahara-2024-time-aware</bibkey>
    </paper>
    <paper id="1406">
      <title>Title-based Extractive Summarization via <fixed-case>MRC</fixed-case> Framework</title>
      <author><first>Hongjin</first><last>Kim</last></author>
      <author><first>Jai-Eun</first><last>Kim</last></author>
      <author><first>Harksoo</first><last>Kim</last></author>
      <pages>16175–16186</pages>
      <abstract>Existing studies on extractive summarization have primarily focused on scoring and selecting summary sentences independently. However, these models are limited to sentence-level extraction and tend to select highly generalized sentences while overlooking the overall content of a document. To effectively consider the semantics of a document, in this study, we introduce a novel machine reading comprehension (MRC) framework for extractive summarization (MRCSum) by setting a query as the title. Our framework enables MRCSum to consider the semantic coherence and relevance of summary sentences in relation to the overall content. In particular, when a title is not available, we generate a title-like query, which is expected to achieve the same effect as a title. Our title-like query consists of the topic and keywords to serve as information on the main topic or theme of the document. We conduct experiments in both Korean and English languages, evaluating the performance of MRCSum on datasets comprising both long and short summaries. Our results demonstrate the effectiveness of MRCSum in extractive summarization, showcasing its ability to generate concise and informative summaries with or without explicit titles. Furthermore, our MRCSum outperforms existing models by capturing the essence of the document content and producing more coherent summaries.</abstract>
      <url hash="c4e7a27c">2024.lrec-main.1406</url>
      <bibkey>kim-etal-2024-title-based</bibkey>
    </paper>
    <paper id="1407">
      <title><fixed-case>TMFN</fixed-case>: A Target-oriented Multi-grained Fusion Network for End-to-end Aspect-based Multimodal Sentiment Analysis</title>
      <author><first>Di</first><last>Wang</last></author>
      <author><first>Yuzheng</first><last>He</last></author>
      <author><first>Xiao</first><last>Liang</last></author>
      <author><first>Yumin</first><last>Tian</last></author>
      <author><first>Shaofeng</first><last>Li</last></author>
      <author><first>Lin</first><last>Zhao</last></author>
      <pages>16187–16197</pages>
      <abstract>End-to-end multimodal aspect-based sentiment analysis (MABSA) combines multimodal aspect terms extraction (MATE) with multimodal aspect sentiment classification (MASC), aiming to simultaneously extract aspect words and classify the sentiment polarity of each aspect. However, existing MABSA methods have overlooked two issues: (i) They only focus on fusing image regional information and textual words for two subtasks of MABSA. Whereas, MATE subtask relies more on global image information to assist in obtaining the quantity and attributes of aspects. Ignoring the integration with global information may affect the performance of MABSA methods. (ii) They fail to take advantage of target information. Nevertheless, the fine-grained details of targets are important for classifying sentiments of aspects. To solve these problems, we propose a Target-oriented Multi-grained Fusion Network(TMFN). It fuses text information with global coarse-grained image information for MATE subtask and with fine-grained image information for MASC subtask. In addition, a target-oriented feature alignment (TOFA) module is designed to enhance target-related information in image features with target details. In such a way, image features will contain more target emotional-related information which is beneficial to sentiment classification. Extensive experiments show that our method outperforms state-of-the-art methods on two benchmark datasets.</abstract>
      <url hash="9439a911">2024.lrec-main.1407</url>
      <bibkey>wang-etal-2024-tmfn-target</bibkey>
    </paper>
    <paper id="1408">
      <title>To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case Study in <fixed-case>J</fixed-case>apanese</title>
      <author><first>Yukiko</first><last>Ishizuki</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last></author>
      <author><first>Yuichiroh</first><last>Matsubayashi</last></author>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>16198–16210</pages>
      <abstract>Speakers sometimes omit certain arguments of a predicate in a sentence; such omission is especially frequent in pro-drop languages. This study addresses a question about ellipsis—what can explain the native speakers’ ellipsis decisions?—motivated by the interest in human discourse processing and writing assistance for this choice. To this end, we first collect large-scale human annotations of whether and why a particular argument should be omitted across over 2,000 data points in the balanced corpus of Japanese, a prototypical pro-drop language. The data indicate that native speakers overall share common criteria for such judgments and further clarify their quantitative characteristics, e.g., the distribution of related linguistic factors in the balanced corpus. Furthermore, the performance of the language model–based argument ellipsis judgment model is examined, and the gap between the systems’ prediction and human judgments in specific linguistic aspects is revealed. We hope our fundamental resource encourages further studies on natural human ellipsis judgment.</abstract>
      <url hash="80b82133">2024.lrec-main.1408</url>
      <bibkey>ishizuki-etal-2024-drop-drop</bibkey>
    </paper>
    <paper id="1409">
      <title>To Err Is Human, How about Medical Large Language Models? Comparing Pre-trained Language Models for Medical Assessment Errors and Reliability</title>
      <author><first>Wen-wai</first><last>Yim</last></author>
      <author><first>Yujuan</first><last>Fu</last></author>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Meliha</first><last>Yetisgen</last></author>
      <pages>16211–16223</pages>
      <abstract>Unpredictability, especially unpredictability with unknown error characteristics, is a highly undesirable trait, particularly in medical patient care applications. Although large pre-trained language models (LLM) have been applied to a variety of unseen tasks with highly competitive and successful results, their sensitivity to language inputs and resulting performance variability is not well-studied. In this work, we test state-of-the-art pre-trained language models from a variety of families to characterize their error generation and reliability in medical assessment ability. Particularly, we experiment with general medical assessment multiple choice tests, as well as their open-ended and true-false alternatives. We also profile model consistency, error agreements with each other and to humans; and finally, quantify their ability to recover and explain errors. The findings in this work can be used to give further information about medical models so that modelers can make better-informed decisions rather than relying on standalone performance metrics alone.</abstract>
      <url hash="ca328adf">2024.lrec-main.1409</url>
      <bibkey>yim-etal-2024-err-human</bibkey>
    </paper>
    <paper id="1410">
      <title>Token-length Bias in Minimal-pair Paradigm Datasets</title>
      <author><first>Naoya</first><last>Ueda</last></author>
      <author><first>Masato</first><last>Mita</last></author>
      <author><first>Teruaki</first><last>Oka</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>16224–16236</pages>
      <abstract>Minimal-pair paradigm datasets have been used as benchmarks to evaluate the linguistic knowledge of models and provide an unsupervised method of acceptability judgment. The model performances are evaluated based on the percentage of minimal pairs in the MPP dataset where the model assigns a higher sentence log-likelihood to an acceptable sentence than to an unacceptable sentence. Each minimal pair in MPP datasets is controlled to align the number of words per sentence because the sentence length affects the sentence log-likelihood. However, aligning the number of words may be insufficient because recent language models tokenize sentences with subwords. Tokenization may cause a token length difference in minimal pairs, introducing token-length bias that skews the evaluation results. This study demonstrates that MPP datasets suffer from token-length bias and fail to evaluate the linguistic knowledge of a language model correctly. The results proved that sentences with a shorter token length would likely be assigned a higher log-likelihood regardless of their acceptability, which becomes problematic when comparing models with different tokenizers. To address this issue, we propose a debiased minimal pair generation method, allowing MPP datasets to measure language ability correctly and provide comparable results for all models.</abstract>
      <url hash="fcffaf19">2024.lrec-main.1410</url>
      <bibkey>ueda-etal-2024-token-length</bibkey>
    </paper>
    <paper id="1411">
      <title>To Learn or Not to Learn: Replaced Token Detection for Learning the Meaning of Negation</title>
      <author><first>Gunjan</first><last>Bhattarai</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <pages>16237–16250</pages>
      <abstract>State-of-the-art language models perform well on a variety of language tasks, but they continue to struggle with understanding negation cues in tasks like natural language inference (NLI). Inspired by Hossain et al. (2020), who show under-representation of negation in language model pretraining datasets, we experiment with additional pretraining with negation data for which we introduce two new datasets. We also introduce a new learning strategy for negation building on ELECTRA’s (Clark et al., 2020) replaced token detection objective. We find that continuing to pretrain ELECTRA-Small’s discriminator leads to substantial gains on a variant of RTE (Recognizing Textual Entailment) with additional negation. On SNLI (Stanford NLI) (Bowman et al., 2015), there are no gains due to the extreme under-representation of negation in the data. Finally, on MNLI (Multi-NLI) (Williams et al., 2018), we find that performance on negation cues is primarily stymied by neutral-labeled examples.</abstract>
      <url hash="7113a887">2024.lrec-main.1411</url>
      <bibkey>bhattarai-erk-2024-learn-learn</bibkey>
    </paper>
    <paper id="1412">
      <title><fixed-case>T</fixed-case>o<fixed-case>NER</fixed-case>: Type-oriented Named Entity Recognition with Generative Language Model</title>
      <author><first>Guochao</first><last>Jiang</last></author>
      <author><first>Ziqin</first><last>Luo</last></author>
      <author><first>Yuchen</first><last>Shi</last></author>
      <author><first>Dixuan</first><last>Wang</last></author>
      <author><first>Jiaqing</first><last>Liang</last></author>
      <author><first>Deqing</first><last>Yang</last></author>
      <pages>16251–16262</pages>
      <abstract>In recent years, the fine-tuned generative models have been proven more powerful than the previous tagging-based or span-based models on named entity recognition (NER) task. It has also been found that the information related to entities, such as entity types, can prompt a model to achieve NER better. However, it is not easy to determine the entity types indeed existing in the given sentence in advance, and inputting too many potential entity types would distract the model inevitably. To exploit entity types’ merit on promoting NER task, in this paper we propose a novel NER framework, namely ToNER based on a generative model. In ToNER, a type matching model is proposed at first to identify the entity types most likely to appear in the sentence. Then, we append a multiple binary classification task to fine-tune the generative model’s encoder, so as to generate the refined representation of the input sentence. Moreover, we add an auxiliary task for the model to discover the entity types which further fine-tunes the model to output more accurate results. Our extensive experiments on some NER benchmarks verify the effectiveness of our proposed strategies in ToNER that are oriented towards entity types’ exploitation.</abstract>
      <url hash="2521be7d">2024.lrec-main.1412</url>
      <bibkey>jiang-etal-2024-toner-type</bibkey>
    </paper>
    <paper id="1413">
      <title><fixed-case>T</fixed-case>ool<fixed-case>R</fixed-case>erank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval</title>
      <author><first>Yuanhang</first><last>Zheng</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>16263–16273</pages>
      <abstract>Tool learning aims to extend the capabilities of large language models (LLMs) with external tools. A major challenge in tool learning is how to support a large number of tools, including unseen tools. To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query. However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval. Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results. Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries. Experimental results show that ToolRerank can improve the quality of the retrieval results, leading to better execution results generated by the LLM.</abstract>
      <url hash="274fd89c">2024.lrec-main.1413</url>
      <bibkey>zheng-etal-2024-toolrerank-adaptive</bibkey>
    </paper>
    <paper id="1414">
      <title>Topic Classification and Headline Generation for <fixed-case>M</fixed-case>altese Using a Public News Corpus</title>
      <author><first>Amit Kumar</first><last>Chaudhary</last></author>
      <author><first>Kurt</first><last>Micallef</last></author>
      <author><first>Claudia</first><last>Borg</last></author>
      <pages>16274–16281</pages>
      <abstract>The development of NLP tools for low-resource languages is impeded by the lack of data. While recent unsupervised pre-training approaches ease this requirement, the need for labelled data is crucial to progress the development of such tools. Moreover, publicly available datasets for such languages typically cover low-level syntactic tasks. In this work, we introduce new semantic datasets for Maltese generated automatically using associated metadata from a corpus in the news domain. The datasets are a news tag multi-label classification and a news abstractive summarisation task by generating its title. We also present an evaluation using publicly available models as baselines. Our results show that current models are lacking the semantic knowledge required to solve such tasks, shedding light on the need to use better modelling approaches for Maltese.</abstract>
      <url hash="64635b77">2024.lrec-main.1414</url>
      <bibkey>chaudhary-etal-2024-topic-classification</bibkey>
    </paper>
    <paper id="1415">
      <title>Topic-Controllable Summarization: Topic-Aware Evaluation and Transformer Methods</title>
      <author><first>Tatiana</first><last>Passali</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>16282–16292</pages>
      <abstract>Topic-controllable summarization is an emerging research area with a wide range of potential applications. However, existing approaches suffer from significant limitations. For example, the majority of existing methods built upon recurrent architectures, which can significantly limit their performance compared to more recent Transformer-based architectures, while they also require modifications to the model’s architecture for controlling the topic. At the same time, there is currently no established evaluation metric designed specifically for topic-controllable summarization. This work proposes a new topic-oriented evaluation measure to automatically evaluate the generated summaries based on the topic affinity between the generated summary and the desired topic. The reliability of the proposed measure is demonstrated through appropriately designed human evaluation. In addition, we adapt topic embeddings to work with powerful Transformer architectures and propose a novel and efficient approach for guiding the summary generation through control tokens. Experimental results reveal that control tokens can achieve better performance compared to more complicated embedding-based approaches while also being significantly faster.</abstract>
      <url hash="57c444ff">2024.lrec-main.1415</url>
      <bibkey>passali-tsoumakas-2024-topic-controllable</bibkey>
    </paper>
    <paper id="1416">
      <title>Topic Detection and Tracking with Time-Aware Document Embeddings</title>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Doug</first><last>Beeferman</last></author>
      <author><first>Weiquan</first><last>Mao</last></author>
      <author><first>Deb</first><last>Roy</last></author>
      <pages>16293–16303</pages>
      <abstract>The time at which a message is communicated is a vital piece of metadata in many real-world natural language processing tasks such as Topic Detection and Tracking (TDT). TDT systems aim to cluster a corpus of news articles by event, and in that context, stories that describe the same event are likely to have been written at around the same time. Prior work on time modeling for TDT takes this into account, but does not well capture how time interacts with the semantic nature of the event. For example, stories about a tropical storm are likely to be written within a short time interval, while stories about a movie release may appear over weeks or months. In our work, we design a neural method that fuses temporal and textual information into a single representation of news documents for event detection. We fine-tune these time-aware document embeddings with a triplet loss architecture, integrate the model into downstream TDT systems, and evaluate the systems on two benchmark TDT data sets in English. In the retrospective setting, we apply clustering algorithms to the time-aware embeddings and show substantial improvements over baselines on the News2013 data set. In the online streaming setting, we add our document encoder to an existing state-of-the-art TDT pipeline and demonstrate that it can benefit the overall performance. We conduct ablation studies on the time representation and fusion algorithm strategies, showing that our proposed model outperforms alternative strategies. Finally, we probe the model to examine how it handles recurring events more effectively than previous TDT systems.</abstract>
      <url hash="9aef255f">2024.lrec-main.1416</url>
      <bibkey>jiang-etal-2024-topic-detection</bibkey>
    </paper>
    <paper id="1417">
      <title><fixed-case>T</fixed-case>opic<fixed-case>D</fixed-case>iff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection</title>
      <author><first>Jiamin</first><last>Luo</last></author>
      <author><first>Jingjing</first><last>Wang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>16304–16314</pages>
      <abstract>Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of TopicDiff in capturing such information. Furthermore, we observe an interesting finding that the topic information in acoustic and vision is more discriminative and robust compared to the language.</abstract>
      <url hash="bbbb619a">2024.lrec-main.1417</url>
      <bibkey>luo-etal-2024-topicdiff-topic</bibkey>
    </paper>
    <paper id="1418">
      <title>Topics as Entity Clusters: Entity-based Topics from Large Language Models and Graph Neural Networks</title>
      <author><first>Manuel V.</first><last>Loureiro</last></author>
      <author><first>Steven</first><last>Derby</last></author>
      <author><first>Tri Kurniawan</first><last>Wijaya</last></author>
      <pages>16315–16330</pages>
      <abstract>Topic models aim to reveal latent structures within a corpus of text, typically through the use of term-frequency statistics over bag-of-words representations from documents. In recent years, conceptual entities — interpretable, language-independent features linked to external knowledge resources — have been used in place of word-level tokens, as words typically require extensive language processing with a minimal assurance of interpretability. However, current literature is limited when it comes to exploring purely entity-driven neural topic modeling. For instance, despite the advantages of using entities for eliciting thematic structure, it is unclear whether current techniques are compatible with these sparsely organised, information-dense conceptual units. In this work, we explore entity-based neural topic modeling and propose a novel topic clustering approach using bimodal vector representations of entities. Concretely, we extract these latent representations from large language models and graph neural networks trained on a knowledge base of symbolic relations, in order to derive the most salient aspects of these conceptual units. Analysis of coherency metrics confirms that our approach is better suited to working with entities in comparison to state-of-the-art models, particularly when using graph-based embeddings trained on a knowledge base.</abstract>
      <url hash="b2e28af6">2024.lrec-main.1418</url>
      <bibkey>loureiro-etal-2024-topics-entity</bibkey>
    </paper>
    <paper id="1419">
      <title>To Share or Not to Share: What Risks Would Laypeople Accept to Give Sensitive Data to Differentially-Private <fixed-case>NLP</fixed-case> Systems?</title>
      <author><first>Christopher</first><last>Weiss</last></author>
      <author><first>Frauke</first><last>Kreuter</last></author>
      <author><first>Ivan</first><last>Habernal</last></author>
      <pages>16331–16342</pages>
      <abstract>Although the NLP community has adopted central differential privacy as a go-to framework for privacy-preserving model training or data sharing, the choice and interpretation of the key parameter, privacy budget <tex-math>\varepsilon</tex-math> that governs the strength of privacy protection, remains largely arbitrary. We argue that determining the <tex-math>\varepsilon</tex-math> value should not be solely in the hands of researchers or system developers, but must also take into account the actual people who share their potentially sensitive data. In other words: Would <i>you</i> share your instant messages for <tex-math>\varepsilon</tex-math> of 10? We address this research gap by designing, implementing, and conducting a behavioral experiment (311 lay participants) to study the behavior of people in uncertain decision-making situations with respect to privacy-threatening situations. Framing the risk perception in terms of two realistic NLP scenarios and using a vignette behavioral study help us determine what <tex-math>\varepsilon</tex-math> thresholds would lead lay people to be willing to share sensitive textual data – to our knowledge, the first study of its kind.</abstract>
      <url hash="b0ce2d56">2024.lrec-main.1419</url>
      <bibkey>weiss-etal-2024-share-share</bibkey>
    </paper>
    <paper id="1420">
      <title>Towards a Corpus of Spoken <fixed-case>M</fixed-case>altese: Korpus tal-Malti Mitkellem, <fixed-case>KMM</fixed-case></title>
      <author><first>Alexandra (Sandra)</first><last>Vella</last></author>
      <author><first>Sarah</first><last>Agius</last></author>
      <author><first>Aiden</first><last>Williams</last></author>
      <author><first>Claudia</first><last>Borg</last></author>
      <pages>16343–16352</pages>
      <abstract>This paper presents the rationale for a “dedicated” corpus of spoken Maltese, Korpus tal-Malti Mitkellem, KMM, ‘Corpus of Spoken Maltese’, based on the concept of a gold-standard Core collection. The Core collection is designed to cater to as wide a variety of user needs as possible whilst respecting basic principles governing corpus design, such as representativeness and balance, and delivering high quality in terms of both audio quality and annotations. An overview is provided of the composition of the current Core corpus of around 20 hours of data and of the human annotation effort involved. We also carry out a small qualitative analysis of the output of a Maltese ASR system and compare it to the human annotators’ output. Initial results are promising, showing that the ASR is robust enough to generate first-pass texts for annotators to work on, thus reducing the human effort, and consequently, the cost involved.</abstract>
      <url hash="2535f6c3">2024.lrec-main.1420</url>
      <bibkey>vella-etal-2024-towards-corpus</bibkey>
    </paper>
    <paper id="1421">
      <title>Towards a <fixed-case>D</fixed-case>anish Semantic Reasoning Benchmark - Compiled from Lexical-Semantic Resources for Assessing Selected Language Understanding Capabilities of Large Language Models</title>
      <author><first>Bolette</first><last>Pedersen</last></author>
      <author><first>Nathalie</first><last>Sørensen</last></author>
      <author><first>Sussi</first><last>Olsen</last></author>
      <author><first>Sanni</first><last>Nimb</last></author>
      <author><first>Simon</first><last>Gray</last></author>
      <pages>16353–16363</pages>
      <abstract>We present the first version of a semantic reasoning benchmark for Danish compiled semi-automatically from a number of human-curated lexical-semantic resources, which function as our gold standard. Taken together, the datasets constitute a benchmark for assessing selected language understanding capacities of large language models (LLMs) for Danish. This first version comprises 25 datasets across 6 different tasks and include 3,800 test instances. Although still somewhat limited in size, we go beyond comparative evaluation datasets for Danish by including both negative and contrastive examples as well as low-frequent vocabulary; aspects which tend to challenge current LLMs when based substantially on language transfer. The datasets focus on features such as semantic inference and entailment, similarity, relatedness, and ability to disambiguate words in context. We use ChatGPT to assess to which degree our datasets challenge the ceiling performance of state-of-the-art LLMs, average performance being relatively high with an average accuracy of 0.6 on ChatGPT 3.5 turbo and 0.8 on ChatGPT 4.0.</abstract>
      <url hash="a603477a">2024.lrec-main.1421</url>
      <bibkey>pedersen-etal-2024-towards-danish</bibkey>
    </paper>
    <paper id="1422">
      <title>Towards a Framework for Evaluating Explanations in Automated Fact Verification</title>
      <author><first>Neema</first><last>Kotonya</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <pages>16364–16377</pages>
      <abstract>As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater. A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions. In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically. We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure). Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures.</abstract>
      <url hash="6cdf0c5a">2024.lrec-main.1422</url>
      <bibkey>kotonya-toni-2024-towards-framework</bibkey>
    </paper>
    <paper id="1423">
      <title>Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data</title>
      <author><first>Shinka</first><last>Mori</last></author>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Andrew</first><last>Lee</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>16378–16391</pages>
      <abstract>Synthetic data generation has the potential to impact applications and domains with scarce data. However, before such data is used for sensitive tasks such as mental health, we need an understanding of how different demographics are represented in it. In our paper, we analyze the potential of producing synthetic data using GPT-3 by exploring the various stressors it attributes to different race and gender combinations, to provide insight for future researchers looking into using LLMs for data generation. Using GPT-3, we develop HeadRoom, a synthetic dataset of 3,120 posts about depression-triggering stressors, by controlling for race, gender, and time frame (before and after COVID-19). Using this dataset, we conduct semantic and lexical analyses to (1) identify the predominant stressors for each demographic group; and (2) compare our synthetic data to a human-generated dataset. We present the procedures to generate queries to develop depression data using GPT-3, and conduct analyzes to uncover the types of stressors it assigns to demographic groups, which could be used to test the limitations of LLMs for synthetic data generation for depression data. Our findings show that synthetic data mimics some of the human-generated data distribution for the predominant depression stressors across diverse demographics.</abstract>
      <url hash="b4cefd1e">2024.lrec-main.1423</url>
      <bibkey>mori-etal-2024-towards-algorithmic</bibkey>
    </paper>
    <paper id="1424">
      <title>Towards an Ideal Tool for Learner Error Annotation</title>
      <author><first>Špela</first><last>Arhar Holdt</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Iztok</first><last>Kosem</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <pages>16392–16398</pages>
      <abstract>Annotation and analysis of corrections in learner corpora have always presented technical challenges, mainly on account of the fact that until now there has not been any standard tool available, and that original and corrected versions of texts have been mostly stored together rather than treated as individual texts. In this paper, we present CJVT Svala 1.0, the Slovene version of the SVALA tool, which was originally used for the annotation of Swedish learner language. The localisation into Slovene resulted in the development of several new features in SVALA such as the support for multiple annotation systems, localisation into other languages, and the support for more complex annotation systems. Adopting the parallel aligned approach to text visualisation and annotation, as well as storing the data, combined with the tool supporting this, i.e. SVALA, are proposed as new standards in Learner Corpus Research.</abstract>
      <url hash="37d11473">2024.lrec-main.1424</url>
      <bibkey>arhar-holdt-etal-2024-towards-ideal</bibkey>
    </paper>
    <paper id="1425">
      <title>Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches</title>
      <author><first>Deepak</first><last>Gupta</last></author>
      <author><first>Kush</first><last>Attal</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last></author>
      <pages>16399–16411</pages>
      <abstract>The increase in the availability of online videos has transformed the way we access information and knowledge. A growing number of individuals now prefer instructional videos as they offer a series of step-by-step procedures to accomplish particular tasks. Instructional videos from the medical domain may provide the best possible visual answers to first aid, medical emergency, and medical education questions. This paper focuses on answering health-related questions asked by health consumers by providing visual answers from medical videos. The scarcity of large-scale datasets in the medical domain is a key challenge that hinders the development of applications that can help the public with their health-related questions. To address this issue, we first proposed a pipelined approach to create two large-scale datasets: HealthVidQA-CRF and HealthVidQA-Prompt. Leveraging the datasets, we developed monomodal and multimodal approaches that can effectively provide visual answers from medical videos to natural language questions. We conducted a comprehensive analysis of the results and outlined the findings, focusing on the impact of the created datasets on model training and the significance of visual features in enhancing the performance of the monomodal and multi-modal approaches for medical visual answer localization task.</abstract>
      <url hash="53d613f6">2024.lrec-main.1425</url>
      <bibkey>gupta-etal-2024-towards-answering</bibkey>
    </paper>
    <paper id="1426">
      <title>Towards a Unified Taxonomy of Deep Syntactic Relations</title>
      <author><first>Kira</first><last>Droganova</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>16412–16421</pages>
      <abstract>This paper analyzes multiple deep-syntactic frameworks with the goal of creating a proposal for a set of universal semantic role labels. The proposal examines various theoretic linguistic perspectives and focuses on Meaning-Text Theory and Functional Generative Description frameworks and PropBank. The research is based on the data from four Indo-European and one Uralic language – Spanish and Catalan (Taulé et al., 2011), Czech (Hajič et al., 2017), English (Hajič et al., 2012), and Finnish (Haverinen et al., 2015). Updated datasets with the new universal semantic role labels are now publicly available as a result of our work. Nevertheless, our proposal is oriented towards Universal Dependencies (UD) (de Marneffe et al., 2021) and our ultimate goal is to apply a subset of the universal labels to the full UD data.</abstract>
      <url hash="6980e2a6">2024.lrec-main.1426</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fe193ab8">2024.lrec-main.1426.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>droganova-zeman-2024-towards-unified</bibkey>
    </paper>
    <paper id="1427">
      <title>Towards Autonomous Tool Utilization in Language Models: A Unified, Efficient and Scalable Framework</title>
      <author><first>Zhi</first><last>Li</last></author>
      <author><first>Yicheng</first><last>Li</last></author>
      <author><first>Hequan</first><last>Ye</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <pages>16422–16432</pages>
      <abstract>In recent research, significant advancements have been achieved in tool learning for large language models. Looking towards future advanced studies, the issue of fully autonomous tool utilization is particularly intriguing: given only a query, language models can autonomously decide whether to employ a tool, which specific tool to select, and how to utilize these tools, all without needing any tool-specific prompts within the context. To achieve this, we introduce a unified, efficient, and scalable framework for fine-tuning language models. Based on the degree of tool dependency, we initially categorize queries into three distinct types. By transforming the entire process into a sequential decision-making problem through conditional probability decomposition, our approach unifies the three types and autoregressively generates decision processes. Concurrently, we’ve introduced an “instruct, execute, and reformat” strategy specifically designed for efficient data annotation. Through end-to-end training on the annotated dataset comprising 26 diverse APIs, the model demonstrates a level of self-awareness, automatically seeking tool assistance when necessary. It significantly surpasses original instruction-tuned open-source language models and GPT-3.5/4 on multiple evaluation metrics. To address real-world scalability needs, we’ve enhanced our framework with a dynamic rehearsal strategy for continual learning, proven to require minimal new annotations to exhibit remarkable performance.</abstract>
      <url hash="56fe13e6">2024.lrec-main.1427</url>
      <bibkey>li-etal-2024-towards-autonomous</bibkey>
    </paper>
    <paper id="1428">
      <title>Towards a Zero-Data, Controllable, Adaptive Dialog System</title>
      <author><first>Dirk</first><last>Väth</last></author>
      <author><first>Lindsey</first><last>Vanderlyn</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>16433–16449</pages>
      <abstract>Conversational Tree Search (Väth et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.</abstract>
      <url hash="fafdaa88">2024.lrec-main.1428</url>
      <bibkey>vath-etal-2024-towards-zero</bibkey>
    </paper>
    <paper id="1429">
      <title>Towards Building the <fixed-case>LEMI</fixed-case> Readability Platform for Children’s Literature in the <fixed-case>R</fixed-case>omanian Language</title>
      <author><first>Madalina</first><last>Chitez</last></author>
      <author><first>Mihai</first><last>Dascalu</last></author>
      <author><first>Aura Cristina</first><last>Udrea</last></author>
      <author><first>Cosmin</first><last>Strilețchi</last></author>
      <author><first>Karla</first><last>Csürös</last></author>
      <author><first>Roxana</first><last>Rogobete</last></author>
      <author><first>Alexandru</first><last>Oravițan</last></author>
      <pages>16450–16456</pages>
      <abstract>Readability is a crucial characteristic of texts, greatly influencing comprehension and reading efficacy. Unfortunately, limited research is available for less-resourced languages, especially for young populations where its impact is even higher. This paper introduces a new readability tool for children’s literature in the Romanian language, explicitly targeting primary school students aged 7-11. The tool consists of a digital repository of school reading texts (self-compiled corpus) and a text analysis interface that generates automatic readability reports for uploaded short texts. The methodology involves extracting, testing, and calibrating a readability formula for Romanian using the children’s literature corpus. Related work on readability and readability tools is discussed, followed by a description of the children’s literature corpus and the platform functionalities. The first steps are presented towards validating the readability formula for children’s literature in Romanian using the ReaderBench framework, while calibration variables relevant to the Romanian language and children’s literature are examined. Currently, no existing platform integrates a research-based readability formula for the Romanian language, making this tool unique. Overall, this research contributes to applied corpus linguistics and Digital Humanities studies and offers a valuable resource for educators, parents, and children in accessing age-appropriate and readable texts.</abstract>
      <url hash="9bdf6abb">2024.lrec-main.1429</url>
      <bibkey>chitez-etal-2024-towards-building</bibkey>
    </paper>
    <paper id="1430">
      <title>Towards Comprehensive Language Analysis for Clinically Enriched Spontaneous Dialogue</title>
      <author><first>Baris</first><last>Karacan</last></author>
      <author><first>Ankit</first><last>Aich</last></author>
      <author><first>Avery</first><last>Quynh</last></author>
      <author><first>Amy</first><last>Pinkham</last></author>
      <author><first>Philip</first><last>Harvey</last></author>
      <author><first>Colin</first><last>Depp</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>16457–16472</pages>
      <abstract>Contemporary NLP has rapidly progressed from feature-based classification to fine-tuning and prompt-based techniques leveraging large language models. Many of these techniques remain understudied in the context of real-world, clinically enriched spontaneous dialogue. We fill this gap by systematically testing the efficacy and overall performance of a wide variety of NLP techniques ranging from feature-based to in-context learning on transcribed speech collected from patients with bipolar disorder, schizophrenia, and healthy controls taking a focused, clinically-validated language test. We observe impressive utility of a range of feature-based and language modeling techniques, finding that these approaches may provide a plethora of information capable of upholding clinical truths about these subjects. Building upon this, we establish pathways for future research directions in automated detection and understanding of psychiatric conditions.</abstract>
      <url hash="87c2e1aa">2024.lrec-main.1430</url>
      <bibkey>karacan-etal-2024-towards-comprehensive</bibkey>
    </paper>
    <paper id="1431">
      <title>Towards Cost-effective Multi-style Conversations: A Pilot Study in Task-oriented Dialogue Generation</title>
      <author><first>Tiziano</first><last>Labruna</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>16473–16479</pages>
      <abstract>Conversations exhibit significant variation when different styles are employed by participants, often leading to subpar performance when a dialogue model is exclusively trained on single-style datasets. We present a cost-effective methodology for generating multi-style conversations, which can be used in the development of conversational agents. This methodology only assumes the availability of a conversational domain, such as a knowledge base, and leverages the generative capabilities of large language models. In a pilot study focused on the generation aspect of task-oriented dialogues, we extended the well-known MultiWOZ dataset to encompass multi-style variations. Our findings highlight two key experimental outcomes: (i) these novel resources pose challenges for current single-style models, and (ii) multi-style resources enhance the dialogue model’s resilience to stylistic variations.</abstract>
      <url hash="52919493">2024.lrec-main.1431</url>
      <bibkey>labruna-magnini-2024-towards-cost</bibkey>
    </paper>
    <paper id="1432">
      <title>Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification</title>
      <author><first>Artem</first><last>Abzaliev</last></author>
      <author><first>Humberto</first><last>Perez-Espinosa</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>16480–16486</pages>
      <abstract>Similar to humans, animals make extensive use of verbal and non-verbal forms of communication, including a large range of audio signals. In this paper, we address dog vocalizations and explore the use of self-supervised speech representation models pre-trained on human speech to address dog bark classification tasks that find parallels in human-centered tasks in speech recognition. We specifically address four tasks: dog recognition, breed identification, gender classification, and context grounding. We show that using speech embedding representations significantly improves over simpler classification baselines. Further, we also find that models pre-trained on large human speech acoustics can provide additional performance boosts on several tasks.</abstract>
      <url hash="198b643f">2024.lrec-main.1432</url>
      <bibkey>abzaliev-etal-2024-towards-dog</bibkey>
    </paper>
    <paper id="1433">
      <title>Towards Equitable Natural Language Understanding Systems for Dialectal Cohorts: Debiasing Training Data</title>
      <author><first>Khadige</first><last>Abboud</last></author>
      <author><first>Gokmen</first><last>Oz</last></author>
      <pages>16487–16499</pages>
      <abstract>Despite being widely spoken, dialectal variants of languages are frequently considered low in resources due to lack of writing standards and orthographic inconsistencies. As a result, training natural language understanding (NLU) systems relies primarily on standard language resources leading to biased and inequitable NLU technology that underserves dialectal speakers. In this paper, we propose to address this problem through a framework composed of a dialect identification model that is used to obtain targeted training data augmentation for under-represented dialects, in an effort to debias NLU model for dialectal cohorts in NLU systems. We conduct experiments on two dialect rich non-English languages: Arabic and German, using large-scale commercial NLU datasets as well as open-source datasets. Results show that such framework can provide insights on dialect disparity in real-world NLU systems and targeted data argumentation can help narrow the model’s performance gap between standard language speakers and dialect speakers.</abstract>
      <url hash="806dc1ac">2024.lrec-main.1433</url>
      <bibkey>abboud-oz-2024-towards-equitable</bibkey>
    </paper>
    <paper id="1434">
      <title>Towards Explainability and Fairness in <fixed-case>S</fixed-case>wiss Judgement Prediction: Benchmarking on a Multilingual Dataset</title>
      <author><first>Santosh</first><last>T.y.s.s.</last></author>
      <author><first>Nina</first><last>Baumgartner</last></author>
      <author><first>Matthias</first><last>Stürmer</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <author><first>Joel</first><last>Niklaus</last></author>
      <pages>16500–16513</pages>
      <abstract>The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that ‘support’ and ‘oppose’ judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance, underscoring the significance of evaluating models from an explainability perspective. Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models’ biases.</abstract>
      <url hash="cfd47593">2024.lrec-main.1434</url>
      <bibkey>t-y-s-s-etal-2024-towards-explainability</bibkey>
    </paper>
    <paper id="1435">
      <title>Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation</title>
      <author><first>Prashant</first><last>Krishnan</last></author>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Yangkun</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>16514–16526</pages>
      <abstract>Recent advances of incorporating layout information, typically bounding box coordinates, into pre-trained language models have achieved significant performance in entity recognition from document images. Using coordinates can easily model the position of each token, but they are sensitive to manipulations in document images (e.g., shifting, rotation or scaling) which are common in real scenarios. Such limitation becomes even worse when the training data is limited in few-shot settings. In this paper, we propose a novel framework, LAGER, which leverages the topological adjacency relationship among the tokens through learning their relative layout information with graph neural networks. Specifically, we consider the tokens in the documents as nodes and formulate the edges based on the topological heuristics. Such adjacency graphs are invariant to affine transformations, making it robust to the common image manipulations. We incorporate these graphs into the pre-trained language model by adding graph neural network layers on top of the language model embeddings. Extensive experiments on two benchmark datasets show that LAGER significantly outperforms strong baselines under different few-shot settings and also demonstrate better robustness to manipulations.</abstract>
      <url hash="7097748a">2024.lrec-main.1435</url>
      <bibkey>krishnan-etal-2024-towards-shot</bibkey>
    </paper>
    <paper id="1436">
      <title>Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation</title>
      <author><first>Zhouhao</first><last>Sun</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Bibo</first><last>Cai</last></author>
      <author><first>Jinglong</first><last>Gao</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>16527–16538</pages>
      <abstract>Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system’s completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system outperforms previous works by achieving state-of-the-art performances in complex scenarios while maintaining performances in simple scenarios. Besides, we observe that GFaiR is faithful to its reasoning process.</abstract>
      <url hash="1b78d288">2024.lrec-main.1436</url>
      <bibkey>sun-etal-2024-towards-generalizable</bibkey>
    </paper>
    <paper id="1437">
      <title>Towards Graph-hop Retrieval and Reasoning in Complex Question Answering over Textual Database</title>
      <author><first>Minjun</first><last>Zhu</last></author>
      <author><first>Yixuan</first><last>Weng</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Liu</last></author>
      <author><first>Yang jun</first><last>Jun</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>16539–16549</pages>
      <abstract>In textual question answering (TQA) systems, complex questions often require retrieving multiple textual fact chains with multiple reasoning steps. While existing benchmarks are limited to single-chain or single-hop retrieval scenarios. In this paper, we propose to conduct Graph-Hop —— a novel multi-chains and multi-hops retrieval and reasoning paradigm in complex question answering. We construct a new benchmark called ReasonGraphQA, which provides explicit and fine-grained evidence graphs for complex question to support comprehensive and detailed reasoning. In order to further study how graph-based evidential reasoning can be performed, we explore what form of Graph-Hop works best for generating textual evidence explanations in knowledge reasoning and question answering. We have thoroughly evaluated existing evidence retrieval and reasoning models on the ReasonGraphQA. Experiments highlight Graph-Hop is a promising direction for answering complex questions, but it still has certain limitations. We have further studied mitigation strategies to meet these challenges and discuss future directions.</abstract>
      <url hash="2bd8977e">2024.lrec-main.1437</url>
      <attachment type="OptionalSupplementaryMaterial" hash="20ef04d3">2024.lrec-main.1437.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>zhu-etal-2024-towards-graph</bibkey>
    </paper>
    <paper id="1438">
      <title>Towards Human-aligned Evaluation for Linear Programming Word Problems</title>
      <author><first>Linzi</first><last>Xing</last></author>
      <author><first>Xinglu</first><last>Wang</last></author>
      <author><first>Yuxi</first><last>Feng</last></author>
      <author><first>Zhenan</first><last>Fan</last></author>
      <author><first>Jing</first><last>Xiong</last></author>
      <author><first>Zhijiang</first><last>Guo</last></author>
      <author><first>Xiaojin</first><last>Fu</last></author>
      <author><first>Rindra</first><last>Ramamonjison</last></author>
      <author><first>Mahdi</first><last>Mostajabdaveh</last></author>
      <author><first>Xiongwei</first><last>Han</last></author>
      <author><first>Zirui</first><last>Zhou</last></author>
      <author><first>Yong</first><last>Zhang</last></author>
      <pages>16550–16556</pages>
      <abstract>Math Word Problem (MWP) is a crucial NLP task aimed at providing solutions for given mathematical descriptions. A notable sub-category of MWP is the Linear Programming Word Problem (LPWP), which holds significant relevance in real-world decision-making and operations research. While the recent rise of generative large language models (LLMs) has brought more advanced solutions to LPWPs, existing evaluation methodologies for this task still diverge from human judgment and face challenges in recognizing mathematically equivalent answers. In this paper, we introduce a novel evaluation metric rooted in graph edit distance, featuring benefits such as permutation invariance and more accurate program equivalence identification. Human evaluations empirically validate the superior efficacy of our proposed metric when particularly assessing LLM-based solutions for LPWP.</abstract>
      <url hash="bb9a25b2">2024.lrec-main.1438</url>
      <bibkey>xing-etal-2024-towards-human</bibkey>
    </paper>
    <paper id="1439">
      <title>Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents</title>
      <author><first>Hao</first><last>Wang</last></author>
      <author><first>Tang</first><last>Li</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Pinpin</first><last>Zhu</last></author>
      <pages>16557–16569</pages>
      <abstract>Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification techniques. This approach aims to generate relation representations that are more aware of the spatial context and unseen relation in a manner similar to human perception. Experimental results demonstrate the effectiveness of our proposed method by showcasing its ability to outperform existing methods. This study also opens up new possibilities for practical applications.</abstract>
      <url hash="e2d69cfc">2024.lrec-main.1439</url>
      <bibkey>wang-etal-2024-towards-human-like</bibkey>
    </paper>
    <paper id="1440">
      <title>Towards More Realistic <fixed-case>C</fixed-case>hinese Spell Checking with New Benchmark and Specialized Expert Model</title>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Zilong</first><last>Zheng</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Zhihui</first><last>Liu</last></author>
      <author><first>Jinxiong</first><last>Chang</last></author>
      <author><first>Qishen</first><last>Zhang</last></author>
      <author><first>Zhongyi</first><last>Liu</last></author>
      <author><first>Guannan</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>16570–16580</pages>
      <abstract>Large Language Models (LLMs) hold considerable promise for artificial general intelligence, given their intrinsic abilities to accomplish a wide range of open-domain tasks either independently or in tandem with specialized expert models. However, despite these capabilities, the performance of LLMs has yet to be comprehensively evaluated in realistic scenarios. To this end, in this work, we introduce a novel task, the <b>R</b>ealistic <b>C</b>hinese <b>S</b>pell <b>C</b>hecking (<b>RCSC</b>), to evaluate the effectiveness of existing methods comprehensively. In contrast to existing works that solely address Chinese character misspellings or pinyin conversions, our task aims to convert the realistic Chinese text into the corresponding correct text. The realistic Chinese text may potentially contain both Chinese misspellings and pinyin conversions. We first present the <b>R</b>ealistic <b>C</b>hinese <b>S</b>pell <b>C</b>hecking <b>B</b>enchmark (<b>RCSCB</b>), which consists of two subsets and contains a total of 581,657 samples. Then, we benchmark the performance of various baselines and find that all the existing methods, including instruction-based LLMs, achieve unsatisfactory results on RCSCB. To further improve the performance on RCSCB, we propose <b>P</b>inyin-<b>E</b>nhanced <b>S</b>pell <b>C</b>hecker (<b>PESC</b>), which is specifically designed to address pinyin-related misspellings. Experimental results demonstrate that PESC can achieve state-of-the-art performance on RCSCB. Despite the progress made, the current state-of-the-art performance is still far from satisfactory. We expect further progress on this crucial and challenging task.</abstract>
      <url hash="104b435c">2024.lrec-main.1440</url>
      <bibkey>wang-etal-2024-towards-realistic-chinese</bibkey>
    </paper>
    <paper id="1441">
      <title>Towards Multi-modal Sarcasm Detection via Disentangled Multi-grained Multi-modal Distilling</title>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Guimin</first><last>Hu</last></author>
      <author><first>Yaowei</first><last>Li</last></author>
      <author><first>Zhiqi</first><last>Huang</last></author>
      <author><first>Yuexian</first><last>Zou</last></author>
      <pages>16581–16591</pages>
      <abstract>Multi-modal sarcasm detection aims to identify whether a given sample with multi-modal information (i.e., text and image) is sarcastic, which has received increasing attention due to the rapid growth of multi-modal posts on modern social media. However, mainstream models process the input of each modality in a holistic manner, resulting in redundant and unrefined information. Moreover, the representations of different modalities are entangled in one common latent space to perform complex cross-modal interactions, neglecting the heterogeneity and distribution gap of different modalities. To address these issues, we propose a novel framework DMMD (short for Disentangled Multi-grained Multi-modal Distilling) for multi-modal sarcasm detection, which conducts multi-grained knowledge distilling (i.e., intra-subspace and inter-subspace) based on the disentangled multi-modal representations. Concretely, the representations of each modality are disentangled explicitly into modality-agnostic/specific subspaces. Then we transfer cross-modal knowledge by conducting intra-subspace knowledge distilling in a self-adaptive pattern. We also apply mutual learning to regularize the underlying inter-subspace consistency. Extensive experiments on a commonly used benchmark demonstrate the efficacy of our DMMD over cutting-edge methods. More encouragingly, visualization results indicate the multi-modal representations display meaningful distributional patterns, and we hope it will be helpful for the community of multi-modal knowledge transfer.</abstract>
      <url hash="f86b4a08">2024.lrec-main.1441</url>
      <bibkey>zhu-etal-2024-towards-multi-modal</bibkey>
    </paper>
    <paper id="1442">
      <title>Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation</title>
      <author><first>Fahmida</first><last>Alam</last></author>
      <author><first>Md Asiful</first><last>Islam</last></author>
      <author><first>Robert</first><last>Vacareanu</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>16592–16606</pages>
      <abstract>We introduce a meta dataset for few-shot relation extraction, which includes two datasets derived from existing supervised relation extraction datasets – NYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKI- DATA (Sorokin and Gurevych, 2017) – as well as a few-shot form of the TACRED dataset (Sabo et al., 2021). Importantly, all these few-shot datasets were generated under realistic assumptions such as: the test relations are different from any relations a model might have seen before, limited training data, and a preponderance of candidate relation mentions that do not correspond to any of the relations of interest. Using this large resource, we conduct a comprehensive evaluation of six recent few-shot relation extraction methods, and observe that no method comes out as a clear winner. Further, the overall performance on this task is low, indicating substantial need for future research. We release all versions of the data, i.e., both supervised and few-shot, for future research.</abstract>
      <url hash="8d1574fb">2024.lrec-main.1442</url>
      <bibkey>alam-etal-2024-towards-realistic</bibkey>
    </paper>
    <paper id="1443">
      <title>Towards Robust Evidence-Aware Fake News Detection via Improving Semantic Perception</title>
      <author><first>Yike</first><last>Wu</last></author>
      <author><first>Yang</first><last>Xiao</last></author>
      <author><first>Mengting</first><last>Hu</last></author>
      <author><first>Mengying</first><last>Liu</last></author>
      <author><first>Pengcheng</first><last>Wang</last></author>
      <author><first>Mingming</first><last>Liu</last></author>
      <pages>16607–16618</pages>
      <abstract>Evidence-aware fake news detection aims to determine the veracity of a given news (i.e., claim) with external evidences. We find that existing methods lack sufficient semantic perception and are easily blinded by textual expressions. For example, they still make the same prediction after we flip the semantics of a claim, which makes them vulnerable to malicious attacks. In this paper, we propose a model-agnostic training framework to improve the semantic perception of evidence-aware fake news detection. Specifically, we first introduce two kinds of data augmentation to complement the original training set with synthetic data. The semantic-flipped augmentation synthesizes claims with similar textual expressions but opposite semantics, while the semantic-invariant augmentation synthesizes claims with the same semantics but different writing styles. Moreover, we design a novel module to learn better claim representation which is more sensitive to the semantics, and further incorporate it into a multi-objective optimization paradigm. In the experiments, we also extend the original test set of benchmark datasets with the synthetic data to better evaluate the model perception of semantics. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods on the extended test set, while achieving competitive performance on the original one. Our source code are released at https://github.com/Xyang1998/RobustFND.</abstract>
      <url hash="c25716e7">2024.lrec-main.1443</url>
      <bibkey>wu-etal-2024-towards-robust</bibkey>
    </paper>
    <paper id="1444">
      <title>Towards Robust In-Context Learning for Machine Translation with Large Language Models</title>
      <author><first>Shaolin</first><last>Zhu</last></author>
      <author><first>Menglong</first><last>Cui</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>16619–16629</pages>
      <abstract>Using large language models (LLMs) for machine translation via in-context learning (ICL) has become an interesting research direction of machine translation (MT) in recent years. Its main idea is to retrieve a few translation pairs as demonstrations from an additional datastore (parallel corpus) to guide translation without updating the LLMs. However, the underlying noise of retrieved demonstrations usually dramatically deteriorate the performance of LLMs. In this paper, we propose a robust method to enable LLMs to achieve robust translation with ICL. The method incorporates a multi-view approach, considering both sentence- and word-level information, to select demonstrations that effectively avoid noise. At the sentence level, a margin-based score is designed to avoid semantic noise. At the word level, word embeddings are utilized to evaluate the related tokens and change the weight of words in demonstrations. By considering both sentence- and word-level similarity, the proposed method provides fine-grained demonstrations that effectively prompt the translation of LLMs. Experimental results demonstrate the effectiveness of our method, particularly in domain adaptation.</abstract>
      <url hash="fcd579da">2024.lrec-main.1444</url>
      <bibkey>zhu-etal-2024-towards-robust-context</bibkey>
    </paper>
    <paper id="1445">
      <title>Towards Robust Temporal Activity Localization Learning with Noisy Labels</title>
      <author><first>Daizong</first><last>Liu</last></author>
      <author><first>Xiaoye</first><last>Qu</last></author>
      <author><first>Xiang</first><last>Fang</last></author>
      <author><first>Jianfeng</first><last>Dong</last></author>
      <author><first>Pan</first><last>Zhou</last></author>
      <author><first>Guoshun</first><last>Nan</last></author>
      <author><first>Keke</first><last>Tang</last></author>
      <author><first>Wanlong</first><last>Fang</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <pages>16630–16642</pages>
      <abstract>This paper addresses the task of temporal activity localization (TAL). Although recent works have made significant progress in TAL research, almost all of them implicitly assume that the dense frame-level correspondences in each video-query pair are correctly annotated. However, in reality, such an assumption is extremely expensive and even impossible to satisfy due to subjective labeling. To alleviate this issue, in this paper, we explore a new TAL setting termed Noisy Temporal activity localization (NTAL), where a TAL model should be robust to the mixed training data with noisy moment boundaries. Inspired by the memorization effect of neural networks, we propose a novel method called Co-Teaching Regularizer (CTR) for NTAL. Specifically, we first learn a Gaussian Mixture Model to divide the mixed training data into preliminary clean and noisy subsets. Subsequently, we refine the labels of the two subsets by an adaptive prediction function so that their true positive and false positive samples could be identified. To avoid single model being prone to its mistakes learned by the mixed data, we adopt a co-teaching paradigm, which utilizes two models sharing the same framework to teach each other for robust learning. A curriculum strategy is further introduced to gradually learn the moment confidence from easy to hard. Experiments on three datasets demonstrate that our CTR is significantly more robust to the noisy training data compared to the existing methods.</abstract>
      <url hash="e902ebda">2024.lrec-main.1445</url>
      <bibkey>liu-etal-2024-towards-robust</bibkey>
    </paper>
    <paper id="1446">
      <title>Towards Semantic Tagging for <fixed-case>I</fixed-case>rish</title>
      <author><first>Tim</first><last>Czerniak</last></author>
      <author><first>Elaine</first><last>Uí Dhonnchadha</last></author>
      <pages>16643–16652</pages>
      <abstract>Well annotated corpora have been shown to have great value, both in linguistic and non-linguistic research, and in supporting machine-learning and many other non-research activities including language teaching. For minority languages, annotated corpora can help in understanding language usage norms among native and non-native speakers, providing valuable information both for lexicography and for teaching, and helping to combat the decline of speaker numbers. At the same time, minority languages suffer from having fewer available language resources than majority languages, and far less-developed annotation tooling. To date there is very little work in semantic annotation for Irish. In this paper we report on progress to date in the building of a standard tool-set for semantic annotation of Irish, including a novel method for evaluation of semantic annotation. A small corpus of Irish language data has been manually annotated with semantic tags, and manually checked. A semantic type tagging framework has then been developed using existing technologies, and using a semantic lexicon that has been built from a variety of sources. Semantic disambiguation methods have been added with a view to increasing accuracy. That framework has then been tested using the manually tagged corpus, resulting in over 90% lexical coverage and almost 80% tag accuracy. Development is ongoing as part of a larger corpus development project, and plans include expansion of the manually tagged corpus, expansion of the lexicon, and exploration of further disambiguation methods. As the first semantic tagger for Irish, to our knowledge, it is hoped that this research will form a sound basis for semantic annotation of Irish corpora in to the future.</abstract>
      <url hash="18a280c1">2024.lrec-main.1446</url>
      <bibkey>czerniak-ui-dhonnchadha-2024-towards-semantic</bibkey>
    </paper>
    <paper id="1447">
      <title>Towards Standardized Annotation and Parsing for <fixed-case>K</fixed-case>orean <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <author><first>Yige</first><last>Chen</last></author>
      <author><first>Jae</first><last>Ihn</last></author>
      <author><first>KyungTae</first><last>Lim</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>16653–16658</pages>
      <abstract>Previous research on Korean FrameNet has produced several datasets that serve as resources for FrameNet parsing in Korean. However, these datasets suffer from the problem that annotations are assigned on the word level, which is not optimally designed based on the agglutinative feature of Korean. To address this issue, we introduce a morphologically enhanced annotation strategy for Korean FrameNet datasets and parsing by leveraging the CoNLL-U format. We present the results of the FrameNet parsers trained on the Korean FrameNet data in the original format and our proposed format, respectively, and further elaborate on the linguistic rationales of our proposed scheme. We suggest the morpheme-based scheme to be the standard of Korean FrameNet data annotation.</abstract>
      <url hash="ae7574b6">2024.lrec-main.1447</url>
      <bibkey>chen-etal-2024-towards-standardized</bibkey>
    </paper>
    <paper id="1448">
      <title>Towards the <fixed-case>W</fixed-case>h<fixed-case>AP</fixed-case> Corpus: A Resource for the Study of <fixed-case>I</fixed-case>talian on <fixed-case>W</fixed-case>hats<fixed-case>A</fixed-case>pp</title>
      <author><first>Ilaria</first><last>Fiorentini</last></author>
      <author><first>Marco</first><last>Forlano</last></author>
      <author><first>Nicholas</first><last>Nese</last></author>
      <pages>16659–16663</pages>
      <abstract>Over the past two decades, the rise of new technologies and social networks has significantly shaped written language, imbuing it with characteristics akin to the spoken language. This study reports on the ongoing initiative to build the WhAP corpus, a resource featuring WhatsApp conversations in Italian, encompassing both written and spoken messages and totaling at present more than 400.000 tokens, 89 conversations, and 194 participants from diverse age groups and geographical regions of Italy. More specifically, this paper focuses on the practical steps involved in the construction of the resource. Once publicly accessible, the WhAP Corpus will enable in-depth linguistic research on the language used on WhatsApp, which shows unique features such as the blending of written and spoken elements.</abstract>
      <url hash="fe710201">2024.lrec-main.1448</url>
      <bibkey>fiorentini-etal-2024-towards-whap</bibkey>
    </paper>
    <paper id="1449">
      <title>Towards Understanding the Relationship between In-context Learning and Compositional Generalization</title>
      <author><first>Sungjun</first><last>Han</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>16664–16679</pages>
      <abstract>According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data. However, many neural network models, including Transformers, have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal Transformer in a setting that renders ‘ordinary’ learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible few-shot learning problems attainable from the dataset. The model can solve the task, however, by utilizing earlier examples to generalize to later ones – i.e., in-context learning. In evaluations on the datasets, SCAN, COGS, and GeoQuery, models trained in this manner indeed show improved compositional generalization. This indicates the usefulness of in-context learning problems as an inductive bias for generalization.</abstract>
      <url hash="db115861">2024.lrec-main.1449</url>
      <bibkey>han-pado-2024-towards-understanding</bibkey>
    </paper>
    <paper id="1450">
      <title>Towards <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for <fixed-case>A</fixed-case>ncash <fixed-case>Q</fixed-case>uechua</title>
      <author><first>Johanna</first><last>Cordova</last></author>
      <pages>16680–16685</pages>
      <abstract>This paper presents a brief description of some morphosyntactic features of Ancash Quechua, the majority variety of the Central Quechua language family (QI), for the purpose of building a corpus annotated according to the Universal Dependencies (UD) schema. The creation of such a corpus has two objectives: for Quechua linguistics, it opens up the possibility of more systematic linguistic studies and comparisons with other languages. It also enables the development of a syntactic parser, which would be the first NLP tool for a Quechua language of this family. For the UD project, adding Quechua, an agglutinative language with a rich morphology, makes it possible to point out some possible shortcomings of the universal annotation schema, and to fuel the discussion to adapt this schema to the specific features of the languages with a similar typology. The first step towards this work was first to gather and digitise the available linguistic resources, thus creating the first bilingual and sentence-aligned digital corpus in Ancash Quechua and Spanish. After identifying some linguistic features not fully described in the UD schema, we proposed annotation solutions, and built an initial corpus of around twenty sentences, which we are making freely available.</abstract>
      <url hash="de53e5cb">2024.lrec-main.1450</url>
      <bibkey>cordova-2024-towards-universal</bibkey>
    </paper>
    <paper id="1451">
      <title><fixed-case>TP</fixed-case>-Link: Fine-grained Pre-Training for Text-to-<fixed-case>SQL</fixed-case> Parsing with Linking Information</title>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Shujie</first><last>Li</last></author>
      <author><first>Zefeng</first><last>Cai</last></author>
      <author><first>Xiangyu</first><last>Li</last></author>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Chengming</first><last>Li</last></author>
      <author><first>Xiping</first><last>Hu</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <pages>16686–16697</pages>
      <abstract>In this paper, we introduce an innovative pre-training framework TP-Link, which aims to improve context-dependent Text-to-SQL Parsing by leveraging Linking information. This enhancement is achieved through better representation of both natural language utterances and the database schema, ultimately facilitating more effective text-to-SQL conversations. We present two novel pre-training objectives: (i) utterance linking prediction (ULP) task that models intricate syntactic relationships among natural language utterances in context-dependent text-to-SQL scenarios, and (ii) schema linking prediction (SLP) task that focuses on capturing fine-grained schema linking relationships between the utterances and the database schema. Extensive experiments demonstrate that our proposed TP-Link achieves state-of-the-art performance on two leading downstream benchmarks (i.e., SParC and CoSQL).</abstract>
      <url hash="11d71f62">2024.lrec-main.1451</url>
      <bibkey>liu-etal-2024-tp-link</bibkey>
    </paper>
    <paper id="1452">
      <title>Training <fixed-case>BERT</fixed-case> Models to Carry over a Coding System Developed on One Corpus to Another</title>
      <author><first>Dalma</first><last>Galambos</last></author>
      <author><first>Pal</first><last>Zsamboki</last></author>
      <pages>16698–16712</pages>
      <abstract>This paper describes how we train BERT models to carry over a coding system developed on the paragraphs of a Hungarian literary journal to another. The aim of the coding system is to track trends in the perception of literary translation around the political transformation in 1989 in Hungary. To evaluate not only task performance but also the consistence of the annotation, moreover, to get better predictions from an ensemble, we use 10-fold crossvalidation. Extensive hyperparameter tuning is used to obtain the best possible results and fair comparisons. To handle label imbalance, we use loss functions and metrics robust to it. Evaluation of the effect of domain shift is carried out by sampling a test set from the target domain. We establish the sample size by estimating the bootstrapped confidence interval via simulations. This way, we show that our models can carry over one annotation system to the target domain. Comparisons are drawn to provide insights such as learning multilabel correlations and confidence penalty improve resistance to domain shift, and domain adaptation on OCR-ed text on another domain improves performance almost to the same extent as that on the corpus under study. See our code at https://codeberg.org/zsamboki/bert-annotator-ensemble</abstract>
      <url hash="d491fe85">2024.lrec-main.1452</url>
      <bibkey>galambos-zsamboki-2024-training-bert</bibkey>
    </paper>
    <paper id="1453">
      <title><fixed-case>T</fixed-case>rans<fixed-case>C</fixed-case>oder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills</title>
      <author><first>Qiushi</first><last>Sun</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <pages>16713–16726</pages>
      <abstract>Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various code intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related knowledge like human programmers. Specifically, we employ a tunable prefix encoder to first capture cross-task and cross-language transferable knowledge, subsequently applying the acquired knowledge for optimized downstream adaptation. Besides, our approach confers benefits for tasks with minor training sample sizes and languages with smaller corpora, underscoring versatility and efficacy. Extensive experiments conducted on representative datasets clearly demonstrate that our method can lead to superior performance on various code-related tasks and encourage mutual reinforcement, especially in low-resource scenarios. Our codes are available at https://github.com/QiushiSun/TransCoder.</abstract>
      <url hash="673e40f4">2024.lrec-main.1453</url>
      <bibkey>sun-etal-2024-transcoder-towards</bibkey>
    </paper>
    <paper id="1454">
      <title><fixed-case>T</fixed-case>rans<fixed-case>ERR</fixed-case>: Translation-based Knowledge Graph Embedding via Efficient Relation Rotation</title>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Xiangdong</first><last>Su</last></author>
      <author><first>Fujun</first><last>Zhang</last></author>
      <author><first>Guanglai</first><last>Gao</last></author>
      <pages>16727–16737</pages>
      <abstract>This paper presents a translation-based knowledge geraph embedding method via efficient relation rotation (TransERR), a straightforward yet effective alternative to traditional translation-based knowledge graph embedding models. Different from the previous translation-based models, TransERR encodes knowledge graphs in the hypercomplex-valued space, thus enabling it to possess a higher degree of translation freedom in mining latent information between the head and tail entities. To further minimize the translation distance, TransERR adaptively rotates the head entity and the tail entity with their corresponding unit quaternions, which are learnable in model training. We also provide mathematical proofs to demonstrate the ability of TransERR in modeling various relation patterns, including symmetry, antisymmetry, inversion, composition, and subrelation patterns. The experiments on 10 benchmark datasets validate the effectiveness and the generalization of TransERR. The results also indicate that TransERR can better encode large-scale datasets with fewer parameters than the previous translation-based models. Our code and datasets are available at <url>https://github.com/dellixx/TransERR</url>.</abstract>
      <url hash="7b9679ec">2024.lrec-main.1454</url>
      <bibkey>li-etal-2024-transerr-translation</bibkey>
    </paper>
    <paper id="1455">
      <title>Transfer Fine-tuning for Quality Estimation of Text Simplification</title>
      <author><first>Yuki</first><last>Hironaka</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>16738–16744</pages>
      <abstract>To efficiently train quality estimation of text simplification on a small-scale labeled corpus, we train sentence difficulty estimation prior to fine-tuning the pre-trained language models. Our proposed method improves the quality estimation of text simplification in the framework of transfer fine-tuning, in which pre-trained language models can improve the performance of the target task by additional training on the relevant task prior to fine-tuning. Since the labeled corpus for quality estimation of text simplification is small (600 sentence pairs), an efficient training method is desired. Therefore, we propose a training method for pseudo quality estimation that does not require labels for quality estimation. As a relevant task for quality estimation of text simplification, we train the estimation of sentence difficulty. This is a binary classification task that identifies which sentence is simpler using an existing parallel corpus for text simplification. Experimental results on quality estimation of English text simplification showed that not only the quality estimation performance on simplicity that was trained, but also the quality estimation performance on fluency and meaning preservation could be improved in some cases.</abstract>
      <url hash="28e3bcf8">2024.lrec-main.1455</url>
      <bibkey>hironaka-etal-2024-transfer-fine</bibkey>
    </paper>
    <paper id="1456">
      <title>Transferring <fixed-case>BERT</fixed-case> Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching</title>
      <author><first>Piotr</first><last>Rybak</last></author>
      <pages>16745–16750</pages>
      <abstract>Pre-trained language models have revolutionized the natural language understanding landscape, most notably BERT (Bidirectional Encoder Representations from Transformers). However, a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring BERT capabilities from high-resource to low-resource languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of BERT models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train BERT models for low-resource languages, thus democratizing access to advanced language understanding models.</abstract>
      <url hash="c70f8442">2024.lrec-main.1456</url>
      <bibkey>rybak-2024-transferring-bert</bibkey>
    </paper>
    <paper id="1457">
      <title>Transformer-based Joint Modelling for Automatic Essay Scoring and Off-Topic Detection</title>
      <author><first>Sourya Dipta</first><last>Das</last></author>
      <author><first>Yash A.</first><last>Vadi</last></author>
      <author><first>Kuldeep</first><last>Yadav</last></author>
      <pages>16751–16761</pages>
      <abstract>Automated Essay Scoring (AES) systems are widely popular in the market as they constitute a cost-effective and time-effective option for grading systems. Nevertheless, many studies have demonstrated that the AES system fails to assign lower grades to irrelevant responses. Thus, detecting the off-topic response in automated essay scoring is crucial in practical tasks where candidates write unrelated text responses to the given task in the question. In this paper, we are proposing an unsupervised technique that jointly scores essays and detects off-topic essays. The proposed Automated Open Essay Scoring (AOES) model uses a novel topic regularization module (TRM), which can be attached on top of a transformer model, and is trained using a proposed hybrid loss function. After training, the AOES model is further used to calculate the Mahalanobis distance score for off-topic essay detection. Our proposed method outperforms the baseline we created and earlier conventional methods on two essay-scoring datasets in off-topic detection as well as on-topic scoring. Experimental evaluation results on different adversarial strategies also show how the suggested method is robust for detecting possible human-level perturbations.</abstract>
      <url hash="e5f23cc7">2024.lrec-main.1457</url>
      <bibkey>das-etal-2024-transformer-based</bibkey>
    </paper>
    <paper id="1458">
      <title>Transformer-based <fixed-case>S</fixed-case>wedish Semantic Role Labeling through Transfer Learning</title>
      <author><first>Dana</first><last>Dannélls</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <author><first>Lucy</first><last>Yang Buhr</last></author>
      <pages>16762–16769</pages>
      <abstract>Semantic Role Labeling (SRL) is a task in natural language understanding where the goal is to extract semantic roles for a given sentence. English SRL has achieved state-of-the-art performance using Transformer techniques and supervised learning. However, this technique is not a viable choice for smaller languages like Swedish due to the limited amount of training data. In this paper, we present the first effort in building a Transformer-based SRL system for Swedish by exploring multilingual and cross-lingual transfer learning methods and leveraging the Swedish FrameNet resource. We demonstrate that multilingual transfer learning outperforms two different cross-lingual transfer models. We also found some differences between frames in FrameNet that can either hinder or enhance the model’s performance. The resulting end-to-end model is freely available and will be made accessible through Språkbanken Text’s research infrastructure.</abstract>
      <url hash="a61cbc11">2024.lrec-main.1458</url>
      <bibkey>dannells-etal-2024-transformer-based</bibkey>
    </paper>
    <paper id="1459">
      <title>Transformers for Bridging <fixed-case>P</fixed-case>ersian Dialects: Transliteration Model for Tajiki and <fixed-case>I</fixed-case>ranian Scripts</title>
      <author><first>MohammadAli</first><last>SadraeiJavaheri</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last></author>
      <author><first>Hamid Reza</first><last>Rabiee</last></author>
      <pages>16770–16775</pages>
      <abstract>In this study, we address the linguistic challenges posed by Tajiki Persian, a distinct variant of the Persian language that utilizes the Cyrillic script due to historical “Russification”. This distinguishes it from other Persian dialects that adopt the Arabic script. Despite its profound linguistic and cultural significance, Tajiki Persian remains a low-resource language with scant digitized datasets for computational applications. To address this deficiency, we created a parallel corpus using Shahnameh, a seminal Persian epic poem. Employing optical character recognition, we extracted Tajiki Persian verses from primary sources and applied a heuristic method to align them with their Iranian Persian counterparts. We then trained and assessed transliteration models using two prominent sequence-to-sequence architectures: GRU with attention and transformer. Our results underscore the enhanced performance of our models, particularly in contrast to pre-trained large multilingual models like GPT-3.5, emphasizing the value of dedicated datasets in advancing computational approaches for underrepresented languages. With the publication of this work, we are disseminating, for the first time, a vast collection of Persian poetry spanning 1000 years, transcribed in Tajiki scripts for the benefit of the Tajiki-speaking communities. The dataset, along with the model’s code and checkpoints, is accessible at https://github.com/language-ml/Tajiki-Shahname, marking a significant contribution to computational linguistic resources for Tajiki Persian.</abstract>
      <url hash="915307b0">2024.lrec-main.1459</url>
      <bibkey>sadraeijavaheri-etal-2024-transformers-bridging</bibkey>
    </paper>
    <paper id="1460">
      <title>Tree-Instruct: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment</title>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Binyuan</first><last>Hui</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Minghao</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Nevin L.</first><last>Zhang</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>16776–16789</pages>
      <abstract>Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and human preferences. Extensive research has highlighted the importance of the quality and diversity of instruction data. However, the impact of data complexity, as a crucial metric, remains relatively unexplored from three aspects: (1)where the sustainability of performance improvements with increasing complexity is uncertain; (2)whether the improvement brought by complexity merely comes from introducing more training tokens; and (3)where the potential benefits of incorporating instructions from easy to difficult are not yet fully understood. In this paper, we propose Tree-Instruct to systematically enhance the instruction complexity in a controllable manner. By adding a specified number of nodes to instructions’ semantic trees, this approach not only yields new instruction data from the modified tree but also allows us to control the difficulty level of modified instructions. Our preliminary experiments reveal the following insights: (1)Increasing complexity consistently leads to sustained performance improvements of LLMs. (2)Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3)Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key.</abstract>
      <url hash="d64f72e6">2024.lrec-main.1460</url>
      <bibkey>zhao-etal-2024-tree-instruct</bibkey>
    </paper>
    <paper id="1461">
      <title><fixed-case>TRELM</fixed-case>: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models</title>
      <author><first>Junbing</first><last>Yan</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Taolin</first><last>Zhang</last></author>
      <author><first>Xiaofeng</first><last>He</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Hui</first><last>Xue</last></author>
      <pages>16790–16801</pages>
      <abstract>KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs. However, these models do not prioritize learning embeddings for entity-related tokens. Updating all parameters in KEPLM is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models. We observe that text corpora contain entities that follow a long-tail distribution, where some are suboptimally optimized and hinder the pre-training process. To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information. Moreover, updating a small subset of neurons in the feed-forward networks (FFNs) that store factual knowledge is both sufficient and efficient. Specifically, we utilize dynamic knowledge routing to identify knowledge paths in FFNs and selectively update parameters during pre-training. Experimental results show that TRELM achieves at least a 50% reduction in pre-training time and outperforms other KEPLMs in knowledge probing tasks and multiple knowledge-aware language understanding tasks.</abstract>
      <url hash="5c5b9ad1">2024.lrec-main.1461</url>
      <bibkey>yan-etal-2024-trelm-towards</bibkey>
    </paper>
    <paper id="1462">
      <title>Tricking <fixed-case>LLM</fixed-case>s into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks</title>
      <author><first>Abhinav Sukumar</first><last>Rao</last></author>
      <author><first>Atharva Roshan</first><last>Naik</last></author>
      <author><first>Sachin</first><last>Vashistha</last></author>
      <author><first>Somak</first><last>Aditya</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>16802–16830</pages>
      <abstract>Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For further analysis, we release a dataset of model outputs across 3700 jailbreak prompts over 4 tasks.</abstract>
      <url hash="6194b074">2024.lrec-main.1462</url>
      <bibkey>rao-etal-2024-tricking-llms</bibkey>
    </paper>
    <paper id="1463">
      <title>Triple-<fixed-case>R</fixed-case>: Automatic Reasoning for Fact Verification Using Language Models</title>
      <author><first>Mohammadamin</first><last>Kanaani</last></author>
      <author><first>Sajjad</first><last>Dadkhah</last></author>
      <author><first>Ali A.</first><last>Ghorbani</last></author>
      <pages>16831–16840</pages>
      <abstract>The rise of online social media platforms has made them a popular source of news. However, they are also prone to misinformation and fake news. To combat this, fact-checking is essential to verify the accuracy of claims made on these platforms. However, the existing methods in this field often lack the use of external sources and human-understandable explanations for system decisions. In this paper, we introduce a framework called Triple-R (Retriever, Ranker, Reasoner) that addresses these challenges. The framework uses the Web as an external knowledge source to retrieve relevant evidence for claims and includes a method to generate reasons based on the retrieved evidence for datasets lacking explanations. We then use this modified dataset to fine-tune a causal language model that generates natural language explanations and labels for pairs of retrieved evidence and claims. Our approach aims to improve the transparency and interpretability of fact-checking systems by providing understandable explanations for decision-making processes. We evaluated our method on a popular dataset and demonstrated its performance through an ablation study. The modified dataset is available on the Canadian Institute for Cybersecurity datasets webpage at https://www.unb.ca/cic/datasets/index.html.</abstract>
      <url hash="1df69927">2024.lrec-main.1463</url>
      <bibkey>kanaani-2024-triple-r</bibkey>
    </paper>
    <paper id="1464">
      <title>Triples-to-isi<fixed-case>X</fixed-case>hosa (<fixed-case>T</fixed-case>2<fixed-case>X</fixed-case>): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation</title>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Jan</first><last>Buys</last></author>
      <pages>16841–16854</pages>
      <abstract>Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short. Fine-tuning machine translation models emerges as the best method overall. These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal. We conclude with a qualitative analysis of generation errors and an ablation study.</abstract>
      <url hash="d7a164fb">2024.lrec-main.1464</url>
      <attachment type="OptionalSupplementaryMaterial" hash="efb76591">2024.lrec-main.1464.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>meyer-buys-2024-triples-isixhosa</bibkey>
    </paper>
    <paper id="1465">
      <title>Trustworthiness and Self-awareness in Large Language Models: An Exploration through the Think-Solve-Verify Framework</title>
      <author><first>Zhendong</first><last>Liu</last></author>
      <author><first>Changhong</first><last>Xia</last></author>
      <author><first>Wei</first><last>He</last></author>
      <author><first>Chongjun</first><last>Wang</last></author>
      <pages>16855–16866</pages>
      <abstract>As Large Language Models (LLMs) become increasingly influential in reasoning tasks, ensuring their trustworthiness and introspective self-awareness is critical. This research introduces the Think-Solve-Verify (TSV) framework, an innovative strategy tailored to explore LLMs’ trustworthiness, introspective self-awareness, and collaborative reasoning. This method accentuates a model’s capability to construct introspective reasoning processes from answers and ensure their trustworthiness. The reasoning with TSV consistently performs at or near the top across the majority of datasets with a single interaction with LLM. Moreover, we refine the voting process of self-consistency within the Chain-of-Thought (CoT) approach, leading to notable accuracy enhancements. In our evaluations, this approach improved performance from 67.3% to 72.8% on the AQuA dataset. Furthermore, we delve into the model’s ability to explain the given answers, highlighting the significance of discerning genuine comprehension from mere guesswork.</abstract>
      <url hash="dee933e2">2024.lrec-main.1465</url>
      <attachment type="OptionalSupplementaryMaterial" hash="adc93873">2024.lrec-main.1465.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>liu-etal-2024-trustworthiness-self</bibkey>
    </paper>
    <paper id="1466">
      <title>Tug-of-War between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models</title>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Xiaojian</first><last>Jiang</last></author>
      <author><first>Jiexin</first><last>Xu</last></author>
      <author><first>Li</first><last>Qiuxia</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>16867–16878</pages>
      <abstract>Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter <b>knowledge conflicts</b> when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability bias towards common knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence: We reveal that RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently. Moreover, we find that RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory. To solve the challenge of knowledge conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding (CD2) to better calibrate the model’s confidence. Experimental results demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.</abstract>
      <url hash="88121933">2024.lrec-main.1466</url>
      <bibkey>jin-etal-2024-tug-war</bibkey>
    </paper>
    <paper id="1467">
      <title><fixed-case>T</fixed-case>un<fixed-case>A</fixed-case>r<fixed-case>TTS</fixed-case>: <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic Text-To-Speech Corpus</title>
      <author><first>Imen</first><last>Laouirine</last></author>
      <author><first>Rami</first><last>Kammoun</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <pages>16879–16889</pages>
      <abstract>Being labeled as a low-resource language, the Tunisian dialect has no existing prior TTS research. In this paper, we present a speech corpus for Tunisian Arabic Text-to-Speech (TunArTTS) to initiate the development of end-to-end TTS systems for the Tunisian dialect. Our Speech corpus is extracted from an online English and Tunisian Arabic dictionary. We were able to extract a mono-speaker speech corpus of +3 hours of a male speaker sampled at 44100 kHz. The corpus is processed and manually diacritized. Furthermore, we develop various TTS systems based on two approaches: training from scratch and transfer learning. Both Tacotron2 and FastSpeech2 were used and evaluated using subjective and objective metrics. The experimental results show that our best results are obtained with the transfer learning from a pre-trained model on the English LJSpeech dataset. This model obtained a mean opinion score (MOS) of 3.88. TunArTTS will be publicly available for research purposes along with the baseline TTS system demo. Keywords: Tunisian Dialect, Text-To-Speech, Low-resource, Transfer Learning, TunArTTS</abstract>
      <url hash="986f4772">2024.lrec-main.1467</url>
      <bibkey>laouirine-etal-2024-tunartts-tunisian</bibkey>
    </paper>
    <paper id="1468">
      <title><fixed-case>T</fixed-case>weet<fixed-case>TER</fixed-case>: A Benchmark for Target Entity Retrieval on <fixed-case>T</fixed-case>witter without Knowledge Bases</title>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>16890–16896</pages>
      <abstract>Entity linking is a well-established task in NLP consisting of associating entity mentions with entries in a knowledge base. Current models have demonstrated competitive performance in standard text settings. However, when it comes to noisy domains such as social media, certain challenges still persist. Typically, to evaluate entity linking on existing benchmarks, a comprehensive knowledge base is necessary and models are expected to possess an understanding of all the entities contained within the knowledge base. However, in practical scenarios where the objective is to retrieve sentences specifically related to a particular entity, strict adherence to a complete understanding of all entities in the knowledge base may not be necessary. To address this gap, we introduce TweetTER (Tweet Target Entity Retrieval), a novel benchmark that aims to bridge the challenges in entity linking. The distinguishing feature of this benchmark is its approach of re-framing entity linking as a binary entity retrieval task. This enables the evaluation of language models’ performance without relying on a conventional knowledge base, providing a more practical and versatile evaluation framework for assessing the effectiveness of language models in entity retrieval tasks.</abstract>
      <url hash="3ee16aa5">2024.lrec-main.1468</url>
      <bibkey>rezaee-etal-2024-tweetter-benchmark</bibkey>
    </paper>
    <paper id="1469">
      <title>Two Counterexamples to Tokenization and the Noiseless Channel</title>
      <author><first>Marco</first><last>Cognetta</last></author>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Sangwhan</first><last>Moon</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>16897–16906</pages>
      <abstract>In Tokenization and the Noiseless Channel (Zouhar et al., 2023), Rényi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest Rényi efficiency of the unigram distribution should be chosen. The Rényi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that Rényi efficiency alone cannot capture. We describe two variants of BPE tokenization which can arbitrarily increase Rényi efficiency while decreasing the downstream model performance. These counterexamples expose cases where Rényi efficiency fails as an intrinsic tokenization metric and thus give insight for building more accurate predictors.</abstract>
      <url hash="b9578df7">2024.lrec-main.1469</url>
      <bibkey>cognetta-etal-2024-two-counterexamples</bibkey>
    </paper>
    <paper id="1470">
      <title>Typos Correction Training against Misspellings from Text-to-Text Transformers</title>
      <author><first>Guicai</first><last>Xie</last></author>
      <author><first>Ke</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Duan</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Zeqian</first><last>Huang</last></author>
      <pages>16907–16918</pages>
      <abstract>Dense retrieval (DR) has become a mainstream approach to information seeking, where a system is required to return relevant information to a user query. In real-life applications, typoed queries resulting from the users’ mistyping words or phonetic typing errors exist widely in search behaviors. Current dense retrievers experience a significant drop in retrieval effectiveness when they encounter typoed queries. Therefore, the search system requires the extra introduction of spell-checkers to deal with typos and then applies the DR model to perform robust matching. Herein, we argue that directly conducting the typos correction training would be beneficial to make an end-to-end retriever against misspellings. To this end, we propose a novel approach that can facilitate the incorporation of the spelling correction objective into the DR model using the encoder-decoder architecture. During typos correction training, we also develop a prompt-based augmentation technique to enhance the DR space alignment of the typoed query and its original query. Extensive experiments demonstrate that the effectiveness of our proposed end-to-end retriever significantly outperforms existing typos-aware training approaches and sophisticated training advanced retrievers. Our code is available at https://github.com/striver314/ToCoTR.</abstract>
      <url hash="fb739318">2024.lrec-main.1470</url>
      <bibkey>xie-etal-2024-typos-correction</bibkey>
    </paper>
    <paper id="1471">
      <title><fixed-case>UC</fixed-case>xn: Typologically Informed Annotation of Constructions Atop <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Leonie</first><last>Weissweiler</last></author>
      <author><first>Nina</first><last>Böbel</last></author>
      <author><first>Kirian</first><last>Guiller</last></author>
      <author><first>Santiago</first><last>Herrera</last></author>
      <author><first>Wesley</first><last>Scivetti</last></author>
      <author><first>Arthur</first><last>Lorenzi</last></author>
      <author><first>Nurit</first><last>Melnik</last></author>
      <author><first>Archna</first><last>Bhatia</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>William</first><last>Croft</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>16919–16932</pages>
      <abstract>The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages. However, the UD annotations do not tell the full story. Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements—for example, interrogative sentences with special markers and/or word orders—are not labeled holistically. We argue for (i) augmenting UD annotations with a ‘UCxn’ annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages. As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns. In addition to findings regarding these particular constructions, our study yields important insights on methodology for describing and identifying constructions in language-general and language-particular ways, and lays the foundation for future constructional enrichment of UD treebanks.</abstract>
      <url hash="629105f8">2024.lrec-main.1471</url>
      <bibkey>weissweiler-etal-2024-ucxn-typologically</bibkey>
    </paper>
    <paper id="1472">
      <title><fixed-case>UDM</fixed-case>orph: Morphosyntactically Tagged <fixed-case>UD</fixed-case> Corpora</title>
      <author><first>Maarten</first><last>Janssen</last></author>
      <pages>16933–16940</pages>
      <abstract>UDMorph provides an infrastructure parallel to that provided by UD for annotated corpus data that follow the UD guidelines, but do not provide dependency relations: a place where new annotated data-sets can be deposited, and existing data-sets can be found and downloaded. It also provides a corpus creation environment to easily create annotated data for additional languages. And it provides a REST and GUI interface to a growing collection taggers with a CoNLL-U output, currently for around 150 different languages.</abstract>
      <url hash="128ffad2">2024.lrec-main.1472</url>
      <bibkey>janssen-2024-udmorph-morphosyntactically</bibkey>
    </paper>
    <paper id="1473">
      <title><fixed-case>U</fixed-case>krai<fixed-case>NER</fixed-case>: A New Corpus and Annotation Scheme towards Comprehensive Entity Recognition</title>
      <author><first>Lauriane</first><last>Aufrant</last></author>
      <author><first>Lucie</first><last>Chasseur</last></author>
      <pages>16941–16952</pages>
      <abstract>Named entity recognition as it is traditionally envisioned excludes in practice a significant part of the entities of potential interest for real-word applications: nested, discontinuous, non-named entities. Despite various attempts to broaden their coverage, subsequent annotation schemes have achieved little adoption in the literature and the most restrictive variant of NER remains the default. This is partly due to the complexity of those annotations and their format. In this paper, we introduce a new annotation scheme that offers higher comprehensiveness while preserving simplicity, together with an annotation tool to implement that scheme. We also release the corpus UkraiNER, comprised of 10,000 French sentences in the geopolitical news domain and manually annotated with comprehensive entity recognition. Our baseline experiments on UkraiNER provide a first point of comparison to facilitate future research (82 F1 for comprehensive entity recognition, 87 F1 when focusing on traditional nested NER), as well as various insights on the composition and challenges that this corpus presents for state-of-the-art named entity recognition models.</abstract>
      <url hash="dc6354a1">2024.lrec-main.1473</url>
      <bibkey>aufrant-chasseur-2024-ukrainer-new</bibkey>
    </paper>
    <paper id="1474">
      <title><fixed-case>UMTIT</fixed-case>: Unifying Recognition, Translation, and Generation for Multimodal Text Image Translation</title>
      <author><first>Liqiang</first><last>Niu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>16953–16972</pages>
      <abstract>Prior research in Image Machine Translation (IMT) has focused on either translating the source image solely into the target language text or exclusively into the target image. As a result, the former approach lacked the capacity to generate target images, while the latter was insufficient in producing target text. In this paper, we present a Unified Multimodal Text Image Translation (UMTIT) model that not only translates text images into the target language but also generates consistent target images. The UMTIT model consists of two image-text modality conversion steps: the first step converts images to text to recognize the source text and generate translations, while the second step transforms text to images to create target images based on the translations. Due to the limited availability of public datasets, we have constructed two multimodal image translation datasets. Experimental results show that our UMTIT model is versatile enough to handle tasks across multiple modalities and outperforms previous methods. Notably, UMTIT surpasses the state-of-the-art TrOCR in text recognition tasks, achieving a lower Character Error Rate (CER); it also outperforms cascading methods in text translation tasks, obtaining a higher BLEU score; and, most importantly, UMTIT can generate high-quality target text images.</abstract>
      <url hash="ce129d71">2024.lrec-main.1474</url>
      <attachment type="OptionalSupplementaryMaterial" hash="aeeab958">2024.lrec-main.1474.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>niu-etal-2024-umtit-unifying</bibkey>
    </paper>
    <paper id="1475">
      <title>Uncertainty-Aware Cross-Modal Alignment for Hate Speech Detection</title>
      <author><first>Chuanpeng</first><last>Yang</last></author>
      <author><first>Fuqing</first><last>Zhu</last></author>
      <author><first>Yaxin</first><last>Liu</last></author>
      <author><first>Jizhong</first><last>Han</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>16973–16983</pages>
      <abstract>Hate speech detection has become an urgent task with the emergence of huge multimodal harmful content (, memes) on social media platforms. Previous studies mainly focus on complex feature extraction and fusion to learn discriminative information from memes. However, these methods ignore two key points: 1) the misalignment of image and text in memes caused by the modality gap, and 2) the uncertainty between modalities caused by the contribution degree of each modality to hate sentiment. To this end, this paper proposes an uncertainty-aware cross-modal alignment (UCA) framework for modeling the misalignment and uncertainty in multimodal hate speech detection. Specifically, we first utilize the cross-modal feature encoder to capture image and text feature representations in memes. Then, a cross-modal alignment module is applied to reduce semantic gaps between modalities by aligning the feature representations. Next, a cross-modal fusion module is designed to learn semantic interactions between modalities to capture cross-modal correlations, providing complementary features for memes. Finally, a cross-modal uncertainty learning module is proposed, which evaluates the divergence between unimodal feature distributions to to balance unimodal and cross-modal fusion features. Extensive experiments on five publicly available datasets show that the proposed UCA produces a competitive performance compared with the existing multimodal hate speech detection methods.</abstract>
      <url hash="62e38072">2024.lrec-main.1475</url>
      <bibkey>yang-etal-2024-uncertainty-aware</bibkey>
    </paper>
    <paper id="1476">
      <title>Uncovering Agendas: A Novel <fixed-case>F</fixed-case>rench &amp; <fixed-case>E</fixed-case>nglish Dataset for Agenda Detection on Social Media</title>
      <author><first>Gregorios</first><last>Katsios</last></author>
      <author><first>Ning</first><last>Sa</last></author>
      <author><first>Ankita</first><last>Bhaumik</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <pages>16984–16997</pages>
      <abstract>The behavior and decision making of groups or communities can be dramatically influenced by individuals pushing particular agendas, e.g., to promote or disparage a person or an activity, to call for action, etc.. In the examination of online influence campaigns, particularly those related to important political and social events, scholars often concentrate on identifying the sources responsible for setting and controlling the agenda (e.g., public media). In this article we present a methodology for detecting specific instances of agenda control through social media where annotated data is limited or non-existent. By using a modest corpus of Twitter messages centered on the 2022 French Presidential Elections, we carry out a comprehensive evaluation of various approaches and techniques that can be applied to this problem. Our findings demonstrate that by treating the task as a textual entailment problem, it is possible to overcome the requirement for a large annotated training dataset.</abstract>
      <url hash="b0678345">2024.lrec-main.1476</url>
      <bibkey>katsios-etal-2024-uncovering-agendas</bibkey>
    </paper>
    <paper id="1477">
      <title>Uncovering the Potential of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for Discourse Analysis in Dialogue: An Empirical Study</title>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Peifeng</first><last>Li</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>16998–17010</pages>
      <abstract>Large language models, like ChatGPT, have shown remarkable capability in many downstream tasks, yet their ability to understand discourse structures of dialogues remains less explored, where it requires higher level capabilities of understanding and reasoning. In this paper, we aim to systematically inspect ChatGPT’s performance in two discourse analysis tasks: topic segmentation and discourse parsing, focusing on its deep semantic understanding of linear and hierarchical discourse structures underlying dialogue. To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input. Then, we conduct experiments on four popular topic segmentation datasets and two discourse parsing datasets. The experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures. Our deeper investigation indicates that ChatGPT can give more reasonable topic structures than human annotations but only linearly parses the hierarchical rhetorical structures. In addition, we delve into the impact of in-context learning (e.g., chain-of-thought) on ChatGPT and conduct the ablation study on various prompt components, which can provide a research foundation for future work. The code is available at <url>https://github.com/yxfanSuda/GPTforDDA</url>.</abstract>
      <url hash="3d2025e5">2024.lrec-main.1477</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a421008e">2024.lrec-main.1477.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>fan-etal-2024-uncovering-potential</bibkey>
    </paper>
    <paper id="1478">
      <title>Understanding How Positional Encodings Work in Transformer Model</title>
      <author><first>Taro</first><last>Miyazaki</last></author>
      <author><first>Hideya</first><last>Mino</last></author>
      <author><first>Hiroyuki</first><last>Kaneko</last></author>
      <pages>17011–17018</pages>
      <abstract>A transformer model is used in general tasks such as pre-trained language models and specific tasks including machine translation. Such a model mainly relies on positional encodings (PEs) to handle the sequential order of input vectors. There are variations of PEs, such as absolute and relative, and several studies have reported on the superiority of relative PEs. In this paper, we focus on analyzing in which part of a transformer model PEs work and the different characteristics between absolute and relative PEs through a series of experiments. Experimental results indicate that PEs work in both self- and cross-attention blocks in a transformer model, and PEs should be added only to the query and key of an attention mechanism, not to the value. We also found that applying two PEs in combination, a relative PE in the self-attention block and an absolute PE in the cross-attention block, can improve translation quality.</abstract>
      <url hash="eb2acd5c">2024.lrec-main.1478</url>
      <bibkey>miyazaki-etal-2024-understanding-positional</bibkey>
    </paper>
    <paper id="1479">
      <title><fixed-case>U</fixed-case>nicode Normalization and Grapheme Parsing of <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Nazmuddoha</first><last>Ansary</last></author>
      <author><first>Quazi Adibur Rahman</first><last>Adib</last></author>
      <author><first>Tahsin</first><last>Reasat</last></author>
      <author><first>Asif Shahriyar</first><last>Sushmit</last></author>
      <author><first>Ahmed Imtiaz</first><last>Humayun</last></author>
      <author><first>Sazia</first><last>Mehnaz</last></author>
      <author><first>Kanij</first><last>Fatema</last></author>
      <author><first>Mohammad Mamun Or</first><last>Rashid</last></author>
      <author><first>Farig</first><last>Sadeque</last></author>
      <pages>17019–17030</pages>
      <abstract>Writing systems of Indic languages have orthographic syllables, also known as complex graphemes, as unique horizontal units. A prominent feature of these languages is these complex grapheme units that comprise consonants/consonant conjuncts, vowel diacritics, and consonant diacritics, which, together make a unique Language. Unicode-based writing schemes of these languages often disregard this feature of these languages and encode words as linear sequences of Unicode characters using an intricate scheme of connector characters and font interpreters. Due to this way of using a few dozen Unicode glyphs to write thousands of different unique glyphs (complex graphemes), there are serious ambiguities that lead to malformed words. In this paper, we are proposing two libraries: i) a normalizer for normalizing inconsistencies caused by a Unicode-based encoding scheme for Indic languages and ii) a grapheme parser for Abugida text. It deconstructs words into visually distinct orthographic syllables or complex graphemes and their constituents. Our proposed normalizer is a more efficient and effective tool than the previously used IndicNLP normalizer. Moreover, our parser and normalizer are also suitable tools for general Abugida text processing as they performed well in our robust word-based and NLP experiments. We report the pipeline for the scripts of 7 languages in this work and develop the framework for the integration of more scripts.</abstract>
      <url hash="d7d6e4c0">2024.lrec-main.1479</url>
      <bibkey>ansary-etal-2024-unicode-normalization</bibkey>
    </paper>
    <paper id="1480">
      <title>Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval</title>
      <author><first>Haowei</first><last>Liu</last></author>
      <author><first>Yaya</first><last>Shi</last></author>
      <author><first>Haiyang</first><last>Xu</last></author>
      <author><first>Chunfeng</first><last>Yuan</last></author>
      <author><first>Qinghao</first><last>Ye</last></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Bing</first><last>Li</last></author>
      <author><first>Weiming</first><last>Hu</last></author>
      <pages>17031–17041</pages>
      <abstract>In video-text retrieval, most existing methods adopt the dual-encoder architecture for fast retrieval, which employs two individual encoders to extract global latent representations for videos and texts. However, they face challenges in capturing fine-grained semantic concepts. In this work, we propose the UNIFY framework, which learns lexicon representations to capture fine-grained semantics and combines the strengths of latent and lexicon representations for video-text retrieval. Specifically, we map videos and texts into a pre-defined lexicon space, where each dimension corresponds to a semantic concept. A two-stage semantics grounding approach is proposed to activate semantically relevant dimensions and suppress irrelevant dimensions. The learned lexicon representations can thus reflect fine-grained semantics of videos and texts. Furthermore, to leverage the complementarity between latent and lexicon representations, we propose a unified learning scheme to facilitate mutual learning via structure sharing and self-distillation. Experimental results show our UNIFY framework largely outperforms previous video-text retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and DiDeMo respectively.</abstract>
      <url hash="6028cebb">2024.lrec-main.1480</url>
      <bibkey>liu-etal-2024-unifying-latent</bibkey>
    </paper>
    <paper id="1481">
      <title><fixed-case>U</fixed-case>ni<fixed-case>PCM</fixed-case>: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt</title>
      <author><first>Yucheng</first><last>Cai</last></author>
      <author><first>Wentao</first><last>Ma</last></author>
      <author><first>Yuchuan</first><last>Wu</last></author>
      <author><first>Shuzheng</first><last>Si</last></author>
      <author><first>Yuan</first><last>Shao</last></author>
      <author><first>Zhijian</first><last>Ou</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>17042–17061</pages>
      <abstract>Recent researches have shown that multi-task instruction tuning after pre-training greatly improves the model’s robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task instruction tuning rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity.In this work, we propose to use Task-aware Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different datasets ranging from task-oriented dialog to open-domain conversation. Furthermore, we are amazed to find that TAP can generate prompts on par with those collected with crowdsourcing.</abstract>
      <url hash="0a254777">2024.lrec-main.1481</url>
      <bibkey>cai-etal-2024-unipcm-universal</bibkey>
    </paper>
    <paper id="1482">
      <title><fixed-case>U</fixed-case>ni<fixed-case>PSDA</fixed-case>: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot Cross-Lingual Natural Language Understanding</title>
      <author><first>Dongyang</first><last>Li</last></author>
      <author><first>Taolin</first><last>Zhang</last></author>
      <author><first>Jiali</first><last>Deng</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Xiaofeng</first><last>He</last></author>
      <author><first>Hui</first><last>Xue</last></author>
      <pages>17062–17073</pages>
      <abstract>Cross-lingual representation learning transfers knowledge from resource-rich data to resource-scarce ones to improve the semantic understanding abilities of different languages. However, previous works rely on shallow unsupervised data generated by token surface matching, regardless of the global context-aware semantics of the surrounding text tokens. In this paper, we propose an Unsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for cross-lingual natural language understanding to enrich the training data without human interventions. Specifically, to retrieve the tokens with similar meanings for the semantic data augmentation across different languages, we propose a sequential clustering process in 3 stages: within a single language, across multiple languages of a language family, and across languages from multiple language families. Meanwhile, considering the multi-lingual knowledge infusion with context-aware semantics while alleviating computation burden, we directly replace the key constituents of the sentences with the above-learned multi-lingual family knowledge, viewed as pseudo-semantic. The infusion process is further optimized via three de-biasing techniques without introducing any neural parameters. Extensive experiments demonstrate that our model consistently improves the performance on general zero-shot cross-lingual natural language understanding tasks, including sequence classification, information extraction, and question answering.</abstract>
      <url hash="47ddcb43">2024.lrec-main.1482</url>
      <bibkey>li-etal-2024-unipsda-unsupervised</bibkey>
    </paper>
    <paper id="1483">
      <title><fixed-case>U</fixed-case>ni<fixed-case>R</fixed-case>etriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval</title>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Baohang</first><last>Zhou</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Weichao</first><last>Wang</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>17074–17086</pages>
      <abstract>Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives. Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously.</abstract>
      <url hash="09bf8b6d">2024.lrec-main.1483</url>
      <bibkey>wang-etal-2024-uniretriever-multi</bibkey>
    </paper>
    <paper id="1484">
      <title>Universal Anaphora: The First Three Years</title>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Maciej</first><last>Ogrodniczuk</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <author><first>Sameer</first><last>Pradhan</last></author>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Anna</first><last>Nedoluzhko</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>17087–17100</pages>
      <abstract>The aim of the Universal Anaphora initiative is to push forward the state of the art in anaphora and anaphora resolution by expanding the aspects of anaphoric interpretation which are or can be reliably annotated in anaphoric corpora, producing unified standards to annotate and encode these annotations, delivering datasets encoded according to these standards, and developing methods for evaluating models that carry out this type of interpretation. Although several papers on aspects of the initiative have appeared, no overall description of the initiative’s goals, proposals and achievements has been published yet except as an online draft. This paper aims to fill this gap, as well as to discuss its progress so far.</abstract>
      <url hash="da279016">2024.lrec-main.1484</url>
      <bibkey>poesio-etal-2024-universal-anaphora</bibkey>
    </paper>
    <paper id="1485">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies: Extensions for Modern and Historical <fixed-case>G</fixed-case>erman</title>
      <author><first>Stefanie</first><last>Dipper</last></author>
      <author><first>Cora</first><last>Haiber</last></author>
      <author><first>Anna Maria</first><last>Schröter</last></author>
      <author><first>Alexandra</first><last>Wiemann</last></author>
      <author><first>Maike</first><last>Brinkschulte</last></author>
      <pages>17101–17111</pages>
      <abstract>In this paper we present extensions of the UD scheme for modern and historical German. The extensions relate in part to fundamental differences such as those between different kinds of arguments and modifiers. We illustrate the extensions with examples from the MHG data and discuss a number of MHG-specific constructions. At the current time, we have annotated a corpus of Middle High German with almost 29K tokens using this scheme, which to our knowledge is the first UD treebank for Middle High German. Inter-annotator agreement is very high: the annotators achieve a score of α = 0.85. A statistical analysis of the annotations shows some interesting differences in the distribution of labels between modern and historical German.</abstract>
      <url hash="4f9a4a06">2024.lrec-main.1485</url>
      <bibkey>dipper-etal-2024-universal-dependencies</bibkey>
    </paper>
    <paper id="1486">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for Learner <fixed-case>R</fixed-case>ussian</title>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>17112–17119</pages>
      <abstract>We introduce a pilot annotation of Russian learner data with syntactic dependency relations. The annotation is performed on a subset of sentences from RULEC-GEC and RU-Lang8, two error-corrected Russian learner datasets. We provide manually labeled Universal Dependency (UD) trees for 500 sentence pairs, annotating both the original (source) and the corrected (target) version of each sentence. Further, we outline guidelines for annotating learner Russian data containing non-standard erroneous text and analyze the effect that the individual errors have on the resulting dependency trees. This study should contribute to a wide range of computational and theoretical research directions in second language learning and grammatical error correction.</abstract>
      <url hash="90e0a7bb">2024.lrec-main.1486</url>
      <bibkey>rozovskaya-2024-universal-dependencies</bibkey>
    </paper>
    <paper id="1487">
      <title>Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion</title>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <author><first>Wen</first><last>Zhang</last></author>
      <pages>17120–17130</pages>
      <abstract>Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training strategy which can outperform 19 recent MMKGC methods and achieve new state-of-the-art results on three public MMKGC benchmarks. Our code and data have been released at https://github.com/zjukg/AdaMF-MAT.</abstract>
      <url hash="3faf33cc">2024.lrec-main.1487</url>
      <bibkey>zhang-etal-2024-unleashing-power</bibkey>
    </paper>
    <paper id="1488">
      <title>Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction</title>
      <author><first>Guozheng</first><last>Li</last></author>
      <author><first>Wenjun</first><last>Ke</last></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Zijie</first><last>Xu</last></author>
      <author><first>Ke</first><last>Ji</last></author>
      <author><first>Jiajun</first><last>Liu</last></author>
      <author><first>Ziyu</first><last>Shang</last></author>
      <author><first>Qiqing</first><last>Luo</last></author>
      <pages>17131–17143</pages>
      <abstract>The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (TableIE) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I<tex-math>^2</tex-math>CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples. Specifically, we first adopt off-the-shelf LLMs to perform schema-agnostic pre-extraction of triples in unlabeled samples using TableIE. Then we propose a novel triple-level similarity metric considering triple semantics between these samples and train a sample retrieval model based on calculated similarities in pre-extracted unlabeled data. We also devise three different sample annotation strategies for various scenarios. Finally, the annotated samples are considered as few-shot demonstrations in ICL for RTE. Experimental results on two RTE benchmarks show that I<tex-math>^2</tex-math>CL with TableIE achieves state-of-the-art performance compared to other methods under various few-shot RTE settings.</abstract>
      <url hash="66807735">2024.lrec-main.1488</url>
      <bibkey>li-etal-2024-unlocking-instructive</bibkey>
    </paper>
    <paper id="1489">
      <title>Unmasking Biases: Exploring Gender Bias in <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>atalan Machine Translation through Tokenization Analysis and Novel Dataset</title>
      <author><first>Audrey</first><last>Mash</last></author>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Aleix</first><last>Sant</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <author><first>Francesca</first><last>de Luca Fornaciari</last></author>
      <pages>17144–17153</pages>
      <abstract>This paper presents a comprehensive evaluation of gender bias in English-Catalan machine translation, encompassing the creation of a novel language resource and an analysis of translation quality across four different tokenization models. The study introduces a new dataset derived from the MuST-SHE corpus, focusing on gender-neutral terms that necessitate gendered translations in Catalan. The results reveal noteworthy gender bias across all translation models, with a consistent preference for masculine forms. Notably, the study finds that when context is available, BPE and Sentencepiece Unigram tokenization methods outperform others, achieving higher accuracy in gender translation. However, when no context is provided, Morfessor outputs more feminine forms than other tokenization methods, albeit still a small percentage. The study also reflects that stereotypes present in the data are amplified in the translation output. Ultimately, this work serves as a valuable resource for addressing and mitigating gender bias in machine translation, emphasizing the need for improved awareness and sensitivity to gender issues in natural language processing applications.</abstract>
      <url hash="41a67d2e">2024.lrec-main.1489</url>
      <bibkey>mash-etal-2024-unmasking-biases</bibkey>
    </paper>
    <paper id="1490">
      <title>Unpacking Bias: An Empirical Study of Bias Measurement Metrics, Mitigation Algorithms, and Their Interactions</title>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <author><first>Maria Jose</first><last>Zambrano</last></author>
      <pages>17154–17164</pages>
      <abstract>Word embeddings (WE) have been shown to capture biases from the text they are trained on, which has led to the development of several bias measurement metrics and bias mitigation algorithms (i.e., methods that transform the embedding space to reduce bias). This study identifies three confounding factors that hinder the comparison of bias mitigation algorithms with bias measurement metrics: (1) reliance on different word sets when applying bias mitigation algorithms, (2) leakage between training words employed by mitigation methods and evaluation words used by metrics, and (3) inconsistencies in normalization transformations between mitigation algorithms. We propose a very simple comparison methodology that carefully controls for word sets and vector normalization to address these factors. We conduct a component isolation experiment to assess how each component of our methodology impacts bias measurement. After comparing the bias mitigation algorithms using our comparison methodology, we observe increased consistency between different debiasing algorithms when evaluated using our approach.</abstract>
      <url hash="d6ab234e">2024.lrec-main.1490</url>
      <bibkey>bravo-marquez-zambrano-2024-unpacking-bias</bibkey>
    </paper>
    <paper id="1491">
      <title>Unraveling Spontaneous Speech Dimensions for Cross-Corpus <fixed-case>ASR</fixed-case> System Evaluation for <fixed-case>F</fixed-case>rench</title>
      <author><first>Solene Virginie</first><last>Evain</last></author>
      <author><first>Solange</first><last>Rossato</last></author>
      <author><first>François</first><last>Portet</last></author>
      <pages>17165–17175</pages>
      <abstract>Many papers on speech processing use the term ‘spontaneous speech’ as a catch-all term for situations like speaking with a friend, being interviewed on radio/TV or giving a lecture. However, Automatic Speech Recognition (ASR) systems performance seems to exhibit variation on this type of speech: the more spontaneous the speech, the higher the WER (Word Error Rate). Our study focuses on better understanding the elements influencing the levels of spontaneity in order to evaluate the relation between categories of spontaneity and ASR systems performance and improve the recognition on those categories. We first analyzed the literature, listed and unraveled those elements, and finally identified four axes: the situation of communication, the level of intimacy between speakers, the channel and the type of communication. Then, we trained ASR systems and measured the impact of instances of face-to-face interaction labeled with the previous dimensions (different levels of spontaneity) on WER. We made two axes vary and found that both dimensions have an impact on the WER. The situation of communication seems to have the biggest impact on spontaneity: ASR systems give better results for situations like an interview than for friends having a conversation at home.</abstract>
      <url hash="018e65e5">2024.lrec-main.1491</url>
      <bibkey>evain-etal-2024-unraveling-spontaneous</bibkey>
    </paper>
    <paper id="1492">
      <title>Unsupervised Grouping of Public Procurement Similar Items: Which Text Representation Should <fixed-case>I</fixed-case> Use?</title>
      <author><first>Pedro P. V.</first><last>Brum</last></author>
      <author><first>Mariana O.</first><last>Silva</last></author>
      <author><first>Gabriel P.</first><last>Oliveira</last></author>
      <author><first>Lucas G. L.</first><last>Costa</last></author>
      <author><first>Anisio</first><last>Lacerda</last></author>
      <author><first>Gisele</first><last>Pappa</last></author>
      <pages>17176–17185</pages>
      <abstract>In public procurement, establishing reference prices is essential to guide competitors in setting product prices. Group-purchased products, which are not standardized by default, are necessary to estimate reference prices. Text clustering techniques can be used to group similar items based on their descriptions, enabling the definition of reference prices for specific products or services. However, selecting an appropriate representation for text is challenging. This paper introduces a framework for text cleaning, extraction, and representation. We test eight distinct sentence representations tailored for public procurement item descriptions. Among these representations, we propose an approach that captures the most important components of item descriptions. Through extensive evaluation of a dataset comprising over 2 million items, our findings show that using sophisticated supervised methods to derive vectors for unsupervised tasks offers little advantages over leveraging unsupervised methods. Our results also highlight that domain-specific contextual knowledge is crucial for representation improvement.</abstract>
      <url hash="51413336">2024.lrec-main.1492</url>
      <bibkey>brum-etal-2024-unsupervised-grouping</bibkey>
    </paper>
    <paper id="1493">
      <title>Untangle the <fixed-case>KNOT</fixed-case>: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models</title>
      <author><first>Yantao</first><last>Liu</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Yuchen</first><last>Fan</last></author>
      <author><first>Shulin</first><last>Cao</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>17186–17204</pages>
      <abstract>Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs’ parameters. This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to &lt;b&gt;reason&lt;/b&gt; with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning. To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering. KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions. (2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question. (3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions. We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at our &lt;a href=https://github.com/THU-KEG/KNOT&gt;GitHub repository&lt;/a&gt; .</abstract>
      <url hash="b3cbaa3e">2024.lrec-main.1493</url>
      <bibkey>liu-etal-2024-untangle-knot</bibkey>
    </paper>
    <paper id="1494">
      <title>Unveiling Project-Specific Bias in Neural Code Models</title>
      <author><first>Zhiming</first><last>Li</last></author>
      <author><first>Yanzhou</first><last>Li</last></author>
      <author><first>Tianlin</first><last>Li</last></author>
      <author><first>Mengnan</first><last>Du</last></author>
      <author><first>Bozhi</first><last>Wu</last></author>
      <author><first>Yushi</first><last>Cao</last></author>
      <author><first>Junzhe</first><last>Jiang</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>17205–17216</pages>
      <abstract>Deep learning has introduced significant improvements in many software analysis tasks. Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data. In this work, we show that this phenomenon is caused by the heavy reliance on project-specific shortcuts for prediction instead of ground-truth evidence. We propose a Cond-Idf measurement to interpret this behavior, which quantifies the relatedness of a token with a label and its project-specificness. The strong correlation between model behavior and the proposed measurement indicates that without proper regularization, models tend to leverage spurious statistical cues for prediction. Equipped with these observations, we propose a novel bias mitigation mechanism that regularizes the model’s learning behavior by leveraging latent logic relations among samples. Experimental results on two representative program analysis tasks indicate that our mitigation framework can improve both inter-project OOD generalization and adversarial robustness, while not sacrificing accuracy on intra-project IID data.</abstract>
      <url hash="f1f8c15c">2024.lrec-main.1494</url>
      <bibkey>li-etal-2024-unveiling-project</bibkey>
    </paper>
    <paper id="1495">
      <title>Unveiling Strengths and Weaknesses of <fixed-case>NLP</fixed-case> Systems Based on a Rich Evaluation Corpus: The Case of <fixed-case>NER</fixed-case> in <fixed-case>F</fixed-case>rench</title>
      <author><first>Alice</first><last>Millour</last></author>
      <author><first>Yoann</first><last>Dupont</last></author>
      <author><first>Karen</first><last>Fort</last></author>
      <author><first>Liam</first><last>Duignan</last></author>
      <pages>17217–17224</pages>
      <abstract>Named Entity Recognition (NER) is an applicative task for which annotation schemes vary. To compare the performance of systems which tagsets differ in precision and coverage, it is necessary to assess (i) the comparability of their annotation schemes and (ii) the individual adequacy of the latter to a common annotation scheme. What is more, and given the lack of robustness of some tools towards textual variation, we cannot expect an evaluation led on an homogeneous corpus with low-coverage to provide a reliable prediction of the actual tools performance. To tackle both these limitations in evaluation, we provide a gold corpus for French covering 6 textual genres and annotated with a rich tagset that enables comparison with multiple annotation schemes. We use the flexibility of this gold corpus to provide both: (i) an individual evaluation of four heterogeneous NER systems on their target tagsets, (ii) a comparison of their performance on a common scheme. This rich evaluation framework enables a fair comparison of NER systems across textual genres and annotation schemes.</abstract>
      <url hash="1330cf1c">2024.lrec-main.1495</url>
      <bibkey>millour-etal-2024-unveiling-strengths</bibkey>
    </paper>
    <paper id="1496">
      <title>Unveiling Vulnerability of Self-Attention</title>
      <author><first>Khai Jiet</first><last>Liong</last></author>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>17225–17236</pages>
      <abstract>Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a significant threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attacks. This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism. (1) We propose a powerful perturbation technique named ‘HackAttend,’ which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art PLMs fall into heavy vulnerability, with minor attention perturbations (1%) resulting in a very high attack success rate (98%). Our paper extends the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce ‘S-Attend,’ a novel smoothing technique that effectively makes SA robust via structural perturbations. We empirically demonstrate that this simple yet effective technique achieves robust performance on par with adversarial training when facing various text attackers.</abstract>
      <url hash="80769bc4">2024.lrec-main.1496</url>
      <bibkey>liong-etal-2024-unveiling-vulnerability</bibkey>
    </paper>
    <paper id="1497">
      <title><fixed-case>UQA</fixed-case>: Corpus for <fixed-case>U</fixed-case>rdu Question Answering</title>
      <author><first>Samee</first><last>Arif</last></author>
      <author><first>Sualeha</first><last>Farid</last></author>
      <author><first>Awais</first><last>Athar</last></author>
      <author><first>Agha Ali</first><last>Raza</last></author>
      <pages>17237–17244</pages>
      <abstract>This paper introduces UQA, a novel dataset for question answering and text comprehension in Urdu, a low-resource language with over 70 million native speakers. UQA is generated by translating the Stanford Question Answering Dataset (SQuAD2.0), a large-scale English QA dataset, using a technique called EATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in the translated context paragraphs. The paper describes the process of selecting and evaluating the best translation model among two candidates: Google Translator and Seamless M4T. The paper also benchmarks several state-of-the-art multilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and reports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and 74.56 EM. UQA is a valuable resource for developing and testing multilingual NLP systems for Urdu and for enhancing the cross-lingual transferability of existing models. Further, the paper demonstrates the effectiveness of EATS for creating high-quality datasets for other languages and domains. The UQA dataset and the code are publicly available at www.github.com/sameearif/UQA</abstract>
      <url hash="a0e0fbf9">2024.lrec-main.1497</url>
      <bibkey>arif-etal-2024-uqa-corpus</bibkey>
    </paper>
    <paper id="1498">
      <title><fixed-case>U</fixed-case>rdu<fixed-case>MASD</fixed-case>: A Multimodal Abstractive Summarization Dataset for <fixed-case>U</fixed-case>rdu</title>
      <author><first>Ali</first><last>Faheem</last></author>
      <author><first>Faizad</first><last>Ullah</last></author>
      <author><first>Muhammad Sohaib</first><last>Ayub</last></author>
      <author><first>Asim</first><last>Karim</last></author>
      <pages>17245–17253</pages>
      <abstract>In this era of multimedia dominance, the surge of multimodal content on social media has transformed our methods of communication and information exchange. With the widespread use of multimedia content, the ability to effectively summarize this multimodal content is crucial for enhancing consumption, searchability, and retrieval. The scarcity of such training datasets has been a barrier to research in this area, especially for low-resource languages like Urdu. To address this gap, this paper introduces “UrduMASD”, a video-based Urdu multimodal abstractive text summarization dataset. The dataset contains 15,374 collections of videos, audio, titles, transcripts, and corresponding text summaries. To ensure the quality of the dataset, intrinsic evaluation metrics such as Abstractivity, Compression, Redundancy, and Semantic coherence have been employed. It was observed that our dataset surpasses existing datasets on numerous key quality metrics. Additionally, we present baseline results achieved using both text-based and state-of-the-art multimodal summarization models. On adding visual information, an improvement of 2.6% was observed in the ROUGE scores, highlighting the efficacy of utilizing multimodal inputs for summarization. To the best of our knowledge, this is the first dataset in Urdu that provides video-based multimodal data for abstractive text summarization, making it a valuable resource for advancing research in this field.</abstract>
      <url hash="d5d6e625">2024.lrec-main.1498</url>
      <bibkey>faheem-etal-2024-urdumasd-multimodal</bibkey>
    </paper>
    <paper id="1499">
      <title>User Guide for <fixed-case>KOTE</fixed-case>: <fixed-case>K</fixed-case>orean Online That-gul Emotions Dataset</title>
      <author><first>Duyoung</first><last>Jeon</last></author>
      <author><first>Junho</first><last>Lee</last></author>
      <author><first>Cheongtag</first><last>Kim</last></author>
      <pages>17254–17270</pages>
      <abstract>Despite the lack of comprehensive exploration of emotional connotations, sentiment analysis, which categorizes data as positive or negative, has been widely employed to identify emotional aspects in texts. Recently, corpora labeled with more than just valence or polarity have been built to surpass this limitation. However, most Korean emotion corpora are limited by their small size and narrow range of emotions covered. In this paper, we introduce the KOTE dataset. The KOTE dataset comprises 50,000 Korean online comments, totaling 250,000 cases, each manually labeled for 43 emotions and NO EMOTION through crowdsourcing. The taxonomy for the 43 emotions was systematically derived through cluster analysis of Korean emotion concepts within the word embedding space. After detailing the development of KOTE, we further discuss the results of fine-tuning, as well as analysis for social discrimination within the corpus.</abstract>
      <url hash="acc54da4">2024.lrec-main.1499</url>
      <bibkey>jeon-etal-2024-user-guide</bibkey>
    </paper>
    <paper id="1500">
      <title>Using Bibliodata <fixed-case>LOD</fixed-case>ification to Create Metadata-Enriched Literary Corpora in Line with <fixed-case>FAIR</fixed-case> Principles</title>
      <author><first>Agnieszka</first><last>Karlinska</last></author>
      <author><first>Cezary</first><last>Rosiński</last></author>
      <author><first>Marek</first><last>Kubis</last></author>
      <author><first>Patryk</first><last>Hubar</last></author>
      <author><first>Jan</first><last>Wieczorek</last></author>
      <pages>17271–17284</pages>
      <abstract>This paper discusses the design principles and procedures for creating a balanced corpus for research in computational literary studies, building on the experience of computational linguistics but adapting it to the specificities of the digital humanities. It showcases the development of the Metadata-enriched Polish Novel Corpus from the 19th and 20th centuries (19/20MetaPNC), consisting of 1,000 novels from 1854–1939, as an illustrative case and proposes a comprehensive workflow for the creation and reuse of literary corpora. What sets 19/20MetaPNC apart is its approach to balance, which considers the spatial dimension, the inclusion of non-canonical texts previously overlooked by other corpora, and the use of a complex, multi-stage metadata enrichment and verification process. Emphasis is placed on research-oriented metadata design, efficient data collection and data sharing according to the FAIR principles as well as 5- and 7-star data standards to increase the visibility and reusability of the corpus. A knowledge graph-based solution for the creation of exchangeable and machine-readable metadata describing corpora has been developed. For this purpose, metadata from bibliographic catalogs and other sources were transformed into Linked Data following the bibliodata LODification approach.</abstract>
      <url hash="204dddc3">2024.lrec-main.1500</url>
      <bibkey>karlinska-etal-2024-using-bibliodata</bibkey>
    </paper>
    <paper id="1501">
      <title>Using Persuasive Writing Strategies to Explain and Detect Health Misinformation</title>
      <author><first>Danial</first><last>Kamali</last></author>
      <author><first>Joseph D.</first><last>Romain</last></author>
      <author><first>Huiyi</first><last>Liu</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Jingbo</first><last>Meng</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>17285–17309</pages>
      <abstract>Nowadays, the spread of misinformation is a prominent problem in society. Our research focuses on aiding the automatic identification of misinformation by analyzing the persuasive strategies employed in textual documents. We introduce a novel annotation scheme encompassing common persuasive writing tactics to achieve our objective. Additionally, we provide a dataset on health misinformation, thoroughly annotated by experts utilizing our proposed scheme. Our contribution includes proposing a new task of annotating pieces of text with their persuasive writing strategy types. We evaluate fine-tuning and prompt-engineering techniques with pre-trained language models of the BERT family and the generative large language models of the GPT family using persuasive strategies as an additional source of information. We evaluate the effects of employing persuasive strategies as intermediate labels in the context of misinformation detection. Our results show that those strategies enhance accuracy and improve the explainability of misinformation detection models. The persuasive strategies can serve as valuable insights and explanations, enabling other models or even humans to make more informed decisions regarding the trustworthiness of the information.</abstract>
      <url hash="3e6367b5">2024.lrec-main.1501</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7209f7bf">2024.lrec-main.1501.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kamali-etal-2024-using-persuasive</bibkey>
    </paper>
    <paper id="1502">
      <title>Using Pre-Trained Language Models in an End-to-End Pipeline for Antithesis Detection</title>
      <author><first>Ramona</first><last>Kühn</last></author>
      <author><first>Khouloud</first><last>Saadi</last></author>
      <author><first>Jelena</first><last>Mitrović</last></author>
      <author><first>Michael</first><last>Granitzer</last></author>
      <pages>17310–17320</pages>
      <abstract>Rhetorical figures play an important role in influencing readers and listeners. Some of these word constructs that deviate from the usual language structure are known to be persuasive – antithesis is one of them. This figure combines parallel phrases with opposite ideas or words to highlight a contradiction. By identifying this figure, persuasive actors can be better identified. For this task, we create an annotated German dataset for antithesis detection. The dataset consists of posts from a Telegram channel criticizing the COVID-19 politics in Germany. Furthermore, we propose a three-block pipeline approach to detect the figure antithesis using large language models. Our pipeline splits the text into phrases, identifies phrases with a syntactically parallel structure, and detects if these parallel phrase pairs present opposing ideas by fine-tuning the German ELECTRA model, a state-of-the-art deep learning model for the German language. Furthermore, we compare the results with multilingual BERT and German BERT. Our novel approach outperforms the state-of-the-art methods (F1-score of 50.43 %) for antithesis detection by achieving an F1-score of 65.11 %.</abstract>
      <url hash="be15c125">2024.lrec-main.1502</url>
      <bibkey>kuhn-etal-2024-using-pre</bibkey>
    </paper>
    <paper id="1503">
      <title>Using Speech Technology to Test Theories of Phonetic and Phonological Typology</title>
      <author><first>Anisia</first><last>Popescu</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <pages>17321–17325</pages>
      <abstract>The present paper uses speech technology derived tools and methodologies to test theories about phonetic typology. We specifically look at how the two-way laryngeal contrast (voiced /b, d, g, v, z/ vs. voiceless /p, t, k, f, s/ obstruents) is implemented in European Portuguese, a language that has been suggested to exhibit a different voicing system than its sister Romance languages, more similar to the one found for Germanic languages. A large European Portuguese corpus was force aligned using (1) different combinations of parallel Portuguese (original), Italian (Romance language) and German (Germanic language) acoustic phone models and letting an ASR system choose the best fitting one, and (2) pronunciation variants (/b, d, g, v, z/ produced as either [b, d, g, v, z] or [p, t, k, f, s]) for obstruent consonants. Results support previous accounts in the literature that European Portuguese is diverging from the traditional voicing system known for Romance language, towards a hybrid system where stops and fricatives are specified for different voicing features.</abstract>
      <url hash="bd89ec9a">2024.lrec-main.1503</url>
      <bibkey>popescu-etal-2024-using-speech</bibkey>
    </paper>
    <paper id="1504">
      <title>Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>17326–17336</pages>
      <abstract>Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an adversarial framework. We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.</abstract>
      <url hash="5e796692">2024.lrec-main.1504</url>
      <attachment type="OptionalSupplementaryMaterial" hash="67f0ed1d">2024.lrec-main.1504.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>wang-etal-2024-utilizing-local</bibkey>
    </paper>
    <paper id="1505">
      <title>Utilizing Longer Context than Speech Bubbles in Automated Manga Translation</title>
      <author><first>Hiroto</first><last>Kaino</last></author>
      <author><first>Soichiro</first><last>Sugihara</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <author><first>Joshua B.</first><last>Tanner</last></author>
      <author><first>Shonosuke</first><last>Ishiwatari</last></author>
      <pages>17337–17342</pages>
      <abstract>This paper focuses on improving the performance of machine translation for manga (Japanese-style comics). In manga machine translation, text consists of a sequence of speech bubbles and each speech bubble is translated individually. However, each speech bubble itself does not contain sufficient information for translation. Therefore, previous work has proposed methods to use contextual information, such as the previous speech bubble, speech bubbles within the same scene, and corresponding scene images. In this research, we propose two new approaches to capture broader contextual information. Our first approach involves scene-based translation that considers the previous scene. The second approach considers broader context information, including details about the work, author, and manga genre. Through our experiments, we confirm that each of our methods improves translation quality, with the combination of both methods achieving the highest quality. Additionally, detailed analysis reveals the effect of zero-anaphora resolution in translation, such as supplying missing subjects not mentioned within a scene, highlighting the usefulness of longer contextual information in manga machine translation.</abstract>
      <url hash="51896f49">2024.lrec-main.1505</url>
      <bibkey>kaino-etal-2024-utilizing-longer</bibkey>
    </paper>
    <paper id="1506">
      <title><fixed-case>U</fixed-case>zbek<fixed-case>V</fixed-case>erb<fixed-case>D</fixed-case>etection: Rule-based Detection of Verbs in <fixed-case>U</fixed-case>zbek Texts</title>
      <author><first>Maksud</first><last>Sharipov</last></author>
      <author><first>Elmurod</first><last>Kuriyozov</last></author>
      <author><first>Ollabergan</first><last>Yuldashev</last></author>
      <author><first>Ogabek</first><last>Sobirov</last></author>
      <pages>17343–17347</pages>
      <abstract>Verb detection is a fundamental task in natural language processing that involves identifying the action or state expressed by a verb in a sentence. However, in Uzbek language, verb detection is challenging due to the complexity of its morphology and the agglutinative nature of the language. In this paper, we propose a rule-based approach for verb detection in Uzbek texts based on affixes/suffixes. Our method is based on a set of rules that capture the morphological patterns of verb forms in Uzbek language. We evaluate the proposed approach on a dataset of Uzbek texts and report an F1-score of 0.97, which outperforms existing methods for verb detection in Uzbek language. Our results suggest that rule-based approaches can be effective for verb detection in Uzbek texts and have potential applications in various natural language processing tasks.</abstract>
      <url hash="f5502192">2024.lrec-main.1506</url>
      <bibkey>sharipov-etal-2024-uzbekverbdetection-rule</bibkey>
    </paper>
    <paper id="1507">
      <title>Validating and Exploring Large Geographic Corpora</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <pages>17348–17358</pages>
      <abstract>This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora. Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific outlier detection. The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets. The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations. The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distributed across languages and populations. This result shows how standard corpus creation techniques can accidentally exclude under-represented populations.</abstract>
      <url hash="34cc6824">2024.lrec-main.1507</url>
      <bibkey>dunn-2024-validating-exploring</bibkey>
    </paper>
    <paper id="1508">
      <title>Verbing Weirds Language (Models): Evaluation of <fixed-case>E</fixed-case>nglish Zero-Derivation in Five <fixed-case>LLM</fixed-case>s</title>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Valentina</first><last>Izrailevitch</last></author>
      <author><first>Yunze</first><last>Xiao</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Leonie</first><last>Weissweiler</last></author>
      <pages>17359–17364</pages>
      <abstract>Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility—the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models—two proprietary models (GPT-3.5 and GPT-4), three open source model (Mistral 7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7-billion parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4.</abstract>
      <url hash="2894a0db">2024.lrec-main.1508</url>
      <bibkey>mortensen-etal-2024-verbing-weirds</bibkey>
    </paper>
    <paper id="1509">
      <title><fixed-case>V</fixed-case>iet<fixed-case>M</fixed-case>ed: A Dataset and Benchmark for Automatic Speech Recognition of <fixed-case>V</fixed-case>ietnamese in the Medical Domain</title>
      <author><first>Khai</first><last>Le-Duc</last></author>
      <pages>17365–17370</pages>
      <abstract>Due to privacy restrictions, there’s a shortage of publicly available speech recognition datasets in the medical domain. In this work, we present VietMed - a Vietnamese speech recognition dataset in the medical domain comprising 16h of labeled medical speech, 1000h of unlabeled medical speech and 1200h of unlabeled general-domain speech. To our best knowledge, VietMed is by far the world’s largest public medical speech recognition dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese speech dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available here.</abstract>
      <url hash="817b99c6">2024.lrec-main.1509</url>
      <attachment type="OptionalSupplementaryMaterial" hash="40f97b4b">2024.lrec-main.1509.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>le-duc-2024-vietmed-dataset</bibkey>
    </paper>
    <paper id="1510">
      <title><fixed-case>VI</fixed-case>-<fixed-case>OOD</fixed-case>: A Unified Framework of Representation Learning for Textual Out-of-distribution Detection</title>
      <author><first>Li-Ming</first><last>Zhan</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Xiao-Ming</first><last>Wu</last></author>
      <pages>17371–17383</pages>
      <abstract>Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood <tex-math>p(y|x)</tex-math> can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution <tex-math>p(x, y)</tex-math> instead of <tex-math>p(y|x)</tex-math>. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at <url>https://github.com/liam0949/LLM-OOD</url>.</abstract>
      <url hash="a92dc7b5">2024.lrec-main.1510</url>
      <bibkey>zhan-etal-2024-vi-ood</bibkey>
    </paper>
    <paper id="1511">
      <title>Visual-Linguistic Dependency Encoding for Image-Text Retrieval</title>
      <author><first>Wenxin</first><last>Guo</last></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Zhang</last></author>
      <author><first>Yi</first><last>Liu</last></author>
      <author><first>Zhendong</first><last>Mao</last></author>
      <pages>17384–17396</pages>
      <abstract>Image-text retrieval is a fundamental task to bridge the semantic gap between natural language and vision. Recent works primarily focus on aligning textual meanings with visual appearance. However, they often overlook the semantic discrepancy caused by syntactic structure in natural language expressions and relationships among visual entities. This oversight would lead to sub-optimal alignment and degraded retrieval performance, since the underlying semantic dependencies and object interactions remain inadequately encoded in both textual and visual embeddings. In this paper, we propose a novel Visual-Linguistic Dependency Encoding (VL-DE) framework, which explicitly models the dependency information among textual words and interaction patterns between image regions, improving the discriminative power of cross-modal representations for more accurate image-text retrieval. Specifically, VL-DE enhances textual representations by considering syntactic relationships and dependency types, and visual representations by attending to its spatially neighboring regions. Cross-attention mechanism is then introduced to aggregate aligned region-word pairs into image-text similarities. Analysis on Winoground, a dataset specially designed to measure vision-linguistic compositional structure reasoning, shows that VL-DE outperforms existing methods, demonstrating its effectiveness at this task. Comprehensive experiments on two benchmarks, Flickr30K and MS-COCO, further validates the competitiveness of our approach.</abstract>
      <url hash="5344eef0">2024.lrec-main.1511</url>
      <bibkey>guo-etal-2024-visual-linguistic</bibkey>
    </paper>
    <paper id="1512">
      <title>Visual-Textual Entailment with Quantities Using Model Checking and Knowledge Injection</title>
      <author><first>Nobuyuki</first><last>Iokawa</last></author>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <pages>17397–17408</pages>
      <abstract>In recent years, there has been great interest in multimodal inference. We concentrate on visual-textual entailment (VTE), a critical task in multimodal inference. VTE is the task of determining entailment relations between an image and a sentence. Several deep learning-based approaches have been proposed for VTE, but current approaches struggle with accurately handling quantities. On the other hand, one promising approach, one based on logical inference that can successfully deal with large quantities, has also been proposed. However, that approach uses automated theorem provers, increasing the computational cost for problems involving many entities. In addition, that approach cannot deal well with lexical differences between the semantic representations of images and sentences. In this paper, we present a logic-based VTE system that overcomes these drawbacks, using model checking for inference to increase efficiency and knowledge injection to perform more robust inference. We create a VTE dataset containing quantities and negation to assess how well VTE systems understand such phenomena. Using this dataset, we demonstrate that our system solves VTE tasks with quantities and negation more robustly than previous approaches.</abstract>
      <url hash="594fa949">2024.lrec-main.1512</url>
      <bibkey>iokawa-yanaka-2024-visual-textual</bibkey>
    </paper>
    <paper id="1513">
      <title><fixed-case>V</fixed-case>ygotsky Distance: Measure for Benchmark Task Similarity</title>
      <author><first>Maxim K.</first><last>Surkov</last></author>
      <author><first>Ivan P.</first><last>Yamshchikov</last></author>
      <pages>17409–17420</pages>
      <abstract>Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure “Vygotsky distance”. The core idea of this similarity measure is that it is based on relative performance of the “students” on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on various benchmarks, including GLUE, SuperGLUE, CLUE, and RussianSuperGLUE, demonstrate that a vast majority of NLP benchmarks could be at least 40% smaller in terms of the tasks included. Most importantly, Vygotsky distance could also be used for the validation of new tasks thus increasing the generalization potential of the future NLP models.</abstract>
      <url hash="59a12351">2024.lrec-main.1513</url>
      <bibkey>surkov-yamshchikov-2024-vygotsky-distance</bibkey>
    </paper>
    <paper id="1514">
      <title><fixed-case>W</fixed-case>a<fixed-case>C</fixed-case>adie: Towards an Acadian <fixed-case>F</fixed-case>rench Corpus</title>
      <author><first>Jeremy</first><last>Robichaud</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>17421–17432</pages>
      <abstract>Corpora are important assets within the natural language processing (NLP) and linguistics communities, as they allow the training of models and corpus-based studies of languages. However, corpora do not exist for many languages and language varieties, such as Acadian French. In this paper, we first show that off-the-shelf NLP systems perform more poorly on Acadian French than on standard French. An Acadian French corpus could, therefore, potentially be used to improve NLP models for this dialect. Then, leveraging web-as-corpus methodologies, specifically BootCaT, domain crawling, and social media scraping, we create three corpora of Acadian French. To evaluate these corpora, drawing on the linguistic literature on Acadian French, we propose 22 statistical corpus-based measures of the extent to which a corpus is Acadian French. We use these measures to compare these newly built corpora to known Acadian French text and find that all three corpora include some traces of Acadian French.</abstract>
      <url hash="155a4473">2024.lrec-main.1514</url>
      <bibkey>robichaud-cook-2024-wacadie-towards</bibkey>
    </paper>
    <paper id="1515">
      <title>Well Begun Is Half Done: An Implicitly Augmented Generative Framework with Distribution Modification for Hierarchical Text Classification</title>
      <author><first>Huawen</first><last>Feng</last></author>
      <author><first>Jingsong</first><last>Yan</last></author>
      <author><first>Junlong</first><last>Liu</last></author>
      <author><first>Junhao</first><last>Zheng</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <pages>17433–17443</pages>
      <abstract>Hierarchical Text Classification (HTC) is a challenging task which aims to extract the labels in a tree structure corresponding to a given text. Discriminative methods usually incorporate the hierarchical structure information into the encoding process, while generative methods decode the features according to it. However, the data distribution varies widely among different categories of samples, but current methods ignore the data imbalance, making the predictions biased and susceptible to error propagation. In this paper, we propose an **IM**plicitly **A**ugmented **G**enerativ **E** framework with distribution modification for hierarchical text classification (**IMAGE**). Specifically, we translate the distributions of original samples along various directions through implicit augmentation to get more diverse data. Furthermore, given the scarcity of the samples of tail classes, we adjust their distributions by transferring knowledge from other classes in label space. In this way, the generative framework learns a better beginning of the feature sequence without a prediction bias and avoids being misled by its wrong predictions for head classes. Experimental results show that **IMAGE** obtains competitive results compared with state-of-the-art methods and prove its superiority on unbalanced data.</abstract>
      <url hash="b330a61e">2024.lrec-main.1515</url>
      <bibkey>feng-etal-2024-well-begun</bibkey>
    </paper>
    <paper id="1516">
      <title>What Are the Implications of Your Question? Non-Information Seeking Question-Type Identification in <fixed-case>CNN</fixed-case> Transcripts</title>
      <author><first>Yao</first><last>Sun</last></author>
      <author><first>Anastasiia</first><last>Tatlubaeva</last></author>
      <author><first>Zhihan</first><last>Li</last></author>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <pages>17444–17448</pages>
      <abstract>Non-information seeking questions (NISQ) capture the subtle dynamics of human discourse. In this work, we utilize a dataset of over 1,500 information-seeking question(ISQ) and NISQ to evaluate human and machine performance on classifying fine-grained NISQ types. We introduce the first publicly available corpus focused on annotating both ISQs and NISQs as an initial benchmark. Additionally, we establish competitive baselines by assessing diverse systems, including Generative Pre-Trained Transformer Language models, on a new question classification task. Our results demonstrate the inherent complexity of making nuanced NISQ distinctions. The dataset is publicly available at https://github.com/YaoSun0422/NISQ_dataset.git</abstract>
      <url hash="7edafceb">2024.lrec-main.1516</url>
      <bibkey>sun-etal-2024-implications-question</bibkey>
    </paper>
    <paper id="1517">
      <title>What Can Diachronic Contexts and Topics Tell Us about the Present-Day Compositionality of <fixed-case>E</fixed-case>nglish Noun Compounds?</title>
      <author><first>Samin</first><last>Mahdizadeh Sani</last></author>
      <author><first>Malak</first><last>Rassem</last></author>
      <author><first>Chris</first><last>Jenkins</last></author>
      <author><first>Filip</first><last>Miletić</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>17449–17458</pages>
      <abstract>Predicting the compositionality of noun compounds such as climate change and tennis elbow is a vital component in natural language understanding. While most previous computational methods that automatically determine the semantic relatedness between compounds and their constituents have applied a synchronic perspective, the current study investigates what diachronic changes in contexts and semantic topics of compounds and constituents reveal about the compounds’ present-day degrees of compositionality. We define a binary classification task that utilizes two diachronic vector spaces based on contextual co-occurrences and semantic topics, and demonstrate that diachronic changes in cosine similarities – measured over context or topic distributions – uncover patterns that distinguish between compounds with low and high present-day compositionality. Despite fewer dimensions in the topic models, the topic space performs on par with the co-occurrence space and captures rather similar information. Temporal similarities between compounds and modifiers as well as between compounds and their prepositional paraphrases predict the compounds’ present-day compositionality with accuracy &gt;0.7.</abstract>
      <url hash="5cd51c3a">2024.lrec-main.1517</url>
      <bibkey>mahdizadeh-sani-etal-2024-diachronic-contexts</bibkey>
    </paper>
    <paper id="1518">
      <title>What Do Transformers Know about Government?</title>
      <author><first>Jue</first><last>Hou</last></author>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Lari</first><last>Kotilainen</last></author>
      <author><first>Sathianpong</first><last>Trangcasanchai</last></author>
      <author><first>Anh-Duc</first><last>Vu</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>17459–17472</pages>
      <abstract>This paper investigates what insights about linguistic features and what knowledge about the structure of natural language can be obtained from the encodings in transformer language models. In particular, we explore how BERT encodes the government relation between constituents in a sentence. We use several probing classifiers, and data from two morphologically rich languages. Our experiments show that information about government is encoded across all transformer layers, but predominantly in the early layers of the model. We find that, for both languages, a small number of attention heads encode enough information about the government relations to enable us to train a classifier capable of discovering new, previously unknown types of government, never seen in the training data. Currently, data is lacking for the research community working on grammatical constructions, and government in particular. We release the Government Bank—a dataset defining the government relations for thousands of lemmas in the languages in our experiments.</abstract>
      <url hash="597cc37b">2024.lrec-main.1518</url>
      <bibkey>hou-etal-2024-transformers-know</bibkey>
    </paper>
    <paper id="1519">
      <title>What Factors Influence <fixed-case>LLM</fixed-case>s’ Judgments? A Case Study on Question Answering</title>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Bobo</first><last>Li</last></author>
      <author><first>Li</first><last>Zheng</last></author>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Zixiang</first><last>Meng</last></author>
      <author><first>Runfeng</first><last>Shi</last></author>
      <author><first>Hao</first><last>Fei</last></author>
      <author><first>Jun</first><last>Zhou</last></author>
      <author><first>Fei</first><last>Li</last></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>17473–17485</pages>
      <abstract>Large Language Models (LLMs) are now being considered as judges of high efficiency to evaluate the quality of answers generated by candidate models. However, their judgments may be influenced by complex scenarios and inherent biases, raising concerns about their reliability. This study aims to bridge this gap by introducing four unexplored factors and examining the performance of LLMs as judges, namely answer quantity, inducing statements, judging strategy, and judging style. Additionally, we introduce a new dimension of question difficulty to provide a more comprehensive understanding of LLMs’ judgments across varying question intricacies. We employ ChatGPT, GPT-4, Gemini, and Claude-2 as judges and conduct experiments on Vicuna Benchmark and MT-bench. Our study reveals that LLMs’ judging abilities are susceptible to the influence of these four factors, and analyzing from the newly proposed dimension of question difficulty is highly necessary. We also provide valuable insights into optimizing LLMs’ performance as judges, enhancing their reliability and adaptability across diverse evaluation scenarios.</abstract>
      <url hash="b725d38b">2024.lrec-main.1519</url>
      <bibkey>chen-etal-2024-factors-influence</bibkey>
    </paper>
    <paper id="1520">
      <title>What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?</title>
      <author><first>Richard</first><last>Johansson</last></author>
      <pages>17486–17492</pages>
      <abstract>We investigate the behavior of methods using linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method. A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets. After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label. As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method.</abstract>
      <url hash="443b18d1">2024.lrec-main.1520</url>
      <bibkey>johansson-2024-happens-dataset</bibkey>
    </paper>
    <paper id="1521">
      <title>What Has <fixed-case>L</fixed-case>e<fixed-case>B</fixed-case>enchmark Learnt about <fixed-case>F</fixed-case>rench Syntax?</title>
      <author><first>Zdravko</first><last>Dugonjić</last></author>
      <author><first>Adrien</first><last>Pupier</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <pages>17493–17499</pages>
      <abstract>The paper reports on a series of experiments aiming at probing LeBenchmark, a pretrained acoustic model trained on 7k hours of spoken French, for syntactic information. Pretrained acoustic models are increasingly used for downstream speech tasks such as automatic speech recognition, speech translation, spoken language understanding or speech parsing. They are trained on very low level information (the raw speech signal), and do not have explicit lexical knowledge. Despite that, they obtained reasonable results on tasks that requires higher level linguistic knowledge. As a result, an emerging question is whether these models encode syntactic information. We probe each representation layer of LeBenchmark for syntax, using the Orféo treebank, and observe that it has learnt some syntactic information. Our results show that syntactic information is more easily extractable from the middle layers of the network, after which a very sharp decrease is observed.</abstract>
      <url hash="b5e7e25a">2024.lrec-main.1521</url>
      <bibkey>dugonjic-etal-2024-lebenchmark-learnt</bibkey>
    </paper>
    <paper id="1522">
      <title>What Is Needed for Intra-document Disambiguation of Math Identifiers?</title>
      <author><first>Takuto</first><last>Asakura</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>17500–17512</pages>
      <abstract>In automated scientific document analysis, accurately interpreting math formulae is imperative alongside comprehending natural language. Ambiguity in math identifiers within a single document poses significant challenges to understanding math formulae. While disambiguating math identifiers across documents has seen some progress, resolving ambiguity within a document remains inadequately researched due to complexity and insufficient datasets. The level of difficulty and information required to accomplish this task was uncertain. This study aims to determine which information is necessary for the intra-document disambiguation of math identifiers. Our findings indicate that the position data and local formula structure surrounding the identifiers, including modifiers, are particularly critical. For our study, we expanded a dataset for formula grounding and doubled its size to include annotations for 27,655 math identifier occurrences. We have created a multi-layer perceptron model that performs similarly to humans, with an 85% accuracy and a kappa value of 0.73, outperforming rule-based baselines. We trained and evaluated the model with papers in natural language processing (NLP). Our findings were also confirmed valid in fields other than NLP by applying the trained models to papers from various fields. These results will aid in improving mathematical language processing, such as mathematical information retrieval.</abstract>
      <url hash="649fdb2a">2024.lrec-main.1522</url>
      <bibkey>asakura-miyao-2024-needed-intra</bibkey>
    </paper>
    <paper id="1523">
      <title>When Argumentation Meets Cohesion: Enhancing Automatic Feedback in Student Writing</title>
      <author><first>Yuning</first><last>Ding</last></author>
      <author><first>Omid</first><last>Kashefi</last></author>
      <author><first>Swapna</first><last>Somasundaran</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <pages>17513–17524</pages>
      <abstract>In this paper, we investigate the role of arguments in the automatic scoring of cohesion in argumentative essays. The feature analysis reveals that in argumentative essays, the lexical cohesion between claims is more important to the overall cohesion, while the evidence is expected to be diverse and divergent. Our results show that combining features related to argument segments and cohesion features improves the performance of the automatic cohesion scoring model trained on a transformer. The cohesion score is also learned more accurately in a multi-task learning process by adding the automatic segmentation of argumentative elements as an auxiliary task. Our findings contribute to both the understanding of cohesion in argumentative writing and the development of automatic feedback.</abstract>
      <url hash="230ad573">2024.lrec-main.1523</url>
      <bibkey>ding-etal-2024-argumentation-meets</bibkey>
    </paper>
    <paper id="1524">
      <title>When Cohesion Lies in the Embedding Space: Embedding-Based Reference-Free Metrics for Topic Segmentation</title>
      <author><first>Iacopo</first><last>Ghinassi</last></author>
      <author><first>Lin</first><last>Wang</last></author>
      <author><first>Chris</first><last>Newell</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>17525–17536</pages>
      <abstract>In this paper we propose a new framework and new methods for the reference-free evaluation of topic segmentation systems directly in the embedding space. Specifically, we define a common framework for reference-free, embedding-based topic segmentation metrics, and show how this applies to an existing metric. We then define new metrics, based on a previously defined cohesion score, Average Relative Proximity. Using this approach, we show that Large Language Models (LLMs) yield features that, if used correctly, can strongly correlate with traditional topic segmentation metrics based on costly and rare human annotations, while outperforming existing reference-free metrics borrowed from clustering evaluation in most domains. We then show that smaller language models specifically fine-tuned for different sentence-level tasks can outperform LLMs several orders of magnitude larger. Via a thorough comparison of our metric’s performance across different datasets, we see that conversational data present the biggest challenge in this framework. Finally, we analyse the behaviour of our metrics in specific error cases, such as those of under-generation and moving of ground truth topic boundaries, and show that our metrics behave more consistently than other reference-free methods.</abstract>
      <url hash="32be0cb0">2024.lrec-main.1524</url>
      <bibkey>ghinassi-etal-2024-cohesion-lies</bibkey>
    </paper>
    <paper id="1525">
      <title>When Do “More Contexts” Help with Sarcasm Recognition?</title>
      <author><first>Ojas</first><last>Nimase</last></author>
      <author><first>Sanghyun</first><last>Hong</last></author>
      <pages>17537–17543</pages>
      <abstract>Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer contexts, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more contexts. We also identify inherent drawbacks of using more contexts, highlighting that in the pursuit of even better results, the model may need to adopt societal biases.</abstract>
      <url hash="f6ece129">2024.lrec-main.1525</url>
      <bibkey>nimase-hong-2024-contexts-help</bibkey>
    </paper>
    <paper id="1526">
      <title>When Your Cousin Has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages</title>
      <author><first>Niyati</first><last>Bafna</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <pages>17544–17556</pages>
      <abstract>Most existing approaches for unsupervised bilingual lexicon induction (BLI) depend on good quality static or contextual embeddings requiring large monolingual corpora for both languages. However, unsupervised BLI is most likely to be useful for low-resource languages (LRLs), where large datasets are not available. Often we are interested in building bilingual resources for LRLs against related high-resource languages (HRLs), resulting in severely imbalanced data settings for BLI. We first show that state-of-the-art BLI methods in the literature exhibit near-zero performance for severely data-imbalanced language pairs, indicating that these settings require more robust techniques. We then present a new method for unsupervised BLI between a related LRL and HRL that only requires inference on a masked language model of the HRL, and demonstrate its effectiveness on truly low-resource languages Bhojpuri and Magahi (with &lt;5M monolingual tokens each), against Hindi. We further present experiments on (mid-resource) Marathi and Nepali to compare approach performances by resource range, and release our resulting lexicons for five low-resource Indic languages: Bhojpuri, Magahi, Awadhi, Braj, and Maithili, against Hindi.</abstract>
      <url hash="960f272b">2024.lrec-main.1526</url>
      <bibkey>bafna-etal-2024-cousin-right</bibkey>
    </paper>
    <paper id="1527">
      <title>Which Sense Dominates Multisensory Semantic Understanding? A Brain Decoding Study</title>
      <author><first>Dandan</first><last>Huang</last></author>
      <author><first>Lu</first><last>Cao</last></author>
      <author><first>Zhenting</first><last>Li</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>17557–17563</pages>
      <abstract>Decoding semantic meanings from brain activity has attracted increasing attention. Neurolinguists have found that semantic perception is open to multisensory stimulation, as word meanings can be delivered by both auditory and visual inputs. Prior work which decodes semantic meanings from neuroimaging data largely exploits brain activation patterns triggered by stimulation in cross-modality (i.e. text-audio pairs, text-picture pairs). Their goal is to develop a more sophisticated computational model to probing what information from the act of language understanding is represented in human brain. While how the brain receiving such information influences decoding performance is underestimated. This study dissociates multisensory integration of word understanding into written text, spoken text and image perception respectively, exploring the decoding efficiency and reliability of unisensory information in the brain representation. The findings suggest that, in terms of unisensory, decoding is most successful when semantics is represented in pictures, but the effect disappears in the case of congeneric words which share a related meaning. These results reveal the modality dependence and multisensory enhancement in the brain decoding methodology.</abstract>
      <url hash="57484092">2024.lrec-main.1527</url>
      <bibkey>huang-etal-2024-sense-dominates</bibkey>
    </paper>
    <paper id="1528">
      <title>Who Did You Blame When Your Project Failed? Designing a Corpus for Presupposition Generation in Cross-Examination Dialogues</title>
      <author><first>Maria</first><last>Francis</last></author>
      <author><first>Julius</first><last>Steuer</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Volha</first><last>Petukhova</last></author>
      <pages>17564–17574</pages>
      <abstract>This paper introduces the corpus for the novel task of presupposition generation - a natural language generation problem where a model produces a list of presuppositions carried by the given input sentence, in the context of the presented research - given the cross-examination question. Two datasets, PECaN (<b>P</b>resupposition, <b>E</b>ntailment, <b>C</b>ontradiction <b>a</b>nd <b>N</b>eutral) and PGen (<b>P</b>resuppostion <b>Gen</b>eration), are designed to fine-tune existing BERT (CITATION) and T5 (CITATION) models for classification and generation tasks. Various corpora construction methods are proposed ranging from manual annotations, prompting the GPT 3.0 model, to augmenting data from the existing corpora. The fine-tuned models achieved high accuracy on the novel Presupposition as Natural Language Inference (PNLI) task which extends the traditional Natural Language Inference (NLI) incorporating instances of presupposition into classification. T5 outperforms BERT by broad margin achieving an overall accuracy of 84.35% compared to 71.85% of BERT, and specifically when classifying presuppositions (93% vs 73% respectively). Regarding presupposition generation, we observed that despite the limited amount of data used for fine-tuning, the model displays an emerging proficiency in generation presuppositions reaching ROUGE scores of 43.47, adhering to systematic patterns that mirror valid strategies for presupposition generation, although failed to generate the complete lists.</abstract>
      <url hash="98309e39">2024.lrec-main.1528</url>
      <bibkey>francis-etal-2024-blame-project</bibkey>
    </paper>
    <paper id="1529">
      <title>Who Is Bragging More Online? A Large Scale Analysis of Bragging in Social Media</title>
      <author><first>Mali</first><last>Jin</last></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last></author>
      <author><first>A. Seza</first><last>Doğruöz</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>17575–17587</pages>
      <abstract>Bragging is the act of uttering statements that are likely to be positively viewed by others and it is extensively employed in human communication with the aim to build a positive self-image of oneself. Social media is a natural platform for users to employ bragging in order to gain admiration, respect, attention and followers from their audiences. Yet, little is known about the scale of bragging online and its characteristics. This paper employs computational sociolinguistics methods to conduct the first large scale study of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence, temporal dynamics and impact of demographic factors. Our study shows that the prevalence of bragging decreases over time within the same population of users. In addition, younger, more educated and popular users in the U.S. are more likely to brag. Finally, we conduct an extensive linguistics analysis to unveil specific bragging themes associated with different user traits.</abstract>
      <url hash="6259a039">2024.lrec-main.1529</url>
      <bibkey>jin-etal-2024-bragging-online</bibkey>
    </paper>
    <paper id="1530">
      <title>Who Said What: Formalization and Benchmarks for the Task of Quote Attribution</title>
      <author><first>Wenjie</first><last>Zhong</last></author>
      <author><first>Jason</first><last>Naradowsky</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>17588–17602</pages>
      <abstract>The task of quote attribution seeks to pair textual utterances with the name of their speakers. Despite continuing research efforts on the task, models are rarely evaluated systematically against previous models in comparable settings on the same datasets. This has resulted in a poor understanding of the relative strengths and weaknesses of various approaches. In this work we formalize the task of quote attribution, and in doing so, establish a basis of comparison across existing models. We present an exhaustive benchmark of known models, including natural extensions to larger LLM base models, on all available datasets in both English and Chinese. Our benchmarking results reveal that the CEQA model attains state-of-the-art performance among all supervised methods, and ChatGPT, operating in a four-shot setting, demonstrates performance on par with or surpassing that of supervised methods on some datasets. Detailed error analysis identify several key factors contributing to prediction errors.</abstract>
      <url hash="a44dab0e">2024.lrec-main.1530</url>
      <bibkey>zhong-etal-2024-said-formalization</bibkey>
    </paper>
    <paper id="1531">
      <title>Why Voice Biomarkers of Psychiatric Disorders Are Not Used in Clinical Practice? Deconstructing the Myth of the Need for Objective Diagnosis</title>
      <author><first>Vincent P.</first><last>Martin</last></author>
      <author><first>Jean-Luc</first><last>Rouas</last></author>
      <pages>17603–17613</pages>
      <abstract>Given the high prevalence of mental disorders and the significant diagnostic delays and difficulties in patient follow-up, voice biomarkers hold the promise of improving access to care and therapeutic follow-up for people with psychiatric disorders. Yet, despite many years of successful research in the field, none of these voice biomarkers are implemented in clinical practice. Beyond the reductive explanation of the lack of explainability of the involved machine learning systems, we look for arguments in the epistemology and sociology of psychiatry. We show that the estimation of diagnoses, the major task in the literature, is of little interest to both clinicians and patients. After tackling the common misbeliefs about diagnosis in psychiatry in a didactic way, we propose a paradigm shift towards the estimation of clinical symptoms and signs, which not only address the limitations raised against diagnosis estimation but also enable the formulation of new machine learning tasks. We hope that this paradigm shift will empower the use of vocal biomarkers in clinical practice. It is however conditional on a change in database labeling practices, but also on a profound change in the speech processing community’s practices towards psychiatry.</abstract>
      <url hash="63ae9219">2024.lrec-main.1531</url>
      <bibkey>martin-rouas-2024-voice-biomarkers</bibkey>
    </paper>
    <paper id="1532">
      <title><fixed-case>W</fixed-case>iki<fixed-case>F</fixed-case>act<fixed-case>D</fixed-case>iff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models</title>
      <author><first>Hichem</first><last>Ammar Khodja</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Quentin</first><last>Brabant</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <pages>17614–17624</pages>
      <abstract>The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are “unknown” to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff.</abstract>
      <url hash="87657e76">2024.lrec-main.1532</url>
      <bibkey>ammar-khodja-etal-2024-wikifactdiff-large</bibkey>
    </paper>
    <paper id="1533">
      <title><fixed-case>W</fixed-case>iki<fixed-case>S</fixed-case>plit++: Easy Data Refinement for Split and Rephrase</title>
      <author><first>Hayato</first><last>Tsukagoshi</last></author>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Katsuki</first><last>Chousa</last></author>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <author><first>Koichi</first><last>Takeda</last></author>
      <pages>17625–17636</pages>
      <abstract>The task of Split and Rephrase, which splits a complex sentence into multiple simple sentences with the same meaning, improves readability and enhances the performance of downstream tasks in natural language processing (NLP). However, while Split and Rephrase can be improved using a text-to-text generation approach that applies encoder-decoder models fine-tuned with a large-scale dataset, it still suffers from hallucinations and under-splitting. To address these issues, this paper presents a simple and strong data refinement approach. Here, we create WikiSplit++ by removing instances in WikiSplit where complex sentences do not entail at least one of the simpler sentences and reversing the order of reference simple sentences. Experimental results show that training with WikiSplit++ leads to better performance than training with WikiSplit, even with fewer training instances. In particular, our approach yields significant gains in the number of splits and the entailment ratio, a proxy for measuring hallucinations.</abstract>
      <url hash="1998ec47">2024.lrec-main.1533</url>
      <bibkey>tsukagoshi-etal-2024-wikisplit-easy</bibkey>
    </paper>
    <paper id="1534">
      <title>Willkommens-Merkel, Chaos-<fixed-case>J</fixed-case>ohnson, and Tore-Klose: Modeling the Evaluative Meaning of <fixed-case>G</fixed-case>erman Personal Name Compounds</title>
      <author><first>Annerose</first><last>Eichel</last></author>
      <author><first>Tana</first><last>Deeg</last></author>
      <author><first>Andre</first><last>Blessing</last></author>
      <author><first>Milena</first><last>Belosevic</last></author>
      <author><first>Sabine</first><last>Arndt-Lappe</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>17637–17650</pages>
      <abstract>We present a comprehensive computational study of the under-investigated phenomenon of personal name compounds (PNCs) in German such as Willkommens-Merkel (‘Welcome-Merkel’). Prevalent in news, social media, and political discourse, PNCs are hypothesized to exhibit an evaluative function that is reflected in a more positive or negative perception as compared to the respective personal full name (such as Angela Merkel). We model 321 PNCs and their corresponding full names at discourse level, and show that PNCs bear an evaluative nature that can be captured through a variety of computational methods. Specifically, we assess through valence information whether a PNC is more positively or negatively evaluative than the person’s name, by applying and comparing two approaches using (i) valence norms and (ii) pre-trained language models (PLMs). We further enrich our data with personal, domain-specific, and extra-linguistic information and perform a range of regression analyses revealing that factors including compound and modifier valence, domain, and political party membership influence how a PNC is evaluated.</abstract>
      <url hash="bec4eccf">2024.lrec-main.1534</url>
      <bibkey>eichel-etal-2024-willkommens-merkel</bibkey>
    </paper>
    <paper id="1535">
      <title><fixed-case>W</fixed-case>k<fixed-case>NER</fixed-case>: Enhancing Named Entity Recognition with Word Segmentation Constraints and k<fixed-case>NN</fixed-case> Retrieval</title>
      <author><first>Yanchun</first><last>Li</last></author>
      <author><first>Senlin</first><last>Deng</last></author>
      <author><first>Dongsu</first><last>Shen</last></author>
      <author><first>Shujuan</first><last>Tian</last></author>
      <author><first>Saiqin</first><last>Long</last></author>
      <pages>17651–17663</pages>
      <abstract>Fine-tuning Pre-trained Language Models (PLMs) is a popular Natural Language Processing (NLP) paradigm for addressing Named Entity Recognition (NER) tasks. However, neural network models often demonstrate poor generalization capabilities due to significant disparities between the knowledge learned by PLMs and the distribution of the target dataset, as well as data scarcity issues. In addition, token omission in predictions due to insufficient learning remains a challenge in NER. In this paper, we propose a kNN retrieval enhancement algorithm (WkNER) that incorporates word segmentation information to enhance the model’s generalization ability and alleviate the problem of missing entity tokens in prediction. The introduction of word segmentation information is used to preliminarily determine the boundaries of entities and alleviate the common prediction errors of missing tokens within entities made by the fine-tuned model. Secondly, we find that non-entities in the retrieval table contain a large amount of redundant information, and explore the effects of introducing non-entity information of different scales on the model. Experimental results show that our proposed method significantly improves the performance of baseline models, and achieves better or compared recognition accuracy than previous state-of-the-art models in multiple public Chinese and English datasets. Especially in low-resource scenarios, our method achieves higher accuracy on 20% of the dataset than the original method on the full dataset.</abstract>
      <url hash="0eaa70bf">2024.lrec-main.1535</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4a92b13c">2024.lrec-main.1535.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2024-wkner-enhancing</bibkey>
    </paper>
    <paper id="1536">
      <title>Word-Aware Modality Stimulation for Multimodal Fusion</title>
      <author><first>Shuhei</first><last>Tateishi</last></author>
      <author><first>Makoto</first><last>Nakatsuji</last></author>
      <author><first>Yasuhito</first><last>Osugi</last></author>
      <pages>17664–17674</pages>
      <abstract>Multimodal learning is generally expected to make more accurate predictions than text-only analysis. Here, although various methods for fusing multimodal inputs have been proposed for sentiment analysis tasks, we found that they may be inhibiting their fusion methods, which are based on attention-based language models, from learning non-verbal modalities, because non-verbal ones are isolated from the linguistic semantics and contexts and do not include them, meaning that they are unsuitable for applying attention to text modalities during the fusion phase. To address this issue, we propose Word-aware Modality Stimulation Fusion (WA-MSF) for facilitating integration of non-verbal modalities with the text modality. The Modality Stimulation Unit layer (MSU-layer) is the core concept of WA-MSF; it integrates language contexts and semantics into non-verbal modalities, thereby instilling linguistic essence into these modalities. Moreover, WA-MSF uses aMLP in the fusion phase in order to utilize spatial and temporal representations of non-verbal modalities more effectively than transformer fusion. In our experiments, WA-MSF set a new state-of-the-art level of performance on sentiment prediction tasks.</abstract>
      <url hash="8d7d4f71">2024.lrec-main.1536</url>
      <attachment type="OptionalSupplementaryMaterial" hash="265064a3">2024.lrec-main.1536.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>tateishi-etal-2024-word-aware</bibkey>
      <revision id="1" href="2024.lrec-main.1536v1" hash="a7c67f5b"/>
      <revision id="2" href="2024.lrec-main.1536v2" hash="8d7d4f71" date="2024-06-13">Minor update.</revision>
    </paper>
    <paper id="1537">
      <title>Word-level Commonsense Knowledge Selection for Event Detection</title>
      <author><first>Shuai</first><last>Yang</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Shiming</first><last>He</last></author>
      <author><first>Qingting</first><last>Xu</last></author>
      <author><first>Jianmin</first><last>Yao</last></author>
      <pages>17675–17682</pages>
      <abstract>Event Detection (ED) is a task of automatically extracting multi-class trigger words. The understanding of word sense is crucial for ED. In this paper, we utilize context-specific commonsense knowledge to strengthen word sense modeling. Specifically, we leverage a Context-specific Knowledge Selector (CKS) to select the exact commonsense knowledge of words from a large knowledge base, i.e., ConceptNet. Context-specific selection is made in terms of the relevance of knowledge to the living contexts. On this basis, we incorporate the commonsense knowledge into the word-level representations before decoding. ChatGPT is an ideal generative CKS when the prompts are deliberately designed, though it is cost-prohibitive. To avoid the heavy reliance on ChatGPT, we train an offline CKS using the predictions of ChatGPT over a small number of examples (about 9% of all). We experiment on the benchmark ACE-2005 dataset. The test results show that our approach yields substantial improvements compared to the BERT baseline, achieving the F1-score of about 78.3%. All models, source codes and data will be made publicly available.</abstract>
      <url hash="06fcac32">2024.lrec-main.1537</url>
      <bibkey>yang-etal-2024-word-level</bibkey>
    </paper>
    <paper id="1538">
      <title><fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et under Scrutiny: Dictionary Examples in the Era of Large Language Models</title>
      <author><first>Fatemah Yousef</first><last>Almeman</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <author><first>Luis</first><last>Espinosa Anke</last></author>
      <pages>17683–17695</pages>
      <abstract>Dictionary definitions play a prominent role in a wide range of NLP tasks, for instance by providing additional context about the meaning of rare and emerging terms. Many dictionaries also provide examples to illustrate the prototypical usage of words, which brings further opportunities for training or enriching NLP models. The intrinsic qualities of dictionaries, and related lexical resources such as glossaries and encyclopedias, are however still not well-understood. While there has been significant work on developing best practices, such guidance has been aimed at traditional usages of dictionaries (e.g. supporting language learners), and it is currently unclear how different quality aspects affect the NLP systems that rely on them. To address this issue, we compare WordNet, the most commonly used lexical resource in NLP, with a variety of dictionaries, as well as with examples that were generated by ChatGPT. Our analysis involves human judgments as well as automatic metrics. We furthermore study the quality of word embeddings derived from dictionary examples, as a proxy for downstream performance. We find that WordNet’s examples lead to lower-quality embeddings than those from the Oxford dictionary. Surprisingly, however, the ChatGPT generated examples were found to be most effective overall.</abstract>
      <url hash="cbc42b5b">2024.lrec-main.1538</url>
      <bibkey>almeman-etal-2024-wordnet-scrutiny</bibkey>
    </paper>
    <paper id="1539">
      <title><fixed-case>W</fixed-case>orld<fixed-case>V</fixed-case>alues<fixed-case>B</fixed-case>ench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models</title>
      <author><first>Wenlong</first><last>Zhao</last></author>
      <author><first>Debanjan</first><last>Mondal</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Danica</first><last>Dillion</last></author>
      <author><first>Kurt</first><last>Gray</last></author>
      <author><first>Yuling</first><last>Gu</last></author>
      <pages>17696–17706</pages>
      <abstract>The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses. However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values. In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts. Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide. We have constructed more than 20 million examples of the type "(demographic attributes, value question) → answer” from the WVS responses. We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models. On merely 11.1%, 25.0%, 72.2%, and 75.0% of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve &lt;0.2 Wasserstein 1-distance from the human normalized answer distributions. WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs.</abstract>
      <url hash="aec5419d">2024.lrec-main.1539</url>
      <bibkey>zhao-etal-2024-worldvaluesbench-large</bibkey>
    </paper>
    <paper id="1540">
      <title>Would You Like to Make a Donation? A Dialogue System to Persuade You to Donate</title>
      <author><first>Yuhan</first><last>Song</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>17707–17717</pages>
      <abstract>Persuasive dialogue is a type of dialogue commonly used in human daily life in scenarios such as promotion and sales. Its purpose is to influence the decision, attitude or behavior of another person through the dialogue process. Persuasive automated dialogue systems can be applied in a variety of fields such as charity, business, education, and healthcare. Regardless of their amazing abilities, Large Language Models (LLMs) such as ChatGPT still have limitations in persuasion. There is few research dedicated to persuasive dialogue in the current research of automated dialogue systems. In this paper, we introduce a persuasive automated dialogue system. In the system, a context-aware persuasion strategy selection module makes dialogue system flexibly use different persuasion strategies to persuade users; Then a natural language generation module is used to output a response. We also propose a persuasiveness prediction model to automatically evaluate the persuasiveness of generated text. Experimental results show that our dialogue system can achieve better performance on several automated evaluation metrics than baseline models.</abstract>
      <url hash="16961fef">2024.lrec-main.1540</url>
      <bibkey>song-wang-2024-like-make</bibkey>
    </paper>
    <paper id="1541">
      <title><fixed-case>WW</fixed-case>-<fixed-case>CSL</fixed-case>: A New Dataset for Word-Based Wearable <fixed-case>C</fixed-case>hinese <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Detection</title>
      <author><first>Fan</first><last>Xu</last></author>
      <author><first>Kai</first><last>Liu</last></author>
      <author><first>Yifeng</first><last>Yang</last></author>
      <author><first>Keyu</first><last>Yan</last></author>
      <pages>17718–17724</pages>
      <abstract>Sign language is an effective non-verbal communication mode for the hearing-impaired people. Since the video-based sign language detection models have high requirements for enough lighting and clear background, current wearing glove-based sign language models are robust for poor light and occlusion situations. In this paper, we annotate a new dataset of Word-based Wearable Chinese Sign Languag (WW-CSL) gestures. Specifically, we propose a three-form (e.g., sequential sensor data, gesture video, and gesture text) scheme to represent dynamic CSL gestures. Guided by the scheme, a total of 3,000 samples were collected, corresponding to 100 word-based CSL gestures. Furthermore, we present a transformer-based baseline model to fuse 2 inertial measurement unites (IMUs) and 10 flex sensors for the wearable CSL detection. In order to integrate the advantage of video-based and wearable glove-based CSL gestures, we also propose a transformer-based Multi-Modal CSL Detection (MM-CSLD) framework which adeptly integrates the local sequential sensor data derived from wearable-based CSL gestures with the global, fine-grained skeleton representations captured from video-based CSL gestures simultaneously.</abstract>
      <url hash="9a8ac583">2024.lrec-main.1541</url>
      <attachment type="OptionalSupplementaryMaterial" hash="5fbc542e">2024.lrec-main.1541.OptionalSupplementaryMaterial.rar</attachment>
      <bibkey>xu-etal-2024-ww-csl</bibkey>
    </paper>
    <paper id="1542">
      <title><fixed-case>XAI</fixed-case>-Attack: Utilizing Explainable <fixed-case>AI</fixed-case> to Find Incorrectly Learned Patterns for Black-Box Adversarial Example Creation</title>
      <author><first>Markus</first><last>Bayer</last></author>
      <author><first>Markus</first><last>Neiczer</last></author>
      <author><first>Maximilian</first><last>Samsinger</last></author>
      <author><first>Björn</first><last>Buchhold</last></author>
      <author><first>Christian</first><last>Reuter</last></author>
      <pages>17725–17738</pages>
      <abstract>Adversarial examples, capable of misleading machine learning models into making erroneous predictions, pose significant risks in safety-critical domains such as crisis informatics, medicine, and autonomous driving. To counter this, we introduce a novel textual adversarial example method that identifies falsely learned word indicators by leveraging explainable AI methods as importance functions on incorrectly predicted instances, thus revealing and understanding the weaknesses of a model. To evaluate the effectiveness of our approach, we conduct a human and a transfer evaluation and propose a novel adversarial training evaluation setting for better robustness assessment. While outperforming current adversarial example and training methods, the results also show our method’s potential in facilitating the development of more resilient transformer models by detecting and rectifying biases and patterns in training data, showing baseline improvements of up to 23 percentage points in accuracy on adversarial tasks. The code of our approach is freely available for further exploration and use.</abstract>
      <url hash="836a615e">2024.lrec-main.1542</url>
      <bibkey>bayer-etal-2024-xai-attack</bibkey>
    </paper>
    <paper id="1543">
      <title><fixed-case>XATU</fixed-case>: A Fine-grained Instruction-based Benchmark for Explainable Text Updates</title>
      <author><first>Haopeng</first><last>Zhang</last></author>
      <author><first>Hayate</first><last>Iso</last></author>
      <author><first>Sairam</first><last>Gurajada</last></author>
      <author><first>Nikita</first><last>Bhutani</last></author>
      <pages>17739–17752</pages>
      <abstract>Text editing is a crucial task of modifying text to better align with user intents. However, existing text editing benchmark datasets contain only coarse-grained instructions and lack explainability, thus resulting in outputs that deviate from the intended changes outlined in the gold reference. To comprehensively investigate the text editing capabilities of large language models (LLMs), this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU considers finer-grained text editing tasks of varying difficulty (simplification, grammar check, fact-check, etc.), incorporating lexical, syntactic, semantic, and knowledge-intensive edit aspects. To enhance interpretability, we combine LLM-based annotation and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing LLMs against our benchmark, we demonstrate the effectiveness of instruction tuning and the impact of underlying architecture across various editing tasks. Furthermore, extensive experimentation reveals the significant role of explanations in fine-tuning language models for text editing tasks. The benchmark will be open-sourced to support reproduction and facilitate future research at https://github.com/megagonlabs/xatu.</abstract>
      <url hash="8f2cf3f6">2024.lrec-main.1543</url>
      <bibkey>zhang-etal-2024-xatu-fine</bibkey>
    </paper>
    <paper id="1544">
      <title><fixed-case>XVD</fixed-case>: Cross-Vocabulary Differentiable Training for Generative Adversarial Attacks</title>
      <author><first>Tom</first><last>Roth</last></author>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Alsharif</first><last>Abuadbba</last></author>
      <author><first>Massimo</first><last>Piccardi</last></author>
      <pages>17753–17763</pages>
      <abstract>An adversarial attack to a text classifier consists of an input that induces the classifier into an incorrect class prediction, while retaining all the linguistic properties of correctly-classified examples. A popular class of adversarial attacks exploits the gradients of the victim classifier to train a dedicated generative model to produce effective adversarial examples. However, this training signal alone is not sufficient to ensure other desirable properties of the adversarial attacks, such as similarity to non-adversarial examples, linguistic fluency, grammaticality, and so forth. For this reason, in this paper we propose a novel training objective which leverages a set of pretrained language models to promote such properties in the adversarial generation. A core component of our approach is a set of vocabulary-mapping matrices which allow cascading the generative model to any victim or component model of choice, while retaining differentiability end-to-end. The proposed approach has been tested in an ample set of experiments covering six text classification datasets, two victim models, and four baselines. The results show that it has been able to produce effective adversarial attacks, outperforming the compared generative approaches in a majority of cases and proving highly competitive against established token-replacement approaches.</abstract>
      <url hash="ac5eac54">2024.lrec-main.1544</url>
      <bibkey>roth-etal-2024-xvd-cross</bibkey>
    </paper>
    <paper id="1545">
      <title>Your Stereotypical Mileage May Vary: Practical Challenges of Evaluating Biases in Multiple Languages and Cultural Contexts</title>
      <author><first>Karen</first><last>Fort</last></author>
      <author><first>Laura</first><last>Alonso Alemany</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <author><first>Julien</first><last>Bezançon</last></author>
      <author><first>Claudia</first><last>Borg</last></author>
      <author><first>Marthese</first><last>Borg</last></author>
      <author><first>Yongjian</first><last>Chen</last></author>
      <author><first>Fanny</first><last>Ducel</last></author>
      <author><first>Yoann</first><last>Dupont</last></author>
      <author><first>Guido</first><last>Ivetta</last></author>
      <author><first>Zhijian</first><last>Li</last></author>
      <author><first>Margot</first><last>Mieskes</last></author>
      <author><first>Marco</first><last>Naguib</last></author>
      <author><first>Yuyan</first><last>Qian</last></author>
      <author><first>Matteo</first><last>Radaelli</last></author>
      <author><first>Wolfgang S.</first><last>Schmeisser-Nieto</last></author>
      <author><first>Emma</first><last>Raimundo Schulz</last></author>
      <author><first>Thiziri</first><last>Saci</last></author>
      <author><first>Sarah</first><last>Saidi</last></author>
      <author><first>Javier</first><last>Torroba Marchante</last></author>
      <author><first>Shilin</first><last>Xie</last></author>
      <author><first>Sergio E.</first><last>Zanotto</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>17764–17769</pages>
      <abstract>Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting The study of bias, fairness and social impact in Natural Language Processing (NLP) lacks resources in languages other than English. Our objective is to support the evaluation of bias in language models in a multilingual setting. We use stereotypes across nine types of biases to build a corpus containing contrasting sentence pairs, one sentence that presents a stereotype concerning an underadvantaged group and another minimally changed sentence, concerning a matching advantaged group. We build on the French CrowS-Pairs corpus and guidelines to provide translations of the existing material into seven additional languages. In total, we produce 11,139 new sentence pairs that cover stereotypes dealing with nine types of biases in seven cultural contexts. We use the final resource for the evaluation of relevant monolingual and multilingual masked language models. We find that language models in all languages favor sentences that express stereotypes in most bias categories. The process of creating a resource that covers a wide range of language types and cultural settings highlights the difficulty of bias evaluation, in particular comparability across languages and contexts.</abstract>
      <url hash="6c5b794b">2024.lrec-main.1545</url>
      <bibkey>fort-etal-2024-stereotypical-mileage</bibkey>
    </paper>
    <paper id="1546">
      <title><fixed-case>ZAEBUC</fixed-case>-Spoken: A Multilingual Multidialectal <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>nglish Speech Corpus</title>
      <author><first>Injy</first><last>Hamed</last></author>
      <author><first>Fadhl</first><last>Eryani</last></author>
      <author><first>David</first><last>Palfreyman</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>17770–17782</pages>
      <abstract>We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech corpus. The corpus comprises twelve hours of Zoom meetings involving multiple speakers role-playing a work situation where Students brainstorm ideas for a certain topic and then discuss it with an Interlocutor. The meetings cover different topics and are divided into phases with different language setups. The corpus presents a challenging set for automatic speech recognition (ASR), including two languages (Arabic and English) with Arabic spoken in multiple variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English used with various accents. Adding to the complexity of the corpus, there is also code-switching between these languages and dialects. As part of our work, we take inspiration from established sets of transcription guidelines to present a set of guidelines handling issues of conversational speech, code-switching and orthography of both languages. We further enrich the corpus with two layers of annotations; (1) dialectness level annotation for the portion of the corpus where mixing occurs between different variants of Arabic, and (2) automatic morphological annotations, including tokenization, lemmatization, and part-of-speech tagging.</abstract>
      <url hash="f81ef68e">2024.lrec-main.1546</url>
      <bibkey>hamed-etal-2024-zaebuc-spoken</bibkey>
    </paper>
    <paper id="1547">
      <title><fixed-case>Z</fixed-case>e<fixed-case>L</fixed-case>a: Advancing Zero-Shot Multilingual Semantic Parsing with Large Language Models and Chain-of-Thought Strategies</title>
      <author><first>Truong Dinh</first><last>Do</last></author>
      <author><first>Phuong Minh</first><last>Nguyen</last></author>
      <author><first>Minh</first><last>Nguyen</last></author>
      <pages>17783–17794</pages>
      <abstract>In recent years, there have been significant advancements in semantic parsing tasks, thanks to the introduction of pre-trained language models. However, a substantial gap persists between English and other languages due to the scarcity of annotated data. One promising strategy to bridge this gap involves augmenting multilingual datasets using labeled English data and subsequently leveraging this augmented dataset for training semantic parsers (known as zero-shot multilingual semantic parsing). In our study, we propose a novel framework to effectively perform zero-shot multilingual semantic parsing under the support of large language models (LLMs). Given data annotated pairs (sentence, semantic representation) in English, our proposed framework automatically augments data in other languages via multilingual chain-of-thought (CoT) prompting techniques that progressively construct the semantic form in these languages. By breaking down the entire semantic representation into sub-semantic fragments, our CoT prompting technique simplifies the intricate semantic structure at each step, thereby facilitating the LLMs in generating accurate outputs more efficiently. Notably, this entire augmentation process is achieved without the need for any demonstration samples in the target languages (zero-shot learning). In our experiments, we demonstrate the effectiveness of our method by evaluating it on two well-known multilingual semantic parsing datasets: MTOP and MASSIVE.</abstract>
      <url hash="999c907f">2024.lrec-main.1547</url>
      <bibkey>do-etal-2024-zela-advancing</bibkey>
    </paper>
    <paper id="1548">
      <title><fixed-case>Z</fixed-case>en<fixed-case>P</fixed-case>ropaganda: A Comprehensive Study on Identifying Propaganda Techniques in <fixed-case>R</fixed-case>ussian Coronavirus-Related Media</title>
      <author><first>Anton</first><last>Chernyavskiy</last></author>
      <author><first>Svetlana</first><last>Shomova</last></author>
      <author><first>Irina</first><last>Dushakova</last></author>
      <author><first>Ilya</first><last>Kiriya</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>17795–17807</pages>
      <abstract>The topic of automatic detection of manipulation and propaganda in the media is not a novel issue; however, it remains an urgent concern that necessitates continuous research focus. The topic is studied within the framework of various papers, competitions and shared tasks, which provide different techniques definitions and include the analysis of text data, images, as well as multi-lingual sources. In this study, we propose a novel multi-level classification scheme for identifying propaganda techniques. We introduce a new Russian dataset ZenPropaganda consisting of coronavirus-related texts collected from Vkontakte and Yandex.Zen platforms, which have been expertly annotated with fine-grained labeling of manipulative spans. We further conduct a comprehensive analysis by comparing our dataset with existing related ones and evaluate the performance of state-of-the-art approaches that have been proposed for them. Furthermore, we provide a detailed discussion of our findings, which can serve as a valuable resource for future research in this field.</abstract>
      <url hash="89ba0aec">2024.lrec-main.1548</url>
      <bibkey>chernyavskiy-etal-2024-zenpropaganda-comprehensive</bibkey>
    </paper>
    <paper id="1549">
      <title>Zero- and Few-Shot Prompting with <fixed-case>LLM</fixed-case>s: A Comparative Study with Fine-tuned Models for <fixed-case>B</fixed-case>angla Sentiment Analysis</title>
      <author><first>Md. Arid</first><last>Hasan</last></author>
      <author><first>Shudipta</first><last>Das</last></author>
      <author><first>Afiyat</first><last>Anjum</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Anika</first><last>Anjum</last></author>
      <author><first>Avijit</first><last>Sarker</last></author>
      <author><first>Sheak Rashed Haider</first><last>Noori</last></author>
      <pages>17808–17818</pages>
      <abstract>The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,606 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community.</abstract>
      <url hash="7180f4fd">2024.lrec-main.1549</url>
      <bibkey>hasan-etal-2024-zero-shot</bibkey>
    </paper>
    <paper id="1550">
      <title>Zero-shot Cross-lingual Automated Essay Scoring</title>
      <author><first>Junyi</first><last>He</last></author>
      <author><first>Xia</first><last>Li</last></author>
      <pages>17819–17832</pages>
      <abstract>Due to the difficulty of creating high-quality labelled training data for different languages, the low-resource problem is crucial yet challenging for automated essay scoring (AES). However, little attention has been paid to addressing this challenge. In this paper, we propose a novel zero-shot cross-lingual scoring method from the perspectives of pretrained multilingual representation and writing quality alignment to score essays in unseen languages. Specifically, we adopt multilingual pretrained language models as the encoder backbone to deeply and comprehensively represent multilingual essays. Motivated by the fact that the scoring knowledge for evaluating writing quality is comparable across different languages, we introduce an innovative strategy for aligning essays in a language-independent manner. The proposed strategy aims to capture shared knowledge from diverse languages, thereby enhancing the representation of essays written in unseen languages with respect to their quality. We include essay datasets in six languages (Czech, German, English, Spanish, Italian and Portuguese) to establish extensive experiments, and the results demonstrate that our method achieves state-of-the-art cross-lingual scoring performance.</abstract>
      <url hash="a69da2f9">2024.lrec-main.1550</url>
      <bibkey>he-li-2024-zero-shot</bibkey>
    </paper>
    <paper id="1551">
      <title>Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning</title>
      <author><first>Zhitao</first><last>He</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>17833–17850</pages>
      <abstract>Event Causality Identification (ECI) refers to the detection of causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource languages, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over a document. Then, to improve cross-lingual transferability of causal knowledge learned from the source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms the previous state-of-the-art model by 9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios respectively. Notably, in the multilingual scenario, our zero-shot framework even exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance.</abstract>
      <url hash="33d36841">2024.lrec-main.1551</url>
      <bibkey>he-etal-2024-zero-shot-cross</bibkey>
    </paper>
    <paper id="1552">
      <title>Zero-shot Event Detection Using a Textual Entailment Model as an Enhanced Annotator</title>
      <author><first>Ziqian</first><last>Zeng</last></author>
      <author><first>Runyu</first><last>Wu</last></author>
      <author><first>Yuxiang</first><last>Xiao</last></author>
      <author><first>Xiaoda</first><last>Zhong</last></author>
      <author><first>Hanlin</first><last>Wang</last></author>
      <author><first>Zhengdong</first><last>Lu</last></author>
      <author><first>Huiping</first><last>Zhuang</last></author>
      <pages>17851–17857</pages>
      <abstract>Zero-shot event detection is a challenging task. Recent research work proposed to use a pre-trained textual entailment (TE) model on this task. However, those methods treated the TE model as a frozen annotator. We treat the TE model as an annotator that can be enhanced. We propose to use TE models to annotate large-scale unlabeled text and use annotated data to finetune the TE model, yielding an improved TE model. Finally, the improved TE model is used for inference on the test set. To improve the efficiency, we propose to use keywords to filter out sentences with a low probability of expressing event(s). To improve the coverage of keywords, we expand limited number of seed keywords using WordNet, so that we can use the TE model to annotate unlabeled text efficiently. The experimental results show that our method can outperform other baselines by 15% on the ACE05 dataset.</abstract>
      <url hash="e3932438">2024.lrec-main.1552</url>
      <bibkey>zeng-etal-2024-zero-shot</bibkey>
    </paper>
    <paper id="1553">
      <title>Zero-shot Learning for Multilingual Discourse Relation Classification</title>
      <author><first>Eleni</first><last>Metheniti</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Margarita</first><last>Hernández Casas</last></author>
      <pages>17858–17876</pages>
      <abstract>Classifying discourse relations is known as a hard task, relying on complex indices. On the other hand, discourse-annotated data is scarce, especially for languages other than English: many corpora, of limited size, exist for several languages but the domain is split between different theoretical frameworks that have a huge impact on the nature of the textual spans to be linked, and the label set used. Moreover, each annotation project implements modifications compared to the theoretical background and other projects. These discrepancies hinder the development of systems taking advantage of all the available data to tackle data sparsity and work on transfer between languages is very limited, almost nonexistent between frameworks, while it could improve our understanding of some theoretical aspects and enhance many applications. In this paper, we propose the first experiments on zero-shot learning for discourse relation classification and investigate several paths in the way source data can be combined, either based on languages, frameworks, or similarity measures. We demonstrate how difficult transfer is for the task at hand, and that the most impactful factor is label set divergence, where the notion of underlying framework possibly conceals crucial disagreements.</abstract>
      <url hash="d09a41e9">2024.lrec-main.1553</url>
      <bibkey>metheniti-etal-2024-zero-shot</bibkey>
    </paper>
    <paper id="1554">
      <title>Zero-Shot Spoken Language Understanding via Large Language Models: A Preliminary Study</title>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Hao</first><last>An</last></author>
      <author><first>Zhichang</first><last>Wang</last></author>
      <author><first>Dongsheng</first><last>Chen</last></author>
      <author><first>Zhiqi</first><last>Huang</last></author>
      <pages>17877–17883</pages>
      <abstract>Zero-shot Spoken Language Understanding (SLU) aims to enable task-oriented dialogue systems to understand user needs without training data. Challenging but worthwhile, zero-shot SLU reduces the time and effort that data labeling takes. Recent advancements in large language models (LLMs), such as GPT3.5 and ChatGPT, have shown promising results in zero-shot settings, which motivates us to explore prompt-based methods. In this study, we investigate whether strong SLU models can be constructed by directly prompting LLMs. Specifically, we propose a simple yet effective two-stage framework dubbed GPT-SLU, which transforms the SLU task into a question-answering problem. Powered by multi-stage mutual guided prompts, GPT-SLU can leverage the correlations between two subtasks in SLU to achieve better predictions, which is greatly explored in the traditional fine-tuning paradigm. Experimental results on three SLU benchmark datasets demonstrate the significant potential of LLMs for zero-shot SLU. Comprehensive analyses validate the effectiveness of our proposed framework and also indicate that there is still room for further improvement of LLMs in SLU scenarios.</abstract>
      <url hash="ac4bd9c7">2024.lrec-main.1554</url>
      <bibkey>zhu-etal-2024-zero-shot</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2024-05-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries</booktitle>
      <editor><first>Roman</first><last>Klinger</last></editor>
      <editor><first>Naozaki</first><last>Okazaki</last></editor>
      <editor><first>Nicoletta</first><last>Calzolari</last></editor>
      <editor><first>Min-Yen</first><last>Kan</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="15bba0c4">2024.lrec-tutorials</url>
      <venue>lrec</venue>
    </meta>
    <frontmatter>
      <url hash="c022a703">2024.lrec-tutorials.0</url>
      <bibkey>lrec-2024-2024-joint</bibkey>
    </frontmatter>
    <paper id="1">
      <title>From Multimodal <fixed-case>LLM</fixed-case> to Human-level <fixed-case>AI</fixed-case>: Modality, Instruction, Reasoning, Efficiency and beyond</title>
      <author><first>Hao</first><last>Fei</last></author>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Ao</first><last>Zhang</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>1–8</pages>
      <abstract>Artificial intelligence (AI) encompasses knowledge acquisition and real-world grounding across various modalities. As a multidisciplinary research field, multimodal large language models (MLLMs) have recently garnered growing interest in both academia and industry, showing an unprecedented trend to achieve human-level AI via MLLMs. These large models offer an effective vehicle for understanding, reasoning, and planning by integrating and modeling diverse information modalities, including language, visual, auditory, and sensory data. This tutorial aims to deliver a comprehensive review of cutting-edge research in MLLMs, focusing on four key areas: MLLM architecture design, instructional learning, multimodal reasoning, and the efficiency of MLLMs. We will explore technical advancements, synthesize key challenges, and discuss potential avenues for future research.</abstract>
      <url hash="c40d7c45">2024.lrec-tutorials.1</url>
      <bibkey>fei-etal-2024-multimodal</bibkey>
    </paper>
    <paper id="2">
      <title>Geo-Cultural Representation and Inclusion in Language Technologies</title>
      <author><first>Sunipa</first><last>Dev</last></author>
      <author><first>Rida</first><last>Qadri</last></author>
      <pages>9–12</pages>
      <abstract>Training and evaluation of language models are increasingly relying on semi-structured data that is annotated by humans, along with techniques such as RLHF growing in usage across the board. As a result, both the data and the human perspectives involved in this process play a key role in what is taken as ground truth by our models. As annotation tasks are becoming increasingly more subjective and culturally complex, it is unclear how much of their socio-cultural identity annotators use to respond to tasks. We also currently do not have ways to integrate rich and diverse community perspectives into our language technologies. Accounting for such cross-cultural differences in interacting with technology is an increasingly crucial step for evaluating AI harms holistically. Without this, the state of the art of the AI models being deployed is at risk of causing unprecedented biases at a global scale. In this tutorial, we will take an interactive approach by utilizing some different types of annotation tasks to investigate together how our different socio-cultural perspectives and lived experiences influence what we consider as appropriate representations of global concepts.</abstract>
      <url hash="6785235f">2024.lrec-tutorials.2</url>
      <bibkey>dev-qadri-2024-geo</bibkey>
    </paper>
    <paper id="3">
      <title>Meaning Representations for Natural Languages: Design, Models and Applications</title>
      <author><first>Julia</first><last>Bonn</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Ishan</first><last>Jindal</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>13–18</pages>
      <abstract>This tutorial reviews the design of common meaning representations, SoTA models for predicting meaning representations, and the applications of meaning representations in a wide range of downstream NLP tasks and real-world applications. Reporting by a diverse team of NLP researchers from academia and industry with extensive experience in designing, building and using meaning representations, our tutorial has three components: (1) an introduction to common meaning representations, including basic concepts and design challenges; (2) a review of SoTA methods on building models for meaning representations; and (3) an overview of applications of meaning representations in downstream NLP tasks and real-world applications. We propose a cutting-edge, full-day tutorial for all stakeholders in the AI community, including NLP researchers, domain-specific practitioners, and students</abstract>
      <url hash="76df5399">2024.lrec-tutorials.3</url>
      <bibkey>bonn-etal-2024-meaning</bibkey>
    </paper>
    <paper id="4">
      <title>Navigating the Modern Evaluation Landscape: Considerations in Benchmarks and Frameworks for Large Language Models (<fixed-case>LLM</fixed-case>s)</title>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Ariel</first><last>Gera</last></author>
      <author><first>Yotam</first><last>Perlitz</last></author>
      <author><first>Michal</first><last>Shmueli-Scheuer</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>19–25</pages>
      <abstract>General-Purpose Language Models have changed the world of Natural Language Processing, if not the world itself. The evaluation of such versatile models, while supposedly similar to evaluation of generation models before them, in fact presents a host of new evaluation challenges and opportunities. In this Tutorial, we will start from the building blocks of evaluation. The tutorial welcomes people from diverse backgrounds and assumes little familiarity with metrics, datasets, prompts and benchmarks. It will lay the foundations and explain the basics and their importance, while touching on the major points and breakthroughs of the recent era of evaluation. It will also compare traditional evaluation methods – which are still widely used – to newly developed methods. We will contrast new to old approaches, from evaluating on many-task benchmarks rather than on dedicated datasets to efficiency constraints, and from testing stability and prompts on in-context learning to using the models themselves as evaluation metrics. Finally, the tutorial will cover practical issues, ranging from reviewing widely-used benchmarks and prompt banks to efficient evaluation.</abstract>
      <url hash="5552e2cc">2024.lrec-tutorials.4</url>
      <bibkey>choshen-etal-2024-navigating</bibkey>
    </paper>
    <paper id="5">
      <title>Mining, Assessing, and Improving Arguments in <fixed-case>NLP</fixed-case> and the Social Sciences</title>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Eva Maria</first><last>Vecchi</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>26–32</pages>
      <abstract>Computational argumentation is an interdisciplinary research field, connecting Natural Language Processing (NLP) to other disciplines such as the social sciences. The focus of recent research has concentrated on <i>argument quality assessment</i>: what makes an argument good or bad? We present a tutorial which is an updated edition of the EACL 2023 tutorial presented by the same authors. As in the previous version, the tutorial will have a strong interdisciplinary and interactive nature, and will be structured along three main coordinates: (1) the notions of argument quality (AQ) across disciplines (how do we recognize good and bad arguments?), with a particular focus on the interface between Argument Mining (AM) and Deliberation Theory; (2) the modeling of subjectivity (who argues to whom; what are their beliefs?); and (3) the generation of improved arguments (what makes an argument better?). The tutorial will also touch upon a series of topics that are particularly relevant for the LREC-COLING audience (the issue of resource quality for the assessment of AQ; the interdisciplinary application of AM and AQ in a text-as-data approach to Political Science), in line with the developments in NLP (LLMs for AQ assessment), and relevant for the societal applications of AQ assessment (bias and debiasing). We will involve the participants in two annotation studies on the assessment and the improvement of quality.</abstract>
      <url hash="ad00e52c">2024.lrec-tutorials.5</url>
      <bibkey>lapesa-etal-2024-mining</bibkey>
    </paper>
    <paper id="6">
      <title>Knowledge Editing for Large Language Models</title>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Yunzhi</first><last>Yao</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <pages>33–41</pages>
      <abstract>Even with their impressive abilities, Large Language Models (LLMs) such as ChatGPT are not immune to issues of factual or logically consistent. Concretely, the key concern is how to seamlessly update those LLMs to correct mistakes without resorting to an exhaustive retraining or continuous training procedure, both of which can demand significant computational resources and time. Thus, the capability to edit LLMs offers an efficient solution to alter a model’s behavior, notably within a distinct area of interest, without negatively impacting its performance on other tasks. Through this tutorial, we strive to acquaint interested NLP researchers with recent and emerging techniques for editing LLMs. Specifically, we aim to present a systematic and current overview of cutting-edge methods, supplemented with practical tools, and unveil new research opportunities for our audiences. All the valuable resources can be accessed at https://github.com/zjunlp/KnowledgeEditingPapers.</abstract>
      <url hash="bb1b8586">2024.lrec-tutorials.6</url>
      <bibkey>zhang-etal-2024-knowledge</bibkey>
    </paper>
    <paper id="7">
      <title>The <fixed-case>DB</fixed-case>pedia Databus Tutorial: Increase the Visibility and Usability of Your Data</title>
      <author><first>Milan</first><last>Dojchinovski</last></author>
      <pages>42–44</pages>
      <abstract>This tutorial introduces DBpedia Databus (https://databus.dbpedia.org), a FAIR data publishing platform, to address challenges faced by data producers and consumers. It covers data organization, publishing, and consumption on the DBpedia Databus, with an exclusive focus on Linguistic Knowledge Graphs. The tutorial offers practical insights for knowledge graph stakeholders, aiding data integration and accessibility in the Linked Open Data community. Designed for a diverse audience, it fosters hands-on learning to familiarize participants with the DBpedia Databus technology.</abstract>
      <url hash="335a6229">2024.lrec-tutorials.7</url>
      <bibkey>dojchinovski-2024-dbpedia</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>NLP</fixed-case> for Chemistry – Introduction and Recent Advances</title>
      <author><first>Camilo</first><last>Thorne</last></author>
      <author><first>Saber</first><last>Akhondi</last></author>
      <pages>45–49</pages>
      <abstract>In this half-day tutorial we will be giving an introductory overview to a number of recent applications of natural language processing to a relatively underrepresented application domain: chemistry. Specifically, we will see how neural language models (transformers) can be applied (oftentimes with near-human performance) to chemical text mining, reaction extraction, or more importantly computational chemistry (forward and backward synthesis of chemical compounds). At the same time, a number of gold standards for experimentation have been made available to the research –academic and otherwise– community. Theoretical results will be, whenever possible, supported by system demonstrations in the form of Jupyter notebooks. This tutorial targets an audience interested in bioinformatics and biomedical applications, but pre-supposes no advanced knowledge of either.</abstract>
      <url hash="8b57ca46">2024.lrec-tutorials.8</url>
      <bibkey>thorne-akhondi-2024-nlp</bibkey>
    </paper>
    <paper id="9">
      <title>Formal Semantic Controls over Language Models</title>
      <author><first>Danilo</first><last>Silva de Carvalho</last></author>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>50–55</pages>
      <abstract>Text embeddings provide a concise representation of the semantics of sentences and larger spans of text, rather than individual words, capturing a wide range of linguistic features. They have found increasing application to a variety of NLP tasks, including machine translation and natural language inference. While most recent breakthroughs in task performance are being achieved by large scale distributional models, there is a growing disconnection between their knowledge representation and traditional semantics, which hinders efforts to capture such knowledge in human interpretable form or explain model inference behaviour. In this tutorial, we examine from basics to the cutting edge research on the analysis and control of text representations, aiming to shorten the gap between deep latent semantics and formal symbolics. This includes the considerations on knowledge formalisation, the linguistic information that can be extracted and measured from distributional models, and intervention techniques that enable explainable reasoning and controllable text generation, covering methods from pooling to LLM-based.</abstract>
      <url hash="f4d501c4">2024.lrec-tutorials.9</url>
      <bibkey>silva-de-carvalho-etal-2024-formal</bibkey>
    </paper>
    <paper id="10">
      <title>Towards a Human-Computer Collaborative Scientific Paper Lifecycle: A Pilot Study and Hands-On Tutorial</title>
      <author><first>Qingyun</first><last>Wang</last></author>
      <author><first>Carl</first><last>Edwards</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Tom</first><last>Hope</last></author>
      <pages>56–67</pages>
      <abstract>Due to the rapid growth of publications varying in quality, there exists a pressing need to help scientists digest and evaluate relevant papers, thereby facilitating scientific discovery. This creates a number of urgent questions; however, computer-human collaboration in the scientific paper lifecycle is still in the exploratory stage and lacks a unified framework for analyzing the relevant tasks. Additionally, with the recent significant success of large language models (LLMs), they have increasingly played an important role in academic writing. In this cutting-edge tutorial, we aim to provide an all-encompassing overview of the paper lifecycle, detailing how machines can augment every stage of the research process for the scientist, including scientific literature understanding, experiment development, manuscript draft writing, and finally draft evaluation. This tutorial is devised for researchers interested in this rapidly-developing field of NLP-augmented paper writing. The tutorial will also feature a session of hands-on exercises during which participants can guide machines in generating ideas and automatically composing key paper elements. Furthermore, we will address current challenges, explore future directions, and discuss potential ethical issues. A toolkit designed for human-computer collaboration throughout the paper lifecycle will also be made publically available.</abstract>
      <url hash="48c515ae">2024.lrec-tutorials.10</url>
      <bibkey>wang-etal-2024-towards</bibkey>
    </paper>
    <paper id="11">
      <title>Tutorial Proposal: Hallucination in Large Language Models</title>
      <author><first>Vipula</first><last>Rawte</last></author>
      <author><first>Aman</first><last>Chadha</last></author>
      <author><first>Amit</first><last>Sheth</last></author>
      <author><first>Amitava</first><last>Das</last></author>
      <pages>68–72</pages>
      <abstract>In the fast-paced domain of Large Language Models (LLMs), the issue of hallucination is a prominent challenge. Despite continuous endeavors to address this concern, it remains a highly active area of research within the LLM landscape. Grasping the intricacies of this problem can be daunting, especially for those new to the field. This tutorial aims to bridge this knowledge gap by introducing the emerging realm of hallucination in LLMs. It will comprehensively explore the key aspects of hallucination, including benchmarking, detection, and mitigation techniques. Furthermore, we will delve into the specific constraints and shortcomings of current approaches, providing valuable insights to guide future research efforts for participants.</abstract>
      <url hash="21947296">2024.lrec-tutorials.11</url>
      <bibkey>rawte-etal-2024-tutorial</bibkey>
    </paper>
    <paper id="12">
      <title>Addressing Bias and Hallucination in Large Language Models</title>
      <author><first>Nihar Ranjan</first><last>Sahoo</last></author>
      <author><first>Ashita</first><last>Saxena</last></author>
      <author><first>Kishan</first><last>Maharaj</last></author>
      <author><first>Arif A.</first><last>Ahmad</last></author>
      <author><first>Abhijit</first><last>Mishra</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>73–79</pages>
      <abstract>In the landscape of natural language processing (NLP), addressing the challenges of bias and hallucination is paramount to ensuring the ethical and unbiased development of Large Language Models (LLMs). This tutorial delves into the intricate dimensions of LLMs, shedding light on the critical importance of understanding and mitigating the profound impacts of bias and hallucination. Divided into two parts, the first part delves deep into the complexity of bias propagation in LLM development, where we dissect its origins and far-reaching impacts. We then present innovative methodologies for mitigating diverse forms of bias, including dynamic word embeddings and robust benchmarking strategies. The second part of the tutorial discusses hallucination - a prevalent issue in generative AI systems such as LLMs. Through advanced data-driven techniques, we decode its intricate effects and complexities, followed factually-driven mitigation strategies. Furthermore, we shed light on the pivotal role of human cognitive behavior in the context of hallucination, drawing insights from cognitive data, including human eye-tracking data. Ultimately, this cutting-edge tutorial serves as a guiding light, equipping participants with indispensable tools and insights to navigate the ethical complexities of LLMs, thus paving the way for the development of unbiased and ethically robust NLP systems.</abstract>
      <url hash="44e8fc20">2024.lrec-tutorials.12</url>
      <bibkey>sahoo-etal-2024-addressing</bibkey>
    </paper>
    <paper id="13">
      <title>Knowledge-enhanced Response Generation in Dialogue Systems: Current Advancements and Emerging Horizons</title>
      <author><first>Priyanshu</first><last>Priya</last></author>
      <author><first>Deeksha</first><last>Varshney</last></author>
      <author><first>Mauajama</first><last>Firdaus</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>80–87</pages>
      <abstract>This tutorial provides an in-depth exploration of Knowledge-enhanced Dialogue Systems (KEDS), diving into their foundational aspects, methodologies, advantages, and practical applications. Topics include the distinction between internal and external knowledge integration, diverse methodologies employed in grounding dialogues, and innovative approaches to leveraging knowledge graphs for enhanced conversation quality. Furthermore, the tutorial touches upon the rise of biomedical text mining, the advent of domain-specific language models, and the challenges and strategies specific to medical dialogue generation. The primary objective is to give attendees a comprehensive understanding of KEDS. By delineating the nuances of these systems, the tutorial aims to elucidate their significance, highlight advancements made using deep learning, and pinpoint the current challenges. Special emphasis is placed on showcasing how KEDS can be fine-tuned for domain-specific requirements, with a spotlight on the healthcare sector. The tutorial is crafted for both beginners and intermediate researchers in the dialogue systems domain, with a focus on those keen on advancing research in KEDS. It will also be valuable for practitioners in sectors like healthcare, seeking to integrate advanced dialogue systems.</abstract>
      <url hash="26fbb81e">2024.lrec-tutorials.13</url>
      <bibkey>priya-etal-2024-knowledge</bibkey>
    </paper>
  </volume>
  <event id="lrec-2024">
    <meta>
      <title>The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
      <location>Torino, Italia</location>
      <dates>May, 2024</dates>
    </meta>
    <links>
      <url type="website">https://lrec-coling-2024.org</url>
    </links>
    <colocated>
      <volume-id>2024.bucc-1</volume-id>
      <volume-id>2024.cawl-1</volume-id>
      <volume-id>2024.cl4health-1</volume-id>
      <volume-id>2024.cogalex-1</volume-id>
      <volume-id>2024.determit-1</volume-id>
      <volume-id>2024.delite-1</volume-id>
      <volume-id>2024.dlnld-1</volume-id>
      <volume-id>2024.dmr-1</volume-id>
      <volume-id>2024.ecnlp-1</volume-id>
      <volume-id>2024.eurali-1</volume-id>
      <volume-id>2024.finnlp-1</volume-id>
      <volume-id>2024.games-1</volume-id>
      <volume-id>2024.htres-1</volume-id>
      <volume-id>2024.humeval-1</volume-id>
      <volume-id>2024.isa-1</volume-id>
      <volume-id>2024.ldl-1</volume-id>
      <volume-id>2024.legal-1</volume-id>
      <volume-id>2024.lt4hala-1</volume-id>
      <volume-id>2024.mathnlp-1</volume-id>
      <volume-id>2024.mwe-1</volume-id>
      <volume-id>2024.neusymbridge-1</volume-id>
      <volume-id>2024.nlperspectives-1</volume-id>
      <volume-id>2024.osact-1</volume-id>
      <volume-id>2024.parlaclarin-1</volume-id>
      <volume-id>2024.politicalnlp-1</volume-id>
      <volume-id>2024.rail-1</volume-id>
      <volume-id>2024.rapid-1</volume-id>
      <volume-id>2024.readi-1</volume-id>
      <volume-id>2024.rfp-1</volume-id>
      <volume-id>2024.safety4convai-1</volume-id>
      <volume-id>2024.sigul-1</volume-id>
      <volume-id>2024.signlang-1</volume-id>
      <volume-id>2024.tdle-1</volume-id>
      <volume-id>2024.trac-1</volume-id>
      <volume-id>2024.unlp-1</volume-id>
      <volume-id>2024.wildre-1</volume-id>
    </colocated>
  </event>
</collection>
