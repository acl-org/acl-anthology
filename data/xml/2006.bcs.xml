<?xml version='1.0' encoding='UTF-8'?>
<collection id="2006.bcs">
  <volume id="1" ingest-date="2021-09-13" type="proceedings">
    <meta>
      <booktitle>Proceedings of the International Conference on the Challenge of Arabic for NLP/MT</booktitle>
      <address>London, UK</address>
      <month>October 23</month>
      <year>2006</year>
      <venue>bcs</venue>
    </meta>
    <paper id="1">
      <title>Challenges in Processing Colloquial <fixed-case>A</fixed-case>rabic</title>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <author><first>Richard</first><last>Sproat</last></author>
      <author><first>Elabbas</first><last>Benmamoun</last></author>
      <pages>4-14</pages>
      <url hash="81ed56a1">2006.bcs-1.1</url>
      <abstract>Processing of Colloquial Arabic is a relatively new area of research, and a number of interesting challenges pertaining to spoken Arabic dialects arise. On the one hand, a whole continuum of Arabic dialects exists, with linguistic differences on phonological, morphological, syntactic, and lexical levels. On the other hand, there are inter-dialectal similarities that need be explored. Furthermore, due to scarcity of dialect-specific linguistic resources and availability of a wide range of resources for Modern Standard Arabic (MSA), it is desirable to explore the possibility of exploiting MSA tools when working on dialects. This paper describes challenges in processing of Colloquial Arabic in the context of language modeling for Automatic Speech Recognition. Using data from Egyptian Colloquial Arabic and MSA, we investigate the question of improving language modeling of Egyptian Arabic with MSA data and resources. As part of the project, we address the problem of linguistic variation between Egyptian Arabic and MSA. To account for differences between MSA and Colloquial Arabic, we experiment with the following techniques of data transformation: morphological simplification (stemming), lexical transductions, and syntactic transformations. While the best performing model remains the one built using only dialectal data, these techniques allow us to obtain an improvement over the baseline MSA model. More specifically, while the effect on perplexity of syntactic transformations is not very significant, stemming of the training and testing data improves the baseline perplexity of the MSA model trained on words by 51%, and lexical transductions yield an 82% perplexity reduction. Although the focus of the present work is on language modeling, we believe the findings of the study will be useful for researchers involved in other areas of processing Arabic dialects, such as parsing and machine translation.</abstract>
      <bibkey>rozovskaya-etal-2006-challenges</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>A</fixed-case>rabic <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et and the Challenges of <fixed-case>A</fixed-case>rabic</title>
      <author><first>Sabri</first><last>Elkateb</last></author>
      <author><first>William</first><last>Black</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>David</first><last>Farwell</last></author>
      <author><first>Horacio</first><last>Rodríguez</last></author>
      <author><first>Adam</first><last>Pease</last></author>
      <author><first>Musa</first><last>Alkhalifa</last></author>
      <author><first>Christiane</first><last>Fellbaum</last></author>
      <pages>15-24</pages>
      <url hash="a371ee44">2006.bcs-1.2</url>
      <abstract>Arabic WordNet is a lexical resource for Modern Standard Arabic based on the widely used Princeton WordNet for English (Fellbaum, 1998). Arabic WordNet (AWN) is based on the design and contents of the universally accepted Princeton WordNet (PWN) and will be mappable straightforwardly onto PWN 2.0 and EuroWordNet (EWN), enabling translation on the lexical level to English and dozens of other languages. We have developed and linked the AWN with the Suggested Upper Merged Ontology (SUMO), where concepts are defined with machine interpretable semantics in first order logic (Niles and Pease, 2001). We have greatly extended the ontology and its set of mappings to provide formal terms and definitions for each synset. The end product would be a linguistic resource with a deep formal semantic foundation that is able to capture the richness of Arabic as described in Elkateb (2005). Tools we have developed as part of this effort include a lexicographer's interface modeled on that used for EuroWordNet, with added facilities for Arabic script, following Black and Elkateb's earlier work (2004). In this paper we describe our methodology for building a lexical resource in Arabic and the challenge of Arabic for lexical resources.</abstract>
      <bibkey>elkateb-etal-2006-arabic</bibkey>
    </paper>
    <paper id="3">
      <title>Tips and Tricks of the <fixed-case>P</fixed-case>rague <fixed-case>A</fixed-case>rabic Dependency Treebank</title>
      <author><first>Otakar</first><last>Smrž</last></author>
      <pages>25-34</pages>
      <url hash="9f652413">2006.bcs-1.3</url>
      <abstract>In this paper, we report on several software implementations that we have developed within Prague Arabic Dependency Treebank or some other projects concerned with Arabic Natural Language Processing. We try to guide the reader through some essential tasks and note the solutions that we have designed and used. We as well point to third-party computational systems that the research community might exploit in the future work in this field.</abstract>
      <bibkey>smrz-2006-tips</bibkey>
    </paper>
    <paper id="4">
      <title>Diacritization: A Challenge to <fixed-case>A</fixed-case>rabic Treebank Annotation and Parsing</title>
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <pages>35-47</pages>
      <url hash="5ac64278">2006.bcs-1.4</url>
      <abstract>Arabic diacritization (referred to sometimes as vocalization or vowelling), defined as the full or partial representation of short vowels, shadda (consonantal length or germination), tanween (nunation or definiteness), and hamza (the glottal stop and its support letters), is still largely understudied in the current NLP literature. In this paper, the lack of diacritics in standard Arabic texts is presented as a major challenge to most Arabic natural language processing tasks, including parsing. Recent studies (Messaoudi, et al. 2004; Vergyri &amp; Kirchhoff 2004; Zitouni, et al. 2006 and Maamouri, et al. forthcoming) about the place and impact of diacritization in text-based NLP research are presented along with an analysis of the weight of the missing diacritics on Treebank morphological and syntactic analyses and the impact on parser development.</abstract>
      <bibkey>maamouri-etal-2006-diacritization</bibkey>
    </paper>
    <paper id="5">
      <title>An Ambiguity-Controlled Morphological Analyzer for <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic Modeling Finite State Networks</title>
      <author><first>Mohammed A.</first><last>Attia</last></author>
      <pages>48-67</pages>
      <url hash="bc5cb4c3">2006.bcs-1.5</url>
      <abstract>Morphological ambiguity is a major concern for syntactic parsers, POS taggers and other NLP tools. For example, the greater the number of morphological analyses given for a lexical entry, the longer a parser takes in analyzing a sentence, and the greater the number of parses it produces. Xerox Arabic Finite State Morphology and Buckwalter Arabic Morphological Analyzer are two of the best known, well documented, morphological analyzers for Modern Standard Arabic (MSA). Yet there are significant problems with both systems in design as well as coverage that increase the ambiguity rate. This paper shows how an ambiguity-controlled morphological analyzer for Arabic is built in a rule-based system that takes the stem as the base form using finite state technology. The paper also points out sources of legal and illegal ambiguities in MSA, and how ambiguity in the new system is reduced without compromising precision. At the end, an evaluation of Xerox, Buckwalter, and our system is conducted, and the performance is compared and analyzed.</abstract>
      <bibkey>attia-2006-ambiguity</bibkey>
    </paper>
    <paper id="6">
      <title>Effective Stemming for <fixed-case>A</fixed-case>rabic Information Retrieval</title>
      <author><first>Youssef</first><last>Kadri</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <pages>68-75</pages>
      <url hash="a13f8831">2006.bcs-1.6</url>
      <abstract>Arabic has a very rich and complex morphology. Its appropriate morphological processing is very important for Information Retrieval (IR). In this paper, we propose a new stemming technique that tries to determine the stem of a word representing the semantic core of this word according to Arabic morphology. This method is compared to a commonly used light stemming technique which truncates a word by simple rules. Our tests on TREC collections show that the new stemming technique is more effective than the light stemming.</abstract>
      <bibkey>kadri-nie-2006-effective</bibkey>
    </paper>
    <paper id="7">
      <title>Automatic Transliteration of Proper Nouns from <fixed-case>A</fixed-case>rabic to <fixed-case>E</fixed-case>nglish</title>
      <author><first>Mehdi M.</first><last>Kashani</last></author>
      <author><first>Fred</first><last>Popowich</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>76-83</pages>
      <url hash="52e89095">2006.bcs-1.7</url>
      <abstract>After providing a brief introduction to the transliteration problem, and highlighting some issues specific to Arabic to English translation, a three phase algorithm is introduced as a computational solution to the problem. The algorithm is based on a Hidden Markov Model approach, but also leverages information available in on-line databases. The algorithm is then evaluated, and shown to achieve accuracy approaching .80%</abstract>
      <bibkey>kashani-etal-2006-automatic</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic Formalization and Linguistic Platform for its Analysis</title>
      <author><first>Slim</first><last>Mesfar</last></author>
      <pages>84-94</pages>
      <url hash="2247a496">2006.bcs-1.8</url>
      <abstract>This article describes the construction of a lexicon and a morphological description for standard Arabic. This system uses finite state technology to parse vowelled texts, as well as partially and not vowelled ones. It is based on large-coverage morphological grammars covering all grammatical rules.</abstract>
      <bibkey>mesfar-2006-standard</bibkey>
    </paper>
    <paper id="9">
      <title>Using Cross-language Information Retrieval for Sentence Alignment</title>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>Meriama</first><last>Laib</last></author>
      <author><first>Christian</first><last>Fluhr</last></author>
      <pages>95-104</pages>
      <url hash="66e53f17">2006.bcs-1.9</url>
      <abstract>Cross-language information retrieval consists in providing a query in one language and searching documents in different languages. Retrieved documents are ordered by the probability of being relevant to the user's request with the highest ranked being considered the most relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents to be indexed. This system, designed to work on Arabic, Chinese, English, French, German and Spanish, is composed of a multilingual linguistic analyzer, a statistical analyzer, a reformulator, a comparator and a search engine. The multilingual linguistic analyzer includes a morphological analyzer, a part-of-speech tagger and a syntactic analyzer. In the case of Arabic, a clitic stemmer is added to the morphological analyzer to segment the input words into proclitics, simple forms and enclitics. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The statistical analyzer computes for documents to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands queries during the search. The expansion is used to infer from the original query words other words expressing the same concepts. The expansion can be in the same language or in different languages. The search engine retrieves the ranked, relevant documents from the indexes according to the corresponding reformulated query and then merges the results obtained for each language, taking into account the original words of the query and their weights in order to score the documents. Sentence alignment consists in estimating which sentence or sentences in the source language correspond with which sentence or sentences in a target language. We present in this paper a new approach to aligning sentences from a parallel corpora based on the LIC2M cross-language information retrieval system. This approach consists in building a database of sentences of the target text and considering each sentence of the source text as a "query" to that database. The aligned bilingual parallel corpora can be used as a translation memory in a computer-aided translation tool.</abstract>
      <bibkey>semmar-etal-2006-using-cross</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>DCU</fixed-case> 250 <fixed-case>A</fixed-case>rabic Dependency Bank: An <fixed-case>LFG</fixed-case> Gold Standard Resource for the <fixed-case>A</fixed-case>rabic <fixed-case>P</fixed-case>enn <fixed-case>T</fixed-case>reebank</title>
      <author><first>Yafa</first><last>Al-Raheb</last></author>
      <author><first>A.</first><last>Akrout</last></author>
      <author><first>J.</first><last>van Genabith</last></author>
      <pages>105-117</pages>
      <url hash="f3d4f3ee">2006.bcs-1.10</url>
      <abstract>This paper describes the construction of a dependency bank gold standard for Arabic, DCU 250 Arabic Dependency Bank (DCU 250), based on the Arabic Penn Treebank Corpus (ATB) (Bies and Maamouri, 2003; Maamouri and Bies, 2004) within the theoretical framework of Lexical Functional Grammar (LFG). For parsing and automatically extracting grammatical and lexical resources from treebanks, it is necessary to evaluate against established gold standard resources. Gold standards for various languages have been developed, but to our knowledge, such a resource has not yet been constructed for Arabic. The construction of the DCU 250 marks the first step towards the creation of an automatic LFG f-structure annotation algorithm for the ATB, and for the extraction of Arabic grammatical and lexical resources.</abstract>
      <bibkey>al-raheb-etal-2006-dcu</bibkey>
    </paper>
    <paper id="11">
      <title>Problems of <fixed-case>A</fixed-case>rabic Machine Translation: Evaluation of Three Systems</title>
      <author><first>Sattar</first><last>Izwaini</last></author>
      <pages>118-148</pages>
      <url hash="e9424c18">2006.bcs-1.11</url>
      <abstract>The paper describes the translations of three online systems: Google, Sakhr, and Systran, using two sets of texts (Arabic and English) as input. It diagnoses the faults and attempts to detect the reasons, trying to shed light on the areas where the right translation solution is missed. Flaws and translation problems are categorized and analyzed, and recommendations are given. The two modes of translation (from and into Arabic) face a wide range of common linguistic problems as well as mode-specific problems. These problems are discussed and examples of output are given. The paper raises questions whose answers should help in the improvement of MT systems. The questions deal with establishing equivalents, lexical environment, and collocation. Cases that triggered these questions are illustrated and discussed.</abstract>
      <bibkey>izwaini-2006-problems</bibkey>
    </paper>
    <paper id="12">
      <title>Mapping Interlingua Representations to Feature Structures of <fixed-case>A</fixed-case>rabic Sentences</title>
      <author><first>Khaled</first><last>Shaalan</last></author>
      <author><first>Azza Abdel</first><last>Monem</last></author>
      <author><first>Ahmed</first><last>Rafea</last></author>
      <author><first>Hoda</first><last>Baraka</last></author>
      <pages>149-159</pages>
      <url hash="41124ca7">2006.bcs-1.12</url>
      <abstract>The interlingua approach to Machine Translation (MT) aims to achieve the translation task in two independent steps. First, the meanings of source language sentences are represented in an intermediate (interlingua) representation. Then, sentences of the target language are generated from those meaning representations. In the generation of the target sentence, determining sentence structures becomes more difficult, especially when the interlingua does not contain any syntactic information. Hence, the sentence structures cannot be transferred exactly from the interlingua representations. In this paper, we present a mapping approach for task- oriented interlingua-based spoken dialogue that transforms an interlingua representation, so-called Interchange Format (IF), into a feature structure (FS) that reflects the syntactic structure of the target Arabic sentence. This approach addresses the handling of the problem of Arabic syntactic structure determination in the interlingua approach. A mapper is developed primarily within the framework of the NESPOLE! (NEgotiating through SPOken Language in E-commerce) multilingual speech-to-speech MT project. The IF-to-Arabic FS mapper is implemented in SICStus Prolog. Examples of Arabic syntactic mapping, using the output from the English analyzer provided by Carnegie Mellon University (CMU), will illustrate how the system works.</abstract>
      <bibkey>shaalan-etal-2006-mapping</bibkey>
    </paper>
  </volume>
</collection>
