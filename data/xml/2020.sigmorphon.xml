<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.sigmorphon">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Kyle</first><last>Gorman</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="52a1928d">2020.sigmorphon-1</url>
    </meta>
    <frontmatter>
      <url hash="348ca84d">2020.sigmorphon-1.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task 0: Typologically Diverse Morphological Inflection</title>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <author><first>Jennifer</first><last>White</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Sabrina J.</first><last>Mielke</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Rowan</first><last>Hall Maudslay</last></author>
      <author><first>Ran</first><last>Zmigrod</last></author>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Svetlana</first><last>Toldova</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Elena</first><last>Klyachko</last></author>
      <author><first>Ilya</first><last>Yegorov</last></author>
      <author><first>Natalia</first><last>Krizhanovsky</last></author>
      <author><first>Paula</first><last>Czarnowska</last></author>
      <author><first>Irene</first><last>Nikkarinen</last></author>
      <author><first>Andrew</first><last>Krizhanovsky</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Lucas</first><last>Torroba Hennigen</last></author>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Hilaria</first><last>Cruz</last></author>
      <author><first>Eleanor</first><last>Chodroff</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>1–39</pages>
      <abstract>A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systems’ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.</abstract>
      <url hash="846f3659">2020.sigmorphon-1.1</url>
      <doi>10.18653/v1/2020.sigmorphon-1.1</doi>
      <video tag="video" href="http://slideslive.com/38929870"/>
    </paper>
    <paper id="2">
      <title>The <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion</title>
      <author><first>Kyle</first><last>Gorman</last></author>
      <author><first>Lucas F.E.</first><last>Ashby</last></author>
      <author><first>Aaron</first><last>Goyzueta</last></author>
      <author><first>Arya</first><last>McCarthy</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Daniel</first><last>You</last></author>
      <pages>40–50</pages>
      <abstract>We describe the design and findings of the SIGMORPHON 2020 shared task on multilingual grapheme-to-phoneme conversion. Participants were asked to submit systems which take in a sequence of graphemes in a given language as input, then output a sequence of phonemes representing the pronunciation of that grapheme sequence. Nine teams submitted a total of 23 systems, at best achieving a 18% relative reduction in word error rate (macro-averaged over languages), versus strong neural sequence-to-sequence baselines. To facilitate error analysis, we publicly release the complete outputs for all systems—a first for the SIGMORPHON workshop.</abstract>
      <url hash="e3d471a8">2020.sigmorphon-1.2</url>
      <doi>10.18653/v1/2020.sigmorphon-1.2</doi>
      <video tag="video" href="http://slideslive.com/38929871"/>
    </paper>
    <paper id="3">
      <title>The <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task on Unsupervised Morphological Paradigm Completion</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>51–62</pages>
      <abstract>In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology. Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma. In order to simulate a realistic use case, we first released data for 5 development languages. However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline. We provided a modular baseline system, which is a pipeline of 4 components. 3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic.</abstract>
      <url hash="d3c91fbb">2020.sigmorphon-1.3</url>
      <doi>10.18653/v1/2020.sigmorphon-1.3</doi>
      <video tag="video" href="http://slideslive.com/38929872"/>
    </paper>
    <paper id="4">
      <title>One-Size-Fits-All Multilingual Models</title>
      <author><first>Ben</first><last>Peters</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>63–69</pages>
      <abstract>This paper presents DeepSPIN’s submissions to Tasks 0 and 1 of the SIGMORPHON 2020 Shared Task. For both tasks, we present multilingual models, training jointly on data in all languages. We perform no language-specific hyperparameter tuning – each of our submissions uses the same model for all languages. Our basic architecture is the sparse sequence-to-sequence model with entmax attention and loss, which allows our models to learn sparse, local alignments while still being trainable with gradient-based techniques. For Task 1, we achieve strong performance with both RNN- and transformer-based sparse models. For Task 0, we extend our RNN-based model to a multi-encoder set-up in which separate modules encode the lemma and inflection sequences. Despite our models’ lack of language-specific tuning, they tie for first in Task 0 and place third in Task 1.</abstract>
      <url hash="c577bb7e">2020.sigmorphon-1.4</url>
      <doi>10.18653/v1/2020.sigmorphon-1.4</doi>
    </paper>
    <paper id="5">
      <title>Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection</title>
      <author><first>Xiang</first><last>Yu</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <pages>70–78</pages>
      <abstract>We present an iterative data augmentation framework, which trains and searches for an optimal ensemble and simultaneously annotates new training data in a self-training style. We apply this framework on two SIGMORPHON 2020 shared tasks: grapheme-to-phoneme conversion and morphological inflection. With very simple base models in the ensemble, we rank the first and the fourth in these two tasks. We show in the analysis that our system works especially well on low-resource languages.</abstract>
      <url hash="f997930f">2020.sigmorphon-1.5</url>
      <doi>10.18653/v1/2020.sigmorphon-1.5</doi>
    </paper>
    <paper id="6">
      <title>The <fixed-case>CMU</fixed-case>-<fixed-case>LTI</fixed-case> submission to the <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task 0: Language-Specific Cross-Lingual Transfer</title>
      <author><first>Nikitha</first><last>Murikinati</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>79–84</pages>
      <abstract>This paper describes the CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0 on typologically diverse morphological inflection. The (unrestricted) submission uses the cross-lingual approach of our last year’s winning submission (Anastasopoulos and Neubig, 2019), but adapted to use specific transfer languages for each test language. Our system, with fixed non-tuned hyperparameters, achieved a macro-averaged accuracy of 80.65 ranking 20th among 31 systems, but it was still tied for best system in 25 of the 90 total languages.</abstract>
      <url hash="78fecf87">2020.sigmorphon-1.6</url>
      <doi>10.18653/v1/2020.sigmorphon-1.6</doi>
    </paper>
    <paper id="7">
      <title>Grapheme-to-Phoneme Conversion with a Multilingual Transformer Model</title>
      <author><first>Omnia</first><last>ElSaadany</last></author>
      <author><first>Benjamin</first><last>Suter</last></author>
      <pages>85–89</pages>
      <abstract>In this paper, we describe our three submissions to the SIGMORPHON 2020 shared task 1 on grapheme-to-phoneme conversion for 15 languages. We experimented with a single multilingual transformer model. We observed that the multilingual model achieves results on par with our separately trained monolingual models and is even able to avoid a few of the errors made by the monolingual models.</abstract>
      <url hash="f2153579">2020.sigmorphon-1.7</url>
      <doi>10.18653/v1/2020.sigmorphon-1.7</doi>
    </paper>
    <paper id="8">
      <title>The <fixed-case>NYU</fixed-case>-<fixed-case>CUB</fixed-case>oulder Systems for <fixed-case>SIGMORPHON</fixed-case> 2020 Task 0 and Task 2</title>
      <author><first>Assaf</first><last>Singer</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>90–98</pages>
      <abstract>We describe the NYU-CUBoulder systems for the SIGMORPHON 2020 Task 0 on typologically diverse morphological inflection and Task 2 on unsupervised morphological paradigm completion. The former consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. The latter requires generating entire paradigms for a set of given lemmas from raw text alone. We model morphological inflection as a sequence-to-sequence problem, where the input is the sequence of the lemma’s characters with morphological tags, and the output is the sequence of the inflected form’s characters. First, we apply a transformer model to the task. Second, as inflected forms share most characters with the lemma, we further propose a pointer-generator transformer model to allow easy copying of input characters.</abstract>
      <url hash="d60711c9">2020.sigmorphon-1.8</url>
      <doi>10.18653/v1/2020.sigmorphon-1.8</doi>
    </paper>
    <paper id="9">
      <title>The <fixed-case>IMS</fixed-case>–<fixed-case>CUB</fixed-case>oulder System for the <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task on Unsupervised Morphological Paradigm Completion</title>
      <author><first>Manuel</first><last>Mager</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>99–105</pages>
      <abstract>In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMS--CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.</abstract>
      <url hash="4704eca8">2020.sigmorphon-1.9</url>
      <doi>10.18653/v1/2020.sigmorphon-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>SIGMORPHON</fixed-case> 2020 Task 0 System Description: <fixed-case>ETH</fixed-case> <fixed-case>Z</fixed-case>ürich Team</title>
      <author><first>Martina</first><last>Forster</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <pages>106–110</pages>
      <abstract>This paper presents our system for the SIGMORPHON 2020 Shared Task. We build off of the baseline systems, performing exact inference on models trained on language family data. Our systems return the globally best solution under these models. Our two systems achieve 80.9% and 75.6% accuracy on the test set. We ultimately find that, in this setting, exact inference does not seem to help or hinder the performance of morphological inflection generators, which stands in contrast to its affect on Neural Machine Translation (NMT) models.</abstract>
      <url hash="82a471c9">2020.sigmorphon-1.10</url>
      <doi>10.18653/v1/2020.sigmorphon-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>KU</fixed-case>-<fixed-case>CST</fixed-case> at the <fixed-case>SIGMORPHON</fixed-case> 2020 Task 2 on Unsupervised Morphological Paradigm Completion</title>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <author><first>Jürgen</first><last>Wedekind</last></author>
      <pages>111–116</pages>
      <abstract>We present a model for the unsupervised dis- covery of morphological paradigms. The goal of this model is to induce morphological paradigms from the bible (raw text) and a list of lemmas. We have created a model that splits each lemma in a stem and a suffix, and then we try to create a plausible suffix list by con- sidering lemma pairs. Our model was not able to outperform the official baseline, and there is still room for improvement, but we believe that the ideas presented here are worth considering.</abstract>
      <url hash="53f2349d">2020.sigmorphon-1.11</url>
      <doi>10.18653/v1/2020.sigmorphon-1.11</doi>
    </paper>
    <paper id="12">
      <title>Low-Resource <fixed-case>G</fixed-case>2<fixed-case>P</fixed-case> and <fixed-case>P</fixed-case>2<fixed-case>G</fixed-case> Conversion with Synthetic Training Data</title>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Amir Ahmad</first><last>Habibi</last></author>
      <author><first>Yixing</first><last>Luan</last></author>
      <author><first>Arnob</first><last>Mallik</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>117–122</pages>
      <abstract>This paper presents the University of Alberta systems and results in the SIGMORPHON 2020 Task 1: Multilingual Grapheme-to-Phoneme Conversion. Following previous SIGMORPHON shared tasks, we define a low-resource setting with 100 training instances. We experiment with three transduction approaches in both standard and low-resource settings, as well as on the related task of phoneme-to-grapheme conversion. We propose a method for synthesizing training data using a combination of diverse models.</abstract>
      <url hash="1c6bb043">2020.sigmorphon-1.12</url>
      <doi>10.18653/v1/2020.sigmorphon-1.12</doi>
    </paper>
    <paper id="13">
      <title>Frustratingly Easy Multilingual Grapheme-to-Phoneme Conversion</title>
      <author><first>Nikhil</first><last>Prabhu</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>123–127</pages>
      <abstract>In this paper, we describe two CU-Boulder submissions to the SIGMORPHON 2020 Task 1 on multilingual grapheme-to-phoneme conversion (G2P). Inspired by the high performance of a standard transformer model (Vaswani et al., 2017) on the task, we improve over this approach by adding two modifications: (i) Instead of training exclusively on G2P, we additionally create examples for the opposite direction, phoneme-to-grapheme conversion (P2G). We then perform multi-task training on both tasks. (ii) We produce ensembles of our models via majority voting. Our approaches, though being conceptually simple, result in systems that place 6th and 8th amongst 23 submitted systems, and obtain the best results out of all systems on Lithuanian and Modern Greek, respectively.</abstract>
      <url hash="b67caf54">2020.sigmorphon-1.13</url>
      <doi>10.18653/v1/2020.sigmorphon-1.13</doi>
    </paper>
    <paper id="14">
      <title>Exploring Neural Architectures And Techniques For Typologically Diverse Morphological Inflection</title>
      <author><first>Pratik</first><last>Jayarao</last></author>
      <author><first>Siddhanth</first><last>Pillay</last></author>
      <author><first>Pranav</first><last>Thombre</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <pages>128–136</pages>
      <abstract>Morphological inflection in low resource languages is critical to augment existing corpora in Low Resource Languages, which can help develop several applications in these languages with very good social impact. We describe our attention-based encoder-decoder approach that we implement using LSTMs and Transformers as the base units. We also describe the ancillary techniques that we experimented with, such as hallucination, language vector injection, sparsemax loss and adversarial language network alongside our approach to select the related language(s) for training. We present the results we generated on the constrained as well as unconstrained SIGMORPHON 2020 dataset (CITATION). One of the primary goals of our paper was to study the contribution varied components described above towards the performance of our system and perform an analysis on the same.</abstract>
      <url hash="1ac67b74">2020.sigmorphon-1.14</url>
      <doi>10.18653/v1/2020.sigmorphon-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>U</fixed-case>niversity of <fixed-case>I</fixed-case>llinois Submission to the <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task 0: Typologically Diverse Morphological Inflection</title>
      <author><first>Marc</first><last>Canby</last></author>
      <author><first>Aidana</first><last>Karipbayeva</last></author>
      <author><first>Bryan</first><last>Lunt</last></author>
      <author><first>Sahand</first><last>Mozaffari</last></author>
      <author><first>Charlotte</first><last>Yoder</last></author>
      <author><first>Julia</first><last>Hockenmaier</last></author>
      <pages>137–145</pages>
      <abstract>The objective of this shared task is to produce an inflected form of a word, given its lemma and a set of tags describing the attributes of the desired form. In this paper, we describe a transformer-based model that uses a bidirectional decoder to perform this task, and evaluate its performance on the 90 languages and 18 language families used in this task.</abstract>
      <url hash="8ee8826c">2020.sigmorphon-1.15</url>
      <doi>10.18653/v1/2020.sigmorphon-1.15</doi>
    </paper>
    <paper id="16">
      <title>One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme Conversion With a Transformer Ensemble</title>
      <author><first>Kaili</first><last>Vesik</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <pages>146–152</pages>
      <abstract>The task of grapheme-to-phoneme (G2P) conversion is important for both speech recognition and synthesis. Similar to other speech and language processing tasks, in a scenario where only small-sized training data are available, learning G2P models is challenging. We describe a simple approach of exploiting model ensembles, based on multilingual Transformers and self-training, to develop a highly effective G2P solution for 15 languages. Our models are developed as part of our participation in the SIGMORPHON 2020 Shared Task 1 focused at G2P. Our best models achieve 14.99 word error rate (WER) and 3.30 phoneme error rate (PER), a sizeable improvement over the shared task competitive baselines.</abstract>
      <url hash="5e9e06c6">2020.sigmorphon-1.16</url>
      <doi>10.18653/v1/2020.sigmorphon-1.16</doi>
    </paper>
    <paper id="17">
      <title>Leveraging Principal Parts for Morphological Inflection</title>
      <author><first>Ling</first><last>Liu</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>153–161</pages>
      <abstract>This paper presents the submission by the CU Ling team from the University of Colorado to SIGMORPHON 2020 shared task 0 on morphological inflection. The task is to generate the target inflected word form given a lemma form and a target morphosyntactic description. Our system uses the Transformer architecture. Our overall approach is to treat the morphological inflection task as a paradigm cell filling problem and to design the system to leverage principal parts information for better morphological inflection when the training data is limited. We train one model for each language separately without external data. The overall average performance of our submission ranks the first in both average accuracy and Levenshtein distance from the gold inflection among all submissions including those using external resources.</abstract>
      <url hash="e1277421">2020.sigmorphon-1.17</url>
      <doi>10.18653/v1/2020.sigmorphon-1.17</doi>
    </paper>
    <paper id="18">
      <title>Linguist vs. Machine: Rapid Development of Finite-State Morphological Grammars</title>
      <author><first>Sarah</first><last>Beemer</last></author>
      <author><first>Zak</first><last>Boston</last></author>
      <author><first>April</first><last>Bukoski</last></author>
      <author><first>Daniel</first><last>Chen</last></author>
      <author><first>Princess</first><last>Dickens</last></author>
      <author><first>Andrew</first><last>Gerlach</last></author>
      <author><first>Torin</first><last>Hopkins</last></author>
      <author><first>Parth</first><last>Anand Jawale</last></author>
      <author><first>Chris</first><last>Koski</last></author>
      <author><first>Akanksha</first><last>Malhotra</last></author>
      <author><first>Piyush</first><last>Mishra</last></author>
      <author><first>Saliha</first><last>Muradoglu</last></author>
      <author><first>Lan</first><last>Sang</last></author>
      <author><first>Tyler</first><last>Short</last></author>
      <author><first>Sagarika</first><last>Shreevastava</last></author>
      <author><first>Elizabeth</first><last>Spaulding</last></author>
      <author><first>Testumichi</first><last>Umada</last></author>
      <author><first>Beilei</first><last>Xiang</last></author>
      <author><first>Changbing</first><last>Yang</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>162–170</pages>
      <abstract>Sequence-to-sequence models have proven to be highly successful in learning morphological inflection from examples as the series of SIGMORPHON/CoNLL shared tasks have shown. It is usually assumed, however, that a linguist working with inflectional examples could in principle develop a gold standard-level morphological analyzer and generator that would surpass a trained neural network model in accuracy of predictions, but that it may require significant amounts of human labor. In this paper, we discuss an experiment where a group of people with some linguistic training develop 25+ grammars as part of the shared task and weigh the cost/benefit ratio of developing grammars by hand. We also present tools that can help linguists triage difficult complex morphophonological phenomena within a language and hypothesize inflectional class membership. We conclude that a significant development effort by trained linguists to analyze and model morphophonological patterns are required in order to surpass the accuracy of neural models.</abstract>
      <url hash="526a49bc">2020.sigmorphon-1.18</url>
      <doi>10.18653/v1/2020.sigmorphon-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>CLUZH</fixed-case> at <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion</title>
      <author><first>Peter</first><last>Makarov</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <pages>171–176</pages>
      <abstract>This paper describes the submission by the team from the Institute of Computational Linguistics, Zurich University, to the Multilingual Grapheme-to-Phoneme Conversion (G2P) Task of the SIGMORPHON 2020 challenge. The submission adapts our system from the 2018 edition of the SIGMORPHON shared task. Our system is a neural transducer that operates over explicit edit actions and is trained with imitation learning. It is well-suited for morphological string transduction partly because it exploits the fact that the input and output character alphabets overlap. The challenge posed by G2P has been to adapt the model and the training procedure to work with disjoint alphabets. We adapt the model to use substitution edits and train it with a weighted finite-state transducer acting as the expert policy. An ensemble of such models produces competitive results on G2P. Our submission ranks second out of 23 submissions by a total of nine teams.</abstract>
      <url hash="e2268aa8">2020.sigmorphon-1.19</url>
      <doi>10.18653/v1/2020.sigmorphon-1.19</doi>
    </paper>
    <paper id="20">
      <title>The <fixed-case>U</fixed-case>ni<fixed-case>M</fixed-case>elb Submission to the <fixed-case>SIGMORPHON</fixed-case> 2020 Shared Task 0: Typologically Diverse Morphological Inflection</title>
      <author><first>Andreas</first><last>Scherbakov</last></author>
      <pages>177–183</pages>
      <abstract>The paper describes the University of Melbourne’s submission to the SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection. Our team submitted three systems in total, two neural and one non-neural. Our analysis of systems’ performance shows positive effects of newly introduced data hallucination technique that we employed in one of neural systems, especially in low-resource scenarios. A non-neural system based on observed inflection patterns shows optimistic results even in its simple implementation (&gt;75% accuracy for 50% of languages). With possible improvement within the same modeling principle, accuracy might grow to values above 90%.</abstract>
      <url hash="54c49dd4">2020.sigmorphon-1.20</url>
      <doi>10.18653/v1/2020.sigmorphon-1.20</doi>
    </paper>
    <paper id="21">
      <title>Data Augmentation for Transformer-based <fixed-case>G</fixed-case>2<fixed-case>P</fixed-case></title>
      <author><first>Zach</first><last>Ryan</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>184–188</pages>
      <abstract>The Transformer model has been shown to outperform other neural seq2seq models in several character-level tasks. It is unclear, however, if the Transformer would benefit as much as other seq2seq models from data augmentation strategies in the low-resource setting. In this paper we explore strategies for data augmentation in the g2p task together with the Transformer model. Our results show that a relatively simple alignment-based strategy of identifying consistent input-output subsequences in grapheme-phoneme data coupled together with a subsequent splicing together of such pieces to generate hallucinated data works well in the low-resource setting, often delivering substantial performance improvement over a standard Transformer model.</abstract>
      <url hash="4533a908">2020.sigmorphon-1.21</url>
      <doi>10.18653/v1/2020.sigmorphon-1.21</doi>
    </paper>
    <paper id="22">
      <title>Transliteration for Cross-Lingual Morphological Inflection</title>
      <author><first>Nikitha</first><last>Murikinati</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>189–197</pages>
      <abstract>Cross-lingual transfer between typologically related languages has been proven successful for the task of morphological inflection. However, if the languages do not share the same script, current methods yield more modest improvements. We explore the use of transliteration between related languages, as well as grapheme-to-phoneme conversion, as data preprocessing methods in order to alleviate this issue. We experimented with several diverse language pairs, finding that in most cases transliterating the transfer language data into the target one leads to accuracy improvements, even up to 9 percentage points. Converting both languages into a shared space like the International Phonetic Alphabet or the Latin alphabet is also beneficial, leading to improvements of up to 16 percentage points.</abstract>
      <url hash="38ae6532">2020.sigmorphon-1.22</url>
      <doi>10.18653/v1/2020.sigmorphon-1.22</doi>
      <video tag="video" href="http://slideslive.com/38929875"/>
    </paper>
    <paper id="23">
      <title>Evaluating Neural Morphological Taggers for <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Ashim</first><last>Gupta</last></author>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <author><first>Oliver</first><last>Hellwig</last></author>
      <pages>198–203</pages>
      <abstract>Neural sequence labelling approaches have achieved state of the art results in morphological tagging. We evaluate the efficacy of four standard sequence labelling models on Sanskrit, a morphologically rich, fusional Indian language. As its label space can theoretically contain more than 40,000 labels, systems that explicitly model the internal structure of a label are more suited for the task, because of their ability to generalise to labels not seen during training. We find that although some neural models perform better than others, one of the common causes for error for all of these models is mispredictions due to syncretism.</abstract>
      <url hash="28c1a4e3">2020.sigmorphon-1.23</url>
      <doi>10.18653/v1/2020.sigmorphon-1.23</doi>
      <video tag="video" href="http://slideslive.com/38929876"/>
    </paper>
    <paper id="24">
      <title>Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?</title>
      <author><first>Stav</first><last>Klein</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>204–209</pages>
      <abstract>This work investigates the most basic units that underlie contextualized word embeddings, such as BERT — the so-called word pieces. In Morphologically-Rich Languages (MRLs) which exhibit morphological fusion and non-concatenative morphology, the different units of meaning within a word may be fused, intertwined, and cannot be separated linearly. Therefore, when using word-pieces in MRLs, we must consider that: (1) a linear segmentation into sub-word units might not capture the full morphological complexity of words; and (2) representations that leave morphological knowledge on sub-word units inaccessible might negatively affect performance. Here we empirically examine the capacity of word-pieces to capture morphology by investigating the task of multi-tagging in Modern Hebrew, as a proxy to evaluate the underlying segmentation. Our results show that, while models trained to predict multi-tags for complete words outperform models tuned to predict the distinct tags of WPs, we can improve the WPs tag prediction by purposefully constraining the word-pieces to reflect their internal functions. We suggest that linguistically-informed word-pieces schemes, that make the morphological structure explicit, might boost performance for MRLs.</abstract>
      <url hash="eec33e19">2020.sigmorphon-1.24</url>
      <doi>10.18653/v1/2020.sigmorphon-1.24</doi>
      <video tag="video" href="http://slideslive.com/38929877"/>
    </paper>
    <paper id="25">
      <title>Induced Inflection-Set Keyword Search in Speech</title>
      <author><first>Oliver</first><last>Adams</last></author>
      <author><first>Matthew</first><last>Wiesner</last></author>
      <author><first>Jan</first><last>Trmal</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <pages>210–216</pages>
      <abstract>We investigate the problem of searching for a lexeme-set in speech by searching for its inflectional variants. Experimental results indicate how lexeme-set search performance changes with the number of hypothesized inflections, while ablation experiments highlight the relative importance of different components in the lexeme-set search pipeline and the value of using curated inflectional paradigms. We provide a recipe and evaluation set for the community to use as an extrinsic measure of the performance of inflection generation approaches.</abstract>
      <url hash="c9e278ab">2020.sigmorphon-1.25</url>
      <doi>10.18653/v1/2020.sigmorphon-1.25</doi>
      <video tag="video" href="http://slideslive.com/38929878"/>
    </paper>
    <paper id="26">
      <title>Representation Learning for Discovering Phonemic Tone Contours</title>
      <author><first>Bai</first><last>Li</last></author>
      <author><first>Jing Yi</first><last>Xie</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>217–223</pages>
      <abstract>Tone is a prosodic feature used to distinguish words in many languages, some of which are endangered and scarcely documented. In this work, we use unsupervised representation learning to identify probable clusters of syllables that share the same phonemic tone. Our method extracts the pitch for each syllable, then trains a convolutional autoencoder to learn a low-dimensional representation for each contour. We then apply the mean shift algorithm to cluster tones in high-density regions of the latent space. Furthermore, by feeding the centers of each cluster into the decoder, we produce a prototypical contour that represents each cluster. We apply this method to spoken multi-syllable words in Mandarin Chinese and Cantonese and evaluate how closely our clusters match the ground truth tone categories. Finally, we discuss some difficulties with our approach, including contextual tone variation and allophony effects.</abstract>
      <url hash="7fcb48c0">2020.sigmorphon-1.26</url>
      <doi>10.18653/v1/2020.sigmorphon-1.26</doi>
      <video tag="video" href="http://slideslive.com/38929879"/>
    </paper>
    <paper id="27">
      <title>Joint learning of constraint weights and gradient inputs in Gradient Symbolic Computation with constrained optimization</title>
      <author><first>Max</first><last>Nelson</last></author>
      <pages>224–232</pages>
      <abstract>This paper proposes a method for the joint optimization of constraint weights and symbol activations within the Gradient Symbolic Computation (GSC) framework. The set of grammars representable in GSC is proven to be a subset of those representable with lexically-scaled faithfulness constraints. This fact is then used to recast the problem of learning constraint weights and symbol activations in GSC as a quadratically-constrained version of learning lexically-scaled faithfulness grammars. This results in an optimization problem that can be solved using Sequential Quadratic Programming.</abstract>
      <url hash="abb6ae60">2020.sigmorphon-1.27</url>
      <doi>10.18653/v1/2020.sigmorphon-1.27</doi>
      <video tag="video" href="http://slideslive.com/38929880"/>
    </paper>
    <paper id="28">
      <title>In search of isoglosses: continuous and discrete language embeddings in <fixed-case>S</fixed-case>lavic historical phonology</title>
      <author><first>Chundra</first><last>Cathcart</last></author>
      <author><first>Florian</first><last>Wandl</last></author>
      <pages>233–244</pages>
      <abstract>This paper investigates the ability of neural network architectures to effectively learn diachronic phonological generalizations in amultilingual setting. We employ models using three different types of language embedding (dense, sigmoid, and straight-through). We find that the Straight-Through model out-performs the other two in terms of accuracy, but the Sigmoid model’s language embeddings show the strongest agreement with the traditional subgrouping of the Slavic languages. We find that the Straight-Through model has learned coherent, semi-interpretable information about sound change, and outline directions for future research.</abstract>
      <url hash="f2b8a1c8">2020.sigmorphon-1.28</url>
      <doi>10.18653/v1/2020.sigmorphon-1.28</doi>
      <video tag="video" href="http://slideslive.com/38929873"/>
    </paper>
    <paper id="29">
      <title>Multi-Tiered Strictly Local Functions</title>
      <author><first>Phillip</first><last>Burness</last></author>
      <author><first>Kevin</first><last>McMullin</last></author>
      <pages>245–255</pages>
      <abstract>Tier-based Strictly Local functions, as they have so far been defined, are equipped with just a single tier. In light of this fact, they are currently incapable of modelling simultaneous phonological processes that would require different tiers. In this paper we consider whether and how we can allow a single function to operate over more than one tier. We conclude that multiple tiers can and should be permitted, but that the relationships between them must be restricted in some way to avoid overgeneration. The particular restriction that we propose comes in two parts. First, each input element is associated with a set of tiers that on their own can fully determine what the element is mapped to. Second, the set of tiers associated to a given input element must form a strict superset-subset hierarchy. In this way, we can track multiple, related sources of information when deciding how to process a particular input element. We demonstrate that doing so enables simple and intuitive analyses to otherwise challenging phonological phenomena.</abstract>
      <url hash="da360ed7">2020.sigmorphon-1.29</url>
      <revision id="1" href="2020.sigmorphon-1.29v1" hash="cde00479"/>
      <revision id="2" href="2020.sigmorphon-1.29v2" hash="da360ed7" date="2020-07-08">Corrected bibliography entry</revision>
      <doi>10.18653/v1/2020.sigmorphon-1.29</doi>
      <video tag="video" href="http://slideslive.com/38929874"/>
    </paper>
  </volume>
</collection>
