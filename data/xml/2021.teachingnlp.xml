<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.teachingnlp">
  <volume id="1" ingest-date="2021-05-24" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on Teaching NLP</booktitle>
      <editor><first>David</first><last>Jurgens</last></editor>
      <editor><first>Varada</first><last>Kolhatkar</last></editor>
      <editor><first>Lucy</first><last>Li</last></editor>
      <editor><first>Margot</first><last>Mieskes</last></editor>
      <editor><first>Ted</first><last>Pedersen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.teachingnlp-1</url>
      <venue>teachingnlp</venue>
    </meta>
    <frontmatter>
      <url hash="076b2c31">2021.teachingnlp-1.0</url>
      <bibkey>teachingnlp-2021-teaching</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Pedagogical Principles in the Online Teaching of Text Mining: A Retrospection</title>
      <author><first>Rajkumar</first><last>Saini</last></author>
      <author><first>György</first><last>Kovács</last></author>
      <author><first>Mohamadreza</first><last>Faridghasemnia</last></author>
      <author><first>Hamam</first><last>Mokayed</last></author>
      <author><first>Oluwatosin</first><last>Adewumi</last></author>
      <author><first>Pedro</first><last>Alonso</last></author>
      <author><first>Sumit</first><last>Rakesh</last></author>
      <author><first>Marcus</first><last>Liwicki</last></author>
      <pages>1–12</pages>
      <abstract>The ongoing COVID-19 pandemic has brought online education to the forefront of pedagogical discussions. To make this increased interest sustainable in a post-pandemic era, online courses must be built on strong pedagogical foundations. With a long history of pedagogic research, there are many principles, frameworks, and models available to help teachers in doing so. These models cover different teaching perspectives, such as constructive alignment, feedback, and the learning environment. In this paper, we discuss how we designed and implemented our online Natural Language Processing (NLP) course following constructive alignment and adhering to the pedagogical principles of LTU. By examining our course and analyzing student evaluation forms, we show that we have met our goal and successfully delivered the course. Furthermore, we discuss the additional benefits resulting from the current mode of delivery, including the increased reusability of course content and increased potential for collaboration between universities. Lastly, we also discuss where we can and will further improve the current course design.</abstract>
      <url hash="555e7aa7">2021.teachingnlp-1.1</url>
      <doi>10.18653/v1/2021.teachingnlp-1.1</doi>
      <bibkey>saini-etal-2021-pedagogical</bibkey>
    </paper>
    <paper id="2">
      <title>Teaching a Massive Open Online Course on Natural Language Processing</title>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Murat</first><last>Apishev</last></author>
      <author><first>Denis</first><last>Kirianov</last></author>
      <author><first>Veronica</first><last>Sarkisyan</last></author>
      <author><first>Sergey</first><last>Aksenov</last></author>
      <author><first>Oleg</first><last>Serikov</last></author>
      <pages>13–27</pages>
      <abstract>In this paper we present a new Massive Open Online Course on Natural Language Processing, targeted at non-English speaking students. The course lasts 12 weeks, every week consists of lectures, practical sessions and quiz assigments. Three weeks out of 12 are followed by Kaggle-style coding assigments. Our course intents to serve multiple purposes: (i) familirize students with the core concepts and methods in NLP, such as language modelling or word or sentence representations, (ii) show that recent advances, including pre-trained Transformer-based models, are build upon these concepts; (iii) to introduce architectures for most most demanded real-life applications, (iii) to develop practical skills to process texts in multiple languages. The course was prepared and recorded during 2020 and so far have received positive feedback.</abstract>
      <url hash="1d822912">2021.teachingnlp-1.2</url>
      <doi>10.18653/v1/2021.teachingnlp-1.2</doi>
      <bibkey>artemova-etal-2021-teaching</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="3">
      <title>Natural Language Processing 4 All (<fixed-case>NLP</fixed-case>4<fixed-case>A</fixed-case>ll): A New Online Platform for Teaching and Learning <fixed-case>NLP</fixed-case> Concepts</title>
      <author><first>Rebekah</first><last>Baglini</last></author>
      <author><first>Hermes</first><last>Hjorth</last></author>
      <pages>28–33</pages>
      <abstract>Natural Language Processing offers new insights into language data across almost all disciplines and domains, and allows us to corroborate and/or challenge existing knowledge. The primary hurdles to widening participation in and use of these new research tools are, first, a lack of coding skills in students across K-16, and in the population at large, and second, a lack of knowledge of how NLP-methods can be used to answer questions of disciplinary interest outside of linguistics and/or computer science. To broaden participation in NLP and improve NLP-literacy, we introduced a new tool web-based tool called Natural Language Processing 4 All (NLP4All). The intended purpose of NLP4All is to help teachers facilitate learning with and about NLP, by providing easy-to-use interfaces to NLP-methods, data, and analyses, making it possible for non- and novice-programmers to learn NLP concepts interactively.</abstract>
      <url hash="cad60b21">2021.teachingnlp-1.3</url>
      <revision id="1" href="2021.teachingnlp-1.3v1" hash="87bed85c"/>
      <revision id="2" href="2021.teachingnlp-1.3v2" hash="cad60b21" date="2021-06-01">Fixed typo in equation and added new figure</revision>
      <doi>10.18653/v1/2021.teachingnlp-1.3</doi>
      <bibkey>baglini-hjorth-2021-natural</bibkey>
    </paper>
    <paper id="4">
      <title>A New Broad <fixed-case>NLP</fixed-case> Training from Speech to Knowledge</title>
      <author><first>Maxime</first><last>Amblard</last></author>
      <author><first>Miguel</first><last>Couceiro</last></author>
      <pages>34–45</pages>
      <abstract>In 2018, the Master Sc. in NLP opened at IDMC - Institut des Sciences du Digital, du Management et de la Cognition, Université de Lorraine - Nancy, France. Far from being a creation ex-nihilo, it is the product of a history and many reflections on the field and its teaching. This article proposes epistemological and critical elements on the opening and maintainance of this so far new master’s program in NLP.</abstract>
      <url hash="960bf07e">2021.teachingnlp-1.4</url>
      <doi>10.18653/v1/2021.teachingnlp-1.4</doi>
      <bibkey>amblard-couceiro-2021-new</bibkey>
    </paper>
    <paper id="5">
      <title>Applied Language Technology: <fixed-case>NLP</fixed-case> for the Humanities</title>
      <author><first>Tuomo</first><last>Hiippala</last></author>
      <pages>46–48</pages>
      <abstract>This contribution describes a two-course module that seeks to provide humanities majors with a basic understanding of language technology and its applications using Python. The learning materials consist of interactive Jupyter Notebooks and accompanying YouTube videos, which are openly available with a Creative Commons licence.</abstract>
      <url hash="da6f3e5a">2021.teachingnlp-1.5</url>
      <doi>10.18653/v1/2021.teachingnlp-1.5</doi>
      <bibkey>hiippala-2021-applied</bibkey>
    </paper>
    <paper id="6">
      <title>A Crash Course on Ethics for Natural Language Processing</title>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>49–51</pages>
      <abstract>It is generally agreed upon in the natural language processing (NLP) community that ethics should be integrated into any curriculum. Being aware of and understanding the relevant core concepts is a prerequisite for following and participating in the discourse on ethical NLP. We here present ready-made teaching material in the form of slides and practical exercises on ethical issues in NLP, which is primarily intended to be integrated into introductory NLP or computational linguistics courses. By making this material freely available, we aim at lowering the threshold to adding ethics to the curriculum. We hope that increased awareness will enable students to identify potentially unethical behavior.</abstract>
      <url hash="2373da92">2021.teachingnlp-1.6</url>
      <doi>10.18653/v1/2021.teachingnlp-1.6</doi>
      <bibkey>friedrich-zesch-2021-crash</bibkey>
    </paper>
    <paper id="7">
      <title>A dissemination workshop for introducing young <fixed-case>I</fixed-case>talian students to <fixed-case>NLP</fixed-case></title>
      <author><first>Lucio</first><last>Messina</last></author>
      <author><first>Lucia</first><last>Busso</last></author>
      <author><first>Claudia Roberta</first><last>Combei</last></author>
      <author><first>Alessio</first><last>Miaschi</last></author>
      <author><first>Ludovica</first><last>Pannitto</last></author>
      <author><first>Gabriele</first><last>Sarti</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>52–54</pages>
      <abstract>We describe and make available the game-based material developed for a laboratory run at several Italian science festivals to popularize NLP among young students.</abstract>
      <url hash="bf7cfe13">2021.teachingnlp-1.7</url>
      <attachment type="OptionalSupplementaryMaterial" hash="f93d21f2">2021.teachingnlp-1.7.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.teachingnlp-1.7</doi>
      <bibkey>messina-etal-2021-dissemination</bibkey>
      <pwccode url="https://bitbucket.org/melfnt/malvisindi" additional="false">melfnt/malvisindi</pwccode>
    </paper>
    <paper id="8">
      <title><fixed-case>M</fixed-case>ini<fixed-case>VQA</fixed-case> - A resource to build your tailored <fixed-case>VQA</fixed-case> competition</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last></author>
      <pages>55–58</pages>
      <abstract>MiniVQA is a Jupyter notebook to build a tailored VQA competition for your students. The resource creates all the needed resources to create a classroom competition that engages and inspires your students on the free, self-service Kaggle platform. “InClass competitions make machine learning fun¡‘.</abstract>
      <url hash="50758ec6">2021.teachingnlp-1.8</url>
      <doi>10.18653/v1/2021.teachingnlp-1.8</doi>
      <bibkey>delbrouck-2021-minivqa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="9">
      <title>From back to the roots into the gated woods: Deep learning for <fixed-case>NLP</fixed-case></title>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>59–61</pages>
      <abstract>Deep neural networks have revolutionized many fields, including Natural Language Processing. This paper outlines teaching materials for an introductory lecture on deep learning in Natural Language Processing (NLP). The main submitted material covers a summer school lecture on encoder-decoder models. Complementary to this is a set of jupyter notebook slides from earlier teaching, on which parts of the lecture were based on. The main goal of this teaching material is to provide an overview of neural network approaches to natural language processing, while linking modern concepts back to the roots showing traditional essential counterparts. The lecture departs from count-based statistical methods and spans up to gated recurrent networks and attention, which is ubiquitous in today’s NLP.</abstract>
      <url hash="9c78dd8d">2021.teachingnlp-1.9</url>
      <doi>10.18653/v1/2021.teachingnlp-1.9</doi>
      <bibkey>plank-2021-back</bibkey>
    </paper>
    <paper id="10">
      <title>Learning <fixed-case>P</fixed-case>y<fixed-case>T</fixed-case>orch Through A Neural Dependency Parsing Exercise</title>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>62–64</pages>
      <abstract>Dependency parsing is increasingly the popular parsing formalism in practice. This assignment provides a practice exercise in implementing the shift-reduce dependency parser of Chen and Manning (2014). This parser is a two-layer feed-forward neural network, which students implement in PyTorch, providing practice in developing deep learning models and exposure to developing parser models.</abstract>
      <url hash="9cbe921e">2021.teachingnlp-1.10</url>
      <doi>10.18653/v1/2021.teachingnlp-1.10</doi>
      <bibkey>jurgens-2021-learning</bibkey>
    </paper>
    <paper id="11">
      <title>A Balanced and Broadly Targeted Computational Linguistics Curriculum</title>
      <author><first>Emma</first><last>Manning</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <pages>65–69</pages>
      <abstract>This paper describes the primarily-graduate computational linguistics and NLP curriculum at Georgetown University, a U.S. university that has seen significant growth in these areas in recent years. We reflect on the principles behind our curriculum choices, including recognizing the various academic backgrounds and goals of our students; teaching a variety of skills with an emphasis on working directly with data; encouraging collaboration and interdisciplinary work; and including languages beyond English. We reflect on challenges we have encountered, such as the difficulty of teaching programming skills alongside NLP fundamentals, and discuss areas for future growth.</abstract>
      <url hash="a6dfe8c5">2021.teachingnlp-1.11</url>
      <doi>10.18653/v1/2021.teachingnlp-1.11</doi>
      <bibkey>manning-etal-2021-balanced</bibkey>
    </paper>
    <paper id="12">
      <title>Gaining Experience with Structured Data: Using the Resources of Dialog State Tracking Challenge 2</title>
      <author><first>Ronnie</first><last>Smith</last></author>
      <pages>70–79</pages>
      <abstract>This paper describes a class project for a recently introduced undergraduate NLP course that gives computer science students the opportunity to explore the data of Dialog State Tracking Challenge 2 (DSTC 2). Student background, curriculum choices, and project details are discussed. The paper concludes with some instructor advice and final reflections.</abstract>
      <url hash="6b9f3fed">2021.teachingnlp-1.12</url>
      <doi>10.18653/v1/2021.teachingnlp-1.12</doi>
      <bibkey>smith-2021-gaining</bibkey>
    </paper>
    <paper id="13">
      <title>The Flipped Classroom model for teaching Conditional Random Fields in an <fixed-case>NLP</fixed-case> course</title>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <pages>80–86</pages>
      <abstract>In this article, we show and discuss our experience in applying the flipped classroom method for teaching Conditional Random Fields in a Natural Language Processing course. We present the activities that we developed together with their relationship to a cognitive complexity model (Bloom’s taxonomy). After this, we provide our own reflections and expectations of the model itself. Based on the evaluation got from students, it seems that students learn about the topic and also that the method is rewarding for some students. Additionally, we discuss some shortcomings and we propose possible solutions to them. We conclude the paper with some possible future work.</abstract>
      <url hash="e50843ff">2021.teachingnlp-1.13</url>
      <attachment type="OptionalSupplementaryMaterial" hash="48a32274">2021.teachingnlp-1.13.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.teachingnlp-1.13</doi>
      <bibkey>agirrezabal-2021-flipped</bibkey>
    </paper>
    <paper id="14">
      <title>Flamingos and Hedgehogs in the Croquet-Ground: Teaching Evaluation of <fixed-case>NLP</fixed-case> Systems for Undergraduate Students</title>
      <author><first>Brielen</first><last>Madureira</last></author>
      <pages>87–91</pages>
      <abstract>This report describes the course Evaluation of NLP Systems, taught for Computational Linguistics undergraduate students during the winter semester 20/21 at the University of Potsdam, Germany. It was a discussion-based seminar that covered different aspects of evaluation in NLP, namely paradigms, common procedures, data annotation, metrics and measurements, statistical significance testing, best practices and common approaches in specific NLP tasks and applications.</abstract>
      <url hash="bf300cdc">2021.teachingnlp-1.14</url>
      <doi>10.18653/v1/2021.teachingnlp-1.14</doi>
      <bibkey>madureira-2021-flamingos</bibkey>
    </paper>
    <paper id="15">
      <title>An Immersive Computational Text Analysis Course for Non-Computer Science Students at Barnard College</title>
      <author><first>Adam</first><last>Poliak</last></author>
      <author><first>Jalisha</first><last>Jenifer</last></author>
      <pages>92–95</pages>
      <abstract>We provide an overview of a new Computational Text Analysis course that will be taught at Barnard College over a six week period in May and June 2021. The course is targeted to non Computer Science at a Liberal Arts college that wish to incorporate fundamental Natural Language Processing tools in their re- search and studies. During the course, students will complete daily programming tutorials, read and review contemporary research papers, and propose and develop independent research projects.</abstract>
      <url hash="b1e5be1c">2021.teachingnlp-1.15</url>
      <doi>10.18653/v1/2021.teachingnlp-1.15</doi>
      <bibkey>poliak-jenifer-2021-immersive</bibkey>
    </paper>
    <paper id="16">
      <title>Introducing Information Retrieval for Biomedical Informatics Students</title>
      <author><first>Sanya</first><last>Taneja</last></author>
      <author><first>Richard</first><last>Boyce</last></author>
      <author><first>William</first><last>Reynolds</last></author>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <pages>96–98</pages>
      <abstract>Introducing biomedical informatics (BMI) students to natural language processing (NLP) requires balancing technical depth with practical know-how to address application-focused needs. We developed a set of three activities introducing introductory BMI students to information retrieval with NLP, covering document representation strategies and language models from TF-IDF to BERT. These activities provide students with hands-on experience targeted towards common use cases, and introduce fundamental components of NLP workflows for a wide variety of applications.</abstract>
      <url hash="8b421aec">2021.teachingnlp-1.16</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ec927f84">2021.teachingnlp-1.16.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.teachingnlp-1.16</doi>
      <bibkey>taneja-etal-2021-introducing</bibkey>
      <pwccode url="https://github.com/dbmi-pitt/bioinf_teachingNLP" additional="false">dbmi-pitt/bioinf_teachingNLP</pwccode>
    </paper>
    <paper id="17">
      <title>Contemporary <fixed-case>NLP</fixed-case> Modeling in Six Comprehensive Programming Assignments</title>
      <author><first>Greg</first><last>Durrett</last></author>
      <author><first>Jifan</first><last>Chen</last></author>
      <author><first>Shrey</first><last>Desai</last></author>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Lucas</first><last>Kabela</last></author>
      <author><first>Yasumasa</first><last>Onoe</last></author>
      <author><first>Jiacheng</first><last>Xu</last></author>
      <pages>99–103</pages>
      <abstract>We present a series of programming assignments, adaptable to a range of experience levels from advanced undergraduate to PhD, to teach students design and implementation of modern NLP systems. These assignments build from the ground up and emphasize full-stack understanding of machine learning models: initially, students implement inference and gradient computation by hand, then use PyTorch to build nearly state-of-the-art neural networks using current best practices. Topics are chosen to cover a wide range of modeling and inference techniques that one might encounter, ranging from linear models suitable for industry applications to state-of-the-art deep learning models used in NLP research. The assignments are customizable, with constrained options to guide less experienced students or open-ended options giving advanced students freedom to explore. All of them can be deployed in a fully autogradable fashion, and have collectively been tested on over 300 students across several semesters.</abstract>
      <url hash="cc3ea62c">2021.teachingnlp-1.17</url>
      <doi>10.18653/v1/2021.teachingnlp-1.17</doi>
      <bibkey>durrett-etal-2021-contemporary</bibkey>
    </paper>
    <paper id="18">
      <title>Interactive Assignments for Teaching Structured Neural <fixed-case>NLP</fixed-case></title>
      <author><first>David</first><last>Gaddy</last></author>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Nikita</first><last>Kitaev</last></author>
      <author><first>Mitchell</first><last>Stern</last></author>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>John</first><last>DeNero</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>104–107</pages>
      <abstract>We present a set of assignments for a graduate-level NLP course. Assignments are designed to be interactive, easily gradable, and to give students hands-on experience with several key types of structure (sequences, tags, parse trees, and logical forms), modern neural architectures (LSTMs and Transformers), inference algorithms (dynamic programs and approximate search) and training methods (full and weak supervision). We designed assignments to build incrementally both within each assignment and across assignments, with the goal of enabling students to undertake graduate-level research in NLP by the end of the course.</abstract>
      <url hash="9edc5b4e">2021.teachingnlp-1.18</url>
      <doi>10.18653/v1/2021.teachingnlp-1.18</doi>
      <bibkey>gaddy-etal-2021-interactive</bibkey>
    </paper>
    <paper id="19">
      <title>Learning about Word Vector Representations and Deep Learning through Implementing Word2vec</title>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>108–111</pages>
      <abstract>Word vector representations are an essential part of an NLP curriculum. Here, we describe a homework that has students implement a popular method for learning word vectors, word2vec. Students implement the core parts of the method, including text preprocessing, negative sampling, and gradient descent. Starter code provides guidance and handles basic operations, which allows students to focus on the conceptually challenging aspects. After generating their vectors, students evaluate them using qualitative and quantitative tests.</abstract>
      <url hash="d8bd617a">2021.teachingnlp-1.19</url>
      <doi>10.18653/v1/2021.teachingnlp-1.19</doi>
      <bibkey>jurgens-2021-learning-word</bibkey>
    </paper>
    <paper id="20">
      <title>Naive <fixed-case>B</fixed-case>ayes versus <fixed-case>BERT</fixed-case>: <fixed-case>J</fixed-case>upyter notebook assignments for an introductory <fixed-case>NLP</fixed-case> course</title>
      <author><first>Jennifer</first><last>Foster</last></author>
      <author><first>Joachim</first><last>Wagner</last></author>
      <pages>112–114</pages>
      <abstract>We describe two Jupyter notebooks that form the basis of two assignments in an introductory Natural Language Processing (NLP) module taught to final year undergraduate students at Dublin City University. The notebooks show the students how to train a bag-of-words polarity classifier using multinomial Naive Bayes, and how to fine-tune a polarity classifier using BERT. The students take the code as a starting point for their own experiments.</abstract>
      <url hash="8262ec41">2021.teachingnlp-1.20</url>
      <doi>10.18653/v1/2021.teachingnlp-1.20</doi>
      <bibkey>foster-wagner-2021-naive</bibkey>
    </paper>
    <paper id="21">
      <title>Natural Language Processing for Computer Scientists and Data Scientists at a Large State University</title>
      <author><first>Casey</first><last>Kennington</last></author>
      <pages>115–124</pages>
      <abstract>The field of Natural Language Processing (NLP) changes rapidly, requiring course offerings to adjust with those changes, and NLP is not just for computer scientists; it’s a field that should be accessible to anyone who has a sufficient background. In this paper, I explain how students with Computer Science and Data Science backgrounds can be well-prepared for an upper-division NLP course at a large state university. The course covers probability and information theory, elementary linguistics, machine and deep learning, with an attempt to balance theoretical ideas and concepts with practical applications. I explain the course objectives, topics and assignments, reflect on adjustments to the course over the last four years, as well as feedback from students.</abstract>
      <url hash="3d504ce6">2021.teachingnlp-1.21</url>
      <doi>10.18653/v1/2021.teachingnlp-1.21</doi>
      <bibkey>kennington-2021-natural</bibkey>
    </paper>
    <paper id="22">
      <title>On Writing a Textbook on Natural Language Processing</title>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>125–130</pages>
      <abstract>There are thousands of papers about natural language processing and computational linguistics, but very few textbooks. I describe the motivation and process for writing a college textbook on natural language processing, and offer advice and encouragement for readers who may be interested in writing a textbook of their own.</abstract>
      <url hash="8b27403f">2021.teachingnlp-1.22</url>
      <doi>10.18653/v1/2021.teachingnlp-1.22</doi>
      <bibkey>eisenstein-2021-writing</bibkey>
    </paper>
    <paper id="23">
      <title>Learning How To Learn <fixed-case>NLP</fixed-case>: Developing Introductory Concepts Through Scaffolded Discovery</title>
      <author><first>Alexandra</first><last>Schofield</last></author>
      <author><first>Richard</first><last>Wicentowski</last></author>
      <author><first>Julie</first><last>Medero</last></author>
      <pages>131–137</pages>
      <abstract>We present a scaffolded discovery learning approach to introducing concepts in a Natural Language Processing course aimed at computer science students at liberal arts institutions. We describe some of the objectives of this approach, as well as presenting specific ways that four of our discovery-based assignments combine specific natural language processing concepts with broader analytic skills. We argue this approach helps prepare students for many possible future paths involving both application and innovation of NLP technology by emphasizing experimental data navigation, experiment design, and awareness of the complexities and challenges of analysis.</abstract>
      <url hash="988d906c">2021.teachingnlp-1.23</url>
      <doi>10.18653/v1/2021.teachingnlp-1.23</doi>
      <bibkey>schofield-etal-2021-learning</bibkey>
    </paper>
    <paper id="24">
      <title>The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges</title>
      <author><first>Beatrice</first><last>Alex</last></author>
      <author><first>Clare</first><last>Llewellyn</last></author>
      <author><first>Pawel</first><last>Orzechowski</last></author>
      <author><first>Maria</first><last>Boutchkova</last></author>
      <pages>138–148</pages>
      <abstract>In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students’ learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.</abstract>
      <url hash="092ea5d2">2021.teachingnlp-1.24</url>
      <doi>10.18653/v1/2021.teachingnlp-1.24</doi>
      <bibkey>alex-etal-2021-online</bibkey>
    </paper>
    <paper id="25">
      <title>Teaching <fixed-case>NLP</fixed-case> outside Linguistics and Computer Science classrooms: Some challenges and some opportunities</title>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <pages>149–159</pages>
      <abstract>NLP’s sphere of influence went much beyond computer science research and the development of software applications in the past decade. We see people using NLP methods in a range of academic disciplines from Asian Studies to Clinical Oncology. We also notice the presence of NLP as a module in most of the data science curricula within and outside of regular university setups. These courses are taken by students from very diverse backgrounds. This paper takes a closer look at some issues related to teaching NLP to these diverse audiences based on my classroom experiences, and identifies some challenges the instructors face, particularly when there is no ecosystem of related courses for the students. In this process, it also identifies a few challenge areas for both NLP researchers and tool developers.</abstract>
      <url hash="e04207cb">2021.teachingnlp-1.25</url>
      <doi>10.18653/v1/2021.teachingnlp-1.25</doi>
      <bibkey>vajjala-2021-teaching</bibkey>
    </paper>
    <paper id="26">
      <title>Teaching <fixed-case>NLP</fixed-case> with Bracelets and Restaurant Menus: An Interactive Workshop for <fixed-case>I</fixed-case>talian Students</title>
      <author><first>Ludovica</first><last>Pannitto</last></author>
      <author><first>Lucia</first><last>Busso</last></author>
      <author><first>Claudia Roberta</first><last>Combei</last></author>
      <author><first>Lucio</first><last>Messina</last></author>
      <author><first>Alessio</first><last>Miaschi</last></author>
      <author><first>Gabriele</first><last>Sarti</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>160–170</pages>
      <abstract>Although Natural Language Processing is at the core of many tools young people use in their everyday life, high school curricula (in Italy) do not include any computational linguistics education. This lack of exposure makes the use of such tools less responsible than it could be, and makes choosing computational linguistics as a university degree unlikely. To raise awareness, curiosity, and longer-term interest in young people, we have developed an interactive workshop designed to illustrate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years. The workshop takes the form of a game in which participants play the role of machines needing to solve some of the most common problems a computer faces in understanding language: from voice recognition to Markov chains to syntactic parsing. Participants are guided through the workshop with the help of instructors, who present the activities and explain core concepts from computational linguistics. The workshop was presented at numerous outlets in Italy between 2019 and 2020, both face-to-face and online.</abstract>
      <url hash="cfda28bb">2021.teachingnlp-1.26</url>
      <doi>10.18653/v1/2021.teachingnlp-1.26</doi>
      <bibkey>pannitto-etal-2021-teaching</bibkey>
      <pwccode url="https://bitbucket.org/melfnt/malvisindi" additional="false">melfnt/malvisindi</pwccode>
    </paper>
  </volume>
</collection>
