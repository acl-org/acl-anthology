<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.law">
  <volume id="1" ingest-date="2024-03-04" type="proceedings">
    <meta>
      <booktitle>Proceedings of The 18th Linguistic Annotation Workshop (LAW-XVIII)</booktitle>
      <editor><first>Sophie</first><last>Henning</last></editor>
      <editor><first>Manfred</first><last>Stede</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julians, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="554af0c8">2024.law-1</url>
      <venue>law</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="6881a83b">2024.law-1.0</url>
      <bibkey>law-2024-linguistic</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>ree<fixed-case>F</fixed-case>orm: End-to-end Annotation and Evaluation for Form Document Parsing</title>
      <author><first>Ran</first><last>Zmigrod</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Zhiqiang</first><last>Ma</last><affiliation>JPMorgan Chase</affiliation></author>
      <author><first>Armineh</first><last>Nourbakhsh</last><affiliation>CMU, JP Morgan Chase</affiliation></author>
      <author><first>Sameena</first><last>Shah</last><affiliation>JP Morgan</affiliation></author>
      <pages>1-11</pages>
      <abstract>Visually Rich Form Understanding (VRFU) poses a complex research problemdue to the documents’ highly structured nature and yet highly variable style and content. Current annotation schemes decompose form understanding and omit key hierarchical structure, making development and evaluation of end-to-end models difficult. In this paper, we propose a novel F1 metric to evaluate form parsers and describe a new content-agnostic, tree-based annotation scheme for VRFU: TreeForm. We provide methods to convert previous annotation schemes into TreeForm structures and evaluate TreeForm predictions using a modified version of the normalized tree-edit distance. We present initial baselines for our end-to-end performance metric and the TreeForm edit distance, averaged over the FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm encourages deeper research in annotating, modeling, and evaluating the complexities of form-like documents.</abstract>
      <url hash="81adf17f">2024.law-1.1</url>
      <bibkey>zmigrod-etal-2024-treeform</bibkey>
      <video href="2024.law-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Annotation Scheme for <fixed-case>E</fixed-case>nglish Argument Structure Constructions Treebank</title>
      <author><first>Hakyung</first><last>Sung</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Kristopher</first><last>Kyle</last><affiliation>University of Oregon</affiliation></author>
      <pages>12-18</pages>
      <abstract>We introduce a detailed annotation scheme for argument structure constructions (ASCs) along with a manually annotated ASC treebank. This treebank encompasses 10,204 sentences from both first (5,936) and second language English datasets (1,948 for written; 2,320 for spoken). We detail the annotation process and evaluate inter-annotation agreement for overall and each ASC category.</abstract>
      <url hash="faeacd8c">2024.law-1.2</url>
      <bibkey>sung-kyle-2024-annotation</bibkey>
      <video href="2024.law-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>A Mapping on Current Classifying Categories of Emotions Used in Multimodal Models for Emotion Recognition</title>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Muyin</first><last>Yao</last><affiliation>Tufts University</affiliation></author>
      <author><first>Xinyi</first><last>Hu</last><affiliation>Tufts University</affiliation></author>
      <author><first>Xiaoning</first><last>Zhu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University in the City of New York</affiliation></author>
      <pages>19-28</pages>
      <abstract>In Emotion Detection within Natural Language Processing and related multimodal research, the growth of datasets and models has led to a challenge: disparities in emotion classification methods. The lack of commonly agreed upon conventions on the classification of emotions creates boundaries for model comparisons and dataset adaptation. In this paper, we compare the current classification methods in recent models and datasets and propose a valid method to combine different emotion categories. Our proposal arises from experiments across models, psychological theories, and human evaluations, and we examined the effect of proposed mapping on models.</abstract>
      <url hash="94c5c4db">2024.law-1.3</url>
      <bibkey>gong-etal-2024-mapping</bibkey>
    </paper>
    <paper id="4">
      <title>Surveying the <fixed-case>FAIR</fixed-case>ness of Annotation Tools: Difficult to find, difficult to reuse</title>
      <author><first>Ekaterina</first><last>Borisova</last><affiliation>Deutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI)</affiliation></author>
      <author><first>Raia</first><last>Abu Ahmad</last><affiliation>Deutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI)</affiliation></author>
      <author><first>Leyla</first><last>Garcia-Castro</last><affiliation>ZB MED – Information Centre for Life Sciences</affiliation></author>
      <author><first>Ricardo</first><last>Usbeck</last><affiliation>Leuphana University Lüneburg</affiliation></author>
      <author><first>Georg</first><last>Rehm</last><affiliation>DFKI</affiliation></author>
      <pages>29-45</pages>
      <abstract>In the realm of Machine Learning and Deep Learning, there is a need for high-quality annotated data to train and evaluate supervised models. An extensive number of annotation tools have been developed to facilitate the data labelling process. However, finding the right tool is a demanding task involving thorough searching and testing. Hence, to effectively navigate the multitude of tools, it becomes essential to ensure their findability, accessibility, interoperability, and reusability (FAIR). This survey addresses the FAIRness of existing annotation software by evaluating 50 different tools against the FAIR principles for research software (FAIR4RS). The study indicates that while being accessible and interoperable, annotation tools are difficult to find and reuse. In addition, there is a need to establish community standards for annotation software development, documentation, and distribution.</abstract>
      <url hash="ab4add42">2024.law-1.4</url>
      <bibkey>borisova-etal-2024-surveying</bibkey>
      <video href="2024.law-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Automatic Annotation Elaboration as Feedback to Sign Language Learners</title>
      <author><first>Alessia</first><last>Battisti</last><affiliation>University of Zurich, Switzerland</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>University of Zurich</affiliation></author>
      <pages>46-60</pages>
      <abstract>Beyond enabling linguistic analyses, linguistic annotations may serve as training material for developing automatic language assessment models as well as for providing textual feedback to language learners. Yet these linguistic annotations in their original form are often not easily comprehensible for learners. In this paper, we explore the utilization of GPT-4, as an example of a large language model (LLM), to process linguistic annotations into clear and understandable feedback on their productions for language learners, specifically sign language learners.</abstract>
      <url hash="8ac3b4b1">2024.law-1.5</url>
      <bibkey>battisti-ebling-2024-automatic</bibkey>
      <video href="2024.law-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Towards Better Inclusivity: A Diverse Tweet Corpus of <fixed-case>E</fixed-case>nglish Varieties</title>
      <author><first>Nhi</first><last>Pham</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Lachlan</first><last>Pham</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Adam</first><last>Meyers</last><affiliation>New York University</affiliation></author>
      <pages>61-70</pages>
      <abstract>The prevalence of social media presents a growing opportunity to collect and analyse examples of English varieties. Whilst usage of these varieties is often used only in spoken contexts or hard-to-access private messages, social media sites like Twitter provide a platform for users to communicate informally in a scrapeable format. Notably, Indian English (Hinglish), Singaporean English (Singlish), and African-American English (AAE) can be commonly found online. These varieties pose a challenge to existing natural language processing (NLP) tools as they often differ orthographically and syntactically from standard English for which the majority of these tools are built. NLP models trained on standard English texts produced biased outcomes for users of underrepresented varieties (Blodgett and O’Connor, 2017). Some research has aimed to overcome the inherent biases caused by unrepresentative data through techniques like data augmentation or adjusting training models. We aim to address the issue of bias at its root - the data itself. We curate a dataset of tweets from countries with high proportions of underserved English variety speakers, and propose an annotation framework of six categorical classifications along a pseudo-spectrum that measures the degree of standard English and that thereby indirectly aims to surface the manifestations of English varieties in these tweets.</abstract>
      <url hash="64a23995">2024.law-1.6</url>
      <bibkey>pham-etal-2024-towards</bibkey>
    </paper>
    <paper id="7">
      <title>Building a corpus for the anonymization of <fixed-case>R</fixed-case>omanian jurisprudence</title>
      <author><first>Vasile</first><last>Păiș</last><affiliation>Research Institute for Artificial Intelligence, Romanian Academy</affiliation></author>
      <author><first>Dan</first><last>Tufis</last><affiliation>Research Institute for Artificial Intelligence, Romanian Academy</affiliation></author>
      <author><first>Elena</first><last>Irimia</last><affiliation>Research Institute for Artificial Intelligence, Romanian Academy (RACAI)</affiliation></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last><affiliation>RACAI</affiliation></author>
      <pages>71-76</pages>
      <abstract>Access to jurisprudence is of paramount importance for both law professionals (judges, lawyers, law students) and for the larger public. In Romania, the Superior Council of Magistracy holds a large database of jurisprudence from different courts in the country, which is updated daily. However, granting public access requires its anonymization. This paper presents the efforts behind building a corpus for the anonymization process. We present the annotation scheme, the manual annotation methods, and the platform used.</abstract>
      <url hash="2db28bf2">2024.law-1.7</url>
      <bibkey>pais-etal-2024-building</bibkey>
      <video href="2024.law-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Class Balancing for Efficient Active Learning in Imbalanced Datasets</title>
      <author><first>Yaron</first><last>Fairstein</last><affiliation>Amazon</affiliation></author>
      <author><first>Oren</first><last>Kalinsky</last><affiliation>Amazon</affiliation></author>
      <author><first>Zohar</first><last>Karnin</last><affiliation>Amazon</affiliation></author>
      <author><first>Guy</first><last>Kushilevitz</last><affiliation>Amazon</affiliation></author>
      <author><first>Alexander</first><last>Libov</last><affiliation>Amazon</affiliation></author>
      <author><first>Sofia</first><last>Tolmach</last><affiliation>Amazon</affiliation></author>
      <pages>77-86</pages>
      <abstract>Recent developments in active learning algorithms for NLP tasks show promising results in terms of reducing labelling complexity. In this paper we extend this effort to imbalanced datasets; we bridge between the active learning approach of obtaining diverse andinformative examples, and the heuristic of class balancing used in imbalanced datasets. We develop a novel tune-free weighting technique that canbe applied to various existing active learning algorithms, adding a component of class balancing. We compare several active learning algorithms to their modified version on multiple public datasetsand show that when the classes are imbalanced, with manual annotation effort remaining equal the modified version significantly outperforms the original both in terms of the test metric and the number of obtained minority examples. Moreover, when the imbalance is mild or non-existent (classes are completely balanced), our technique does not harm the base algorithms.</abstract>
      <url hash="62ff1c92">2024.law-1.8</url>
      <bibkey>fairstein-etal-2024-class</bibkey>
      <video href="2024.law-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>When is a Metaphor Actually Novel? Annotating Metaphor Novelty in the Context of Automatic Metaphor Detection</title>
      <author><first>Sebastian</first><last>Reimann</last><affiliation>Ruhr Universität Bochum</affiliation></author>
      <author><first>Tatjana</first><last>Scheffler</last><affiliation>Ruhr University Bochum</affiliation></author>
      <pages>87-97</pages>
      <abstract>We present an in-depth analysis of metaphor novelty, a relatively overlooked phenomenon in NLP. Novel metaphors have been analyzed via scores derived from crowdsourcing in NLP, while in theoretical work they are often defined by comparison to senses in dictionary entries. We reannotate metaphorically used words in the large VU Amsterdam Metaphor Corpus based on whether their metaphoric meaning is present in the dictionary. Based on this, we find that perceived metaphor novelty often clash with the dictionary based definition. We use the new labels to evaluate the performance of state-of-the-art language models for automatic metaphor detection and notice that novel metaphors according to our dictionary-based definition are easier to identify than novel metaphors according to crowd-sourced novelty scores. In a subsequent analysis, we study the correlation between high novelty scores and word frequencies in the pretraining and finetuning corpora, as well as potential problems with rare words for pre-trained language models. In line with previous works, we find a negative correlation between word frequency in the training data and novelty scores and we link these aspects to problems with the tokenization of BERT and RoBERTa.</abstract>
      <url hash="fc66e5b5">2024.law-1.9</url>
      <bibkey>reimann-scheffler-2024-metaphor</bibkey>
      <video href="2024.law-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Enhancing Text Classification through <fixed-case>LLM</fixed-case>-Driven Active Learning and Human Annotation</title>
      <author><first>Hamidreza</first><last>Rouzegar</last><affiliation>Ontario Tech University</affiliation></author>
      <author><first>Masoud</first><last>Makrehchi</last><affiliation>Ontario Tech University</affiliation></author>
      <pages>98-111</pages>
      <abstract>In the context of text classification, the financial burden of annotation exercises for creating training data is a critical issue. Active learning techniques, particularly those rooted in uncertainty sampling, offer a cost-effective solution by pinpointing the most instructive samples for manual annotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability. This study introduces a novel methodology that integrates human annotators and LLMs within an Active Learning framework. We conducted evaluations on three public datasets. IMDB for sentiment analysis, a Fake News dataset for authenticity discernment, and a Movie Genres dataset for multi-label classification.The proposed framework integrates human annotation with the output of LLMs, depending on the model uncertainty levels. This strategy achieves an optimal balance between cost efficiency and classification performance. The empirical results show a substantial decrease in the costs associated with data annotation while either maintaining or improving model accuracy.</abstract>
      <url hash="291ad332">2024.law-1.10</url>
      <bibkey>rouzegar-makrehchi-2024-enhancing</bibkey>
      <video href="2024.law-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Using <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for Annotation of Attitude within the Appraisal Theory: Lessons Learned</title>
      <author><first>Mirela</first><last>Imamovic</last><affiliation>University of Hildesheim</affiliation></author>
      <author><first>Silvana</first><last>Deilen</last><affiliation>UniversitätHildesheim</affiliation></author>
      <author><first>Dylan</first><last>Glynn</last><affiliation>University of Paris 8</affiliation></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last><affiliation>University of Hildesheim</affiliation></author>
      <pages>112-123</pages>
      <abstract>We investigate the potential of using ChatGPT to annotate complex linguistic phenomena, such as language of evaluation, attitude and emotion. For this, we automatically annotate 11 texts in English, which represent spoken popular science, and evaluate the annotations manually. Our results show that ChatGPT has good precision in itemisation, i.e. detecting linguistic items in the text that carry evaluative meaning. However, we also find that the recall is very low. Besides that, we state that the tool fails in labeling the detected items with the correct categories on a more fine-grained level of granularity. We analyse the errors to find systematic errors related to specific categories in the annotation scheme.</abstract>
      <url hash="cd7a0437">2024.law-1.11</url>
      <bibkey>imamovic-etal-2024-using</bibkey>
    </paper>
    <paper id="12">
      <title>Are You Serious? Handling Disagreement When Annotating Conspiracy Theory Texts</title>
      <author><first>Ashley</first><last>Hemm</last><affiliation>University of Miami</affiliation></author>
      <author><first>Sandra</first><last>Kübler</last><affiliation>Indiana University</affiliation></author>
      <author><first>Michelle</first><last>Seelig</last><affiliation>University of Miami</affiliation></author>
      <author><first>John</first><last>Funchion</last><affiliation>University of Miami</affiliation></author>
      <author><first>Manohar</first><last>Murthi</last><affiliation>University of Miami</affiliation></author>
      <author><first>Kamal</first><last>Premaratne</last><affiliation>University of Miami</affiliation></author>
      <author><first>Daniel</first><last>Verdear</last><affiliation>University of Miami</affiliation></author>
      <author><first>Stefan</first><last>Wuchty</last><affiliation>University of Miami</affiliation></author>
      <pages>124-132</pages>
      <abstract>We often assume that annotation tasks, such as annotating for the presence of conspiracy theories, can be annotated with hard labels, without definitions or guidelines. Our annotation experiments, comparing students and experts, show that there is little agreement on basic annotations even among experts. For this reason, we conclude that we need to accept disagreement as an integral part of such annotations.</abstract>
      <url hash="297657ab">2024.law-1.12</url>
      <bibkey>hemm-etal-2024-serious</bibkey>
      <video href="2024.law-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>A <fixed-case>GPT</fixed-case> among Annotators: <fixed-case>LLM</fixed-case>-based Entity-Level Sentiment Annotation</title>
      <author><first>Egil</first><last>Rønningstad</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Erik</first><last>Velldal</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept of Informatics, University of Oslo</affiliation></author>
      <pages>133-139</pages>
      <abstract>We investigate annotator variation for the novel task of Entity-Level Sentiment Analysis (ELSA) which annotates the aggregated sentiment directed towards volitional entities in a text. More specifically, we analyze the annotations of a newly constructed Norwegian ELSA dataset and release additional data with each annotator’s labels for the 247 entities in the dataset’s test split. We also perform a number of experiments prompting ChatGPT for these sentiment labels regarding each entity in the text and compare the generated annotations with the human labels. Cohen’s Kappa for agreement between the best LLM-generated labels and curated gold was 0.425, which indicates that these labels would not have high quality. Our analyses further investigate the errors that ChatGPT outputs, and compare them with the variations that we find among the 5 trained annotators that all annotated the same test data.</abstract>
      <url hash="86abc0d1">2024.law-1.13</url>
      <bibkey>ronningstad-etal-2024-gpt</bibkey>
    </paper>
    <paper id="14">
      <title>Datasets Creation and Empirical Evaluations of Cross-Lingual Learning on Extremely Low-Resource Languages: A Focus on Comorian Dialects</title>
      <author><first>Abdou Mohamed</first><last>Naira</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Benelallam</first><last>Imade</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Bahafid</first><last>Abdessalam</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Erraji</first><last>Zakarya</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <pages>140-149</pages>
      <abstract>In this era of extensive digitalization, there are a profusion of Intelligent Systems that attempt to understand how languages are structured for the aim of providing solutions in various tasks like Text Summarization, Sentiment Analysis, Speech Recognition, etc. But for multiple reasons going from lack of data to the nonexistence of initiatives, these applications are in an embryonic stage in certain languages and dialects, especially those spoken in the African continent, like Comorian dialects. Today, thanks to the improvement of Pre-trained Large Language Models, a spacious way is open to enable these kind of technologies on these languages. In this study, we are pioneering the representation of Comorian dialects in the field of Natural Language Processing (NLP) by constructing datasets (Lexicons, Speech Recognition and Raw Text datasets) that could be used on different tasks. We also measure the impact of using pre-trained models on languages closely related to Comorian dialects to enhance the state-of-the-art in NLP for these latter, compared to using pre-trained models on languages that may not necessarily be close to these dialects. We construct models covering the following use cases: Language Identification, Sentiment Analysis, Part-Of-Speech Tagging, and Speech Recognition. Ultimately, we hope that these solutions can catalyze the improvement of similar initiatives in Comorian dialects and in languages facing similar challenges.</abstract>
      <url hash="bc6564b1">2024.law-1.14</url>
      <bibkey>naira-etal-2024-datasets</bibkey>
    </paper>
    <paper id="15">
      <title>Prompting Implicit Discourse Relation Annotation</title>
      <author><first>Frances</first><last>Yung</last><affiliation>Saarland University</affiliation></author>
      <author><first>Mansoor</first><last>Ahmad</last><affiliation>Saarland University</affiliation></author>
      <author><first>Merel</first><last>Scholman</last><affiliation>Saarland University</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Saarland University</affiliation></author>
      <pages>150-165</pages>
      <abstract>Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT’s performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT’s recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.</abstract>
      <url hash="097817e2">2024.law-1.15</url>
      <bibkey>yung-etal-2024-prompting</bibkey>
      <video href="2024.law-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title><fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank goes Public: Incorporation into <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Elizabeth</first><last>Spaulding</last><affiliation>CU-Boulder</affiliation></author>
      <author><first>Kathryn</first><last>Conger</last><affiliation>Universitiy of Colorado, Boulder</affiliation></author>
      <author><first>Anatole</first><last>Gershman</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Mahir</first><last>Morshed</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Susan Windisch</first><last>Brown</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>James</first><last>Pustejovsky</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Rosario</first><last>Uceda-Sosa</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sijia</first><last>Ge</last><affiliation>University of Colorado-Boulder</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <pages>166-175</pages>
      <abstract>This paper presents the first integration of PropBank role information into Wikidata, in order to provide a novel resource for information extraction, one combining Wikidata’s ontological metadata with PropBank’s rich argument structure encoding for event classes. We discuss a technique for PropBank augmentation to existing eventive Wikidata items, as well as identification of gaps in Wikidata’s coverage based on manual examination of over 11,300 PropBank rolesets. We propose five new Wikidata properties to integrate PropBank structure into Wikidata so that the annotated mappings can be added en masse. We then outline the methodology and challenges of this integration, including annotation with the combined resources.</abstract>
      <url hash="122b3ac7">2024.law-1.16</url>
      <bibkey>spaulding-etal-2024-propbank</bibkey>
    </paper>
    <paper id="17">
      <title>Reference and discourse structure annotation of elicited chat continuations in <fixed-case>G</fixed-case>erman</title>
      <author><first>Katja</first><last>Jasinskaja</last><affiliation>University of Cologne</affiliation></author>
      <author><first>Yuting</first><last>Li</last><affiliation>University of Cologne</affiliation></author>
      <author><first>Fahime</first><last>Same</last><affiliation>University of Cologne</affiliation></author>
      <author><first>David</first><last>Uerlings</last><affiliation>Universität zu Köln</affiliation></author>
      <pages>176-187</pages>
      <abstract>We present the construction of a German chat corpus in an experimental setting. Our primary objective is to advance the methodology of discourse continuation for dialogue. The corpus features a fine-grained, multi-layer annotation of referential expressions and coreferential chains. Additionally, we have developed a comprehensive annotation scheme for coherence relations to describe discourse structure.</abstract>
      <url hash="2517995e">2024.law-1.17</url>
      <bibkey>jasinskaja-etal-2024-reference</bibkey>
      <video href="2024.law-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Dependency Annotation of <fixed-case>O</fixed-case>ttoman <fixed-case>T</fixed-case>urkish with Multilingual <fixed-case>BERT</fixed-case></title>
      <author><first>Şaziye Betül</first><last>Özateş</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Tarık</first><last>Tıraş</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Efe</first><last>Genç</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Esma</first><last>Bilgin Tasdemir</last><affiliation>Istanbul Medeniyet University</affiliation></author>
      <pages>188-196</pages>
      <abstract>This study introduces a pretrained large language model-based annotation methodology of the first dependency treebank in Ottoman Turkish. Our experimental results show that, through iteratively i) pseudo-annotating data using a multilingual BERT-based parsing model, ii) manually correcting the pseudo-annotations, and iii) fine-tuning the parsing model with the corrected annotations, we speed up and simplify the challenging dependency annotation process. The resulting treebank, that will be a part of the Universal Dependencies (UD) project, will facilitate automated analysis of Ottoman Turkish documents, unlocking the linguistic richness embedded in this historical heritage.</abstract>
      <url hash="8b28f181">2024.law-1.18</url>
      <bibkey>ozates-etal-2024-dependency</bibkey>
    </paper>
    <paper id="19">
      <title>Donkii: Characterizing and Detecting Errors in Instruction-Tuning Datasets</title>
      <author><first>Leon</first><last>Weber</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Robert</first><last>Litschko</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Ekaterina</first><last>Artemova</last><affiliation>Toloka.AI</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>LMU Munich</affiliation></author>
      <pages>197-215</pages>
      <abstract>Instruction tuning has become an integral part of training pipelines for Large Language Models (LLMs) and has been shown to yield strong performance gains. In an orthogonal line of research, Annotation Error Detection (AED) has emerged as a tool for detecting quality problems in gold standard labels. So far, however, the application of AED methods has been limited to classification tasks. It is an open question how well AED methods generalize to language generation settings, which are becoming more widespread via LLMs. In this paper, we present a first and novel benchmark for AED on instruction tuning data: Donkii.It comprises three instruction-tuning datasets enriched with error annotations by experts and semi-automatic methods. We also provide a novel taxonomy of error types for instruction-tuning data.We find that all three datasets contain clear errors, which sometimes propagate directly into instruction-tuned LLMs. We propose four AED baselines for the generative setting and evaluate them extensively on the newly introduced dataset. Our results show that the choice of the right AED method and model size is indeed crucial and derive practical recommendations for how to use AED methods to clean instruction-tuning data.</abstract>
      <url hash="bc0ad0ac">2024.law-1.19</url>
      <bibkey>weber-etal-2024-donkii</bibkey>
      <video href="2024.law-1.19.mp4"/>
    </paper>
    <paper id="20">
      <title><fixed-case>EEVEE</fixed-case>: An Easy Annotation Tool for Natural Language Processing</title>
      <author><first>Axel</first><last>Sorensen</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Siyao</first><last>Peng</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Rob</first><last>Van Der Goot</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>216-221</pages>
      <abstract>Annotation tools are the starting point for creating Natural Language Processing (NLP) datasets. There is a wide variety of tools available; setting up these tools is however a hindrance. We propose Eevee, an annotation tool focused on simplicity, efficiency, and ease of use. It can run directly in the browser (no setup required) and uses tab-separated files (as opposed to character offsets or task-specific formats) for annotation. It allows for annotation of multiple tasks on a single dataset and supports four task-types: sequence labeling, span labeling, text classification and seq2seq.</abstract>
      <url hash="ca7c3d3a">2024.law-1.20</url>
      <bibkey>sorensen-etal-2024-eevee</bibkey>
      <video href="2024.law-1.20.mp4"/>
    </paper>
  </volume>
</collection>
