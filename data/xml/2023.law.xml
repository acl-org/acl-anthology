<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.law">
  <volume id="1" ingest-date="2023-07-09" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)</booktitle>
      <editor><first>Jakob</first><last>Prange</last></editor>
      <editor><first>Annemarie</first><last>Friedrich</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="13ffc7ed">2023.law-1</url>
      <venue>law</venue>
    </meta>
    <frontmatter>
      <url hash="632eefb1">2023.law-1.0</url>
      <bibkey>law-2023-linguistic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Medieval Social Media: Manual and Automatic Annotation of Byzantine <fixed-case>G</fixed-case>reek Marginal Writing</title>
      <author><first>Colin</first><last>Swaelens</last><affiliation>Ghent University</affiliation></author>
      <author><first>Ilse</first><last>De Vos</last><affiliation>Ghent University</affiliation></author>
      <author><first>Els</first><last>Lefever</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>1-9</pages>
      <abstract>In this paper, we present the interim results of a transformer-based annotation pipeline for Ancient and Medieval Greek. As the texts in the Database of Byzantine Book Epigrams have not been normalised, they pose more challenges for manual and automatic annotation than Ancient Greek, normalised texts do. As a result, the existing annotation tools perform poorly. We compiled three data sets for the development of an automatic annotation tool and carried out an inter-annotator agreement study, with a promising agreement score. The experimental results show that our part-of-speech tagger yields accuracy scores that are almost 50 percentage points higher than the widely used rule-based system Morpheus. In addition, error analysis revealed problems related to phenomena also occurring in current social media language.</abstract>
      <url hash="c36a79ed">2023.law-1.1</url>
      <bibkey>swaelens-etal-2023-medieval</bibkey>
      <doi>10.18653/v1/2023.law-1.1</doi>
    </paper>
    <paper id="2">
      <title>“Orpheus Came to His End by Being Struck by a Thunderbolt”: Annotating Events in Mythological Sequences</title>
      <author><first>Franziska</first><last>Pannach</last><affiliation>University of Göttingen</affiliation></author>
      <pages>10-18</pages>
      <abstract>The mythological domain has various ways of expressing events and background knowledge. Using data extracted according to the hylistic approach (Zgoll, 2019), we annotated a data set of 6315 sentences from various mythological contexts and geographical origins, like Ancient Greece and Rome or Mesopotamia, into four categories: single-point events (e.g. actions), durative-constant (background knowledge, continuous states), durative-initial, and durative-resultativ. This data is used to train a classifier, which is able to reliably distinguish event types.</abstract>
      <url hash="e67de1ba">2023.law-1.2</url>
      <bibkey>pannach-2023-orpheus</bibkey>
      <doi>10.18653/v1/2023.law-1.2</doi>
      <video href="2023.law-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Difficulties in Handling Mathematical Expressions in <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Lauren</first><last>Levine</last><affiliation>Georgetown University</affiliation></author>
      <pages>19-30</pages>
      <abstract>In this paper, we give a brief survey of the difficulties in handling the syntax of mathematical expressions in Universal Dependencies, focusing on examples from English language corpora. We first examine the prevalence and current handling of mathematical expressions in UD corpora. We then examine several strategies for how to approach the handling of syntactic dependencies for such expressions: as multi-word expressions, as a domain appropriate for code-switching, or as approximate to other types of natural language. Ultimately, we argue that mathematical expressions should primarily be analyzed as natural language, and we offer recommendations for the treatment of basic mathematical expressions as analogous to English natural language.</abstract>
      <url hash="7f5a61e9">2023.law-1.3</url>
      <bibkey>levine-2023-difficulties</bibkey>
      <doi>10.18653/v1/2023.law-1.3</doi>
    </paper>
    <paper id="4">
      <title>A Dataset for Physical and Abstract Plausibility and Sources of Human Disagreement</title>
      <author><first>Annerose</first><last>Eichel</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>31-45</pages>
      <abstract>We present a novel dataset for physical and abstract plausibility of events in English. Based on naturally occurring sentences extracted from Wikipedia, we infiltrate degrees of abstractness, and automatically generate perturbed pseudo-implausible events. We annotate a filtered and balanced subset for plausibility using crowd-sourcing, and perform extensive cleansing to ensure annotation quality. In-depth quantitative analyses indicate that annotators favor plausibility over implausibility and disagree more on implausible events. Furthermore, our plausibility dataset is the first to capture abstractness in events to the same extent as concreteness, and we find that event abstractness has an impact on plausibility ratings: more concrete event participants trigger a perception of implausibility.</abstract>
      <url hash="25a5e2eb">2023.law-1.4</url>
      <bibkey>eichel-schulte-im-walde-2023-dataset</bibkey>
      <doi>10.18653/v1/2023.law-1.4</doi>
      <video href="2023.law-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Annotating and Disambiguating the Discourse Usage of the Enclitic d<fixed-case>A</fixed-case> in <fixed-case>T</fixed-case>urkish</title>
      <author><first>Ebru</first><last>Ersöyleyen</last><affiliation>Middle East Technical University</affiliation></author>
      <author><first>Deniz</first><last>Zeyrek</last><affiliation>Middle East Technical University</affiliation></author>
      <author><first>Fırat</first><last>Öter</last><affiliation>Middle East Technical University</affiliation></author>
      <pages>46-54</pages>
      <abstract>The Turkish particle dA is a focus-associated enclitic, and it can act as a discourse connective conveying multiple senses, like additive, contrastive, causal etc. Like many other linguistic expressions, it is subject to usage ambiguity and creates a challenge in natural language automatization tasks. For the first time, we annotate the discourse and non-discourse connnective occurrences of dA in Turkish with the PDTB principles. Using a minimal set of linguistic features, we develop binary classifiers to distinguish its discourse connective usage from its other usages. We show that despite its ability to cliticize to any syntactic type, variable position in the sentence and having a wide argument span, its discourse/non-discourse connective usage can be annotated reliably and its discourse usage can be disambiguated by exploiting local cues.</abstract>
      <url hash="1aa9ae17">2023.law-1.5</url>
      <bibkey>ersoyleyen-etal-2023-annotating</bibkey>
      <doi>10.18653/v1/2023.law-1.5</doi>
      <video href="2023.law-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>An Active Learning Pipeline for <fixed-case>NLU</fixed-case> Error Detection in Conversational Agents</title>
      <author><first>Damian</first><last>Pascual</last><affiliation>Telepathy Labs</affiliation></author>
      <author><first>Aritz</first><last>Bercher</last><affiliation>Telepathy Labs</affiliation></author>
      <author><first>Akansha</first><last>Bhardwaj</last><affiliation>University of Fribourg</affiliation></author>
      <author><first>Mingbo</first><last>Cui</last><affiliation>Telepathy Labs</affiliation></author>
      <author><first>Dominic</first><last>Kohler</last><affiliation>Telepathy Labs</affiliation></author>
      <author><first>Liam</first><last>Van Der Poel</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Paolo</first><last>Rosso</last><affiliation>University of Fribourg</affiliation></author>
      <pages>55-60</pages>
      <abstract>High-quality labeled data is paramount to the performance of modern machine learning models. However, annotating data is a time-consuming and costly process that requires human experts to examine large collections of raw data. For conversational agents in production settings with access to large amounts of user-agent conversations, the challenge is to decide what data should be annotated first. We consider the Natural Language Understanding (NLU) component of a conversational agent deployed in a real-world setup with limited resources. We present an active learning pipeline for offline detection of classification errors that leverages two strong classifiers. Then, we perform topic modeling on the potentially mis-classified samples to ease data analysis and to reveal error patterns. In our experiments, we show on a real-world dataset that by using our method to prioritize data annotation we reach 100% of the performance annotating only 36% of the data. Finally, we present an analysis of some of the error patterns revealed and argue that our pipeline is a valuable tool to detect critical errors and reduce the workload of annotators.</abstract>
      <url hash="0195e769">2023.law-1.6</url>
      <bibkey>pascual-etal-2023-active</bibkey>
      <doi>10.18653/v1/2023.law-1.6</doi>
    </paper>
    <paper id="7">
      <title>Multi-layered Annotation of Conversation-like Narratives in <fixed-case>G</fixed-case>erman</title>
      <author><first>Magdalena</first><last>Repp</last><affiliation>University of Cologne</affiliation></author>
      <author><first>Petra B.</first><last>Schumacher</last><affiliation>University of Cologne</affiliation></author>
      <author><first>Fahime</first><last>Same</last><affiliation>University of Cologne</affiliation></author>
      <pages>61-72</pages>
      <abstract>This work presents two corpora based on excerpts from two novels with an informal narration style in German. We performed fine-grained multi-layer annotations of animate referents, assigning local and global prominence-lending features to the annotated referring expressions. In addition, our corpora include annotations of intra-sentential segments, which can serve as a more reliable unit of length measurement. Furthermore, we present two exemplary studies demonstrating how to use these corpora.</abstract>
      <url hash="76fd128a">2023.law-1.7</url>
      <bibkey>repp-etal-2023-multi</bibkey>
      <doi>10.18653/v1/2023.law-1.7</doi>
      <video href="2023.law-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Crowdsourcing on Sensitive Data with Privacy-Preserving Text Rewriting</title>
      <author><first>Nina</first><last>Mouhammad</last><affiliation>DIPF | Leibniz Institute for Research and Information in Education</affiliation></author>
      <author><first>Johannes</first><last>Daxenberger</last><affiliation>summetix GmbH</affiliation></author>
      <author><first>Benjamin</first><last>Schiller</last><affiliation>summetix GmbH</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <pages>73-84</pages>
      <abstract>Most tasks in NLP require labeled data. Data labeling is often done on crowdsourcing platforms due to scalability reasons. However, publishing data on public platforms can only be done if no privacy-relevant information is included. Textual data often contains sensitive information like person names or locations. In this work, we investigate how removing personally identifiable information (PII) as well as applying differential privacy (DP) rewriting can enable text with privacy-relevant information to be used for crowdsourcing. We find that DP-rewriting before crowdsourcing can preserve privacy while still leading to good label quality for certain tasks and data. PII-removal led to good label quality in all examined tasks, however, there are no privacy guarantees given.</abstract>
      <url hash="e751e21d">2023.law-1.8</url>
      <bibkey>mouhammad-etal-2023-crowdsourcing</bibkey>
      <doi>10.18653/v1/2023.law-1.8</doi>
    </paper>
    <paper id="9">
      <title>Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned <fixed-case>LLM</fixed-case>s Suggestions</title>
      <author><first>Jana</first><last>Straková</last><affiliation>Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics</affiliation></author>
      <author><first>Eva</first><last>Fučíková</last><affiliation>Charles University</affiliation></author>
      <author><first>Jan</first><last>Hajič</last><affiliation>Charles University</affiliation></author>
      <author><first>Zdeňka</first><last>Urešová</last><affiliation>Charles University</affiliation></author>
      <pages>85-95</pages>
      <abstract>In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several research questions have been focused on, from the investigation of a possible heuristics to provide at least hints to annotators which verbs to include and which are outside the current version of the ontology, to the possible use of the automatic scores to help the annotators to be more efficient in finding a threshold for identifying verbs that cannot be assigned to any existing class and therefore they are to be used as seeds for a new class. We have also carefully examined the correlation of the automatic scores with the human annotation. While the correlation turned out to be strong, its influence on the annotation proper is modest due to its near linearity, even though the mere fact of such pre-annotation leads to relatively short annotation times.</abstract>
      <url hash="f40a9fe9">2023.law-1.9</url>
      <attachment type="data" hash="edb57a39">2023.law-1.9.data.zip</attachment>
      <bibkey>strakova-etal-2023-extending</bibkey>
      <doi>10.18653/v1/2023.law-1.9</doi>
      <video href="2023.law-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Temporal and Second Language Influence on Intra-Annotator Agreement and Stability in Hate Speech Labelling</title>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot Watt University</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last><affiliation>Google</affiliation></author>
      <pages>96-103</pages>
      <abstract>Much work in natural language processing (NLP) relies on human annotation. The majority of this implicitly assumes that annotator’s labels are temporally stable, although the reality is that human judgements are rarely consistent over time. As a subjective annotation task, hate speech labels depend on annotator’s emotional and moral reactions to the language used to convey the message. Studies in Cognitive Science reveal a ‘foreign language effect’, whereby people take differing moral positions and perceive offensive phrases to be weaker in their second languages. Does this affect annotations as well? We conduct an experiment to investigate the impacts of (1) time and (2) different language conditions (English and German) on measurements of intra-annotator agreement in a hate speech labelling task. While we do not observe the expected lower stability in the different language condition, we find that overall agreement is significantly lower than is implicitly assumed in annotation tasks, which has important implications for dataset reproducibility in NLP.</abstract>
      <url hash="8182a261">2023.law-1.10</url>
      <bibkey>abercrombie-etal-2023-temporal</bibkey>
      <doi>10.18653/v1/2023.law-1.10</doi>
      <video href="2023.law-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title><fixed-case>B</fixed-case>en<fixed-case>C</fixed-case>oref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations</title>
      <author><first>Shadman</first><last>Rohan</last><affiliation>North South University</affiliation></author>
      <author><first>Mojammel</first><last>Hossain</last><affiliation>North South University</affiliation></author>
      <author><first>Mohammad</first><last>Rashid</last><affiliation>Jahangirnagar University</affiliation></author>
      <author><first>Nabeel</first><last>Mohammed</last><affiliation>North South University</affiliation></author>
      <pages>104-117</pages>
      <abstract>Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morphological richness compared to English. In this article, we introduce a new dataset, BenCoref, comprising coreference annotations for Bengali texts gathered from four distinct domains. This relatively small dataset contains 5200 mention annotations forming 502 mention clusters within 48,569 tokens. We describe the process of creating this dataset and report performance of multiple models trained using BenCoref. We anticipate that our work sheds some light on the variations in coreference phenomena across multiple domains in Bengali and encourages the development of additional resources for Bengali. Furthermore, we found poor crosslingual performance at zero-shot setting from English, highlighting the need for more language-specific resources for this task.</abstract>
      <url hash="dcb40cf3">2023.law-1.11</url>
      <bibkey>rohan-etal-2023-bencoref</bibkey>
      <doi>10.18653/v1/2023.law-1.11</doi>
    </paper>
    <paper id="12">
      <title>Annotators-in-the-loop: Testing a Novel Annotation Procedure on <fixed-case>I</fixed-case>talian Case Law</title>
      <author><first>Emma</first><last>Zanoli</last><affiliation>IUSS Pavia - School for Advanced Studies</affiliation></author>
      <author><first>Matilde</first><last>Barbini</last><affiliation>IUSS Pavia - School for Advanced Studies</affiliation></author>
      <author><first>Davide</first><last>Riva</last><affiliation>Università degli Studi di Milano, Department of Computer Science</affiliation></author>
      <author><first>Sergio</first><last>Picascia</last><affiliation>Università degli Studi di Milano, Department of Computer Science</affiliation></author>
      <author><first>Emanuela</first><last>Furiosi</last><affiliation>IUSS Pavia - School for Advanced Studies</affiliation></author>
      <author><first>Stefano</first><last>D’Ancona</last><affiliation>IUSS Pavia - School for Advanced Studies</affiliation></author>
      <author><first>Cristiano</first><last>Chesi</last><affiliation>IUSS Pavia - School for Advanced Studies, NETS Lab</affiliation></author>
      <pages>118-128</pages>
      <abstract>The availability of annotated legal corpora is crucial for a number of tasks, such as legal search, legal information retrieval, and predictive justice. Annotation is mostly assumed to be a straightforward task: as long as the annotation scheme is well defined and the guidelines are clear, annotators are expected to agree on the labels. This is not always the case, especially in legal annotation, which can be extremely difficult even for expert annotators. We propose a legal annotation procedure that takes into account annotator certainty and improves it through negotiation. We also collect annotator feedback and show that our approach contributes to a positive annotation environment. Our work invites reflection on often neglected ethical concerns regarding legal annotation.</abstract>
      <url hash="1637f7d6">2023.law-1.12</url>
      <bibkey>zanoli-etal-2023-annotators</bibkey>
      <doi>10.18653/v1/2023.law-1.12</doi>
      <video href="2023.law-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Annotating Decomposition in Time: Three Approaches for Again</title>
      <author><first>Martin</first><last>Kopf</last><affiliation>Saarland University</affiliation></author>
      <author><first>Remus</first><last>Gergel</last><affiliation>Saarland University</affiliation></author>
      <pages>129-135</pages>
      <abstract>This submission reports on a three-part series of original methods geared towards producing semantic annotations for the decompositional marker “again”. The three methods are (i) exhaustive expert annotation based on a comprehensive set of guidelines, (ii) extension of expert annotation by predicting presuppositions with a Multinomial Naïve Bayes classifier in the context of a meta-analysis to optimize feature selection and (iii) quality-controlled crowdsourcing with ensuing evaluation and KMeans clustering of annotation vectors.</abstract>
      <url hash="07f9eea0">2023.law-1.13</url>
      <bibkey>kopf-gergel-2023-annotating</bibkey>
      <doi>10.18653/v1/2023.law-1.13</doi>
    </paper>
    <paper id="14">
      <title>How Good Is the Model in Model-in-the-loop Event Coreference Resolution Annotation?</title>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Abhijnan</first><last>Nath</last><affiliation>Colorado State University</affiliation></author>
      <author><first>Michael</first><last>Regan</last><affiliation>University of Washington</affiliation></author>
      <author><first>Adam</first><last>Pollins</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last><affiliation>Colorado State University</affiliation></author>
      <author><first>James H.</first><last>Martin</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>136-145</pages>
      <abstract>Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests likely corefering event pairs only. We evaluate the effectiveness of this approach by first simulating the annotation process and then, using a novel annotator-centric Recall-Annotation effort trade-off metric, we compare the results of various underlying models and datasets. We finally present a method for obtaining 97% recall while substantially reducing the workload required by a fully manual annotation process.</abstract>
      <url hash="169a3f0d">2023.law-1.14</url>
      <bibkey>ahmed-etal-2023-good</bibkey>
      <doi>10.18653/v1/2023.law-1.14</doi>
      <video href="2023.law-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Pragmatic Annotation of Articles Related to Police Brutality</title>
      <author><first>Tess</first><last>Feyen</last><affiliation>Ecole Normale Supérieure</affiliation></author>
      <author><first>Alda</first><last>Mari</last><affiliation>Ecole Normale Supérieure</affiliation></author>
      <author><first>Paul</first><last>Portner</last><affiliation>Georgetown University</affiliation></author>
      <pages>146-153</pages>
      <abstract>The annotation task we elaborated aims at describing the contextual factors that influence the appearance and interpretation of moral predicates, in newspaper articles on police brutality, in French and in English. The paper provides a brief review of the literature on moral predicates and their relation with context. The paper also describes the elaboration of the corpus and the ontology. Our hypothesis is that the use of moral adjectives and their appearance in context could change depending on the political orientation of the journal. We elaborated an annotation task to investigate the precise contexts discussed in articles on police brutality. The paper concludes by describing the study and the annotation task in details.</abstract>
      <url hash="dc7145b6">2023.law-1.15</url>
      <bibkey>feyen-etal-2023-pragmatic</bibkey>
      <doi>10.18653/v1/2023.law-1.15</doi>
    </paper>
    <paper id="16">
      <title>The <fixed-case>RST</fixed-case> Continuity Corpus</title>
      <author><first>Debopam</first><last>Das</last><affiliation>Åbo Akademi University</affiliation></author>
      <author><first>Markus</first><last>Egg</last><affiliation>Humboldt-Universität zu Berlin</affiliation></author>
      <pages>154-165</pages>
      <abstract>We present the RST Continuity Corpus (RST-CC), a corpus of discourse relations annotated for continuity dimensions. Continuity or discontinuity (maintaining or shifting deictic centres across discourse segments) is an important property of discourse relations, but the two are correlated in greatly varying ways. To analyse this correlation, the relations in the RST-CC are annotated using operationalised versions of Givón’s (1993) continuity dimensions. We also report on the inter-annotator agreement, and discuss recurrent annotation issues. First results show substantial variation of continuity dimensions within and across relation types.</abstract>
      <url hash="7ad59220">2023.law-1.16</url>
      <bibkey>das-egg-2023-rst</bibkey>
      <doi>10.18653/v1/2023.law-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>GENTLE</fixed-case>: A Genre-Diverse Multilayer Challenge Set for <fixed-case>E</fixed-case>nglish <fixed-case>NLP</fixed-case> and Linguistic Evaluation</title>
      <author><first>Tatsuya</first><last>Aoyama</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Shabnam</first><last>Behzad</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Luke</first><last>Gessler</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Lauren</first><last>Levine</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Jessica</first><last>Lin</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Yang Janet</first><last>Liu</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Siyao</first><last>Peng</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Yilun</first><last>Zhu</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Amir</first><last>Zeldes</last><affiliation>Georgetown University</affiliation></author>
      <pages>166-178</pages>
      <abstract>We present GENTLE, a new mixed-genre English challenge corpus totaling 17K tokens and consisting of 8 unusual text types for out-of-domain evaluation: dictionary entries, esports commentaries, legal documents, medical notes, poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually annotated for a variety of popular NLP tasks, including syntactic dependency parsing, entity recognition, coreference resolution, and discourse parsing. We evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for at least some genres in their performance on all tasks, which indicates GENTLE’s utility as an evaluation dataset for NLP systems.</abstract>
      <url hash="980b0883">2023.law-1.17</url>
      <bibkey>aoyama-etal-2023-gentle</bibkey>
      <doi>10.18653/v1/2023.law-1.17</doi>
    </paper>
    <paper id="18">
      <title>A Study on Annotation Interfaces for Summary Comparison</title>
      <author><first>Sian</first><last>Gooding</last><affiliation>Google</affiliation></author>
      <author><first>Lucas</first><last>Werner</last><affiliation>Google</affiliation></author>
      <author><first>Victor</first><last>Cărbune</last><affiliation>Google</affiliation></author>
      <pages>179-187</pages>
      <abstract>The task of summarisation is notoriously difficult to evaluate, with agreement even between expert raters unlikely to be perfect. One technique for summary evaluation relies on collecting comparison data by presenting annotators with generated summaries and tasking them with selecting the best one. This paradigm is currently being exploited in reinforcement learning using human feedback, whereby a reward function is trained using pairwise choice data. Comparisons are an easier way to elicit human feedback for summarisation, however, such decisions can be bottle necked by the usability of the annotator interface. In this paper, we present the results of a pilot study exploring how the user interface impacts annotator agreement when judging summary quality.</abstract>
      <url hash="b7cd83c8">2023.law-1.18</url>
      <bibkey>gooding-etal-2023-study</bibkey>
      <doi>10.18653/v1/2023.law-1.18</doi>
    </paper>
    <paper id="19">
      <title>A Question Answering Benchmark Database for <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Attila</first><last>Novák</last><affiliation>Pázmány Péter Catholic University, Faculty of Information Technology and Bionics</affiliation></author>
      <author><first>Borbála</first><last>Novák</last><affiliation>Pázmány Péter Catholic University, Faculty of Information Technology and Bionics</affiliation></author>
      <author><first>Tamás</first><last>Zombori</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Gergő</first><last>Szabó</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Zsolt</first><last>Szántó</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Richárd</first><last>Farkas</last><affiliation>University of Szeged</affiliation></author>
      <pages>188-198</pages>
      <abstract>Within the research presented in this article, we created a new question answering benchmark database for Hungarian called MILQA. When creating the dataset, we basically followed the principles of the English SQuAD 2.0, however, like in some more recent English question answering datasets, we introduced a number of innovations beyond SQuAD: e.g., yes/no-questions, list-like answers consisting of several text spans, long answers, questions requiring calculation and other question types where you cannot simply copy the answer from the text. For all these non-extractive question types, the pragmatically adequate form of the answer was also added to make the training of generative models possible. We implemented and evaluated a set of baseline retrieval and answer span extraction models on the dataset. BM25 performed better than any vector-based solution for retrieval. Cross-lingual transfer from English significantly improved span extraction models.</abstract>
      <url hash="9afffe3f">2023.law-1.19</url>
      <bibkey>novak-etal-2023-question</bibkey>
      <doi>10.18653/v1/2023.law-1.19</doi>
    </paper>
    <paper id="20">
      <title>No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference</title>
      <author><first>Animesh</first><last>Nighojkar</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Antonio</first><last>Laverghetta Jr.</last><affiliation>University of South Florida</affiliation></author>
      <author><first>John</first><last>Licato</last><affiliation>University of South Florida</affiliation></author>
      <pages>199-210</pages>
      <abstract>Natural Language Inference (NLI) has been a cornerstone task in evaluating language models’ inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models’ ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP community.</abstract>
      <url hash="7c62ec2c">2023.law-1.20</url>
      <bibkey>nighojkar-etal-2023-strong</bibkey>
      <doi>10.18653/v1/2023.law-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>UMR</fixed-case>-Writer 2.0: Incorporating a New Keyboard Interface and Workflow into <fixed-case>UMR</fixed-case>-Writer</title>
      <author><first>Sijia</first><last>Ge</last><affiliation>University of Colorado-Boulder</affiliation></author>
      <author><first>Jin</first><last>Zhao</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Kristin</first><last>Wright-bettner</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Skatje</first><last>Myers</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Nianwen</first><last>Xue</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado</affiliation></author>
      <pages>211-219</pages>
      <abstract>UMR-Writer is a web-based tool for annotating semantic graphs with the Uniform Meaning Representation (UMR) scheme. UMR is a graph-based semantic representation that can be applied cross-linguistically for deep semantic analysis of texts. In this work, we implemented a new keyboard interface in UMR-Writer 2.0, which is a powerful addition to the original mouse interface, supporting faster annotation for more experienced annotators. The new interface also addresses issues with the original mouse interface. Additionally, we demonstrate an efficient workflow for annotation project management in UMR-Writer 2.0, which has been applied to many projects.</abstract>
      <url hash="293fa2b5">2023.law-1.21</url>
      <bibkey>ge-etal-2023-umr</bibkey>
      <doi>10.18653/v1/2023.law-1.21</doi>
    </paper>
    <paper id="22">
      <title>Unified Syntactic Annotation of <fixed-case>E</fixed-case>nglish in the <fixed-case>CGEL</fixed-case> Framework</title>
      <author><first>Brett</first><last>Reynolds</last><affiliation>Humber College</affiliation></author>
      <author><first>Aryaman</first><last>Arora</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Nathan</first><last>Schneider</last><affiliation>Georgetown University</affiliation></author>
      <pages>220-234</pages>
      <abstract>We investigate whether the Cambridge Grammar of the English Language (2002) and its extensive descriptions work well as a corpus annotation scheme. We develop annotation guidelines and in the process outline some interesting linguistic uncertainties that we had to resolve. To test the applicability of CGEL to real-world corpora, we conduct an interannotator study on sentences from the English Web Treebank, showing that consistent annotation of even complex syntactic phenomena like gapping using the CGEL formalism is feasible. Why introduce yet another formalism for English syntax? We argue that CGEL is attractive due to its exhaustive analysis of English syntactic phenomena, its labeling of both constituents and functions, and its accessibility. We look towards expanding CGELBank and augmenting it with automatic conversions from existing treebanks in the future.</abstract>
      <url hash="b19a0ced">2023.law-1.22</url>
      <bibkey>reynolds-etal-2023-unified</bibkey>
      <doi>10.18653/v1/2023.law-1.22</doi>
    </paper>
    <paper id="23">
      <title>Annotating Discursive Roles of Sentences in Patent Descriptions</title>
      <author><first>Lufei</first><last>Liu</last><affiliation>Qatent</affiliation></author>
      <author><first>Xu</first><last>Sun</last><affiliation>Qatent</affiliation></author>
      <author><first>François</first><last>Veltz</last><affiliation>Qatent</affiliation></author>
      <author><first>Kim</first><last>Gerdes</last><affiliation>Qatent</affiliation></author>
      <pages>235-243</pages>
      <abstract>Patent descriptions are a crucial component of patent applications, as they are key to understanding the invention and play a significant role in securing patent grants. While discursive analyses have been undertaken for scientific articles, they have not been as thoroughly explored for patent descriptions, despite the increasing importance of Intellectual Property and the constant rise of the number of patent applications. In this study, we propose an annotation scheme containing 16 classes that allows categorizing each sentence in patent descriptions according to their discursive roles. We publish an experimental human-annotated corpus of 16 patent descriptions and analyze challenges that may be encountered in such work. This work can be base for an automated annotation and thus contribute to enriching linguistic resources in the patent domain.</abstract>
      <url hash="f1afedff">2023.law-1.23</url>
      <bibkey>liu-etal-2023-annotating</bibkey>
      <doi>10.18653/v1/2023.law-1.23</doi>
    </paper>
    <paper id="24">
      <title>The Effect of Alignment Correction on Cross-Lingual Annotation Projection</title>
      <author><first>Shabnam</first><last>Behzad</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Seth</first><last>Ebner</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Marc</first><last>Marone</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University / Microsoft</affiliation></author>
      <author><first>Mahsa</first><last>Yarmohammadi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>244-251</pages>
      <abstract>Cross-lingual annotation projection is a practical method for improving performance on low resource structured prediction tasks. An important step in annotation projection is obtaining alignments between the source and target texts, which enables the mapping of annotations across the texts. By manually correcting automatically generated alignments, we examine the impact of alignment quality—automatic, manual, and mixed—on downstream performance for two information extraction tasks and quantify the trade-off between annotation effort and model performance.</abstract>
      <url hash="5748752f">2023.law-1.24</url>
      <bibkey>behzad-etal-2023-effect</bibkey>
      <doi>10.18653/v1/2023.law-1.24</doi>
    </paper>
    <paper id="25">
      <title>When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the <fixed-case>POPQUORN</fixed-case> Dataset</title>
      <author><first>Jiaxin</first><last>Pei</last><affiliation>University of Michigan</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan</affiliation></author>
      <pages>252-265</pages>
      <abstract>Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the Potato-Prolific dataset for Question-Answering, Offensiveness, text Rewriting and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators’ background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and annotation interface are available at <url>https://github.com/Jiaxin-Pei/potato-prolific-dataset</url>.</abstract>
      <url hash="7bc196b8">2023.law-1.25</url>
      <bibkey>pei-jurgens-2023-annotator</bibkey>
      <doi>10.18653/v1/2023.law-1.25</doi>
      <revision id="1" href="2023.law-1.25v1" hash="502ad7f7"/>
      <revision id="2" href="2023.law-1.25v2" hash="7bc196b8" date="2023-09-04">This revision added three new references and updated the funding source.</revision>
    </paper>
    <paper id="26">
      <title>Enriching the <fixed-case>NA</fixed-case>rabizi Treebank: A Multifaceted Approach to Supporting an Under-Resourced Language</title>
      <author><first>Arij</first><last>Riabi</last><affiliation>Inria; Sorbonne Université</affiliation></author>
      <author><first>Menel</first><last>Mahamdi</last><affiliation>Inria Paris</affiliation></author>
      <author><first>Djamé</first><last>Seddah</last><affiliation>Inria</affiliation></author>
      <pages>266-278</pages>
      <abstract>In this paper we address the scarcity of annotated data for NArabizi, a Romanized form of North African Arabic used mostly on social media, which poses challenges for Natural Language Processing (NLP). We introduce an enriched version of NArabizi Treebank (Seddah et al., 2020) with three main contributions: the addition of two novel annotation layers (named entity recognition and offensive language detection) and a re-annotation of the tokenization, morpho-syntactic and syntactic layers that ensure annotation consistency. Our experimental results, using different tokenization schemes, showcase the value of our contributions and highlight the impact of working with non-gold tokenization for NER and dependency parsing. To facilitate future research, we make these annotations publicly available. Our enhanced NArabizi Treebank paves the way for creating sophisticated language models and NLP tools for this under-represented language.</abstract>
      <url hash="6fa9eadc">2023.law-1.26</url>
      <bibkey>riabi-etal-2023-enriching</bibkey>
      <doi>10.18653/v1/2023.law-1.26</doi>
      <video href="2023.law-1.26.mp4"/>
    </paper>
  </volume>
</collection>
