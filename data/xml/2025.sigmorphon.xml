<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.sigmorphon">
  <volume id="main" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the The 22nd SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics</booktitle>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Eleanor</first><last>Chodroff</last></editor>
      <editor><first>Frederic</first><last>Mailhot</last></editor>
      <editor><first>Çağrı</first><last>Çöltekin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico, USA</address>
      <month>May</month>
      <year>2025</year>
      <url hash="2697d13e">2025.sigmorphon-main</url>
      <venue>sigmorphon</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-231-2</isbn>
    </meta>
    <frontmatter>
      <url hash="a72b671e">2025.sigmorphon-main.0</url>
      <bibkey>sigmorphon-ws-2025-main</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Prompt and circumstance”:" A word-by-word <fixed-case>LLM</fixed-case> prompting approach to interlinear glossing for low-resource languages</title>
      <author><first>Micha</first><last>Elsner</last></author>
      <author><first>David</first><last>Liu</last></author>
      <pages>1-14</pages>
      <abstract>This paper presents VeLePa, an inflected verbal lexicon of Central Pame (pbs, cent2154), an Otomanguean language from Mexico. This resource contains 12528 words in phonological form representing the complete inflectional paradigms of 216 verbs, supplemented with use frequencies. Computer-operable (CLDF) inflected lexicons of non-WEIRD underresourced languages are urgently needed to expand digital capacities in this languages (e.g. in NLP). VeLePa contributes to this, and does so with data from a language which is morphologically extraordinary, with unusually high levels of irregularity and multiple conjugations at various loci within the word”:" prefixes, stems, tone, and suffixes constitute different albeit interrelated subsystems of inflection. Partly automated creation of interlinear glossed text (IGT) has the potential to assist in linguistic documentation. We argue that LLMs can make this process more accessible to linguists because of their capacity to follow natural-language instructions. We investigate the effectiveness of a retrieval-based LLM prompting approach to glossing, applied to the seven languages from the SIGMORPHON 2023 shared task. Our system beats the BERTbased shared task baseline for every language in the morpheme-level score category, and we show that a simple 3-best oracle has higher word-level scores than the challenge winner (a tuned sequence model) in five languages. In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature. Our results thus demonstrate the potential contributions which LLMs can make in interactive systems for glossing, both in making suggestions to human annotators and following directions.</abstract>
      <url hash="64c34645">2025.sigmorphon-main.1</url>
      <bibkey>elsner-liu-2025-prompt</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>W</fixed-case>est <fixed-case>G</fixed-case>ermanic noun-noun compounds and the morphology-syntax trade-off</title>
      <author><first>Pablo</first><last>Mosteiro</last></author>
      <author><first>Damián</first><last>Blasi</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <pages>15-22</pages>
      <abstract>This paper examines the linguistic distinction between syntax and morphology, focusing on noun-noun compounds in three West Germanic languages (English, Dutch, and German). Previous studies using the Parallel Bible Corpus have found a trade-off between word order (syntax) and word structure (morphology), with languages optimizing information conveyance through these systems. Our research question is whether manipulating English noun-noun compounds to resemble Dutch and German constructions can reproduce the observed distance between these languages in the order-structure plane. We extend a word-pasting procedure to merge increasingly common noun-noun pairs in English Bible translations. After each merge, we estimate the information contained in word order and word structure using entropy calculations. Our results show that pasting noun-noun pairs reduces the difference between English and the other languages, suggesting that orthographic conventions defining word boundaries play a role in this distinction. However, the effect is not pronounced, and results are statistically inconclusive.</abstract>
      <url hash="074f0b74">2025.sigmorphon-main.2</url>
      <bibkey>mosteiro-etal-2025-west</bibkey>
    </paper>
    <paper id="3">
      <title>The Impact of Dialect Variation on Robust Automatic Speech Recognition for <fixed-case>C</fixed-case>atalan</title>
      <author><first>Zachary</first><last>Hopton</last></author>
      <author><first>Eleanor</first><last>Chodroff</last></author>
      <pages>23-33</pages>
      <abstract>To accurately transcribe a speech signal, automatic speech recognition (ASR) systems must show robustness to a wide range of task independent variation, such as speaker factors, recording quality, or even ädversarial noisedesigned to disrupt performance.We manipulated the dialect composition of fine-tuning data for ASR to study whether balancing the relative proportion of dialects had an impact on models robustness to two such sources of variation”:" dialect variation and adversarial perturbations. We fine-tuned XLSR-53 for Catalan ASR using four different dialect compositions, each containing the Central Catalan dialect. These were defined as 100%, 80%, 50%, and 20% Central Catalan, with the remaining portions split evenly between four other Catalan dialects. While increasing the relative proportion of dialect variants improved models’ dialect robustness, this did not have a meaningful impact on adversarial robustness. These findings suggest that while improvements to ASR can be made by diversifying the training data, such changes do not sufficiently counteract adversarial attacks, leaving the technology open to security threats.</abstract>
      <url hash="bab59ab1">2025.sigmorphon-main.3</url>
      <bibkey>hopton-chodroff-2025-impact</bibkey>
    </paper>
    <paper id="4">
      <title>Probing Neural Network Generalization using Default Patterns</title>
      <author><first>Brandon</first><last>Prickett</last></author>
      <author><first>Tianyi</first><last>Nyu</last></author>
      <author><first>Katya</first><last>Pertsova</last></author>
      <pages>34-44</pages>
      <abstract>Whether neural-net models can learn minoritydefault patterns has been a matter of some controversy. Results based on modeling real human language data are hard to interpret due to complexity. Therefore, we examine the learning of a simple artificial language pattern involving defaults using three computational models”:" an Encoder-Decoder RNN, a Transformer Encoder, and a Logistic Regression. Overall, we find that the models have the hardest time with minority defaults, but can eventually learn them and apply them to novel words (although not always extend them to completely novel segments or novel CV-sequences). Typefrequency has the largest effect on learning in all models, trumping the effect of distribution. We examine the weights of two models to provide further insights into how defaults are represented inside the models.</abstract>
      <url hash="502acea8">2025.sigmorphon-main.4</url>
      <bibkey>prickett-etal-2025-probing</bibkey>
    </paper>
  </volume>
</collection>
