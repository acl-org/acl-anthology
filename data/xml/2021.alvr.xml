<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.alvr">
  <volume id="1" ingest-date="2021-06-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Advances in Language and Vision Research</booktitle>
      <editor><first/><last>Xin</last></editor>
      <editor><first>Ronghang</first><last>Hu</last></editor>
      <editor><first>Drew</first><last>Hudson</last></editor>
      <editor><first>Tsu-Jui</first><last>Fu</last></editor>
      <editor><first>Marcus</first><last>Rohrbach</last></editor>
      <editor><first>Daniel</first><last>Fried</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="95670c66">2021.alvr-1</url>
      <venue>alvr</venue>
    </meta>
    <frontmatter>
      <url hash="eef9d62d">2021.alvr-1.0</url>
      <bibkey>alvr-2021-advances</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Feature-level Incongruence Reduction for Multimodal Translation</title>
      <author><first>Zhifeng</first><last>Li</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Yuchen</first><last>Pan</last></author>
      <author><first>Jian</first><last>Tang</last></author>
      <author><first>Jianmin</first><last>Yao</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>1–10</pages>
      <abstract>Caption translation aims to translate image annotations (captions for short). Recently, Multimodal Neural Machine Translation (MNMT) has been explored as the essential solution. Besides of linguistic features in captions, MNMT allows visual(image) features to be used. The integration of multimodal features reinforces the semantic representation and considerably improves translation performance. However, MNMT suffers from the incongruence between visual and linguistic features. To overcome the problem, we propose to extend MNMT architecture with a harmonization network, which harmonizes multimodal features(linguistic and visual features)by unidirectional modal space conversion. It enables multimodal translation to be carried out in a seemingly monomodal translation pipeline. We experiment on the golden Multi30k-16 and 17. Experimental results show that, compared to the baseline,the proposed method yields the improvements of 2.2% BLEU for the scenario of translating English captions into German (En→De) at best,7.6% for the case of English-to-French translation(En→Fr) and 1.5% for English-to-Czech(En→Cz). The utilization of harmonization network leads to the competitive performance to the-state-of-the-art.</abstract>
      <url hash="f36f2fc8">2021.alvr-1.1</url>
      <doi>10.18653/v1/2021.alvr-1.1</doi>
      <bibkey>li-etal-2021-feature</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi30k">Multi30K</pwcdataset>
    </paper>
    <paper id="2">
      <title>Error Causal inference for Multi-Fusion models</title>
      <author><first>Chengxi</first><last>Li</last></author>
      <author><first>Brent</first><last>Harrison</last></author>
      <pages>11–15</pages>
      <abstract>In this paper, we propose an error causal inference method that could be used for finding dominant features for a faulty instance under a well-trained multi-modality input model, which could apply to any testing instance. We evaluate our method using a well-trained multi-modalities stylish caption generation model and find those causal inferences that could provide us the insights for next step optimization.</abstract>
      <url hash="c7e38187">2021.alvr-1.2</url>
      <doi>10.18653/v1/2021.alvr-1.2</doi>
      <bibkey>li-harrison-2021-error</bibkey>
    </paper>
    <paper id="3">
      <title>Leveraging Partial Dependency Trees to Control Image Captions</title>
      <author><first>Wenjie</first><last>Zhong</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>16–21</pages>
      <abstract>Controlling the generation of image captions attracts lots of attention recently. In this paper, we propose a framework leveraging partial syntactic dependency trees as control signals to make image captions include specified words and their syntactic structures. To achieve this purpose, we propose a Syntactic Dependency Structure Aware Model (SDSAM), which explicitly learns to generate the syntactic structures of image captions to include given partial dependency trees. In addition, we come up with a metric to evaluate how many specified words and their syntactic dependencies are included in generated captions. We carry out experiments on two standard datasets: Microsoft COCO and Flickr30k. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. The code is available on GitHub.</abstract>
      <url hash="c9112d4c">2021.alvr-1.3</url>
      <doi>10.18653/v1/2021.alvr-1.3</doi>
      <bibkey>zhong-miyao-2021-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="4">
      <title>Grounding Plural Phrases: Countering Evaluation Biases by Individuation</title>
      <author><first>Julia</first><last>Suter</last></author>
      <author><first>Letitia</first><last>Parcalabescu</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>22–28</pages>
      <abstract>Phrase grounding (PG) is a multimodal task that grounds language in images. PG systems are evaluated on well-known benchmarks, using Intersection over Union (IoU) as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel metric, c-IoU, based on a union box’s component boxes. We experimentally show that our new metric greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks.</abstract>
      <url hash="a92f2c3a">2021.alvr-1.4</url>
      <doi>10.18653/v1/2021.alvr-1.4</doi>
      <bibkey>suter-etal-2021-grounding</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>P</fixed-case>an<fixed-case>GEA</fixed-case>: The Panoramic Graph Environment Annotation Toolkit</title>
      <author><first>Alexander</first><last>Ku</last></author>
      <author><first>Peter</first><last>Anderson</last></author>
      <author><first>Jordi</first><last>Pont Tuset</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <pages>29–33</pages>
      <abstract>PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks – collecting navigation instructions and navigation instruction following – and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support.</abstract>
      <url hash="27d10761">2021.alvr-1.5</url>
      <doi>10.18653/v1/2021.alvr-1.5</doi>
      <bibkey>ku-etal-2021-pangea</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/matterport3d">Matterport3D</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="6">
      <title>Learning to Learn Semantic Factors in Heterogeneous Image Classification</title>
      <author><first>Boyue</first><last>Fan</last></author>
      <author><first>Zhenting</first><last>Liu</last></author>
      <pages>34–38</pages>
      <abstract>Few-shot learning is to recognize novel classes with a few labeled samples per class. Although numerous meta-learning methods have made significant progress, they struggle to directly address the heterogeneity of training and evaluating task distributions, resulting in the domain shift problem when transitioning to new tasks with disjoint spaces. In this paper, we propose a novel method to deal with the heterogeneity. Specifically, by simulating class-difference domain shift during the meta-train phase, a bilevel optimization procedure is applied to learn a transferable representation space that can rapidly adapt to heterogeneous tasks. Experiments demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="0f741cf6">2021.alvr-1.6</url>
      <doi>10.18653/v1/2021.alvr-1.6</doi>
      <bibkey>fan-liu-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cub-200-2011">CUB-200-2011</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mini-imagenet">mini-Imagenet</pwcdataset>
    </paper>
    <paper id="7">
      <title>Reference and coreference in situated dialogue</title>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>39–44</pages>
      <abstract>In recent years several corpora have been developed for vision and language tasks. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. Working with two corpora approaching this goal, we present a linguistic perspective on some of the challenges in creating and extending resources combining language and vision while preserving continuity with the existing best practices in the area of coreference annotation.</abstract>
      <url hash="0ea1eb50">2021.alvr-1.7</url>
      <doi>10.18653/v1/2021.alvr-1.7</doi>
      <bibkey>loaiciga-etal-2021-reference</bibkey>
    </paper>
  </volume>
</collection>
