<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.mmloso">
  <volume id="1" ingest-date="2026-01-13" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo 2025)</booktitle>
      <editor><first>Ankita</first><last>Shukla</last></editor>
      <editor><first>Sandeep</first><last>Kumar</last></editor>
      <editor><first>Amrit Singh</first><last>Bedi</last></editor>
      <editor><first>Tanmoy</first><last>Chakraborty</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mumbai, India</address>
      <month>December</month>
      <year>2025</year>
      <url hash="8b8f6463">2025.mmloso-1</url>
      <venue>mmloso</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-311-1</isbn>
    </meta>
    <frontmatter>
      <url hash="7c781585">2025.mmloso-1.0</url>
      <bibkey>mmloso-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols</title>
      <author><first>Sebastian</first><last>Padó</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author orcid="0009-0001-8893-1517"><first>Kerstin</first><last>Thomas</last><affiliation>Universität Stuttgart</affiliation></author>
      <pages>1-12</pages>
      <abstract>Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.</abstract>
      <url hash="f73bd173">2025.mmloso-1.1</url>
      <bibkey>pado-thomas-2025-artwork</bibkey>
    </paper>
    <paper id="2">
      <title>Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach</title>
      <author><first>Haruki</first><last>Sakajo</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hiroshi</first><last>Takato</last><affiliation>.STFY</affiliation></author>
      <author><first>Hiroshi</first><last>Tsutsui</last></author>
      <author><first>Komei</first><last>Soda</last><affiliation>.STFY</affiliation></author>
      <author orcid="0000-0002-5249-5813"><first>Hidetaka</first><last>Kamigaito</last><affiliation>CyberAgent, Inc. and Nara Institute of Science and Technology</affiliation></author>
      <author orcid="0000-0001-8349-3522"><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>13-24</pages>
      <abstract>Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.</abstract>
      <url hash="3c794c37">2025.mmloso-1.2</url>
      <bibkey>sakajo-etal-2025-toward</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>TR</fixed-case>ep<fixed-case>L</fixed-case>i<fixed-case>N</fixed-case>a: Layer-wise <fixed-case>CKA</fixed-case>+<fixed-case>REPINA</fixed-case> Alignment Improves Low-Resource Machine Translation in Aya-23 8<fixed-case>B</fixed-case></title>
      <author><first>Toshiki</first><last>Nakai</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Ravikiran</first><last>Chikkala</last></author>
      <author><first>Lena</first><last>Oberkircher</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author orcid="0009-0004-3731-940X"><first>Nicholas</first><last>Jennings</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author orcid="0000-0002-1752-3452"><first>Natalia</first><last>Skachkova</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Tatiana</first><last>Anikina</last><affiliation>German Research Center for AI</affiliation></author>
      <author orcid="0000-0001-8843-5460"><first>Jesujoba Oluwadara</first><last>Alabi</last></author>
      <pages>25-34</pages>
      <abstract>The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India’s most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings. Upon acceptance of the paper, we make our code public.</abstract>
      <url hash="297878e1">2025.mmloso-1.3</url>
      <bibkey>nakai-etal-2025-treplina</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>M</fixed-case>eme<fixed-case>G</fixed-case>uard: Transformer-Based Fusion for Multimodal Propaganda Detection in Low-Resource Social Media Memes</title>
      <author><first>Md.</first><last>Mohiuddin</last></author>
      <author><first>Kawsar</first><last>Ahmed</last></author>
      <author orcid="0009-0003-9940-9681"><first>Shawly</first><last>Ahsan</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author orcid="0000-0001-8806-708X"><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>35-47</pages>
      <abstract>Memes are now a common means of communication on social media. Their humor and short format help messages spread quickly and easily. Propagandistic memes use both words and images to influence opinions and behaviors, often appealing to emotions or ideologies. While propaganda detection has been well-studied in high-resource languages (HRLs), there has been a limited focus on low-resource languages (LRLs), such as Bengali. In this study, we introduce MemeGuard, a new dataset of 3,745 memes for detecting propaganda in Bengali. We tested more than 45 different methods, including both single and combined approaches with fusion. For text, BanglaBERT-1 achieved the best macro F1 score of 80.34%, whereas the CLIP vision transformer scored 78.94% for images. The proposed multimodal model, which combines BanglaBERT-2 and CLIP using Adaptive Modality Fusion, achieved the highest macro F1 score of 85.36%. This work establishes a strong baseline and offers valuable insights for future research in Bengali multimodal content analysis.</abstract>
      <url hash="a82cbc11">2025.mmloso-1.4</url>
      <bibkey>mohiuddin-etal-2025-memeguard</bibkey>
    </paper>
    <paper id="5">
      <title>Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity</title>
      <author><first>Ming Ze</first><last>Tang</last></author>
      <author><first>Jubal Chandy</first><last>Jacob</last></author>
      <pages>48-57</pages>
      <abstract>This paper investigates how the specificity of natural language prompts influences zero-shot classification performance in modern vision language models (VLMs) under severe data scarcity. Using a curated 285 image subset of MS COCO containing three everyday postures (sitting, standing, and walking/running), we evaluate OpenCLIP, MetaCLIP2, and SigLIP alongside unimodal and pose-based baselines. We introduce a three tier prompt design, minimal labels, action cues, and compact geometric descriptions and systematically vary only the linguistic detail. Our results reveal a counterintuitive trend where simpler prompts consistently outperform more detailed ones, a phenomenon we term prompt overfitting. Grad-CAM attribution further shows that prompt specificity shifts attention between contextual and pose-relevant regions, explaining the model dependent behaviour. The study provides a controlled analysis of prompt granularity in low resource image based posture recognition, highlights the need for careful prompt design when labels are scarce.</abstract>
      <url hash="4df75c3d">2025.mmloso-1.5</url>
      <bibkey>tang-jacob-2025-language</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>B</fixed-case>engali<fixed-case>F</fixed-case>ig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in <fixed-case>B</fixed-case>engali</title>
      <author><first>Abdullah</first><last>Al Sefat</last></author>
      <pages>58-77</pages>
      <abstract>Large language models excel on broad multilingual benchmarks but remains to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource context. We present **BengaliFig**, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple–choice format through a constraint–aware, AI–assisted pipeline. We evaluate eight frontier LLMs from major providers under zero–shot and few–shot chain–of–thought prompting revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation. Data and evaluation code is available at https://github.com/chaoSefat/Bengali-Fig</abstract>
      <url hash="b0ead699">2025.mmloso-1.6</url>
      <bibkey>al-sefat-2025-bengalifig</bibkey>
    </paper>
    <paper id="7">
      <title>Domain-Specific Adaptation for <fixed-case>ASR</fixed-case> through Text-Only Fine-Tuning</title>
      <author><first>Betty</first><last>Kurian</last></author>
      <author><first>Abhinav</first><last>Upadhyay</last><affiliation>Accenture</affiliation></author>
      <author><first>Abhijeet</first><last>Sengupta</last></author>
      <pages>78-85</pages>
      <abstract>Speech recognition models often struggle in specialized domains due to the lack of domain-specific paired audio-text data, making it difficult to adapt general-purpose systems to unique terminology and linguistic patterns. In this work, we propose a text-only domain adaptation method for Whisper, fine-tuning only the decoder using domain-relevant text. Our approach introduces trainable cross-attention bias embeddings, extended with a gated mixture-of-experts routing mechanism, enabling the model to encode domain-specific linguistic priors without any audio data. Unlike ASR adaptation methods that require paired audio-text datasets, our approach is lightweight and resource-efficient. We observe up to a 56% relative improvement in word error rate over the baseline. Our findings demonstrate that text-only adaptation is a practical and effective strategy for improving speech recognition in specialized domains with limited or no domain-specific audio.</abstract>
      <url hash="bb61cfb9">2025.mmloso-1.7</url>
      <bibkey>kurian-etal-2025-domain</bibkey>
    </paper>
    <paper id="8">
      <title>Towards Blind and Low-Vision Accessibility of Lightweight <fixed-case>VLM</fixed-case>s and Custom <fixed-case>LLM</fixed-case>-Evals</title>
      <author><first>Shruti Singh</first><last>Baghel</last></author>
      <author><first>Yash Pratap Singh</first><last>Rathore</last></author>
      <author><first>Anurag</first><last>Pradhan</last></author>
      <author><first>Sushovan</first><last>Jena</last></author>
      <author><first>Arnav</first><last>Bhavsar</last><affiliation>IIT Mandi</affiliation></author>
      <author orcid="0000-0002-0537-5851"><first>Amit</first><last>Shukla</last><affiliation>IIT Mandi</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>86-94</pages>
      <abstract>Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.</abstract>
      <url hash="51e09a71">2025.mmloso-1.8</url>
      <bibkey>baghel-etal-2025-towards</bibkey>
    </paper>
    <paper id="9">
      <title>Evaluating <fixed-case>I</fixed-case>ndic<fixed-case>T</fixed-case>rans2 and <fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5 for <fixed-case>E</fixed-case>nglish–<fixed-case>S</fixed-case>antali Machine Translation Using the Ol Chiki Script</title>
      <author orcid="0009-0009-9275-788X"><first>Kshetrimayum Boynao</first><last>Singh</last><affiliation>National Institute of Technology Silchar,</affiliation></author>
      <author orcid="0000-0003-3612-8834"><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <author orcid="0000-0003-3834-5154"><first>Partha</first><last>Pakray</last><affiliation>National Institute of Technology Silchar</affiliation></author>
      <pages>95-100</pages>
      <abstract>In this study, we examine and evaluate two multilingual NMT models, IndicTrans2 and ByT5, for English-Santali bidirectional translation using the Ol Chiki script. The models are trained on the MMLoSo Shared Task dataset, supplemented with public English-Santali resources, and evaluated on the AI4Bharat IN22 and Flores test sets, specifically IN22-Gen and Flores200-dev. IndicTrans2 finetune strongly outperforms ByT5 across both directions. On IN22-Gen, it achieves 26.8 BLEU and 53.9 chrF++ for Santali→English and 7.3 BLEU and 40.3 chrF++ for English→Santali, compared to ByT5’s 5.6 BLEU and 30.2 chrF++ for Santali→English and 2.9 BLEU and 32.6 chrF++ for English→Santali. On the Flores test set, IndicTrans2 finetune achieves 22 BLEU, 49.2 chrF++, and 4.7 BLEU, 32.7 chrF++. Again, it surpasses ByT5. While ByT5’s bytelevel modelling is script-agnostic, it struggles with Santali morphology. IndicTrans2 benefits from multilingual pre-training and script unification.</abstract>
      <url hash="42aa4a9d">2025.mmloso-1.9</url>
      <bibkey>singh-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="10">
      <title>Challenge Track: <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>s in All Directions: Directional Adapters and Noisy-Channel Reranking for <fixed-case>I</fixed-case>ndic <fixed-case>MT</fixed-case></title>
      <author><first>Sajay</first><last>Raj</last></author>
      <pages>101-105</pages>
      <abstract>Low-resource machine translation for Indic languages remains challenging, especially when high-resource languages such as Hindi and English must be translated to and from very low-resource, grammatically rich languages like Bhili, Mundari, Santali, and Gondi.We describe our winning system for the MMLoSo 2025 Shared Task in this setting. We start from a strong pretrained Indic MT backbone, IndicTrans2, and fine-tune it jointly on all translation directions, pushing the model close to memorization under strict data constraints. On top of this backbone, we add direction-specific low-rank adapters (LoRA) that allow each language pair to specialize while still sharing most parameters. At inference time, we further couple these directional adapters through a noisy-channel objective, in which forward and reverse models jointly score a set of candidate translations, encouraging outputs that are both fluent in the target language and informative about the source. This combination of shared pretraining, directional parameter-efficient adaptation, and noisy-channel reranking substantially improves over a strong fine-tuned baseline and achieves the top overall score on the shared-task leaderboard. We release our codebase at https://github.com/SajayR/LoRA-in-All-Directions</abstract>
      <url hash="c8ac6e62">2025.mmloso-1.10</url>
      <bibkey>raj-2025-challenge</bibkey>
    </paper>
    <paper id="11">
      <title>Challenge Track: Breaking Language Barriers: Adapting <fixed-case>NLLB</fixed-case>-200 and m<fixed-case>BART</fixed-case> for Bhilli, <fixed-case>G</fixed-case>ondi, <fixed-case>M</fixed-case>undari, and <fixed-case>S</fixed-case>antali Without Source Language Proficiency</title>
      <author><first>Paul</first><last>Kamau</last></author>
      <pages>106-108</pages>
      <abstract>This paper presents a language-agnostic approach to neural machine translation for low-resource Indian tribal languages: Bhilli, Gondi, Mundari, and Santali. Developed under the constraint of zero proficiency in the source languages, the methodology relies on the cross-lingual transfer capabilities of two foundation models, NLLB-200 and mBART-50. The approach employs a unified bidirectional fine-tuning strategy to maximize limited parallel corpora. A primary contribution of this work is a smart post-processing pipeline and a “conservative ensemble” mechanism. This mechanism integrates predictions from a secondary model specifically as a safety net to mitigate hallucinations and length-ratio artifacts generated by the primary model. The approach achieved a private leaderboard score of 179.49 in the MMLoSo 2025 Language Challenge. These findings demonstrate that effective translation systems for underrepresented languages can be engineered without native linguistic intuition by leveraging data-centric validation and the latent knowledge within massive multilingual models</abstract>
      <url hash="460f7993">2025.mmloso-1.11</url>
      <bibkey>kamau-2025-challenge</bibkey>
    </paper>
    <paper id="12">
      <title>Challenge Track: Divide and Translate: Parameter Isolation with Encoder Freezing for Low-Resource <fixed-case>I</fixed-case>ndic <fixed-case>NMT</fixed-case></title>
      <author><first>Vaibhav</first><last>Kanojia</last></author>
      <pages>109-113</pages>
      <abstract>We present a direction-specialized neural machine translation framework for ultra-low-resource Indic and tribal languages, including Bhili, Gondi, Mundari, and Santali. Using the NLLB-600M backbone, we freeze the multilingual encoder and fine-tune direction-specific decoders to reduce negative transfer and improve morphological fidelity under severe data scarcity. Our system is trained with leakage-safe splits, bitext reversal augmentation, and memory-efficient mixed-precision optimization. On the official MMLoSo 2025 Kaggle benchmark, we achieve a public score of 171.4 and a private score of 161.1, demonstrating stable generalization in highly noisy low-resource conditions.</abstract>
      <url hash="9f666573">2025.mmloso-1.12</url>
      <bibkey>kanojia-2025-challenge</bibkey>
    </paper>
    <paper id="13">
      <title>Challenge Track: <fixed-case>JHARNA</fixed-case>-<fixed-case>MT</fixed-case>: A Copy-Augmented Hybrid of <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-Tuned <fixed-case>NLLB</fixed-case> and Lexical <fixed-case>SMT</fixed-case> with Minimum <fixed-case>B</fixed-case>ayes Risk Decoding for Low-Resource <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Dao Sy Duy</first><last>Minh</last></author>
      <author><first>Trung Kiet</first><last>Huynh</last></author>
      <author orcid="0009-0007-6716-7269"><first>Tran Chi</first><last>Nguyen</last></author>
      <author><first>Phu Quy Nguyen</first><last>Lam</last></author>
      <author orcid="0009-0001-5471-2578"><first>Phu-Hoa</first><last>Pham</last></author>
      <author><first>Nguyễn Đình Hà</first><last>Dương</last></author>
      <author><first>Dien</first><last>Dinh</last></author>
      <author orcid="0000-0002-0884-1635"><first>Long HB</first><last>Nguyen</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <pages>114-120</pages>
      <abstract>This paper describes JHARNA-MT, our system for the MMLoSo 2025 Shared Task on translation between high-resource languages (Hindi, English) and four low-resource Indic tribal languages: Bhili, Gondi, Mundari, and Santali. The task poses significant challenges, including data sparsity, morphological richness, and structural divergence across language pairs. To address these, we propose a hybrid translation pipeline that integrates non-parametric retrieval, lexical statistical machine translation (SMT), and LoRA-tuned NLLB-200 neural machine translation under a unified Minimum Bayes Risk (MBR) decoding framework. Exact and fuzzy retrieval exploit redundancy in government and administrative texts, SMT with diagonal alignment priors and back-translation provides lexically faithful hypotheses, and the NLLB-LoRA component contributes fluent neural candidates. MBR decoding selects consensus translations using a metric-matched utility based on a weighted combination of BLEU and chrF, mitigating the complementary error modes of SMT and NMT. Our final system, further enhanced with script-aware digit normalization and entity-preserving post-processing, achieves a private leaderboard score of 186.37 and ranks 2nd overall in the shared task, with ablation studies confirming the contribution of each component.</abstract>
      <url hash="1c7f74dd">2025.mmloso-1.13</url>
      <bibkey>minh-etal-2025-challenge</bibkey>
    </paper>
    <paper id="14">
      <title>Findings of the <fixed-case>MML</fixed-case>o<fixed-case>S</fixed-case>o 2025 Shared Task on Machine Translation into Tribal Languages</title>
      <author><first>Pooja</first><last>Singh</last></author>
      <author orcid="0009-0006-2285-0104"><first>Sandeep</first><last>Chatterjee</last></author>
      <author orcid="0000-0003-4354-9629"><first>Gullal S.</first><last>Cheema</last></author>
      <author><first>Amrit Singh</first><last>Bedi</last></author>
      <author orcid="0000-0002-0210-0369"><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Sandeep</first><last>Kumar</last><affiliation>Indian Institute of Technology Delhi</affiliation></author>
      <author><first>Ankita</first><last>Shukla</last><affiliation>University of Nevada, Reno</affiliation></author>
      <pages>121-129</pages>
      <abstract>This paper presents the findings of the MMLoSo Shared Task on Machine Translation. The competition features four tribal languages from India: Bhili, Mundari, Gondi, and Santali, each with 20,000 high-quality parallel sentence pairs and a 16,000-sentence evaluation set. A total of 18 teams submitted across all language pairs. The shared task addresses the challenges of translating India’s severely low-resource tribal languages, which, despite having millions of speakers, remain digitally marginalized due to limited textual resources, diverse scripts, rich morphology, and minimal publicly available parallel corpora. Systems were ranked using a weighted composite score combining BLEU (60%) and chrF (40%) to balance structural accuracy and character-level fluency. The best-performing system leveraged IndicTrans2 with directional LoRA adapters and reverse-model reranking. This work establishes the first reproducible benchmark for machine translation in these tribal languages. All datasets, baseline models, and system outputs are publicly released to support continued research in India’s tribal language technologies.</abstract>
      <url hash="d3b00333">2025.mmloso-1.14</url>
      <bibkey>singh-etal-2025-findings-mmloso</bibkey>
    </paper>
  </volume>
</collection>
