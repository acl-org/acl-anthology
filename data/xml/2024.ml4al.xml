<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.ml4al">
  <volume id="1" ingest-date="2024-07-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024)</booktitle>
      <editor><first>John</first><last>Pavlopoulos</last></editor>
      <editor><first>Thea</first><last>Sommerschield</last></editor>
      <editor><first>Yannis</first><last>Assael</last></editor>
      <editor><first>Shai</first><last>Gordin</last></editor>
      <editor><first>Kyunghyun</first><last>Cho</last></editor>
      <editor><first>Marco</first><last>Passarotti</last></editor>
      <editor><first>Rachele</first><last>Sprugnoli</last></editor>
      <editor><first>Yudong</first><last>Liu</last></editor>
      <editor><first>Bin</first><last>Li</last></editor>
      <editor><first>Adam</first><last>Anderson</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hybrid in Bangkok, Thailand and online</address>
      <month>August</month>
      <year>2024</year>
      <url hash="2c6873e9">2024.ml4al-1</url>
      <venue>ml4al</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="a6896f80">2024.ml4al-1.0</url>
      <bibkey>ml4al-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Challenging Error Correction in Recognised Byzantine <fixed-case>G</fixed-case>reek</title>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Vasiliki</first><last>Kougia</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Esteban</first><last>Garces Arias</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Paraskevi</first><last>Platanou</last></author>
      <author><first>Stepan</first><last>Shabalin</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Konstantina</first><last>Liagkou</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Emmanouil</first><last>Papadatos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Holger</first><last>Essler</last><affiliation>University of Venice</affiliation></author>
      <author><first>Jean-Baptiste</first><last>Camps</last><affiliation>École nationale des chartes, PSL</affiliation></author>
      <author><first>Franz</first><last>Fischer</last><affiliation>University of Venice</affiliation></author>
      <pages>1-12</pages>
      <abstract>Automatic correction of errors in Handwritten Text Recognition (HTR) output poses persistent challenges yet to be fully resolved. In this study, we introduce a shared task aimed at addressing this challenge, which attracted 271 submissions, yielding only a handful of promising approaches. This paper presents the datasets, the most effective methods, and an experimental analysis in error-correcting HTRed manuscripts and papyri in Byzantine Greek, the language that followed Classical and preceded Modern Greek. By using recognised and transcribed data from seven centuries, the two best-performing methods are compared, one based on a neural encoder-decoder architecture and the other based on engineered linguistic rules. We show that the recognition error rate can be reduced by both, up to 2.5 points at the level of characters and up to 15 at the level of words, while also elucidating their respective strengths and weaknesses.</abstract>
      <url hash="3d63685b">2024.ml4al-1.1</url>
      <bibkey>pavlopoulos-etal-2024-challenging</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>M</fixed-case>s<fixed-case>BERT</fixed-case>: A New Model for the Reconstruction of Lacunae in <fixed-case>H</fixed-case>ebrew Manuscripts</title>
      <author><first>Avi</first><last>Shmidman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Ometz</first><last>Shmidman</last><affiliation>DICTA: The Israel Center for Text Analysis</affiliation></author>
      <author><first>Hillel</first><last>Gershuni</last><affiliation>DICTA, Bar-Ilan University and University of Haifa</affiliation></author>
      <author><first>Moshe</first><last>Koppel</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>13-18</pages>
      <abstract>Hebrew manuscripts preserve thousands of textual transmissions of post-Biblical Hebrew texts from the first millennium. In many cases, the text in the manuscripts is not fully decipherable, whether due to deterioration, perforation, burns, or otherwise. Existing BERT models for Hebrew struggle to fill these gaps, due to the many orthographical deviations found in Hebrew manuscripts. We have pretrained a new dedicated BERT model, dubbed MsBERT (short for: Manuscript BERT), designed from the ground up to handle Hebrew manuscript text. MsBERT substantially outperforms all existing Hebrew BERT models regarding the prediction of missing words in fragmentary Hebrew manuscript transcriptions in multiple genres, as well as regarding the task of differentiating between quoted passages and exegetical elaborations. We provide MsBERT for free download and unrestricted use, and we also provide an interactive and user-friendly website to allow manuscripts scholars to leverage the power of MsBERT in their scholarly work of reconstructing fragmentary Hebrew manuscripts.</abstract>
      <url hash="e285ccae">2024.ml4al-1.2</url>
      <bibkey>shmidman-etal-2024-msbert</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.2</doi>
    </paper>
    <paper id="3">
      <title>Predicate Sense Disambiguation for <fixed-case>UMR</fixed-case> Annotation of <fixed-case>L</fixed-case>atin: Challenges and Insights</title>
      <author><first>Federica</first><last>Gamba</last><affiliation>Institute of Formal and Applied Linguistics, Charles University Prague</affiliation></author>
      <pages>19-29</pages>
      <abstract>This paper explores the possibility to exploit different Pretrained Language Models (PLMs) to assist in a manual annotation task consisting in assigning the appropriate sense to verbal predicates in a Latin text. Indeed, this represents a crucial step when annotating data according to the Uniform Meaning Representation (UMR) framework, designed to annotate the semantic content of a text in a cross-linguistic perspective. We approach the study as a Word Sense Disambiguation task, with the primary goal of assessing the feasibility of leveraging available resources for Latin to streamline the labor-intensive annotation process. Our methodology revolves around the exploitation of contextual embeddings to compute token similarity, under the assumption that predicates sharing a similar sense would also share their context of occurrence. We discuss our findings, emphasizing applicability and limitations of this approach in the context of Latin, for which the limited amount of available resources poses additional challenges.</abstract>
      <url hash="a483f56f">2024.ml4al-1.3</url>
      <bibkey>gamba-2024-predicate</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.3</doi>
    </paper>
    <paper id="4">
      <title>Classification of Paleographic Artifacts at Scale: Mitigating Confounds and Distribution Shift in Cuneiform Tablet Dating</title>
      <author><first>Danlu</first><last>Chen</last></author>
      <author><first>Jiahe</first><last>Tian</last></author>
      <author><first>Yufei</first><last>Weng</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jacobo</first><last>Myerston</last></author>
      <pages>30-41</pages>
      <abstract>Cuneiform is the oldest writing system used for more than 3,000 years in ancient Mesopotamia. Cuneiform is written on clay tablets, which are hard to date because they often lack explicit references to time periods and their paleographic traits are not always reliable as a dating criterion. In this paper, we systematically analyse cuneiform dating problems using machine learning. We build baseline models for both visual and textual features and identify two major issues: confounds and distribution shift. We apply adversarial regularization and deep domain adaptation to mitigate these issues. On tablets from the same museum collections represented in the training set, we achieve accuracies as high as 84.42%. However, when test tablets are taken from held-out collections, models generalize more poorly. This is only partially mitigated by robust learning techniques, highlighting important challenges for future work.</abstract>
      <url hash="7c699498">2024.ml4al-1.4</url>
      <bibkey>chen-etal-2024-classification</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.4</doi>
    </paper>
    <paper id="5">
      <title>Classifier identification in <fixed-case>A</fixed-case>ncient <fixed-case>E</fixed-case>gyptian as a low-resource sequence-labelling task</title>
      <author><first>Dmitry</first><last>Nikolaev</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Jorke</first><last>Grotenhuis</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Haleli</first><last>Harel</last><affiliation>Hebrew University of Jerusalem and Polis Institute</affiliation></author>
      <author><first>Orly</first><last>Goldwasser</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>42-47</pages>
      <abstract>The complex Ancient Egyptian (AE) writing system was characterised by widespread use of graphemic classifiers (determinatives): silent (unpronounced) hieroglyphic signs clarifying the meaning or indicating the pronunciation of the host word. The study of classifiers has intensified in recent years with the launch and quick growth of the iClassifier project, a web-based platform for annotation and analysis of classifiers in ancient and modern languages. Thanks to the data contributed by the project participants, it is now possible to formulate the identification of classifiers in AE texts as an NLP task. In this paper, we make first steps towards solving this task by implementing a series of sequence-labelling neural models, which achieve promising performance despite the modest amount of training data. We discuss tokenisation and operationalisation issues arising from tackling AE texts and contrast our approach with frequency-based baselines.</abstract>
      <url hash="98a7d380">2024.ml4al-1.5</url>
      <bibkey>nikolaev-etal-2024-classifier</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.5</doi>
    </paper>
    <paper id="6">
      <title>Long Unit Word Tokenization and Bunsetsu Segmentation of Historical <fixed-case>J</fixed-case>apanese</title>
      <author><first>Hiroaki</first><last>Ozaki</last><affiliation>Tokyo University of Agriculture and Technology, Tokyo Institute of Technology and Hitachi, Ltd.</affiliation></author>
      <author><first>Kanako</first><last>Komiya</last><affiliation>Tokyo University of Agriculture and Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Masayuki</first><last>Asahara</last><affiliation>National Institute for Japanese Language and Linguistics, Japan</affiliation></author>
      <author><first>Toshinobu</first><last>Ogiso</last><affiliation>National Insititute for Japanese Language and Linguistics</affiliation></author>
      <pages>48-55</pages>
      <abstract>In Japanese, the natural minimal phrase of a sentence is the “bunsetsu” and it serves as a natural boundary of a sentence for native speakers rather than words, and thus grammatical analysis in Japanese linguistics commonly operates on the basis of bunsetsu units.In contrast, because Japanese does not have delimiters between words, there are two major categories of word definition, namely, Short Unit Words (SUWs) and Long Unit Words (LUWs).Though a SUW dictionary is available, LUW is not.Hence, this study focuses on providing deep learning-based (or LLM-based) bunsetsu and Long Unit Words analyzer for the Heian period (AD 794-1185) and evaluating its performances.We model the parser as transformer-based joint sequential labels model, which combine bunsetsu BI tag, LUW BI tag, and LUW Part-of-Speech (POS) tag for each SUW token.We train our models on corpora of each period including contemporary and historical Japanese.The results range from 0.976 to 0.996 in f1 value for both bunsetsu and LUW reconstruction indicating that our models achieve comparable performance with models for a contemporary Japanese corpus.Through the statistical analysis and diachronic case study, the estimation of bunsetsu could be influenced by the grammaticalization of morphemes.</abstract>
      <url hash="8130ba7b">2024.ml4al-1.6</url>
      <bibkey>ozaki-etal-2024-long</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.6</doi>
    </paper>
    <paper id="7">
      <title>A new machine-actionable corpus for ancient text restoration</title>
      <author><first>Will</first><last>Fitzgerald</last><affiliation>Western Michigan University</affiliation></author>
      <author><first>Justin</first><last>Barney</last><affiliation>Western Michigan University</affiliation></author>
      <pages>56-60</pages>
      <abstract>The Machine-Actionable Ancient Text (MAAT) Corpus is a new resource providing training and evaluation data for restoring lacunae in ancient Greek, Latin, and Coptic texts. Current text restoration systems require large amounts of data for training and task-relevant means for evaluation. The MAAT Corpus addresses this need by converting texts available in EpiDoc XML format into a machine-actionable format that preserves the most textually salient aspects needed for machine learning: the text itself, lacunae, and textual restorations. Structured test cases are generated from the corpus that align with the actual text restoration task performed by papyrologists and epigraphist, enabling more realistic evaluation than the synthetic tasks used previously. The initial 1.0 beta release contains approximately 134,000 text editions, 178,000 text blocks, and 750,000 individual restorations, with Greek and Latin predominating. This corpus aims to facilitate the development of computational methods to assist scholars in accurately restoring ancient texts.</abstract>
      <url hash="4046a3d9">2024.ml4al-1.7</url>
      <bibkey>fitzgerald-barney-2024-new</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.7</doi>
    </paper>
    <paper id="8">
      <title>Lacuna Language Learning: Leveraging <fixed-case>RNN</fixed-case>s for Ranked Text Completion in Digitized <fixed-case>C</fixed-case>optic Manuscripts</title>
      <author><first>Lauren</first><last>Levine</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Cindy</first><last>Li</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Lydia</first><last>Bremer-McCollum</last></author>
      <author><first>Nicholas</first><last>Wagner</last><affiliation>Duke University</affiliation></author>
      <author><first>Amir</first><last>Zeldes</last><affiliation>Georgetown University</affiliation></author>
      <pages>61-70</pages>
      <abstract>Ancient manuscripts are frequently damaged, containing gaps in the text known as lacunae. In this paper, we present a bidirectional RNN model for character prediction of Coptic characters in manuscript lacunae. Our best model performs with 72% accuracy on single character reconstruction, but falls to 37% when reconstructing lacunae of various lengths. While not suitable for definitive manuscript reconstruction, we argue that our RNN model can help scholars rank the likelihood of textual reconstructions. As evidence, we use our RNN model to rank reconstructions in two early Coptic manuscripts. Our investigation shows that neural models can augment traditional methods of textual restoration, providing scholars with an additional tool to assess lacunae in Coptic manuscripts.</abstract>
      <url hash="994613d0">2024.ml4al-1.8</url>
      <bibkey>levine-etal-2024-lacuna</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.8</doi>
    </paper>
    <paper id="9">
      <title>Deep Learning Meets Egyptology: a Hieroglyphic Transformer for Translating <fixed-case>A</fixed-case>ncient <fixed-case>E</fixed-case>gyptian</title>
      <author><first>Mattia</first><last>De Cao</last></author>
      <author><first>Nicola</first><last>De Cao</last><affiliation>Google</affiliation></author>
      <author><first>Angelo</first><last>Colonna</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Alessandro</first><last>Lenci</last><affiliation>University of Pisa</affiliation></author>
      <pages>71-86</pages>
      <abstract>This work explores the potential of Transformer models focusing on the translation of ancient Egyptian hieroglyphs. We present a novel Hieroglyphic Transformer model, built upon the powerful M2M-100 multilingual translation framework and trained on a dataset we customised from the Thesaurus Linguae Aegyptiae database. Our experiments demonstrate promising results, with the model achieving significant accuracy in translating hieroglyphics into both German and English. This work holds significant implications for Egyptology, potentially accelerating the translation process and unlocking new research approaches.</abstract>
      <url hash="6bc66890">2024.ml4al-1.9</url>
      <bibkey>cao-etal-2024-deep</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.9</doi>
      <revision id="1" href="2024.ml4al-1.9v1" hash="8097234c"/>
      <revision id="2" href="2024.ml4al-1.9v2" hash="6bc66890" date="2024-10-18">This revision corrects special characters not shown in two tables, and fixes and adds two major reference entries.</revision>
    </paper>
    <paper id="10">
      <title>Neural Lemmatization and <fixed-case>POS</fixed-case>-tagging models for <fixed-case>C</fixed-case>optic, Demotic and Earlier <fixed-case>E</fixed-case>gyptian</title>
      <author><first>Aleksi</first><last>Sahala</last></author>
      <author><first>Eliese-Sophia</first><last>Lincke</last><affiliation>Freie Universität Berlin</affiliation></author>
      <pages>87-97</pages>
      <abstract>We present models for lemmatizing and POS-tagging Earlier Egyptian, Coptic and Demotic to test the performance of our pipeline for the ancient languages of Egypt. Of these languages, Demotic and Egyptian are known to be difficult to annotate due to their high extent of ambiguity. We report lemmatization accuracy of 86%, 91% and 99%, and XPOS-tagging accuracy of 89%, 95% and 98% for Earlier Egyptian, Demotic and Coptic, respectively.</abstract>
      <url hash="00de1640">2024.ml4al-1.10</url>
      <bibkey>sahala-lincke-2024-neural</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>UFCN</fixed-case>et: Unsupervised Network based on <fixed-case>F</fixed-case>ourier transform and Convolutional attention for Oracle Character Recognition</title>
      <author><first>Yanan</first><last>Zhou</last></author>
      <author><first>Guoqi</first><last>Liu</last><affiliation>Henan Normal University</affiliation></author>
      <author><first>Yiping</first><last>Yang</last></author>
      <author><first>Linyuan</first><last>Ru</last><affiliation>Henan Normal University</affiliation></author>
      <author><first>Dong</first><last>Liu</last><affiliation>Henan Normal University</affiliation></author>
      <author><first>Xueshan</first><last>Li</last><affiliation>Henan Normal University</affiliation></author>
      <pages>98-106</pages>
      <abstract>Oracle bone script (OBS) is the earliest writing system in China, which is of great value in the improvement of archaeology and Chinese cultural history. However, there are some problems such as the lack of labels and the difficulty to distinguish the glyphs from the background of OBS, which makes the automatic recognition of OBS in the real world not achieve the satisfactory effect. In this paper, we propose a character recognition method based on an unsupervised domain adaptive network (UFCNet). Firstly, a convolutional attention fusion module (CAFM) is designed in the encoder to obtain more global features through multi-layer feature fusion. Second, we construct a Fourier transform (FT) module that focuses on the differences between glyphs and backgrounds. Finally, to further improve the network’s ability to recognize character edges, we introduce a kernel norm-constrained loss function. Extensive experiments perform on the Oracle-241 dataset show that the proposed method is superior to other adaptive methods. The code will be available at https://github.com/zhouynan/UFCNet.</abstract>
      <url hash="1e0ca315">2024.ml4al-1.11</url>
      <bibkey>zhou-etal-2024-ufcnet</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.11</doi>
    </paper>
    <paper id="12">
      <title>Coarse-to-Fine Generative Model for Oracle Bone Inscriptions Inpainting</title>
      <author><first>Shibin</first><last>Wang</last><affiliation>Henan Normal University and Henan Normal University</affiliation></author>
      <author><first>Wenjie</first><last>Guo</last><affiliation>Henan Normal University and Henan Normal University</affiliation></author>
      <author><first>Yubo</first><last>Xu</last></author>
      <author><first>Dong</first><last>Liu</last><affiliation>Henan Normal University</affiliation></author>
      <author><first>Xueshan</first><last>Li</last><affiliation>Henan Normal University</affiliation></author>
      <pages>107-114</pages>
      <abstract>Due to ancient origin, there are many incomplete characters in the unearthed Oracle Bone Inscriptions(OBI), which brings the great challenges to recognition and research. In recent years, image inpainting techniques have made remarkable progress. However, these models are unable to adapt to the unique font shape and complex text background of OBI. To meet these aforementioned challenges, we propose a two-stage method for restoring damaged OBI using Generative Adversarial Networks (GAN), which incorporates a dual discriminator structure to capture both global and local image information. In order to accurately restore the image structure and details, the spatial attention mechanism and a novel loss function are proposed. By feeding clear copies of existing OBI and various types of masks into the network, it learns to generate content for the missing regions. Experimental results demonstrate the effectiveness of our proposed method in completing OBI compared to several state-of-the-art techniques.</abstract>
      <url hash="0903b3d6">2024.ml4al-1.12</url>
      <bibkey>wang-etal-2024-coarse</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.12</doi>
    </paper>
    <paper id="13">
      <title>Restoring <fixed-case>M</fixed-case>ycenaean <fixed-case>L</fixed-case>inear <fixed-case>B</fixed-case> ‘A&amp;<fixed-case>B</fixed-case>’ series tablets using supervised and transfer learning</title>
      <author><first>Katerina</first><last>Papavassileiou</last><affiliation>University of Patras</affiliation></author>
      <author><first>Dimitrios</first><last>Kosmopoulos</last></author>
      <pages>115-129</pages>
      <abstract>We investigate the problem of restoring Mycenaean linear B clay tablets, dating from about 1400 B.C. to roughly 1200 B.C., by using text infilling methods based on machine learning models. Our goals here are: first to try to improve the results of the methods used in the related literature by focusing on the characteristics of the Mycenaean Linear B writing system (series D), second to examine the same problem for the first time on series A&amp;B and finally to investigate transfer learning using series D as source and the smaller series A&amp;B as target. Our results show promising results in the supervised learning tasks, while further investigation is needed to better exploit the merits of transfer learning.</abstract>
      <url hash="67ee1ef0">2024.ml4al-1.13</url>
      <bibkey>papavassileiou-kosmopoulos-2024-restoring</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>C</fixed-case>u<fixed-case>R</fixed-case>e<fixed-case>D</fixed-case>: Deep Learning Optical Character Recognition for Cuneiform Text Editions and Legacy Materials</title>
      <author><first>Shai</first><last>Gordin</last><affiliation>Ariel University</affiliation></author>
      <author><first>Morris</first><last>Alper</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Avital</first><last>Romach</last></author>
      <author><first>Luis</first><last>Saenz Santos</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Naama</first><last>Yochai</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Roey</first><last>Lalazar</last></author>
      <pages>130-140</pages>
      <abstract>Cuneiform documents, the earliest known form of writing, are prolific textual sources of the ancient past. Experts publish editions of these texts in transliteration using specialized typesetting, but most remain inaccessible for computational analysis in traditional printed books or legacy materials. Off-the-shelf OCR systems are insufficient for digitization without adaptation. We present CuReD (Cuneiform Recognition-Documents), a deep learning-based human-in-the-loop OCR pipeline for digitizing scanned transliterations of cuneiform texts. CuReD has a character error rate of 9% on clean data and 11% on representative scans. We digitized a challenging sample of transliterated cuneiform documents, as well as lexical index cards from the University of Pennsylvania Museum, demonstrating the feasibility of our platform for enabling computational analysis and bolstering machine-readable cuneiform text datasets. Our result provide the first human-in-the-loop pipeline and interface for digitizing transliterated cuneiform sources and legacy materials, enabling the enrichment of digital sources of these low-resource languages.</abstract>
      <url hash="098f50dd">2024.ml4al-1.14</url>
      <bibkey>gordin-etal-2024-cured</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.14</doi>
    </paper>
    <paper id="15">
      <title>Towards Context-aware Normalization of Variant Characters in Classical <fixed-case>C</fixed-case>hinese Using Parallel Editions and <fixed-case>BERT</fixed-case></title>
      <author><first>Florian</first><last>Kessler</last><affiliation>Friedrich-Alexander Universität Erlangen-Nürnberg</affiliation></author>
      <pages>141-151</pages>
      <abstract>For the automatic processing of Classical Chinese texts it is highly desirable to normalize variant characters, i.e. characters with different visual forms that are being used to represent the same morpheme, into a single form. However, there are some variant characters that are used interchangeably by some writers but deliberately employed to distinguish between different meanings by others. Hence, in order to avoid losing information in the normalization processes by conflating meaningful distinctions between variants, an intelligent normalization system that takes context into account is needed. Towards the goal of developing such a system, in this study, we describe how a dataset with usage samples of variant characters can be extracted from a corpus of paired editions of multiple texts. Using the dataset, we conduct two experiments, testing whether models can be trained with contextual word embeddings to predict variant characters. The results of the experiments show that while this is often possible for single texts, most conventions learned do not transfer well between documents.</abstract>
      <url hash="aba1f984">2024.ml4al-1.15</url>
      <bibkey>kessler-2024-towards</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.15</doi>
    </paper>
    <paper id="16">
      <title>“Gotta catch ‘em all!”: Retrieving people in <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek texts combining transformer models and domain knowledge</title>
      <author><first>Marijke</first><last>Beersmans</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Alek</first><last>Keersmaekers</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Evelien</first><last>de Graaf</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Tim</first><last>Van de Cruys</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Mark</first><last>Depauw</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Margherita</first><last>Fantoli</last><affiliation>KU Leuven</affiliation></author>
      <pages>152-164</pages>
      <abstract>In this paper, we present a study of transformer-based Named Entity Recognition (NER) as applied to Ancient Greek texts, with an emphasis on retrieving personal names. Recent research shows that, while the task remains difficult, the use of transformer models results in significant improvements. We, therefore, compare the performance of four transformer models on the task of NER for the categories of people, locations and groups, and add an out-of-domain test set to the existing datasets. Results on this set highlight the shortcomings of the models when confronted with a random sample of sentences. To be able to more straightforwardly integrate domain and linguistic knowledge to improve performance, we narrow down our approach to the category of people. The task is simplified to a binary PERS/MISC classification on the token level, starting from capitalised words. Next, we test the use of domain and linguistic knowledge to improve the results. We find that including simple gazetteer information as a binary mask has a marginally positive effect on newly annotated data and that treebanks can be used to help identify multi-word individuals if they are scarcely or inconsistently annotated in the available training data. The qualitative error analysis identifies the potential for improvement in both manual annotation and the inclusion of domain and linguistic knowledge in the transformer models.</abstract>
      <url hash="2ab6db51">2024.ml4al-1.16</url>
      <bibkey>beersmans-etal-2024-gotta</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.16</doi>
    </paper>
    <paper id="17">
      <title>Adapting transformer models to morphological tagging of two highly inflectional languages: a case study on <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek and <fixed-case>L</fixed-case>atin</title>
      <author><first>Alek</first><last>Keersmaekers</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Wouter</first><last>Mercelis</last><affiliation>KU Leuven</affiliation></author>
      <pages>165-176</pages>
      <abstract>Natural language processing for Greek and Latin, inflectional languages with small corpora, requires special techniques. For morphological tagging, transformer models show promising potential, but the best approach to use these models is unclear. For both languages, this paper examines the impact of using morphological lexica, training different model types (a single model with a combined feature tag, multiple models for separate features, and a multi-task model for all features), and adding linguistic constraints. We find that, although simply fine-tuning transformers to predict a monolithic tag may already yield decent results, each of these adaptations can further improve tagging accuracy.</abstract>
      <url hash="e8df5aba">2024.ml4al-1.17</url>
      <bibkey>keersmaekers-mercelis-2024-adapting</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.17</doi>
    </paper>
    <paper id="18">
      <title>A deep learning pipeline for the palaeographical dating of ancient <fixed-case>G</fixed-case>reek papyrus fragments</title>
      <author><first>Graham</first><last>West</last></author>
      <author><first>Matthew I.</first><last>Swindall</last></author>
      <author><first>James H.</first><last>Brusuelas</last></author>
      <author><first>Francesca</first><last>Maltomini</last></author>
      <author><first>Marius</first><last>Gerhardt</last></author>
      <author><first>Marzia</first><last>D’Angelo</last></author>
      <author><first>John F.</first><last>Wallin</last></author>
      <pages>177-185</pages>
      <abstract>In this paper we present a deep learning pipeline for automatically dating ancient Greek papyrus fragments based solely on fragment images. The overall pipeline consists of several stages, including handwritten text recognition (HTR) to detect and classify characters, filtering and grouping of detected characters, 24 character-level date prediction models, and a fragment-level date prediction model that utilizes the per-character predictions. A new dataset (containing approximately 7,000 fragment images and 778,000 character images) was created by scraping papyrus databases, extracting fragment images with known dates, and running them through our HTR models to obtain labeled character images. Transfer learning was then used to fine-tune separate ResNets to predict dates for individual characters which are then used, in aggregate, to train the fragment-level date prediction model. Experiments show that even though the average accuracies of character-level dating models is low, between 35%-45%, the fragment-level model can achieve up to 79% accuracy in predicting a broad, two-century date range for fragments with many characters. We then discuss the limitations of this approach and outline future work to improve temporal resolution and further testing on additional papyri. This image-based deep learning approach has great potential to assist scholars in the palaeographical analysis and dating of ancient Greek manuscripts.</abstract>
      <url hash="b9274c6a">2024.ml4al-1.18</url>
      <bibkey>west-etal-2024-deep</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>UD</fixed-case>-<fixed-case>ETCSUX</fixed-case>: Toward a Better Understanding of <fixed-case>S</fixed-case>umerian Syntax</title>
      <author><first>Kenan</first><last>Jiang</last><affiliation>Emory University</affiliation></author>
      <author><first>Adam</first><last>Anderson</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>186-191</pages>
      <abstract>Beginning with the discovery of the cuneiform writing system in 1835, there have been numerous grammars published illustrating the complexities of the Sumerian language. However, the one thing they have in common is their omission of dependency rules for syntax in Sumerian linguistics. For this reason we are working toward a better understanding of Sumerian syntax, by means of dependency-grammar in the Universal Dependencies (UD) framework. Therefore, in this study we articulate the methods and engineering techniques that can address the hardships in annotating dependency relationships in the Sumerian texts in transliteration from the Electronic Text Corpora of Sumerian (ETCSUX). Our code can be found at https://github.com/ancient-world-citation-analysis/UD-ETCSUX.</abstract>
      <url hash="e9097875">2024.ml4al-1.19</url>
      <bibkey>jiang-anderson-2024-ud</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>S</fixed-case>um<fixed-case>T</fixed-case>ablets: A Transliteration Dataset of <fixed-case>S</fixed-case>umerian Tablets</title>
      <author><first>Cole</first><last>Simmons</last></author>
      <author><first>Richard</first><last>Diehl Martinez</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>192-202</pages>
      <abstract>Transliterating Sumerian is a key step in understanding Sumerian texts, but remains a difficult and time-consuming task. With more than 100,000 known texts and comparatively few specialists, manually maintaining up-to-date transliterations for the entire corpus is impractical. While many transliterations have been published online thanks to the dedicated effort of previous projects, the lack of a comprehensive, easily accessible dataset that pairs digital representations of source glyphs with their transliterations has hindered the application of natural language processing (NLP) methods to this task.To address this gap, we present SumTablets, the largest collection of Sumerian cuneiform tablets structured as Unicode glyph–transliteration pairs. Our dataset comprises 91,606 tablets (totaling 6,970,407 glyphs) with associated period and genre metadata. We release SumTablets as a Hugging Face Dataset.To construct SumTablets, we first preprocess and standardize publicly available transliterations. We then map them back to a Unicode representation of their source glyphs, retaining parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens.We leverage SumTablets to implement and evaluate two transliteration approaches: 1) weighted sampling from a glyph’s possible readings, 2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the potential use of deep learning methods in Assyriological research.</abstract>
      <url hash="a2a2abf7">2024.ml4al-1.20</url>
      <bibkey>simmons-etal-2024-sumtablets</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>L</fixed-case>atin Treebanks in Review: An Evaluation of Morphological Tagging Across Time</title>
      <author><first>Marisa</first><last>Hudspeth</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author><first>Laure</first><last>Thompson</last><affiliation>Princeton University and University of Massachusetts, Amherst</affiliation></author>
      <pages>203-218</pages>
      <abstract>Existing Latin treebanks draw from Latin’s long written tradition, spanning 17 centuries and a variety of cultures. Recent efforts have begun to harmonize these treebanks’ annotations to better train and evaluate morphological taggers. However, the heterogeneity of these treebanks must be carefully considered to build effective and reliable data. In this work, we review existing Latin treebanks to identify the texts they draw from, identify their overlap, and document their coverage across time and genre. We additionally design automated conversions of their morphological feature annotations into the conventions of standard Latin grammar. From this, we build new time-period data splits that draw from the existing treebanks which we use to perform a broad cross-time analysis for POS and morphological feature tagging. We find that BERT-based taggers outperform existing taggers while also being more robust to cross-domain shifts.</abstract>
      <url hash="7c94f49a">2024.ml4al-1.21</url>
      <bibkey>hudspeth-etal-2024-latin</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.21</doi>
    </paper>
    <paper id="22">
      <title>The Metronome Approach to <fixed-case>S</fixed-case>anskrit Meter: Analysis for the Rigveda</title>
      <author><first>Yuzuki</first><last>Tsukagoshi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Ikki</first><last>Ohmukai</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>219-223</pages>
      <abstract>This study analyzes the verses of the Rigveda, the oldest Sanskrit text, from a metrical perspective. Based on metrical structures, the verses are represented by four elements: light syllables, heavy syllables, word boundaries, and line boundaries. As a result, it became evident that among verses traditionally categorized under the same metrical name, there are those forming distinct clusters. Furthermore, the study reveals commonalities in metrical structures, such as similar metrical patterns grouping together despite differences in the number of lines. Going forward, it is anticipated that this methodology will enable comparisons across multiple languages within the Indo-European language family.</abstract>
      <url hash="fcb7f843">2024.ml4al-1.22</url>
      <bibkey>tsukagoshi-ohmukai-2024-metronome</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.22</doi>
    </paper>
    <paper id="23">
      <title>Ancient Wisdom, Modern Tools: Exploring Retrieval-Augmented <fixed-case>LLM</fixed-case>s for <fixed-case>A</fixed-case>ncient <fixed-case>I</fixed-case>ndian Philosophy</title>
      <author><first>Priyanka</first><last>Mandikal</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>224-250</pages>
      <abstract>LLMs have revolutionized the landscape of information retrieval and knowledge dissemination. However, their application in specialized areas is often hindered by limitations such as factual inaccuracies and hallucinations, especially in long-tail knowledge distributions. In this work, we explore the potential of retrieval-augmented generation (RAG) models in performing long-form question answering (LFQA) on a specially curated niche and custom knowledge domain. We present VedantaNY-10M, a dataset curated from extensive public discourses on the ancient Indian philosophy of Advaita Vedanta. We develop and benchmark a RAG model against a standard, non-RAG LLM, focusing on transcription, retrieval, and generation performance. A human evaluation involving computational linguists and domain experts, shows that the RAG model significantly outperforms the standard model in producing factual, comprehensive responses having fewer hallucinations. In addition, we find that a keyword-based hybrid retriever that focuses on unique low-frequency words further improves results. Our study provides insights into the future development of real-world RAG models for custom and niche areas of knowledge.</abstract>
      <url hash="83a9e965">2024.ml4al-1.23</url>
      <bibkey>mandikal-2024-ancient</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.23</doi>
    </paper>
    <paper id="24">
      <title>Leveraging Part-of-Speech Tagging for Enhanced Stylometry of <fixed-case>L</fixed-case>atin Literature</title>
      <author><first>Sarah Li</first><last>Chen</last></author>
      <author><first>Patrick J.</first><last>Burns</last><affiliation>New York University</affiliation></author>
      <author><first>Thomas J.</first><last>Bolt</last><affiliation>Lafayette College</affiliation></author>
      <author><first>Pramit</first><last>Chaudhuri</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <pages>251-259</pages>
      <abstract>In literary critical applications, stylometry can benefit from hand-curated feature sets capturing various syntactic and rhetorical functions. For premodern languages, calculation of such features is hampered by a lack of adequate computational resources for accurate part-of-speech tagging and semantic disambiguation. This paper reports an evaluation of POS-taggers for Latin and their use in augmenting a hand-curated stylometric feature set. Our experiments show that POS-augmented features not only provide more accurate counts than POS-blind features but also perform better on tasks such as genre classification. In the course of this work we introduce POS n-grams as a feature for Latin stylometry.</abstract>
      <url hash="13f9ce7f">2024.ml4al-1.24</url>
      <bibkey>chen-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.24</doi>
    </paper>
    <paper id="25">
      <title>Exploring intertextuality across the <fixed-case>H</fixed-case>omeric poems through language models</title>
      <author><first>Maria</first><last>Konstantinidou</last><affiliation>Democritus University of Thrace</affiliation></author>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Elton</first><last>Barker</last><affiliation>Open University</affiliation></author>
      <pages>260-268</pages>
      <abstract>Past research has modelled statistically the language of the Homeric poems, assessing the degree of surprisal for each verse through diverse metrics and resulting to the HoLM resource. In this study we utilise the HoLM resource to explore cross poem affinity at the verse level, looking at Iliadic verses and passages that are less surprising to the Odyssean model than to the Iliadic one and vice-versa. Using the same tool, we investigate verses that evoke greater surprise when assessed by a local model trained solely on their source book, compared to a global model trained on the entire source poem. Investigating deeper on the distribution of such verses across the Homeric poems we employ machine learning text classification to further analyse quantitatively cross-poem affinity in selected books.</abstract>
      <url hash="39e40da3">2024.ml4al-1.25</url>
      <bibkey>konstantinidou-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.25</doi>
    </paper>
    <paper id="26">
      <title>Detecting Narrative Patterns in Biblical <fixed-case>H</fixed-case>ebrew and <fixed-case>G</fixed-case>reek</title>
      <author><first>Hope</first><last>McGovern</last></author>
      <author><first>Hale</first><last>Sirin</last></author>
      <author><first>Tom</first><last>Lippincott</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Andrew</first><last>Caines</last><affiliation>University of Cambridge</affiliation></author>
      <pages>269-279</pages>
      <abstract>We present a novel approach to extracting recurring narrative patterns, or type-scenes, in Biblical Hebrew and Biblical Greek with an information retrieval network. We use cross-references to train an encoder model to create similar representations for verses linked by a cross-reference. We then query our trained model with phrases informed by humanities scholarship and designed to elicit particular kinds of narrative scenes. Our models can surface relevant instances in the top-10 ranked candidates in many cases.Through manual error analysis and discussion, we address the limitations and challenges inherent in our approach. Our findings contribute to the field of Biblical scholarship by offering a new perspective on narrative analysis within ancient texts, and to computational modeling of narrative with a genre-agnostic approach for pattern-finding in long, literary texts.</abstract>
      <url hash="2b00c238">2024.ml4al-1.26</url>
      <bibkey>mcgovern-etal-2024-detecting</bibkey>
      <doi>10.18653/v1/2024.ml4al-1.26</doi>
    </paper>
  </volume>
</collection>
