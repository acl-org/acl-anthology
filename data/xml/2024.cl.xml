<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.cl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 1 - March 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>1</journal-issue>
    </meta>
    <paper id="1">
      <title>My Big, Fat 50-Year Journey</title>
      <author><first>Martha</first><last>Palmer</last></author>
      <doi>10.1162/coli_a_00499</doi>
      <abstract>My most heartfelt thanks to ACL for this tremendous honor. I’m completely thrilled. I cannot tell you how surprised I was when I got Iryna’s email. It is amazing that my first ACL conference since 2019 in Florence includes this award. What a wonderful way to be back with all of my friends and family here at ACL. I’m going to tell you about my big fat 50-year journey. What have I been doing for the last 50 years? Well, finding meaning, quite literally in words. Or in other words, exploring how computational lexical semantics can support natural language understanding. This is going to be quick. Hold onto your hats, here we go.</abstract>
      <pages>1–24</pages>
      <url hash="350741f4">2024.cl-1.1</url>
      <bibkey>palmer-2024-big</bibkey>
    </paper>
    <paper id="2">
      <title>Rethinking the Exploitation of Monolingual Data for Low-Resource Neural Machine Translation</title>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Baosong</first><last>Yang*</last></author>
      <author><first>Derek Fai</first><last>Wong*</last></author>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <doi>10.1162/coli_a_00496</doi>
      <abstract>The utilization of monolingual data has been shown to be a promising strategy for addressing low-resource machine translation problems. Previous studies have demonstrated the effectiveness of techniques such as back-translation and self-supervised objectives, including masked language modeling, causal language modeling, and denoise autoencoding, in improving the performance of machine translation models. However, the manner in which these methods contribute to the success of machine translation tasks and how they can be effectively combined remains an under-researched area. In this study, we carry out a systematic investigation of the effects of these techniques on linguistic properties through the use of probing tasks, including source language comprehension, bilingual word alignment, and translation fluency. We further evaluate the impact of pre-training, back-translation, and multi-task learning on bitexts of varying sizes. Our findings inform the design of more effective pipelines for leveraging monolingual data in extremely low-resource and low-resource machine translation tasks. Experiment results show consistent performance gains in seven translation directions, which provide further support for our conclusions and understanding of the role of monolingual data in machine translation.</abstract>
      <pages>25–47</pages>
      <url hash="97fc3d67">2024.cl-1.2</url>
      <bibkey>pang-etal-2024-rethinking</bibkey>
    </paper>
    <paper id="3">
      <title>How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models</title>
      <author><first>Mark</first><last>Ormerod</last></author>
      <author><first>Jesús Martínez</first><last>del Rincón</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <doi>10.1162/coli_a_00495</doi>
      <abstract>Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., KITCHEN CHAIR: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.</abstract>
      <pages>49–81</pages>
      <url hash="0aff206a">2024.cl-1.3</url>
      <bibkey>ormerod-etal-2024-kitchen</bibkey>
    </paper>
    <paper id="4">
      <title>Universal Generation for <fixed-case>O</fixed-case>ptimality <fixed-case>T</fixed-case>heory Is <fixed-case>PSPACE</fixed-case>-Complete</title>
      <author><first>Sophie</first><last>Hao</last></author>
      <doi>10.1162/coli_a_00494</doi>
      <abstract>This article shows that the universal generation problem for Optimality Theory (OT) is PSPACE-complete. While prior work has shown that universal generation is at least NP-hard and at most EXPSPACE-hard, our results place universal generation in between those two classes, assuming that NP ≠ PSPACE. We additionally show that when the number of constraints is bounded in advance, universal generation is at least NL-hard and at most NPNP-hard. Our proofs rely on a close connection between OT and the intersection non-emptiness problem for finite automata, which is PSPACE-complete in general and NL-complete when the number of automata is bounded. Our analysis shows that constraint interaction is the main contributor to the complexity of OT: The ability to factor transformations into simple, interacting constraints allows OT to furnish compact descriptions of intricate phonological phenomena.</abstract>
      <pages>83–117</pages>
      <url hash="59b66d33">2024.cl-1.4</url>
      <bibkey>hao-2024-universal</bibkey>
    </paper>
    <paper id="5">
      <title>Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering</title>
      <author><first>Akshay</first><last>Chaturvedi</last></author>
      <author><first>Swarnadeep</first><last>Bhar</last></author>
      <author><first>Soumadeep</first><last>Saha</last></author>
      <author><first>Utpal</first><last>Garain</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <doi>10.1162/coli_a_00493</doi>
      <abstract>Transformer-based language models have been shown to be highly effective for several NLP tasks. In this article, we consider three transformer models, BERT, RoBERTa, and XLNet, in both small and large versions, and investigate how faithful their representations are with respect to the semantic content of texts. We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally figure in a model’s inferences in question answering. We then test this notion by observing a model’s behavior on answering questions about a story after performing two novel semantic interventions—deletion intervention and negation intervention. While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (∼ 50% for deletion intervention, and ∼ 20% drop in accuracy for negation intervention). We then propose an intervention-based training regime that can mitigate the undesirable effects for deletion intervention by a significant margin (from ∼ 50% to ∼ 6%). We analyze the inner-workings of the models to better understand the effectiveness of intervention-based training for deletion intervention. But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models’ inability to deal with negation intervention or to capture the predicate–argument structure of texts. We also test InstructGPT, via prompting, for its ability to handle the two interventions and to capture predicate–argument structure. While InstructGPT models do achieve very high performance on predicate–argument structure task, they fail to respond adequately to our deletion and negation interventions.</abstract>
      <pages>119–155</pages>
      <url hash="1e92522f">2024.cl-1.5</url>
      <bibkey>chaturvedi-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="6">
      <title>On the Role of Morphological Information for Contextual Lemmatization</title>
      <author><first>Olia</first><last>Toporkov</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <doi>10.1162/coli_a_00497</doi>
      <abstract>Lemmatization is a natural language processing (NLP) task that consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this article we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish, and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, which constitutes, after all, their most common application use. The results of our study are rather surprising. It turns out that providing lemmatizers with fine-grained morphological features during training is not that beneficial, not even for agglutinative languages. In fact, modern contextual word representations seem to implicitly encode enough morphological information to obtain competitive contextual lemmatizers without seeing any explicit morphological signal. Moreover, our experiments suggest that the best lemmatizers out-of-domain are those using simple UPOS tags or those trained without morphology and, lastly, that current evaluation practices for lemmatization are not adequate to clearly discriminate between models.</abstract>
      <pages>157–191</pages>
      <url hash="c1b837bb">2024.cl-1.6</url>
      <bibkey>toporkov-agerri-2024-role</bibkey>
    </paper>
    <paper id="7">
      <title>Stance Detection with Explanations</title>
      <author><first>Rudra Ranajee</first><last>Saha</last></author>
      <author><first>Laks V. S.</first><last>Lakshmanan</last></author>
      <author><first>Raymond T.</first><last>Ng</last></author>
      <doi>10.1162/coli_a_00501</doi>
      <abstract>Identification of stance has recently gained a lot of attention with the extreme growth of fake news and filter bubbles. Over the last decade, many feature-based and deep-learning approaches have been proposed to solve stance detection. However, almost none of the existing works focus on providing a meaningful explanation for their prediction. In this work, we study stance detection with an emphasis on generating explanations for the predicted stance by capturing the pivotal argumentative structure embedded in a document. We propose to build a stance tree that utilizes rhetorical parsing to construct an evidence tree and to use Dempster Shafer Theory to aggregate the evidence. Human studies show that our unsupervised technique of generating stance explanations outperforms the SOTA extractive summarization method in terms of informativeness, non-redundancy, coverage, and overall quality. Furthermore, experiments show that our explanation-based stance prediction excels or matches the performance of the SOTA model on various benchmark datasets.</abstract>
      <pages>193–235</pages>
      <url hash="8243fb1a">2024.cl-1.7</url>
      <bibkey>saha-etal-2024-stance</bibkey>
    </paper>
    <paper id="8">
      <title>Can Large Language Models Transform Computational Social Science?</title>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>William</first><last>Held</last></author>
      <author><first>Omar</first><last>Shaikh</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Zhehao</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <doi>10.1162/coli_a_00502</doi>
      <abstract>Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.</abstract>
      <pages>237–291</pages>
      <url hash="47b4f1d0">2024.cl-1.8</url>
      <bibkey>ziems-etal-2024-large</bibkey>
    </paper>
    <paper id="9">
      <title>Language Model Behavior: A Comprehensive Survey</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin K.</first><last>Bergen</last></author>
      <doi>10.1162/coli_a_00492</doi>
      <abstract>Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.</abstract>
      <pages>293–350</pages>
      <url hash="c1447504">2024.cl-1.9</url>
      <bibkey>chang-bergen-2024-language</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>P</fixed-case>olysemy—<fixed-case>E</fixed-case>vidence from Linguistics, Behavioral Science, and Contextualized Language Models</title>
      <author><first>Janosch</first><last>Haber</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <doi>10.1162/coli_a_00500</doi>
      <abstract>Polysemy is the type of lexical ambiguity where a word has multiple distinct but related interpretations. In the past decade, it has been the subject of a great many studies across multiple disciplines including linguistics, psychology, neuroscience, and computational linguistics, which have made it increasingly clear that the complexity of polysemy precludes simple, universal answers, especially concerning the representation and processing of polysemous words. But fuelled by the growing availability of large, crowdsourced datasets providing substantial empirical evidence; improved behavioral methodology; and the development of contextualized language models capable of encoding the fine-grained meaning of a word within a given context, the literature on polysemy recently has developed more complex theoretical analyses. In this survey we discuss these recent contributions to the investigation of polysemy against the backdrop of a long legacy of research across multiple decades and disciplines. Our aim is to bring together different perspectives to achieve a more complete picture of the heterogeneity and complexity of the phenomenon of polysemy. Specifically, we highlight evidence supporting a range of hybrid models of the mental processing of polysemes. These hybrid models combine elements from different previous theoretical approaches to explain patterns and idiosyncrasies in the processing of polysemous that the best known models so far have failed to account for. Our literature review finds that (i) traditional analyses of polysemy can be limited in their generalizability by loose definitions and selective materials; (ii) linguistic tests provide useful evidence on individual cases, but fail to capture the full range of factors involved in the processing of polysemous sense extensions; and (iii) recent behavioral (psycho) linguistics studies, large-scale annotation efforts, and investigations leveraging contextualized language models provide accumulating evidence suggesting that polysemous sense similarity covers a wide spectrum between identity of sense and homonymy-like unrelatedness of meaning. We hope that the interdisciplinary account of polysemy provided in this survey inspires further fundamental research on the nature of polysemy and better equips applied research to deal with the complexity surrounding the phenomenon, for example, by enabling the development of benchmarks and testing paradigms for large language models informed by a greater portion of the rich evidence on the phenomenon currently available.</abstract>
      <pages>351–417</pages>
      <url hash="2b6ed76e">2024.cl-1.10</url>
      <bibkey>haber-poesio-2024-polysemy</bibkey>
    </paper>
  </volume>
  <volume id="2" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 2 - June 2023</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>2</journal-issue>
    </meta>
    <paper id="1">
      <title>Assessing the Cross-linguistic Utility of <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Shira</first><last>Wein</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <doi>10.1162/coli_a_00503</doi>
      <abstract>Semantic representations capture the meaning of a text. Abstract Meaning Representation (AMR), a type of semantic representation, focuses on predicate-argument structure and abstracts away from surface form. Though AMR was developed initially for English, it has now been adapted to a multitude of languages in the form of non-English annotation schemas, cross-lingual text-to-AMR parsing, and AMR-to-(non-English) text generation. We advance prior work on cross-lingual AMR by thoroughly investigating the amount, types, and causes of differences that appear in AMRs of different languages. Further, we compare how AMR captures meaning in cross-lingual pairs versus strings, and show that AMR graphs are able to draw out fine-grained differences between parallel sentences. We explore three primary research questions: (1) What are the types and causes of differences in parallel AMRs? (2) How can we measure the amount of difference between AMR pairs in different languages? (3) Given that AMR structure is affected by language and exhibits cross-lingual differences, how do cross-lingual AMR pairs compare to string-based representations of cross-lingual sentence pairs? We find that the source language itself does have a measurable impact on AMR structure, and that translation divergences and annotator choices also lead to differences in cross-lingual AMR pairs. We explore the implications of this finding throughout our study, concluding that, although AMR is useful to capture meaning across languages, evaluations need to take into account source language influences if they are to paint an accurate picture of system output, and meaning generally.</abstract>
      <pages>419–473</pages>
      <url hash="7b8c94ac">2024.cl-2.1</url>
      <bibkey>wein-schneider-2024-assessing</bibkey>
    </paper>
    <paper id="2">
      <title>Context-aware Transliteration of <fixed-case>R</fixed-case>omanized <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>Cibu</first><last>Johny</last></author>
      <author><first>Anna</first><last>Katanova</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <doi>10.1162/coli_a_00510</doi>
      <abstract>While most transliteration research is focused on single tokens such as named entities—for example, transliteration of from the Gujarati script to the Latin script “Ahmedabad” footnoteThe most populous city in the Indian state of Gujarat. the informal romanization prevalent in South Asia and elsewhere often requires transliteration of full sentences. The lack of large parallel text collections of full sentence (as opposed to single word) transliterations necessitates incorporation of contextual information into transliteration via non-parallel resources, such as via mono-script text collections. In this article, we present a number of methods for improving transliteration in context for such a use scenario. Some of these methods in fact improve performance without making use of sentential context, allowing for better quantification of the degree to which contextual information in particular is responsible for system improvements. Our final systems, which ultimately rely upon ensembles including large pretrained language models fine-tuned on simulated parallel data, yield substantial improvements over the best previously reported results for full sentence transliteration from Latin to native script on all 12 languages in the Dakshina dataset (Roark et al. 2020), with an overall 3.3% absolute (18.6% relative) mean word-error rate reduction.</abstract>
      <pages>475–534</pages>
      <url hash="51e628c3">2024.cl-2.2</url>
      <bibkey>kirov-etal-2024-context</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>UG</fixed-case>-schematic Annotation for Event Nominals: A Case Study in <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wenxi</first><last>Li</last></author>
      <author><first>Yutong</first><last>Zhang</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <doi>10.1162/coli_a_00504</doi>
      <abstract>Divergence of languages observed at the surface level is a major challenge encountered by multilingual data representation, especially when typologically distant languages are involved. Drawing inspiration from a formalist Chomskyan perspective towards language universals, Universal Grammar (UG), this article uses deductively pre-defined universals to analyze a multilingually heterogeneous phenomenon, event nominals. In this way, deeper universality of event nominals beneath their huge divergence in different languages is uncovered, which empowers us to break barriers between languages and thus extend insights from some synthetic languages to a non-inflectional language, Mandarin Chinese. Our empirical investigation also demonstrates this UG-inspired schema is effective: With its assistance, the inter-annotator agreement (IAA) for identifying event nominals in Mandarin grows from 88.02% to 94.99%, and automatic detection of event-reading nominalizations on the newly-established data achieves an accuracy of 94.76% and an F1 score of 91.3%, which significantly surpass those achieved on the pre-existing resource by 9.8% and 5.2%, respectively. Our systematic analysis also sheds light on nominal semantic role labeling. By providing a clear definition and classification on arguments of event nominal, the IAA of this task significantly increases from 90.46% to 98.04%.</abstract>
      <pages>535–561</pages>
      <url hash="b3a3104e">2024.cl-2.3</url>
      <bibkey>li-etal-2024-ug</bibkey>
    </paper>
    <paper id="4">
      <title>A <fixed-case>B</fixed-case>ayesian Approach to Uncertainty in Word Embedding Bias Estimation</title>
      <author><first>Alicja</first><last>Dobrzeniecka</last></author>
      <author><first>Rafal</first><last>Urbaniak</last></author>
      <doi>10.1162/coli_a_00507</doi>
      <abstract>Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of bias present in word embeddings in terms of a single-number metric. However, such metrics and the related statistical significance calculations rely on treating pre-averaged data as individual data points and utilizing bootstrapping techniques with low sample sizes. We show that similar results can be easily obtained using such methods even if the data are generated by a null model lacking the intended bias. Consequently, we argue that this approach generates false confidence. To address this issue, we propose a Bayesian alternative: hierarchical Bayesian modeling, which enables a more uncertainty-sensitive inspection of bias in word embeddings at different levels of granularity. To showcase our method, we apply it to Religion, Gender, and Race word lists from the original research, together with our control neutral word lists. We deploy the method using Google, GloVe, and Reddit embeddings. Further, we utilize our approach to evaluate a debiasing technique applied to the Reddit word embedding. Our findings reveal a more complex landscape than suggested by the proponents of single-number metrics. The datasets and source code for the paper are publicly available.1</abstract>
      <pages>563–617</pages>
      <url hash="a4cb9506">2024.cl-2.4</url>
      <bibkey>dobrzeniecka-urbaniak-2024-bayesian</bibkey>
    </paper>
    <paper id="5">
      <title>Topics in the Haystack: Enhancing Topic Quality through Corpus Expansion</title>
      <author><first>Anton</first><last>Thielmann</last></author>
      <author><first>Arik</first><last>Reuter</last></author>
      <author><first>Quentin</first><last>Seifert</last></author>
      <author><first>Elisabeth</first><last>Bergherr</last></author>
      <author><first>Benjamin</first><last>Säfken</last></author>
      <doi>10.1162/coli_a_00506</doi>
      <abstract>Extracting and identifying latent topics in large text corpora have gained increasing importance in Natural Language Processing (NLP). Most models, whether probabilistic models similar to Latent Dirichlet Allocation (LDA) or neural topic models, follow the same underlying approach of topic interpretability and topic extraction. We propose a method that incorporates a deeper understanding of both sentence and document themes, and goes beyond simply analyzing word frequencies in the data. Through simple corpus expansion, our model can detect latent topics that may include uncommon words or neologisms, as well as words not present in the documents themselves. Additionally, we propose several new evaluation metrics based on intruder words and similarity measures in the semantic space. We present correlation coefficients with human identification of intruder words and achieve near-human level results at the word-intrusion task. We demonstrate the competitive performance of our method with a large benchmark study, and achieve superior results compared with state-of-the-art topic modeling and document clustering models. The code is available at the following link: https://github.com/AnFreTh/STREAM.</abstract>
      <pages>619–655</pages>
      <url hash="0dcc9b5a">2024.cl-2.5</url>
      <bibkey>thielmann-etal-2024-topics</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Faithful Model Explanation in <fixed-case>NLP</fixed-case>: A Survey</title>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <doi>10.1162/coli_a_00511</doi>
      <abstract>End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.</abstract>
      <pages>657–723</pages>
      <url hash="3f0e150f">2024.cl-2.6</url>
      <bibkey>lyu-etal-2024-towards</bibkey>
    </paper>
    <paper id="7">
      <title>A Systematic Review of Computational Approaches to Deciphering Bronze Age Aegean and <fixed-case>C</fixed-case>ypriot Scripts</title>
      <author><first>Maja</first><last>Braović</last></author>
      <author><first>Damir</first><last>Krstinić</last></author>
      <author><first>Maja</first><last>Štula</last></author>
      <author><first>Antonia</first><last>Ivanda</last></author>
      <doi>10.1162/coli_a_00514</doi>
      <abstract>This article provides a detailed insight into computational approaches for deciphering Bronze Age Aegean and Cypriot scripts, namely, the Archanes script and the Archanes formula, Phaistos Disk, Cretan hieroglyphic (including the Malia Altar Stone and Arkalochori Axe), Linear A, Linear B, Cypro-Minoan, and Cypriot scripts. The unique contributions of this article are threefold: (1) a thorough review of major Bronze Age Aegean and Cypriot scripts and inscriptions, digital data and corpora associated with them, existing computational decipherment methods developed in order to decipher them, and possible links to other scripts and languages; (2) the definition of 15 major challenges that can be encountered in computational decipherments of ancient scripts; and (3) an outline of a computational model that could possibly be used to simulate traditional decipherment processes of ancient scripts based on palaeography and epigraphy. In the context of this article the term decipherment denotes the process of discovery of the language and/or the set of symbols behind an unknown script, and the meaning behind it.</abstract>
      <pages>725–779</pages>
      <url hash="ae407bd7">2024.cl-2.7</url>
      <bibkey>braovic-etal-2024-systematic</bibkey>
    </paper>
    <paper id="8">
      <title>The Role of Typological Feature Prediction in <fixed-case>NLP</fixed-case> and Linguistics</title>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <doi>10.1162/coli_a_00498</doi>
      <abstract>Computational typology has gained traction in the field of Natural Language Processing (NLP) in recent years, as evidenced by the increasing number of papers on the topic and the establishment of a Special Interest Group on the topic (SIGTYP), including the organization of successful workshops and shared tasks. A considerable amount of work in this sub-field is concerned with prediction of typological features, for example, for databases such as the World Atlas of Language Structures (WALS) or Grambank. Prediction is argued to be useful either because (1) it allows for obtaining feature values for relatively undocumented languages, alleviating the sparseness in WALS, in turn argued to be useful for both NLP and linguistics; and (2) it allows us to probe models to see whether or not these typological features are encapsulated in, for example, language representations. In this article, we present a critical stance concerning prediction of typological features, investigating to what extent this line of research is aligned with purported needs—both from the perspective of NLP practitioners, and perhaps more importantly, from the perspective of linguists specialized in typology and language documentation. We provide evidence that this line of research in its current state suffers from a lack of interdisciplinary alignment. Based on an extensive survey of the linguistic typology community, we present concrete recommendations for future research in order to improve this alignment between linguists and NLP researchers, beyond the scope of typological feature prediction.</abstract>
      <pages>781–794</pages>
      <url hash="c7cdf8f4">2024.cl-2.8</url>
      <bibkey>bjerva-2024-role</bibkey>
    </paper>
    <paper id="9">
      <title>Common Flaws in Running Human Evaluation Experiments in <fixed-case>NLP</fixed-case></title>
      <author><first>Craig</first><last>Thomson</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <doi>10.1162/coli_a_00508</doi>
      <abstract>While conducting a coordinated set of repeat runs of human evaluation experiments in NLP, we discovered flaws in every single experiment we selected for inclusion via a systematic process. In this squib, we describe the types of flaws we discovered, which include coding errors (e.g., loading the wrong system outputs to evaluate), failure to follow standard scientific practice (e.g., ad hoc exclusion of participants and responses), and mistakes in reported numerical results (e.g., reported numbers not matching experimental data). If these problems are widespread, it would have worrying implications for the rigor of NLP evaluation experiments as currently conducted. We discuss what researchers can do to reduce the occurrence of such flaws, including pre-registration, better code development practices, increased testing and piloting, and post-publication addressing of errors.</abstract>
      <pages>795–805</pages>
      <url hash="2c6297cf">2024.cl-2.9</url>
      <bibkey>thomson-etal-2024-common</bibkey>
    </paper>
    <paper id="10">
      <title>The Pitfalls of Defining Hallucination</title>
      <author><first>Kees van</first><last>Deemter</last></author>
      <doi>10.1162/coli_a_00509</doi>
      <abstract>Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs.</abstract>
      <pages>807–816</pages>
      <url hash="1638361a">2024.cl-2.10</url>
      <bibkey>deemter-2024-pitfalls</bibkey>
    </paper>
  </volume>
</collection>
