<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.cl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 1 - March 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>1</journal-issue>
    </meta>
    <paper id="1">
      <title>My Big, Fat 50-Year Journey</title>
      <author><first>Martha</first><last>Palmer</last></author>
      <doi>10.1162/coli_a_00499</doi>
      <abstract>My most heartfelt thanks to ACL for this tremendous honor. I’m completely thrilled. I cannot tell you how surprised I was when I got Iryna’s email. It is amazing that my first ACL conference since 2019 in Florence includes this award. What a wonderful way to be back with all of my friends and family here at ACL. I’m going to tell you about my big fat 50-year journey. What have I been doing for the last 50 years? Well, finding meaning, quite literally in words. Or in other words, exploring how computational lexical semantics can support natural language understanding. This is going to be quick. Hold onto your hats, here we go.</abstract>
      <pages>1–24</pages>
      <url hash="350741f4">2024.cl-1.1</url>
      <bibkey>palmer-2024-big</bibkey>
    </paper>
    <paper id="2">
      <title>Rethinking the Exploitation of Monolingual Data for Low-Resource Neural Machine Translation</title>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Derek Fai</first><last>Wong</last></author>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <doi>10.1162/coli_a_00496</doi>
      <abstract>The utilization of monolingual data has been shown to be a promising strategy for addressing low-resource machine translation problems. Previous studies have demonstrated the effectiveness of techniques such as back-translation and self-supervised objectives, including masked language modeling, causal language modeling, and denoise autoencoding, in improving the performance of machine translation models. However, the manner in which these methods contribute to the success of machine translation tasks and how they can be effectively combined remains an under-researched area. In this study, we carry out a systematic investigation of the effects of these techniques on linguistic properties through the use of probing tasks, including source language comprehension, bilingual word alignment, and translation fluency. We further evaluate the impact of pre-training, back-translation, and multi-task learning on bitexts of varying sizes. Our findings inform the design of more effective pipelines for leveraging monolingual data in extremely low-resource and low-resource machine translation tasks. Experiment results show consistent performance gains in seven translation directions, which provide further support for our conclusions and understanding of the role of monolingual data in machine translation.</abstract>
      <pages>25–47</pages>
      <url hash="97fc3d67">2024.cl-1.2</url>
      <bibkey>pang-etal-2024-rethinking</bibkey>
    </paper>
    <paper id="3">
      <title>How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models</title>
      <author><first>Mark</first><last>Ormerod</last></author>
      <author><first>Jesús Martínez</first><last>del Rincón</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <doi>10.1162/coli_a_00495</doi>
      <abstract>Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., KITCHEN CHAIR: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.</abstract>
      <pages>49–81</pages>
      <url hash="0aff206a">2024.cl-1.3</url>
      <bibkey>ormerod-etal-2024-kitchen</bibkey>
    </paper>
    <paper id="4">
      <title>Universal Generation for <fixed-case>O</fixed-case>ptimality <fixed-case>T</fixed-case>heory Is <fixed-case>PSPACE</fixed-case>-Complete</title>
      <author><first>Sophie</first><last>Hao</last></author>
      <doi>10.1162/coli_a_00494</doi>
      <abstract>This article shows that the universal generation problem for Optimality Theory (OT) is PSPACE-complete. While prior work has shown that universal generation is at least NP-hard and at most EXPSPACE-hard, our results place universal generation in between those two classes, assuming that NP ≠ PSPACE. We additionally show that when the number of constraints is bounded in advance, universal generation is at least NL-hard and at most NPNP-hard. Our proofs rely on a close connection between OT and the intersection non-emptiness problem for finite automata, which is PSPACE-complete in general and NL-complete when the number of automata is bounded. Our analysis shows that constraint interaction is the main contributor to the complexity of OT: The ability to factor transformations into simple, interacting constraints allows OT to furnish compact descriptions of intricate phonological phenomena.</abstract>
      <pages>83–117</pages>
      <url hash="59b66d33">2024.cl-1.4</url>
      <bibkey>hao-2024-universal</bibkey>
    </paper>
    <paper id="5">
      <title>Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering</title>
      <author><first>Akshay</first><last>Chaturvedi</last></author>
      <author><first>Swarnadeep</first><last>Bhar</last></author>
      <author><first>Soumadeep</first><last>Saha</last></author>
      <author><first>Utpal</first><last>Garain</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <doi>10.1162/coli_a_00493</doi>
      <abstract>Transformer-based language models have been shown to be highly effective for several NLP tasks. In this article, we consider three transformer models, BERT, RoBERTa, and XLNet, in both small and large versions, and investigate how faithful their representations are with respect to the semantic content of texts. We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally figure in a model’s inferences in question answering. We then test this notion by observing a model’s behavior on answering questions about a story after performing two novel semantic interventions—deletion intervention and negation intervention. While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (∼ 50% for deletion intervention, and ∼ 20% drop in accuracy for negation intervention). We then propose an intervention-based training regime that can mitigate the undesirable effects for deletion intervention by a significant margin (from ∼ 50% to ∼ 6%). We analyze the inner-workings of the models to better understand the effectiveness of intervention-based training for deletion intervention. But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models’ inability to deal with negation intervention or to capture the predicate–argument structure of texts. We also test InstructGPT, via prompting, for its ability to handle the two interventions and to capture predicate–argument structure. While InstructGPT models do achieve very high performance on predicate–argument structure task, they fail to respond adequately to our deletion and negation interventions.</abstract>
      <pages>119–155</pages>
      <url hash="1e92522f">2024.cl-1.5</url>
      <bibkey>chaturvedi-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="6">
      <title>On the Role of Morphological Information for Contextual Lemmatization</title>
      <author><first>Olia</first><last>Toporkov</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <doi>10.1162/coli_a_00497</doi>
      <abstract>Lemmatization is a natural language processing (NLP) task that consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this article we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish, and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, which constitutes, after all, their most common application use. The results of our study are rather surprising. It turns out that providing lemmatizers with fine-grained morphological features during training is not that beneficial, not even for agglutinative languages. In fact, modern contextual word representations seem to implicitly encode enough morphological information to obtain competitive contextual lemmatizers without seeing any explicit morphological signal. Moreover, our experiments suggest that the best lemmatizers out-of-domain are those using simple UPOS tags or those trained without morphology and, lastly, that current evaluation practices for lemmatization are not adequate to clearly discriminate between models.</abstract>
      <pages>157–191</pages>
      <url hash="c1b837bb">2024.cl-1.6</url>
      <bibkey>toporkov-agerri-2024-role</bibkey>
    </paper>
    <paper id="7">
      <title>Stance Detection with Explanations</title>
      <author><first>Rudra Ranajee</first><last>Saha</last></author>
      <author><first>Laks V. S.</first><last>Lakshmanan</last></author>
      <author><first>Raymond T.</first><last>Ng</last></author>
      <doi>10.1162/coli_a_00501</doi>
      <abstract>Identification of stance has recently gained a lot of attention with the extreme growth of fake news and filter bubbles. Over the last decade, many feature-based and deep-learning approaches have been proposed to solve stance detection. However, almost none of the existing works focus on providing a meaningful explanation for their prediction. In this work, we study stance detection with an emphasis on generating explanations for the predicted stance by capturing the pivotal argumentative structure embedded in a document. We propose to build a stance tree that utilizes rhetorical parsing to construct an evidence tree and to use Dempster Shafer Theory to aggregate the evidence. Human studies show that our unsupervised technique of generating stance explanations outperforms the SOTA extractive summarization method in terms of informativeness, non-redundancy, coverage, and overall quality. Furthermore, experiments show that our explanation-based stance prediction excels or matches the performance of the SOTA model on various benchmark datasets.</abstract>
      <pages>193–235</pages>
      <url hash="8243fb1a">2024.cl-1.7</url>
      <bibkey>saha-etal-2024-stance</bibkey>
    </paper>
    <paper id="8">
      <title>Can Large Language Models Transform Computational Social Science?</title>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>William</first><last>Held</last></author>
      <author><first>Omar</first><last>Shaikh</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Zhehao</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <doi>10.1162/coli_a_00502</doi>
      <abstract>Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.</abstract>
      <pages>237–291</pages>
      <url hash="47b4f1d0">2024.cl-1.8</url>
      <bibkey>ziems-etal-2024-large</bibkey>
    </paper>
    <paper id="9">
      <title>Language Model Behavior: A Comprehensive Survey</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin K.</first><last>Bergen</last></author>
      <doi>10.1162/coli_a_00492</doi>
      <abstract>Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.</abstract>
      <pages>293–350</pages>
      <url hash="c1447504">2024.cl-1.9</url>
      <bibkey>chang-bergen-2024-language</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>P</fixed-case>olysemy—<fixed-case>E</fixed-case>vidence from Linguistics, Behavioral Science, and Contextualized Language Models</title>
      <author><first>Janosch</first><last>Haber</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <doi>10.1162/coli_a_00500</doi>
      <abstract>Polysemy is the type of lexical ambiguity where a word has multiple distinct but related interpretations. In the past decade, it has been the subject of a great many studies across multiple disciplines including linguistics, psychology, neuroscience, and computational linguistics, which have made it increasingly clear that the complexity of polysemy precludes simple, universal answers, especially concerning the representation and processing of polysemous words. But fuelled by the growing availability of large, crowdsourced datasets providing substantial empirical evidence; improved behavioral methodology; and the development of contextualized language models capable of encoding the fine-grained meaning of a word within a given context, the literature on polysemy recently has developed more complex theoretical analyses. In this survey we discuss these recent contributions to the investigation of polysemy against the backdrop of a long legacy of research across multiple decades and disciplines. Our aim is to bring together different perspectives to achieve a more complete picture of the heterogeneity and complexity of the phenomenon of polysemy. Specifically, we highlight evidence supporting a range of hybrid models of the mental processing of polysemes. These hybrid models combine elements from different previous theoretical approaches to explain patterns and idiosyncrasies in the processing of polysemous that the best known models so far have failed to account for. Our literature review finds that (i) traditional analyses of polysemy can be limited in their generalizability by loose definitions and selective materials; (ii) linguistic tests provide useful evidence on individual cases, but fail to capture the full range of factors involved in the processing of polysemous sense extensions; and (iii) recent behavioral (psycho) linguistics studies, large-scale annotation efforts, and investigations leveraging contextualized language models provide accumulating evidence suggesting that polysemous sense similarity covers a wide spectrum between identity of sense and homonymy-like unrelatedness of meaning. We hope that the interdisciplinary account of polysemy provided in this survey inspires further fundamental research on the nature of polysemy and better equips applied research to deal with the complexity surrounding the phenomenon, for example, by enabling the development of benchmarks and testing paradigms for large language models informed by a greater portion of the rich evidence on the phenomenon currently available.</abstract>
      <pages>351–417</pages>
      <url hash="2b6ed76e">2024.cl-1.10</url>
      <bibkey>haber-poesio-2024-polysemy</bibkey>
    </paper>
  </volume>
  <volume id="2" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 2 - June 2023</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>2</journal-issue>
    </meta>
    <paper id="1">
      <title>Assessing the Cross-linguistic Utility of <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Shira</first><last>Wein</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <doi>10.1162/coli_a_00503</doi>
      <abstract>Semantic representations capture the meaning of a text. Abstract Meaning Representation (AMR), a type of semantic representation, focuses on predicate-argument structure and abstracts away from surface form. Though AMR was developed initially for English, it has now been adapted to a multitude of languages in the form of non-English annotation schemas, cross-lingual text-to-AMR parsing, and AMR-to-(non-English) text generation. We advance prior work on cross-lingual AMR by thoroughly investigating the amount, types, and causes of differences that appear in AMRs of different languages. Further, we compare how AMR captures meaning in cross-lingual pairs versus strings, and show that AMR graphs are able to draw out fine-grained differences between parallel sentences. We explore three primary research questions: (1) What are the types and causes of differences in parallel AMRs? (2) How can we measure the amount of difference between AMR pairs in different languages? (3) Given that AMR structure is affected by language and exhibits cross-lingual differences, how do cross-lingual AMR pairs compare to string-based representations of cross-lingual sentence pairs? We find that the source language itself does have a measurable impact on AMR structure, and that translation divergences and annotator choices also lead to differences in cross-lingual AMR pairs. We explore the implications of this finding throughout our study, concluding that, although AMR is useful to capture meaning across languages, evaluations need to take into account source language influences if they are to paint an accurate picture of system output, and meaning generally.</abstract>
      <pages>419–473</pages>
      <url hash="7b8c94ac">2024.cl-2.1</url>
      <bibkey>wein-schneider-2024-assessing</bibkey>
    </paper>
    <paper id="2">
      <title>Context-aware Transliteration of <fixed-case>R</fixed-case>omanized <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>Cibu</first><last>Johny</last></author>
      <author><first>Anna</first><last>Katanova</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <doi>10.1162/coli_a_00510</doi>
      <abstract>While most transliteration research is focused on single tokens such as named entities—for example, transliteration of from the Gujarati script to the Latin script “Ahmedabad” footnoteThe most populous city in the Indian state of Gujarat. the informal romanization prevalent in South Asia and elsewhere often requires transliteration of full sentences. The lack of large parallel text collections of full sentence (as opposed to single word) transliterations necessitates incorporation of contextual information into transliteration via non-parallel resources, such as via mono-script text collections. In this article, we present a number of methods for improving transliteration in context for such a use scenario. Some of these methods in fact improve performance without making use of sentential context, allowing for better quantification of the degree to which contextual information in particular is responsible for system improvements. Our final systems, which ultimately rely upon ensembles including large pretrained language models fine-tuned on simulated parallel data, yield substantial improvements over the best previously reported results for full sentence transliteration from Latin to native script on all 12 languages in the Dakshina dataset (Roark et al. 2020), with an overall 3.3% absolute (18.6% relative) mean word-error rate reduction.</abstract>
      <pages>475–534</pages>
      <url hash="51e628c3">2024.cl-2.2</url>
      <bibkey>kirov-etal-2024-context</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>UG</fixed-case>-schematic Annotation for Event Nominals: A Case Study in <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wenxi</first><last>Li</last></author>
      <author><first>Yutong</first><last>Zhang</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <doi>10.1162/coli_a_00504</doi>
      <abstract>Divergence of languages observed at the surface level is a major challenge encountered by multilingual data representation, especially when typologically distant languages are involved. Drawing inspiration from a formalist Chomskyan perspective towards language universals, Universal Grammar (UG), this article uses deductively pre-defined universals to analyze a multilingually heterogeneous phenomenon, event nominals. In this way, deeper universality of event nominals beneath their huge divergence in different languages is uncovered, which empowers us to break barriers between languages and thus extend insights from some synthetic languages to a non-inflectional language, Mandarin Chinese. Our empirical investigation also demonstrates this UG-inspired schema is effective: With its assistance, the inter-annotator agreement (IAA) for identifying event nominals in Mandarin grows from 88.02% to 94.99%, and automatic detection of event-reading nominalizations on the newly-established data achieves an accuracy of 94.76% and an F1 score of 91.3%, which significantly surpass those achieved on the pre-existing resource by 9.8% and 5.2%, respectively. Our systematic analysis also sheds light on nominal semantic role labeling. By providing a clear definition and classification on arguments of event nominal, the IAA of this task significantly increases from 90.46% to 98.04%.</abstract>
      <pages>535–561</pages>
      <url hash="b3a3104e">2024.cl-2.3</url>
      <bibkey>li-etal-2024-ug</bibkey>
    </paper>
    <paper id="4">
      <title>A <fixed-case>B</fixed-case>ayesian Approach to Uncertainty in Word Embedding Bias Estimation</title>
      <author><first>Alicja</first><last>Dobrzeniecka</last></author>
      <author><first>Rafal</first><last>Urbaniak</last></author>
      <doi>10.1162/coli_a_00507</doi>
      <abstract>Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of bias present in word embeddings in terms of a single-number metric. However, such metrics and the related statistical significance calculations rely on treating pre-averaged data as individual data points and utilizing bootstrapping techniques with low sample sizes. We show that similar results can be easily obtained using such methods even if the data are generated by a null model lacking the intended bias. Consequently, we argue that this approach generates false confidence. To address this issue, we propose a Bayesian alternative: hierarchical Bayesian modeling, which enables a more uncertainty-sensitive inspection of bias in word embeddings at different levels of granularity. To showcase our method, we apply it to Religion, Gender, and Race word lists from the original research, together with our control neutral word lists. We deploy the method using Google, GloVe, and Reddit embeddings. Further, we utilize our approach to evaluate a debiasing technique applied to the Reddit word embedding. Our findings reveal a more complex landscape than suggested by the proponents of single-number metrics. The datasets and source code for the paper are publicly available.1</abstract>
      <pages>563–617</pages>
      <url hash="a4cb9506">2024.cl-2.4</url>
      <bibkey>dobrzeniecka-urbaniak-2024-bayesian</bibkey>
    </paper>
    <paper id="5">
      <title>Topics in the Haystack: Enhancing Topic Quality through Corpus Expansion</title>
      <author><first>Anton</first><last>Thielmann</last></author>
      <author><first>Arik</first><last>Reuter</last></author>
      <author><first>Quentin</first><last>Seifert</last></author>
      <author><first>Elisabeth</first><last>Bergherr</last></author>
      <author><first>Benjamin</first><last>Säfken</last></author>
      <doi>10.1162/coli_a_00506</doi>
      <abstract>Extracting and identifying latent topics in large text corpora have gained increasing importance in Natural Language Processing (NLP). Most models, whether probabilistic models similar to Latent Dirichlet Allocation (LDA) or neural topic models, follow the same underlying approach of topic interpretability and topic extraction. We propose a method that incorporates a deeper understanding of both sentence and document themes, and goes beyond simply analyzing word frequencies in the data. Through simple corpus expansion, our model can detect latent topics that may include uncommon words or neologisms, as well as words not present in the documents themselves. Additionally, we propose several new evaluation metrics based on intruder words and similarity measures in the semantic space. We present correlation coefficients with human identification of intruder words and achieve near-human level results at the word-intrusion task. We demonstrate the competitive performance of our method with a large benchmark study, and achieve superior results compared with state-of-the-art topic modeling and document clustering models. The code is available at the following link: https://github.com/AnFreTh/STREAM.</abstract>
      <pages>619–655</pages>
      <url hash="0dcc9b5a">2024.cl-2.5</url>
      <bibkey>thielmann-etal-2024-topics</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Faithful Model Explanation in <fixed-case>NLP</fixed-case>: A Survey</title>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <doi>10.1162/coli_a_00511</doi>
      <abstract>End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.</abstract>
      <pages>657–723</pages>
      <url hash="3f0e150f">2024.cl-2.6</url>
      <bibkey>lyu-etal-2024-towards</bibkey>
    </paper>
    <paper id="7">
      <title>A Systematic Review of Computational Approaches to Deciphering Bronze Age Aegean and <fixed-case>C</fixed-case>ypriot Scripts</title>
      <author><first>Maja</first><last>Braović</last></author>
      <author><first>Damir</first><last>Krstinić</last></author>
      <author><first>Maja</first><last>Štula</last></author>
      <author><first>Antonia</first><last>Ivanda</last></author>
      <doi>10.1162/coli_a_00514</doi>
      <abstract>This article provides a detailed insight into computational approaches for deciphering Bronze Age Aegean and Cypriot scripts, namely, the Archanes script and the Archanes formula, Phaistos Disk, Cretan hieroglyphic (including the Malia Altar Stone and Arkalochori Axe), Linear A, Linear B, Cypro-Minoan, and Cypriot scripts. The unique contributions of this article are threefold: (1) a thorough review of major Bronze Age Aegean and Cypriot scripts and inscriptions, digital data and corpora associated with them, existing computational decipherment methods developed in order to decipher them, and possible links to other scripts and languages; (2) the definition of 15 major challenges that can be encountered in computational decipherments of ancient scripts; and (3) an outline of a computational model that could possibly be used to simulate traditional decipherment processes of ancient scripts based on palaeography and epigraphy. In the context of this article the term decipherment denotes the process of discovery of the language and/or the set of symbols behind an unknown script, and the meaning behind it.</abstract>
      <pages>725–779</pages>
      <url hash="ae407bd7">2024.cl-2.7</url>
      <bibkey>braovic-etal-2024-systematic</bibkey>
    </paper>
    <paper id="8">
      <title>The Role of Typological Feature Prediction in <fixed-case>NLP</fixed-case> and Linguistics</title>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <doi>10.1162/coli_a_00498</doi>
      <abstract>Computational typology has gained traction in the field of Natural Language Processing (NLP) in recent years, as evidenced by the increasing number of papers on the topic and the establishment of a Special Interest Group on the topic (SIGTYP), including the organization of successful workshops and shared tasks. A considerable amount of work in this sub-field is concerned with prediction of typological features, for example, for databases such as the World Atlas of Language Structures (WALS) or Grambank. Prediction is argued to be useful either because (1) it allows for obtaining feature values for relatively undocumented languages, alleviating the sparseness in WALS, in turn argued to be useful for both NLP and linguistics; and (2) it allows us to probe models to see whether or not these typological features are encapsulated in, for example, language representations. In this article, we present a critical stance concerning prediction of typological features, investigating to what extent this line of research is aligned with purported needs—both from the perspective of NLP practitioners, and perhaps more importantly, from the perspective of linguists specialized in typology and language documentation. We provide evidence that this line of research in its current state suffers from a lack of interdisciplinary alignment. Based on an extensive survey of the linguistic typology community, we present concrete recommendations for future research in order to improve this alignment between linguists and NLP researchers, beyond the scope of typological feature prediction.</abstract>
      <pages>781–794</pages>
      <url hash="c7cdf8f4">2024.cl-2.8</url>
      <bibkey>bjerva-2024-role</bibkey>
    </paper>
    <paper id="9">
      <title>Common Flaws in Running Human Evaluation Experiments in <fixed-case>NLP</fixed-case></title>
      <author><first>Craig</first><last>Thomson</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <doi>10.1162/coli_a_00508</doi>
      <abstract>While conducting a coordinated set of repeat runs of human evaluation experiments in NLP, we discovered flaws in every single experiment we selected for inclusion via a systematic process. In this squib, we describe the types of flaws we discovered, which include coding errors (e.g., loading the wrong system outputs to evaluate), failure to follow standard scientific practice (e.g., ad hoc exclusion of participants and responses), and mistakes in reported numerical results (e.g., reported numbers not matching experimental data). If these problems are widespread, it would have worrying implications for the rigor of NLP evaluation experiments as currently conducted. We discuss what researchers can do to reduce the occurrence of such flaws, including pre-registration, better code development practices, increased testing and piloting, and post-publication addressing of errors.</abstract>
      <pages>795–805</pages>
      <url hash="2c6297cf">2024.cl-2.9</url>
      <bibkey>thomson-etal-2024-common</bibkey>
    </paper>
    <paper id="10">
      <title>The Pitfalls of Defining Hallucination</title>
      <author><first>Kees</first><last>van Deemter</last></author>
      <doi>10.1162/coli_a_00509</doi>
      <abstract>Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs.</abstract>
      <pages>807–816</pages>
      <url hash="1638361a">2024.cl-2.10</url>
      <bibkey>deemter-2024-pitfalls</bibkey>
    </paper>
  </volume>
  <volume id="3" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 3 - September 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>September</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>3</journal-issue>
    </meta>
    <paper id="1">
      <title>Analyzing Dataset Annotation Quality Management in the Wild</title>
      <author><first>Jan-Christoph</first><last>Klie</last></author>
      <author><first>Richard</first><last>Eckart de Castilho</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00516</doi>
      <abstract>Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models as well as for their correct evaluation. Recent work, however, has shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, biases, or artifacts. While practices and guidelines regarding dataset creation projects exist, to our knowledge, large-scale analysis has yet to be performed on how quality management is conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions for applying them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication, or data validation. Using these annotations, we then analyze how quality management is conducted in practice. A majority of the annotated publications apply good or excellent quality management. However, we deem the effort of 30% of the studies as only subpar. Our analysis also shows common errors, especially when using inter-annotator agreement and computing annotation error rates.</abstract>
      <pages>817–866</pages>
      <url hash="4d72b557">2024.cl-3.1</url>
      <bibkey>klie-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>LLM</fixed-case>-Assisted Data Augmentation for <fixed-case>C</fixed-case>hinese Dialogue-Level Dependency Parsing</title>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Gongyao</first><last>Jiang</last></author>
      <author><first>Shuang</first><last>Liu</last></author>
      <author><first>Jing</first><last>Chen</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <doi>10.1162/coli_a_00515</doi>
      <abstract>Dialogue-level dependency parsing, despite its growing academic interest, often encounters underperformance issues due to resource shortages. A potential solution to this challenge is data augmentation. In recent years, large language models (LLMs) have demonstrated strong capabilities in generation, which can facilitate data augmentation greatly. In this study, we focus on Chinese dialogue-level dependency parsing, presenting three simple and effective strategies with LLM to augment the original training instances, namely word-level, syntax-level, and discourse-level augmentations, respectively. These strategies enable LLMs to either preserve or modify dependency structures, thereby assuring accuracy while increasing the diversity of instances at different levels. We conduct experiments on the benchmark dataset released by Jiang et al. (2023) to validate our approach. Results show that our method can greatly boost the parsing performance in various settings, particularly in dependencies among elementary discourse units. Lastly, we provide in-depth analysis to show the key points of our data augmentation strategies.</abstract>
      <pages>867–891</pages>
      <url hash="49ab2050">2024.cl-3.2</url>
      <bibkey>zhang-etal-2024-llm-assisted</bibkey>
    </paper>
    <paper id="3">
      <title>Aligning Human and Computational Coherence Evaluations</title>
      <author><first>Jia Peng</first><last>Lim</last></author>
      <author><first>Hady W.</first><last>Lauw</last></author>
      <doi>10.1162/coli_a_00518</doi>
      <abstract>Automated coherence metrics constitute an efficient and popular way to evaluate topic models. Previous work presents a mixed picture of their presumed correlation with human judgment. This work proposes a novel sampling approach to mining topic representations at a large scale while seeking to mitigate bias from sampling, enabling the investigation of widely used automated coherence metrics via large corpora. Additionally, this article proposes a novel user study design, an amalgamation of different proxy tasks, to derive a finer insight into the human decision-making processes. This design subsumes the purpose of simple rating and outlier-detection user studies. Similar to the sampling approach, the user study conducted is extensive, comprising 40 study participants split into eight different study groups tasked with evaluating their respective set of 100 topic representations. Usually, when substantiating the use of these metrics, human responses are treated as the gold standard. This article further investigates the reliability of human judgment by flipping the comparison and conducting a novel extended analysis of human response at the group and individual level against a generic corpus. The investigation results show a moderate to good correlation between these metrics and human judgment, especially for generic corpora, and derive further insights into the human perception of coherence. Analyzing inter-metric correlations across corpora shows moderate to good correlation among these metrics. As these metrics depend on corpus statistics, this article further investigates the topical differences between corpora, revealing nuances in applications of these metrics.</abstract>
      <pages>893–952</pages>
      <url hash="5e57cfa7">2024.cl-3.3</url>
      <bibkey>lim-lauw-2024-aligning</bibkey>
    </paper>
    <paper id="4">
      <title>Relation Extraction in Underexplored Biomedical Domains: A Diversity-optimized Sampling and Synthetic Data Generation Approach</title>
      <author><first>Maxime</first><last>Delmas</last></author>
      <author><first>Magdalena</first><last>Wysocka</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <doi>10.1162/coli_a_00520</doi>
      <abstract>The sparsity of labeled data is an obstacle to the development of Relation Extraction (RE) models and the completion of databases in various biomedical areas. While being of high interest in drug-discovery, the literature on natural products, reporting the identification of potential bioactive compounds from organisms, is a concrete example of such an overlooked topic. To mark the start of this new task, we created the first curated evaluation dataset and extracted literature items from the LOTUS database to build training sets. To this end, we developed a new sampler, inspired by diversity metrics in ecology, named Greedy Maximum Entropy sampler (https://github.com/idiap/gme-sampler). The strategic optimization of both balance and diversity of the selected items in the evaluation set is important given the resource-intensive nature of manual curation. After quantifying the noise in the training set, in the form of discrepancies between the text of input abstracts and the expected output labels, we explored different strategies accordingly. Framing the task as an end-to-end Relation Extraction, we evaluated the performance of standard fine-tuning (BioGPT, GPT-2, and Seq2rel) and few-shot learning with open Large Language Models (LLMs) (LLaMA 7B-65B). In addition to their evaluation in few-shot settings, we explore the potential of open LLMs as synthetic data generators and propose a new workflow for this purpose. All evaluated models exhibited substantial improvements when fine-tuned on synthetic abstracts rather than the original noisy data. We provide our best performing (F1-score = 59.0) BioGPT-Large model for end-to-end RE of natural products relationships along with all the training and evaluation datasets. See more details at https://github.com/idiap/abroad-re.</abstract>
      <pages>953–1000</pages>
      <url hash="9ce3cb73">2024.cl-3.4</url>
      <bibkey>delmas-etal-2024-relation</bibkey>
    </paper>
    <paper id="5">
      <title>Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation</title>
      <author><first>Ran</first><last>Zhang</last></author>
      <author><first>Jihed</first><last>Ouni</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <doi>10.1162/coli_a_00519</doi>
      <abstract>While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility and understanding. This article comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We (1) build the first CLCTS corpus with 328 instances for hDe-En (extended version with 455 instances) and 289 for hEn-De (extended version with 501 instances), leveraging historical fiction texts and Wikipedia summaries in English and German; (2) examine the effectiveness of popular transformer end-to-end models with different intermediate fine-tuning tasks; (3) explore the potential of GPT-3.5 as a summarizer; and (4) report evaluations from humans, GPT-4, and several recent automatic evaluation metrics. Our results indicate that intermediate task fine-tuned end-to-end models generate bad to moderate quality summaries while GPT-3.5, as a zero-shot summarizer, provides moderate to good quality outputs. GPT-3.5 also seems very adept at normalizing historical text. To assess data contamination in GPT-3.5, we design an adversarial attack scheme in which we find that GPT-3.5 performs slightly worse for unseen source documents compared to seen documents. Moreover, it sometimes hallucinates when the source sentences are inverted against its prior knowledge with a summarization accuracy of 0.67 for plot omission, 0.71 for entity swap, and 0.53 for plot negation. Overall, our regression results of model performances suggest that longer, older, and more complex source texts (all of which are more characteristic for historical language variants) are harder to summarize for all models, indicating the difficulty of the CLCTS task. Regarding evaluation, we observe that both the GPT-4 and BERTScore correlate moderately with human evaluations, implicating great potential for future improvement.</abstract>
      <pages>1001–1047</pages>
      <url hash="a4690640">2024.cl-3.5</url>
      <bibkey>zhang-etal-2024-cross</bibkey>
    </paper>
    <paper id="6">
      <title>Cognitive Plausibility in Natural Language Processing</title>
      <author><first>Yevgen</first><last>Matusevych</last></author>
      <doi>10.1162/coli_r_00517</doi>
      <pages>1049–1052</pages>
      <url hash="f2fbdf46">2024.cl-3.6</url>
      <bibkey>matusevych-2024-cognitive</bibkey>
    </paper>
    <paper id="7">
      <title>Large Language Model Instruction Following: A Survey of Progresses and Challenges</title>
      <author><first>Renze</first><last>Lou</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <doi>10.1162/coli_a_00523</doi>
      <abstract>Task semantics can be expressed by a set of input-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: First, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning to follow task instructions, that is, instruction following. Despite its impressive progress, there are some unsolved research equations that the community struggles with. This survey tries to summarize and provide insights into the current research on instruction following, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How should we model instructions? (iii) What are popular instruction following datasets and evaluation metrics? (iv) What factors influence and explain the instructions’ performance? (v) What challenges remain in instruction following? To our knowledge, this is the first comprehensive survey about instruction following.1</abstract>
      <pages>1053–1095</pages>
      <url hash="f3e80b64">2024.cl-3.7</url>
      <bibkey>lou-etal-2024-large</bibkey>
    </paper>
    <paper id="8">
      <title>Bias and Fairness in Large Language Models: A Survey</title>
      <author><first>Isabel O.</first><last>Gallegos</last></author>
      <author><first>Ryan A.</first><last>Rossi</last></author>
      <author><first>Joe</first><last>Barrow</last></author>
      <author><first>Md Mehrab</first><last>Tanjim</last></author>
      <author><first>Sungchul</first><last>Kim</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Tong</first><last>Yu</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Nesreen K.</first><last>Ahmed</last></author>
      <doi>10.1162/coli_a_00524</doi>
      <abstract>Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.</abstract>
      <pages>1097–1179</pages>
      <url hash="687f3ad2">2024.cl-3.8</url>
      <bibkey>gallegos-etal-2024-bias</bibkey>
    </paper>
    <paper id="10">
      <title>A Novel Alignment-based Approach for <fixed-case>PARSEVAL</fixed-case> Measures</title>
      <author><first>Eunkyul Leah</first><last>Jo</last></author>
      <author><first>Angela Yoonseo</first><last>Park</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <doi>10.1162/coli_a_00512</doi>
      <abstract>We propose a novel method for calculating PARSEVAL measures to evaluate constituent parsing results. Previous constituent parsing evaluation techniques were constrained by the requirement for consistent sentence boundaries and tokenization results, proving to be stringent and inconvenient. Our new approach handles constituent parsing results obtained from raw text, even when sentence boundaries and tokenization differ from the preprocessed gold sentence. Implementing this measure is our evaluation by alignment approach. The algorithm enables the alignment of tokens and sentences in the gold and system parse trees. Our proposed algorithm draws on the analogy of sentence and word alignment commonly used in machine translation (MT). To demonstrate the intricacy of calculations and clarify any integration of configurations, we explain the implementations in detailed pseudo-code and provide empirical proof for how sentence and word alignment can improve evaluation reliability.</abstract>
      <pages>1181–1190</pages>
      <url hash="658565b0">2024.cl-3.10</url>
      <bibkey>jo-etal-2024-novel</bibkey>
    </paper>
    <paper id="12">
      <title>Do Language Models’ Words Refer?</title>
      <author><first>Matthew</first><last>Mandelkern</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <doi>10.1162/coli_a_00522</doi>
      <abstract>What do language models (LMs) do with language? They can produce sequences of (mostly) coherent strings closely resembling English. But do those sentences mean something, or are LMs simply babbling in a convincing simulacrum of language use? We address one aspect of this broad question: whether LMs’ words can refer, that is, achieve “word-to-world” connections. There is prima facie reason to think they do not, since LMs do not interact with the world in the way that ordinary language users do. Drawing on the externalist tradition in philosophy of language, we argue that those appearances are misleading: Even if the inputs to LMs are simply strings of text, they are strings of text with natural histories, and that may suffice for LMs’ words to refer.</abstract>
      <pages>1191–1200</pages>
      <url hash="b83cf24c">2024.cl-3.12</url>
      <bibkey>mandelkern-linzen-2024-language</bibkey>
    </paper>
  </volume>
  <volume id="4" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 4 - December 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>December</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>3</journal-issue>
    </meta>
    <paper id="1">
      <title>Language Learning, Representation, and Processing in Humans and Machines: Introduction to the Special Issue</title>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <doi>10.1162/coli_e_00539</doi>
      <abstract>Large Language Models (LLMs) and humans acquire knowledge about language without direct supervision. LLMs do so by means of specific training objectives, while humans rely on sensory experience and social interaction. This parallelism has created a feeling in NLP and cognitive science that a systematic understanding of how LLMs acquire and use the encoded knowledge could provide useful insights for studying human cognition. Conversely, methods and findings from the field of cognitive science have occasionally inspired language model development. Yet, the differences in the way that language is processed by machines and humans—in terms of learning mechanisms, amounts of data used, grounding and access to different modalities—make a direct translation of insights challenging. The aim of this edited volume has been to create a forum of exchange and debate along this line of research, inviting contributions that further elucidate similarities and differences between humans and LLMs.</abstract>
      <pages>1201–1210</pages>
      <url hash="e0914689">2024.cl-4.1</url>
      <bibkey>apidianaki-etal-2024-language</bibkey>
    </paper>
    <paper id="2">
      <title>Exceptions, Instantiations, and Overgeneralization: Insights into How Language Models Process Generics</title>
      <author><first>Emily</first><last>Allaway</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last></author>
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Sarah-Jane</first><last>Leslie</last></author>
      <doi>10.1162/coli_a_00530</doi>
      <abstract>Large language models (LLMs) have garnered a great deal of attention for their exceptional generative performance on commonsense and reasoning tasks. In this work, we investigate LLMs’ capabilities for generalization using a particularly challenging type of statement: generics. Generics express generalizations (e.g., birds can fly) but do so without explicit quantification. They are notable because they generalize over their instantiations (e.g., sparrows can fly) yet hold true even in the presence of exceptions (e.g., penguins do not). For humans, these generic generalizations play a fundamental role in cognition, concept acquisition, and intuitive reasoning. We investigate how LLMs respond to and reason about generics. To this end, we first propose a framework grounded in pragmatics to automatically generate both exceptions and instantiations – collectively exemplars. We make use of focus—a pragmatic phenomenon that highlights meaning-bearing elements in a sentence—to capture the full range of interpretations of generics across different contexts of use. This allows us to derive precise logical definitions for exemplars and operationalize them to automatically generate exemplars from LLMs. Using our system, we generate a dataset of ∼370kexemplars across ∼17k generics and conduct a human validation of a sample of the generated data. We use our final generated dataset to investigate how LLMs reason about generics. Humans have a documented tendency to conflate universally quantified statements (e.g., all birds can fly) with generics. Therefore, we probe whether LLMs exhibit similar overgeneralization behavior in terms of quantification and in property inheritance. We find that LLMs do show evidence of overgeneralization, although they sometimes struggle to reason about exceptions. Furthermore, we find that LLMs may exhibit similar non-logical behavior to humans when considering property inheritance from generics.</abstract>
      <pages>1211–1275</pages>
      <url hash="16dc9c24">2024.cl-4.2</url>
      <bibkey>allaway-etal-2024-exceptions</bibkey>
    </paper>
    <paper id="3">
      <title>Humans Learn Language from Situated Communicative Interactions. What about Machines?</title>
      <author><first>Katrien</first><last>Beuls</last></author>
      <author><first>Paul</first><last>Van Eecke</last></author>
      <doi>10.1162/coli_a_00534</doi>
      <abstract>Humans acquire their native languages by taking part in communicative interactions with their caregivers. These interactions are meaningful, intentional, and situated in their everyday environment. The situated and communicative nature of the interactions is essential to the language acquisition process, as language learners depend on clues provided by the communicative environment to make sense of the utterances they perceive. As such, the linguistic knowledge they build up is rooted in linguistic forms, their meaning, and their communicative function. When it comes to machines, the situated, communicative, and interactional aspects of language learning are often passed over. This applies in particular to today’s large language models (LLMs), where the input is predominantly text-based, and where the distribution of character groups or words serves as a basis for modeling the meaning of linguistic expressions. In this article, we argue that this design choice lies at the root of a number of important limitations, in particular regarding the data hungriness of the models, their limited ability to perform human-like logical and pragmatic reasoning, and their susceptibility to biases. At the same time, we make a case for an alternative approach that models how artificial agents can acquire linguistic structures by participating in situated communicative interactions. Through a selection of experiments, we show how the linguistic knowledge that is captured in the resulting models is of a fundamentally different nature than the knowledge captured by LLMs and argue that this change of perspective provides a promising path towards more human-like language processing in machines.</abstract>
      <pages>1277–1311</pages>
      <url hash="cf98f814">2024.cl-4.3</url>
      <bibkey>beuls-van-eecke-2024-humans</bibkey>
    </paper>
    <paper id="4">
      <title>Meaning Beyond Lexicality: Capturing Pseudoword Definitions with Language Models</title>
      <author><first>Andrea Gregor</first><last>de Varda</last></author>
      <author><first>Daniele</first><last>Gatti</last></author>
      <author><first>Marco</first><last>Marelli</last></author>
      <author><first>Fritz</first><last>Günther</last></author>
      <doi>10.1162/coli_a_00527</doi>
      <abstract>Pseudowords such as “knackets” or “spechy”—letter strings that are consistent with the orthotactical rules of a language but do not appear in its lexicon—are traditionally considered to be meaningless, and used as such in empirical studies. However, recent studies that show specific semantic patterns associated with these words as well as semantic effects on human pseudoword processing have cast doubt on this view. While these studies suggest that pseudowords have meanings, they provide only extremely limited insight as to whether humans are able to ascribe explicit and declarative semantic content to unfamiliar word forms. In the present study, we utilized an exploratory-confirmatory study design to examine this question. In a first exploratory study, we started from a pre-existing dataset of words and pseudowords alongside human-generated definitions for these items. Using 18 different language models, we showed that the definitions actually produced for (pseudo)words were closer to their respective (pseudo)words than the definitions for the other items. Based on these initial results, we conducted a second, pre-registered, high-powered confirmatory study collecting a new, controlled set of (pseudo)word interpretations. This second study confirmed the results of the first one. Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.</abstract>
      <pages>1313–1343</pages>
      <url hash="c04acacf">2024.cl-4.4</url>
      <bibkey>de-varda-etal-2024-meaning</bibkey>
    </paper>
    <paper id="5">
      <title>Decode, Move and Speak! Self-supervised Learning of Speech Units, Gestures, and Sound Relationships Using Vocal Imitation</title>
      <author><first>Marc-Antoine</first><last>Georges</last></author>
      <author><first>Marvin</first><last>Lavechin</last></author>
      <author><first>Jean-Luc</first><last>Schwartz</last></author>
      <author><first>Thomas</first><last>Hueber</last></author>
      <doi>10.1162/coli_a_00532</doi>
      <abstract>Speech learning encompasses mastering a complex motor system to produce speech sounds from articulatory gestures while simultaneously uncovering discrete units that provide entry to the linguistic system. Remarkably, children acquire these associations between speech sounds, articulatory gestures, and linguistic units in a weakly supervised manner, without the need for explicit labeling of auditory inputs or access to target articulatory gestures. This study uses self-supervised deep learning to investigate the respective roles of sounds, gestures, and linguistic units in speech acquisition and control. In a first experiment, we analyzed the quantized representations learned by vector-quantized variational autoencoders (VQ-VAE) from ground truth acoustic and articulatory data using ABX tests. We show an interesting complementarity between acoustic and articulatory modalities that may help in the discovery of phonemes. In a second experiment, we introduce a computational agent that repeats auditory speech inputs by controlling a virtual vocal apparatus. This agent integrates an articulatory synthesizer capable of reproducing diverse speech stimuli from interpretable parameters, along with two internal models implementing the articulatory-to-acoustic (forward) and acoustic-to-articulatory (inverse) mapping, respectively. Additionally, two inductive biases are used to regularize the ill-posed acoustic-to-articulatory inverse mapping. In line with the first experiment, we explore the complementarity between the auditory input and the articulatory parameters inferred by the agent. We also evaluate the impact of discretizing auditory inputs using VQ-VAE. While the majority of the agent’s productions are intelligible (according to perceptual evaluations), our analysis highlights inconsistencies in the underlying articulatory trajectories. In particular, we show that the agent’s productions only partially reproduce the complementarity between the auditory and articulatory modalities observed in humans.</abstract>
      <pages>1345–1373</pages>
      <url hash="ed24dae8">2024.cl-4.5</url>
      <bibkey>georges-etal-2024-decode</bibkey>
    </paper>
    <paper id="6">
      <title>Usage-based Grammar Induction from Minimal Cognitive Principles</title>
      <author><first>Anna</first><last>Jon-And</last></author>
      <author><first>Jérôme</first><last>Michaud</last></author>
      <doi>10.1162/coli_a_00528</doi>
      <abstract>This study explores the cognitive mechanisms underlying human language acquisition through grammar induction by a minimal cognitive architecture, with a short and flexible sequence memory as its most central feature. We use reinforcement learning for the task of identifying sentences in a stream of words from artificial languages. Results demonstrate the model’s ability to identify frequent and informative multi-word chunks, reproducing characteristics of natural language acquisition. The model successfully navigates varying degrees of linguistic complexity, exposing efficient adaptation to combinatorial challenges through the reuse of sequential patterns. The emergence of parsimonious tree structures suggests an optimization for the sentence identification task, balancing economy and information. The cognitive architecture reflects aspects of human memory systems and decision-making processes, enhancing its cognitive plausibility. While the model exhibits limitations in generalization and semantic representation, its minimalist nature offers insights into some fundamental mechanisms of language learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory in language learning. Since other animals do not seem to have faithful sequence memory, this may be a key to understanding why only humans have developed complex languages.</abstract>
      <pages>1375–1414</pages>
      <url hash="cb2bfc89">2024.cl-4.6</url>
      <bibkey>jon-and-michaud-2024-usage</bibkey>
    </paper>
    <paper id="7">
      <title>Do Multimodal Large Language Models and Humans Ground Language Similarly?</title>
      <author><first>Cameron R.</first><last>Jones</last></author>
      <author><first>Benjamin</first><last>Bergen</last></author>
      <author><first>Sean</first><last>Trott</last></author>
      <doi>10.1162/coli_a_00531</doi>
      <abstract>Large Language Models (LLMs) have been criticized for failing to connect linguistic meaning to the world—for failing to solve the “symbol grounding problem.” Multimodal Large Language Models (MLLMs) offer a potential solution to this challenge by combining linguistic representations and processing with other modalities. However, much is still unknown about exactly how and to what degree MLLMs integrate their distinct modalities—and whether the way they do so mirrors the mechanisms believed to underpin grounding in humans. In humans, it has been hypothesized that linguistic meaning is grounded through “embodied simulation,” the activation of sensorimotor and affective representations reflecting described experiences. Across four pre-registered studies, we adapt experimental techniques originally developed to investigate embodied simulation in human comprehenders to ask whether MLLMs are sensitive to sensorimotor features that are implied but not explicit in descriptions of an event. In Experiment 1, we find sensitivity to some features (color and shape) but not others (size, orientation, and volume). In Experiment 2, we identify likely bottlenecks to explain an MLLM’s lack of sensitivity. In Experiment 3, we find that despite sensitivity to implicit sensorimotor features, MLLMs cannot fully account for human behavior on the same task. Finally, in Experiment 4, we compare the psychometric predictive power of different MLLM architectures and find that ViLT, a single-stream architecture, is more predictive of human responses to one sensorimotor feature (shape) than CLIP, a dual-encoder architecture—despite being trained on orders of magnitude less data. These results reveal strengths and limitations in the ability of current MLLMs to integrate language with other modalities, and also shed light on the likely mechanisms underlying human language comprehension.</abstract>
      <pages>1415–1440</pages>
      <url hash="b8036c10">2024.cl-4.7</url>
      <bibkey>jones-etal-2024-multimodal</bibkey>
    </paper>
    <paper id="8">
      <title>Can Language Models Handle Recursively Nested Grammatical Structures? A Case Study on Comparing Models and Humans</title>
      <author><first>Andrew</first><last>Lampinen</last></author>
      <doi>10.1162/coli_a_00525</doi>
      <abstract>How should we compare the capabilities of language models (LMs) and humans? In this article, I draw inspiration from comparative psychology to highlight challenges in these comparisons. I focus on a case study: processing of recursively nested grammatical structures. Prior work suggests that LMs cannot process these structures as reliably as humans can. However, the humans were provided with instructions and substantial training, while the LMs were evaluated zero-shot. I therefore match the evaluation more closely. Providing large LMs with a simple prompt—with substantially less content than the human training—allows the LMs to consistently outperform the human results, even in more deeply nested conditions than were tested with humans. Furthermore, the effects of prompting are robust to the particular structures and vocabulary used in the prompt. Finally, reanalyzing the existing human data suggests that the humans may not perform above chance at the difficult structures initially. Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably. This case study highlights how discrepancies in the evaluation methods can confound comparisons of language models and humans. I conclude by reflecting on the broader challenge of comparing human and model capabilities, and highlight an important difference between evaluating cognitive models and foundation models.</abstract>
      <pages>1441–1476</pages>
      <url hash="871b877f">2024.cl-4.8</url>
      <bibkey>lampinen-2024-language</bibkey>
    </paper>
    <paper id="9">
      <title>Exploring Temporal Sensitivity in the Brain Using Multi-timescale Language Models: An <fixed-case>EEG</fixed-case> Decoding Study</title>
      <author><first>Sijie</first><last>Ling</last></author>
      <author><first>Alex</first><last>Murphy</last></author>
      <author><first>Alona</first><last>Fyshe</last></author>
      <doi>10.1162/coli_a_00533</doi>
      <abstract>The brain’s ability to perform complex computations at varying timescales is crucial, ranging from understanding single words to grasping the overarching narrative of a story. Recently, multi-timescale long short-term memory (MT-LSTM) models (Mahto et al. 2020; Jain et al. 2020) have been introduced, which use temporally tuned parameters to induce sensitivity to different timescales of language processing (i.e., related to near/distant words). However, there has not been an exploration of the relationship between such temporally tuned information processing in MT-LSTMs and the brain’s processing of language using high temporal resolution recording modalities, such as electroencephalography (EEG). To bridge this gap, we used an EEG dataset recorded while participants listened to Chapter 1 of “Alice in Wonderland” and trained ridge regression models to predict the temporally tuned MT-LSTM embeddings from EEG responses. Our analysis reveals that EEG signals can be used to predict MT-LSTM embeddings across various timescales. For longer timescales, our models produced accurate predictions within an extended time window of ±2 s around word onset, while for shorter timescales, significant predictions are confined to a narrower window ranging from −180 ms to 790 ms. Intriguingly, we observed that short timescale information is not only processed in the vicinity of word onset but also at more distant time points. These observations underscore the parallels and discrepancies between computational models and the neural mechanisms of the brain. As word embeddings are used more as in silico models of semantic representation in the brain, a more explicit consideration of timescale-dependent processing enables more targeted explorations of language processing in humans and machines.</abstract>
      <pages>1477–1506</pages>
      <url hash="1ab4f06d">2024.cl-4.9</url>
      <bibkey>ling-etal-2024-exploring</bibkey>
    </paper>
    <paper id="10">
      <title>From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency</title>
      <author><first>Xenia</first><last>Ohmer</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <author><first>Dieuwke</first><last>Hupke</last></author>
      <doi>10.1162/coli_a_00529</doi>
      <abstract>The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what “understanding” means for a language model and how it compares to human understanding. This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say. In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes—inspired by Fregean senses—of the same meaning. Specifically, we focus on consistency across languages as well as paraphrases. Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks. We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks. We find that the model’s multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding. We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding.</abstract>
      <pages>1507–1556</pages>
      <url hash="928f38c9">2024.cl-4.10</url>
      <bibkey>ohmer-etal-2024-form</bibkey>
    </paper>
    <paper id="11">
      <title>Perception of Phonological Assimilation by Neural Speech Recognition Models</title>
      <author><first>Charlotte</first><last>Pouw</last></author>
      <author><first>Marianne de Heer</first><last>Kloots</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <author><first>Willem</first><last>Zuidema</last></author>
      <doi>10.1162/coli_a_00526</doi>
      <abstract>Human listeners effortlessly compensate for phonological changes during speech perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as “clea[m] pan”, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural speech recognition model Wav2Vec2 perceives assimilated sounds, and identifies the linguistic knowledge that is implemented by the model to compensate for assimilation during Automatic Speech Recognition (ASR). Using psycholinguistic stimuli, we systematically analyze how various linguistic context cues influence compensation patterns in the model’s output. Complementing these behavioral experiments, our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers. Finally, our causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift. These findings represent a step towards better understanding the similarities and differences in phonological processing between neural ASR models and humans.</abstract>
      <pages>1557–1585</pages>
      <url hash="135f238e">2024.cl-4.11</url>
      <bibkey>pouw-etal-2024-perception</bibkey>
    </paper>
  </volume>
</collection>
