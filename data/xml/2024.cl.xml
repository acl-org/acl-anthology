<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.cl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 1 - March 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>1</journal-issue>
    </meta>
    <paper id="1">
      <title>My Big, Fat 50-Year Journey</title>
      <author><first>Martha</first><last>Palmer</last></author>
      <doi>10.1162/coli_a_00499</doi>
      <abstract>My most heartfelt thanks to ACL for this tremendous honor. I’m completely thrilled. I cannot tell you how surprised I was when I got Iryna’s email. It is amazing that my first ACL conference since 2019 in Florence includes this award. What a wonderful way to be back with all of my friends and family here at ACL. I’m going to tell you about my big fat 50-year journey. What have I been doing for the last 50 years? Well, finding meaning, quite literally in words. Or in other words, exploring how computational lexical semantics can support natural language understanding. This is going to be quick. Hold onto your hats, here we go.</abstract>
      <pages>1–24</pages>
      <url hash="350741f4">2024.cl-1.1</url>
      <bibkey>palmer-2024-big</bibkey>
    </paper>
    <paper id="2">
      <title>Rethinking the Exploitation of Monolingual Data for Low-Resource Neural Machine Translation</title>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Baosong</first><last>Yang*</last></author>
      <author><first>Derek Fai</first><last>Wong*</last></author>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <doi>10.1162/coli_a_00496</doi>
      <abstract>The utilization of monolingual data has been shown to be a promising strategy for addressing low-resource machine translation problems. Previous studies have demonstrated the effectiveness of techniques such as back-translation and self-supervised objectives, including masked language modeling, causal language modeling, and denoise autoencoding, in improving the performance of machine translation models. However, the manner in which these methods contribute to the success of machine translation tasks and how they can be effectively combined remains an under-researched area. In this study, we carry out a systematic investigation of the effects of these techniques on linguistic properties through the use of probing tasks, including source language comprehension, bilingual word alignment, and translation fluency. We further evaluate the impact of pre-training, back-translation, and multi-task learning on bitexts of varying sizes. Our findings inform the design of more effective pipelines for leveraging monolingual data in extremely low-resource and low-resource machine translation tasks. Experiment results show consistent performance gains in seven translation directions, which provide further support for our conclusions and understanding of the role of monolingual data in machine translation.</abstract>
      <pages>25–47</pages>
      <url hash="97fc3d67">2024.cl-1.2</url>
      <bibkey>pang-etal-2024-rethinking</bibkey>
    </paper>
    <paper id="3">
      <title>How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models</title>
      <author><first>Mark</first><last>Ormerod</last></author>
      <author><first>Jesús Martínez</first><last>del Rincón</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <doi>10.1162/coli_a_00495</doi>
      <abstract>Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., KITCHEN CHAIR: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.</abstract>
      <pages>49–81</pages>
      <url hash="0aff206a">2024.cl-1.3</url>
      <bibkey>ormerod-etal-2024-kitchen</bibkey>
    </paper>
    <paper id="4">
      <title>Universal Generation for <fixed-case>O</fixed-case>ptimality <fixed-case>T</fixed-case>heory Is <fixed-case>PSPACE</fixed-case>-Complete</title>
      <author><first>Sophie</first><last>Hao</last></author>
      <doi>10.1162/coli_a_00494</doi>
      <abstract>This article shows that the universal generation problem for Optimality Theory (OT) is PSPACE-complete. While prior work has shown that universal generation is at least NP-hard and at most EXPSPACE-hard, our results place universal generation in between those two classes, assuming that NP ≠ PSPACE. We additionally show that when the number of constraints is bounded in advance, universal generation is at least NL-hard and at most NPNP-hard. Our proofs rely on a close connection between OT and the intersection non-emptiness problem for finite automata, which is PSPACE-complete in general and NL-complete when the number of automata is bounded. Our analysis shows that constraint interaction is the main contributor to the complexity of OT: The ability to factor transformations into simple, interacting constraints allows OT to furnish compact descriptions of intricate phonological phenomena.</abstract>
      <pages>83–117</pages>
      <url hash="59b66d33">2024.cl-1.4</url>
      <bibkey>hao-2024-universal</bibkey>
    </paper>
    <paper id="5">
      <title>Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering</title>
      <author><first>Akshay</first><last>Chaturvedi</last></author>
      <author><first>Swarnadeep</first><last>Bhar</last></author>
      <author><first>Soumadeep</first><last>Saha</last></author>
      <author><first>Utpal</first><last>Garain</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <doi>10.1162/coli_a_00493</doi>
      <abstract>Transformer-based language models have been shown to be highly effective for several NLP tasks. In this article, we consider three transformer models, BERT, RoBERTa, and XLNet, in both small and large versions, and investigate how faithful their representations are with respect to the semantic content of texts. We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally figure in a model’s inferences in question answering. We then test this notion by observing a model’s behavior on answering questions about a story after performing two novel semantic interventions—deletion intervention and negation intervention. While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (∼ 50% for deletion intervention, and ∼ 20% drop in accuracy for negation intervention). We then propose an intervention-based training regime that can mitigate the undesirable effects for deletion intervention by a significant margin (from ∼ 50% to ∼ 6%). We analyze the inner-workings of the models to better understand the effectiveness of intervention-based training for deletion intervention. But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models’ inability to deal with negation intervention or to capture the predicate–argument structure of texts. We also test InstructGPT, via prompting, for its ability to handle the two interventions and to capture predicate–argument structure. While InstructGPT models do achieve very high performance on predicate–argument structure task, they fail to respond adequately to our deletion and negation interventions.</abstract>
      <pages>119–155</pages>
      <url hash="1e92522f">2024.cl-1.5</url>
      <bibkey>chaturvedi-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="6">
      <title>On the Role of Morphological Information for Contextual Lemmatization</title>
      <author><first>Olia</first><last>Toporkov</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <doi>10.1162/coli_a_00497</doi>
      <abstract>Lemmatization is a natural language processing (NLP) task that consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this article we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish, and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, which constitutes, after all, their most common application use. The results of our study are rather surprising. It turns out that providing lemmatizers with fine-grained morphological features during training is not that beneficial, not even for agglutinative languages. In fact, modern contextual word representations seem to implicitly encode enough morphological information to obtain competitive contextual lemmatizers without seeing any explicit morphological signal. Moreover, our experiments suggest that the best lemmatizers out-of-domain are those using simple UPOS tags or those trained without morphology and, lastly, that current evaluation practices for lemmatization are not adequate to clearly discriminate between models.</abstract>
      <pages>157–191</pages>
      <url hash="c1b837bb">2024.cl-1.6</url>
      <bibkey>toporkov-agerri-2024-role</bibkey>
    </paper>
    <paper id="7">
      <title>Stance Detection with Explanations</title>
      <author><first>Rudra Ranajee</first><last>Saha</last></author>
      <author><first>Laks V. S.</first><last>Lakshmanan</last></author>
      <author><first>Raymond T.</first><last>Ng</last></author>
      <doi>10.1162/coli_a_00501</doi>
      <abstract>Identification of stance has recently gained a lot of attention with the extreme growth of fake news and filter bubbles. Over the last decade, many feature-based and deep-learning approaches have been proposed to solve stance detection. However, almost none of the existing works focus on providing a meaningful explanation for their prediction. In this work, we study stance detection with an emphasis on generating explanations for the predicted stance by capturing the pivotal argumentative structure embedded in a document. We propose to build a stance tree that utilizes rhetorical parsing to construct an evidence tree and to use Dempster Shafer Theory to aggregate the evidence. Human studies show that our unsupervised technique of generating stance explanations outperforms the SOTA extractive summarization method in terms of informativeness, non-redundancy, coverage, and overall quality. Furthermore, experiments show that our explanation-based stance prediction excels or matches the performance of the SOTA model on various benchmark datasets.</abstract>
      <pages>193–235</pages>
      <url hash="8243fb1a">2024.cl-1.7</url>
      <bibkey>saha-etal-2024-stance</bibkey>
    </paper>
    <paper id="8">
      <title>Can Large Language Models Transform Computational Social Science?</title>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>William</first><last>Held</last></author>
      <author><first>Omar</first><last>Shaikh</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Zhehao</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <doi>10.1162/coli_a_00502</doi>
      <abstract>Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.</abstract>
      <pages>237–291</pages>
      <url hash="47b4f1d0">2024.cl-1.8</url>
      <bibkey>ziems-etal-2024-large</bibkey>
    </paper>
    <paper id="9">
      <title>Language Model Behavior: A Comprehensive Survey</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin K.</first><last>Bergen</last></author>
      <doi>10.1162/coli_a_00492</doi>
      <abstract>Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.</abstract>
      <pages>293–350</pages>
      <url hash="c1447504">2024.cl-1.9</url>
      <bibkey>chang-bergen-2024-language</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>P</fixed-case>olysemy—<fixed-case>E</fixed-case>vidence from Linguistics, Behavioral Science, and Contextualized Language Models</title>
      <author><first>Janosch</first><last>Haber</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <doi>10.1162/coli_a_00500</doi>
      <abstract>Polysemy is the type of lexical ambiguity where a word has multiple distinct but related interpretations. In the past decade, it has been the subject of a great many studies across multiple disciplines including linguistics, psychology, neuroscience, and computational linguistics, which have made it increasingly clear that the complexity of polysemy precludes simple, universal answers, especially concerning the representation and processing of polysemous words. But fuelled by the growing availability of large, crowdsourced datasets providing substantial empirical evidence; improved behavioral methodology; and the development of contextualized language models capable of encoding the fine-grained meaning of a word within a given context, the literature on polysemy recently has developed more complex theoretical analyses. In this survey we discuss these recent contributions to the investigation of polysemy against the backdrop of a long legacy of research across multiple decades and disciplines. Our aim is to bring together different perspectives to achieve a more complete picture of the heterogeneity and complexity of the phenomenon of polysemy. Specifically, we highlight evidence supporting a range of hybrid models of the mental processing of polysemes. These hybrid models combine elements from different previous theoretical approaches to explain patterns and idiosyncrasies in the processing of polysemous that the best known models so far have failed to account for. Our literature review finds that (i) traditional analyses of polysemy can be limited in their generalizability by loose definitions and selective materials; (ii) linguistic tests provide useful evidence on individual cases, but fail to capture the full range of factors involved in the processing of polysemous sense extensions; and (iii) recent behavioral (psycho) linguistics studies, large-scale annotation efforts, and investigations leveraging contextualized language models provide accumulating evidence suggesting that polysemous sense similarity covers a wide spectrum between identity of sense and homonymy-like unrelatedness of meaning. We hope that the interdisciplinary account of polysemy provided in this survey inspires further fundamental research on the nature of polysemy and better equips applied research to deal with the complexity surrounding the phenomenon, for example, by enabling the development of benchmarks and testing paradigms for large language models informed by a greater portion of the rich evidence on the phenomenon currently available.</abstract>
      <pages>351–417</pages>
      <url hash="2b6ed76e">2024.cl-1.10</url>
      <bibkey>haber-poesio-2024-polysemy</bibkey>
    </paper>
  </volume>
  <volume id="2" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 2 - June 2023</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>June</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>2</journal-issue>
    </meta>
    <paper id="1">
      <title>Assessing the Cross-linguistic Utility of <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Shira</first><last>Wein</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <doi>10.1162/coli_a_00503</doi>
      <abstract>Semantic representations capture the meaning of a text. Abstract Meaning Representation (AMR), a type of semantic representation, focuses on predicate-argument structure and abstracts away from surface form. Though AMR was developed initially for English, it has now been adapted to a multitude of languages in the form of non-English annotation schemas, cross-lingual text-to-AMR parsing, and AMR-to-(non-English) text generation. We advance prior work on cross-lingual AMR by thoroughly investigating the amount, types, and causes of differences that appear in AMRs of different languages. Further, we compare how AMR captures meaning in cross-lingual pairs versus strings, and show that AMR graphs are able to draw out fine-grained differences between parallel sentences. We explore three primary research questions: (1) What are the types and causes of differences in parallel AMRs? (2) How can we measure the amount of difference between AMR pairs in different languages? (3) Given that AMR structure is affected by language and exhibits cross-lingual differences, how do cross-lingual AMR pairs compare to string-based representations of cross-lingual sentence pairs? We find that the source language itself does have a measurable impact on AMR structure, and that translation divergences and annotator choices also lead to differences in cross-lingual AMR pairs. We explore the implications of this finding throughout our study, concluding that, although AMR is useful to capture meaning across languages, evaluations need to take into account source language influences if they are to paint an accurate picture of system output, and meaning generally.</abstract>
      <pages>419–473</pages>
      <url hash="7b8c94ac">2024.cl-2.1</url>
      <bibkey>wein-schneider-2024-assessing</bibkey>
    </paper>
    <paper id="2">
      <title>Context-aware Transliteration of <fixed-case>R</fixed-case>omanized <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian Languages</title>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>Cibu</first><last>Johny</last></author>
      <author><first>Anna</first><last>Katanova</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <doi>10.1162/coli_a_00510</doi>
      <abstract>While most transliteration research is focused on single tokens such as named entities—for example, transliteration of from the Gujarati script to the Latin script “Ahmedabad” footnoteThe most populous city in the Indian state of Gujarat. the informal romanization prevalent in South Asia and elsewhere often requires transliteration of full sentences. The lack of large parallel text collections of full sentence (as opposed to single word) transliterations necessitates incorporation of contextual information into transliteration via non-parallel resources, such as via mono-script text collections. In this article, we present a number of methods for improving transliteration in context for such a use scenario. Some of these methods in fact improve performance without making use of sentential context, allowing for better quantification of the degree to which contextual information in particular is responsible for system improvements. Our final systems, which ultimately rely upon ensembles including large pretrained language models fine-tuned on simulated parallel data, yield substantial improvements over the best previously reported results for full sentence transliteration from Latin to native script on all 12 languages in the Dakshina dataset (Roark et al. 2020), with an overall 3.3% absolute (18.6% relative) mean word-error rate reduction.</abstract>
      <pages>475–534</pages>
      <url hash="51e628c3">2024.cl-2.2</url>
      <bibkey>kirov-etal-2024-context</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>UG</fixed-case>-schematic Annotation for Event Nominals: A Case Study in <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wenxi</first><last>Li</last></author>
      <author><first>Yutong</first><last>Zhang</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <doi>10.1162/coli_a_00504</doi>
      <abstract>Divergence of languages observed at the surface level is a major challenge encountered by multilingual data representation, especially when typologically distant languages are involved. Drawing inspiration from a formalist Chomskyan perspective towards language universals, Universal Grammar (UG), this article uses deductively pre-defined universals to analyze a multilingually heterogeneous phenomenon, event nominals. In this way, deeper universality of event nominals beneath their huge divergence in different languages is uncovered, which empowers us to break barriers between languages and thus extend insights from some synthetic languages to a non-inflectional language, Mandarin Chinese. Our empirical investigation also demonstrates this UG-inspired schema is effective: With its assistance, the inter-annotator agreement (IAA) for identifying event nominals in Mandarin grows from 88.02% to 94.99%, and automatic detection of event-reading nominalizations on the newly-established data achieves an accuracy of 94.76% and an F1 score of 91.3%, which significantly surpass those achieved on the pre-existing resource by 9.8% and 5.2%, respectively. Our systematic analysis also sheds light on nominal semantic role labeling. By providing a clear definition and classification on arguments of event nominal, the IAA of this task significantly increases from 90.46% to 98.04%.</abstract>
      <pages>535–561</pages>
      <url hash="b3a3104e">2024.cl-2.3</url>
      <bibkey>li-etal-2024-ug</bibkey>
    </paper>
    <paper id="4">
      <title>A <fixed-case>B</fixed-case>ayesian Approach to Uncertainty in Word Embedding Bias Estimation</title>
      <author><first>Alicja</first><last>Dobrzeniecka</last></author>
      <author><first>Rafal</first><last>Urbaniak</last></author>
      <doi>10.1162/coli_a_00507</doi>
      <abstract>Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of bias present in word embeddings in terms of a single-number metric. However, such metrics and the related statistical significance calculations rely on treating pre-averaged data as individual data points and utilizing bootstrapping techniques with low sample sizes. We show that similar results can be easily obtained using such methods even if the data are generated by a null model lacking the intended bias. Consequently, we argue that this approach generates false confidence. To address this issue, we propose a Bayesian alternative: hierarchical Bayesian modeling, which enables a more uncertainty-sensitive inspection of bias in word embeddings at different levels of granularity. To showcase our method, we apply it to Religion, Gender, and Race word lists from the original research, together with our control neutral word lists. We deploy the method using Google, GloVe, and Reddit embeddings. Further, we utilize our approach to evaluate a debiasing technique applied to the Reddit word embedding. Our findings reveal a more complex landscape than suggested by the proponents of single-number metrics. The datasets and source code for the paper are publicly available.1</abstract>
      <pages>563–617</pages>
      <url hash="a4cb9506">2024.cl-2.4</url>
      <bibkey>dobrzeniecka-urbaniak-2024-bayesian</bibkey>
    </paper>
    <paper id="5">
      <title>Topics in the Haystack: Enhancing Topic Quality through Corpus Expansion</title>
      <author><first>Anton</first><last>Thielmann</last></author>
      <author><first>Arik</first><last>Reuter</last></author>
      <author><first>Quentin</first><last>Seifert</last></author>
      <author><first>Elisabeth</first><last>Bergherr</last></author>
      <author><first>Benjamin</first><last>Säfken</last></author>
      <doi>10.1162/coli_a_00506</doi>
      <abstract>Extracting and identifying latent topics in large text corpora have gained increasing importance in Natural Language Processing (NLP). Most models, whether probabilistic models similar to Latent Dirichlet Allocation (LDA) or neural topic models, follow the same underlying approach of topic interpretability and topic extraction. We propose a method that incorporates a deeper understanding of both sentence and document themes, and goes beyond simply analyzing word frequencies in the data. Through simple corpus expansion, our model can detect latent topics that may include uncommon words or neologisms, as well as words not present in the documents themselves. Additionally, we propose several new evaluation metrics based on intruder words and similarity measures in the semantic space. We present correlation coefficients with human identification of intruder words and achieve near-human level results at the word-intrusion task. We demonstrate the competitive performance of our method with a large benchmark study, and achieve superior results compared with state-of-the-art topic modeling and document clustering models. The code is available at the following link: https://github.com/AnFreTh/STREAM.</abstract>
      <pages>619–655</pages>
      <url hash="0dcc9b5a">2024.cl-2.5</url>
      <bibkey>thielmann-etal-2024-topics</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Faithful Model Explanation in <fixed-case>NLP</fixed-case>: A Survey</title>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <doi>10.1162/coli_a_00511</doi>
      <abstract>End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, that is, an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.</abstract>
      <pages>657–723</pages>
      <url hash="3f0e150f">2024.cl-2.6</url>
      <bibkey>lyu-etal-2024-towards</bibkey>
    </paper>
    <paper id="7">
      <title>A Systematic Review of Computational Approaches to Deciphering Bronze Age Aegean and <fixed-case>C</fixed-case>ypriot Scripts</title>
      <author><first>Maja</first><last>Braović</last></author>
      <author><first>Damir</first><last>Krstinić</last></author>
      <author><first>Maja</first><last>Štula</last></author>
      <author><first>Antonia</first><last>Ivanda</last></author>
      <doi>10.1162/coli_a_00514</doi>
      <abstract>This article provides a detailed insight into computational approaches for deciphering Bronze Age Aegean and Cypriot scripts, namely, the Archanes script and the Archanes formula, Phaistos Disk, Cretan hieroglyphic (including the Malia Altar Stone and Arkalochori Axe), Linear A, Linear B, Cypro-Minoan, and Cypriot scripts. The unique contributions of this article are threefold: (1) a thorough review of major Bronze Age Aegean and Cypriot scripts and inscriptions, digital data and corpora associated with them, existing computational decipherment methods developed in order to decipher them, and possible links to other scripts and languages; (2) the definition of 15 major challenges that can be encountered in computational decipherments of ancient scripts; and (3) an outline of a computational model that could possibly be used to simulate traditional decipherment processes of ancient scripts based on palaeography and epigraphy. In the context of this article the term decipherment denotes the process of discovery of the language and/or the set of symbols behind an unknown script, and the meaning behind it.</abstract>
      <pages>725–779</pages>
      <url hash="ae407bd7">2024.cl-2.7</url>
      <bibkey>braovic-etal-2024-systematic</bibkey>
    </paper>
    <paper id="8">
      <title>The Role of Typological Feature Prediction in <fixed-case>NLP</fixed-case> and Linguistics</title>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <doi>10.1162/coli_a_00498</doi>
      <abstract>Computational typology has gained traction in the field of Natural Language Processing (NLP) in recent years, as evidenced by the increasing number of papers on the topic and the establishment of a Special Interest Group on the topic (SIGTYP), including the organization of successful workshops and shared tasks. A considerable amount of work in this sub-field is concerned with prediction of typological features, for example, for databases such as the World Atlas of Language Structures (WALS) or Grambank. Prediction is argued to be useful either because (1) it allows for obtaining feature values for relatively undocumented languages, alleviating the sparseness in WALS, in turn argued to be useful for both NLP and linguistics; and (2) it allows us to probe models to see whether or not these typological features are encapsulated in, for example, language representations. In this article, we present a critical stance concerning prediction of typological features, investigating to what extent this line of research is aligned with purported needs—both from the perspective of NLP practitioners, and perhaps more importantly, from the perspective of linguists specialized in typology and language documentation. We provide evidence that this line of research in its current state suffers from a lack of interdisciplinary alignment. Based on an extensive survey of the linguistic typology community, we present concrete recommendations for future research in order to improve this alignment between linguists and NLP researchers, beyond the scope of typological feature prediction.</abstract>
      <pages>781–794</pages>
      <url hash="c7cdf8f4">2024.cl-2.8</url>
      <bibkey>bjerva-2024-role</bibkey>
    </paper>
    <paper id="9">
      <title>Common Flaws in Running Human Evaluation Experiments in <fixed-case>NLP</fixed-case></title>
      <author><first>Craig</first><last>Thomson</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <doi>10.1162/coli_a_00508</doi>
      <abstract>While conducting a coordinated set of repeat runs of human evaluation experiments in NLP, we discovered flaws in every single experiment we selected for inclusion via a systematic process. In this squib, we describe the types of flaws we discovered, which include coding errors (e.g., loading the wrong system outputs to evaluate), failure to follow standard scientific practice (e.g., ad hoc exclusion of participants and responses), and mistakes in reported numerical results (e.g., reported numbers not matching experimental data). If these problems are widespread, it would have worrying implications for the rigor of NLP evaluation experiments as currently conducted. We discuss what researchers can do to reduce the occurrence of such flaws, including pre-registration, better code development practices, increased testing and piloting, and post-publication addressing of errors.</abstract>
      <pages>795–805</pages>
      <url hash="2c6297cf">2024.cl-2.9</url>
      <bibkey>thomson-etal-2024-common</bibkey>
    </paper>
    <paper id="10">
      <title>The Pitfalls of Defining Hallucination</title>
      <author><first>Kees</first><last>van Deemter</last></author>
      <doi>10.1162/coli_a_00509</doi>
      <abstract>Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs.</abstract>
      <pages>807–816</pages>
      <url hash="1638361a">2024.cl-2.10</url>
      <bibkey>deemter-2024-pitfalls</bibkey>
    </paper>
  </volume>
  <volume id="3" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 3 - September 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>September</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>3</journal-issue>
    </meta>
    <paper id="1">
      <title>Analyzing Dataset Annotation Quality Management in the Wild</title>
      <author><first>Jan-Christoph</first><last>Klie</last></author>
      <author><first>Richard</first><last>Eckart de Castilho</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00516</doi>
      <abstract>Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models as well as for their correct evaluation. Recent work, however, has shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, biases, or artifacts. While practices and guidelines regarding dataset creation projects exist, to our knowledge, large-scale analysis has yet to be performed on how quality management is conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions for applying them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication, or data validation. Using these annotations, we then analyze how quality management is conducted in practice. A majority of the annotated publications apply good or excellent quality management. However, we deem the effort of 30% of the studies as only subpar. Our analysis also shows common errors, especially when using inter-annotator agreement and computing annotation error rates.</abstract>
      <pages>817–866</pages>
      <url hash="4d72b557">2024.cl-3.1</url>
      <bibkey>klie-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>LLM</fixed-case>-Assisted Data Augmentation for <fixed-case>C</fixed-case>hinese Dialogue-Level Dependency Parsing</title>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Gongyao</first><last>Jiang</last></author>
      <author><first>Shuang</first><last>Liu</last></author>
      <author><first>Jing</first><last>Chen</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <doi>10.1162/coli_a_00515</doi>
      <abstract>Dialogue-level dependency parsing, despite its growing academic interest, often encounters underperformance issues due to resource shortages. A potential solution to this challenge is data augmentation. In recent years, large language models (LLMs) have demonstrated strong capabilities in generation, which can facilitate data augmentation greatly. In this study, we focus on Chinese dialogue-level dependency parsing, presenting three simple and effective strategies with LLM to augment the original training instances, namely word-level, syntax-level, and discourse-level augmentations, respectively. These strategies enable LLMs to either preserve or modify dependency structures, thereby assuring accuracy while increasing the diversity of instances at different levels. We conduct experiments on the benchmark dataset released by Jiang et al. (2023) to validate our approach. Results show that our method can greatly boost the parsing performance in various settings, particularly in dependencies among elementary discourse units. Lastly, we provide in-depth analysis to show the key points of our data augmentation strategies.</abstract>
      <pages>867–891</pages>
      <url hash="49ab2050">2024.cl-3.2</url>
      <bibkey>zhang-etal-2024-llm-assisted</bibkey>
    </paper>
    <paper id="3">
      <title>Aligning Human and Computational Coherence Evaluations</title>
      <author><first>Jia Peng</first><last>Lim</last></author>
      <author><first>Hady W.</first><last>Lauw</last></author>
      <doi>10.1162/coli_a_00518</doi>
      <abstract>Automated coherence metrics constitute an efficient and popular way to evaluate topic models. Previous work presents a mixed picture of their presumed correlation with human judgment. This work proposes a novel sampling approach to mining topic representations at a large scale while seeking to mitigate bias from sampling, enabling the investigation of widely used automated coherence metrics via large corpora. Additionally, this article proposes a novel user study design, an amalgamation of different proxy tasks, to derive a finer insight into the human decision-making processes. This design subsumes the purpose of simple rating and outlier-detection user studies. Similar to the sampling approach, the user study conducted is extensive, comprising 40 study participants split into eight different study groups tasked with evaluating their respective set of 100 topic representations. Usually, when substantiating the use of these metrics, human responses are treated as the gold standard. This article further investigates the reliability of human judgment by flipping the comparison and conducting a novel extended analysis of human response at the group and individual level against a generic corpus. The investigation results show a moderate to good correlation between these metrics and human judgment, especially for generic corpora, and derive further insights into the human perception of coherence. Analyzing inter-metric correlations across corpora shows moderate to good correlation among these metrics. As these metrics depend on corpus statistics, this article further investigates the topical differences between corpora, revealing nuances in applications of these metrics.</abstract>
      <pages>893–952</pages>
      <url hash="5e57cfa7">2024.cl-3.3</url>
      <bibkey>lim-lauw-2024-aligning</bibkey>
    </paper>
    <paper id="4">
      <title>Relation Extraction in Underexplored Biomedical Domains: A Diversity-optimized Sampling and Synthetic Data Generation Approach</title>
      <author><first>Maxime</first><last>Delmas</last></author>
      <author><first>Magdalena</first><last>Wysocka</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <doi>10.1162/coli_a_00520</doi>
      <abstract>The sparsity of labeled data is an obstacle to the development of Relation Extraction (RE) models and the completion of databases in various biomedical areas. While being of high interest in drug-discovery, the literature on natural products, reporting the identification of potential bioactive compounds from organisms, is a concrete example of such an overlooked topic. To mark the start of this new task, we created the first curated evaluation dataset and extracted literature items from the LOTUS database to build training sets. To this end, we developed a new sampler, inspired by diversity metrics in ecology, named Greedy Maximum Entropy sampler (https://github.com/idiap/gme-sampler). The strategic optimization of both balance and diversity of the selected items in the evaluation set is important given the resource-intensive nature of manual curation. After quantifying the noise in the training set, in the form of discrepancies between the text of input abstracts and the expected output labels, we explored different strategies accordingly. Framing the task as an end-to-end Relation Extraction, we evaluated the performance of standard fine-tuning (BioGPT, GPT-2, and Seq2rel) and few-shot learning with open Large Language Models (LLMs) (LLaMA 7B-65B). In addition to their evaluation in few-shot settings, we explore the potential of open LLMs as synthetic data generators and propose a new workflow for this purpose. All evaluated models exhibited substantial improvements when fine-tuned on synthetic abstracts rather than the original noisy data. We provide our best performing (F1-score = 59.0) BioGPT-Large model for end-to-end RE of natural products relationships along with all the training and evaluation datasets. See more details at https://github.com/idiap/abroad-re.</abstract>
      <pages>953–1000</pages>
      <url hash="9ce3cb73">2024.cl-3.4</url>
      <bibkey>delmas-etal-2024-relation</bibkey>
    </paper>
    <paper id="5">
      <title>Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation</title>
      <author><first>Ran</first><last>Zhang</last></author>
      <author><first>Jihed</first><last>Ouni</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <doi>10.1162/coli_a_00519</doi>
      <abstract>While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility and understanding. This article comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We (1) build the first CLCTS corpus with 328 instances for hDe-En (extended version with 455 instances) and 289 for hEn-De (extended version with 501 instances), leveraging historical fiction texts and Wikipedia summaries in English and German; (2) examine the effectiveness of popular transformer end-to-end models with different intermediate fine-tuning tasks; (3) explore the potential of GPT-3.5 as a summarizer; and (4) report evaluations from humans, GPT-4, and several recent automatic evaluation metrics. Our results indicate that intermediate task fine-tuned end-to-end models generate bad to moderate quality summaries while GPT-3.5, as a zero-shot summarizer, provides moderate to good quality outputs. GPT-3.5 also seems very adept at normalizing historical text. To assess data contamination in GPT-3.5, we design an adversarial attack scheme in which we find that GPT-3.5 performs slightly worse for unseen source documents compared to seen documents. Moreover, it sometimes hallucinates when the source sentences are inverted against its prior knowledge with a summarization accuracy of 0.67 for plot omission, 0.71 for entity swap, and 0.53 for plot negation. Overall, our regression results of model performances suggest that longer, older, and more complex source texts (all of which are more characteristic for historical language variants) are harder to summarize for all models, indicating the difficulty of the CLCTS task. Regarding evaluation, we observe that both the GPT-4 and BERTScore correlate moderately with human evaluations, implicating great potential for future improvement.</abstract>
      <pages>1001–1047</pages>
      <url hash="a4690640">2024.cl-3.5</url>
      <bibkey>zhang-etal-2024-cross</bibkey>
    </paper>
    <paper id="6">
      <title>Cognitive Plausibility in Natural Language Processing</title>
      <author><first>Yevgen</first><last>Matusevych</last></author>
      <doi>10.1162/coli_r_00517</doi>
      <pages>1049–1052</pages>
      <url hash="f2fbdf46">2024.cl-3.6</url>
      <bibkey>matusevych-2024-cognitive</bibkey>
    </paper>
    <paper id="7">
      <title>Large Language Model Instruction Following: A Survey of Progresses and Challenges</title>
      <author><first>Renze</first><last>Lou</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <doi>10.1162/coli_a_00523</doi>
      <abstract>Task semantics can be expressed by a set of input-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: First, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning to follow task instructions, that is, instruction following. Despite its impressive progress, there are some unsolved research equations that the community struggles with. This survey tries to summarize and provide insights into the current research on instruction following, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How should we model instructions? (iii) What are popular instruction following datasets and evaluation metrics? (iv) What factors influence and explain the instructions’ performance? (v) What challenges remain in instruction following? To our knowledge, this is the first comprehensive survey about instruction following.1</abstract>
      <pages>1053–1095</pages>
      <url hash="f3e80b64">2024.cl-3.7</url>
      <bibkey>lou-etal-2024-large</bibkey>
    </paper>
    <paper id="8">
      <title>Bias and Fairness in Large Language Models: A Survey</title>
      <author><first>Isabel O.</first><last>Gallegos</last></author>
      <author><first>Ryan A.</first><last>Rossi</last></author>
      <author><first>Joe</first><last>Barrow</last></author>
      <author><first>Md Mehrab</first><last>Tanjim</last></author>
      <author><first>Sungchul</first><last>Kim</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Tong</first><last>Yu</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Nesreen K.</first><last>Ahmed</last></author>
      <doi>10.1162/coli_a_00524</doi>
      <abstract>Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.</abstract>
      <pages>1097–1179</pages>
      <url hash="687f3ad2">2024.cl-3.8</url>
      <bibkey>gallegos-etal-2024-bias</bibkey>
    </paper>
    <paper id="10">
      <title>A Novel Alignment-based Approach for <fixed-case>PARSEVAL</fixed-case> Measuress</title>
      <author><first>Eunkyul Leah</first><last>Jo</last></author>
      <author><first>Angela Yoonseo</first><last>Park</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <doi>10.1162/coli_a_00512</doi>
      <abstract>We propose a novel method for calculating PARSEVAL measures to evaluate constituent parsing results. Previous constituent parsing evaluation techniques were constrained by the requirement for consistent sentence boundaries and tokenization results, proving to be stringent and inconvenient. Our new approach handles constituent parsing results obtained from raw text, even when sentence boundaries and tokenization differ from the preprocessed gold sentence. Implementing this measure is our evaluation by alignment approach. The algorithm enables the alignment of tokens and sentences in the gold and system parse trees. Our proposed algorithm draws on the analogy of sentence and word alignment commonly used in machine translation (MT). To demonstrate the intricacy of calculations and clarify any integration of configurations, we explain the implementations in detailed pseudo-code and provide empirical proof for how sentence and word alignment can improve evaluation reliability.</abstract>
      <pages>1181–1190</pages>
      <url hash="658565b0">2024.cl-3.10</url>
      <bibkey>jo-etal-2024-novel</bibkey>
    </paper>
    <paper id="12">
      <title>Do Language Models’ Words Refer?</title>
      <author><first>Matthew</first><last>Mandelkern</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <doi>10.1162/coli_a_00522</doi>
      <abstract>What do language models (LMs) do with language? They can produce sequences of (mostly) coherent strings closely resembling English. But do those sentences mean something, or are LMs simply babbling in a convincing simulacrum of language use? We address one aspect of this broad question: whether LMs’ words can refer, that is, achieve “word-to-world” connections. There is prima facie reason to think they do not, since LMs do not interact with the world in the way that ordinary language users do. Drawing on the externalist tradition in philosophy of language, we argue that those appearances are misleading: Even if the inputs to LMs are simply strings of text, they are strings of text with natural histories, and that may suffice for LMs’ words to refer.</abstract>
      <pages>1191–1200</pages>
      <url hash="b83cf24c">2024.cl-3.12</url>
      <bibkey>mandelkern-linzen-2024-language</bibkey>
    </paper>
  </volume>
</collection>
