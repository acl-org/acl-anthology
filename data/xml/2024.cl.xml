<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.cl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Computational Linguistics, Volume 50, Issue 1 - March 2024</booktitle>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <month>March</month>
      <year>2024</year>
      <venue>cl</venue>
      <journal-volume>50</journal-volume>
      <journal-issue>1</journal-issue>
    </meta>
    <paper id="1">
      <title>My Big, Fat 50-Year Journey</title>
      <author><first>Martha</first><last>Palmer</last></author>
      <doi>10.1162/coli_a_00499</doi>
      <abstract>My most heartfelt thanks to ACL for this tremendous honor. I’m completely thrilled. I cannot tell you how surprised I was when I got Iryna’s email. It is amazing that my first ACL conference since 2019 in Florence includes this award. What a wonderful way to be back with all of my friends and family here at ACL. I’m going to tell you about my big fat 50-year journey. What have I been doing for the last 50 years? Well, finding meaning, quite literally in words. Or in other words, exploring how computational lexical semantics can support natural language understanding. This is going to be quick. Hold onto your hats, here we go.</abstract>
      <pages>1–24</pages>
      <url hash="350741f4">2024.cl-1.1</url>
      <bibkey>palmer-2024-big</bibkey>
    </paper>
    <paper id="2">
      <title>Rethinking the Exploitation of Monolingual Data for Low-Resource Neural Machine Translation</title>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Baosong</first><last>Yang*</last></author>
      <author><first>Derek Fai</first><last>Wong*</last></author>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Lidia Sam</first><last>Chao</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <doi>10.1162/coli_a_00496</doi>
      <abstract>The utilization of monolingual data has been shown to be a promising strategy for addressing low-resource machine translation problems. Previous studies have demonstrated the effectiveness of techniques such as back-translation and self-supervised objectives, including masked language modeling, causal language modeling, and denoise autoencoding, in improving the performance of machine translation models. However, the manner in which these methods contribute to the success of machine translation tasks and how they can be effectively combined remains an under-researched area. In this study, we carry out a systematic investigation of the effects of these techniques on linguistic properties through the use of probing tasks, including source language comprehension, bilingual word alignment, and translation fluency. We further evaluate the impact of pre-training, back-translation, and multi-task learning on bitexts of varying sizes. Our findings inform the design of more effective pipelines for leveraging monolingual data in extremely low-resource and low-resource machine translation tasks. Experiment results show consistent performance gains in seven translation directions, which provide further support for our conclusions and understanding of the role of monolingual data in machine translation.</abstract>
      <pages>25–47</pages>
      <url hash="97fc3d67">2024.cl-1.2</url>
      <bibkey>pang-etal-2024-rethinking</bibkey>
    </paper>
    <paper id="3">
      <title>How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models</title>
      <author><first>Mark</first><last>Ormerod</last></author>
      <author><first>Jesús Martínez</first><last>del Rincón</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <doi>10.1162/coli_a_00495</doi>
      <abstract>Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., KITCHEN CHAIR: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.</abstract>
      <pages>49–81</pages>
      <url hash="0aff206a">2024.cl-1.3</url>
      <bibkey>ormerod-etal-2024-kitchen</bibkey>
    </paper>
    <paper id="4">
      <title>Universal Generation for <fixed-case>O</fixed-case>ptimality <fixed-case>T</fixed-case>heory Is <fixed-case>PSPACE</fixed-case>-Complete</title>
      <author><first>Sophie</first><last>Hao</last></author>
      <doi>10.1162/coli_a_00494</doi>
      <abstract>This article shows that the universal generation problem for Optimality Theory (OT) is PSPACE-complete. While prior work has shown that universal generation is at least NP-hard and at most EXPSPACE-hard, our results place universal generation in between those two classes, assuming that NP ≠ PSPACE. We additionally show that when the number of constraints is bounded in advance, universal generation is at least NL-hard and at most NPNP-hard. Our proofs rely on a close connection between OT and the intersection non-emptiness problem for finite automata, which is PSPACE-complete in general and NL-complete when the number of automata is bounded. Our analysis shows that constraint interaction is the main contributor to the complexity of OT: The ability to factor transformations into simple, interacting constraints allows OT to furnish compact descriptions of intricate phonological phenomena.</abstract>
      <pages>83–117</pages>
      <url hash="59b66d33">2024.cl-1.4</url>
      <bibkey>hao-2024-universal</bibkey>
    </paper>
    <paper id="5">
      <title>Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering</title>
      <author><first>Akshay</first><last>Chaturvedi</last></author>
      <author><first>Swarnadeep</first><last>Bhar</last></author>
      <author><first>Soumadeep</first><last>Saha</last></author>
      <author><first>Utpal</first><last>Garain</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <doi>10.1162/coli_a_00493</doi>
      <abstract>Transformer-based language models have been shown to be highly effective for several NLP tasks. In this article, we consider three transformer models, BERT, RoBERTa, and XLNet, in both small and large versions, and investigate how faithful their representations are with respect to the semantic content of texts. We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally figure in a model’s inferences in question answering. We then test this notion by observing a model’s behavior on answering questions about a story after performing two novel semantic interventions—deletion intervention and negation intervention. While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (∼ 50% for deletion intervention, and ∼ 20% drop in accuracy for negation intervention). We then propose an intervention-based training regime that can mitigate the undesirable effects for deletion intervention by a significant margin (from ∼ 50% to ∼ 6%). We analyze the inner-workings of the models to better understand the effectiveness of intervention-based training for deletion intervention. But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models’ inability to deal with negation intervention or to capture the predicate–argument structure of texts. We also test InstructGPT, via prompting, for its ability to handle the two interventions and to capture predicate–argument structure. While InstructGPT models do achieve very high performance on predicate–argument structure task, they fail to respond adequately to our deletion and negation interventions.</abstract>
      <pages>119–155</pages>
      <url hash="1e92522f">2024.cl-1.5</url>
      <bibkey>chaturvedi-etal-2024-analyzing</bibkey>
    </paper>
    <paper id="6">
      <title>On the Role of Morphological Information for Contextual Lemmatization</title>
      <author><first>Olia</first><last>Toporkov</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <doi>10.1162/coli_a_00497</doi>
      <abstract>Lemmatization is a natural language processing (NLP) task that consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this article we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish, and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, which constitutes, after all, their most common application use. The results of our study are rather surprising. It turns out that providing lemmatizers with fine-grained morphological features during training is not that beneficial, not even for agglutinative languages. In fact, modern contextual word representations seem to implicitly encode enough morphological information to obtain competitive contextual lemmatizers without seeing any explicit morphological signal. Moreover, our experiments suggest that the best lemmatizers out-of-domain are those using simple UPOS tags or those trained without morphology and, lastly, that current evaluation practices for lemmatization are not adequate to clearly discriminate between models.</abstract>
      <pages>157–191</pages>
      <url hash="c1b837bb">2024.cl-1.6</url>
      <bibkey>toporkov-agerri-2024-role</bibkey>
    </paper>
    <paper id="7">
      <title>Stance Detection with Explanations</title>
      <author><first>Rudra Ranajee</first><last>Saha</last></author>
      <author><first>Laks V. S.</first><last>Lakshmanan</last></author>
      <author><first>Raymond T.</first><last>Ng</last></author>
      <doi>10.1162/coli_a_00501</doi>
      <abstract>Identification of stance has recently gained a lot of attention with the extreme growth of fake news and filter bubbles. Over the last decade, many feature-based and deep-learning approaches have been proposed to solve stance detection. However, almost none of the existing works focus on providing a meaningful explanation for their prediction. In this work, we study stance detection with an emphasis on generating explanations for the predicted stance by capturing the pivotal argumentative structure embedded in a document. We propose to build a stance tree that utilizes rhetorical parsing to construct an evidence tree and to use Dempster Shafer Theory to aggregate the evidence. Human studies show that our unsupervised technique of generating stance explanations outperforms the SOTA extractive summarization method in terms of informativeness, non-redundancy, coverage, and overall quality. Furthermore, experiments show that our explanation-based stance prediction excels or matches the performance of the SOTA model on various benchmark datasets.</abstract>
      <pages>193–235</pages>
      <url hash="8243fb1a">2024.cl-1.7</url>
      <bibkey>saha-etal-2024-stance</bibkey>
    </paper>
    <paper id="8">
      <title>Can Large Language Models Transform Computational Social Science?</title>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>William</first><last>Held</last></author>
      <author><first>Omar</first><last>Shaikh</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Zhehao</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <doi>10.1162/coli_a_00502</doi>
      <abstract>Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.</abstract>
      <pages>237–291</pages>
      <url hash="47b4f1d0">2024.cl-1.8</url>
      <bibkey>ziems-etal-2024-large</bibkey>
    </paper>
    <paper id="9">
      <title>Language Model Behavior: A Comprehensive Survey</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin K.</first><last>Bergen</last></author>
      <doi>10.1162/coli_a_00492</doi>
      <abstract>Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.</abstract>
      <pages>293–350</pages>
      <url hash="c1447504">2024.cl-1.9</url>
      <bibkey>chang-bergen-2024-language</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>P</fixed-case>olysemy—<fixed-case>E</fixed-case>vidence from Linguistics, Behavioral Science, and Contextualized Language Models</title>
      <author><first>Janosch</first><last>Haber</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <doi>10.1162/coli_a_00500</doi>
      <abstract>Polysemy is the type of lexical ambiguity where a word has multiple distinct but related interpretations. In the past decade, it has been the subject of a great many studies across multiple disciplines including linguistics, psychology, neuroscience, and computational linguistics, which have made it increasingly clear that the complexity of polysemy precludes simple, universal answers, especially concerning the representation and processing of polysemous words. But fuelled by the growing availability of large, crowdsourced datasets providing substantial empirical evidence; improved behavioral methodology; and the development of contextualized language models capable of encoding the fine-grained meaning of a word within a given context, the literature on polysemy recently has developed more complex theoretical analyses. In this survey we discuss these recent contributions to the investigation of polysemy against the backdrop of a long legacy of research across multiple decades and disciplines. Our aim is to bring together different perspectives to achieve a more complete picture of the heterogeneity and complexity of the phenomenon of polysemy. Specifically, we highlight evidence supporting a range of hybrid models of the mental processing of polysemes. These hybrid models combine elements from different previous theoretical approaches to explain patterns and idiosyncrasies in the processing of polysemous that the best known models so far have failed to account for. Our literature review finds that (i) traditional analyses of polysemy can be limited in their generalizability by loose definitions and selective materials; (ii) linguistic tests provide useful evidence on individual cases, but fail to capture the full range of factors involved in the processing of polysemous sense extensions; and (iii) recent behavioral (psycho) linguistics studies, large-scale annotation efforts, and investigations leveraging contextualized language models provide accumulating evidence suggesting that polysemous sense similarity covers a wide spectrum between identity of sense and homonymy-like unrelatedness of meaning. We hope that the interdisciplinary account of polysemy provided in this survey inspires further fundamental research on the nature of polysemy and better equips applied research to deal with the complexity surrounding the phenomenon, for example, by enabling the development of benchmarks and testing paradigms for large language models informed by a greater portion of the rich evidence on the phenomenon currently available.</abstract>
      <pages>351–417</pages>
      <url hash="2b6ed76e">2024.cl-1.10</url>
      <bibkey>haber-poesio-2024-polysemy</bibkey>
    </paper>
  </volume>
</collection>
