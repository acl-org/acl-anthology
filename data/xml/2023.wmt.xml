<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.wmt">
  <volume id="1" ingest-date="2023-11-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Eighth Conference on Machine Translation</booktitle>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Tom</first><last>Kocmi</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="a6bae8ff">2023.wmt-1</url>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="1ccf4472">2023.wmt-1.0</url>
      <bibkey>wmt-2023</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the 2023 Conference on Machine Translation (<fixed-case>WMT</fixed-case>23): <fixed-case>LLM</fixed-case>s Are Here but Not Quite There Yet</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Anton</first><last>Dvorkovich</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Thamme</first><last>Gowda</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Benjamin</first><last>Marie</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <author><first>Makoto</first><last>Nagata</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Mariya</first><last>Shmatova</last></author>
      <pages>1–42</pages>
      <abstract>This paper presents the results of the General Machine Translation Task organised as part of the 2023 Conference on Machine Translation (WMT). In the general MT task, participants were asked to build machine translation systems for any of 8 language pairs (corresponding to 14 translation directions), to be evaluated on test sets consisting of up to four different domains. We evaluate system outputs with professional human annotators using a combination of source-based Direct Assessment and scalar quality metric (DA+SQM).</abstract>
      <url hash="b43f2718">2023.wmt-1.1</url>
      <bibkey>kocmi-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.1</doi>
    </paper>
    <paper id="2">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Biomedical Translation Shared Task: Evaluation of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> 3.5 as a Comparison System</title>
      <author><first>Mariana</first><last>Neves</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Giorgio Maria</first><last>Di Nunzio</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Philippe</first><last>Thomas</last></author>
      <author><first>Federica</first><last>Vezzani</last></author>
      <author><first>Maika</first><last>Vicente Navarro</last></author>
      <author><first>Lana</first><last>Yeganova</last></author>
      <author><first>Dina</first><last>Wiemann</last></author>
      <author><first>Cristian</first><last>Grozea</last></author>
      <pages>43–54</pages>
      <abstract>We present an overview of the Biomedical Translation Task that was part of the Eighth Conference on Machine Translation (WMT23). The aim of the task was the automatic translation of biomedical abstracts from the PubMed database. It included twelve language directions, namely, French, Spanish, Portuguese, Italian, German, and Russian, from and into English. We received submissions from 18 systems and for all the test sets that we released. Our comparison system was based on ChatGPT 3.5 and performed very well in comparison to many of the submissions.</abstract>
      <url hash="2fd919bb">2023.wmt-1.2</url>
      <bibkey>neves-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.2</doi>
    </paper>
    <paper id="3">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Yan</first><last>Gu</last></author>
      <author><first>Siyou</first><last>Liu</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Chenyang</first><last>Lyu</last></author>
      <author><first>Liting</first><last>Zhou</last></author>
      <author><first>Chao-Hong</first><last>Liu</last></author>
      <author><first>Yufeng</first><last>Ma</last></author>
      <author><first>Weiyu</first><last>Chen</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Yulin</first><last>Yuan</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>55–67</pages>
      <abstract>Translating literary works has perennially stood as an elusive dream in machine translation (MT), a journey steeped in intricate challenges. To foster progress in this domain, we hold a new shared task at WMT 2023, the first edition of the Discourse-Level Literary Translation. First, we (Tencent AI Lab and China Literature Ltd.) release a copyrighted and document-level Chinese-English web novel corpus. Furthermore, we put forth an industry-endorsed criteria to guide human evaluation process. This year, we totally received 14 submissions from 7 academia and industry teams. We employ both automatic and human evaluations to measure the performance of the submitted systems. The official ranking of the systems is based on the overall human judgments. In addition, our extensive analysis reveals a series of interesting findings on literary and discourse-aware MT. We release data, system outputs, and leaderboard at http://www2.statmt.org/wmt23/literary-translation-task.html.</abstract>
      <url hash="5782e632">2023.wmt-1.3</url>
      <bibkey>wang-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.3</doi>
    </paper>
    <paper id="4">
      <title>Findings of the Second <fixed-case>WMT</fixed-case> Shared Task on Sign Language Translation (<fixed-case>WMT</fixed-case>-<fixed-case>SLT</fixed-case>23)</title>
      <author><first>Mathias</first><last>Müller</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Richard</first><last>Bowden</last></author>
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Necati</first><last>Cihan Camgöz</last></author>
      <author><first>Sarah</first><last>Ebling</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Anne</first><last>Göhring</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Mert</first><last>Inan</last></author>
      <author><first>Zifan</first><last>Jiang</last></author>
      <author><first>Oscar</first><last>Koller</last></author>
      <author><first>Amit</first><last>Moryossef</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Sandra</first><last>Sidler-Miserez</last></author>
      <author><first>Katja</first><last>Tissi</last></author>
      <author><first>Davy</first><last>Van Landuyt</last></author>
      <pages>68–94</pages>
      <abstract>This paper presents the results of the Second WMT Shared Task on Sign Language Translation (WMT-SLT23; https://www.wmt-slt.com/). This shared task is concerned with automatic translation between signed and spoken languages. The task is unusual in the sense that it requires processing visual information (such as video frames or human pose estimation) beyond the well-known paradigm of text-to-text machine translation (MT). The task offers four tracks involving the following languages: Swiss German Sign Language (DSGS), French Sign Language of Switzerland (LSF-CH), Italian Sign Language of Switzerland (LIS-CH), German, French and Italian. Four teams (including one working on a baseline submission) participated in this second edition of the task, all submitting to the DSGS-to-German track. Besides a system ranking and system papers describing state-of-the-art techniques, this shared task makes the following scientific contributions: novel corpora and reproducible baseline systems. Finally, the task also resulted in publicly available sets of system outputs and more human evaluation scores for sign language translation.</abstract>
      <url hash="49c83597">2023.wmt-1.4</url>
      <bibkey>muller-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.4</doi>
    </paper>
    <paper id="5">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Shared Task on Parallel Data Curation</title>
      <author><first>Steve</first><last>Sloto</last></author>
      <author><first>Brian</first><last>Thompson</last></author>
      <author><first>Huda</first><last>Khayrallah</last></author>
      <author><first>Tobias</first><last>Domhan</last></author>
      <author><first>Thamme</first><last>Gowda</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>95–102</pages>
      <abstract>Building upon prior WMT shared tasks in document alignment and sentence filtering, we posed the open-ended shared task of finding the best subset of possible training data from a collection of Estonian-Lithuanian web data. Participants could focus on any portion of the end-to-end data curation pipeline, including alignment and filtering. We evaluated results based on downstream machine translation quality. We release processed Common Crawl data, along with various intermediate states from a strong baseline system, which we believe will enable future research on this topic.</abstract>
      <url hash="aedecab8">2023.wmt-1.5</url>
      <bibkey>sloto-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>S</fixed-case>amsung <fixed-case>R</fixed-case>&amp;<fixed-case>D</fixed-case> Institute <fixed-case>P</fixed-case>hilippines at <fixed-case>WMT</fixed-case> 2023</title>
      <author><first>Jan Christian Blaise</first><last>Cruz</last></author>
      <pages>103–109</pages>
      <abstract>In this paper, we describe the constrained submission systems of Samsung R&amp;D Institute Philippines to the WMT 2023 General Translation Task for two directions: en-&gt;he and he-&gt;en. Our systems comprise of Transformer-based sequence-to-sequence models that are trained with a mix of best practices: comprehensive data preprocessing pipelines, synthetic backtranslated data, and the use of noisy channel reranking during online decoding. Our models perform comparably to, and sometimes outperform, strong baseline unconstrained systems such as mBART50 M2M and NLLB 200 MoE despite having significantly fewer parameters on two public benchmarks: FLORES-200 and NTREX-128.</abstract>
      <url hash="1574937f">2023.wmt-1.6</url>
      <bibkey>cruz-2023-samsung</bibkey>
      <doi>10.18653/v1/2023.wmt-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>NAIST</fixed-case>-<fixed-case>NICT</fixed-case> <fixed-case>WMT</fixed-case>’23 General <fixed-case>MT</fixed-case> Task Submission</title>
      <author><first>Hiroyuki</first><last>Deguchi</last></author>
      <author><first>Kenji</first><last>Imamura</last></author>
      <author><first>Yuto</first><last>Nishida</last></author>
      <author><first>Yusuke</first><last>Sakai</last></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <pages>110–118</pages>
      <abstract>In this paper, we describe our NAIST-NICT submission to the WMT’23 English ↔ Japanese general machine translation task. Our system generates diverse translation candidates and reranks them using a two-stage reranking system to find the best translation. First, we generated 50 candidates each from 18 translation methods using a variety of techniques to increase the diversity of the translation candidates. We trained seven models per language direction using various combinations of hyperparameters. From these models we used various decoding algorithms, ensembling the models, and using kNN-MT (Khandelwal et al., 2021). We processed the 900 translation candidates through a two-stage reranking system to find the most promising candidate. In the first step, we compared 50 candidates from each translation method using DrNMT (Lee et al., 2021) and returned the candidate with the best score. We ranked the final 18 candidates using COMET-MBR (Fernandes et al., 2022) and returned the best score as the system output. We found that generating diverse translation candidates improved translation quality using the well-designed reranker model.</abstract>
      <url hash="8b1719a6">2023.wmt-1.7</url>
      <bibkey>deguchi-etal-2023-naist</bibkey>
      <doi>10.18653/v1/2023.wmt-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>CUNI</fixed-case> at <fixed-case>WMT</fixed-case>23 General Translation Task: <fixed-case>MT</fixed-case> and a Genetic Algorithm</title>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>119–127</pages>
      <abstract>This paper presents the contributions of Charles University teams to the WMT23 General translation task (English to Czech and Czech to Ukrainian translation directions). Our main submission, CUNI-GA, is a result of applying a novel n-best list reranking and modification method on translation candidates produced by the two other submitted systems, CUNI-Transformer and CUNI-DocTransformer (document-level translation only used for the <tex-math>en \rightarrow cs</tex-math> direction). Our method uses a genetic algorithm and MBR decoding to search for optimal translation under a given metric (in our case, a weighted combination of ChrF, BLEU, COMET22-DA, and COMET22-QE-DA). Our submissions are first in the constrained track and show competitive performance against top-tier unconstrained systems across various automatic metrics.</abstract>
      <url hash="62329ec6">2023.wmt-1.8</url>
      <bibkey>jon-etal-2023-cuni</bibkey>
      <doi>10.18653/v1/2023.wmt-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>SKIM</fixed-case> at <fixed-case>WMT</fixed-case> 2023 General Translation Task</title>
      <author><first>Keito</first><last>Kudo</last></author>
      <author><first>Takumi</first><last>Ito</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <pages>128–136</pages>
      <abstract>The SKIM team’s submission used a standard procedure to build ensemble Transformer models, including base-model training, back-translation of base models for data augmentation, and retraining of several final models using back-translated training data. Each final model had its own architecture and configuration, including up to 10.5B parameters, and substituted self- and cross-sublayers in the decoder with a cross+self-attention sub-layer. We selected the best candidate from a large candidate pool, namely 70 translations generated from 13 distinct models for each sentence, using an MBR reranking method using COMET and COMET-QE. We also applied data augmentation and selection techniques to the training data of the Transformer models.</abstract>
      <url hash="fe0c147e">2023.wmt-1.9</url>
      <bibkey>kudo-etal-2023-skim</bibkey>
      <doi>10.18653/v1/2023.wmt-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>KYB</fixed-case> General Machine Translation Systems for <fixed-case>WMT</fixed-case>23</title>
      <author><first>Ben</first><last>Li</last></author>
      <author><first>Yoko</first><last>Matsuzaki</last></author>
      <author><first>Shivam</first><last>Kalkar</last></author>
      <pages>137–142</pages>
      <abstract>This paper describes our approach to constructing a neural machine translation system for the WMT 2023 general machine translation shared task. Our model is based on the Transformer architecture’s base settings. We optimize system performance through various strategies. Enhancing our model’s capabilities involves fine-tuning the pretrained model with an extended dataset. To further elevate translation quality, specialized pre- and post-processing techniques are deployed. Our central focus is on efficient model training, aiming for exceptional accuracy through the synergy of a compact model and curated data. We also performed ensembling augmented by N-best ranking, for both directions of English to Japanese and Japanese to English translation.</abstract>
      <url hash="5ad80ab4">2023.wmt-1.10</url>
      <bibkey>li-etal-2023-kyb</bibkey>
      <doi>10.18653/v1/2023.wmt-1.10</doi>
    </paper>
    <paper id="11">
      <title>Yishu: Yishu at <fixed-case>WMT</fixed-case>2023 Translation Task</title>
      <author><first>Luo</first><last>Min</last></author>
      <author><first>Yixin</first><last>Tan</last></author>
      <author><first>Qiulin</first><last>Chen</last></author>
      <pages>143–149</pages>
      <abstract>This paper introduces the Dtranx AI translation system, developed for the WMT 2023 Universal Translation Shared Task. Our team participated in two language directions: English to Chinese and Chinese to English. Our primary focus was on enhancing the effectiveness of the Chinese-to-English model through the implementation of bilingual models. Our approach involved various techniques such as data corpus filtering, model size scaling, sparse expert models (especially the Transformer model with adapters), large-scale back-translation, and language model reordering. According to automatic evaluation, our system secured the first place in the English-to-Chinese category and the second place in the Chinese-to-English category.</abstract>
      <url hash="5d897b26">2023.wmt-1.11</url>
      <bibkey>min-etal-2023-yishu</bibkey>
      <doi>10.18653/v1/2023.wmt-1.11</doi>
      <revision id="1" href="2023.wmt-1.11v1" hash="fb5f60ec"/>
      <revision id="2" href="2023.wmt-1.11v2" hash="5d897b26" date="2024-03-19">Updated the format.</revision>
    </paper>
    <paper id="12">
      <title><fixed-case>PROMT</fixed-case> Systems for <fixed-case>WMT</fixed-case>23 Shared General Translation Task</title>
      <author><first>Alexander</first><last>Molchanov</last></author>
      <author><first>Vladislav</first><last>Kovalenko</last></author>
      <pages>150–154</pages>
      <abstract>This paper describes the PROMT submissions for the WMT23 Shared General Translation Task. This year we participated in two directions of the Shared Translation Task: English to Russian and Russian to English. Our models are trained with the MarianNMT toolkit using the transformer-big configuration. We use BPE for text encoding, both models are unconstrained. We achieve competitive results according to automatic metrics in both directions.</abstract>
      <url hash="8559f5e1">2023.wmt-1.12</url>
      <bibkey>molchanov-kovalenko-2023-promt</bibkey>
      <doi>10.18653/v1/2023.wmt-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>AIST</fixed-case> <fixed-case>AIRC</fixed-case> Submissions to the <fixed-case>WMT</fixed-case>23 Shared Task</title>
      <author><first>Matiss</first><last>Rikters</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <pages>155–161</pages>
      <abstract>This paper describes the development process of NMT systems that were submitted to the WMT 2023 General Translation task by the team of AIST AIRC. We trained constrained track models for translation between English, German, and Japanese. Before training the final models, we first filtered the parallel and monolingual data, then performed iterative back-translation as well as parallel data distillation to be used for non-autoregressive model training. We experimented with training Transformer models, Mega models, and custom non-autoregressive sequence-to-sequence models with encoder and decoder weights initialised by a multilingual BERT base. Our primary submissions contain translations from ensembles of two Mega model checkpoints and our contrastive submissions are generated by our non-autoregressive models.</abstract>
      <url hash="4a31fc0f">2023.wmt-1.13</url>
      <bibkey>rikters-miwa-2023-aist</bibkey>
      <doi>10.18653/v1/2023.wmt-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>MUNI</fixed-case>-<fixed-case>NLP</fixed-case> Submission for <fixed-case>C</fixed-case>zech-<fixed-case>U</fixed-case>krainian Translation Task at <fixed-case>WMT</fixed-case>23</title>
      <author><first>Pavel</first><last>Rychly</last></author>
      <author><first>Yuliia</first><last>Teslia</last></author>
      <pages>162–165</pages>
      <abstract>The system is trained on officialy provided data only. We have heavily filtered all the data to remove machine translated text, Russian text and other noise. We use the DeepNorm modification of the transformer architecture in the TorchScale library with 18 encoder layers and 6 decoder layers. The initial systems for backtranslation uses HFT tokenizer, the final system uses custom tokenizer derived from HFT.</abstract>
      <url hash="e40dc047">2023.wmt-1.14</url>
      <bibkey>rychly-teslia-2023-muni</bibkey>
      <doi>10.18653/v1/2023.wmt-1.14</doi>
    </paper>
    <paper id="15">
      <title>Exploring Prompt Engineering with <fixed-case>GPT</fixed-case> Language Models for Document-Level Machine Translation: Insights and Findings</title>
      <author><first>Yangjian</first><last>Wu</last></author>
      <author><first>Gang</first><last>Hu</last></author>
      <pages>166–169</pages>
      <abstract>This paper describes Lan-Bridge Translation systems for the WMT 2023 General Translation shared task. We participate in 2 directions: English to and from Chinese. With the emergence of large-scale models, various industries have undergone significant transformations, particularly in the realm of document-level machine translation. This has introduced a novel research paradigm that we have embraced in our participation in the WMT23 competition. Focusing on advancements in models such as GPT-3.5 and GPT-4, we have undertaken numerous prompt-based experiments. Our objective is to achieve optimal human evaluation results for document-level machine translation, resulting in our submission of the final outcomes in the general track.</abstract>
      <url hash="f142d9e9">2023.wmt-1.15</url>
      <bibkey>wu-hu-2023-exploring</bibkey>
      <doi>10.18653/v1/2023.wmt-1.15</doi>
    </paper>
    <paper id="16">
      <title>Treating General <fixed-case>MT</fixed-case> Shared Task as a Multi-Domain Adaptation Problem: <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case>23 General <fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Shaojun</first><last>Li</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Yuhao</first><last>Xie</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yanfei</first><last>Jiang</last></author>
      <pages>170–174</pages>
      <abstract>This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT23 general machine translation (MT) shared task, in which we participate in Chinese↔English (zh↔en) language pair. We use Transformer architecture and obtain the best performance via a variant with larger parameter size. We perform fine-grained pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. We mainly use model enhancement strategies, including Regularized Dropout, Bidirectional Training, Data Diversification, Forward Translation, Back Translation, Alternated Training, Curriculum Learning and Transductive Ensemble Learning. Our submissions obtain competitive results in the final evaluation.</abstract>
      <url hash="0fda4630">2023.wmt-1.16</url>
      <bibkey>wu-etal-2023-treating</bibkey>
      <doi>10.18653/v1/2023.wmt-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>U</fixed-case>v<fixed-case>A</fixed-case>-<fixed-case>MT</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2023 General Translation Shared Task</title>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Shaomu</first><last>Tan</last></author>
      <author><first>David</first><last>Stap</last></author>
      <author><first>Ali</first><last>Araabi</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <pages>175–180</pages>
      <abstract>This paper describes the UvA-MT’s submission to the WMT 2023 shared task on general machine translation. We participate in the constrained track in two directions: English <tex-math>\leftrightarrow</tex-math> Hebrew. In this competition, we show that by using one model to handle bidirectional tasks, as a minimal setting of Multilingual Machine Translation (MMT), it is possible to achieve comparable results with that of traditional bilingual translation for both directions. By including effective strategies, like back-translation, re-parameterized embedding table, and task-oriented fine-tuning, we obtained competitive final results in the automatic evaluation for both English <tex-math>\rightarrow</tex-math> Hebrew and Hebrew <tex-math>\rightarrow</tex-math> English directions.</abstract>
      <url hash="5fa7a958">2023.wmt-1.17</url>
      <bibkey>wu-etal-2023-uva</bibkey>
      <doi>10.18653/v1/2023.wmt-1.17</doi>
    </paper>
    <paper id="18">
      <title>Achieving State-of-the-Art Multilingual Translation Model with Minimal Data and Parameters</title>
      <author><first>Hui</first><last>Zeng</last></author>
      <pages>181–186</pages>
      <abstract>This is LanguageX (ZengHuiMT)’s submission to WMT 2023 General Machine Translation task for 13 language directions. We initially employ an encoder-decoder model to train on all 13 competition translation directions as our baseline system. Subsequently, we adopt a decoder-only architecture and fine-tune a multilingual language model by partially sampling data from diverse multilingual datasets such as CC100 and WuDaoCorpora. This is further refined using carefully curated high-quality parallel corpora across multiple translation directions to enable the model to perform translation tasks. As per automated evaluation metrics, our model ranks first in the translation directions from English to Russian, English to German, and English to Ukrainian. It secures the second position in the directions from English to Czech, English to Hebrew, Hebrew to English, and Ukrainian to English, and ranks third in German to English, Japanese to English, and Russian to English among all participating teams. Our best-performing model, covering 13 translation directions, stands on par with GPT-4. Among all 13 translation directions, our multilingual model surpasses GPT-4 in bleu scores for 7 translation directions.</abstract>
      <url hash="f997b21d">2023.wmt-1.18</url>
      <bibkey>zeng-2023-achieving</bibkey>
      <doi>10.18653/v1/2023.wmt-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>IOL</fixed-case> Research Machine Translation Systems for <fixed-case>WMT</fixed-case>23 General Machine Translation Shared Task</title>
      <author><first>Wenbo</first><last>Zhang</last></author>
      <pages>187–191</pages>
      <abstract>This paper describes the IOL Research team’s submission systems for the WMT23 general machine translation shared task. We participated in two language translation directions, including English-to-Chinese and Chinese-to-English. Our final primary submissions belong to constrained systems, which means for both translation directions we only use officially provided monolingual and bilingual data to train the translation systems. Our systems are based on Transformer architecture with pre-norm or deep-norm, which has been proven to be helpful for training deeper models. We employ methods such as back-translation, data diversification, domain fine-tuning and model ensemble to build our translation systems. An important aspect worth mentioning is our careful data cleaning process and the utilization of a substantial amount of monolingual data for data augmentation. Compared with the baseline system, our submissions have a large improvement in BLEU score.</abstract>
      <url hash="4b7720fc">2023.wmt-1.19</url>
      <bibkey>zhang-2023-iol</bibkey>
      <doi>10.18653/v1/2023.wmt-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>GTCOM</fixed-case> and <fixed-case>DLUT</fixed-case>’s Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>23</title>
      <author><first>Hao</first><last>Zong</last></author>
      <pages>192–197</pages>
      <abstract>This paper presents the submission by Global Tone Communication Co., Ltd. and Dalian Univeristy of Technology for the WMT23 shared general Machine Translation (MT) task at the Conference on Empirical Methods in Natural Language Processing (EMNLP). Our participation spans 8 language pairs, including English-Ukrainian, Ukrainian-English, Czech-Ukrainian, English-Hebrew, Hebrew-English, English-Czech, German-English, and Japanese-English. Our systems are designed without any specific constraints or requirements, allowing us to explore a wider range of possibilities in machine translation. We prioritize backtranslation, utilize multilingual translation models, and employ fine-tuning strategies to enhance performance. Additionally, we propose a novel data generation method that leverages human annotation to generate high-quality training data, resulting in improved system performance. Specifically, we use a combination of human-generated and machine-generated data to fine-tune our models, leading to more accurate translations. The automatic evaluation results show that our system ranks first in terms of BLEU score in Ukrainian-English, Hebrew-English, English-Hebrew, and German-English.</abstract>
      <url hash="328cb539">2023.wmt-1.20</url>
      <bibkey>zong-2023-gtcom</bibkey>
      <doi>10.18653/v1/2023.wmt-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>R</fixed-case>o<fixed-case>CS</fixed-case>-<fixed-case>MT</fixed-case>: Robustness Challenge Set for Machine Translation</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>198–216</pages>
      <abstract>RoCS-MT, a Robust Challenge Set for Machine Translation (MT), is designed to test MT systems’ ability to translate user-generated content (UGC) that displays non-standard characteristics, such as spelling errors, devowelling, acronymisation, etc. RoCS-MT is composed of English comments from Reddit, selected for their non-standard nature, which have been manually normalised and professionally translated into five languages: French, German, Czech, Ukrainian and Russian. In the context of the WMT23 test suite shared task, we analyse the models submitted to the general MT task for all from-English language pairs, offering some insights into the types of problems faced by state-of-the-art MT models when dealing with non-standard UGC texts. We compare automatic metrics for MT quality, including quality estimation to see if the same conclusions can be drawn without references. In terms of robustness, we find that many of the systems struggle with non-standard variants of words (e.g. due to phonetically inspired spellings, contraction, truncations, etc.), but that this depends on the system and the amount of training data, with the best overall systems performing better across all phenomena. GPT4 is the clear front-runner. However we caution against drawing conclusions about generalisation capacity as it and other systems could be trained on the source side of RoCS and also on similar data.</abstract>
      <url hash="c089f521">2023.wmt-1.21</url>
      <bibkey>bawden-sagot-2023-rocs</bibkey>
      <doi>10.18653/v1/2023.wmt-1.21</doi>
    </paper>
    <paper id="22">
      <title>Multifaceted Challenge Set for Evaluating Machine Translation Performance</title>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Ting</first><last>Zhu</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Ning</first><last>Xie</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yanfei</first><last>Jiang</last></author>
      <pages>217–223</pages>
      <abstract>Machine Translation Evaluation is critical to Machine Translation research, as the evaluation results reflect the effectiveness of training strategies. As a result, a fair and efficient evaluation method is necessary. Many researchers have raised questions about currently available evaluation metrics from various perspectives, and propose suggestions accordingly. However, to our knowledge, few researchers has analyzed the difficulty level of source sentence and its influence on evaluation results. This paper presents HW-TSC’s submission to the WMT23 MT Test Suites shared task. We propose a systematic approach for construing challenge sets from four aspects: word difficulty, length difficulty, grammar difficulty and model learning difficulty. We open-source two Multifaceted Challenge Sets for Zh→En and En→Zh. We also present results of participants in this year’s General MT shared task on our test sets.</abstract>
      <url hash="1663644b">2023.wmt-1.22</url>
      <bibkey>chen-etal-2023-multifaceted</bibkey>
      <doi>10.18653/v1/2023.wmt-1.22</doi>
    </paper>
    <paper id="23">
      <title>Linguistically Motivated Evaluation of the 2023 State-of-the-art Machine Translation: Can <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Outperform <fixed-case>NMT</fixed-case>?</title>
      <author><first>Shushen</first><last>Manakhimova</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Sergei</first><last>Bagdasarov</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>224–245</pages>
      <abstract>This paper offers a fine-grained analysis of the machine translation outputs in the context of the Shared Task at the 8th Conference of Machine Translation (WMT23). Building on the foundation of previous test suite efforts, our analysis includes Large Language Models and an updated test set featuring new linguistic phenomena. To our knowledge, this is the first fine-grained linguistic analysis for the GPT-4 translation outputs. Our evaluation spans German-English, English-German, and English-Russian language directions. Some of the phenomena with the lowest accuracies for German-English are idioms and resultative predicates. For English-German, these include mediopassive voice, and noun formation(er). As for English-Russian, these included idioms and semantic roles. GPT-4 performs equally or comparably to the best systems in German-English and English-German but falls in the second significance cluster for English-Russian.</abstract>
      <url hash="107f2bc6">2023.wmt-1.23</url>
      <bibkey>manakhimova-etal-2023-linguistically</bibkey>
      <doi>10.18653/v1/2023.wmt-1.23</doi>
      <revision id="1" href="2023.wmt-1.23v1" hash="c53a58bd"/>
      <revision id="2" href="2023.wmt-1.23v2" hash="107f2bc6" date="2024-03-19">Minor update.</revision>
    </paper>
    <paper id="24">
      <title><fixed-case>IIIT</fixed-case> <fixed-case>HYD</fixed-case>’s Submission for <fixed-case>WMT</fixed-case>23 Test-suite Task</title>
      <author><first>Ananya</first><last>Mukherjee</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>246–251</pages>
      <abstract>This paper summarizes the results of our test suite evaluation on 12 machine translation systems submitted at the Shared Task of the 8th Conference of Machine Translation (WMT23) for English-German (en-de) language pair. Our test suite covers five specific domains (entertainment, environment, health, science, legal) and spans five distinct writing styles (descriptive, judgments, narrative, reporting, technical-writing). We present our analysis through automatic evaluation methods, conducted with a focus on domain-specific and writing style-specific evaluations.</abstract>
      <url hash="7dacf0f0">2023.wmt-1.24</url>
      <bibkey>mukherjee-shrivastava-2023-iiit</bibkey>
      <doi>10.18653/v1/2023.wmt-1.24</doi>
    </paper>
    <paper id="25">
      <title>Test Suites Task: Evaluation of Gender Fairness in <fixed-case>MT</fixed-case> with <fixed-case>M</fixed-case>u<fixed-case>ST</fixed-case>-<fixed-case>SHE</fixed-case> and <fixed-case>INES</fixed-case></title>
      <author><first>Beatrice</first><last>Savoldi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <pages>252–262</pages>
      <abstract>As part of the WMT-2023 “Test suites” shared task, in this paper we summarize the results of two test suites evaluations: MuST-SHEWMT23 and INES. By focusing on the en-de and de-en language pairs, we rely on these newly created test suites to investigate systems’ ability to translate feminine and masculine gender and produce gender-inclusive translations. Furthermore we discuss metrics associated with our test suites and validate them by means of human evaluations. Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena. Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models, indicating room for future improvements and research on the topic. We make MuST-SHEWMT23 and INES freely available.</abstract>
      <url hash="7fea630d">2023.wmt-1.25</url>
      <bibkey>savoldi-etal-2023-test</bibkey>
      <doi>10.18653/v1/2023.wmt-1.25</doi>
    </paper>
    <paper id="26">
      <title>Biomedical Parallel Sentence Retrieval Using Large Language Models</title>
      <author><first>Sheema</first><last>Firdous</last></author>
      <author><first>Sadaf Abdul</first><last>Rauf</last></author>
      <pages>263–270</pages>
      <abstract>We have explored the effect of in domain knowledge during parallel sentence filtering from in domain corpora. Models built with sentences mined from in domain corpora without domain knowledge performed poorly, whereas model performance improved by more than 2.3 BLEU points on average with further domain centric filtering. We have used Large Language Models for selecting similar and domain aligned sentences. Our experiments show the importance of inclusion of domain knowledge in sentence selection methodologies even if the initial comparable corpora are in domain.</abstract>
      <url hash="c78f491f">2023.wmt-1.26</url>
      <bibkey>firdous-rauf-2023-biomedical</bibkey>
      <doi>10.18653/v1/2023.wmt-1.26</doi>
    </paper>
    <paper id="27">
      <title>The Path to Continuous Domain Adaptation Improvements by <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> for the <fixed-case>WMT</fixed-case>23 Biomedical Translation Shared Task</title>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Shaojun</first><last>Li</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Yuhao</first><last>Xie</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yanfei</first><last>Jiang</last></author>
      <pages>271–274</pages>
      <abstract>This paper presents the domain adaptation methods adopted by Huawei Translation Service Center (HW-TSC) to train the neural machine translation (NMT) system on the English↔German (en↔de) language pair of the WMT23 biomedical translation task. Our NMT system is built on deep Transformer with larger parameter sizes. Based on the biomedical NMT system trained last year, we leverage Curriculum Learning, Data Diversification, Forward translation, Back translation, and Transductive Ensemble Learning to further improve system performance. Overall, we believe our submission can achieve highly competitive result in the official final evaluation.</abstract>
      <url hash="137ee41e">2023.wmt-1.27</url>
      <bibkey>wu-etal-2023-path</bibkey>
      <doi>10.18653/v1/2023.wmt-1.27</doi>
    </paper>
    <paper id="28">
      <title>Investigating Techniques for a Deeper Understanding of Neural Machine Translation (<fixed-case>NMT</fixed-case>) Systems through Data Filtering and Fine-tuning Strategies</title>
      <author><first>Lichao</first><last>Zhu</last></author>
      <author><first>Maria</first><last>Zimina</last></author>
      <author><first>Maud</first><last>Bénard</last></author>
      <author><first>Behnoosh</first><last>Namdar</last></author>
      <author><first>Nicolas</first><last>Ballier</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Jean-Baptiste</first><last>Yunès</last></author>
      <pages>275–281</pages>
      <abstract>In the context of this biomedical shared task, we have implemented data filters to enhance the selection of relevant training data for fine- tuning from the available training data sources. Specifically, we have employed textometric analysis to detect repetitive segments within the test set, which we have then used for re- fining the training data used to fine-tune the mBart-50 baseline model. Through this approach, we aim to achieve several objectives: developing a practical fine-tuning strategy for training biomedical in-domain fr&lt;&gt;en models, defining criteria for filtering in-domain training data, and comparing model predictions, fine-tuning data in accordance with the test set to gain a deeper insight into the functioning of Neural Machine Translation (NMT) systems.</abstract>
      <url hash="68bcc3ff">2023.wmt-1.28</url>
      <bibkey>zhu-etal-2023-investigating</bibkey>
      <doi>10.18653/v1/2023.wmt-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>MAX</fixed-case>-<fixed-case>ISI</fixed-case> System at <fixed-case>WMT</fixed-case>23 Discourse-Level Literary Translation Task</title>
      <author><first>Li</first><last>An</last></author>
      <author><first>Linghao</first><last>Jin</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <pages>282–286</pages>
      <abstract>This paper describes our translation systems for the WMT23 shared task. We participated in the discourse-level literary translation task - constrained track. In our methodology, we conduct a comparative analysis between the conventional Transformer model and the recently introduced MEGA model, which exhibits enhanced capabilities in modeling long-range sequences compared to the traditional Transformers. To explore whether language models can more effectively harness document-level context using paragraph-level data, we took the approach of aggregating sentences into paragraphs from the original literary dataset provided by the organizers. This paragraph-level data was utilized in both the Transformer and MEGA models. To ensure a fair comparison across all systems, we employed a sentence-alignment strategy to reverse our translation results from the paragraph-level back to the sentence-level alignment. Finally, our evaluation process encompassed sentence-level metrics such as BLEU, as well as two document-level metrics: d-BLEU and BlonDe.</abstract>
      <url hash="8285d828">2023.wmt-1.29</url>
      <bibkey>an-etal-2023-max</bibkey>
      <doi>10.18653/v1/2023.wmt-1.29</doi>
    </paper>
    <paper id="30">
      <title>The <fixed-case>MAKE</fixed-case>-<fixed-case>NMTVIZ</fixed-case> System Description for the <fixed-case>WMT</fixed-case>23 Literary Task</title>
      <author><first>Fabien</first><last>Lopez</last></author>
      <author><first>Gabriela</first><last>González</last></author>
      <author><first>Damien</first><last>Hansen</last></author>
      <author><first>Mariam</first><last>Nakhle</last></author>
      <author><first>Behnoosh</first><last>Namdarzadeh</last></author>
      <author><first>Nicolas</first><last>Ballier</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last></author>
      <author><first>Sui</first><last>He</last></author>
      <author><first>Sadaf</first><last>Mohseni</last></author>
      <author><first>Caroline</first><last>Rossi</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Jun</first><last>Yang</last></author>
      <author><first>Jean-Baptiste</first><last>Yunès</last></author>
      <author><first>Lichao</first><last>Zhu</last></author>
      <pages>287–295</pages>
      <abstract>This paper describes the MAKE-NMTVIZ Systems trained for the WMT 2023 Literary task. As a primary submission, we used Train, Valid1, test1 as part of the GuoFeng corpus (Wang et al., 2023) to fine-tune the mBART50 model with Chinese-English data. We followed very similar training parameters to (Lee et al. 2022) when fine-tuning mBART50. We trained for 3 epochs, using gelu as an activation function, with a learning rate of 0.05, dropout of 0.1 and a batch size of 16. We decoded using a beam search of size 5. For our contrastive1 submission, we implemented a fine-tuned concatenation transformer (Lupo et al., 2023). The training was developed in two steps: (i) a sentence-level transformer was implemented for 10 epochs trained using general, test1, and valid1 data (more details in contrastive2 system); (ii) second, we fine-tuned at document-level using 3-sentence concatenation for 4 epochs using train, test2, and valid2 data. During the fine-tuning, we used ReLU as an activation function, with an inverse square root learning rate, dropout of 0.1, and a batch size of 64. We decoded using a beam search of size. Four our contrastive2 and last submission, we implemented a sentence-level transformer model (Vaswani et al., 2017). The model was trained with general data for 10 epochs using general-purpose, test1, and valid 1 data. The training parameters were an inverse square root scheduled learning rate, a dropout of 0.1, and a batch size of 64. We decoded using a beam search of size 4. We then compared the three translation outputs from an interdisciplinary perspective, investigating some of the effects of sentence- vs document-based training. Computer scientists, translators and corpus linguists discussed the linguistic remaining issues for this discourse-level literary translation.</abstract>
      <url hash="a2b3c5a1">2023.wmt-1.30</url>
      <bibkey>lopez-etal-2023-make</bibkey>
      <doi>10.18653/v1/2023.wmt-1.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>DUTNLP</fixed-case> System for the <fixed-case>WMT</fixed-case>2023 Discourse-Level Literary Translation</title>
      <author><first>Anqi</first><last>Zhao</last></author>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>296–301</pages>
      <abstract>This paper describes the submission of DUTNLP Lab submission to WMT23 Discourse-Level Literary Translation in the Chinese to English translation direction under unconstrained conditions. Our primary system aims to leverage a large language model with various prompt strategies, which can fully investigate the potential capabilities of large language models for discourse-level neural machine translation. Moreover, we test a widely used discourse-level machine translation model, G-transformer, with different training strategies. In our experimental results, the method with large language models achieves a BLEU score of 28.16, while the fine-tuned method scores 25.26. These findings indicate that selecting appropriate prompt strategies based on large language models can significantly improve translation performance compared to traditional model training methods.</abstract>
      <url hash="1e832340">2023.wmt-1.31</url>
      <bibkey>zhao-etal-2023-dutnlp</bibkey>
      <doi>10.18653/v1/2023.wmt-1.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submissions to the <fixed-case>WMT</fixed-case>23 Discourse-Level Literary Translation Shared Task</title>
      <author><first>Yuhao</first><last>Xie</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Zhiqiang</first><last>Rao</last></author>
      <author><first>Shaojun</first><last>Li</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yanfei</first><last>Jiang</last></author>
      <pages>302–306</pages>
      <abstract>This paper introduces HW-TSC’s submission to the WMT23 Discourse-Level Literary Translation shared task. We use standard sentence-level transformer as a baseline, and perform domain adaptation and discourse modeling to enhance discourse-level capabilities. Regarding domain adaptation, we employ Back-Translation, Forward-Translation and Data Diversification. For discourse modeling, we apply strategies such as Multi-resolutional Document-to-Document Translation and TrAining Data Augmentation.</abstract>
      <url hash="e6646057">2023.wmt-1.32</url>
      <bibkey>xie-etal-2023-hw</bibkey>
      <doi>10.18653/v1/2023.wmt-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>TJUNLP</fixed-case>:System Description for the <fixed-case>WMT</fixed-case>23 Literary Task in <fixed-case>C</fixed-case>hinese to <fixed-case>E</fixed-case>nglish Translation Direction</title>
      <author><first>Shaolin</first><last>Zhu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>307–311</pages>
      <abstract>This paper introduces the overall situation of the Natural Language Processing Laboratory of Tianjin University participating in the WMT23 machine translation evaluation task from Chinese to English. For this evaluation, the base model used is a Transformer based on a Mixture of Experts (MOE) model. During the model’s construction and training, a basic dense model based on Transformer is first trained on the training set. Then, this model is used to initialize the MOE-based translation model, which is further trained on the training corpus. Since the training dataset provided for this translation task is relatively small, to better utilize sparse models to enhance translation, we employed a data augmentation technique for alignment. Experimental results show that this method can effectively improve neural machine translation performance.</abstract>
      <url hash="f94a5350">2023.wmt-1.33</url>
      <bibkey>zhu-xiong-2023-tjunlp</bibkey>
      <doi>10.18653/v1/2023.wmt-1.33</doi>
    </paper>
    <paper id="34">
      <title>Machine Translation for Nko: Tools, Corpora, and Baseline Results</title>
      <author><first>Moussa</first><last>Doumbouya</last></author>
      <author><first>Baba Mamadi</first><last>Diané</last></author>
      <author><first>Solo Farabado</first><last>Cissé</last></author>
      <author><first>Djibrila</first><last>Diané</last></author>
      <author><first>Abdoulaye</first><last>Sow</last></author>
      <author><first>Séré Moussa</first><last>Doumbouya</last></author>
      <author><first>Daouda</first><last>Bangoura</last></author>
      <author><first>Fodé Moriba</first><last>Bayo</last></author>
      <author><first>Ibrahima Sory</first><last>Conde</last></author>
      <author><first>Kalo Mory</first><last>Diané</last></author>
      <author><first>Chris</first><last>Piech</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>312–343</pages>
      <abstract>Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Fria<tex-math>\parallel</tex-math>el: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the best model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.</abstract>
      <url hash="2cc5f887">2023.wmt-1.34</url>
      <bibkey>doumbouya-etal-2023-machine</bibkey>
      <doi>10.18653/v1/2023.wmt-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>TTIC</fixed-case>’s Submission to <fixed-case>WMT</fixed-case>-<fixed-case>SLT</fixed-case> 23</title>
      <author><first>Marcelo</first><last>Sandoval-Castaneda</last></author>
      <author><first>Yanhong</first><last>Li</last></author>
      <author><first>Bowen</first><last>Shi</last></author>
      <author><first>Diane</first><last>Brentari</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <author><first>Gregory</first><last>Shakhnarovich</last></author>
      <pages>344–350</pages>
      <abstract>In this paper, we describe TTIC’s submission to WMT 2023 Sign Language Translation task on the Swiss-German Sign Language (DSGS) to German track. Our approach explores the advantages of using large-scale self-supervised pre-training in the task of sign language translation, over more traditional approaches that rely heavily on supervision, along with costly labels such as gloss annotations. The proposed model consists of a VideoSwin transformer for image encoding, and a T5 model adapted to receive VideoSwin features as input instead of text. In WMT-SLT 22’s development set, this system achieves 2.03 BLEU score, a 59% increase over the previous best reported performance. In the official test set, our primary submission achieves 1.1 BLEU score and 17.0 chrF score.</abstract>
      <url hash="4a949eef">2023.wmt-1.35</url>
      <bibkey>sandoval-castaneda-etal-2023-ttics</bibkey>
      <doi>10.18653/v1/2023.wmt-1.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>K</fixed-case>now<fixed-case>C</fixed-case>omp Submission for <fixed-case>WMT</fixed-case>23 Sign Language Translation Task</title>
      <author><first>Baixuan</first><last>Xu</last></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Tianshi</first><last>Zheng</last></author>
      <author><first>Qing</first><last>Zong</last></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Zhaowei</first><last>Wang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <pages>351–358</pages>
      <abstract>Sign Language Translation (SLT) is a complex task that involves accurately interpreting sign language gestures and translating them into spoken or written language and vice versa. Its primary objective is to facilitate communication between individuals with hearing difficulties using deep learning systems. Existing approaches leverage gloss annotations of sign language gestures to assist the model in capturing the movement and differentiating various gestures. However, constructing a large-scale gloss-annotated dataset is both expensive and impractical to cover multiple languages, and pre-trained generative models cannot be efficiently used due to the lack of textual source context in SLT. To address these challenges, we propose a gloss-free framework for the WMT23 SLT task. Our system primarily consists of a visual extractor for extracting video embeddings and a generator responsible for producing the translated text. We also employ an embedding alignment block that is trained to align the embedding space of the visual extractor with that of the generator. Despite undergoing extensive training and validation, our system consistently falls short of meeting the baseline performance. Further analysis shows that our model’s poor projection rate prevents it from learning diverse visual embeddings. Our codes and model checkpoints are available at https://github.com/HKUST-KnowComp/SLT.</abstract>
      <url hash="8316145e">2023.wmt-1.36</url>
      <bibkey>xu-etal-2023-knowcomp</bibkey>
      <doi>10.18653/v1/2023.wmt-1.36</doi>
    </paper>
    <paper id="37">
      <title>A Fast Method to Filter Noisy Parallel Data <fixed-case>WMT</fixed-case>2023 Shared Task on Parallel Data Curation</title>
      <author><first>Nguyen-Hoang</first><last>Minh-Cong</last></author>
      <author><first>Nguyen Van</first><last>Vinh</last></author>
      <author><first>Nguyen</first><last>Le-Minh</last></author>
      <pages>359–365</pages>
      <abstract>The effectiveness of a machine translation (MT) system is intricately linked to the quality of its training dataset. In an era where websites offer an extensive repository of translations such as movie subtitles, stories, and TED Talks, the fundamental challenge resides in pinpointing the sentence pairs or documents that represent accurate translations of each other. This paper presents the results of our submission to the shared task WMT2023 (Sloto et al., 2023), which aimed to evaluate parallel data curation methods for improving the MT system. The task involved alignment and filtering data to create high-quality parallel corpora for training and evaluating the MT models. Our approach leveraged a combination of dictionary and rule-based methods to ensure data quality and consistency. We achieved an improvement with the highest 1.6 BLEU score compared to the baseline system. Significantly, our approach showed consistent improvements across all test sets, suggesting its efficiency.</abstract>
      <url hash="a0c3b313">2023.wmt-1.37</url>
      <bibkey>minh-cong-etal-2023-fast</bibkey>
      <doi>10.18653/v1/2023.wmt-1.37</doi>
    </paper>
    <paper id="38">
      <title>A Sentence Alignment Approach to Document Alignment and Multi-faceted Filtering for Curating Parallel Sentence Pairs from Web-crawled Data</title>
      <author><first>Steinthor</first><last>Steingrimsson</last></author>
      <pages>366–374</pages>
      <abstract>This paper describes the AST submission to the WMT23 Shared Task on Parallel Data Curation. We experiment with two approaches for curating data from the provided web-scraped texts. We use sentence alignment to identify document alignments in the data and extract parallel sentence pairs from the aligned documents. All other sentences, not aligned in that step, are paired based on cosine similarity before we apply various different filters. For filtering, we use language detection, fluency classification, word alignments, cosine distance as calculated by multilingual sentence embedding models, and Bicleaner AI. Our best model outperforms the baseline by 1.9 BLEU points on average over the four provided evaluation sets.</abstract>
      <url hash="e4fb9079">2023.wmt-1.38</url>
      <bibkey>steingrimsson-2023-sentence</bibkey>
      <doi>10.18653/v1/2023.wmt-1.38</doi>
    </paper>
    <paper id="39">
      <title>Document-Level Language Models for Machine Translation</title>
      <author><first>Frithjof</first><last>Petrick</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Pavel</first><last>Petrushkov</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>375–391</pages>
      <abstract>Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores significantly and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system. Finally, we explore language model fusion in the light of recent advancements in large language models. Our findings suggest that there might be strong potential in utilizing large language models via model combination.</abstract>
      <url hash="bfe181a6">2023.wmt-1.39</url>
      <bibkey>petrick-etal-2023-document</bibkey>
      <doi>10.18653/v1/2023.wmt-1.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> <fixed-case>MT</fixed-case>: Competitive for High- (but Not Low-) Resource Languages</title>
      <author><first>Nathaniel</first><last>Robinson</last></author>
      <author><first>Perez</first><last>Ogayo</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>392–418</pages>
      <abstract>Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.</abstract>
      <url hash="99414242">2023.wmt-1.40</url>
      <bibkey>robinson-etal-2023-chatgpt</bibkey>
      <doi>10.18653/v1/2023.wmt-1.40</doi>
    </paper>
    <paper id="41">
      <title>Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist</title>
      <author><first>Marzena</first><last>Karpinska</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>419–451</pages>
      <abstract>Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system’s translations are better. We observe that discourse-level LLM translators commit fewer mistranslations, grammar errors, and stylistic inconsistencies than sentence-level approaches. With that said, critical errors still abound, including occasional content omissions, and a human translator’s intervention remains necessary to ensure that the author’s voice remains intact. We publicly release our dataset and error annotations to spur future research on the evaluation of document-level literary translation.</abstract>
      <url hash="5bb0fcc4">2023.wmt-1.41</url>
      <bibkey>karpinska-iyyer-2023-large</bibkey>
      <doi>10.18653/v1/2023.wmt-1.41</doi>
    </paper>
    <paper id="42">
      <title>Identifying Context-Dependent Translations for Evaluation Set Production</title>
      <author><first>Rachel</first><last>Wicks</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>452–467</pages>
      <abstract>A major impediment to the transition to contextual machine translation is the absence of good evaluation metrics and test sets. Sentences that require context to be translated correctly are rare in test sets, reducing the utility of standard corpus-level metrics such as COMET or BLEU. On the other hand, datasets that annotate such sentences are also rare, small in scale, and available for only a few languages. To address this, we modernize, generalize, and extend previous annotation pipelines to produce MultiPro, a tool that identifies subsets of parallel documents containing sentences that require context to correctly translate five phenomena: gender, formality, and animacy for pronouns, verb phrase ellipsis, and ambiguous noun inflections. The input to the pipeline is a set of hand-crafted, per-language, linguistically-informed rules that select contextual sentence pairs using coreference, part-of-speech, and morphological features provided by state-of-the-art tools. We apply this pipeline to seven languages pairs (EN into and out-of DE, ES, FR, IT, PL, PT, and RU) and two datasets (OpenSubtitles and WMT test sets), and validate its performance using both overlap with previous work and its ability to discriminate a contextual MT system from a sentence-based one. We release the MultiPro pipeline and data as open source.</abstract>
      <url hash="6367717a">2023.wmt-1.42</url>
      <bibkey>wicks-post-2023-identifying</bibkey>
      <doi>10.18653/v1/2023.wmt-1.42</doi>
    </paper>
    <paper id="43">
      <title>Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with <fixed-case>QL</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Xuan</first><last>Zhang</last></author>
      <author><first>Navid</first><last>Rajabi</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>468–481</pages>
      <abstract>While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored. In our study, we conduct comprehensive experiments, evaluating 15 publicly available language models on machine translation tasks. We compare the performance across three methodologies: zero-shot prompting, few-shot learning, and fine-tuning. Central to our approach is the use of QLoRA, an efficient fine-tuning method. On French-English, QLoRA fine-tuning outperforms both few-shot learning and models trained from scratch. This superiority is highlighted in both sentence-level and document-level translations, with a significant BLEU score improvement of 28.93 over the prompting method. Impressively, with QLoRA, the enhanced performance is achieved by fine-tuning a mere 0.77% of the model’s parameters.</abstract>
      <url hash="f0fbdb2c">2023.wmt-1.43</url>
      <bibkey>zhang-etal-2023-machine</bibkey>
      <doi>10.18653/v1/2023.wmt-1.43</doi>
    </paper>
    <paper id="44">
      <title>Towards Effective Disambiguation for Machine Translation with Large Language Models</title>
      <author><first>Vivek</first><last>Iyer</last></author>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>482–495</pages>
      <abstract>Resolving semantic ambiguity has long been recognised as a central challenge in the field of Machine Translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to handle many such cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate “ambiguous sentences” - i.e. those containing highly polysemous words and/or rare word senses. We also propose two ways to improve their disambiguation capabilities, through a) in-context learning and b) fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effectively adapting LLMs to become better disambiguators during Machine Translation. We release our curated disambiguation corpora and resources at https://data.statmt.org/ambiguous-europarl.</abstract>
      <url hash="776083fc">2023.wmt-1.44</url>
      <bibkey>iyer-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.wmt-1.44</doi>
    </paper>
    <paper id="45">
      <title>A Closer Look at Transformer Attention for Multilingual Translation</title>
      <author><first>Jingyi</first><last>Zhang</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <pages>496–506</pages>
      <abstract>Transformers are the predominant model for machine translation. Recent works also showed that a single Transformer model can be trained to learn translation for multiple different language pairs, achieving promising results. In this work, we investigate how the multilingual Transformer model pays attention for translating different language pairs. We first performed automatic pruning to eliminate a large number of noisy heads and then analyzed the functions and behaviors of the remaining heads in both self-attention and cross-attention. We find that different language pairs, in spite of having different syntax and word orders, tended to share the same heads for the same functions, such as syntax heads and reordering heads. However, the different characteristics of different language pairs clearly caused interference in function heads and affected head accuracies. Additionally, we reveal an interesting behavior of the Transformer cross-attention: the deep-layer cross-attention heads work in a clear cooperative way to learn different options for word reordering, which can be caused by the nature of translation tasks having multiple different gold translations in the target language for the same source sentence.</abstract>
      <url hash="1a46153e">2023.wmt-1.45</url>
      <bibkey>zhang-etal-2023-closer</bibkey>
      <doi>10.18653/v1/2023.wmt-1.45</doi>
    </paper>
    <paper id="46">
      <title>Bridging the Gap between Position-Based and Content-Based Self-Attention for Neural Machine Translation</title>
      <author><first>Felix</first><last>Schmidt</last></author>
      <author><first>Mattia</first><last>Di Gangi</last></author>
      <pages>507–521</pages>
      <abstract>Position-based token-mixing approaches, such as FNet and MLPMixer, have shown to be exciting attention alternatives for computer vision and natural language understanding. The motivation is usually to remove redundant operations for higher efficiency on consumer GPUs while maintaining Transformer quality. On the hardware side, research on memristive crossbar arrays shows the possibility of efficiency gains up to two orders of magnitude by performing in-memory computation with weights stored on device. While it is impossible to store dynamic attention weights based on token-token interactions on device, position-based weights represent a concrete alternative if they only lead to minimal degradation. In this paper, we propose position-based attention as a variant of multi-head attention where the attention weights are computed from position representations. A naive replacement of token vectors with position vectors in self-attention results in a significant loss in translation quality, which can be recovered by using relative position representations and a gating mechanism. We show analytically that this gating mechanism introduces some form of word dependency and validate its effectiveness experimentally under various conditions. The resulting network, rPosNet, outperforms previous position-based approaches and matches the quality of the Transformer with relative position embedding while requiring 20% less attention parameters after training.</abstract>
      <url hash="fefc1163">2023.wmt-1.46</url>
      <bibkey>schmidt-di-gangi-2023-bridging</bibkey>
      <doi>10.18653/v1/2023.wmt-1.46</doi>
    </paper>
    <paper id="47">
      <title>Visual Prediction Improves Zero-Shot Cross-Modal Machine Translation</title>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>522–535</pages>
      <abstract>Multimodal machine translation (MMT) systems have been successfully developed in recent years for a few language pairs. However, training such models usually requires tuples of a source language text, target language text, and images. Obtaining these data involves expensive human annotations, making it difficult to develop models for unseen text-only language pairs. In this work, we propose the task of zero-shot cross-modal machine translation aiming to transfer multimodal knowledge from an existing multimodal parallel corpus into a new translation direction. We also introduce a novel MMT model with a visual prediction network to learn visual features grounded on multimodal parallel data and provide pseudo-features for text-only language pairs. With this training paradigm, our MMT model outperforms its text-only counterpart. In our extensive analyses, we show that (i) the selection of visual features is important, and (ii) training on image-aware translations and being grounded on a similar language pair are mandatory.</abstract>
      <url hash="0a4af89b">2023.wmt-1.47</url>
      <bibkey>hirasawa-etal-2023-visual</bibkey>
      <doi>10.18653/v1/2023.wmt-1.47</doi>
    </paper>
    <paper id="48">
      <title>The Gender-<fixed-case>GAP</fixed-case> Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages</title>
      <author><first>Benjamin</first><last>Muller</last></author>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Prangthip</first><last>Hansanti</last></author>
      <author><first>Elahe</first><last>Kalbassi</last></author>
      <author><first>Christophe</first><last>Ropers</last></author>
      <author><first>Eric</first><last>Smith</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Pierre</first><last>Andrews</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>536–550</pages>
      <abstract>Gender biases in language generation systems are challenging to mitigate. One possible source for these biases is gender representation disparities in the training and evaluation data. Despite recent progress in documenting this problem and many attempts at mitigating it, we still lack shared methodology and tooling to report gender representation in large datasets. Such quantitative reporting will enable further mitigation, e.g., via data augmentation. This paper describes the Gender-Gap Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. We showcase it to report gender representation in WMT training data and development data for the News task, confirming that current data is skewed towards masculine representation. Having unbalanced datasets may indirectly optimize our systems towards outperforming one gender over the others. We suggest introducing our gender quantification pipeline in current datasets and, ideally, modifying them toward a balanced representation.</abstract>
      <url hash="1b0f1b00">2023.wmt-1.48</url>
      <bibkey>muller-etal-2023-gender</bibkey>
      <doi>10.18653/v1/2023.wmt-1.48</doi>
    </paper>
    <paper id="49">
      <title>Towards Better Evaluation for Formality-Controlled <fixed-case>E</fixed-case>nglish-<fixed-case>J</fixed-case>apanese Machine Translation</title>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Pin Chen</first><last>Wang</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>551–560</pages>
      <abstract>In this paper we propose a novel approach to automatically classify the level of formality in Japanese text, using three categories (formal, polite, and informal). We introduce a new dataset that combine manually-annotated sentences from existing resources, and formal sentences scrapped from the website of the House of Representatives and the House of Councilors of Japan. Based on our data, we propose a Transformer-based classification model for Japanese, which obtains state-of-the-art results in benchmark datasets. We further propose to utilize our classifier to study the effectiveness of prompting techniques for controlling the formality level of machine translation (MT) using Large Language Models (LLM). Our experimental setting includes a large selection of such models and is based on an En-&gt;Ja parallel corpus specifically designed to test formality control in MT. Our results validate the robustness and effectiveness of our proposed approach and while also providing empirical evidence suggesting that prompting LLMs is a viable approach to control the formality level of En-&gt;Ja MT using LLMs.</abstract>
      <url hash="c417812f">2023.wmt-1.49</url>
      <bibkey>marrese-taylor-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.wmt-1.49</doi>
    </paper>
    <paper id="50">
      <title>There’s No Data like Better Data: Using <fixed-case>QE</fixed-case> Metrics for <fixed-case>MT</fixed-case> Data Filtering</title>
      <author><first>Jan-Thorsten</first><last>Peter</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Mara</first><last>Finkelstein</last></author>
      <author><first>Juraj</first><last>Juraska</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <pages>561–577</pages>
      <abstract>Quality Estimation (QE), the evaluation of machine translation output without the need of explicit references, has seen big improvements in the last years with the use of neural metrics. In this paper we analyze the viability of using QE metrics for filtering out bad quality sentence pairs in the training data of neural machine translation systems (NMT). While most corpus filtering methods are focused on detecting noisy examples in collections of texts, usually huge amounts of web crawled data, QE models are trained to discriminate more fine-grained quality differences. We show that by selecting the highest quality sentence pairs in the training data, we can improve translation quality while reducing the training size by half. We also provide a detailed analysis of the filtering results, which highlights the differences between both approaches.</abstract>
      <url hash="93dadb99">2023.wmt-1.50</url>
      <bibkey>peter-etal-2023-theres</bibkey>
      <doi>10.18653/v1/2023.wmt-1.50</doi>
    </paper>
    <paper id="51">
      <title>Results of <fixed-case>WMT</fixed-case>23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Nitika</first><last>Mathur</last></author>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Brian</first><last>Thompson</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Frederic</first><last>Blain</last></author>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Craig</first><last>Stewart</last></author>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Sheila</first><last>Castilho</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <author><first>George</first><last>Foster</last></author>
      <pages>578–628</pages>
      <abstract>This paper presents the results of the WMT23 Metrics Shared Task. Participants submitting automatic MT evaluation metrics were asked to score the outputs of the translation systems competing in the WMT23 News Translation Task. All metrics were evaluated on how well they correlate with human ratings at the system and segment level. Similar to last year, we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics (MQM). Following last year’s success, we also included a challenge set subtask, where participants had to create contrastive test suites for evaluating metrics’ ability to capture and penalise specific types of translation errors. Furthermore, we improved our meta-evaluation procedure by considering fewer tasks and calculating a global score by weighted averaging across the various tasks. We present an extensive analysis on how well metrics perform on three language pairs: Chinese-English, Hebrew-English on the sentence-level and English-German on the paragraph-level. The results strongly confirm the results reported last year, that neural-based metrics are significantly better than non-neural metrics in their levels of correlation with human judgments. Further, we investigate the impact of bad reference translations on the correlations of metrics with human judgment. We present a novel approach for generating synthetic reference translations based on the collection of MT system outputs and their corresponding MQM ratings, which has the potential to mitigate bad reference issues we observed this year for some language pairs. Finally, we also study the connections between the magnitude of metric differences and their expected significance in human evaluation, which should help the community to better understand and adopt new metrics.</abstract>
      <url hash="e4cb798a">2023.wmt-1.51</url>
      <bibkey>freitag-etal-2023-results</bibkey>
      <doi>10.18653/v1/2023.wmt-1.51</doi>
    </paper>
    <paper id="52">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Shared Task on Quality Estimation</title>
      <author><first>Frederic</first><last>Blain</last></author>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Nuno M.</first><last>Guerreiro</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>José G.</first><last>C. de Souza</last></author>
      <author><first>Beatriz</first><last>Silva</last></author>
      <author><first>Tânia</first><last>Vaz</last></author>
      <author><first>Yan</first><last>Jingxuan</last></author>
      <author><first>Fatemeh</first><last>Azadi</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>André</first><last>Martins</last></author>
      <pages>629–653</pages>
      <abstract>We report the results of the WMT 2023 shared task on Quality Estimation, in which the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels, without access to reference translations. This edition introduces a few novel aspects and extensions that aim to enable more fine-grained, and explainable quality estimation approaches. We introduce an updated quality annotation scheme using Multidimensional Quality Metrics to obtain sentence- and word-level quality scores for three language pairs. We also extend the provided data to new language pairs: we specifically target low-resource languages and provide training, development and test data for English-Hindi, English-Tamil, English-Telegu and English-Gujarati as well as a zero-shot test-set for English-Farsi. Further, we introduce a novel fine-grained error prediction task aspiring to motivate research towards more detailed quality predictions.</abstract>
      <url hash="c668a6f5">2023.wmt-1.52</url>
      <bibkey>blain-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.52</doi>
    </paper>
    <paper id="53">
      <title>Findings of the Word-Level <fixed-case>A</fixed-case>uto<fixed-case>C</fixed-case>ompletion Shared Task in <fixed-case>WMT</fixed-case> 2023</title>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Geza</first><last>Kovacs</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>654–662</pages>
      <abstract>This paper presents the overview of the second Word-Level autocompletion (WLAC) shared task for computer-aided translation, which aims to automatically complete a target word given a translation context including a human typed character sequence. We largely adhere to the settings of the previous round of the shared task, but with two main differences: 1) The typed character sequence is obtained from the typing process of human translators to demonstrate system performance under real-world scenarios when preparing some type of testing examples; 2) We conduct a thorough analysis on the results of the submitted systems from three perspectives. From the experimental results, we observe that translation tasks are helpful to improve the performance of WLAC models. Additionally, our further analysis shows that the semantic error accounts for a significant portion of all errors, and thus it would be promising to take this type of errors into account in future.</abstract>
      <url hash="34bb19c4">2023.wmt-1.53</url>
      <bibkey>liu-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.53</doi>
    </paper>
    <paper id="54">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Shared Task on Machine Translation with Terminologies</title>
      <author><first>Kirill</first><last>Semenov</last></author>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Yuchen Eleanor</first><last>Jiang</last></author>
      <pages>663–671</pages>
      <abstract>The WMT 2023 Terminology Shared Task investigates progress in machine translation of texts with specialized vocabulary. The participants were given the source text and segment-level terminology dictionaries for three language pairs: Chinese→English, English→Czech, and German→English. We evaluate 21 submissions from 7 teams on two main criteria: general translation quality and the effectiveness of translating specialized terminology. Systems took varied approaches — incorporating terminology at inference time or weakly supervised training that uses terminology access. While incorporating terminology dictionaries leads to improvement in the translation quality, incorporating an equal amount of information from the reference leads to similar results. This challenges the position of terminologies being the crux of meaning in translation, it can also be explained by inadequate metrics which are not terminology-centric.</abstract>
      <url hash="25819bd4">2023.wmt-1.54</url>
      <bibkey>semenov-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.54</doi>
    </paper>
    <paper id="55">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Shared Task on Automatic Post-Editing</title>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>672–681</pages>
      <abstract>We present the results from the 9th round of the WMT shared task on MT Automatic Post-Editing, which consists of automatically correcting the output of a “black-box” machine translation system by learning from human corrections. Like last year, the task focused on English→Marathi, with data coming from multiple domains (healthcare, tourism, and general/news). Despite the consistent task framework, this year’s data proved to be extremely challenging. As a matter of fact, none of the official submissions from the participating teams succeeded in improving the quality of the already high-level initial translations (with baseline TER and BLEU scores of 26.6 and 70.66, respectively). Only one run, accepted as a “late” submission, achieved automatic evaluation scores that exceeded the baseline.</abstract>
      <url hash="0e6c5d8d">2023.wmt-1.55</url>
      <bibkey>bhattacharyya-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.55</doi>
    </paper>
    <paper id="56">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2023 Shared Task on Low-Resource <fixed-case>I</fixed-case>ndic Language Translation</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Lenin</first><last>Laitonjam</last></author>
      <author><first>Vanlalmuansangi</first><last>Khenglawt</last></author>
      <author><first>Sunita</first><last>Warjri</last></author>
      <author><first>Pankaj Kundan</first><last>Dadure</last></author>
      <author><first>Sandeep Kumar</first><last>Dash</last></author>
      <pages>682–694</pages>
      <abstract>This paper presents the results of the low-resource Indic language translation task organized alongside the Eighth Conference on Machine Translation (WMT) 2023. In this task, participants were asked to build machine translation systems for any of four language pairs, namely, English-Assamese, English-Mizo, English-Khasi, and English-Manipuri. For this task, the IndicNE-Corp1.0 dataset is released, which consists of parallel and monolingual corpora for northeastern Indic languages such as Assamese, Mizo, Khasi, and Manipuri. The evaluation will be carried out using automatic evaluation metrics (BLEU, TER, RIBES, COMET, ChrF) and human evaluation.</abstract>
      <url hash="05767943">2023.wmt-1.56</url>
      <bibkey>pal-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.wmt-1.56</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>ACES</fixed-case>: Translation Accuracy Challenge Sets at <fixed-case>WMT</fixed-case> 2023</title>
      <author><first>Chantal</first><last>Amrhein</last></author>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Liane</first><last>Guillou</last></author>
      <pages>695–712</pages>
      <abstract>We benchmark the performance of segment-level metrics submitted to WMT 2023 using the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists of 36K examples representing challenges from 68 phenomena and covering 146 language pairs. The phenomena range from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. For each metric, we provide a detailed profile of performance over a range of error categories as well as an overall ACES-Score for quick comparison. We also measure the incremental performance of the metrics submitted to both WMT 2023 and 2022. We find that 1) there is no clear winner among the metrics submitted to WMT 2023, and 2) performance change between the 2023 and 2022 versions of the metrics is highly variable. Our recommendations are similar to those from WMT 2022. Metric developers should focus on: building ensembles of metrics from different design families, developing metrics that pay more attention to the source and rely less on surface-level overlap, and carefully determining the influence of multilingual embeddings on MT evaluation.</abstract>
      <url hash="2359c8ba">2023.wmt-1.57</url>
      <bibkey>amrhein-etal-2023-aces</bibkey>
      <doi>10.18653/v1/2023.wmt-1.57</doi>
    </paper>
    <paper id="58">
      <title>Challenging the State-of-the-art Machine Translation Metrics from a Linguistic Perspective</title>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Shushen</first><last>Manakhimova</last></author>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>713–729</pages>
      <abstract>We employ a linguistically motivated challenge set in order to evaluate the state-of-the-art machine translation metrics submitted to the Metrics Shared Task of the 8th Conference for Machine Translation. The challenge set includes about 21,000 items extracted from 155 machine translation systems for three language directions, covering more than 100 linguistically-motivated phenomena organized in 14 categories. The metrics that have the best performance with regard to our linguistically motivated analysis are the Cometoid22-wmt23 (a trained metric based on distillation) for German-English and MetricX-23-c (based on a fine-tuned mT5 encoder-decoder language model) for English-German and English-Russian. Some of the most difficult phenomena are passive voice for German-English, named entities, terminology and measurement units for English-German, and focus particles, adverbial clause and stripping for English-Russian.</abstract>
      <url hash="1ce9ba9b">2023.wmt-1.58</url>
      <bibkey>avramidis-etal-2023-challenging</bibkey>
      <doi>10.18653/v1/2023.wmt-1.58</doi>
    </paper>
    <paper id="59">
      <title><fixed-case>T</fixed-case>okengram_<fixed-case>F</fixed-case>, a Fast and Accurate Token-based chr<fixed-case>F</fixed-case>++ Derivative</title>
      <author><first>Sören</first><last>Dreano</last></author>
      <author><first>Derek</first><last>Molloy</last></author>
      <author><first>Noel</first><last>Murphy</last></author>
      <pages>730–737</pages>
      <abstract>Tokengram_F is an F-score-based evaluation metric for Machine Translation that is heavily in- spired by chrF++ and can act as a more accurate replacement. By replacing word n-grams with n-grams obtained from tokenization algorithms, tokengram_F better captures similarities between words.</abstract>
      <url hash="d5207cec">2023.wmt-1.59</url>
      <bibkey>dreano-etal-2023-tokengram</bibkey>
      <doi>10.18653/v1/2023.wmt-1.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>E</fixed-case>mbed_<fixed-case>L</fixed-case>lama: Using <fixed-case>LLM</fixed-case> Embeddings for the Metrics Shared Task</title>
      <author><first>Sören</first><last>Dreano</last></author>
      <author><first>Derek</first><last>Molloy</last></author>
      <author><first>Noel</first><last>Murphy</last></author>
      <pages>738–745</pages>
      <abstract>Embed_llama is an assessment metric for language translation that hinges upon the utilization of the recently introduced Llama 2 Large Language Model (LLM), specifically, focusing on its embedding layer, with the aim of transforming sentences into a vector space that establishes connections between geometric and semantic proximities</abstract>
      <url hash="10141f60">2023.wmt-1.60</url>
      <bibkey>dreano-etal-2023-embed</bibkey>
      <doi>10.18653/v1/2023.wmt-1.60</doi>
    </paper>
    <paper id="61">
      <title>e<fixed-case>BLEU</fixed-case>: Unexpectedly Good Machine Translation Evaluation Using Simple Word Embeddings</title>
      <author><first>Muhammad</first><last>ElNokrashy</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <pages>746–750</pages>
      <abstract>We propose eBLEU, a metric inspired by BLEU metric that uses embedding similarities instead of string matches. We introduce meaning diffusion vectors to enable matching n-grams of semantically similar words in a BLEU-like algorithm, using efficient, non-contextual word embeddings like fastText. On WMT23 data, eBLEU beats BLEU and ChrF by around 3.8% system-level score, approaching BERTScore at −0.9% absolute difference. In WMT22 scenarios, eBLEU outperforms f101spBLEU and ChrF in MQM by 2.2%−3.6%. Curiously, on MTurk evaluations, eBLEU surpasses past methods by 3.9%−8.2% (f200spBLEU, COMET-22). eBLEU presents an interesting middle-ground between traditional metrics and pretrained metrics.</abstract>
      <url hash="f7d7f513">2023.wmt-1.61</url>
      <bibkey>elnokrashy-kocmi-2023-ebleu</bibkey>
      <doi>10.18653/v1/2023.wmt-1.61</doi>
    </paper>
    <paper id="62">
      <title>Cometoid: Distilling Strong Reference-based Machine Translation Metrics into <fixed-case>E</fixed-case>ven Stronger Quality Estimation Metrics</title>
      <author><first>Thamme</first><last>Gowda</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <pages>751–755</pages>
      <abstract>This paper describes our submissions to the 2023 Conference on Machine Translation (WMT-23) Metrics shared task. Knowledge distillation is commonly used to create smaller student models that mimic larger teacher model while reducing the model size and hence inference cost in production. In this work, we apply knowledge distillation to machine translation evaluation metrics and distill existing reference-based teacher metrics into reference-free (quality estimation; QE) student metrics. We mainly focus on students of Unbabel’s COMET22 reference-based metric. When evaluating on the official WMT-22 Metrics evaluation task, our distilled Cometoid QE metrics outperform all other QE metrics on that set while matching or out-performing the reference-based teacher metric. Our metrics never see the human ground-truth scores directly – only the teacher metric was trained on human scores by its original creators. We also distill ChrF sentence-level scores into a neural QE metric and find that our reference-free (and fully human-score-free) student metric ChrFoid outperforms its teacher metric by over 7% pairwise accuracy on the same WMT-22 task, rivaling other existing QE metrics.</abstract>
      <url hash="ae8407d2">2023.wmt-1.62</url>
      <bibkey>gowda-etal-2023-cometoid</bibkey>
      <doi>10.18653/v1/2023.wmt-1.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>M</fixed-case>etric<fixed-case>X</fixed-case>-23: The <fixed-case>G</fixed-case>oogle Submission to the <fixed-case>WMT</fixed-case> 2023 Metrics Shared Task</title>
      <author><first>Juraj</first><last>Juraska</last></author>
      <author><first>Mara</first><last>Finkelstein</last></author>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Mehdi</first><last>Mirzazadeh</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <pages>756–767</pages>
      <abstract>This report details the MetricX-23 submission to the WMT23 Metrics Shared Task and provides an overview of the experiments that informed which metrics were submitted. Our 3 submissions—each with a quality estimation (or reference-free) version—are all learned regression-based metrics that vary in the data used for training and which pretrained language model was used for initialization. We report results related to understanding (1) which supervised training data to use, (2) the impact of how the training labels are normalized, (3) the amount of synthetic training data to use, (4) how metric performance is related to model size, and (5) the effect of initializing the metrics with different pretrained language models. The most successful training recipe for MetricX employs two-stage fine-tuning on DA and MQM ratings, and includes synthetic training data. Finally, one important takeaway from our extensive experiments is that optimizing for both segment- and system-level performance at the same time is a challenging task.</abstract>
      <url hash="ac55e3cf">2023.wmt-1.63</url>
      <bibkey>juraska-etal-2023-metricx</bibkey>
      <doi>10.18653/v1/2023.wmt-1.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>GEMBA</fixed-case>-<fixed-case>MQM</fixed-case>: Detecting Translation Quality Error Spans with <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <pages>768–775</pages>
      <abstract>This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to detect translation quality errors, specifically for the quality estimation setting without the need for human reference translations. Based on the power of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting technique, querying the GPT-4 model to mark error quality spans. Compared to previous works, our method has language-agnostic prompts, thus avoiding the need for manual prompt preparation for new languages. While preliminary results indicate that GEMBA-MQM achieves state-of-the-art accuracy for system ranking, we advise caution when using it in academic works to demonstrate improvements over other methods due to its dependence on the proprietary, black-box GPT model.</abstract>
      <url hash="ab36198b">2023.wmt-1.64</url>
      <bibkey>kocmi-federmann-2023-gemba</bibkey>
      <doi>10.18653/v1/2023.wmt-1.64</doi>
    </paper>
    <paper id="65">
      <title>Metric Score Landscape Challenge (<fixed-case>MSLC</fixed-case>23): Understanding Metrics’ Performance on a Wider Landscape of Translation Quality</title>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <pages>776–799</pages>
      <abstract>The Metric Score Landscape Challenge (MSLC23) dataset aims to gain insight into metric scores on a broader/wider landscape of machine translation (MT) quality. It provides a collection of low- to medium-quality MT output on the WMT23 general task test set. Together with the high quality systems submitted to the general task, this will enable better interpretation of metric scores across a range of different levels of translation quality. With this wider range of MT quality, we also visualize and analyze metric characteristics beyond just correlation.</abstract>
      <url hash="616492db">2023.wmt-1.65</url>
      <bibkey>lo-etal-2023-metric</bibkey>
      <doi>10.18653/v1/2023.wmt-1.65</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>MEE</fixed-case>4 and <fixed-case>XL</fixed-case>sim : <fixed-case>IIIT</fixed-case> <fixed-case>HYD</fixed-case>’s Submissions’ for <fixed-case>WMT</fixed-case>23 Metrics Shared Task</title>
      <author><first>Ananya</first><last>Mukherjee</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>800–805</pages>
      <abstract>This paper presents our contributions to the WMT2023 shared metrics task, consisting of two distinct evaluation approaches: a) Unsupervised Metric (MEE4) and b) Supervised Metric (XLSim). MEE4 represents an unsupervised, reference-based assessment metric that quantifies linguistic features, encompassing lexical, syntactic, semantic, morphological, and contextual similarities, leveraging embeddings. In contrast, XLsim is a supervised reference-based evaluation metric, employing a Siamese Architecture, which regresses on Direct Assessments (DA) from previous WMT News Translation shared tasks from 2017-2022. XLsim is trained using XLM-RoBERTa (base) on English-German reference and mt pairs with human scores.</abstract>
      <url hash="985414ea">2023.wmt-1.66</url>
      <bibkey>mukherjee-shrivastava-2023-mee4</bibkey>
      <doi>10.18653/v1/2023.wmt-1.66</doi>
    </paper>
    <paper id="67">
      <title>Quality Estimation Using Minimum <fixed-case>B</fixed-case>ayes Risk</title>
      <author><first>Subhajit</first><last>Naskar</last></author>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <pages>806–811</pages>
      <abstract>This report describes the Minimum Bayes Risk Quality Estimation (MBR-QE) submission to the Workshop on Machine Translation’s 2023 Metrics Shared Task. MBR decoding with neural utility metrics like BLEURT is known to be effective in generating high quality machine translations. We use the underlying technique of MBR decoding and develop an MBR based reference-free quality estimation metric. Our method uses an evaluator machine translation system and a reference-based utility metric (specifically BLEURT and MetricX) to calculate a quality estimation score of a model. We report results related to comparing different MBR configurations and utility metrics.</abstract>
      <url hash="202f9589">2023.wmt-1.67</url>
      <bibkey>naskar-etal-2023-quality</bibkey>
      <doi>10.18653/v1/2023.wmt-1.67</doi>
    </paper>
    <paper id="68">
      <title>Evaluating Metrics for Document-context Evaluation in Machine Translation</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>812–814</pages>
      <abstract>We describe our submission of a new metric, SLIDE (Raunak et al., 2023), to the WMT 2023 metrics task. SLIDE is a reference-free quality-estimation metric that works by constructing a fixed sentence-length window over the documents in a test set, concatenating chunks and then sending them for scoring as a single unit by COMET (Rei et al, 2022). We find that SLIDE improves dramatically over its context-less counterpart on the two WMT22 evaluation campaigns (MQM and DA+SQM).</abstract>
      <url hash="9a226523">2023.wmt-1.68</url>
      <bibkey>raunak-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.wmt-1.68</doi>
    </paper>
    <paper id="69">
      <title>Semantically-Informed Regressive Encoder Score</title>
      <author><first>Vasiliy</first><last>Viskov</last></author>
      <author><first>George</first><last>Kokush</last></author>
      <author><first>Daniil</first><last>Larionov</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>815–821</pages>
      <abstract>Machine translation is natural language generation (NLG) problem of translating source text from one language to another. As every task in machine learning domain it requires to have evaluation metric. The most obvious one is human evaluation but it is expensive in case of money and time consumption. In last years with appearing of pretrained transformer architectures and large language models (LLMs) state-of-the-art results in automatic machine translation evaluation got a huge quality step in terms of correlation with expert assessment. We introduce MRE-Score, seMantically-informed Regression Encoder Score, the approach with constructing automatic machine translation evaluation system based on regression encoder and contrastive pretraining for downstream problem.</abstract>
      <url hash="9b1ab18d">2023.wmt-1.69</url>
      <bibkey>viskov-etal-2023-semantically</bibkey>
      <doi>10.18653/v1/2023.wmt-1.69</doi>
    </paper>
    <paper id="70">
      <title>Empowering a Metric with <fixed-case>LLM</fixed-case>-assisted Named Entity Annotation: <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case>23 Metrics Shared Task</title>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Yilun</first><last>Liu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Xiaofeng</first><last>Zhao</last></author>
      <author><first>Junhao</first><last>Zhu</last></author>
      <author><first>Ming</first><last>Zhu</last></author>
      <author><first>Xiaosong</first><last>Qiao</last></author>
      <author><first>Jingfei</first><last>Zhang</last></author>
      <author><first>Ma</first><last>Miaomiao</last></author>
      <author><first>Zhao</first><last>Yanqing</last></author>
      <author><first>Song</first><last>Peng</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yanfei</first><last>Jiang</last></author>
      <pages>822–828</pages>
      <abstract>This paper presents the submission of Huawei Translation Service Center (HW-TSC) to the WMT23 metrics shared task, in which we submit two metrics: KG-BERTScore and HWTSC-EE-Metric. Among them, KG-BERTScore is our primary submission for the reference-free metric, which can provide both segment-level and system-level scoring. While HWTSC-EE-Metric is our primary submission for the reference-based metric, which can only provide system-level scoring. Overall, our metrics show relatively high correlations with MQM scores on the metrics tasks of previous years. Especially on system-level scoring tasks, our metrics achieve new state-of-the-art in many language pairs.</abstract>
      <url hash="4f511db0">2023.wmt-1.70</url>
      <bibkey>wu-etal-2023-empowering</bibkey>
      <doi>10.18653/v1/2023.wmt-1.70</doi>
    </paper>
    <paper id="71">
      <title>Unify Word-level and Span-level Tasks: <fixed-case>NJUNLP</fixed-case>’s Participation for the <fixed-case>WMT</fixed-case>2023 Quality Estimation Shared Task</title>
      <author><first>Xiang</first><last>Geng</last></author>
      <author><first>Zhejian</first><last>Lai</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <pages>829–834</pages>
      <abstract>We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considerable margin.</abstract>
      <url hash="ff94f392">2023.wmt-1.71</url>
      <bibkey>geng-etal-2023-unify</bibkey>
      <doi>10.18653/v1/2023.wmt-1.71</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> 2023 Submission for the Quality Estimation Shared Task</title>
      <author><first>Yuang</first><last>Li</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Ming</first><last>Zhu</last></author>
      <author><first>Mengyao</first><last>Piao</last></author>
      <author><first>Xinglin</first><last>Lyu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>835–840</pages>
      <abstract>Quality estimation (QE) is an essential technique to assess machine translation quality without reference translations. In this paper, we focus on Huawei Translation Services Center’s (HW-TSC’s) submission to the sentence-level QE shared task, named Ensemble-CrossQE. Our system uses CrossQE, the same model architecture as our last year’s submission, which consists of a multilingual base model and a task-specific downstream layer. The input is the concatenation of the source and the translated sentences. To enhance the performance, we finetuned and ensembled multiple base models such as XLM-R, InfoXLM, RemBERT and CometKiwi. Moreover, we introduce a new corruption-based data augmentation method, which generates deletion, substitution and insertion errors in the original translation and uses a reference-based QE model to obtain pseudo scores. Results show that our system achieves impressive performance on sentence-level QE test sets and ranked the first place for three language pairs: English-Hindi, English-Tamil and English-Telegu. In addition, we participated in the error span detection task. The submitted model outperforms the baseline on Chinese-English and Hebrew-English language pairs.</abstract>
      <url hash="ee5029ba">2023.wmt-1.72</url>
      <bibkey>li-etal-2023-hw-tsc</bibkey>
      <doi>10.18653/v1/2023.wmt-1.72</doi>
    </paper>
    <paper id="73">
      <title>Scaling up <fixed-case>C</fixed-case>omet<fixed-case>K</fixed-case>iwi: Unbabel-<fixed-case>IST</fixed-case> 2023 Submission for the Quality Estimation Shared Task</title>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Nuno M.</first><last>Guerreiro</last></author>
      <author><first>JosÃ©</first><last>Pombal</last></author>
      <author><first>Daan</first><last>van Stigt</last></author>
      <author><first>Marcos</first><last>Treviso</last></author>
      <author><first>Luisa</first><last>Coheur</last></author>
      <author><first>José G.</first><last>C. de Souza</last></author>
      <author><first>André</first><last>Martins</last></author>
      <pages>841–848</pages>
      <abstract>We present the joint contribution of Unbabel and Instituto Superior Técnico to the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated on all tasks: Sentence- and Word-level Quality Prediction and Fine-grained error span detection. For all tasks we build on the CometKiwi model (rei et al. 2022). Our multilingual approaches are ranked first for all tasks, reaching state-of-the-art performance for quality estimation at word-, span- and sentence-level granularity. Compared to the previous state-of-the-art, CometKiwi, we show large improvements in correlation with human judgements (up to 10 Spearman points) and surpassing the second-best multilingual submission with up to 3.8 absolute points.</abstract>
      <url hash="c84fe212">2023.wmt-1.73</url>
      <bibkey>rei-etal-2023-scaling</bibkey>
      <doi>10.18653/v1/2023.wmt-1.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>S</fixed-case>urrey<fixed-case>AI</fixed-case> 2023 Submission for the Quality Estimation Shared Task</title>
      <author><first>Archchana</first><last>Sindhujan</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <pages>849–855</pages>
      <abstract>Quality Estimation (QE) systems are important in situations where it is necessary to assess the quality of translations, but there is no reference available. This paper describes the approach adopted by the SurreyAI team for addressing the Sentence-Level Direct Assessment shared task in WMT23. The proposed approach builds upon the TransQuest framework, exploring various autoencoder pre-trained language models within the MonoTransQuest architecture using single and ensemble settings. The autoencoder pre-trained language models employed in the proposed systems are XLMV, InfoXLM-large, and XLMR-large. The evaluation utilizes Spearman and Pearson correlation coefficients, assessing the relationship between machine-predicted quality scores and human judgments for 5 language pairs (English-Gujarati, English-Hindi, English-Marathi, English-Tamil and English-Telugu). The MonoTQ-InfoXLM-large approach emerges as a robust strategy, surpassing all other individual models proposed in this study by significantly improving over the baseline for the majority of the language pairs.</abstract>
      <url hash="e124d2c8">2023.wmt-1.74</url>
      <bibkey>sindhujan-etal-2023-surreyai</bibkey>
      <doi>10.18653/v1/2023.wmt-1.74</doi>
    </paper>
    <paper id="75">
      <title><fixed-case>MMT</fixed-case>’s Submission for the <fixed-case>WMT</fixed-case> 2023 Quality Estimation Shared Task</title>
      <author><first>Yulong</first><last>Wu</last></author>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>856–862</pages>
      <abstract>This paper presents our submission to the WMT 2023 Quality Estimation (QE) shared task 1 (sentence-level subtask). We propose a straightforward training data augmentation approach aimed at improving the correlation between QE model predictions and human quality assessments. Utilising eleven data augmentation approaches and six distinct language pairs, we systematically create augmented training sets by individually applying each method to the original training set of each respective language pair. By evaluating the performance gap between the model before and after training on the augmented dataset, as measured on the development set, we assess the effectiveness of each augmentation method. Experimental results reveal that synonym replacement via the Paraphrase Database (PPDB) yields the most substantial performance boost for language pairs English-German, English-Marathi and English-Gujarati, while for the remaining language pairs, methods such as contextual word embeddings-based words insertion, back translation, and direct paraphrasing prove to be more effective. Training the model on a more diverse and larger set of samples does confer further performance improvements for certain language pairs, albeit to a marginal extent, and this phenomenon is not universally applicable. At the time of submission, we select the model trained on the augmented dataset constructed using the respective most effective method to generate predictions for the test set in each language pair, except for the English-German. Despite not being highly competitive, our system consistently surpasses the baseline performance on most language pairs and secures a third-place ranking in the English-Marathi.</abstract>
      <url hash="3b5ebd0e">2023.wmt-1.75</url>
      <bibkey>wu-etal-2023-mmts</bibkey>
      <doi>10.18653/v1/2023.wmt-1.75</doi>
    </paper>
    <paper id="76">
      <title><fixed-case>IOL</fixed-case> Research’s Submission for <fixed-case>WMT</fixed-case> 2023 Quality Estimation Shared Task</title>
      <author><first>Zeyu</first><last>Yan</last></author>
      <pages>863–871</pages>
      <abstract>This paper presents the submissions of IOL Research in WMT 2023 quality estimation shared task. We participate in task 1 Quality Estimation on both sentence and word levels, which predicts sentence quality score and word quality tags. Our system is a cross-lingual and multitask model for both sentence and word levels. We utilize several multilingual Pretrained Language Models (PLMs) as backbones and build task modules on them to achieve better predictions. A regression module on PLM is used to predict sentence level score and word tagging layer is used to classify the tag of each word in the translation based on the encoded representations from PLM. Each PLM is pretrained on quality estimation and metrics data from the previous WMT tasks before finetuning on training data this year. Furthermore, we integrate predictions from different models for better performance while the weights of each model are automatically searched and optimized by performance on Dev set. Our method achieves competitive results.</abstract>
      <url hash="b22a7a48">2023.wmt-1.76</url>
      <bibkey>yan-2023-iol</bibkey>
      <doi>10.18653/v1/2023.wmt-1.76</doi>
    </paper>
    <paper id="77">
      <title><fixed-case>SJTU</fixed-case>-<fixed-case>MTLAB</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case>23 Word-Level Auto Completion Task</title>
      <author><first>Xingyu</first><last>Chen</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <pages>872–876</pages>
      <abstract>Word-level auto-completion (WLAC) plays a crucial role in Computer-Assisted Translation. In this paper, we describe the SJTU-MTLAB’s submission to the WMT23 WLAC task. We propose a joint method to incorporate the machine translation task to the WLAC task. The proposed approach is general and can be applied to various encoder-based architectures. Through extensive experiments, we demonstrate that our approach can greatly improve performance, while maintaining significantly small model sizes.</abstract>
      <url hash="3a0825af">2023.wmt-1.77</url>
      <bibkey>chen-wang-2023-sjtu</bibkey>
      <doi>10.18653/v1/2023.wmt-1.77</doi>
    </paper>
    <paper id="78">
      <title><fixed-case>PRHLT</fixed-case>’s Submission to <fixed-case>WLAC</fixed-case> 2023</title>
      <author><first>Angel</first><last>Navarro</last></author>
      <author><first>Miguel</first><last>Domingo</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <pages>877–881</pages>
      <abstract>This paper describes our submission to the Word-Level AutoCompletion shared task of WMT23. We participated in the English–German and German–English categories. We extended our last year segment-based interactive machine translation approach to address its weakness when no context is available. Additionally, we fine-tune the pre-trained mT5 large language model to be used for autocompletion.</abstract>
      <url hash="e61bd2b5">2023.wmt-1.78</url>
      <bibkey>navarro-etal-2023-prhlts</bibkey>
      <doi>10.18653/v1/2023.wmt-1.78</doi>
    </paper>
    <paper id="79">
      <title><fixed-case>K</fixed-case>now<fixed-case>C</fixed-case>omp Submission for <fixed-case>WMT</fixed-case>23 Word-Level <fixed-case>A</fixed-case>uto<fixed-case>C</fixed-case>ompletion Task</title>
      <author><first>Yi</first><last>Wu</last></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <pages>882–889</pages>
      <abstract>The NLP community has recently witnessed the success of Large Language Models (LLMs) across various Natural Language Processing (NLP) tasks. However, the potential of LLMs for word-level auto-completion in a multilingual context has not been thoroughly explored yet. To address this gap and benchmark the performance of LLMs, we propose an LLM-based system for the WMT23 Word-Level Auto-Completion (WLAC) task. Our system utilizes ChatGPT to represent LLMs and evaluates its performance in three translation directions: Chinese-English, German-English, and English-German. We also study the task under zero-shot and few-shot settings to assess the potential benefits of incorporating exemplars from the training set in guiding the LLM to perform the task. The results of our experiments show that, on average, our system attains a 29.8% accuracy on the test set. Further analyses reveal that LLMs struggle with WLAC in the zero-shot setting, but performance significantly improves with the help of additional exemplars, though some common errors still appear frequently. These findings have important implications for incorporating LLMs into computer-aided translation systems, as they can potentially enhance the quality of translations. Our codes for evaluation are available at https://github.com/ethanyiwu/WLAC.</abstract>
      <url hash="0e881a33">2023.wmt-1.79</url>
      <bibkey>wu-etal-2023-knowcomp</bibkey>
      <doi>10.18653/v1/2023.wmt-1.79</doi>
    </paper>
    <paper id="80">
      <title>Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting</title>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <pages>890–896</pages>
      <abstract>Terminology correctness is important in the downstream application of machine translation, and a prevalent way to ensure this is to inject terminology constraints into a translation system. In our submission to the WMT 2023 terminology translation task, we adopt a translate-then-refine approach which can be domain-independent and requires minimal manual efforts. We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model. Further, we explore two post-processing methods. First, we use an alignment process to discover whether a terminology constraint has been violated, and if so, we re-decode with the violating word negatively constrained. Alternatively, we leverage a large language model to refine a hypothesis by providing it with terminology constraints. Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.</abstract>
      <url hash="515ac4c9">2023.wmt-1.80</url>
      <bibkey>bogoychev-chen-2023-terminology</bibkey>
      <doi>10.18653/v1/2023.wmt-1.80</doi>
    </paper>
    <paper id="81">
      <title>Lingua Custodia’s Participation at the <fixed-case>WMT</fixed-case> 2023 Terminology Shared Task</title>
      <author><first>Jingshu</first><last>Liu</last></author>
      <author><first>Mariam</first><last>Nakhlé</last></author>
      <author><first>Gaëtan</first><last>Caillout</last></author>
      <author><first>Raheel</first><last>Qadar</last></author>
      <pages>897–901</pages>
      <abstract>This paper presents Lingua Custodia’s submission to the WMT23 shared task on Terminology shared task. Ensuring precise translation of technical terms plays a pivotal role in gauging the final quality of machine translation results. Our goal is to follow the terminology constraint while applying the machine translation system. Inspired by the recent work of terminology control, we propose to annotate the machine learning training data by leveraging a synthetic dictionary extracted in a fully non supervised way from the give parallel corpora. The model learned with this training data can then be then used to translate text with a given terminology in a flexible manner. In addition, we introduce a careful annotated data re-sampling step in order to guide the model to see different terminology types enough times. In this task we consider all the three language directions: Chinese to English, English to Czech and German to English. Our automatic evaluation metrics with the submitted systems show the effectiveness of the proposed method.</abstract>
      <url hash="498ceece">2023.wmt-1.81</url>
      <bibkey>liu-etal-2023-lingua</bibkey>
      <doi>10.18653/v1/2023.wmt-1.81</doi>
    </paper>
    <paper id="82">
      <title>Domain Terminology Integration into Machine Translation: Leveraging Large Language Models</title>
      <author><first>Yasmin</first><last>Moslem</last></author>
      <author><first>Gianfranco</first><last>Romani</last></author>
      <author><first>Mahdi</first><last>Molaei</last></author>
      <author><first>John D.</first><last>Kelleher</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>902–911</pages>
      <abstract>This paper discusses the methods that we used for our submissions to the WMT 2023 Terminology Shared Task for German-to-English (DE-EN), English-to-Czech (EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to advance machine translation (MT) by challenging participants to develop systems that accurately translate technical terms, ultimately enhancing communication and understanding in specialised domains. To this end, we conduct experiments that utilise large language models (LLMs) for two purposes: generating synthetic bilingual terminology-based data, and post-editing translations generated by an MT model through incorporating pre-approved terms. Our system employs a four-step process: (i) using an LLM to generate bilingual synthetic data based on the provided terminology, (ii) fine-tuning a generic encoder-decoder MT model, with a mix of the terminology-based synthetic data generated in the first step and a randomly sampled portion of the original generic training data, (iii) generating translations with the fine-tuned MT model, and (iv) finally, leveraging an LLM for terminology-constrained automatic post-editing of the translations that do not include the required terms. The results demonstrate the effectiveness of our proposed approach in improving the integration of pre-approved terms into translations. The number of terms incorporated into the translations of the blind dataset increases from an average of 36.67% with the generic model to an average of 72.88% by the end of the process. In other words, successful utilisation of terms nearly doubles across the three language pairs.</abstract>
      <url hash="d0a9d960">2023.wmt-1.82</url>
      <bibkey>moslem-etal-2023-domain</bibkey>
      <doi>10.18653/v1/2023.wmt-1.82</doi>
    </paper>
    <paper id="83">
      <title><fixed-case>OPUS</fixed-case>-<fixed-case>CAT</fixed-case> Terminology Systems for the <fixed-case>WMT</fixed-case>23 Terminology Shared Task</title>
      <author><first>Tommi</first><last>Nieminen</last></author>
      <pages>912–918</pages>
      <abstract>This paper describes the submission of the OPUS-CAT project to the WMT 2023 terminology shared task. We trained systems for all three language pairs included in the task. All systems were trained using the same training pipeline with identical methods. Support for terminology was implemented by using the currently popular method of annotating source language terms in the training data with the corresponding target language terms.</abstract>
      <url hash="695bc818">2023.wmt-1.83</url>
      <bibkey>nieminen-2023-opus</bibkey>
      <doi>10.18653/v1/2023.wmt-1.83</doi>
    </paper>
    <paper id="84">
      <title><fixed-case>VARCO</fixed-case>-<fixed-case>MT</fixed-case>: <fixed-case>NCSOFT</fixed-case>’s <fixed-case>WMT</fixed-case>’23 Terminology Shared Task Submission</title>
      <author><first>Geon Woo</first><last>Park</last></author>
      <author><first>Junghwa</first><last>Lee</last></author>
      <author><first>Meiying</first><last>Ren</last></author>
      <author><first>Allison</first><last>Shindell</last></author>
      <author><first>Yeonsoo</first><last>Lee</last></author>
      <pages>919–925</pages>
      <abstract>A lack of consistency in terminology translation undermines quality of translation from even the best performing neural machine translation (NMT) models, especially in narrow domains like literature, medicine, and video game jargon. Dictionaries containing terminologies and their translations are often used to improve consistency but are difficult to construct and incorporate. We accompany our submissions to the WMT ‘23 Terminology Shared Task with a description of our experimental setup and procedure where we propose a framework of terminology-aware machine translation. Our framework comprises of an automatic terminology extraction process that constructs terminology-aware machine translation data in low-supervision settings and two model architectures with terminology constraints. Our models outperform baseline models by 21.51%p and 19.36%p in terminology recall respectively on the Chinese to English WMT’23 Terminology Shared Task test data.</abstract>
      <url hash="72c4345b">2023.wmt-1.84</url>
      <bibkey>park-etal-2023-varco</bibkey>
      <doi>10.18653/v1/2023.wmt-1.84</doi>
    </paper>
    <paper id="85">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2023 Automatic Post Editing Shared Task</title>
      <author><first>Jiawei</first><last>Yu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Zhao</first><last>Yanqing</last></author>
      <author><first>Xiaofeng</first><last>Zhao</last></author>
      <author><first>Yuang</first><last>Li</last></author>
      <author><first>Su</first><last>Chang</last></author>
      <author><first>Yinglu</first><last>Li</last></author>
      <author><first>Ma</first><last>Miaomiao</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>926–930</pages>
      <abstract>The paper presents the submission by HW-TSC in the WMT 2023 Automatic Post Editing (APE) shared task for the English-Marathi (En-Mr) language pair. Our method encompasses several key steps. First, we pre-train an APE model by utilizing synthetic APE data provided by the official task organizers. Then, we fine-tune the model by employing real APE data. For data augmentation, we incorporate candidate translations obtained from an external Machine Translation (MT) system. Furthermore, we integrate the En-Mr parallel corpus from the Flores-200 dataset into our training data. To address the overfitting issue, we employ R-Drop during the training phase. Given that APE systems tend to exhibit a tendency of ‘over-correction’, we employ a sentence-level Quality Estimation (QE) system to select the final output, deciding between the original translation and the corresponding output generated by the APE model. Our experiments demonstrate that pre-trained APE models are effective when being fine-tuned with the APE corpus of a limited size, and the performance can be further improved with external MT augmentation. Our approach improves the TER and BLEU scores on the development set by -2.42 and +3.76 points, respectively.</abstract>
      <url hash="d4aca641">2023.wmt-1.85</url>
      <bibkey>yu-etal-2023-hw</bibkey>
      <doi>10.18653/v1/2023.wmt-1.85</doi>
    </paper>
    <paper id="86">
      <title>Neural Machine Translation for <fixed-case>E</fixed-case>nglish - <fixed-case>M</fixed-case>anipuri and <fixed-case>E</fixed-case>nglish - <fixed-case>A</fixed-case>ssamese</title>
      <author><first>Goutam</first><last>Agrawal</last></author>
      <author><first>Rituraj</first><last>Das</last></author>
      <author><first>Anupam</first><last>Biswas</last></author>
      <author><first>Dalton Meitei</first><last>Thounaojam</last></author>
      <pages>931–934</pages>
      <abstract>The internet is a vast repository of valuable information available in English, but for many people who are more comfortable with their regional languages, accessing this knowledge can be a challenge. Manually translating this kind of text, is a laborious, expensive, and time-consuming operation. This makes machine translation an effective method for translating texts without the need for human intervention. One of the newest and most efficient translation methods among the current machine translation systems is neural machine translation (NMT). In this WMT23 shared task: low resource indic language translation challenge, our team named ATULYA-NITS used the NMT transformer model for the English to/from Assamese and English to/from Manipuri language translation. Our systems achieved the BLEU score of 15.02 for English to Manipuri, 18.7 for Manipuri to English, 5.47 for English to Assamese, and 8.5 for Assamese to English.</abstract>
      <url hash="d84832c1">2023.wmt-1.86</url>
      <bibkey>agrawal-etal-2023-neural</bibkey>
      <doi>10.18653/v1/2023.wmt-1.86</doi>
    </paper>
    <paper id="87">
      <title><fixed-case>GUIT</fixed-case>-<fixed-case>NLP</fixed-case>’s Submission to Shared Task: Low Resource <fixed-case>I</fixed-case>ndic Language Translation</title>
      <author><first>Mazida</first><last>Ahmed</last></author>
      <author><first>Kuwali</first><last>Talukdar</last></author>
      <author><first>Parvez</first><last>Boruah</last></author>
      <author><first>Prof. Shikhar Kumar</first><last>Sarma</last></author>
      <author><first>Kishore</first><last>Kashyap</last></author>
      <pages>935–940</pages>
      <abstract>This paper describes the submission of the GUIT-NLP team in the “Shared Task: Low Resource Indic Language Translation” focusing on three low-resource language pairs: English-Mizo, English-Khasi, and English-Assamese. The initial phase involves an in-depth exploration of Neural Machine Translation (NMT) techniques tailored to the available data. Within this investigation, various Subword Tokenization approaches, model configurations (exploring differnt hyper-parameters etc.) of the general NMT pipeline are tested to identify the most effective method. Subsequently, we address the challenge of low-resource languages by leveraging monolingual data through an innovative and systematic application of the Back Translation technique for English-Mizo. During model training, the monolingual data is progressively integrated into the original bilingual dataset, with each iteration yielding higher-quality back translations. This iterative approach significantly enhances the model’s performance, resulting in a notable increase of +3.65 in BLEU scores. Further improvements of +5.59 are achieved through fine-tuning using authentic parallel data.</abstract>
      <url hash="d90b58e3">2023.wmt-1.87</url>
      <bibkey>ahmed-etal-2023-guit</bibkey>
      <doi>10.18653/v1/2023.wmt-1.87</doi>
    </paper>
    <paper id="88">
      <title><fixed-case>NICT</fixed-case>-<fixed-case>AI</fixed-case>4<fixed-case>B</fixed-case>’s Submission to the <fixed-case>I</fixed-case>ndic <fixed-case>MT</fixed-case> Shared Task in <fixed-case>WMT</fixed-case> 2023</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Jay</first><last>Gala</last></author>
      <author><first>Pranjal</first><last>Chitale</last></author>
      <pages>941–949</pages>
      <abstract>In this paper, we (Team NICT-AI4B) describe our MT systems that we submit to the Indic MT task in WMT 2023. Our primary system consists of 3 stages: Joint denoising and MT training using officially approved monolingual and parallel corpora, backtranslation and, MT training on original and backtranslated parallel corpora. We observe that backtranslation leads to substantial improvements in translation quality up to 4 BLEU points. We also develop 2 contrastive systems on unconstrained settings, where the first system involves fine-tuning of IndicTrans2 DA models on official parallel corpora and seed data used in AI4Bharat et al, (2023), and the second system involves a system combination of the primary and the aforementioned system. Overall, we manage to obtain high-quality translation systems for the 4 low-resource North-East Indian languages of focus.</abstract>
      <url hash="a6672176">2023.wmt-1.88</url>
      <bibkey>dabre-etal-2023-nict</bibkey>
      <doi>10.18653/v1/2023.wmt-1.88</doi>
    </paper>
    <paper id="89">
      <title>Machine Translation Advancements for Low-Resource <fixed-case>I</fixed-case>ndian Languages in <fixed-case>WMT</fixed-case>23: <fixed-case>CFILT</fixed-case>-<fixed-case>IITB</fixed-case>’s Effort for Bridging the Gap</title>
      <author><first>Pranav</first><last>Gaikwad</last></author>
      <author><first>Meet</first><last>Doshi</last></author>
      <author><first>Sourabh</first><last>Deoghare</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>950–953</pages>
      <abstract>This paper is related to the submission of the CFILT-IITB team for the task called IndicMT in WMT23. The paper describes our MT systems submitted to the WMT23 IndicMT shared task. The task focused on MT system development from/to English and four low-resource North-East Indian languages, viz., Assamese, Khasi, Manipuri, and Mizo. We trained them on a small parallel corpus resulting in poor-quality systems. Therefore, we utilize transfer learning with the help of a large pre-trained multilingual NMT system. Since this approach produced the best results, we submitted our NMT models for the shared task using this approach.</abstract>
      <url hash="050f7f1e">2023.wmt-1.89</url>
      <bibkey>gaikwad-etal-2023-machine</bibkey>
      <doi>10.18653/v1/2023.wmt-1.89</doi>
    </paper>
    <paper id="90">
      <title>Low-Resource Machine Translation Systems for <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Ivana</first><last>Kvapilíková</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>954–958</pages>
      <abstract>We present our submission to the WMT23 shared task in translation between English and Assamese, Khasi, Mizo and Manipuri. All our systems were pretrained on the task of multilingual masked language modelling and denoising auto-encoding. Our primary systems for translation into English were further pretrained for multilingual MT in all four language directions and fine-tuned on the limited parallel data available for each language pair separately. We used online back-translation for data augmentation. The same systems were submitted as contrastive for translation out of English as the multilingual MT pretraining step seemed to harm the translation performance. Our primary systems for translation out of English were trained without the multilingual MT pretraining step. Other contrastive systems used additional pseudo-parallel data mined from monolingual corpora for pretraining.</abstract>
      <url hash="e51c8a5d">2023.wmt-1.90</url>
      <bibkey>kvapilikova-bojar-2023-low</bibkey>
      <doi>10.18653/v1/2023.wmt-1.90</doi>
    </paper>
    <paper id="91">
      <title><fixed-case>MUNI</fixed-case>-<fixed-case>NLP</fixed-case> Systems for Low-resource <fixed-case>I</fixed-case>ndic Machine Translation</title>
      <author><first>Edoardo</first><last>Signoroni</last></author>
      <author><first>Pavel</first><last>Rychly</last></author>
      <pages>959–966</pages>
      <abstract>The WMT 2023 Shared Task on Low-Resource Indic Language Translation featured to and from Assamese, Khasi, Manipuri, Mizo on one side and English on the other. We submitted systems supervised neural machine translation systems for each pair and direction and experimented with different configurations and settings for both preprocessing and training. Even if most of them did not reach competitive performance, our experiments uncovered some interesting points for further investigation, namely the relation between dataset and model size, and the impact of the training framework. Moreover, the results of some of our preliminary experiments on the use of word embeddings initialization, backtranslation, and model depth were in contrast with previous work. The final results also show some disagreement in the automated metrics employed in the evaluation.</abstract>
      <url hash="968b7797">2023.wmt-1.91</url>
      <bibkey>signoroni-rychly-2023-muni</bibkey>
      <doi>10.18653/v1/2023.wmt-1.91</doi>
    </paper>
    <paper id="92">
      <title><fixed-case>NITS</fixed-case>-<fixed-case>CNLP</fixed-case> Low-Resource Neural Machine Translation Systems of <fixed-case>E</fixed-case>nglish-<fixed-case>M</fixed-case>anipuri Language Pair</title>
      <author><first>Kshetrimayum Boynao</first><last>Singh</last></author>
      <author><first>Avichandra Singh</first><last>Ningthoujam</last></author>
      <author><first>Loitongbam</first><last>Sanayai Meetei</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <author><first>Thoudam Doren</first><last>Singh</last></author>
      <pages>967–971</pages>
      <abstract>This paper describes the transformer-based Neural Machine translation (NMT) system for the Low-Resource Indic Language Translation task for the English-Manipuri language pair submitted by the Centre for Natural Language Processing in National Institute of Technology Silchar, India (NITS-CNLP) in the WMT 2023 shared task. The model attained an overall BLEU score of 22.75 and 26.92 for the English to Manipuri and Manipuri to English translations respectively. Experimental results for English to Manipuri and Manipuri to English models for character level n-gram F-score (chrF) of 48.35 and 48.64, RIBES of 0.61 and 0.65, TER of 70.02 and 67.62, as well as COMET of 0.70 and 0.66 respectively are reported.</abstract>
      <url hash="504ac42b">2023.wmt-1.92</url>
      <bibkey>singh-etal-2023-nits</bibkey>
      <doi>10.18653/v1/2023.wmt-1.92</doi>
    </paper>
    <paper id="93">
      <title><fixed-case>IACS</fixed-case>-<fixed-case>LRILT</fixed-case>: Machine Translation for Low-Resource <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Dhairya</first><last>Suman</last></author>
      <author><first>Atanu</first><last>Mandal</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Sudip</first><last>Naskar</last></author>
      <pages>972–977</pages>
      <abstract>Even though, machine translation has seen huge improvements in the the last decade, translation quality for Indic languages is still underwhelming, which is attributed to the small amount of parallel data available. In this paper, we present our approach to mitigate the issue of the low amount of parallel training data availability for Indic languages, especially for the language pair English-Manipuri and Assamese-English. Our primary submission for the Manipuri-to-English translation task provided the best scoring system for this language direction. We describe about the systems we built in detail and our findings in the process.</abstract>
      <url hash="f81f985b">2023.wmt-1.93</url>
      <bibkey>suman-etal-2023-iacs</bibkey>
      <doi>10.18653/v1/2023.wmt-1.93</doi>
    </paper>
    <paper id="94">
      <title><fixed-case>IOL</fixed-case> Research Machine Translation Systems for <fixed-case>WMT</fixed-case>23 Low-Resource <fixed-case>I</fixed-case>ndic Language Translation Shared Task</title>
      <author><first>Wenbo</first><last>Zhang</last></author>
      <pages>978–982</pages>
      <abstract>This paper describes the IOL Research team’s submission systems for the WMT23 low-resource Indic language translation shared task. We participated in 4 language pairs, including en-as, en-mz, en-kha, en-mn. We use transformer based neural network architecture to train our machine translation models. Overall, the core of our system is to improve the quality of low resource translation by utilizing monolingual data through pre-training and data augmentation. We first trained two denoising language models similar to T5 and BART using monolingual data, and then used parallel data to fine-tune the pretrained language models to obtain two multilingual machine translation models. The multilingual machine translation models can be used to translate English monolingual data into other multilingual data, forming multilingual parallel data as augmented data. We trained multiple translation models from scratch using augmented data and real parallel data to build the final submission systems by model ensemble. Experimental results show that our method greatly improves the BLEU scores for translation of these four language pairs.</abstract>
      <url hash="d373df50">2023.wmt-1.94</url>
      <bibkey>zhang-2023-iol-research</bibkey>
      <doi>10.18653/v1/2023.wmt-1.94</doi>
    </paper>
    <paper id="95">
      <title>Trained <fixed-case>MT</fixed-case> Metrics Learn to Cope with Machine-translated References</title>
      <author><first>Jannis</first><last>Vamvas</last></author>
      <author><first>Tobias</first><last>Domhan</last></author>
      <author><first>Sony</first><last>Trenous</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Eva</first><last>Hasler</last></author>
      <pages>983–995</pages>
      <abstract>Neural metrics trained on human evaluations of MT tend to correlate well with human judgments, but their behavior is not fully understood. In this paper, we perform a controlled experiment and compare a baseline metric that has not been trained on human evaluations (Prism) to a trained version of the same metric (Prism+FT). Surprisingly, we find that Prism+FT becomes more robust to machine-translated references, which are a notorious problem in MT evaluation. This suggests that the effects of metric training go beyond the intended effect of improving overall correlation with human judgments.</abstract>
      <url hash="aed260fd">2023.wmt-1.95</url>
      <bibkey>vamvas-etal-2023-trained</bibkey>
      <doi>10.18653/v1/2023.wmt-1.95</doi>
      <video href="2023.wmt-1.95.mp4"/>
    </paper>
    <paper id="96">
      <title>Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Juraj</first><last>Juraska</last></author>
      <author><first>Mara</first><last>Finkelstein</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <pages>996–1013</pages>
      <abstract>As research on machine translation moves to translating text beyond the sentence level, it remains unclear how effective automatic evaluation metrics are at scoring longer translations. In this work, we first propose a method for creating paragraph-level data for training and meta-evaluating metrics from existing sentence-level data. Then, we use these new datasets to benchmark existing sentence-level metrics as well as train learned metrics at the paragraph level. Interestingly, our experimental results demonstrate that using sentence-level metrics to score entire paragraphs is equally as effective as using a metric designed to work at the paragraph level. We speculate this result can be attributed to properties of the task of reference-based evaluation as well as limitations of our datasets with respect to capturing all types of phenomena that occur in paragraph-level translations.</abstract>
      <url hash="11b85a82">2023.wmt-1.96</url>
      <bibkey>deutsch-etal-2023-training</bibkey>
      <doi>10.18653/v1/2023.wmt-1.96</doi>
    </paper>
    <paper id="97">
      <title>Automating Behavioral Testing in Machine Translation</title>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Hendra</first><last>Setiawan</last></author>
      <author><first>Dominic</first><last>Telaar</last></author>
      <author><first>Saša</first><last>Hasan</last></author>
      <pages>1014–1030</pages>
      <abstract>Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying only on accuracy.</abstract>
      <url hash="277d08ca">2023.wmt-1.97</url>
      <bibkey>ferrando-etal-2023-automating</bibkey>
      <doi>10.18653/v1/2023.wmt-1.97</doi>
    </paper>
    <paper id="98">
      <title>One Wide Feedforward Is All You Need</title>
      <author><first>Telmo</first><last>Pires</last></author>
      <author><first>António</first><last>Vilarinho Lopes</last></author>
      <author><first>Yannick</first><last>Assogba</last></author>
      <author><first>Hendra</first><last>Setiawan</last></author>
      <pages>1031–1044</pages>
      <abstract>The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model’s parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.</abstract>
      <url hash="1318e0ce">2023.wmt-1.98</url>
      <bibkey>pires-etal-2023-one</bibkey>
      <doi>10.18653/v1/2023.wmt-1.98</doi>
    </paper>
    <paper id="99">
      <title>A Benchmark for Evaluating Machine Translation Metrics on Dialects without Standard Orthography</title>
      <author><first>Noëmi</first><last>Aepli</last></author>
      <author><first>Chantal</first><last>Amrhein</last></author>
      <author><first>Florian</first><last>Schottmann</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>1045–1065</pages>
      <abstract>For sensible progress in natural language processing, it is important that we are aware of the limitations of the evaluation metrics we use. In this work, we evaluate how robust metrics are to non-standardized dialects, i.e. spelling differences in language varieties that do not have a standard orthography. To investigate this, we collect a dataset of human translations and human judgments for automatic machine translations from English to two Swiss German dialects. We further create a challenge set for dialect variation and benchmark existing metrics’ performances. Our results show that existing metrics cannot reliably evaluate Swiss German text generation outputs, especially on segment level. We propose initial design adaptations that increase robustness in the face of non-standardized dialects, although there remains much room for further improvement. The dataset, code, and models are available here: https://github.com/textshuttle/dialect_eval</abstract>
      <url hash="ccb4d589">2023.wmt-1.99</url>
      <bibkey>aepli-etal-2023-benchmark</bibkey>
      <doi>10.18653/v1/2023.wmt-1.99</doi>
    </paper>
    <paper id="100">
      <title>The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</title>
      <author><first>Patrick</first><last>Fernandes</last></author>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Mara</first><last>Finkelstein</last></author>
      <author><first>Parker</first><last>Riley</last></author>
      <author><first>André</first><last>Martins</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Ankush</first><last>Garg</last></author>
      <author><first>Jonathan</first><last>Clark</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <pages>1066–1083</pages>
      <abstract>Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.</abstract>
      <url hash="1eed3e33">2023.wmt-1.100</url>
      <bibkey>fernandes-etal-2023-devil</bibkey>
      <doi>10.18653/v1/2023.wmt-1.100</doi>
    </paper>
  </volume>
</collection>
