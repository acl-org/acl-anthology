<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.fever">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER)</booktitle>
      <editor><first>Christos</first><last>Christodoulopoulos</last></editor>
      <editor><first>James</first><last>Thorne</last></editor>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <editor><first>Oana</first><last>Cocarascu</last></editor>
      <editor><first>Arpit</first><last>Mittal</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="ee435e98">2020.fever-1</url>
    </meta>
    <frontmatter>
      <url hash="c5f632b1">2020.fever-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Simple Compounded-Label Training for Fact Extraction and Verification</title>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Lisa</first><last>Bauer</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1–7</pages>
      <abstract>Automatic fact checking is an important task motivated by the need for detecting and preventing the spread of misinformation across the web. The recently released FEVER challenge provides a benchmark task that assesses systems’ capability for both the retrieval of required evidence and the identification of authentic claims. Previous approaches share a similar pipeline training paradigm that decomposes the task into three subtasks, with each component built and trained separately. Although achieving acceptable scores, these methods induce difficulty for practical application development due to unnecessary complexity and expensive computation. In this paper, we explore the potential of simplifying the system design and reducing training computation by proposing a joint training setup in which a single sequence matching model is trained with compounded labels that give supervision for both sentence selection and claim verification subtasks, eliminating the duplicate computation that occurs when models are designed and trained separately. Empirical results on FEVER indicate that our method: (1) outperforms the typical multi-task learning approach, and (2) gets comparable results to top performing systems with a much simpler training setup and less training computation (in terms of the amount of data consumed and the number of model parameters), facilitating future works on the automatic fact checking task and its practical usage.</abstract>
      <url hash="0472bb38">2020.fever-1.1</url>
      <doi>10.18653/v1/2020.fever-1.1</doi>
      <video tag="video" href="http://slideslive.com/38929663"/>
    </paper>
    <paper id="2">
      <title>Stance Prediction and Claim Verification: An <fixed-case>A</fixed-case>rabic Perspective</title>
      <author><first>Jude</first><last>Khouja</last></author>
      <pages>8–17</pages>
      <abstract>This work explores the application of textual entailment in news claim verification and stance prediction using a new corpus in Arabic. The publicly available corpus comes in two perspectives: a version consisting of 4,547 true and false claims and a version consisting of 3,786 pairs (claim, evidence). We describe the methodology for creating the corpus and the annotation process. Using the introduced corpus, we also develop two machine learning baselines for two proposed tasks: claim verification and stance prediction. Our best model utilizes pretraining (BERT) and achieves 76.7 F1 on the stance prediction task and 64.3 F1 on the claim verification task. Our preliminary experiments shed some light on the limits of automatic claim verification that relies on claims text only. Results hint that while the linguistic features and world knowledge learned during pretraining are useful for stance prediction, such learned representations from pretraining are insufficient for verifying claims without access to context or evidence.</abstract>
      <url hash="1de96f57">2020.fever-1.2</url>
      <doi>10.18653/v1/2020.fever-1.2</doi>
      <video tag="video" href="http://slideslive.com/38929660"/>
    </paper>
    <paper id="3">
      <title>A Probabilistic Model with Commonsense Constraints for Pattern-based Temporal Fact Extraction</title>
      <author><first>Yang</first><last>Zhou</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>18–25</pages>
      <abstract>Textual patterns (e.g., Country’s president Person) are specified and/or generated for extracting factual information from unstructured data. Pattern-based information extraction methods have been recognized for their efficiency and transferability. However, not every pattern is reliable: A major challenge is to derive the most complete and accurate facts from diverse and sometimes conflicting extractions. In this work, we propose a probabilistic graphical model which formulates fact extraction in a generative process. It automatically infers true facts and pattern reliability without any supervision. It has two novel designs specially for temporal facts: (1) it models pattern reliability on two types of time signals, including temporal tag in text and text generation time; (2) it models commonsense constraints as observable variables. Experimental results demonstrate that our model significantly outperforms existing methods on extracting true temporal facts from news data.</abstract>
      <url hash="d3ebe317">2020.fever-1.3</url>
      <doi>10.18653/v1/2020.fever-1.3</doi>
      <video tag="video" href="http://slideslive.com/38929659"/>
    </paper>
    <paper id="4">
      <title>Developing a How-to Tip Machine Comprehension Dataset and its Evaluation in Machine Comprehension by <fixed-case>BERT</fixed-case></title>
      <author><first>Tengyang</first><last>Chen</last></author>
      <author><first>Hongyu</first><last>Li</last></author>
      <author><first>Miho</first><last>Kasamatsu</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Yasuhide</first><last>Kawada</last></author>
      <pages>26–35</pages>
      <abstract>In the field of factoid question answering (QA), it is known that the state-of-the-art technology has achieved an accuracy comparable to that of humans in a certain benchmark challenge. On the other hand, in the area of non-factoid QA, there is still a limited number of datasets for training QA models, i.e., machine comprehension models. Considering such a situation within the field of the non-factoid QA, this paper aims to develop a dataset for training Japanese how-to tip QA models. This paper applies one of the state-of-the-art machine comprehension models to the Japanese how-to tip QA dataset. The trained how-to tip QA model is also compared with a factoid QA model trained with a Japanese factoid QA dataset. Evaluation results revealed that the how-to tip machine comprehension performance was almost comparative with that of the factoid machine comprehension even with the training data size reduced to around 4% of the factoid machine comprehension. Thus, the how-to tip machine comprehension task requires much less training data compared with the factoid machine comprehension task.</abstract>
      <url hash="df076aad">2020.fever-1.4</url>
      <doi>10.18653/v1/2020.fever-1.4</doi>
      <video tag="video" href="http://slideslive.com/38929661"/>
    </paper>
    <paper id="5">
      <title>Language Models as Fact Checkers?</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Belinda</first><last>Li</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <pages>36–41</pages>
      <abstract>Recent work has suggested that language models (LMs) store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our finetuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.</abstract>
      <url hash="fcff8bf1">2020.fever-1.5</url>
      <doi>10.18653/v1/2020.fever-1.5</doi>
      <video tag="video" href="http://slideslive.com/38929662"/>
    </paper>
    <paper id="6">
      <title>Maintaining Quality in <fixed-case>FEVER</fixed-case> Annotation</title>
      <author><first>Leon</first><last>Derczynski</last></author>
      <author><first>Julie</first><last>Binau</last></author>
      <author><first>Henri</first><last>Schulte</last></author>
      <pages>42–46</pages>
      <abstract>We propose two measures for measuring the quality of constructed claims in the FEVER task. Annotating data for this task involves the creation of supporting and refuting claims over a set of evidence. Automatic annotation processes often leave superficial patterns in data, which learning systems can detect instead of performing the underlying task. Humans also can leave these superficial patterns, either voluntarily or involuntarily (due to e.g. fatigue). The two measures introduced attempt to detect the impact of these superficial patterns. One is a new information-theoretic and distributionality based measure, <i>DCI</i>; and the other an extension of neural probing work over the ARCT task, <i>utility</i>. We demonstrate these measures over a recent major dataset, that from the English FEVER task in 2019.</abstract>
      <url hash="7afd17e1">2020.fever-1.6</url>
      <doi>10.18653/v1/2020.fever-1.6</doi>
      <video tag="video" href="http://slideslive.com/38929664"/>
    </paper>
    <paper id="7">
      <title>Distilling the Evidence to Augment Fact Verification Models</title>
      <author><first>Beatrice</first><last>Portelli</last></author>
      <author><first>Jason</first><last>Zhao</last></author>
      <author><first>Tal</first><last>Schuster</last></author>
      <author><first>Giuseppe</first><last>Serra</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <pages>47–51</pages>
      <abstract>The alarming spread of fake news in social media, together with the impossibility of scaling manual fact verification, motivated the development of natural language processing techniques to automatically verify the veracity of claims. Most approaches perform a claim-evidence classification without providing any insights about why the claim is trustworthy or not. We propose, instead, a model-agnostic framework that consists of two modules: (1) a span extractor, which identifies the crucial information connecting claim and evidence; and (2) a classifier that combines claim, evidence, and the extracted spans to predict the veracity of the claim. We show that the spans are informative for the classifier, improving performance and robustness. Tested on several state-of-the-art models over the Fever dataset, the enhanced classifiers consistently achieve higher accuracy while also showing reduced sensitivity to artifacts in the claims.</abstract>
      <url hash="1683c74a">2020.fever-1.7</url>
      <doi>10.18653/v1/2020.fever-1.7</doi>
      <video tag="video" href="http://slideslive.com/38929665"/>
    </paper>
  </volume>
</collection>
