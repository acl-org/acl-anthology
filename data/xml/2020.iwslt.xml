<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.iwslt">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Spoken Language Translation</booktitle>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Alex</first><last>Waibel</last></editor>
      <editor><first>Kevin</first><last>Knight</last></editor>
      <editor><first>Satoshi</first><last>Nakamura</last></editor>
      <editor><first>Hermann</first><last>Ney</last></editor>
      <editor><first>Jan</first><last>Niehues</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>Dekai</first><last>Wu</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Francois</first><last>Yvon</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="4afe5e25">2020.iwslt-1</url>
    </meta>
    <frontmatter>
      <url hash="3be4c876">2020.iwslt-1.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>FINDINGS</fixed-case> <fixed-case>OF</fixed-case> <fixed-case>THE</fixed-case> <fixed-case>IWSLT</fixed-case> 2020 <fixed-case>EVALUATION</fixed-case> <fixed-case>CAMPAIGN</fixed-case></title>
      <author><first>Ebrahim</first><last>Ansari</last></author>
      <author><first>Amittai</first><last>Axelrod</last></author>
      <author><first>Nguyen</first><last>Bach</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Roldano</first><last>Cattoni</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Xing</first><last>Shi</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <author><first>Changhan</first><last>Wang</last></author>
      <pages>1–34</pages>
      <abstract>The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of teams participated in at least one of the tracks. This paper introduces each track’s goal, data and evaluation metrics, and reports the results of the received submissions.</abstract>
      <url hash="239fe261">2020.iwslt-1.1</url>
      <doi>10.18653/v1/2020.iwslt-1.1</doi>
      <revision id="1" href="2020.iwslt-1.1v1" hash="fb60f07e"/>
      <revision id="2" href="2020.iwslt-1.1v2" hash="239fe261" date="2020-08-07">Corrected a typo in the Section 1 title.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>ON</fixed-case>-<fixed-case>TRAC</fixed-case> Consortium for End-to-End and Simultaneous Speech Translation Challenge Tasks at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Maha</first><last>Elbayad</last></author>
      <author><first>Ha</first><last>Nguyen</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Natalia</first><last>Tomashenko</last></author>
      <author><first>Antoine</first><last>Caubrière</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>35–43</pages>
      <abstract>This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline speech translation and simultaneous speech translation. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Université), LIG (Université Grenoble Alpes), and LIUM (Le Mans Université). Attention-based encoder-decoder models, trained end-to-end, were used for our submissions to the offline speech translation track. Our contributions focused on data augmentation and ensembling of multiple models. In the simultaneous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks.</abstract>
      <url hash="5f16383a">2020.iwslt-1.2</url>
      <doi>10.18653/v1/2020.iwslt-1.2</doi>
      <video tag="video" href="http://slideslive.com/38929610"/>
    </paper>
    <paper id="3">
      <title>Start-Before-End and End-to-End: Neural Speech Translation by <fixed-case>A</fixed-case>pp<fixed-case>T</fixed-case>ek and <fixed-case>RWTH</fixed-case> <fixed-case>A</fixed-case>achen <fixed-case>U</fixed-case>niversity</title>
      <author><first>Parnia</first><last>Bahar</last></author>
      <author><first>Patrick</first><last>Wilken</last></author>
      <author><first>Tamer</first><last>Alkhouli</last></author>
      <author><first>Andreas</first><last>Guta</last></author>
      <author><first>Pavel</first><last>Golik</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <pages>44–54</pages>
      <abstract>AppTek and RWTH Aachen University team together to participate in the offline and simultaneous speech translation tracks of IWSLT 2020. For the offline task, we create both cascaded and end-to-end speech translation systems, paying attention to careful data selection and weighting. In the cascaded approach, we combine high-quality hybrid automatic speech recognition (ASR) with the Transformer-based neural machine translation (NMT). Our end-to-end direct speech translation systems benefit from pretraining of adapted encoder and decoder components, as well as synthetic data and fine-tuning and thus are able to compete with cascaded systems in terms of MT quality. For simultaneous translation, we utilize a novel architecture that makes dynamic decisions, learned from parallel data, to determine when to continue feeding on input or generate output words. Experiments with speech and text input show that even at low latency this architecture leads to superior translation results.</abstract>
      <url hash="31036c3c">2020.iwslt-1.3</url>
      <doi>10.18653/v1/2020.iwslt-1.3</doi>
      <video tag="video" href="http://slideslive.com/38929614"/>
    </paper>
    <paper id="4">
      <title><fixed-case>KIT</fixed-case>’s <fixed-case>IWSLT</fixed-case> 2020 <fixed-case>SLT</fixed-case> Translation System</title>
      <author><first>Ngoc-Quan</first><last>Pham</last></author>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Tuan-Nam</first><last>Nguyen</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Thai Son</first><last>Nguyen</last></author>
      <author><first>Maximilian</first><last>Awiszus</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>55–61</pages>
      <abstract>This paper describes KIT’s submissions to the IWSLT2020 Speech Translation evaluation campaign. We first participate in the simultaneous translation task, in which our simultaneous models are Transformer based and can be efficiently trained to obtain low latency with minimized compromise in quality. On the offline speech translation task, we applied our new Speech Transformer architecture to end-to-end speech translation. The obtained model can provide translation quality which is competitive to a complicated cascade. The latter still has the upper hand, thanks to the ability to transparently access to the transcription, and resegment the inputs to avoid fragmentation.</abstract>
      <url hash="1675706b">2020.iwslt-1.4</url>
      <doi>10.18653/v1/2020.iwslt-1.4</doi>
      <video tag="video" href="http://slideslive.com/38929605"/>
    </paper>
    <paper id="5">
      <title>End-to-End Simultaneous Translation System for <fixed-case>IWSLT</fixed-case>2020 Using Modality Agnostic Meta-Learning</title>
      <author><first>Hou Jeung</first><last>Han</last></author>
      <author><first>Mohd Abbas</first><last>Zaidi</last></author>
      <author><first>Sathish Reddy</first><last>Indurthi</last></author>
      <author><first>Nikhil Kumar</first><last>Lakumarapu</last></author>
      <author><first>Beomseok</first><last>Lee</last></author>
      <author><first>Sangha</first><last>Kim</last></author>
      <pages>62–68</pages>
      <abstract>In this paper, we describe end-to-end simultaneous speech-to-text and text-to-text translation systems submitted to IWSLT2020 online translation challenge. The systems are built by adding wait-k and meta-learning approaches to the Transformer architecture. The systems are evaluated on different latency regimes. The simultaneous text-to-text translation achieved a BLEU score of 26.38 compared to the competition baseline score of 14.17 on the low latency regime (Average latency ≤ 3). The simultaneous speech-to-text system improves the BLEU score by 7.7 points over the competition baseline for the low latency regime (Average Latency ≤ 1000).</abstract>
      <url hash="adef0192">2020.iwslt-1.5</url>
      <doi>10.18653/v1/2020.iwslt-1.5</doi>
      <video tag="video" href="http://slideslive.com/38929597"/>
    </paper>
    <paper id="6">
      <title><fixed-case>D</fixed-case>i<fixed-case>D</fixed-case>i Labs’ End-to-end System for the <fixed-case>IWSLT</fixed-case> 2020 Offline Speech <fixed-case>T</fixed-case>ranslation<fixed-case>T</fixed-case>ask</title>
      <author><first>Arkady</first><last>Arkhangorodsky</last></author>
      <author><first>Yiqi</first><last>Huang</last></author>
      <author><first>Amittai</first><last>Axelrod</last></author>
      <pages>69–72</pages>
      <abstract>This paper describes the system that was submitted by DiDi Labs to the offline speech translation task for IWSLT 2020. We trained an end-to-end system that translates audio from English TED talks to German text, without producing intermediate English text. We use the S-Transformer architecture and train using the MuSTC dataset. We also describe several additional experiments that were attempted, but did not yield improved results.</abstract>
      <url hash="39d325a0">2020.iwslt-1.6</url>
      <doi>10.18653/v1/2020.iwslt-1.6</doi>
      <video tag="video" href="http://slideslive.com/38929593"/>
    </paper>
    <paper id="7">
      <title>End-to-End Offline Speech Translation System for <fixed-case>IWSLT</fixed-case> 2020 using Modality Agnostic Meta-Learning</title>
      <author><first>Nikhil Kumar</first><last>Lakumarapu</last></author>
      <author><first>Beomseok</first><last>Lee</last></author>
      <author><first>Sathish Reddy</first><last>Indurthi</last></author>
      <author><first>Hou Jeung</first><last>Han</last></author>
      <author><first>Mohd Abbas</first><last>Zaidi</last></author>
      <author><first>Sangha</first><last>Kim</last></author>
      <pages>73–79</pages>
      <abstract>In this paper, we describe the system submitted to the IWSLT 2020 Offline Speech Translation Task. We adopt the Transformer architecture coupled with the meta-learning approach to build our end-to-end Speech-to-Text Translation (ST) system. Our meta-learning approach tackles the data scarcity of the ST task by leveraging the data available from Automatic Speech Recognition (ASR) and Machine Translation (MT) tasks. The meta-learning approach combined with synthetic data augmentation techniques improves the model performance significantly and achieves BLEU scores of 24.58, 27.51, and 27.61 on IWSLT test 2015, MuST-C test, and Europarl-ST test sets respectively.</abstract>
      <url hash="e8ae4cd7">2020.iwslt-1.7</url>
      <doi>10.18653/v1/2020.iwslt-1.7</doi>
      <video tag="video" href="http://slideslive.com/38929596"/>
    </paper>
    <paper id="8">
      <title>End-to-End Speech-Translation with Knowledge Distillation: <fixed-case>FBK</fixed-case>@<fixed-case>IWSLT</fixed-case>2020</title>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Mattia A.</first><last>Di Gangi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>80–88</pages>
      <abstract>This paper describes FBK’s participation in the IWSLT 2020 offline speech translation (ST) task. The task evaluates systems’ ability to translate English TED talks audio into German texts. The test talks are provided in two versions: one contains the data already segmented with automatic tools and the other is the raw data without any segmentation. Participants can decide whether to work on custom segmentation or not. We used the provided segmentation. Our system is an end-to-end model based on an adaptation of the Transformer for speech data. Its training process is the main focus of this paper and it is based on: i) transfer learning (ASR pretraining and knowledge distillation), ii) data augmentation (SpecAugment, time stretch and synthetic data), iii)combining synthetic and real data marked as different domains, and iv) multi-task learning using the CTC loss. Finally, after the training with word-level knowledge distillation is complete, our ST models are fine-tuned using label smoothed cross entropy. Our best model scored 29 BLEU on the MuST-CEn-De test set, which is an excellent result compared to recent papers, and 23.7 BLEU on the same data segmented with VAD, showing the need for researching solutions addressing this specific data condition.</abstract>
      <url hash="042342a4">2020.iwslt-1.8</url>
      <doi>10.18653/v1/2020.iwslt-1.8</doi>
      <video tag="video" href="http://slideslive.com/38929598"/>
    </paper>
    <paper id="9">
      <title><fixed-case>SRPOL</fixed-case>’s System for the <fixed-case>IWSLT</fixed-case> 2020 End-to-End Speech Translation Task</title>
      <author><first>Tomasz</first><last>Potapczyk</last></author>
      <author><first>Pawel</first><last>Przybysz</last></author>
      <pages>89–94</pages>
      <abstract>We took part in the offline End-to-End English to German TED lectures translation task. We based our solution on our last year’s submission. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder. To improve the model’s quality of translation we introduced two regularization techniques and trained on machine translated Librispeech corpus in addition to iwslt-corpus, TEDLIUM2 andMust_C corpora. Our best model scored almost 3 BLEU higher than last year’s model. To segment 2020 test set we used exactly the same procedure as last year.</abstract>
      <url hash="233224a1">2020.iwslt-1.9</url>
      <doi>10.18653/v1/2020.iwslt-1.9</doi>
    </paper>
    <paper id="10">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki Submission to the <fixed-case>IWSLT</fixed-case>2020 Offline <fixed-case>S</fixed-case>peech<fixed-case>T</fixed-case>ranslation Task</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Mikko</first><last>Aulamo</last></author>
      <author><first>Umut</first><last>Sulubacak</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>95–102</pages>
      <abstract>This paper describes the University of Helsinki Language Technology group’s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year’s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted systems. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for knowledge-transfer across modalities.</abstract>
      <url hash="0b46f3a0">2020.iwslt-1.10</url>
      <doi>10.18653/v1/2020.iwslt-1.10</doi>
      <video tag="video" href="http://slideslive.com/38929617"/>
    </paper>
    <paper id="11">
      <title>The <fixed-case>AFRL</fixed-case> <fixed-case>IWSLT</fixed-case> 2020 Systems: Work-From-Home Edition</title>
      <author><first>Brian</first><last>Ore</last></author>
      <author><first>Eric</first><last>Hansen</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <pages>103–108</pages>
      <abstract>This report summarizes the Air Force Research Laboratory (AFRL) submission to the offline spoken language translation (SLT) task as part of the IWSLT 2020 evaluation campaign. As in previous years, we chose to adopt the cascade approach of using separate systems to perform speech activity detection, automatic speech recognition, sentence segmentation, and machine translation. All systems were neural based, including a fully-connected neural network for speech activity detection, a Kaldi factorized time delay neural network with recurrent neural network (RNN) language model rescoring for speech recognition, a bidirectional RNN with attention mechanism for sentence segmentation, and transformer networks trained with OpenNMT and Marian for machine translation. Our primary submission yielded BLEU scores of 21.28 on tst2019 and 23.33 on tst2020.</abstract>
      <url hash="5dca6d62">2020.iwslt-1.11</url>
      <doi>10.18653/v1/2020.iwslt-1.11</doi>
      <video tag="video" href="http://slideslive.com/38929615"/>
    </paper>
    <paper id="12">
      <title><fixed-case>LIT</fixed-case> Team’s System Description for <fixed-case>J</fixed-case>apanese-<fixed-case>C</fixed-case>hinese Machine Translation Task in <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Yimeng</first><last>Zhuang</last></author>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Lijie</first><last>Wang</last></author>
      <pages>109–113</pages>
      <abstract>This paper describes the LIT Team’s submission to the IWSLT2020 open domain translation task, focusing primarily on Japanese-to-Chinese translation direction. Our system is based on the organizers’ baseline system, but we do more works on improving the Transform baseline system by elaborate data pre-processing. We manage to obtain significant improvements, and this paper aims to share some data processing experiences in this translation task. Large-scale back-translation on monolingual corpus is also investigated. In addition, we also try shared and exclusive word embeddings, compare different granularity of tokens like sub-word level. Our Japanese-to-Chinese translation system achieves a performance of BLEU=34.0 and ranks 2nd among all participating systems.</abstract>
      <url hash="edd866ee">2020.iwslt-1.12</url>
      <doi>10.18653/v1/2020.iwslt-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>OPPO</fixed-case>’s Machine Translation System for the <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Dawei</first><last>Dang</last></author>
      <author><first>Tingxun</first><last>Shi</last></author>
      <author><first>Di</first><last>Ai</last></author>
      <author><first>Zhengshan</first><last>Xue</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <pages>114–121</pages>
      <abstract>In this paper, we demonstrate our machine translation system applied for the Chinese-Japanese bidirectional translation task (aka. open domain translation task) for the IWSLT 2020. Our model is based on Transformer (Vaswani et al., 2017), with the help of many popular, widely proved effective data preprocessing and augmentation methods. Experiments show that these methods can improve the baseline model steadily and significantly.</abstract>
      <url hash="3a418dac">2020.iwslt-1.13</url>
      <doi>10.18653/v1/2020.iwslt-1.13</doi>
      <video tag="video" href="http://slideslive.com/38929611"/>
    </paper>
    <paper id="14">
      <title>Character Mapping and Ad-hoc Adaptation: <fixed-case>E</fixed-case>dinburgh’s <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation System</title>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <pages>122–129</pages>
      <abstract>This paper describes the University of Edinburgh’s neural machine translation systems submitted to the IWSLT 2020 open domain Japanese<tex-math>\leftrightarrow</tex-math>Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.</abstract>
      <url hash="cfaba852">2020.iwslt-1.14</url>
      <doi>10.18653/v1/2020.iwslt-1.14</doi>
      <video tag="video" href="http://slideslive.com/38929590"/>
    </paper>
    <paper id="15">
      <title><fixed-case>CASIA</fixed-case>’s System for <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation</title>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Cong</first><last>Ma</last></author>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>130–139</pages>
      <abstract>This paper describes the CASIA’s system for the IWSLT 2020 open domain translation task. This year we participate in both Chinese→Japanese and Japanese→Chinese translation tasks. Our system is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques.</abstract>
      <url hash="d120b4ec">2020.iwslt-1.15</url>
      <doi>10.18653/v1/2020.iwslt-1.15</doi>
      <video tag="video" href="http://slideslive.com/38929589"/>
    </paper>
    <paper id="16">
      <title>Deep Blue Sonics’ Submission to <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Enmin</first><last>Su</last></author>
      <author><first>Yi</first><last>Ren</last></author>
      <pages>140–144</pages>
      <abstract>We present in this report our submission to IWSLT 2020 Open Domain Translation Task. We built a data pre-processing pipeline to efficiently handle large noisy web-crawled corpora, which boosts the BLEU score of a widely used transformer model in this translation task. To tackle the open-domain nature of this task, back- translation is applied to further improve the translation performance.</abstract>
      <url hash="514a7105">2020.iwslt-1.16</url>
      <doi>10.18653/v1/2020.iwslt-1.16</doi>
      <video tag="video" href="http://slideslive.com/38929592"/>
    </paper>
    <paper id="17">
      <title><fixed-case>U</fixed-case>niversity of <fixed-case>T</fixed-case>sukuba’s Machine Translation System for <fixed-case>IWSLT</fixed-case>20 Open Domain Translation Task</title>
      <author><first>Hongyi</first><last>Cui</last></author>
      <author><first>Yizhen</first><last>Wei</last></author>
      <author><first>Shohei</first><last>Iida</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>145–148</pages>
      <abstract>In this paper, we introduce University of Tsukuba’s submission to the IWSLT20 Open Domain Translation Task. We participate in both Chinese→Japanese and Japanese→Chinese directions. For both directions, our machine translation systems are based on the Transformer architecture. Several techniques are integrated in order to boost the performance of our models: data filtering, large-scale noised training, model ensemble, reranking and postprocessing. Consequently, our efforts achieve 33.0 BLEU scores for Chinese→Japanese translation and 32.3 BLEU scores for Japanese→Chinese translation.</abstract>
      <url hash="10557acc">2020.iwslt-1.17</url>
      <doi>10.18653/v1/2020.iwslt-1.17</doi>
      <video tag="video" href="http://slideslive.com/38929619"/>
    </paper>
    <paper id="18">
      <title>Xiaomi’s Submissions for <fixed-case>IWSLT</fixed-case> 2020 Open Domain Translation Task</title>
      <author><first>Yuhui</first><last>Sun</last></author>
      <author><first>Mengxue</first><last>Guo</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Jianwei</first><last>Cui</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>149–157</pages>
      <abstract>This paper describes the Xiaomi’s submissions to the IWSLT20 shared open domain translation task for Chinese&lt;-&gt;Japanese language pair. We explore different model ensembling strategies based on recent Transformer variants. We also further strengthen our systems via some effective techniques, such as data filtering, data selection, tagged back translation, domain adaptation, knowledge distillation, and re-ranking. Our resulting Chinese-&gt;Japanese primary system ranked second in terms of character-level BLEU score among all submissions. Our resulting Japanese-&gt;Chinese primary system also achieved a competitive performance.</abstract>
      <url hash="1690c60b">2020.iwslt-1.18</url>
      <doi>10.18653/v1/2020.iwslt-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>ISTIC</fixed-case>’s Neural Machine Translation System for <fixed-case>IWSLT</fixed-case>’2020</title>
      <author><first>Jiaze</first><last>Wei</last></author>
      <author><first>Wenbin</first><last>Liu</last></author>
      <author><first>Zhenfeng</first><last>Wu</last></author>
      <author><first>You</first><last>Pan</last></author>
      <author><first>Yanqing</first><last>He</last></author>
      <pages>158–165</pages>
      <abstract>This paper introduces technical details of machine translation system of Institute of Scientific and Technical Information of China (ISTIC) for the 17th International Conference on Spoken Language Translation (IWSLT 2020). ISTIC participated in both translation tasks of the Open Domain Translation track: Japanese-to-Chinese MT task and Chinese-to-Japanese MT task. The paper mainly elaborates on the model framework, data preprocessing methods and decoding strategies adopted in our system. In addition, the system performance on the development set are given under different settings.</abstract>
      <url hash="48f513f1">2020.iwslt-1.19</url>
      <doi>10.18653/v1/2020.iwslt-1.19</doi>
    </paper>
    <paper id="20">
      <title>Octanove Labs’ <fixed-case>J</fixed-case>apanese-<fixed-case>C</fixed-case>hinese Open Domain Translation System</title>
      <author><first>Masato</first><last>Hagiwara</last></author>
      <pages>166–171</pages>
      <abstract>This paper describes Octanove Labs’ submission to the IWSLT 2020 open domain translation challenge. In order to build a high-quality Japanese-Chinese neural machine translation (NMT) system, we use a combination of 1) parallel corpus filtering and 2) back-translation. We have shown that, by using heuristic rules and learned classifiers, the size of the parallel data can be reduced by 70% to 90% without much impact on the final MT performance. We have also shown that including the artificially generated parallel data through back-translation further boosts the metric by 17% to 27%, while self-training contributes little. Aside from a small number of parallel sentences annotated for filtering, no external resources have been used to build our system.</abstract>
      <url hash="5beca769">2020.iwslt-1.20</url>
      <doi>10.18653/v1/2020.iwslt-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>NAIST</fixed-case>’s Machine Translation Systems for <fixed-case>IWSLT</fixed-case> 2020 Conversational Speech Translation Task</title>
      <author><first>Ryo</first><last>Fukuda</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>172–177</pages>
      <abstract>This paper describes NAIST’s NMT system submitted to the IWSLT 2020 conversational speech translation task. We focus on the translation disfluent speech transcripts that include ASR errors and non-grammatical utterances. We tried a domain adaptation method by transferring the styles of out-of-domain data (United Nations Parallel Corpus) to be like in-domain data (Fisher transcripts). Our system results showed that the NMT model with domain adaptation outperformed a baseline. In addition, slight improvement by the style transfer was observed.</abstract>
      <url hash="b615a9a1">2020.iwslt-1.21</url>
      <doi>10.18653/v1/2020.iwslt-1.21</doi>
    </paper>
    <paper id="22">
      <title>Generating Fluent Translations from Disfluent Text Without Access to Fluent References: <fixed-case>IIT</fixed-case> <fixed-case>B</fixed-case>ombay@<fixed-case>IWSLT</fixed-case>2020</title>
      <author><first>Nikhil</first><last>Saini</last></author>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>178–186</pages>
      <abstract>Machine translation systems perform reasonably well when the input is well-formed speech or text. Conversational speech is spontaneous and inherently consists of many disfluencies. Producing fluent translations of disfluent source text would typically require parallel disfluent to fluent training data. However, fluent translations of spontaneous speech are an additional resource that is tedious to obtain. This work describes the submission of IIT Bombay to the Conversational Speech Translation challenge at IWSLT 2020. We specifically tackle the problem of disfluency removal in disfluent-to-fluent text-to-text translation assuming no access to fluent references during training. Common patterns of disfluency are extracted from disfluent references and a noise induction model is used to simulate them starting from a clean monolingual corpus. This synthetically constructed dataset is then considered as a proxy for labeled data during training. We also make use of additional fluent text in the target language to help generate fluent translations. This work uses no fluent references during training and beats a baseline model by a margin of 4.21 and 3.11 BLEU points where the baseline uses disfluent and fluent references, respectively. Index Terms- disfluency removal, machine translation, noise induction, leveraging monolingual data, denoising for disfluency removal.</abstract>
      <url hash="a3020024">2020.iwslt-1.22</url>
      <doi>10.18653/v1/2020.iwslt-1.22</doi>
    </paper>
    <paper id="23">
      <title>The <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> Video Speech Translation System at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yao</first><last>Deng</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Ning</first><last>Xie</last></author>
      <author><first>Xiaochun</first><last>Li</last></author>
      <author><first>Jiaxian</first><last>Guo</last></author>
      <pages>187–190</pages>
      <abstract>The paper presents details of our system in the IWSLT Video Speech Translation evaluation. The system works in a cascade form, which contains three modules: 1) A proprietary ASR system. 2) A disfluency correction system aims to remove interregnums or other disfluent expressions with a fine-tuned BERT and a series of rule-based algorithms. 3) An NMT System based on the Transformer and trained with massive publicly available corpus.</abstract>
      <url hash="c178ea46">2020.iwslt-1.23</url>
      <doi>10.18653/v1/2020.iwslt-1.23</doi>
      <video tag="video" href="http://slideslive.com/38929616"/>
    </paper>
    <paper id="24">
      <title><fixed-case>CUNI</fixed-case> Neural <fixed-case>ASR</fixed-case> with Phoneme-Level Intermediate Step for~<fixed-case>N</fixed-case>on-<fixed-case>N</fixed-case>ative~<fixed-case>SLT</fixed-case> at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Peter</first><last>Polák</last></author>
      <author><first>Sangeet</first><last>Sagar</last></author>
      <author><first>Dominik</first><last>Macháček</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>191–199</pages>
      <abstract>In this paper, we present our submission to the Non-Native Speech Translation Task for IWSLT 2020. Our main contribution is a proposed speech recognition pipeline that consists of an acoustic model and a phoneme-to-grapheme model. As an intermediate representation, we utilize phonemes. We demonstrate that the proposed pipeline surpasses commercially used automatic speech recognition (ASR) and submit it into the ASR track. We complement this ASR with off-the-shelf MT systems to take part also in the speech translation track.</abstract>
      <url hash="c7893255">2020.iwslt-1.24</url>
      <doi>10.18653/v1/2020.iwslt-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>ELITR</fixed-case> Non-Native Speech Translation at <fixed-case>IWSLT</fixed-case> 2020</title>
      <author><first>Dominik</first><last>Macháček</last></author>
      <author><first>Jonáš</first><last>Kratochvíl</last></author>
      <author><first>Sangeet</first><last>Sagar</last></author>
      <author><first>Matúš</first><last>Žilinec</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Thai-Son</first><last>Nguyen</last></author>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Philip</first><last>Williams</last></author>
      <author><first>Yuekun</first><last>Yao</last></author>
      <pages>200–208</pages>
      <abstract>This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</abstract>
      <url hash="e32ca293">2020.iwslt-1.25</url>
      <doi>10.18653/v1/2020.iwslt-1.25</doi>
      <video tag="video" href="http://slideslive.com/38929595"/>
    </paper>
    <paper id="26">
      <title>Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?</title>
      <author><first>Alina</first><last>Karakanta</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>209–219</pages>
      <abstract>Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling, a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length is not the answer to everything in the case of subtitling-oriented ST.</abstract>
      <url hash="5a7d2891">2020.iwslt-1.26</url>
      <doi>10.18653/v1/2020.iwslt-1.26</doi>
      <video tag="video" href="http://slideslive.com/38929602"/>
    </paper>
    <paper id="27">
      <title>Re-translation versus Streaming for Simultaneous Translation</title>
      <author><first>Naveen</first><last>Arivazhagan</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <author><first>Wolfgang</first><last>Macherey</last></author>
      <author><first>George</first><last>Foster</last></author>
      <pages>220–227</pages>
      <abstract>There has been great progress in improving streaming machine translation, a simultaneous paradigm where the system appends to a growing hypothesis as more source content becomes available. We study a related problem in which revisions to the hypothesis beyond strictly appending words are permitted. This is suitable for applications such as live captioning an audio feed. In this setting, we compare custom streaming approaches to re-translation, a straightforward strategy where each new source token triggers a distinct translation from scratch. We find re-translation to be as good or better than state-of-the-art streaming systems, even when operating under constraints that allow very few revisions. We attribute much of this success to a previously proposed data-augmentation technique that adds prefix-pairs to the training data, which alongside wait-k inference forms a strong baseline for streaming translation. We also highlight re-translation’s ability to wrap arbitrarily powerful MT systems with an experiment showing large improvements from an upgrade to its base model.</abstract>
      <url hash="af172ded">2020.iwslt-1.27</url>
      <doi>10.18653/v1/2020.iwslt-1.27</doi>
      <video tag="video" href="http://slideslive.com/38929612"/>
    </paper>
    <paper id="28">
      <title>Towards Stream Translation: Adaptive Computation Time for Simultaneous Machine Translation</title>
      <author><first>Felix</first><last>Schneider</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>228–236</pages>
      <abstract>Simultaneous machine translation systems rely on a policy to schedule read and write operations in order to begin translating a source sentence before it is complete. In this paper, we demonstrate the use of Adaptive Computation Time (ACT) as an adaptive, learned policy for simultaneous machine translation using the transformer model and as a more numerically stable alternative to Monotonic Infinite Lookback Attention (MILk). We achieve state-of-the-art results in terms of latency-quality tradeoffs. We also propose a method to use our model on unsegmented input, i.e. without sentence boundaries, simulating the condition of translating output from automatic speech recognition. We present first benchmark results on this task.</abstract>
      <url hash="188b53be">2020.iwslt-1.28</url>
      <doi>10.18653/v1/2020.iwslt-1.28</doi>
      <video tag="video" href="http://slideslive.com/38929618"/>
    </paper>
    <paper id="29">
      <title>Neural Simultaneous Speech Translation Using Alignment-Based Chunking</title>
      <author><first>Patrick</first><last>Wilken</last></author>
      <author><first>Tamer</first><last>Alkhouli</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Pavel</first><last>Golik</last></author>
      <pages>237–246</pages>
      <abstract>In simultaneous machine translation, the objective is to determine when to produce a partial translation given a continuous stream of source words, with a trade-off between latency and quality. We propose a neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words. The model is composed of two main components: one to dynamically decide on ending a source chunk, and another that translates the consumed chunk. We train the components jointly and in a manner consistent with the inference conditions. To generate chunked training data, we propose a method that utilizes word alignment while also preserving enough context. We compare models with bidirectional and unidirectional encoders of different depths, both on real speech and text input. Our results on the IWSLT 2020 English-to-German task outperform a wait-k baseline by 2.6 to 3.7% BLEU absolute.</abstract>
      <url hash="34bea1f1">2020.iwslt-1.29</url>
      <doi>10.18653/v1/2020.iwslt-1.29</doi>
      <video tag="video" href="http://slideslive.com/38929608"/>
    </paper>
    <paper id="30">
      <title>Adapting End-to-End Speech Recognition for Readable Subtitles</title>
      <author><first>Danni</first><last>Liu</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Gerasimos</first><last>Spanakis</last></author>
      <pages>247–256</pages>
      <abstract>Automatic speech recognition (ASR) systems are primarily evaluated on transcription accuracy. However, in some use cases such as subtitling, verbatim transcription would reduce output readability given limited screen size and reading time. Therefore, this work focuses on ASR with output compression, a task challenging for supervised approaches due to the scarcity of training data. We first investigate a cascaded system, where an unsupervised compression model is used to post-edit the transcribed speech. We then compare several methods of end-to-end speech recognition under output length constraints. The experiments show that with limited data far less than needed for training a model from scratch, we can adapt a Transformer-based ASR model to incorporate both transcription and compression capabilities. Furthermore, the best performance in terms of WER and ROUGE scores is achieved by explicitly modeling the length constraints within the end-to-end ASR system.</abstract>
      <url hash="44c481de">2020.iwslt-1.30</url>
      <doi>10.18653/v1/2020.iwslt-1.30</doi>
      <video tag="video" href="http://slideslive.com/38929588"/>
    </paper>
    <paper id="31">
      <title>From Speech-to-Speech Translation to Automatic Dubbing</title>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Robert</first><last>Enyedi</last></author>
      <author><first>Roberto</first><last>Barra-Chicote</last></author>
      <author><first>Ritwik</first><last>Giri</last></author>
      <author><first>Umut</first><last>Isik</last></author>
      <author><first>Arvindh</first><last>Krishnaswamy</last></author>
      <author><first>Hassan</first><last>Sawaf</last></author>
      <pages>257–264</pages>
      <abstract>We present enhancements to a speech-to-speech translation pipeline in order to perform automatic dubbing. Our architecture features neural machine translation generating output of preferred length, prosodic alignment of the translation with the original speech segments, neural text-to-speech with fine tuning of the duration of each utterance, and, finally, audio rendering to enriches text-to-speech output with background noise and reverberation extracted from the original audio. We report and discuss results of a first subjective evaluation of automatic dubbing of excerpts of TED Talks from English into Italian, which measures the perceived naturalness of automatic dubbing and the relative importance of each proposed enhancement.</abstract>
      <url hash="61e2aea2">2020.iwslt-1.31</url>
      <doi>10.18653/v1/2020.iwslt-1.31</doi>
      <video tag="video" href="http://slideslive.com/38929599"/>
    </paper>
    <paper id="32">
      <title>Joint Translation and Unit Conversion for End-to-end Localization</title>
      <author><first>Georgiana</first><last>Dinu</last></author>
      <author><first>Prashant</first><last>Mathur</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Stanislas</first><last>Lauly</last></author>
      <author><first>Yaser</first><last>Al-Onaizan</last></author>
      <pages>265–271</pages>
      <abstract>A variety of natural language tasks require processing of textual data which contains a mix of natural language and formal languages such as mathematical expressions. In this paper, we take unit conversions as an example and propose a data augmentation technique which lead to models learning both translation and conversion tasks as well as how to adequately switch between them for end-to-end localization.</abstract>
      <url hash="1add4efb">2020.iwslt-1.32</url>
      <doi>10.18653/v1/2020.iwslt-1.32</doi>
      <video tag="video" href="http://slideslive.com/38929604"/>
    </paper>
    <paper id="33">
      <title>Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference</title>
      <author><first>Maury</first><last>Courtland</last></author>
      <author><first>Adam</first><last>Faulkner</last></author>
      <author><first>Gayle</first><last>McElvain</last></author>
      <pages>272–279</pages>
      <abstract>Though people rarely speak in complete sentences, punctuation confers many benefits to the readers of transcribed speech. Unfortunately, most ASR systems do not produce punctuated output. To address this, we propose a solution for automatic punctuation that is both cost efficient and easy to train. Our solution benefits from the recent trend in fine-tuning transformer-based language models. We also modify the typical framing of this task by predicting punctuation for sequences rather than individual tokens, which makes for more efficient training and inference. Finally, we find that aggregating predictions across multiple context windows improves accuracy even further. Our best model achieves a new state of the art on benchmark data (TED Talks) with a combined F1 of 83.9, representing a 48.7% relative improvement (15.3 absolute) over the previous state of the art.</abstract>
      <url hash="45ee1725">2020.iwslt-1.33</url>
      <doi>10.18653/v1/2020.iwslt-1.33</doi>
      <video tag="video" href="http://slideslive.com/38929594"/>
    </paper>
    <paper id="34">
      <title>How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Tom S</first><last>Juzek</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <pages>280–290</pages>
      <abstract>Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we – (i) detail two non-invasive ways of detecting translationese and (ii) compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model (human vs machine) rather than to the data (written vs spoken).</abstract>
      <url hash="19f3cd6a">2020.iwslt-1.34</url>
      <doi>10.18653/v1/2020.iwslt-1.34</doi>
      <video tag="video" href="http://slideslive.com/38929601"/>
    </paper>
  </volume>
</collection>
