<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.clinicalnlp">
  <volume id="1" ingest-date="2023-07-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 5th Clinical Natural Language Processing Workshop</booktitle>
      <editor><first>Tristan</first><last>Naumann</last></editor>
      <editor><first>Asma</first><last>Ben Abacha</last></editor>
      <editor><first>Steven</first><last>Bethard</last></editor>
      <editor><first>Kirk</first><last>Roberts</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="0e3fa54f">2023.clinicalnlp-1</url>
      <venue>clinicalnlp</venue>
    </meta>
    <frontmatter>
      <url hash="2042000e">2023.clinicalnlp-1.0</url>
      <bibkey>clinicalnlp-2023-clinical</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Clinical <fixed-case>BERTS</fixed-case>core: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings</title>
      <author><first>Joel</first><last>Shor</last><affiliation>Google</affiliation></author>
      <author><first>Ruyue Agnes</first><last>Bi</last></author>
      <author><first>Subhashini</first><last>Venugopalan</last><affiliation>Google</affiliation></author>
      <author><first>Steven</first><last>Ibara</last><affiliation>Cornell University</affiliation></author>
      <author><first>Roman</first><last>Goldenberg</last></author>
      <author><first>Ehud</first><last>Rivlin</last><affiliation>Technion, Technion</affiliation></author>
      <pages>1-7</pages>
      <abstract>Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We collect a benchmark of 18 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP) and make it publicly available for the community to further develop clinically-aware ASR metrics. To our knowledge, this is the first public dataset of its kind. We demonstrate that our metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins.</abstract>
      <url hash="7038a157">2023.clinicalnlp-1.1</url>
      <bibkey>shor-etal-2023-clinical</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Medical Visual Textual Entailment for Numerical Understanding of Vision-and-Language Models</title>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo and RIKEN</affiliation></author>
      <author><first>Yuta</first><last>Nakamura</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yuki</first><last>Chida</last></author>
      <author><first>Tomoya</first><last>Kurosawa</last></author>
      <pages>8-18</pages>
      <abstract>Assessing the capacity of numerical understanding of vision-and-language models over images and texts is crucial for real vision-and-language applications, such as systems for automated medical image analysis. We provide a visual reasoning dataset focusing on numerical understanding in the medical domain. The experiments using our dataset show that current vision-and-language models fail to perform numerical inference in the medical domain. However, the data augmentation with only a small amount of our dataset improves the model performance, while maintaining the performance in the general domain.</abstract>
      <url hash="c8a14e7b">2023.clinicalnlp-1.2</url>
      <bibkey>yanaka-etal-2023-medical</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Privacy-Preserving Knowledge Transfer through Partial Parameter Sharing</title>
      <author><first>Paul</first><last>Youssef</last><affiliation>Phillips-Universität Marburg</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <pages>19-23</pages>
      <abstract>Valuable datasets that contain sensitive information are not shared due to privacy and copyright concerns. This hinders progress in many areas and prevents the use of machine learning solutions to solve relevant tasks. One possible solution is sharing models that are trained on such datasets. However, this is also associated with potential privacy risks due to data extraction attacks. In this work, we propose a solution based on sharing parts of the model’s parameters, and using a proxy dataset for complimentary knowledge transfer. Our experiments show encouraging results, and reduced risk to potential training data identification attacks. We present a viable solution to sharing knowledge with data-disadvantaged parties, that do not have the resources to produce high-quality data, with reduced privacy risks to the sharing parties. We make our code publicly available.</abstract>
      <url hash="82179706">2023.clinicalnlp-1.3</url>
      <bibkey>youssef-etal-2023-privacy</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Breaking Barriers: Exploring the Diagnostic Potential of Speech Narratives in <fixed-case>H</fixed-case>indi for <fixed-case>A</fixed-case>lzheimer’s Disease</title>
      <author><first>Kritesh</first><last>Rauniyar</last></author>
      <author><first>Shuvam</first><last>Shiwakoti</last></author>
      <author><first>Sweta</first><last>Poudel</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <author><first>Mehwish</first><last>Nasim</last><affiliation>University of Western Australia and Flinders University of South Australia</affiliation></author>
      <pages>24-30</pages>
      <abstract>Alzheimer’s Disease (AD) is a neurodegenerative disorder that affects cognitive abilities and memory, especially in older adults. One of the challenges of AD is that it can be difficult to diagnose in its early stages. However, recent research has shown that changes in language, including speech decline and difficulty in processing information, can be important indicators of AD and may help with early detection. Hence, the speech narratives of the patients can be useful in diagnosing the early stages of Alzheimer’s disease. While the previous works have presented the potential of using speech narratives to diagnose AD in high-resource languages, this work explores the possibility of using a low-resourced language, i.e., Hindi language, to diagnose AD. In this paper, we present a dataset specifically for analyzing AD in the Hindi language, along with experimental results using various state-of-the-art algorithms to assess the diagnostic potential of speech narratives in Hindi. Our analysis suggests that speech narratives in the Hindi language have the potential to aid in the diagnosis of AD. Our dataset and code are made publicly available at <url>https://github.com/rkritesh210/DementiaBankHindi</url>.</abstract>
      <url hash="f1c2c3f4">2023.clinicalnlp-1.4</url>
      <bibkey>rauniyar-etal-2023-breaking</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.4</doi>
      <video href="2023.clinicalnlp-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning</title>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Gleb</first><last>Erofeev</last></author>
      <author><first>Irina</first><last>Sorokina</last></author>
      <author><first>Serge</first><last>Gladkoff</last><affiliation>Logrus Global AI Lab</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>31-40</pages>
      <abstract>Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI’s MMPLMs “wmt21-dense-24-wide-en-X and X-en (WMT21fb)” which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-<i>Spanish</i> language pair which <i>did not exist at all</i> in their original pre-trained corpora both implicitly and explicitly.We prepare carefully aligned <i>clinical</i> domain data for this fine-tuning, which is different from their original mixed domain knowledge.Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training.To the best of our knowledge, this is the first work on using MMPLMs towards <i>clinical domain transfer-learning NMT</i> successfully for totally unseen languages during pre-training.</abstract>
      <url hash="73fa3c4a">2023.clinicalnlp-1.5</url>
      <bibkey>han-etal-2023-investigating</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Tracking the Evolution of Covid-19 Symptoms through Clinical Conversations</title>
      <author><first>Ticiana</first><last>Coelho Da Silva</last><affiliation>Universidade Federal do Ceará</affiliation></author>
      <author><first>José</first><last>Fernandes De Macêdo</last></author>
      <author><first>Régis</first><last>Magalhães</last><affiliation>Universidade Federal do Ceará</affiliation></author>
      <pages>41-47</pages>
      <abstract>The Coronavirus pandemic has heightened the demand for technological solutions capable of gathering and monitoring data automatically, quickly, and securely. To achieve this need, the Plantão Coronavirus chatbot has been made available to the population of Ceará State in Brazil. This chatbot employs automated symptom detection technology through Natural Language Processing (NLP). The proposal of this work is a symptom tracker, which is a neural network that processes texts and captures symptoms in messages exchanged between citizens of the state and the Plantão Coronavirus nurse/doctor, i.e., clinical conversations. The model has the ability to recognize new patterns and has identified a high incidence of altered psychological behaviors, including anguish, anxiety, and sadness, among users who tested positive or negative for Covid-19. As a result, the tool has emphasized the importance of expanding coverage through community mental health services in the state.</abstract>
      <url hash="26e5d2b5">2023.clinicalnlp-1.6</url>
      <bibkey>coelho-da-silva-etal-2023-tracking</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement Learning</title>
      <author><first>Xiangru</first><last>Tang</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mark</first><last>Gerstein</last><affiliation>Yale University</affiliation></author>
      <pages>48-58</pages>
      <abstract>In the rapidly evolving landscape of medical research, accurate and concise summarization of clinical studies is crucial to support evidence-based practice. This paper presents a novel approach to clinical studies summarization, leveraging reinforcement learning to enhance factual consistency and align with human annotator preferences. Our work focuses on two tasks: Conclusion Generation and Review Generation. We train a CONFIT summarization model that outperforms GPT-3 and previous state-of-the-art models on the same datasets and collects expert and crowd-worker annotations to evaluate the quality and factual consistency of the generated summaries. These annotations enable us to measure the correlation of various automatic metrics, including modern factual evaluation metrics like QAFactEval, with human-assessed factual consistency. By employing top-correlated metrics as objectives for a reinforcement learning model, we demonstrate improved factuality in generated summaries that are preferred by human annotators.</abstract>
      <url hash="5ac51316">2023.clinicalnlp-1.7</url>
      <bibkey>tang-etal-2023-aligning</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Navigating Data Scarcity: Pretraining for Medical Utterance Classification</title>
      <author><first>Do June</first><last>Min</last></author>
      <author><first>Veronica</first><last>Perez-Rosas</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>59-68</pages>
      <abstract>Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data management. In this paper, we study the effectiveness of dialog-aware pretraining objectives and multiphase training in using unlabeled data to improve LMs training for medical utterance classification. The objectives of pretraining for dialog awareness involve tasks that take into account the structure of conversations, including features such as turn-taking and the roles of speakers. The multiphase training process uses unannotated data in a sequence that prioritizes similarities and connections between different domains. We empirically evaluate these methods on conversational dialog classification tasks in the medical and counseling domains, and find that multiphase training can help achieve higher performance than standard pretraining or finetuning.</abstract>
      <url hash="5471d623">2023.clinicalnlp-1.8</url>
      <bibkey>min-etal-2023-navigating</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.8</doi>
      <video href="2023.clinicalnlp-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title><fixed-case>H</fixed-case>indi Chatbot for Supporting Maternal and Child Health Related Queries in Rural <fixed-case>I</fixed-case>ndia</title>
      <author><first>Ritwik</first><last>Mishra</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Simranjeet</first><last>Singh</last></author>
      <author><first>Jasmeet</first><last>Kaur</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Pushpendra</first><last>Singh</last></author>
      <author><first>Rajiv</first><last>Shah</last></author>
      <pages>69-77</pages>
      <abstract>In developing countries like India, doctors and healthcare professionals working in public health spend significant time answering health queries that are fact-based and repetitive. Therefore, we propose an automated way to answer maternal and child health-related queries. A database of Frequently Asked Questions (FAQs) and their corresponding answers generated by experts is curated from rural health workers and young mothers. We develop a Hindi chatbot that identifies k relevant Question and Answer (QnA) pairs from the database in response to a healthcare query (q) written in Devnagri script or Hindi-English (Hinglish) code-mixed script. The curated database covers 80% of all the queries that a user of our study is likely to ask. We experimented with (i) rule-based methods, (ii) sentence embeddings, and (iii) a paraphrasing classifier, to calculate the q-Q similarity. We observed that paraphrasing classifier gives the best result when trained first on an open-domain text and then on the healthcare domain. Our chatbot uses an ensemble of all three approaches. We observed that if a given q can be answered using the database, then our chatbot can provide at least one relevant QnA pair among its top three suggestions for up to 70% of the queries.</abstract>
      <url hash="567e350f">2023.clinicalnlp-1.9</url>
      <bibkey>mishra-etal-2023-hindi</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.9</doi>
      <video href="2023.clinicalnlp-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning</title>
      <author><first>Brihat</first><last>Sharma</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Yanjun</first><last>Gao</last></author>
      <author><first>Timothy</first><last>Miller</last><affiliation>Harvard University</affiliation></author>
      <author><first>Matthew</first><last>Churpek</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Majid</first><last>Afshar</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Dmitriy</first><last>Dligach</last><affiliation>Loyola University Chicago</affiliation></author>
      <pages>78-85</pages>
      <abstract>Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH. We demonstrate that a multi-task, clinically-trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.</abstract>
      <url hash="9fcf9727">2023.clinicalnlp-1.10</url>
      <bibkey>sharma-etal-2023-multi</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.10</doi>
      <video href="2023.clinicalnlp-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Context-aware Medication Event Extraction from Unstructured Text</title>
      <author><first>Noushin</first><last>Salek Faramarzi</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Meet</first><last>Patel</last></author>
      <author><first>Sai Harika</first><last>Bandarupally</last></author>
      <author><first>Ritwik</first><last>Banerjee</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>86-95</pages>
      <abstract>Accurately capturing medication history is crucial in delivering high-quality medical care. The extraction of medication events from unstructured clinical notes, however, is challenging because the information is presented in complex narratives. We address this challenge by leveraging the newly released Contextualized Medication Event Dataset (CMED) as part of our participation in the 2022 National NLP Clinical Challenges (n2c2) shared task. Our study evaluates the performance of various pretrained language models in this task. Further, we find that data augmentation coupled with domain-specific training provides notable improvements. With experiments, we also underscore the importance of careful data preprocessing in medical event detection.</abstract>
      <url hash="d12f38b6">2023.clinicalnlp-1.11</url>
      <bibkey>salek-faramarzi-etal-2023-context</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Improving Automatic <fixed-case>KCD</fixed-case> Coding: Introducing the <fixed-case>K</fixed-case>o<fixed-case>DAK</fixed-case> and an Optimized Tokenization Method for <fixed-case>K</fixed-case>orean Clinical Documents</title>
      <author><first>Geunyeong</first><last>Jeong</last><affiliation>Konkuk University</affiliation></author>
      <author><first>Juoh</first><last>Sun</last></author>
      <author><first>Seokwon</first><last>Jeong</last><affiliation>Kangwon National University</affiliation></author>
      <author><first>Hyunjin</first><last>Shin</last></author>
      <author><first>Harksoo</first><last>Kim</last><affiliation>Konkuk University</affiliation></author>
      <pages>96-101</pages>
      <abstract>International Classification of Diseases (ICD) coding is the task of assigning a patient’s electronic health records into standardized codes, which is crucial for enhancing medical services and reducing healthcare costs. In Korea, automatic Korean Standard Classification of Diseases (KCD) coding has been hindered by limited resources, differences in ICD systems, and language-specific characteristics. Therefore, we construct the Korean Dataset for Automatic KCD coding (KoDAK) by collecting and preprocessing Korean clinical documents. In addition, we propose a tokenization method optimized for Korean clinical documents. Our experiments show that our proposed method outperforms Korean Medical BERT (KM-BERT) in Macro-F1 performance by 0.14%p while using fewer model parameters, demonstrating its effectiveness in Korean clinical documents.</abstract>
      <url hash="4e4af009">2023.clinicalnlp-1.12</url>
      <bibkey>jeong-etal-2023-improving</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Who needs context? Classical techniques for <fixed-case>A</fixed-case>lzheimer’s disease detection</title>
      <author><first>Behrad</first><last>Taghibeyglou</last></author>
      <author><first>Frank</first><last>Rudzicz</last><affiliation>Dalhousie University</affiliation></author>
      <pages>102-107</pages>
      <abstract>Natural language processing (NLP) has shown great potential for Alzheimer’s disease (AD) detection, particularly due to the adverse effect of AD on spontaneous speech. The current body of literature has directed attention toward context-based models, especially Bidirectional Encoder Representations from Transformers (BERTs), owing to their exceptional abilities to integrate contextual information in a wide range of NLP tasks. This comes at the cost of added model opacity and computational requirements. Taking this into consideration, we propose a Word2Vec-based model for AD detection in 108 age- and sex-matched participants who were asked to describe the Cookie Theft picture. We also investigate the effectiveness of our model by fine-tuning BERT-based sequence classification models, as well as incorporating linguistic features. Our results demonstrate that our lightweight and easy-to-implement model outperforms some of the state-of-the-art models available in the literature, as well as BERT models.</abstract>
      <url hash="669f4607">2023.clinicalnlp-1.13</url>
      <bibkey>taghibeyglou-rudzicz-2023-needs</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Knowledge Injection for Disease Names in Logical Inference between <fixed-case>J</fixed-case>apanese Clinical Texts</title>
      <author><first>Natsuki</first><last>Murakami</last><affiliation>Ochanomizu Women’s University</affiliation></author>
      <author><first>Mana</first><last>Ishida</last></author>
      <author><first>Yuta</first><last>Takahashi</last><affiliation>Ochanomizu Women’s University</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo and RIKEN</affiliation></author>
      <author><first>Daisuke</first><last>Bekki</last><affiliation>Ochanomizu University</affiliation></author>
      <pages>108-117</pages>
      <abstract>In the medical field, there are many clinical texts such as electronic medical records, and research on Japanese natural language processing using these texts has been conducted. One such research involves Recognizing Textual Entailment (RTE) in clinical texts using a semantic analysis and logical inference system, ccg2lambda. However, it is difficult for existing inference systems to correctly determine the entailment relations , if the input sentence contains medical domain specific paraphrases such as disease names. In this study, we propose a method to supplement the equivalence relations of disease names as axioms by identifying candidates for paraphrases that lack in theorem proving. Candidates of paraphrases are identified by using a model for the NER task for disease names and a disease name dictionary. We also construct an inference test set that requires knowledge injection of disease names and evaluate our inference system. Experiments showed that our inference system was able to correctly infer for 106 out of 149 inference test sets.</abstract>
      <url hash="9431570a">2023.clinicalnlp-1.14</url>
      <bibkey>murakami-etal-2023-knowledge</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Training Models on Oversampled Data and a Novel Multi-class Annotation Scheme for Dementia Detection</title>
      <author><first>Nadine</first><last>Abdelhalim</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Ingy</first><last>Abdelhalim</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>118-124</pages>
      <abstract>This work introduces a novel three-class annotation scheme for text-based dementia classification in patients, based on their recorded visit interactions. Multiple models were developed utilising BERT, RoBERTa and DistilBERT. Two approaches were employed to improve the representation of dementia samples: oversampling the underrepresented data points in the original Pitt dataset and combining the Pitt with the Holland and Kempler datasets. The DistilBERT models trained on either an oversampled Pitt dataset or the combined dataset performed best in classifying the dementia class. Specifically, the model trained on the oversampled Pitt dataset and the one trained on the combined dataset obtained state-of-the-art performance with 98.8% overall accuracy and 98.6% macro-averaged F1-score, respectively. The models’ outputs were manually inspected through saliency highlighting, using Local Interpretable Model-agnostic Explanations (LIME), to provide a better understanding of its predictions.</abstract>
      <url hash="c95c3a58">2023.clinicalnlp-1.15</url>
      <bibkey>abdelhalim-etal-2023-training</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Improving the Transferability of Clinical Note Section Classification Models with <fixed-case>BERT</fixed-case> and Large Language Model Ensembles</title>
      <author><first>Weipeng</first><last>Zhou</last></author>
      <author><first>Majid</first><last>Afshar</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Dmitriy</first><last>Dligach</last><affiliation>Loyola University Chicago</affiliation></author>
      <author><first>Yanjun</first><last>Gao</last></author>
      <author><first>Timothy</first><last>Miller</last><affiliation>Harvard University</affiliation></author>
      <pages>125-130</pages>
      <abstract>Text in electronic health records is organized into sections, and classifying those sections into section categories is useful for downstream tasks. In this work, we attempt to improve the transferability of section classification models by combining the dataset-specific knowledge in supervised learning models with the world knowledge inside large language models (LLMs). Surprisingly, we find that zero-shot LLMs out-perform supervised BERT-based models applied to out-of-domain data. We also find that their strengths are synergistic, so that a simple ensemble technique leads to additional performance gains.</abstract>
      <url hash="a4d99d09">2023.clinicalnlp-1.16</url>
      <bibkey>zhou-etal-2023-improving-transferability</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Can Large Language Models Safely Address Patient Questions Following Cataract Surgery?</title>
      <author><first>Mohita</first><last>Chowdhury</last></author>
      <author><first>Ernest</first><last>Lim</last></author>
      <author><first>Aisling</first><last>Higham</last></author>
      <author><first>Rory</first><last>McKinnon</last></author>
      <author><first>Nikoletta</first><last>Ventoura</last></author>
      <author><first>Yajie</first><last>He</last></author>
      <author><first>Nick</first><last>De Pennington</last></author>
      <pages>131-137</pages>
      <abstract>Recent advances in large language models (LLMs) have generated significant interest in their application across various domains including healthcare. However, there is limited data on their safety and performance in real-world scenarios. This study uses data collected using an autonomous telemedicine clinical assistant. The assistant asks symptom-based questions to elicit patient concerns and allows patients to ask questions about their post-operative recovery. We utilise real-world postoperative questions posed to the assistant by a cohort of 120 patients to examine the safety and appropriateness of responses generated by a recent popular LLM by OpenAI, ChatGPT. We demonstrate that LLMs have the potential to helpfully address routine patient queries following routine surgery. However, important limitations around the safety of today’s models exist which must be considered.</abstract>
      <url hash="8f6d7324">2023.clinicalnlp-1.17</url>
      <bibkey>chowdhury-etal-2023-large</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Large Scale Sequence-to-Sequence Models for Clinical Note Generation from Patient-Doctor Conversations</title>
      <author><first>Gagandeep</first><last>Singh</last><affiliation>Nuance Communications</affiliation></author>
      <author><first>Yue</first><last>Pan</last><affiliation>Nuance Communications</affiliation></author>
      <author><first>Jesus</first><last>Andres-Ferrer</last></author>
      <author><first>Miguel</first><last>Del-Agua</last><affiliation>Nuance Communications</affiliation></author>
      <author><first>Frank</first><last>Diehl</last><affiliation>Nuance Communications</affiliation></author>
      <author><first>Joel</first><last>Pinto</last></author>
      <author><first>Paul</first><last>Vozila</last><affiliation>Nuance Communications</affiliation></author>
      <pages>138-143</pages>
      <abstract>We present our work on building large scale sequence-to-sequence models for generating clinical note from patient-doctor conversation. This is formulated as an abstractive summarization task for which we use encoder-decoder transformer model with pointer-generator. We discuss various modeling enhancements to this baseline model which include using subword and multiword tokenization scheme, prefixing the targets with a chain-of-clinical-facts, and training with contrastive loss that is defined over various candidate summaries. We also use flash attention during training and query chunked attention during inference to be able to process long input and output sequences and to improve computational efficiency. Experiments are conducted on a dataset containing about 900K encounters from around 1800 healthcare providers covering 27 specialties. The results are broken down into primary care and non-primary care specialties. Consistent accuracy improvements are observed across both of these categories.</abstract>
      <url hash="97f6c362">2023.clinicalnlp-1.18</url>
      <bibkey>singh-etal-2023-large</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.18</doi>
      <video href="2023.clinicalnlp-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title>clulab at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Summarization and classification of medical dialogues</title>
      <author><first>Kadir Bulut</first><last>Ozler</last></author>
      <author><first>Steven</first><last>Bethard</last><affiliation>University of Arizona</affiliation></author>
      <pages>144-149</pages>
      <abstract>Clinical Natural Language Processing has been an increasingly popular research area in the NLP community. With the rise of large language models (LLMs) and their impressive abilities in NLP tasks, it is crucial to pay attention to their clinical applications. Sequence to sequence generative approaches with LLMs have been widely used in recent years. To be a part of the research in clinical NLP with recent advances in the field, we participated in task A of MEDIQA-Chat at ACL-ClinicalNLP Workshop 2023. In this paper, we explain our methods and findings as well as our comments on our results and limitations.</abstract>
      <url hash="e2e71c47">2023.clinicalnlp-1.19</url>
      <bibkey>ozler-bethard-2023-clulab</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Leveraging Natural Language Processing and Clinical Notes for Dementia Detection</title>
      <author><first>Ming</first><last>Liu</last><affiliation>Deakin University</affiliation></author>
      <author><first>Richard</first><last>Beare</last><affiliation>NA</affiliation></author>
      <author><first>Taya</first><last>Collyer</last><affiliation>NA</affiliation></author>
      <author><first>Nadine</first><last>Andrew</last><affiliation>NA</affiliation></author>
      <author><first>Velandai</first><last>Srikanth</last><affiliation>NA</affiliation></author>
      <pages>150-155</pages>
      <abstract>Early detection and automated classification of dementia has recently gained considerable attention using neuroimaging data and spontaneous speech. In this paper, we validate the possibility of dementia detection with in-hospital clinical notes. We collected 954 patients’ clinical notes from a local hospital and assign dementia/non-dementia labels to those patients based on clinical assessment and telephone interview. Given the labeled dementia data sets, we fine tune a ClinicalBioBERT based on some filtered clinical notes and conducted experiments on both binary and three class dementia classification. Our experiment results show that the fine tuned ClinicalBioBERT achieved satisfied performance on binary classification but failed on three class dementia classification. Further analysis suggests that more human prior knowledge should be considered.</abstract>
      <url hash="4f957996">2023.clinicalnlp-1.20</url>
      <bibkey>liu-etal-2023-leveraging</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>Automated Orthodontic Diagnosis from a Summary of Medical Findings</title>
      <author><first>Takumi</first><last>Ohtsuka</last><affiliation>Ehime University</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Chihiro</first><last>Tanikawa</last><affiliation>NA</affiliation></author>
      <author><first>Yuujin</first><last>Shimizu</last><affiliation>NA</affiliation></author>
      <author><first>Hajime</first><last>Nagahara</last><affiliation>Osaka University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <pages>156-160</pages>
      <abstract>We propose a method to automate orthodontic diagnosis with natural language processing. It is worthwhile to assist dentists with such technology to prevent errors by inexperienced dentists and to reduce the workload of experienced ones. However, text length and style inconsistencies in medical findings make an automated orthodontic diagnosis with deep-learning models difficult. In this study, we improve the performance of automatic diagnosis utilizing short summaries of medical findings written in a consistent style by experienced dentists. Experimental results on 970 Japanese medical findings show that summarization consistently improves the performance of various machine learning models for automated orthodontic diagnosis. Although BERT is the model that gains the most performance with the proposed method, the convolutional neural network achieved the best performance.</abstract>
      <url hash="d4a315c7">2023.clinicalnlp-1.21</url>
      <bibkey>ohtsuka-etal-2023-automated</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Harnessing the Power of <fixed-case>BERT</fixed-case> in the <fixed-case>T</fixed-case>urkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios</title>
      <author><first>Hazal</first><last>Türkmen</last></author>
      <author><first>Oguz</first><last>Dikenelli</last><affiliation>Ege University</affiliation></author>
      <author><first>Cenk</first><last>Eraslan</last></author>
      <author><first>Mehmet</first><last>Calli</last><affiliation>NA</affiliation></author>
      <author><first>Suha</first><last>Ozbek</last></author>
      <pages>161-170</pages>
      <abstract>Recent advancements in natural language processing (NLP) have been driven by large language models (LLMs), thereby revolutionizing the field. Our study investigates the impact of diverse pre-training strategies on the performance of Turkish clinical language models in a multi-label classification task involving radiology reports, with a focus on overcoming language resource limitations. Additionally, for the first time, we evaluated the simultaneous pre-training approach by utilizing limited clinical task data. We developed four models: TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our results revealed superior performance from BERTurk and TurkRadBERT-task v1, both of which leverage a broad general-domain corpus. Although task-adaptive pre-training is capable of identifying domain-specific patterns, it may be prone to overfitting because of the constraints of the task-specific corpus. Our findings highlight the importance of domain-specific vocabulary during pre-training to improve performance. They also affirmed that a combination of general domain knowledge and task-specific fine-tuning is crucial for optimal performance across various categories. This study offers key insights for future research on pre-training techniques in the clinical domain, particularly for low-resource languages.</abstract>
      <url hash="93acd9fd">2023.clinicalnlp-1.22</url>
      <bibkey>turkmen-etal-2023-harnessing</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.22</doi>
      <video href="2023.clinicalnlp-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>A Meta-dataset of <fixed-case>G</fixed-case>erman Medical Corpora: Harmonization of Annotations and Cross-corpus <fixed-case>NER</fixed-case> Evaluation</title>
      <author><first>Ignacio</first><last>Llorca</last></author>
      <author><first>Florian</first><last>Borchert</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Matthieu-P.</first><last>Schapranow</last></author>
      <pages>171-181</pages>
      <abstract>Over the last years, an increasing number of publicly available, semantically annotated medical corpora have been released for the German language. While their annotations cover comparable semantic classes, the synergies of such efforts have not been explored, yet. This is due to substantial differences in the data schemas (syntax) and annotated entities (semantics), which hinder the creation of common meta-datasets. For instance, it is unclear whether named entity recognition (NER) taggers trained on one or more of such datasets are useful to detect entities in any of the other datasets. In this work, we create harmonized versions of German medical corpora using the BigBIO framework, and make them available to the community. Using these as a meta-dataset, we perform a series of cross-corpus evaluation experiments on two settings of aligned labels. These consist in fine-tuning various pre-trained Transformers on different combinations of training sets, and testing them against each dataset separately. We find that a) trained NER models generalize poorly, with F1 scores dropping approx. 20 pp. on unseen test data, and b) current pre-trained Transformer models for the German language do not systematically alleviate this issue. However, our results suggest that models benefit from additional training corpora in most cases, even if these belong to different medical fields or text genres.</abstract>
      <url hash="35cde92e">2023.clinicalnlp-1.23</url>
      <bibkey>llorca-etal-2023-meta</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Uncovering the Potential for a Weakly Supervised End-to-End Model in Recognising Speech from Patient with Post-Stroke Aphasia</title>
      <author><first>Giulia</first><last>Sanguedolce</last></author>
      <author><first>Patrick A.</first><last>Naylor</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Fatemeh</first><last>Geranmayeh</last><affiliation>Imperial College London</affiliation></author>
      <pages>182-190</pages>
      <abstract>Post-stroke speech and language deficits (aphasia) significantly impact patients’ quality of life. Many with mild symptoms remain undiagnosed, and the majority do not receive the intensive doses of therapy recommended, due to healthcare costs and/or inadequate services. Automatic Speech Recognition (ASR) may help overcome these difficulties by improving diagnostic rates and providing feedback during tailored therapy. However, its performance is often unsatisfactory due to the high variability in speech errors and scarcity of training datasets. This study assessed the performance of Whisper, a recently released end-to-end model, in patients with post-stroke aphasia (PWA). We tuned its hyperparameters to achieve the lowest word error rate (WER) on aphasic speech. WER was significantly higher in PWA compared to age-matched controls (10.3% vs 38.5%, <tex-math>p&lt;0.001</tex-math>). We demonstrated that worse WER was related to the more severe aphasia as measured by expressive (overt naming, and spontaneous speech production) and receptive (written and spoken comprehension) language assessments. Stroke lesion size did not affect the performance of Whisper. Linear mixed models accounting for demographic factors, therapy duration, and time since stroke, confirmed worse Whisper performance with left hemispheric frontal lesions.We discuss the implications of these findings for how future ASR can be improved in PWA.</abstract>
      <url hash="67738d64">2023.clinicalnlp-1.24</url>
      <bibkey>sanguedolce-etal-2023-uncovering</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.24</doi>
      <video href="2023.clinicalnlp-1.24.mp4"/>
    </paper>
    <paper id="25">
      <title>Textual Entailment for Temporal Dependency Graph Parsing</title>
      <author><first>Jiarui</first><last>Yao</last></author>
      <author><first>Steven</first><last>Bethard</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Kristin</first><last>Wright-Bettner</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Eli</first><last>Goldner</last></author>
      <author><first>David</first><last>Harris</last></author>
      <author><first>Guergana</first><last>Savova</last><affiliation>Harvard University</affiliation></author>
      <pages>191-199</pages>
      <abstract>We explore temporal dependency graph (TDG) parsing in the clinical domain. We leverage existing annotations on the THYME dataset to semi-automatically construct a TDG corpus. Then we propose a new natural language inference (NLI) approach to TDG parsing, and evaluate it both on general domain TDGs from wikinews and the newly constructed clinical TDG corpus. We achieve competitive performance on general domain TDGs with a much simpler model than prior work. On the clinical TDGs, our method establishes the first result of TDG parsing on clinical data with 0.79/0.88 micro/macro F1.</abstract>
      <url hash="9fc00e6e">2023.clinicalnlp-1.25</url>
      <bibkey>yao-etal-2023-textual</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models</title>
      <author><first>Varun</first><last>Nair</last><affiliation>Curai Health</affiliation></author>
      <author><first>Elliot</first><last>Schumacher</last><affiliation>Curai Health and Johns Hopkins University</affiliation></author>
      <author><first>Anitha</first><last>Kannan</last><affiliation>Curai Health</affiliation></author>
      <pages>200-217</pages>
      <abstract>A medical provider’s summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing “patient does not have a fever” when a fever is present) can be detrimental to the outcome of care for the patient. This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively. Both our human evaluation study and metrics for medical correctness show that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting.</abstract>
      <url hash="13dc10dd">2023.clinicalnlp-1.26</url>
      <bibkey>nair-etal-2023-generating</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.26</doi>
    </paper>
    <paper id="27">
      <title>Factors Affecting the Performance of Automated Speaker Verification in <fixed-case>A</fixed-case>lzheimer’s Disease Clinical Trials</title>
      <author><first>Malikeh</first><last>Ehghaghi</last></author>
      <author><first>Marija</first><last>Stanojevic</last><affiliation>WinterLightLabs and Temple University</affiliation></author>
      <author><first>Ali</first><last>Akram</last></author>
      <author><first>Jekaterina</first><last>Novikova</last><affiliation>Winterlight Labs</affiliation></author>
      <pages>218-227</pages>
      <abstract>Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial’s findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer’s disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2) degrades for individuals who are above 70 years old; 3) is comparatively better for non-native English speakers than for native English speakers; 4) is negatively affected by clinician interference, noisy background, and unclear participant speech; 5) tends to decrease with an increase in the severity level of AD. Our study finds that voice biometrics raise fairness concerns as certain subgroups exhibit different ASV performances owing to their inherent voice characteristics. Moreover, the performance of ASV is influenced by the quality of speech recordings, which underscores the importance of improving the data collection settings in clinical trials.</abstract>
      <url hash="80e3fd97">2023.clinicalnlp-1.27</url>
      <bibkey>ehghaghi-etal-2023-factors</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Team Cadence at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Generating, augmenting and summarizing clinical dialogue with large language models</title>
      <author><first>Ashwyn</first><last>Sharma</last><affiliation>Cadence Solutions</affiliation></author>
      <author><first>David</first><last>Feldman</last></author>
      <author><first>Aneesh</first><last>Jain</last></author>
      <pages>228-235</pages>
      <abstract>This paper describes Team Cadence’s winning submission to Task C of the MEDIQA-Chat 2023 shared tasks. We also present the set of methods, including a novel N-pass strategy to summarize a mix of clinical dialogue and an incomplete summarized note, used to complete Task A and Task B, ranking highly on the leaderboard amongst stable and reproducible code submissions. The shared tasks invited participants to summarize, classify and generate patient-doctor conversations. Considering the small volume of training data available, we took a data-augmentation-first approach to the three tasks by focusing on the dialogue generation task, i.e., Task C. It proved effective in improving our models’ performance on Task A and Task B. We also found the BART architecture to be highly versatile, as it formed the base for all our submissions. Finally, based on the results shared by the organizers, we note that Team Cadence was the only team to submit stable and reproducible runs to all three tasks.</abstract>
      <url hash="8412223e">2023.clinicalnlp-1.28</url>
      <bibkey>sharma-etal-2023-team</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.28</doi>
    </paper>
    <paper id="29">
      <title>Method for Designing Semantic Annotation of Sepsis Signs in Clinical Text</title>
      <author><first>Melissa</first><last>Yan</last><affiliation>Norwegian University of Science and Technology</affiliation></author>
      <author><first>Lise</first><last>Gustad</last><affiliation>Nord University and Norwegian University of Science and Technology</affiliation></author>
      <author><first>Lise</first><last>Høvik</last></author>
      <author><first>Øystein</first><last>Nytrø</last><affiliation>Norwegian University of Science and Technology</affiliation></author>
      <pages>236-246</pages>
      <abstract>Annotated clinical text corpora are essential for machine learning studies that model and predict care processes and disease progression. However, few studies describe the necessary experimental design of the annotation guideline and annotation phases. This makes replication, reuse, and adoption challenging. Using clinical questions about sepsis, we designed a semantic annotation guideline to capture sepsis signs from clinical text. The clinical questions aid guideline design, application, and evaluation. Our method incrementally evaluates each change in the guideline by testing the resulting annotated corpus using clinical questions. Additionally, our method uses inter-annotator agreement to judge the annotator compliance and quality of the guideline. We show that the method, combined with controlled design increments, is simple and allows the development and measurable improvement of a purpose-built semantic annotation guideline. We believe that our approach is useful for incremental design of semantic annotation guidelines in general.</abstract>
      <url hash="321c42b8">2023.clinicalnlp-1.29</url>
      <bibkey>yan-etal-2023-method</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.29</doi>
      <video href="2023.clinicalnlp-1.29.mp4"/>
    </paper>
    <paper id="30">
      <title>Prompt Discriminative Language Models for Domain Adaptation</title>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Peter</first><last>Potash</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xihui</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yuwen</first><last>Sun</last></author>
      <author><first>Zihan</first><last>Qian</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tristan</first><last>Naumann</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Tianxi</first><last>Cai</last><affiliation>Harvard T.H. Chan School of Public Health</affiliation></author>
      <author><first>Junwei</first><last>Lu</last><affiliation>Harvard University</affiliation></author>
      <pages>247-258</pages>
      <abstract>Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. However, the potential of discriminative language models in biomedical tasks remains underexplored.To bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. BioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining. In this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.</abstract>
      <url hash="ae20bb11">2023.clinicalnlp-1.30</url>
      <bibkey>lu-etal-2023-prompt</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.30</doi>
    </paper>
    <paper id="31">
      <title>Cross-domain <fixed-case>G</fixed-case>erman Medical Named Entity Recognition using a Pre-Trained Language Model and Unified Medical Semantic Types</title>
      <author><first>Siting</first><last>Liang</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Mareike</first><last>Hartmann</last></author>
      <author><first>Daniel</first><last>Sonntag</last><affiliation>German Research Center for AI and Carl von Ossietzky Universität Oldenburg</affiliation></author>
      <pages>259-271</pages>
      <abstract>Information extraction from clinical text has the potential to facilitate clinical research and personalized clinical care, but annotating large amounts of data for each set of target tasks is prohibitive. We present a German medical Named Entity Recognition (NER) system capable of cross-domain knowledge transferring. The system builds on a pre-trained German language model and a token-level binary classifier, employing semantic types sourced from the Unified Medical Language System (UMLS) as entity labels to identify corresponding entity spans within the input text. To enhance the system’s performance and robustness, we pre-train it using a medical literature corpus that incorporates UMLS semantic term annotations. We evaluate the system’s effectiveness on two German annotated datasets obtained from different clinics in zero- and few-shot settings. The results show that our approach outperforms task-specific Condition Random Fields (CRF) classifiers in terms of accuracy. Our work contributes to developing robust and transparent German medical NER models that can support the extraction of information from various clinical texts.</abstract>
      <url hash="9330b0f7">2023.clinicalnlp-1.31</url>
      <bibkey>liang-etal-2023-cross</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.31</doi>
    </paper>
    <paper id="32">
      <title>Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural Language Processing Applications</title>
      <author><first>Usman</first><last>Naseem</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Liang</first><last>Hu</last><affiliation>Tongji University</affiliation></author>
      <author><first>Anum</first><last>Masood</last></author>
      <author><first>Mehwish</first><last>Nasim</last><affiliation>University of Western Australia and Flinders University of South Australia</affiliation></author>
      <pages>272-277</pages>
      <abstract>Graph-based techniques have gained traction for representing and analyzing data in various natural language processing (NLP) tasks. Knowledge graph-based language representation models have shown promising results in leveraging domain-specific knowledge for NLP tasks, particularly in the biomedical NLP field. However, such models have limitations, including knowledge noise and neglect of contextual relationships, leading to potential semantic errors and reduced accuracy. To address these issues, this paper proposes two novel methods. The first method combines knowledge graph-based language model with nearest-neighbor models to incorporate semantic and category information from neighboring instances. The second method involves integrating knowledge graph-based language model with graph neural networks (GNNs) to leverage feature information from neighboring nodes in the graph. Experiments on relation extraction (RE) and classification tasks in English and Chinese language datasets demonstrate significant performance improvements with both methods, highlighting their potential for enhancing the performance of language models and improving NLP applications in the biomedical domain.</abstract>
      <url hash="1b5e2d03">2023.clinicalnlp-1.32</url>
      <bibkey>naseem-etal-2023-reducing</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.32</doi>
      <video href="2023.clinicalnlp-1.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Medical knowledge-enhanced prompt learning for diagnosis classification from clinical text</title>
      <author><first>Yuxing</first><last>Lu</last></author>
      <author><first>Xukai</first><last>Zhao</last></author>
      <author><first>Jinzhuo</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>278-288</pages>
      <abstract>Artificial intelligence based diagnosis systems have emerged as powerful tools to reform traditional medical care. Each clinician now wants to have his own intelligent diagnostic partner to expand the range of services he can provide. When reading a clinical note, experts make inferences with relevant knowledge. However, medical knowledge appears to be heterogeneous, including structured and unstructured knowledge. Existing approaches are incapable of uniforming them well. Besides, the descriptions of clinical findings in clinical notes, which are reasoned to diagnosis, vary a lot for different diseases or patients. To address these problems, we propose a Medical Knowledge-enhanced Prompt Learning (MedKPL) model for diagnosis classification. First, to overcome the heterogeneity of knowledge, given the knowledge relevant to diagnosis, MedKPL extracts and normalizes the relevant knowledge into a prompt sequence. Then, MedKPL integrates the knowledge prompt with the clinical note into a designed prompt for representation. Therefore, MedKPL can integrate medical knowledge into the models to enhance diagnosis and effectively transfer learned diagnosis capacity to unseen diseases using alternating relevant disease knowledge. The experimental results on two medical datasets show that our method can obtain better medical text classification results and can perform better in transfer and few-shot settings among datasets of different diseases.</abstract>
      <url hash="6a789e88">2023.clinicalnlp-1.33</url>
      <bibkey>lu-etal-2023-medical</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.33</doi>
    </paper>
    <paper id="34">
      <title>Multilingual Clinical <fixed-case>NER</fixed-case>: Translation or Cross-lingual Transfer?</title>
      <author><first>Félix</first><last>Gaschi</last><affiliation>University of Lorraine</affiliation></author>
      <author><first>Xavier</first><last>Fontaine</last></author>
      <author><first>Parisa</first><last>Rastin</last></author>
      <author><first>Yannick</first><last>Toussaint</last><affiliation>Université de Lorraine</affiliation></author>
      <pages>289-311</pages>
      <abstract>Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show that translation-based methods can achieve similar performance to CLT but require more care in their design. And while they can take advantage of monolingual clinical language models, those do not guarantee better results than large general-purpose multilingual models, whether with cross-lingual transfer or translation.</abstract>
      <url hash="4a1440f7">2023.clinicalnlp-1.34</url>
      <bibkey>gaschi-etal-2023-multilingual</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>UMLS</fixed-case>-<fixed-case>KGI</fixed-case>-<fixed-case>BERT</fixed-case>: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition</title>
      <author><first>Aidan</first><last>Mannion</last></author>
      <author><first>Didier</first><last>Schwab</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <author><first>Lorraine</first><last>Goeuriot</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <pages>312-322</pages>
      <abstract>Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS.This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks. All pre-trained models, data processing pipelines and evaluation scripts will be made publicly available.</abstract>
      <url hash="a62e76f7">2023.clinicalnlp-1.35</url>
      <bibkey>mannion-etal-2023-umls</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>W</fixed-case>ang<fixed-case>L</fixed-case>ab at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models</title>
      <author><first>John</first><last>Giorgi</last></author>
      <author><first>Augustin</first><last>Toma</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Ronald</first><last>Xie</last></author>
      <author><first>Sondra</first><last>Chen</last></author>
      <author><first>Kevin</first><last>An</last></author>
      <author><first>Grace</first><last>Zheng</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Bo</first><last>Wang</last><affiliation>Vector Institute</affiliation></author>
      <pages>323-334</pages>
      <abstract>This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.</abstract>
      <url hash="e3c9907f">2023.clinicalnlp-1.36</url>
      <bibkey>giorgi-etal-2023-wanglab</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.36</doi>
      <video href="2023.clinicalnlp-1.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the <fixed-case>C</fixed-case>hilean Public Healthcare System</title>
      <author><first>Fabián</first><last>Villena</last><affiliation>Universidad de Chile</affiliation></author>
      <author><first>Matías</first><last>Rojas</last></author>
      <author><first>Felipe</first><last>Arias</last></author>
      <author><first>Jorge</first><last>Pacheco</last></author>
      <author><first>Paulina</first><last>Vera</last></author>
      <author><first>Jocelyn</first><last>Dunstan</last><affiliation>Universidad de Chile</affiliation></author>
      <pages>335-343</pages>
      <abstract>The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and prevalence of diseases in a determined context. However, the manual coding process is subject to errors as it requires medical personnel to be competent in coding rules and terminology. In addition, this process consumes a lot of time and energy, which could be allocated to more clinically relevant tasks. These difficulties can be addressed by developing computational systems that automatically assign codes to diseases. In this way, we propose a two-step system for automatically coding diseases in referrals from the Chilean public healthcare system. Specifically, our model uses a state-of-the-art NER model for recognizing disease mentions and a search engine system based on Elasticsearch for assigning the most relevant codes associated with these disease mentions. The system’s performance was evaluated on referrals manually coded by clinical experts. Our system obtained a MAP score of 0.63 for the subcategory level and 0.83 for the category level, close to the best-performing models in the literature. This system could be a support tool for health professionals, optimizing the coding and management process. Finally, to guarantee reproducibility, we publicly release the code of our models and experiments.</abstract>
      <url hash="b7d36f33">2023.clinicalnlp-1.37</url>
      <bibkey>villena-etal-2023-automatic</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.37</doi>
    </paper>
    <paper id="38">
      <title>Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts</title>
      <author><first>Sitong</first><last>Zhou</last></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Mari</first><last>Ostendorf</last><affiliation>University of Washington</affiliation></author>
      <pages>344-357</pages>
      <abstract>This paper explores methods for extracting information from radiology reports that generalize across exam modalities to reduce requirements for annotated data. We demonstrate that multi-pass T5-based text-to-text generative models exhibit better generalization across exam modalities compared to approaches that employ BERT-based task-specific classification layers. We then develop methods that reduce the inference cost of the model, making large-scale corpus processing more feasible for clinical applications. Specifically, we introduce a generative technique that decomposes complex tasks into smaller subtask blocks, which improves a single-pass model when combined with multitask training. In addition, we leverage target-domain contexts during inference to enhance domain adaptation, enabling use of smaller models. Analyses offer insights into the benefits of different cost reduction strategies.</abstract>
      <url hash="775ec7e1">2023.clinicalnlp-1.38</url>
      <bibkey>zhou-etal-2023-building</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.38</doi>
    </paper>
    <paper id="39">
      <title>Intersectionality and Testimonial Injustice in Medical Records</title>
      <author><first>Kenya</first><last>Andrews</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Bhuvni</first><last>Shah</last></author>
      <author><first>Lu</first><last>Cheng</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>358-372</pages>
      <abstract>Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute to a patient’s experience. Further, some injustices may only be evident when examining the nuances that arise through the lens of intersectionality. Ignoring such injustices can result in poor quality of care or life-endangering events. Thus, considering intersectionality could result in more accurate classifications and just decisions. To illustrate this, we use real-world medical data to determine whether medical records exhibit words that could lead to testimonial injustice, employ fairness metrics (e.g. demographic parity, differential intersectional fairness, and subgroup fairness) to assess the severity to which subgroups are experiencing testimonial injustice, and analyze how the intersectionality of demographic features (e.g. gender and race) make a difference in uncovering testimonial injustice. From our analysis we found that with intersectionality we can better see disparities in how subgroups are treated and there are differences in how someone is treated based on the intersection of their demographic attributes. This has not been previously studied in clinical records, nor has it been proven through empirical study.</abstract>
      <url hash="4cf1adf4">2023.clinicalnlp-1.39</url>
      <bibkey>andrews-etal-2023-intersectionality</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.39</doi>
    </paper>
    <paper id="40">
      <title>Interactive Span Recommendation for Biomedical Text</title>
      <author><first>Louis</first><last>Blankemeier</last><affiliation>Stanford University</affiliation></author>
      <author><first>Theodore</first><last>Zhao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Robert</first><last>Tinn</last></author>
      <author><first>Sid</first><last>Kiblawi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yu</first><last>Gu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Akshay</first><last>Chaudhari</last><affiliation>Stanford University and Subtle Medical</affiliation></author>
      <author><first>Hoifung</first><last>Poon</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sheng</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mu</first><last>Wei</last><affiliation>Microsoft</affiliation></author>
      <author><first>J.</first><last>Preston</last></author>
      <pages>373-384</pages>
      <abstract>Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We first introduce unsupervised KRISS-Search and show that our method outperforms existing methods in identifying spans that are semantically similar to a given span of interest, with &gt;50% AUPRC improvement relative to PubMedBERT. We then introduce supervised KRISS-Search, which leverages human interaction to improve the notion of similarity used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate an enhanced F1 score of 0.68 in classifying spans as semantically similar or different in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT in few-shot biomedical named entity recognition (NER) across five benchmark datasets, with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing the efficiency of programmatic data labeling and also providing broader utility as an interactive biomedical search engine.</abstract>
      <url hash="a3998099">2023.clinicalnlp-1.40</url>
      <bibkey>blankemeier-etal-2023-interactive</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.40</doi>
    </paper>
    <paper id="41">
      <title>Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning</title>
      <author><first>Giridhar Kaushik</first><last>Ramachandran</last><affiliation>George Mason University</affiliation></author>
      <author><first>Yujuan</first><last>Fu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Bin</first><last>Han</last><affiliation>University of Washington</affiliation></author>
      <author><first>Kevin</first><last>Lybarger</last><affiliation>George Mason University</affiliation></author>
      <author><first>Nic</first><last>Dobbins</last></author>
      <author><first>Ozlem</first><last>Uzuner</last><affiliation>George Mason University</affiliation></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <pages>385-393</pages>
      <abstract>Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified social history sections annotated for SDOH, including substance use, employment, and living status information. We explore the automatic extraction of SDOH information with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction performance with a high-performing supervised approach and perform thorough error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing system among all teams in the n2c2 challenge with SHAC.</abstract>
      <url hash="857d81fb">2023.clinicalnlp-1.41</url>
      <bibkey>ramachandran-etal-2023-prompt</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.41</doi>
    </paper>
    <paper id="42">
      <title>Teddysum at <fixed-case>MEDIQA</fixed-case>-Chat 2023: an analysis of fine-tuning strategy for long dialog summarization</title>
      <author><first>Yongbin</first><last>Jeong</last></author>
      <author><first>Ju-Hyuck</first><last>Han</last></author>
      <author><first>Kyung Min</first><last>Chae</last><affiliation>Konyang University</affiliation></author>
      <author><first>Yousang</first><last>Cho</last></author>
      <author><first>Hyunbin</first><last>Seo</last><affiliation>teddysum</affiliation></author>
      <author><first>KyungTae</first><last>Lim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>Key-Sun</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Konyang University</affiliation></author>
      <author><first>Younggyun</first><last>Hahm</last></author>
      <pages>394-402</pages>
      <abstract>In this paper, we introduce the design and various attempts for TaskB of MEDIQA-Chat 2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full clinical note from doctor-patient consultation dialogues. This task has several challenging issues, such as lack of training data, handling long dialogue inputs, and generating semi-structured clinical note which have section heads. To address these issues, we conducted various experiments and analyzed their results. We utilized the DialogLED model pre-trained on long dialogue data to handle long inputs, and we pre-trained on other dialogue datasets to address the lack of training data. We also attempted methods such as using prompts and contrastive learning for handling sections. This paper provides insights into clinical note generation through analyzing experimental methods and results, and it suggests future research directions.</abstract>
      <url hash="558d0c52">2023.clinicalnlp-1.42</url>
      <bibkey>jeong-etal-2023-teddysum</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.42</doi>
    </paper>
    <paper id="43">
      <title>Rare Codes Count: Mining Inter-code Relations for Long-tail Clinical Text Classification</title>
      <author><first>Jiamin</first><last>Chen</last></author>
      <author><first>Xuhong</first><last>Li</last><affiliation>Baidu</affiliation></author>
      <author><first>Junting</first><last>Xi</last></author>
      <author><first>Lei</first><last>Yu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Haoyi</first><last>Xiong</last><affiliation>Baidu</affiliation></author>
      <pages>403-413</pages>
      <abstract>Multi-label clinical text classification, such as automatic ICD coding, has always been a challenging subject in Natural Language Processing, due to its long, domain-specific documents and long-tail distribution over a large label set. Existing methods adopt different model architectures to encode the clinical notes. Whereas without digging out the useful connections between labels, the model presents a huge gap in predicting performances between rare and frequent codes. In this work, we propose a novel method for further mining the helpful relations between different codes via a relation-enhanced code encoder to improve the rare code performance. Starting from the simple code descriptions, the model reaches comparable, even better performances than models with heavy external knowledge. Our proposed method is evaluated on MIMIC-III, a common dataset in the medical domain. It outperforms the previous state-of-art models on both overall metrics and rare code performances. Moreover, the interpretation results further prove the effectiveness of our methods. Our code is publicly available at <url>https://github.com/jiaminchen-1031/Rare-ICD</url>.</abstract>
      <url hash="6177323f">2023.clinicalnlp-1.43</url>
      <bibkey>chen-etal-2023-rare</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.43</doi>
    </paper>
    <paper id="44">
      <title><fixed-case>N</fixed-case>ew<fixed-case>A</fixed-case>ge<fixed-case>H</fixed-case>ealth<fixed-case>W</fixed-case>arriors at <fixed-case>MEDIQA</fixed-case>-Chat 2023 Task A: Summarizing Short Medical Conversation with Transformers</title>
      <author><first>Prakhar</first><last>Mishra</last></author>
      <author><first>Ravi Theja</first><last>Desetty</last><affiliation>Glance</affiliation></author>
      <pages>414-421</pages>
      <abstract>This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical NLP workshop. The shared task is motivated by the need to develop methods to automatically generate clinical notes from doctor-patient conversations. In this paper, we present our submission for <i>MEDIQA-Chat 2023 Task A: Short Dialogue2Note Summarization</i>. Manual creation of these clinical notes requires extensive human efforts, thus making it a time-consuming and expensive process. To address this, we propose an ensemble-based method over GPT-3, BART, BERT variants, and Rule-based systems to automatically generate clinical notes from these conversations. The proposed system achieves a score of 0.730 and 0.544 for both the sub-tasks on the test set (ranking 8th on the leaderboard for both tasks) and shows better performance compared to a baseline system using BART variants.</abstract>
      <url hash="cb797cd9">2023.clinicalnlp-1.44</url>
      <bibkey>mishra-desetty-2023-newagehealthwarriors</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.44</doi>
    </paper>
    <paper id="45">
      <title>Storyline-Centric Detection of Aphasia and Dysarthria in Stroke Patient Transcripts</title>
      <author><first>Peiqi</first><last>Sui</last></author>
      <author><first>Kelvin</first><last>Wong</last><affiliation>Weill Cornell Medicine, Cornell University and Houston Methodist Research Institute</affiliation></author>
      <author><first>Xiaohui</first><last>Yu</last><affiliation>NA</affiliation></author>
      <author><first>John</first><last>Volpi</last><affiliation>Houston Methodist Neurological Institute</affiliation></author>
      <author><first>Stephen</first><last>Wong</last><affiliation>Houston Methodist Hospital and Weill Cornell Medicine</affiliation></author>
      <pages>422-432</pages>
      <abstract>Aphasia and dysarthria are both common symptoms of stroke, affecting around 30% and 50% of acute ischemic stroke patients. In this paper, we propose a storyline-centric approach to detect aphasia and dysarthria in acute stroke patients using transcribed picture descriptions alone. Our pipeline enriches the training set with healthy data to address the lack of acute stroke patient data and utilizes knowledge distillation to significantly improve upon a document classification baseline, achieving an AUC of 0.814 (aphasia) and 0.764 (dysarthria) on a patient-only validation set.</abstract>
      <url hash="f54a872b">2023.clinicalnlp-1.45</url>
      <bibkey>sui-etal-2023-storyline</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.45</doi>
    </paper>
    <paper id="46">
      <title>Pre-trained language models in <fixed-case>S</fixed-case>panish for health insurance coverage</title>
      <author><first>Claudio</first><last>Aracena</last></author>
      <author><first>Nicolás</first><last>Rodríguez</last></author>
      <author><first>Victor</first><last>Rocco</last></author>
      <author><first>Jocelyn</first><last>Dunstan</last><affiliation>Universidad de Chile</affiliation></author>
      <pages>433-438</pages>
      <abstract>The field of clinical natural language processing (NLP) can extract useful information from clinical text. Since 2017, the NLP field has shifted towards using pre-trained language models (PLMs), improving performance in several tasks. Most of the research in this field has focused on English text, but there are some available PLMs in Spanish. In this work, we use clinical PLMs to analyze text from admission and medical reports in Spanish for an insurance and health provider to give a probability of no coverage in a labor insurance process. Our results show that fine-tuning a PLM pre-trained with the provider’s data leads to better results, but this process is time-consuming and computationally expensive. At least for this task, fine-tuning publicly available clinical PLM leads to comparable results to a custom PLM, but in less time and with fewer resources. Analyzing large volumes of insurance requests is burdensome for employers, and models can ease this task by pre-classifying reports that are likely not to have coverage. Our approach of entirely using clinical-related text improves the current models while reinforcing the idea of clinical support systems that simplify human labor but do not replace it. To our knowledge, the clinical corpus collected for this study is the largest one reported for the Spanish language.</abstract>
      <url hash="fa52faad">2023.clinicalnlp-1.46</url>
      <bibkey>aracena-etal-2023-pre</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.46</doi>
    </paper>
    <paper id="47">
      <title>Utterance Classification with Logical Neural Network: Explainable <fixed-case>AI</fixed-case> for Mental Disorder Diagnosis</title>
      <author><first>Yeldar</first><last>Toleubay</last><affiliation>University of Tsukuba, Tsukuba University</affiliation></author>
      <author><first>Don Joven</first><last>Agravante</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Daiki</first><last>Kimura</last><affiliation>IBM Research</affiliation></author>
      <author><first>Baihan</first><last>Lin</last><affiliation>Columbia University and IBM, International Business Machines</affiliation></author>
      <author><first>Djallel</first><last>Bouneffouf</last></author>
      <author><first>Michiaki</first><last>Tatsubori</last><affiliation>IBM Research</affiliation></author>
      <pages>439-446</pages>
      <abstract>In response to the global challenge of mental health problems, we proposes a Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis of mental disorders. Due to the lack of effective therapy coverage for mental disorders, there is a need for an AI solution that can assist therapists with the diagnosis. However, current Neural Network models lack explainability and may not be trusted by therapists. The LNN is a Recurrent Neural Network architecture that combines the learning capabilities of neural networks with the reasoning capabilities of classical logic-based AI. The proposed system uses input predicates from clinical interviews to output a mental disorder class, and different predicate pruning techniques are used to achieve scalability and higher scores. In addition, we provide an insight extraction method to aid therapists with their diagnosis. The proposed system addresses the lack of explainability of current Neural Network models and provides a more trustworthy solution for mental disorder diagnosis.</abstract>
      <url hash="08061f0a">2023.clinicalnlp-1.47</url>
      <bibkey>toleubay-etal-2023-utterance</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.47</doi>
    </paper>
    <paper id="48">
      <title>A Survey of Evaluation Methods of Generated Medical Textual Reports</title>
      <author><first>Yongxin</first><last>Zhou</last><affiliation>Laboratoire d’Informatique de Grenoble</affiliation></author>
      <author><first>Fabien</first><last>Ringeval</last><affiliation>University of Grenoble-Alpes</affiliation></author>
      <author><first>François</first><last>Portet</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <pages>447-459</pages>
      <abstract>Medical Report Generation (MRG) is a sub-task of Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support information for decision-making. Given the specificity of the medical domain, the evaluation of automatically generated medical reports is of paramount importance to the validity of these systems. Therefore, in this paper, we focus on the evaluation of automatically generated medical reports from the perspective of automatic and human evaluation. We present evaluation methods for general NLG evaluation and how they have been applied to domain-specific medical tasks. The study shows that MRG evaluation methods are very diverse, and that further work is needed to build shared evaluation methods. The state of the art also emphasizes that such an evaluation must be task specific and include human assessments, requesting the participation of experts in the field.</abstract>
      <url hash="fdf73a2a">2023.clinicalnlp-1.48</url>
      <bibkey>zhou-etal-2023-survey</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.48</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>UMASS</fixed-case>_<fixed-case>B</fixed-case>io<fixed-case>NLP</fixed-case> at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Can <fixed-case>LLM</fixed-case>s generate high-quality synthetic note-oriented doctor-patient conversations?</title>
      <author><first>Junda</first><last>Wang</last></author>
      <author><first>Zonghai</first><last>Yao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Avijit</first><last>Mitra</last></author>
      <author><first>Samuel</first><last>Osebe</last></author>
      <author><first>Zhichao</first><last>Yang</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>460-471</pages>
      <abstract>This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets. The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4. This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.</abstract>
      <url hash="38d5263c">2023.clinicalnlp-1.49</url>
      <bibkey>wang-etal-2023-umass</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>H</fixed-case>ealth<fixed-case>M</fixed-case>avericks@<fixed-case>MEDIQA</fixed-case>-Chat 2023: Benchmarking different Transformer based models for Clinical Dialogue Summarization</title>
      <author><first>Kunal</first><last>Suri</last><affiliation>Optum,India</affiliation></author>
      <author><first>Saumajit</first><last>Saha</last></author>
      <author><first>Atul</first><last>Singh</last></author>
      <pages>472-489</pages>
      <abstract>In recent years, we have seen many Transformer based models being created to address Dialog Summarization problem. While there has been a lot of work on understanding how these models stack against each other in summarizing regular conversations such as the ones found in DialogSum dataset, there haven’t been many analysis of these models on Clinical Dialog Summarization. In this article, we describe our solution to MEDIQA-Chat 2023 Shared Tasks as part of ACL-ClinicalNLP 2023 workshop which benchmarks some of the popular Transformer Architectures such as BioBart, Flan-T5, DialogLED, and OpenAI GPT3 on the problem of Clinical Dialog Summarization. We analyse their performance on two tasks - summarizing short conversations and long conversations. In addition to this, we also benchmark two popular summarization ensemble methods and report their performance.</abstract>
      <url hash="371ccef2">2023.clinicalnlp-1.50</url>
      <bibkey>suri-etal-2023-healthmavericks</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>S</fixed-case>umm<fixed-case>QA</fixed-case> at <fixed-case>MEDIQA</fixed-case>-Chat 2023: In-Context Learning with <fixed-case>GPT</fixed-case>-4 for Medical Summarization</title>
      <author><first>Yash</first><last>Mathur</last></author>
      <author><first>Sanketh</first><last>Rangreji</last></author>
      <author><first>Raghav</first><last>Kapoor</last></author>
      <author><first>Medha</first><last>Palavalli</last></author>
      <author><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Matthew</first><last>Gormley</last><affiliation>School of Computer Science, Carnegie Mellon University and 3M</affiliation></author>
      <pages>490-502</pages>
      <abstract>Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminologyin gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for sectionwise summarization (Task A) is a two-stage process of selecting semantically similar dialogues and using the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches. We compare GPT-4 performance with several finetuned baselines. We find that GPT-4 summaries are more abstractive and shorter. We make our code publicly available.</abstract>
      <url hash="57cf3fe4">2023.clinicalnlp-1.51</url>
      <bibkey>mathur-etal-2023-summqa</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.51</doi>
    </paper>
    <paper id="52">
      <title>Overview of the <fixed-case>MEDIQA</fixed-case>-Chat 2023 Shared Tasks on the Summarization &amp; Generation of Doctor-Patient Conversations</title>
      <author><first>Asma</first><last>Ben Abacha</last><affiliation>Microsoft, USA</affiliation></author>
      <author><first>Wen-wai</first><last>Yim</last></author>
      <author><first>Griffin</first><last>Adams</last></author>
      <author><first>Neal</first><last>Snider</last></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <pages>503-513</pages>
      <abstract>Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors’ workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants’ results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes.</abstract>
      <url hash="a6af9e7a">2023.clinicalnlp-1.52</url>
      <bibkey>ben-abacha-etal-2023-overview</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.52</doi>
    </paper>
    <paper id="53">
      <title>Transfer Learning for Low-Resource Clinical Named Entity Recognition</title>
      <author><first>Nevasini</first><last>Sasikumar</last></author>
      <author><first>Krishna Sri Ipsit</first><last>Mantri</last></author>
      <pages>514-518</pages>
      <abstract>We propose a transfer learning method that adapts a high-resource English clinical NER model to low-resource languages and domains using only small amounts of in-domain annotated data. Our approach involves translating in-domain datasets to English, fine-tuning the English model on the translated data, and then transferring it to the target language/domain. Experiments on Spanish, French, and conversational clinical text datasets show accuracy gains over models trained on target data alone. Our method achieves state-of-the-art performance and can enable clinical NLP in more languages and modalities with limited resources.</abstract>
      <url hash="9e113924">2023.clinicalnlp-1.53</url>
      <bibkey>sasikumar-mantri-2023-transfer</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.53</doi>
    </paper>
    <paper id="54">
      <title><fixed-case>IUTEAM</fixed-case>1 at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Is simple fine tuning effective for multi layer summarization of clinical conversations?</title>
      <author><first>Dhananjay</first><last>Srivastava</last></author>
      <pages>519-523</pages>
      <abstract>Clinical conversation summarization has become an important application of Natural language Processing. In this work, we intend to analyze summarization model ensembling approaches, that can be utilized to improve the overall accuracy of the generated medical report called chart note. The work starts with a single summarization model creating the baseline. Then leads to an ensemble of summarization models trained on a separate section of the chart note. This leads to the final approach of passing the generated results to another summarization model in a multi-layer/stage fashion for better coherency of the generated text. Our results indicate that although an ensemble of models specialized in each section produces better results, the multi-layer/stage approach does not improve accuracy. The code for the above paper is available at <url>https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git</url></abstract>
      <url hash="02c98688">2023.clinicalnlp-1.54</url>
      <bibkey>srivastava-2023-iuteam1</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>C</fixed-case>are4<fixed-case>L</fixed-case>ang at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues</title>
      <author><first>Amal</first><last>Alqahtani</last><affiliation>George Washington University</affiliation></author>
      <author><first>Rana</first><last>Salama</last><affiliation>George Washington University</affiliation></author>
      <author><first>Mona</first><last>Diab</last><affiliation>George Washington University</affiliation></author>
      <author><first>Abdou</first><last>Youssef</last><affiliation>George Washington University</affiliation></author>
      <pages>524-528</pages>
      <abstract>Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.</abstract>
      <url hash="9e68b483">2023.clinicalnlp-1.55</url>
      <bibkey>alqahtani-etal-2023-care4lang</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.55</doi>
    </paper>
    <paper id="56">
      <title><fixed-case>C</fixed-case>alvados at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Improving Clinical Note Generation with Multi-Task Instruction Finetuning</title>
      <author><first>Kirill</first><last>Milintsevich</last><affiliation>Université de Caen Basse Normandie and University of Tartu</affiliation></author>
      <author><first>Navneet</first><last>Agarwal</last></author>
      <pages>529-535</pages>
      <abstract>This paper presents our system for the MEDIQA-Chat 2023 shared task on medical conversation summarization. Our approach involves finetuning a LongT5 model on multiple tasks simultaneously, which we demonstrate improves the model’s overall performance while reducing the number of factual errors and hallucinations in the generated summary. Furthermore, we investigated the effect of augmenting the data with in-text annotations from a clinical named entity recognition model, finding that this approach decreased summarization quality. Lastly, we explore using different text generation strategies for medical note generation based on the length of the note. Our findings suggest that the application of our proposed approach can be beneficial for improving the accuracy and effectiveness of medical conversation summarization.</abstract>
      <url hash="62790d1a">2023.clinicalnlp-1.56</url>
      <bibkey>milintsevich-agarwal-2023-calvados</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.56</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>DS</fixed-case>4<fixed-case>DH</fixed-case> at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Leveraging <fixed-case>SVM</fixed-case> and <fixed-case>GPT</fixed-case>-3 Prompt Engineering for Medical Dialogue Classification and Summarization</title>
      <author><first>Boya</first><last>Zhang</last></author>
      <author><first>Rahul</first><last>Mishra</last></author>
      <author><first>Douglas</first><last>Teodoro</last><affiliation>University of Geneva</affiliation></author>
      <pages>536-545</pages>
      <abstract>This paper presents the results of the Data Science for Digital Health (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the power of a classical machine learning method, Support Vector Machine, for classifying medical dialogues, along with the implementation of one-shot prompts using GPT-3.5. We employ dialogues and summaries from the same category as prompts to generate summaries for novel dialogues. Our findings exceed the average benchmark score, offering a robust reference for assessing performance in this field.</abstract>
      <url hash="49c53a44">2023.clinicalnlp-1.57</url>
      <bibkey>zhang-etal-2023-ds4dh</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>G</fixed-case>erstein<fixed-case>L</fixed-case>ab at <fixed-case>MEDIQA</fixed-case>-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning</title>
      <author><first>Xiangru</first><last>Tang</last><affiliation>Yale University</affiliation></author>
      <author><first>Andrew</first><last>Tran</last></author>
      <author><first>Jeffrey</first><last>Tan</last></author>
      <author><first>Mark</first><last>Gerstein</last><affiliation>Yale University</affiliation></author>
      <pages>546-554</pages>
      <abstract>This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.</abstract>
      <url hash="57efd858">2023.clinicalnlp-1.58</url>
      <bibkey>tang-etal-2023-gersteinlab</bibkey>
      <doi>10.18653/v1/2023.clinicalnlp-1.58</doi>
    </paper>
  </volume>
</collection>
