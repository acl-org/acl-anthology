<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.bucc">
  <volume id="1" ingest-date="2022-09-21">
    <meta>
      <booktitle>Proceedings of the BUCC Workshop within LREC 2022</booktitle>
      <editor><first>Reinhard</first><last>Rapp</last></editor>
      <editor><first>Pierre</first><last>Zweigenbaum</last></editor>
      <editor><first>Serge</first><last>Sharoff</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="5d9a3ae3">2022.bucc-1</url>
      <venue>bucc</venue>
    </meta>
    <frontmatter>
      <url hash="bf08bc59">2022.bucc-1.0</url>
      <bibkey>bucc-2022-bucc</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluating Monolingual and Crosslingual Embeddings on Datasets of Word Association Norms</title>
      <author><first>Trina</first><last>Kwong</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Rong</first><last>Xiang</last></author>
      <pages>1–7</pages>
      <abstract>In free word association tasks, human subjects are presented with a stimulus word and are then asked to name the first word (the response word) that comes up to their mind. Those associations, presumably learned on the basis of conceptual contiguity or similarity, have attracted for a long time the attention of researchers in linguistics and cognitive psychology, since they are considered as clues about the internal organization of the lexical knowledge in the semantic memory. Word associations data have also been used to assess the performance of Vector Space Models for English, but evaluations for other languages have been relatively rare so far. In this paper, we introduce word associations datasets for Italian, Spanish and Mandarin Chinese by extracting data from the Small World of Words project, and we propose two different tasks inspired by the previous literature. We tested both monolingual and crosslingual word embeddings on the new datasets, showing that they perform similarly in the evaluation tasks.</abstract>
      <url hash="722fa96c">2022.bucc-1.1</url>
      <bibkey>kwong-etal-2022-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="2">
      <title>About Evaluating Bilingual Lexicon Induction</title>
      <author><first>Martin</first><last>Laville</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>8–14</pages>
      <abstract>With numerous new methods proposed recently, the evaluation of Bilingual Lexicon Induction have been quite hazardous and inconsistent across works. Some studies proposed some guidance to sanitize this; yet, they are not necessarily followed by practitioners. In this study, we try to gather these different recommendations and add our owns, with the aim to propose an unified evaluation protocol. We further show that the easiness of a benchmark while being correlated to the proximity of the language pairs being considered, is even more conditioned on the graphical similarities within the test word pairs.</abstract>
      <url hash="dfc04fe0">2022.bucc-1.2</url>
      <bibkey>laville-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="3">
      <title>Don’t Forget Cheap Training Signals Before Building Unsupervised Bilingual Word Embeddings</title>
      <author><first>Silvia</first><last>Severini</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Masoud</first><last>Jalili Sabet</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>15–22</pages>
      <abstract>Bilingual Word Embeddings (BWEs) are one of the cornerstones of cross-lingual transfer of NLP models. They can be built using only monolingual corpora without supervision leading to numerous works focusing on unsupervised BWEs. However, most of the current approaches to build unsupervised BWEs do not compare their results with methods based on easy-to-access cross-lingual signals. In this paper, we argue that such signals should always be considered when developing unsupervised BWE methods. The two approaches we find most effective are: 1) using identical words as seed lexicons (which unsupervised approaches incorrectly assume are not available for orthographically distinct language pairs) and 2) combining such lexicons with pairs extracted by matching romanized versions of words with an edit distance threshold. We experiment on thirteen non-Latin languages (and English) and show that such cheap signals work well and that they outperform using more complex unsupervised methods on distant language pairs such as Chinese, Japanese, Kannada, Tamil, and Thai. In addition, they are even competitive with the use of high-quality lexicons in supervised approaches. Our results show that these training signals should not be neglected when building BWEs, even for distant languages.</abstract>
      <url hash="1eb3dc86">2022.bucc-1.3</url>
      <bibkey>severini-etal-2022-dont</bibkey>
    </paper>
    <paper id="4">
      <title>Building Domain-specific Corpora from the Web: the Case of <fixed-case>E</fixed-case>uropean Digital Service Infrastructures</title>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Cristian</first><last>García-Romero</last></author>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Leopoldo</first><last>Pla Sempere</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>23–32</pages>
      <abstract>An important goal of the MaCoCu project is to improve EU-specific NLP systems that concern their Digital Service Infrastructures (DSIs). In this paper we aim at boosting the creation of such domain-specific NLP systems. To do so, we explore the feasibility of building an automatic classifier that allows to identify which segments in a generic (potentially parallel) corpus are relevant for a particular DSI. We create an evaluation data set by crawling DSI-specific web domains and then compare different strategies to build our DSI classifier for text in three languages: English, Spanish and Dutch. We use pre-trained (multilingual) language models to perform the classification, with zero-shot classification for Spanish and Dutch. The results are promising, as we are able to classify DSIs with between 70 and 80% accuracy, even without in-language training data. A manual annotation of the data revealed that we can also find DSI-specific data on crawled texts from general web domains with reasonable accuracy. We publicly release all data, predictions and code, as to allow future investigations in whether exploiting this DSI-specific data actually leads to improved performance on particular applications, such as machine translation.</abstract>
      <url hash="54a0286c">2022.bucc-1.4</url>
      <bibkey>van-noord-etal-2022-building</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="5">
      <title>Multilingual Comparative Analysis of Deep-Learning Dependency Parsing Results Using Parallel Corpora</title>
      <author><first>Diego</first><last>Alves</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Božo</first><last>Bekavac</last></author>
      <pages>33–42</pages>
      <abstract>This article presents a comparative analysis of dependency parsing results for a set of 16 languages, coming from a large variety of linguistic families and genera, whose parallel corpora were used to train a deep-learning tool. Results are analyzed in comparison to an innovative way of classifying languages concerning the head directionality parameter used to perform a quantitative syntactic typological classification of languages. It has been shown that, despite using parallel corpora, there is a large discrepancy in terms of LAS results. The obtained results show that this heterogeneity is mainly due to differences in the syntactic structure of the selected languages, where Indo-European ones, especially Romance languages, have the best scores. It has been observed that the differences in the size of the representation of each language in the language model used by the deep-learning tool also play a major role in the dependency parsing efficacy. Other factors, such as the number of dependency parsing labels may also have an influence on results with more complex labeling systems such as the Polish language.</abstract>
      <url hash="be6337b2">2022.bucc-1.5</url>
      <bibkey>alves-etal-2022-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="6">
      <title><fixed-case>CUNI</fixed-case> Submission to the <fixed-case>BUCC</fixed-case> 2022 Shared Task on Bilingual Term Alignment</title>
      <author><first>Borek</first><last>Požár</last></author>
      <author><first>Klára</first><last>Tauchmanová</last></author>
      <author><first>Kristýna</first><last>Neumannová</last></author>
      <author><first>Ivana</first><last>Kvapilíková</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>43–49</pages>
      <abstract>We present our submission to the BUCC Shared Task on bilingual term alignment in comparable specialized corpora. We devised three approaches using static embeddings with post-hoc alignment, the Monoses pipeline for unsupervised phrase-based machine translation, and contextualized multilingual embeddings. We show that contextualized embeddings from pretrained multilingual models lead to similar results as static embeddings but further improvement can be achieved by task-specific fine-tuning. Retrieving term pairs from the running phrase tables of the Monoses systems can match this enhanced performance and leads to an average precision of 0.88 on the train set.</abstract>
      <url hash="e7f28797">2022.bucc-1.6</url>
      <bibkey>pozar-etal-2022-cuni</bibkey>
    </paper>
    <paper id="7">
      <title>Challenges of Building Domain-Specific Parallel Corpora from Public Administration Documents</title>
      <author><first>Filip</first><last>Klubička</last></author>
      <author><first>Lorena</first><last>Kasunić</last></author>
      <author><first>Danijel</first><last>Blazsetin</last></author>
      <author><first>Petra</first><last>Bago</last></author>
      <pages>50–55</pages>
      <abstract>PRINCIPLE was a Connecting Europe Facility (CEF)-funded project that focused on the identification, collection and processing of language resources (LRs) for four European under-resourced languages (Croatian, Icelandic, Irish and Norwegian) in order to improve translation quality of eTranslation, an online machine translation (MT) tool provided by the European Commission. The collected LRs were used for the development of neural MT engines in order to verify the quality of the resources. For all four languages, a total of 66 LRs were collected and made available on the ELRC-SHARE repository under various licenses. For Croatian, we have collected and published 20 LRs: 19 parallel corpora and 1 glossary. The majority of data is in the general domain (72 % of translation units), while the rest is in the eJustice (23 %), eHealth (3 %) and eProcurement (2 %) Digital Service Infrastructures (DSI) domains. The majority of the resources were for the Croatian-English language pair. The data was donated by six data contributors from the public as well as private sector. In this paper we present a subset of 13 Croatian LRs developed based on public administration documents, which are all made freely available, as well as challenges associated with the data collection, cleaning and processing.</abstract>
      <url hash="14c88fff">2022.bucc-1.7</url>
      <bibkey>klubicka-etal-2022-challenges</bibkey>
    </paper>
    <paper id="8">
      <title>Setting Up Bilingual Comparable Corpora with Non-Contemporary Languages</title>
      <author><first>Helena</first><last>Bermudez Sabel</last></author>
      <author><first>Francesca</first><last>Dell’Oro</last></author>
      <author><first>Cyrielle</first><last>Montrichard</last></author>
      <author><first>Corinne</first><last>Rossari</last></author>
      <pages>56–60</pages>
      <abstract>This paper presents the project “Les corpora latins et français: une fabrique pour l’accès à la représentation des connaissances” (Latin and French Corpora: a Factory For Accessing Knowledge Representation) whose focus is the study of modality in both Latin and French by means of multi-genre, diachronic comparable corpora. The setting up of such corpora involves a number of conceptualisation challenges, in particular with regard to how to compare two asynchronous textual productions corresponding to different cultural frameworks. In this paper we outline the rationale of designing comparable corpora to explore our research questions and then focus on some of the issues that arise when comparing different diachronic spans of Latin and French. We also explain how these issues were dealt with, thus providing some grounds upon which other projects could build their methodology.</abstract>
      <url hash="8483f194">2022.bucc-1.8</url>
      <bibkey>bermudez-sabel-etal-2022-setting</bibkey>
    </paper>
    <paper id="9">
      <title>Fusion of linguistic, neural and sentence-transformer features for improved term alignment</title>
      <author><first>Andraz</first><last>Repar</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Matej</first><last>Ulčar</last></author>
      <author><first>Boshko</first><last>Koloski</last></author>
      <pages>61–66</pages>
      <abstract>Crosslingual terminology alignment task has many practical applications. In this work, we propose an aligning method for the shared task of the 15th Workshop on Building and Using Comparable Corpora. Our method combines several different approaches into one cohesive machine learning model, based on SVM. From shared-task specific and external sources, we crafted four types of features: cognate-based, dictionary-based, embedding-based, and combined features, which combine aspects of the other three types. We added a post-processing re-scoring method, which reducess the effect of hubness, where some terms are nearest neighbours of many other terms. We achieved the average precision score of 0.833 on the English-French training set of the shared task.</abstract>
      <url hash="2146ad8a">2022.bucc-1.9</url>
      <bibkey>repar-etal-2022-fusion</bibkey>
    </paper>
  </volume>
</collection>
