<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.ranlp">
  <volume id="1" ingest-date="2021-11-09" type="proceedings">
    <meta>
      <booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</booktitle>
      <editor><first>Ruslan</first><last>Mitkov</last></editor>
      <editor><first>Galia</first><last>Angelova</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Held Online</address>
      <month>September</month>
      <year>2021</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="3db98e19">2021.ranlp-1.0</url>
      <bibkey>ranlp-2021-deep</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>BP</fixed-case>o<fixed-case>MP</fixed-case>: The Benchmark of Poetic Minimal Pairs – Limericks, Rhyme, and Narrative Coherence</title>
      <author><first>Almas</first><last>Abdibayev</last></author>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Daniel</first><last>Rockmore</last></author>
      <pages>1–9</pages>
      <abstract>We adapt BLiMP (Benchmark of Linguistic Minimal Pairs) language model evaluation framework to the context of poetry, introducing the first of a series of tasks titled Benchmark of Poetic Minimal Pairs (BPoMP). The tasks presented herein use one genre of English-language poetry, the limerick (five-lines, rhyme scheme AABBA). Following the BLiMP schema, the BPoMP tasks use 10,000 minimal pairs of limerick/corrupted limerick. The latter is created by (1) shuffling two rhyming end-of-the-line words, (2) shuffling two rhyming lines, (3) replacing end-of-the-line word by a non-rhyming synonym. Our general task is detection of the original limerick, which we believe tests a language model’s capacity to utilize “end rhymes”, a common feature of poetry. We evaluate Transformer-based models by checking if they assign a higher probability to the non-corrupted limerick in each minimal pair. We find that the models identify the original limerick at rates better than chance, but with a nontrivial gap relative to human accuracy (average of 98.3% across tasks). The publicly available curated set of limericks accompanying this paper is an additional contribution. In general, we see this as a first step to create a community of NLP activity around the rigorous computational study of poetry.</abstract>
      <url hash="2410b907">2021.ranlp-1.1</url>
      <bibkey>abdibayev-etal-2021-bpomp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="2">
      <title>Ontology Population Reusing Resources for Dialogue Intent Detection: Generic and Multilingual Approach</title>
      <author><first>Cristina</first><last>Aceta</last></author>
      <author><first>Izaskun</first><last>Fernández</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <pages>10–18</pages>
      <abstract>This work presents a generic semi-automatic strategy to populate the domain ontology of an ontology-driven task-oriented dialogue system, with the aim of performing successful intent detection in the dialogue process, reusing already existing multilingual resources. This semi-automatic approach allows ontology engineers to exploit available resources so as to associate the potential situations in the use case to FrameNet frames and obtain the relevant lexical units associated to them in the target language, following lexical and semantic criteria, without linguistic expert knowledge. This strategy has been validated and evaluated in two use cases, from industrial scenarios, for interaction in Spanish with a guide robot and with a Computerized Maintenance Management System (CMMS). In both cases, this method has allowed the ontology engineer to instantiate the domain ontology with the intent-relevant information with quality data in a simple and low-resource-consuming manner.</abstract>
      <url hash="30fea4af">2021.ranlp-1.2</url>
      <bibkey>aceta-etal-2021-ontology</bibkey>
    </paper>
    <paper id="3">
      <title>Efficient Multilingual Text Classification for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Salil</first><last>Aggarwal</last></author>
      <author><first>Sourav</first><last>Kumar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>19–25</pages>
      <abstract>India is one of the richest language hubs on the earth and is very diverse and multilingual. But apart from a few Indian languages, most of them are still considered to be resource poor. Since most of the NLP techniques either require linguistic knowledge that can only be developed by experts and native speakers of that language or they require a lot of labelled data which is again expensive to generate, the task of text classification becomes challenging for most of the Indian languages. The main objective of this paper is to see how one can benefit from the lexical similarity found in Indian languages in a multilingual scenario. Can a classification model trained on one Indian language be reused for other Indian languages? So, we performed zero-shot text classification via exploiting lexical similarity and we observed that our model performs best in those cases where the vocabulary overlap between the language datasets is maximum. Our experiments also confirm that a single multilingual model trained via exploiting language relatedness outperforms the baselines by significant margins.</abstract>
      <url hash="17e2d010">2021.ranlp-1.3</url>
      <bibkey>aggarwal-etal-2021-efficient</bibkey>
    </paper>
    <paper id="4">
      <title>Domain Adaptation for <fixed-case>H</fixed-case>indi-<fixed-case>T</fixed-case>elugu Machine Translation Using Domain Specific Back Translation</title>
      <author><first>Hema</first><last>Ala</last></author>
      <author><first>Vandan</first><last>Mujadia</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>26–34</pages>
      <abstract>In this paper, we present a novel approachfor domain adaptation in Neural MachineTranslation which aims to improve thetranslation quality over a new domain. Adapting new domains is a highly challeng-ing task for Neural Machine Translation onlimited data, it becomes even more diffi-cult for technical domains such as Chem-istry and Artificial Intelligence due to spe-cific terminology, etc. We propose DomainSpecific Back Translation method whichuses available monolingual data and gen-erates synthetic data in a different way. This approach uses Out Of Domain words. The approach is very generic and can beapplied to any language pair for any domain. We conduct our experiments onChemistry and Artificial Intelligence do-mains for Hindi and Telugu in both direc-tions. It has been observed that the usageof synthetic data created by the proposedalgorithm improves the BLEU scores significantly.</abstract>
      <url hash="4d1fbfec">2021.ranlp-1.4</url>
      <bibkey>ala-etal-2021-domain</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>A</fixed-case>rab<fixed-case>G</fixed-case>loss<fixed-case>BERT</fixed-case>: Fine-Tuning <fixed-case>BERT</fixed-case> on Context-Gloss Pairs for <fixed-case>WSD</fixed-case></title>
      <author><first>Moustafa</first><last>Al-Hajj</last></author>
      <author><first>Mustafa</first><last>Jarrar</last></author>
      <pages>35–43</pages>
      <abstract>Using pre-trained transformer models such as BERT has proven to be effective in many NLP tasks. This paper presents our work to fine-tune BERT models for Arabic Word Sense Disambiguation (WSD). We treated the WSD task as a sentence-pair binary classification task. First, we constructed a dataset of labeled Arabic context-gloss pairs (~167k pairs) we extracted from the Arabic Ontology and the large lexicographic database available at Birzeit University. Each pair was labeled as True or False and target words in each context were identified and annotated. Second, we used this dataset for fine-tuning three pre-trained Arabic BERT models. Third, we experimented the use of different supervised signals used to emphasize target words in context. Our experiments achieved promising results (accuracy of 84%) although we used a large set of senses in the experiment.</abstract>
      <url hash="3fbd1d40">2021.ranlp-1.5</url>
      <bibkey>al-hajj-jarrar-2021-arabglossbert</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>A</fixed-case>rabic Cross-language Plagiarism Detection</title>
      <author><first>Naif</first><last>Alotaibi</last></author>
      <author><first>Mike</first><last>Joy</last></author>
      <pages>44–52</pages>
      <abstract>The advancement of the web and information technology has contributed to the rapid growth of digital libraries and automatic machine translation tools which easily translate texts from one language into another. These have increased the content accessible in different languages, which results in easily performing translated plagiarism, which are referred to as “cross-language plagiarism”. Recognition of plagiarism among texts in different languages is more challenging than identifying plagiarism within a corpus written in the same language. This paper proposes a new technique for enhancing English-Arabic cross-language plagiarism detection at the sentence level. This technique is based on semantic and syntactic feature extraction using word order, word embedding and word alignment with multilingual encoders. Those features, and their combination with different machine learning (ML) algorithms, are then used in order to aid the task of classifying sentences as either plagiarized or non-plagiarized. The proposed approach has been deployed and assessed using datasets presented at SemEval-2017. Analysis of experimental data demonstrates that utilizing extracted features and their combinations with various ML classifiers achieves promising results.</abstract>
      <url hash="e4ac941f">2021.ranlp-1.6</url>
      <bibkey>alotaibi-joy-2021-english</bibkey>
    </paper>
    <paper id="7">
      <title>Towards a Better Understanding of Noise in Natural Language Processing</title>
      <author><first>Khetam</first><last>Al Sharou</last></author>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>53–62</pages>
      <abstract>In this paper, we propose a definition and taxonomy of various types of non-standard textual content – generally referred to as “noise” – in Natural Language Processing (NLP). While data pre-processing is undoubtedly important in NLP, especially when dealing with user-generated content, a broader understanding of different sources of noise and how to deal with them is an aspect that has been largely neglected. We provide a comprehensive list of potential sources of noise, categorise and describe them, and show the impact of a subset of standard pre-processing strategies on different tasks. Our main goal is to raise awareness of non-standard content – which should not always be considered as “noise” – and of the need for careful, task-dependent pre-processing. This is an alternative to blanket, all-encompassing solutions generally applied by researchers through “standard” pre-processing pipelines. The intention is for this categorisation to serve as a point of reference to support NLP researchers in devising strategies to clean, normalise or embrace non-standard content.</abstract>
      <url hash="4b18a85c">2021.ranlp-1.7</url>
      <bibkey>al-sharou-etal-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="8">
      <title>Comparing Supervised Machine Learning Techniques for Genre Analysis in Software Engineering Research Articles</title>
      <author><first>Felipe</first><last>Araújo de Britto</last></author>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>Leonardo Pereira</first><last>Nunes</last></author>
      <author><first>Fernando</first><last>Silva Parreiras</last></author>
      <pages>63–72</pages>
      <abstract>Written communication is of utmost importance to the progress of scientific research. The speed of such development, however, may be affected by the scarcity of reviewers to referee the quality of research articles. In this context, automatic approaches that are able to query linguistic segments in written contributions by detecting the presence or absence of common rhetorical patterns have become a necessity. This paper aims to compare supervised machine learning techniques tested to accomplish genre analysis in Introduction sections of software engineering articles. A semi-supervised approach was carried out to augment the number of annotated sentences in SciSents (Avaliable on: ANONYMOUS). Two supervised approaches using SVM and logistic regression were undertaken to assess the F-score for genre analysis in the corpus. A technique based on logistic regression and BERT has been found to perform genre analysis highly satisfactorily with an average of 88.25 on F-score when retrieving patterns at an overall level.</abstract>
      <url hash="05ca1166">2021.ranlp-1.8</url>
      <bibkey>araujo-de-britto-etal-2021-comparing</bibkey>
    </paper>
    <paper id="9">
      <title>Enriching the Transformer with Linguistic Factors for Low-Resource Machine Translation</title>
      <author><first>Jordi</first><last>Armengol-Estapé</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Carlos</first><last>Escolano</last></author>
      <pages>73–78</pages>
      <abstract>Introducing factors, that is to say, word features such as linguistic information referring to the source tokens, is known to improve the results of neural machine translation systems in certain settings, typically in recurrent architectures. This study proposes enhancing the current state-of-the-art neural machine translation architecture, the Transformer, so that it allows to introduce external knowledge. In particular, our proposed modification, the Factored Transformer, uses linguistic factors that insert additional knowledge into the machine translation system. Apart from using different kinds of features, we study the effect of different architectural configurations. Specifically, we analyze the performance of combining words and features at the embedding level or at the encoder level, and we experiment with two different combination strategies. With the best-found configuration, we show improvements of 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task. Moreover, we experiment with the more challenging FLoRes English-to-Nepali benchmark, which includes both extremely low-resourced and very distant languages, and obtain an improvement of 1.2 BLEU</abstract>
      <url hash="8d853930">2021.ranlp-1.9</url>
      <bibkey>armengol-estape-etal-2021-enriching</bibkey>
    </paper>
    <paper id="10">
      <title>A Multi-Pass Sieve Coreference Resolution for <fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Valentina Kania Prameswara</first><last>Artari</last></author>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Meganingrum Arista</first><last>Jiwanggi</last></author>
      <author><first>Adityo</first><last>Anggraito</last></author>
      <author><first>Indra</first><last>Budi</last></author>
      <pages>79–85</pages>
      <abstract>Coreference resolution is an NLP task to find out whether the set of referring expressions belong to the same concept in discourse. A multi-pass sieve is a deterministic coreference model that implements several layers of sieves, where each sieve takes a pair of correlated mentions from a collection of non-coherent mentions. The multi-pass sieve is based on the principle of high precision, followed by increased recall in each sieve. In this work, we examine the portability of the multi-pass sieve coreference resolution model to the Indonesian language. We conduct the experiment on 201 Wikipedia documents and the multi-pass sieve system yields 72.74% of MUC F-measure and 52.18% of BCUBED F-measure.</abstract>
      <url hash="526e3641">2021.ranlp-1.10</url>
      <bibkey>artari-etal-2021-multi</bibkey>
    </paper>
    <paper id="11">
      <title>Solving <fixed-case>SCAN</fixed-case> Tasks with Data Augmentation and Input Embeddings</title>
      <author><first>Michal</first><last>Auersperger</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>86–91</pages>
      <abstract>We address the compositionality challenge presented by the SCAN benchmark. Using data augmentation and a modification of the standard seq2seq architecture with attention, we achieve SOTA results on all the relevant tasks from the benchmark, showing the models can generalize to words used in unseen contexts. We propose an extension of the benchmark by a harder task, which cannot be solved by the proposed method.</abstract>
      <url hash="7f51c1a5">2021.ranlp-1.11</url>
      <bibkey>auersperger-pecina-2021-solving</bibkey>
      <pwccode url="https://github.com/michal-au/scan-around-twice" additional="false">michal-au/scan-around-twice</pwccode>
    </paper>
    <paper id="12">
      <title><fixed-case>P</fixed-case>y<fixed-case>E</fixed-case>uro<fixed-case>V</fixed-case>oc: A Tool for Multilingual Legal Document Classification with <fixed-case>E</fixed-case>uro<fixed-case>V</fixed-case>oc Descriptors</title>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Dan Ioan</first><last>Tufis</last></author>
      <pages>92–101</pages>
      <abstract>EuroVoc is a multilingual thesaurus that was built for organizing the legislative documentary of the European Union institutions. It contains thousands of categories at different levels of specificity and its descriptors are targeted by legal texts in almost thirty languages. In this work we propose a unified framework for EuroVoc classification on 22 languages by fine-tuning modern Transformer-based pretrained language models. We study extensively the performance of our trained models and show that they significantly improve the results obtained by a similar tool - JEX - on the same dataset. The code and the fine-tuned models were open sourced, together with a programmatic interface that eases the process of loading the weights of a trained model and of classifying a new document.</abstract>
      <url hash="499b00f2">2021.ranlp-1.12</url>
      <bibkey>avram-etal-2021-pyeurovoc</bibkey>
      <pwccode url="https://github.com/racai-ai/pyeurovoc" additional="true">racai-ai/pyeurovoc</pwccode>
    </paper>
    <paper id="13">
      <title><fixed-case>TEASER</fixed-case>: Towards Efficient Aspect-based <fixed-case>SE</fixed-case>ntiment Analysis and Recognition</title>
      <author><first>Vaibhav</first><last>Bajaj</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Ishan</first><last>Upadhyay</last></author>
      <author><first>Srinath</first><last>Nair</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>102–110</pages>
      <abstract>Sentiment analysis aims to detect the overall sentiment, i.e., the polarity of a sentence, paragraph, or text span, without considering the entities mentioned and their aspects. Aspect-based sentiment analysis aims to extract the aspects of the given target entities and their respective sentiments. Prior works formulate this as a sequence tagging problem or solve this task using a span-based extract-then-classify framework where first all the opinion targets are extracted from the sentence, and then with the help of span representations, the targets are classified as positive, negative, or neutral. The sequence tagging problem suffers from issues like sentiment inconsistency and colossal search space. Whereas, Span-based extract-then-classify framework suffers from issues such as half-word coverage and overlapping spans. To overcome this, we propose a similar span-based extract-then-classify framework with a novel and improved heuristic. Experiments on the three benchmark datasets (Restaurant14, Laptop14, Restaurant15) show our model consistently outperforms the current state-of-the-art. Moreover, we also present a novel supervised movie reviews dataset (Movie20) and a pseudo-labeled movie reviews dataset (moviesLarge) made explicitly for this task and report the results on the novel Movie20 dataset as well.</abstract>
      <url hash="42af87f1">2021.ranlp-1.13</url>
      <bibkey>bajaj-etal-2021-teaser</bibkey>
    </paper>
    <paper id="14">
      <title>Interactive Learning Approach for <fixed-case>A</fixed-case>rabic Target-Based Sentiment Analysis</title>
      <author><first>Husamelddin</first><last>Balla</last></author>
      <author><first>Marisa</first><last>Llorens Salvador</last></author>
      <author><first>Sarah Jane</first><last>Delany</last></author>
      <pages>111–120</pages>
      <abstract>Recently, the majority of sentiment analysis researchers focus on target-based sentiment analysis because it delivers in-depth analysis with more accurate results as compared to traditional sentiment analysis. In this paper, we propose an interactive learning approach to tackle a target-based sentiment analysis task for the Arabic language. The proposed IA-LSTM model uses an interactive attention-based mechanism to force the model to focus on different parts (targets) of a sentence. We investigate the ability to use targets, right, and left context, and model them separately to learn their own representations via interactive modeling. We evaluated our model on two different datasets: Arabic hotel review and Arabic book review datasets. The results demonstrate the effectiveness of using this interactive modeling technique for the Arabic target-based task. The model obtained accuracy values of 83.10 compared to SOTA models such as AB-LSTM-PC which obtained 82.60 for the same dataset.</abstract>
      <url hash="6524b089">2021.ranlp-1.14</url>
      <bibkey>balla-etal-2021-interactive</bibkey>
    </paper>
    <paper id="15">
      <title>Litescale: A Lightweight Tool for Best-worst Scaling Annotation</title>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Christian</first><last>Cagnazzo</last></author>
      <pages>121–127</pages>
      <abstract>Best-worst Scaling (BWS) is a methodology for annotation based on comparing and ranking instances, rather than classifying or scoring individual instances. Studies have shown the efficacy of this methodology applied to NLP tasks in terms of a higher quality of the datasets produced by following it. In this system demonstration paper, we present Litescale, a free software library to create and manage BWS annotation tasks. Litescale computes the tuples to annotate, manages the users and the annotation process, and creates the final gold standard. The functionalities of Litescale can be accessed programmatically through a Python module, or via two alternative user interfaces, a textual console-based one and a graphical Web-based one. We further developed and deployed a fully online version of Litescale complete with multi-user support.</abstract>
      <url hash="2b5d8b95">2021.ranlp-1.15</url>
      <bibkey>basile-cagnazzo-2021-litescale</bibkey>
    </paper>
    <paper id="16">
      <title>Probabilistic Ensembles of Zero- and Few-Shot Learning Models for Emotion Classification</title>
      <author><first>Angelo</first><last>Basile</last></author>
      <author><first>Guillermo</first><last>Pérez-Torró</last></author>
      <author><first>Marc</first><last>Franco-Salvador</last></author>
      <pages>128–137</pages>
      <abstract>Emotion Classification is the task of automatically associating a text with a human emotion. State-of-the-art models are usually learned using annotated corpora or rely on hand-crafted affective lexicons. We present an emotion classification model that does not require a large annotated corpus to be competitive. We experiment with pretrained language models in both a zero-shot and few-shot configuration. We build several of such models and consider them as biased, noisy annotators, whose individual performance is poor. We aggregate the predictions of these models using a Bayesian method originally developed for modelling crowdsourced annotations. Next, we show that the resulting system performs better than the strongest individual model. Finally, we show that when trained on few labelled data, our systems outperform fully-supervised models.</abstract>
      <url hash="373b1be2">2021.ranlp-1.16</url>
      <bibkey>basile-etal-2021-probabilistic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="17">
      <title>Cross-Lingual Wolastoqey-<fixed-case>E</fixed-case>nglish Definition Modelling</title>
      <author><first>Diego</first><last>Bear</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>138–146</pages>
      <abstract>Definition modelling is the task of automatically generating a dictionary-style definition given a target word. In this paper, we consider cross-lingual definition generation. Specifically, we generate English definitions for Wolastoqey (Malecite-Passamaquoddy) words. Wolastoqey is an endangered, low-resource polysynthetic language. We hypothesize that sub-word representations based on byte pair encoding (Sennrich et al., 2016) can be leveraged to represent morphologically-complex Wolastoqey words and overcome the challenge of not having large corpora available for training. Our experimental results demonstrate that this approach outperforms baseline methods in terms of BLEU score. ﻿</abstract>
      <url hash="8e321b88">2021.ranlp-1.17</url>
      <bibkey>bear-cook-2021-cross</bibkey>
    </paper>
    <paper id="18">
      <title>Neural Network-Based Generation of Sport Summaries: A Preliminary Study</title>
      <author><first>David Stéphane</first><last>Belemkoabga</last></author>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <author><first>Abdallah</first><last>Essa</last></author>
      <author><first>Christophe</first><last>Rodrigues</last></author>
      <author><first>Kévin</first><last>Sylla</last></author>
      <pages>147–154</pages>
      <abstract>This paper presents a global summarization method for live sport commentaries for which we have a human-written summary available. This method is based on a neural generative summarizer. The amount of data available for training is limited compared to corpora commonly used by neural summarizers. We propose to help the summarizer to learn from a limited amount of data by limiting the entropy of the input texts. This step is performed by a classification into categories derived by a detailed analysis of the human-written summaries. We show that the filtering helps the summarization system to overcome the lack of resources. However, several improving points have emerged from this preliminary study, that we discuss and plan to implement in future work.</abstract>
      <url hash="aa09e8f9">2021.ranlp-1.18</url>
      <bibkey>belemkoabga-etal-2021-neural</bibkey>
    </paper>
    <paper id="19">
      <title>Split-and-Rephrase in a Cross-Lingual Manner: A Complete Pipeline</title>
      <author><first>Paulo</first><last>Berlanga Neto</last></author>
      <author><first>Evandro Eduardo Seron</first><last>Ruiz</last></author>
      <pages>155–164</pages>
      <abstract>Split-and-rephrase is a challenging task that promotes the transformation of a given complex input sentence into multiple shorter sentences retaining equivalent meaning. This rewriting approach conceptualizes that shorter sentences benefit human readers and improve NLP downstream tasks attending as a preprocessing step. This work presents a complete pipeline capable of performing the split-and-rephrase method in a cross-lingual manner. We trained sequence-to-sequence neural models as from English corpora and applied them to predict the transformations in English and Brazilian Portuguese sentences jointly with BERT’s masked language modeling. Contrary to traditional approaches that seek training models with extensive vocabularies, we present a non-trivial way to construct symbolic ones generalized solely by grammatical classes (POS tags) and their respective recurrences, reducing the amount of necessary training data. This pipeline contribution showed competitive results encouraging the expansion of the method to languages other than English.</abstract>
      <url hash="bec29466">2021.ranlp-1.19</url>
      <bibkey>berlanga-neto-ruiz-2021-split</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisplit">WikiSplit</pwcdataset>
    </paper>
    <paper id="20">
      <title>On the Contribution of Per-<fixed-case>ICD</fixed-case> Attention Mechanisms to Classify Health Records in Languages with Fewer Resources than <fixed-case>E</fixed-case>nglish</title>
      <author><first>Alberto</first><last>Blanco</last></author>
      <author><first>Sonja</first><last>Remmer</last></author>
      <author><first>Alicia</first><last>Pérez</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <author><first>Arantza</first><last>Casillas</last></author>
      <pages>165–172</pages>
      <abstract>We introduce a multi-label text classifier with per-label attention for the classification of Electronic Health Records according to the International Classification of Diseases. We apply the model on two Electronic Health Records datasets with Discharge Summaries in two languages with fewer resources than English, Spanish and Swedish. Our model leverages the BERT Multilingual model (specifically the Wikipedia, as the model have been trained with 104 languages, including Spanish and Swedish, with the largest Wikipedia dumps) to share the language modelling capabilities across the languages. With the per-label attention, the model can compute the relevance of each word from the EHR towards the prediction of each label. For the experimental framework, we apply 157 labels from Chapter XI – Diseases of the Digestive System of the ICD, which makes the attention especially important as the model has to discriminate between similar diseases. 1 <url>https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages</url></abstract>
      <url hash="d06ba45b">2021.ranlp-1.20</url>
      <bibkey>blanco-etal-2021-contribution</bibkey>
      <pwccode url="https://github.com/google-research/bert" additional="true">google-research/bert</pwccode>
    </paper>
    <paper id="21">
      <title>Can the Transformer Be Used as a Drop-in Replacement for <fixed-case>RNN</fixed-case>s in Text-Generating <fixed-case>GAN</fixed-case>s?</title>
      <author><first>Kevin</first><last>Blin</last></author>
      <author><first>Andrei</first><last>Kucharavy</last></author>
      <pages>173–181</pages>
      <abstract>In this paper we address the problem of fine-tuned text generation with a limited computational budget. For that, we use a well-performing text generative adversarial network (GAN) architecture - Diversity-Promoting GAN (DPGAN), and attempted a drop-in replacement of the LSTM layer with a self-attention-based Transformer layer in order to leverage their efficiency. The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance, quality and diversity of generated text and stability. Computational experiments suggested that a transformer architecture is unable to drop-in replace the LSTM layer, under-performing during the pre-training phase and undergoing a complete mode collapse during the GAN tuning phase. Our results suggest that the transformer architecture need to be adapted before it can be used as a replacement for RNNs in text-generating GANs.</abstract>
      <url hash="ae4edfc2">2021.ranlp-1.21</url>
      <bibkey>blin-kucharavy-2021-transformer</bibkey>
    </paper>
    <paper id="22">
      <title>Predicting the Factuality of Reporting of News Media Using Observations about User Attention in Their <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube Channels</title>
      <author><first>Krasimira</first><last>Bozhanova</last></author>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Maria</first><last>Castaldo</last></author>
      <author><first>Tommaso</first><last>Venturini</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>182–189</pages>
      <abstract>We propose a novel framework for predicting the factuality of reporting of news media outlets by studying the user attention cycles in their YouTube channels. In particular, we design a rich set of features derived from the temporal evolution of the number of views, likes, dislikes, and comments for a video, which we then aggregate to the channel level. We develop and release a dataset for the task, containing observations of user attention on YouTube channels for 489 news media. Our experiments demonstrate both complementarity and sizable improvements over state-of-the-art textual representations.</abstract>
      <url hash="858a220a">2021.ranlp-1.22</url>
      <bibkey>bozhanova-etal-2021-predicting</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>OCR</fixed-case> Processing of <fixed-case>S</fixed-case>wedish Historical Newspapers Using Deep Hybrid <fixed-case>CNN</fixed-case>–<fixed-case>LSTM</fixed-case> Networks</title>
      <author><first>Molly</first><last>Brandt Skelbye</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <pages>190–198</pages>
      <abstract>Deep CNN–LSTM hybrid neural networks have proven to improve the accuracy of Optical Character Recognition (OCR) models for different languages. In this paper we examine to what extent these networks improve the OCR accuracy rates on Swedish historical newspapers. By experimenting with the open source OCR engine Calamari, we are able to show that mixed deep CNN–LSTM hybrid models outperform previous models on the task of character recognition of Swedish historical newspapers spanning 1818–1848. We achieved an average character accuracy rate (CAR) of 97.43% which is a new state–of–the–art result on 19th century Swedish newspaper text. Our data, code and models are released under CC-BY licence.</abstract>
      <url hash="cdb32371">2021.ranlp-1.23</url>
      <bibkey>brandt-skelbye-dannells-2021-ocr</bibkey>
    </paper>
    <paper id="24">
      <title>A Psychologically Informed Part-of-Speech Analysis of Depression in Social Media</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Ioana R.</first><last>Podina</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>199–207</pages>
      <abstract>In this work, we provide an extensive part-of-speech analysis of the discourse of social media users with depression. Research in psychology revealed that depressed users tend to be self-focused, more preoccupied with themselves and ruminate more about their lives and emotions. Our work aims to make use of large-scale datasets and computational methods for a quantitative exploration of discourse. We use the publicly available depression dataset from the Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract part-of-speech features and several indices based on them. Our results reveal statistically significant differences between the depressed and non-depressed individuals confirming findings from the existing psychology literature. Our work provides insights regarding the way in which depressed individuals are expressing themselves on social media platforms, allowing for better-informed computational models to help monitor and prevent mental illnesses.</abstract>
      <url hash="41e9cd41">2021.ranlp-1.24</url>
      <bibkey>bucur-etal-2021-psychologically</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>I</fixed-case>n<fixed-case>F</fixed-case>o<fixed-case>BERT</fixed-case>: Zero-Shot Approach to Natural Language Understanding Using Contextualized Word Embedding</title>
      <author><first>Pavel</first><last>Burnyshev</last></author>
      <author><first>Andrey</first><last>Bout</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <pages>208–215</pages>
      <abstract>Natural language understanding is an important task in modern dialogue systems. It becomes more important with the rapid extension of the dialogue systems’ functionality. In this work, we present an approach to zero-shot transfer learning for the tasks of intent classification and slot-filling based on pre-trained language models. We use deep contextualized models feeding them with utterances and natural language descriptions of user intents to get text embeddings. These embeddings then used by a small neural network to produce predictions for intent and slot probabilities. This architecture achieves new state-of-the-art results in two zero-shot scenarios. One is a single language new skill adaptation and another one is a cross-lingual adaptation.</abstract>
      <url hash="9ed2b786">2021.ranlp-1.25</url>
      <bibkey>burnyshev-etal-2021-infobert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="26">
      <title>Active Learning for Assisted Corpus Construction: A Case Study in Knowledge Discovery from Biomedical Text</title>
      <author><first>Hian</first><last>Cañizares-Díaz</last></author>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Yoan</first><last>Gutiérrez</last></author>
      <author><first>Yudivián</first><last>Almeida Cruz</last></author>
      <author><first>Andres</first><last>Montoyo</last></author>
      <author><first>Rafael</first><last>Muñoz-Guillena</last></author>
      <pages>216–225</pages>
      <abstract>This paper presents an active learning approach that aims to reduce the human effort required during the annotation of natural language corpora composed of entities and semantic relations. Our approach assists human annotators by intelligently selecting the most informative sentences to annotate and then pre-annotating them with a few highly accurate entities and semantic relations. We define an uncertainty-based query strategy with a weighted density factor, using similarity metrics based on sentence embeddings. As a case study, we evaluate our approach via simulation in a biomedical corpus and estimate the potential reduction in total annotation time. Experimental results suggest that the query strategy reduces by between 35% and 40% the number of sentences that must be manually annotated to develop systems able to reach a target F1 score, while the pre-annotation strategy produces an additional 24% reduction in the total annotation time. Overall, our preliminary experiments suggest that as much as 60% of the annotation time could be saved while producing corpora that have the same usefulness for training machine learning algorithms. An open-source computational tool that implements the aforementioned strategies is presented and published online for the research community.</abstract>
      <url hash="030279a8">2021.ranlp-1.26</url>
      <bibkey>canizares-diaz-etal-2021-active</bibkey>
    </paper>
    <paper id="27">
      <title>Unsupervised Text Style Transfer with Content Embeddings</title>
      <author><first>Keith</first><last>Carlson</last></author>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Daniel</first><last>Rockmore</last></author>
      <pages>226–233</pages>
      <abstract>The style transfer task (here style is used in a broad “authorial” sense with many aspects including register, sentence structure, and vocabulary choice) takes text input and rewrites it in a specified target style preserving the meaning, but altering the style of the source text to match that of the target. Much of the existing research on this task depends on the use of parallel datasets. In this work we employ recent results in unsupervised cross-lingual language modeling (XLM) and machine translation to effect style transfer while treating the input data as unaligned. First, we show that adding “content embeddings” to the XLM which capture human-specified groupings of subject matter can improve performance over the baseline model. Evaluation of style transfer has often relied on metrics designed for machine translation which have received criticism of their suitability for this task. As a second contribution, we propose the use of a suite of classical stylometrics as a useful complement for evaluation. We select a few such measures and include these in the analysis of our results.</abstract>
      <url hash="dfb9a47c">2021.ranlp-1.27</url>
      <bibkey>carlson-etal-2021-unsupervised</bibkey>
    </paper>
    <paper id="28">
      <title>Evaluating Recognizing Question Entailment Methods for a <fixed-case>P</fixed-case>ortuguese Community Question-Answering System about Diabetes Mellitus</title>
      <author><first>Thiago</first><last>Castro Ferreira</last></author>
      <author><first>João</first><last>Victor de Pinho Costa</last></author>
      <author><first>Isabela</first><last>Rigotto</last></author>
      <author><first>Vitoria</first><last>Portella</last></author>
      <author><first>Gabriel</first><last>Frota</last></author>
      <author><first>Ana</first><last>Luisa A. R. Guimarães</last></author>
      <author><first>Adalberto</first><last>Penna</last></author>
      <author><first>Isabela</first><last>Lee</last></author>
      <author><first>Tayane</first><last>A. Soares</last></author>
      <author><first>Sophia</first><last>Rolim</last></author>
      <author><first>Rossana</first><last>Cunha</last></author>
      <author><first>Celso</first><last>França</last></author>
      <author><first>Ariel</first><last>Santos</last></author>
      <author><first>Rivaney</first><last>F. Oliveira</last></author>
      <author><first>Abisague</first><last>Langbehn</last></author>
      <author><first>Daniel</first><last>Hasan Dalip</last></author>
      <author><first>Marcos</first><last>André Gonçalves</last></author>
      <author><first>Rodrigo</first><last>Bastos Fóscolo</last></author>
      <author><first>Adriana</first><last>Pagano</last></author>
      <pages>234–243</pages>
      <abstract>This study describes the development of a Portuguese Community-Question Answering benchmark in the domain of Diabetes Mellitus using a Recognizing Question Entailment (RQE) approach. Given a premise question, RQE aims to retrieve semantically similar, already answered, archived questions. We build a new Portuguese benchmark corpus with 785 pairs between premise questions and archived answered questions marked with relevance judgments by medical experts. Based on the benchmark corpus, we leveraged and evaluated several RQE approaches ranging from traditional information retrieval methods to novel large pre-trained language models and ensemble techniques using learn-to-rank approaches. Our experimental results show that a supervised transformer-based method trained with multiple languages and for multiple tasks (MUSE) outperforms the alternatives. Our results also show that ensembles of methods (stacking) as well as a traditional (light) information retrieval method (BM25) can produce competitive results. Finally, among the tested strategies, those that exploit only the question (not the answer), provide the best effectiveness-efficiency trade-off. Code is publicly available.</abstract>
      <url hash="cf99b93f">2021.ranlp-1.28</url>
      <bibkey>castro-ferreira-etal-2021-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/medquad">MedQuAD</pwcdataset>
    </paper>
    <paper id="29">
      <title>On the Usability of Transformers-based Models for a <fixed-case>F</fixed-case>rench Question-Answering Task</title>
      <author><first>Oralie</first><last>Cattan</last></author>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>244–255</pages>
      <abstract>For many tasks, state-of-the-art results have been achieved with Transformer-based architectures, resulting in a paradigmatic shift in practices from the use of task-specific architectures to the fine-tuning of pre-trained language models. The ongoing trend consists in training models with an ever-increasing amount of data and parameters, which requires considerable resources. It leads to a strong search to improve resource efficiency based on algorithmic and hardware improvements evaluated only for English. This raises questions about their usability when applied to small-scale learning problems, for which a limited amount of training data is available, especially for under-resourced languages tasks. The lack of appropriately sized corpora is a hindrance to applying data-driven and transfer learning-based approaches with strong instability cases. In this paper, we establish a state-of-the-art of the efforts dedicated to the usability of Transformer-based models and propose to evaluate these improvements on the question-answering performances of French language which have few resources. We address the instability relating to data scarcity by investigating various training strategies with data augmentation, hyperparameters optimization and cross-lingual transfer. We also introduce a new compact model for French FrALBERT which proves to be competitive in low-resource settings.</abstract>
      <url hash="09c969d4">2021.ranlp-1.29</url>
      <bibkey>cattan-etal-2021-usability</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fquad">FQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/french-wikipedia">French Wikipedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="30">
      <title>Classification of Code-Mixed Text Using Capsule Networks</title>
      <author><first>Shanaka</first><last>Chathuranga</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <pages>256–263</pages>
      <abstract>A major challenge in analysing social me-dia data belonging to languages that use non-English script is its code-mixed nature. Recentresearch has presented state-of-the-art contex-tual embedding models (both monolingual s.a.BERT and multilingual s.a.XLM-R) as apromising approach. In this paper, we showthat the performance of such embedding mod-els depends on multiple factors, such as thelevel of code-mixing in the dataset, and thesize of the training dataset. We empiricallyshow that a newly introduced Capsule+biGRUclassifier could outperform a classifier built onthe English-BERT as well as XLM-R just witha training dataset of about 6500 samples forthe Sinhala-English code-mixed data.</abstract>
      <url hash="16330b84">2021.ranlp-1.30</url>
      <bibkey>chathuranga-ranathunga-2021-classification</bibkey>
    </paper>
    <paper id="31">
      <title>Character-based <fixed-case>T</fixed-case>hai Word Segmentation with Multiple Attentions</title>
      <author><first>Thodsaporn</first><last>Chay-intr</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>264–273</pages>
      <abstract>Character-based word-segmentation models have been extensively applied to agglutinative languages, including Thai, due to their high performance. These models estimate word boundaries from a character sequence. However, a character unit in sequences has no essential meaning, compared with word, subword, and character cluster units. We propose a Thai word-segmentation model that uses various types of information, including words, subwords, and character clusters, from a character sequence. Our model applies multiple attentions to refine segmentation inferences by estimating the significant relationships among characters and various unit types. The experimental results indicate that our model can outperform other state-of-the-art Thai word-segmentation models.</abstract>
      <url hash="d2d99700">2021.ranlp-1.31</url>
      <bibkey>chay-intr-etal-2021-character</bibkey>
      <pwccode url="https://github.com/tchayintr/thwcc-attn" additional="false">tchayintr/thwcc-attn</pwccode>
    </paper>
    <paper id="32">
      <title>Are Language-Agnostic Sentence Representations Actually Language-Agnostic?</title>
      <author><first>Yu</first><last>Chen</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>274–280</pages>
      <abstract>With the emergence of pre-trained multilingual models, multilingual embeddings have been widely applied in various natural language processing tasks. Language-agnostic models provide a versatile way to convert linguistic units from different languages into a shared vector representation space. The relevant work on multilingual sentence embeddings has reportedly reached low error rate in cross-lingual similarity search tasks. In this paper, we apply the pre-trained embedding models and the cross-lingual similarity search task in diverse scenarios, and observed large discrepancy in results in comparison to the original paper. Our findings on cross-lingual similarity search with different newly constructed multilingual datasets show not only correlation with observable language similarities but also strong influence from factors such as translation paths, which limits the interpretation of the language-agnostic property of the LASER model. %</abstract>
      <url hash="69246c56">2021.ranlp-1.32</url>
      <bibkey>chen-avgustinova-2021-language</bibkey>
    </paper>
    <paper id="33">
      <title>Investigating Dominant Word Order on <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies with Graph Rewriting</title>
      <author><first>Hee-Soo</first><last>Choi</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>281–290</pages>
      <abstract>This paper details experiments we performed on the Universal Dependencies 2.7 corpora in order to investigate the dominant word order in the available languages. For this purpose, we used a graph rewriting tool, GREW, which allowed us to go beyond the surface annotations and identify the implicit subjects. We first measured the distribution of the six different word orders (SVO, SOV, VSO, VOS, OVS, OSV) in the corpora and investigated when there was a significant difference in the corpora within a given language. Then, we compared the obtained results with information provided in the WALS database (Dryer and Haspelmath, 2013) and in ( ̈Ostling, 2015). Finally, we examined the impact of using a graph rewriting tool for this task. The tools and resources used for this research are all freely available.</abstract>
      <url hash="ed424de7">2021.ranlp-1.33</url>
      <bibkey>choi-etal-2021-investigating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="34">
      <title><fixed-case>RED</fixed-case>: A Novel Dataset for <fixed-case>R</fixed-case>omanian Emotion Detection from Tweets</title>
      <author><first>Alexandra</first><last>Ciobotaru</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>291–300</pages>
      <abstract>In Romanian language there are some resources for automatic text comprehension, but for Emotion Detection, not lexicon-based, there are none. To cover this gap, we extracted data from Twitter and created the first dataset containing tweets annotated with five types of emotions: joy, fear, sadness, anger and neutral, with the intent of being used for opinion mining and analysis tasks. In this article we present some features of our novel dataset, and create a benchmark to achieve the first supervised machine learning model for automatic Emotion Detection in Romanian short texts. We investigate the performance of four classical machine learning models: Multinomial Naive Bayes, Logistic Regression, Support Vector Classification and Linear Support Vector Classification. We also investigate more modern approaches like fastText, which makes use of subword information. Lastly, we fine-tune the Romanian BERT for text classification and our experiments show that the BERT-based model has the best performance for the task of Emotion Detection from Romanian tweets. Keywords: Emotion Detection, Twitter, Romanian, Supervised Machine Learning</abstract>
      <url hash="3c209b17">2021.ranlp-1.34</url>
      <bibkey>ciobotaru-dinu-2021-red</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="35">
      <title>Assessing the Eligibility of Backtranslated Samples Based on Semantic Similarity for the Paraphrase Identification Task</title>
      <author><first>Jean-Philippe</first><last>Corbeil</last></author>
      <author><first>Hadi</first><last>Abdi Ghavidel</last></author>
      <pages>301–308</pages>
      <abstract>In the domain of natural language augmentation, the eligibility of generated samples remains not well understood. To gather insights around this eligibility issue, we apply a transformer-based similarity calculation within the BET framework based on backtranslation, in the context of automated paraphrase detection. While providing a rigorous statistical foundation to BET, we push their results by analyzing statistically the impacts of the level of qualification, and several sample sizes. We conducted a vast amount of experiments on the MRPC corpus using six pre-trained models: BERT, XLNet, Albert, RoBERTa, Electra, and DeBerta. We show that our method improves significantly these “base” models while using only a fraction of the corpus. Our results suggest that using some of those smaller pre-trained models, namely RoBERTa base and Electra base, helps us reach F1 scores very close to their large counterparts, as reported on the GLUE benchmark. On top of acting as a regularizer, the proposed method is efficient in dealing with data scarcity with improvements of around 3% in F1 score for most pre-trained models, and more than 7.5% in the case of Electra.</abstract>
      <url hash="aa5c2077">2021.ranlp-1.35</url>
      <bibkey>corbeil-abdi-ghavidel-2021-assessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="36">
      <title>Fine-tuning Neural Language Models for Multidimensional Opinion Mining of <fixed-case>E</fixed-case>nglish-<fixed-case>M</fixed-case>altese Social Data</title>
      <author><first>Keith</first><last>Cortis</last></author>
      <author><first>Kanishk</first><last>Verma</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>309–314</pages>
      <abstract>This paper presents multidimensional Social Opinion Mining on user-generated content gathered from newswires and social networking services in three different languages: English —a high-resourced language, Maltese —a low-resourced language, and Maltese-English —a code-switched language. Multiple fine-tuned neural classification language models which cater for the i) English, Maltese and Maltese-English languages as well as ii) five different social opinion dimensions, namely subjectivity, sentiment polarity, emotion, irony and sarcasm, are presented. Results per classification model for each social opinion dimension are discussed.</abstract>
      <url hash="9e3e4606">2021.ranlp-1.36</url>
      <bibkey>cortis-etal-2021-fine</bibkey>
    </paper>
    <paper id="37">
      <title>Towards an Etymological Map of <fixed-case>R</fixed-case>omanian</title>
      <author><first>Alina Maria</first><last>Cristea</last></author>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <author><first>Simona</first><last>Georgescu</last></author>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Laurentiu</first><last>Zoicas</last></author>
      <pages>315–323</pages>
      <abstract>In this paper we investigate the etymology of Romanian words. We start from the Romanian lexicon and automatically extract information from multiple etymological dictionaries. We evaluate the results and perform extensive quantitative and qualitative analyses with the goal of building an etymological map of the language.</abstract>
      <url hash="0b01377b">2021.ranlp-1.37</url>
      <bibkey>cristea-etal-2021-towards</bibkey>
    </paper>
    <paper id="38">
      <title>A Syntax-Aware Edit-based System for Text Simplification</title>
      <author><first>Oscar M.</first><last>Cumbicus-Pineda</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <pages>324–334</pages>
      <abstract>Edit-based text simplification systems have attained much attention in recent years due to their ability to produce simplification solutions that are interpretable, as well as requiring less training examples compared to traditional seq2seq systems. Edit-based systems learn edit operations at a word level, but it is well known that many of the operations performed when simplifying text are of a syntactic nature. In this paper we propose to add syntactic information into a well known edit-based system. We extend the system with a graph convolutional network module that mimics the dependency structure of the sentence, thus giving the model an explicit representation of syntax. We perform a series of experiments in English, Spanish and Italian, and report improvements of the state of the art in four out of five datasets. Further analysis shows that syntactic information is always beneficial, and suggest that syntax is more helpful in complex sentences.</abstract>
      <url hash="d0a5c594">2021.ranlp-1.38</url>
      <bibkey>cumbicus-pineda-etal-2021-syntax</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
    </paper>
    <paper id="39">
      <title>On Generating Fact-Infused Question Variations</title>
      <author><first>Arthur</first><last>Deschamps</last></author>
      <author><first>Sujatha Das</first><last>Gollapalli</last></author>
      <author><first>See-Kiong</first><last>Ng</last></author>
      <pages>335–345</pages>
      <abstract>To fully model human-like ability to ask questions, automatic question generation (QG) models must be able to produce multiple expressions of the same question with different levels of detail. Unfortunately, existing datasets available for learning QG do not include paraphrases or question variations affecting a model’s ability to learn this capability. We present FIRS, a dataset containing human-generated fact-infused rewrites of questions from the widely-used SQuAD dataset to address this limitation. Questions in FIRS were obtained by combining a given question with facts of entities referenced in the question. We study a double encoder-decoder model, Fact-Infused Question Generator (FIQG), for learning to generate fact-infused questions from a given question. Experimental results show that FIQG effectively incorporates information from facts to add more detail to a given question. To the best of our knowledge, ours is the first study to present fact-infusion as a novel form of question paraphrasing.</abstract>
      <url hash="d696a278">2021.ranlp-1.39</url>
      <bibkey>deschamps-etal-2021-generating</bibkey>
      <pwccode url="https://github.com/nus-ids/ranlp21-fiqv" additional="false">nus-ids/ranlp21-fiqv</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="40">
      <title>Event Prominence Extraction Combining a Knowledge-Based Syntactic Parser and a <fixed-case>BERT</fixed-case> Classifier for <fixed-case>D</fixed-case>utch</title>
      <author><first>Thierry</first><last>Desot</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>346–357</pages>
      <abstract>A core task in information extraction is event detection that identifies event triggers in sentences that are typically classified into event types. In this study an event is considered as the unit to measure diversity and similarity in news articles in the framework of a news recommendation system. Current typology-based event detection approaches fail to handle the variety of events expressed in real-world situations. To overcome this, we aim to perform event salience classification and explore whether a transformer model is capable of classifying new information into less and more general prominence classes. After comparing a Support Vector Machine (SVM) baseline and our transformer-based classifier performances on several event span formats, we conceived multi-word event spans as syntactic clauses. Those are fed into our prominence classifier which is fine-tuned on pre-trained Dutch BERT word embeddings. On top of that we outperform a pipeline of a Conditional Random Field (CRF) approach to event-trigger word detection and the BERT-based classifier. To the best of our knowledge we present the first event extraction approach that combines an expert-based syntactic parser with a transformer-based classifier for Dutch.</abstract>
      <url hash="6ea7b948">2021.ranlp-1.40</url>
      <bibkey>desot-etal-2021-event</bibkey>
    </paper>
    <paper id="41">
      <title>Automatic Detection and Classification of Mental Illnesses from General Social Media Texts</title>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Andreea-Codrina</first><last>Moldovan</last></author>
      <pages>358–366</pages>
      <abstract>Mental health is getting more and more attention recently, depression being a very common illness nowadays, but also other disorders like anxiety, obsessive-compulsive disorders, feeding disorders, autism, or attention-deficit/hyperactivity disorders. The huge amount of data from social media and the recent advances of deep learning models provide valuable means to automatically detecting mental disorders from plain text. In this article, we experiment with state-of-the-art methods on the SMHD mental health conditions dataset from Reddit (Cohan et al., 2018). Our contribution is threefold: using a dataset consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models: BERT, RoBERTa and XLNET. We double the baseline established by Cohan et al. (2018), on just a sample of their dataset. We improve the results obtained by Jiang et al. (2020) on post-level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas depression had the lowest F1 score, probably because depression is more difficult to identify in linguistic acts.</abstract>
      <url hash="bcbf0f49">2021.ranlp-1.41</url>
      <bibkey>dinu-moldovan-2021-automatic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smhd">SMHD</pwcdataset>
    </paper>
    <paper id="42">
      <title>A Pre-trained Transformer and <fixed-case>CNN</fixed-case> Model with Joint Language <fixed-case>ID</fixed-case> and Part-of-Speech Tagging for Code-Mixed Social-Media Text</title>
      <author><first>Suman</first><last>Dowlagar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>367–374</pages>
      <abstract>Code-mixing (CM) is a frequently observed phenomenon that uses multiple languages in an utterance or sentence. There are no strict grammatical constraints observed in code-mixing, and it consists of non-standard variations of spelling. The linguistic complexity resulting from the above factors made the computational analysis of the code-mixed language a challenging task. Language identification (LI) and part of speech (POS) tagging are the fundamental steps that help analyze the structure of the code-mixed text. Often, the LI and POS tagging tasks are interdependent in the code-mixing scenario. We project the problem of dealing with multilingualism and grammatical structure while analyzing the code-mixed sentence as a joint learning task. In this paper, we jointly train and optimize language detection and part of speech tagging models in the code-mixed scenario. We used a Transformer with convolutional neural network architecture. We train a joint learning method by combining POS tagging and LI models on code-mixed social media text obtained from the ICON shared task.</abstract>
      <url hash="83b14d89">2021.ranlp-1.42</url>
      <bibkey>dowlagar-mamidi-2021-pre</bibkey>
    </paper>
    <paper id="43">
      <title>Tracing Source Language Interference in Translation with Graph-Isomorphism Measures</title>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>375–385</pages>
      <abstract>Previous research has used linguistic features to show that translations exhibit traces of source language interference and that phylogenetic trees between languages can be reconstructed from the results of translations into the same language. Recent research has shown that instances of translationese (source language interference) can even be detected in embedding spaces, comparing embeddings spaces of original language data with embedding spaces resulting from translations into the same language, using a simple Eigenvector-based divergence from isomorphism measure. To date, it remains an open question whether alternative graph-isomorphism measures can produce better results. In this paper, we (i) explore Gromov-Hausdorff distance, (ii) present a novel spectral version of the Eigenvector-based method, and (iii) evaluate all approaches against a broad linguistic typological database (URIEL). We show that language distances resulting from our spectral isomorphism approaches can reproduce genetic trees on a par with previous work without requiring any explicit linguistic information and that the results can be extended to non-Indo-European languages. Finally, we show that the methods are robust under a variety of modeling conditions.</abstract>
      <url hash="bc1d80ed">2021.ranlp-1.43</url>
      <bibkey>dutta-chowdhury-etal-2021-tracing</bibkey>
    </paper>
    <paper id="44">
      <title>Decoupled Transformer for Scalable Inference in Open-domain Question Answering</title>
      <author><first>Haytham</first><last>Elfdaeel</last></author>
      <author><first>Stanislav</first><last>Peshterliev</last></author>
      <pages>386–393</pages>
      <abstract>Large transformer models, such as BERT, achieve state-of-the-art results in machine reading comprehension (MRC) for open-domain question answering (QA). However, transformers have a high computational cost for inference which makes them hard to apply to online QA systems for applications like voice assistants. To reduce computational cost and latency, we propose decoupling the transformer MRC model into input-component and cross-component. The decoupling allows for part of the representation computation to be performed offline and cached for online use. To retain the decoupled transformer accuracy, we devised a knowledge distillation objective from a standard transformer model. Moreover, we introduce learned representation compression layers which help reduce by four times the storage requirement for the cache. In experiments on the SQUAD 2.0 dataset, a decoupled transformer reduces the computational cost and latency of open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a standard transformer.</abstract>
      <url hash="0ed36cde">2021.ranlp-1.44</url>
      <bibkey>elfdaeel-peshterliev-2021-decoupled</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="45">
      <title>Towards Task-Agnostic Privacy- and Utility-Preserving Models</title>
      <author><first>Yaroslav</first><last>Emelyanov</last></author>
      <pages>394–401</pages>
      <abstract>Modern deep learning models for natural language processing rely heavily on large amounts of annotated texts. However, obtaining such texts may be difficult when they contain personal or confidential information, for example, in health or legal domains. In this work, we propose a method of de-identifying free-form text documents by carefully redacting sensitive data in them. We show that our method preserves data utility for text classification, sequence labeling and question answering tasks.</abstract>
      <url hash="e7cc2394">2021.ranlp-1.45</url>
      <bibkey>emelyanov-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="46">
      <title>Knowledge Discovery in <fixed-case>COVID</fixed-case>-19 Research Literature</title>
      <author><first>Ernesto L.</first><last>Estevanell-Valladares</last></author>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Yoan</first><last>Gutierrez</last></author>
      <author><first>Andres</first><last>Montoyo</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <author><first>Yudivián</first><last>Almeida Cruz</last></author>
      <pages>402–410</pages>
      <abstract>This paper presents the preliminary results of an ongoing project that analyzes the growing body of scientific research published around the COVID-19 pandemic. In this research, a general-purpose semantic model is used to double annotate a batch of 500 sentences that were manually selected from the CORD-19 corpus. Afterwards, a baseline text-mining pipeline is designed and evaluated via a large batch of 100,959 sentences. We present a qualitative analysis of the most interesting facts automatically extracted and highlight possible future lines of development. The preliminary results show that general-purpose semantic models are a useful tool for discovering fine-grained knowledge in large corpora of scientific documents.</abstract>
      <url hash="bb0bc177">2021.ranlp-1.46</url>
      <bibkey>estevanell-valladares-etal-2021-knowledge</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
    </paper>
    <paper id="47">
      <title>Online Learning over Time in Adaptive Neural Machine Translation</title>
      <author><first>Thierry</first><last>Etchegoyhen</last></author>
      <author><first>David</first><last>Ponce</last></author>
      <author><first>Harritxu</first><last>Gete</last></author>
      <author><first>Victor</first><last>Ruiz</last></author>
      <pages>411–420</pages>
      <abstract>Adaptive Machine Translation purports to dynamically include user feedback to improve translation quality. In a post-editing scenario, user corrections of machine translation output are thus continuously incorporated into translation models, reducing or eliminating repetitive error editing and increasing the usefulness of automated translation. In neural machine translation, this goal may be achieved via online learning approaches, where network parameters are updated based on each new sample. This type of adaptation typically requires higher learning rates, which can affect the quality of the models over time. Alternatively, less aggressive online learning setups may preserve model stability, at the cost of reduced adaptation to user-generated corrections. In this work, we evaluate different online learning configurations over time, measuring their impact on user-generated samples, as well as separate in-domain and out-of-domain datasets. Results in two different domains indicate that mixed approaches combining online learning with periodic batch fine-tuning might be needed to balance the benefits of online learning with model stability.</abstract>
      <url hash="78a594ee">2021.ranlp-1.47</url>
      <bibkey>etchegoyhen-etal-2021-online</bibkey>
    </paper>
    <paper id="48">
      <title>Improving Character-Aware Neural Language Model by Warming up Character Encoder under Skip-gram Architecture</title>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Chenlong</first><last>Hu</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>421–427</pages>
      <abstract>Character-aware neural language models can capture the relationship between words by exploiting character-level information and are particularly effective for languages with rich morphology. However, these models are usually biased towards information from surface forms. To alleviate this problem, we propose a simple and effective method to improve a character-aware neural language model by forcing a character encoder to produce word-based embeddings under Skip-gram architecture in a warm-up step without extra training data. We empirically show that the resulting character-aware neural language model achieves obvious improvements of perplexity scores on typologically diverse languages, that contain many low-frequency or unseen words.</abstract>
      <url hash="d42325b2">2021.ranlp-1.48</url>
      <bibkey>feng-etal-2021-improving</bibkey>
    </paper>
    <paper id="49">
      <title>Interpretable Identification of Cybersecurity Vulnerabilities from News Articles</title>
      <author><first>Pierre</first><last>Frode de la Foret</last></author>
      <author><first>Stefan</first><last>Ruseti</last></author>
      <author><first>Cristian</first><last>Sandescu</last></author>
      <author><first>Mihai</first><last>Dascalu</last></author>
      <author><first>Sebastien</first><last>Travadel</last></author>
      <pages>428–436</pages>
      <abstract>With the increasing adoption of technology, more and more systems become target to information security breaches. In terms of readily identifying zero-day vulnerabilities, a substantial number of news outlets and social media accounts reveal emerging vulnerabilities and threats. However, analysts often spend a lot of time looking through these decentralized sources of information in order to ensure up-to-date countermeasures and patches applicable to their organisation’s information systems. Various automated processing pipelines grounded in Natural Language Processing techniques for text classification were introduced for the early identification of vulnerabilities starting from Open-Source Intelligence (OSINT) data, including news websites, blogs, and social media. In this study, we consider a corpus of more than 1600 labeled news articles, and introduce an interpretable approach to the subject of cyberthreat early detection. In particular, an interpretable classification is performed using the Longformer architecture alongside prototypes from the ProSeNet structure, after performing a preliminary analysis on the Transformer’s encoding capabilities. The best interpretable architecture achieves an 88% F2-Score, arguing for the system’s applicability in real-life monitoring conditions of OSINT data.</abstract>
      <url hash="3df423c2">2021.ranlp-1.49</url>
      <bibkey>frode-de-la-foret-etal-2021-interpretable</bibkey>
    </paper>
    <paper id="50">
      <title>Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of <fixed-case>M</fixed-case>arathi</title>
      <author><first>Saurabh Sampatrao</first><last>Gaikwad</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <pages>437–443</pages>
      <abstract>The widespread presence of offensive language on social media motivated the development of systems capable of recognizing such content automatically. Apart from a few notable exceptions, most research on automatic offensive language identification has dealt with English. To address this shortcoming, we introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first dataset of its kind compiled for Marathi, thus opening a new domain for research in low-resource Indo-Aryan languages. We present results from several machine learning experiments on this dataset, including zero-short and other transfer learning experiments on state-of-the-art cross-lingual transformers from existing data in Bengali, English, and Hindi.</abstract>
      <url hash="0805223a">2021.ranlp-1.50</url>
      <bibkey>gaikwad-etal-2021-cross</bibkey>
      <pwccode url="https://github.com/tharindudr/mold" additional="false">tharindudr/mold</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mold">MOLD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="51">
      <title>Relying on Discourse Analysis to Answer Complex Questions by Neural Machine Reading Comprehension</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <pages>444–453</pages>
      <abstract>Machine reading comprehension (MRC) is one of the most challenging tasks in natural language processing domain. Recent state-of-the-art results for MRC have been achieved with the pre-trained language models, such as BERT and its modifications. Despite the high performance of these models, they still suffer from the inability to retrieve correct answers from the detailed and lengthy passages. In this work, we introduce a novel scheme for incorporating the discourse structure of the text into a self-attention network, and, thus, enrich the embedding obtained from the standard BERT encoder with the additional linguistic knowledge. We also investigate the influence of different types of linguistic information on the model’s ability to answer complex questions that require deep understanding of the whole text. Experiments performed on the SQuAD benchmark and more complex question answering datasets have shown that linguistic enhancing boosts the performance of the standard BERT model significantly.</abstract>
      <url hash="eb8a2048">2021.ranlp-1.51</url>
      <bibkey>galitsky-etal-2021-relying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="52">
      <title>A Dynamic Head Importance Computation Mechanism for Neural Machine Translation</title>
      <author><first>Akshay</first><last>Goindani</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>454–462</pages>
      <abstract>Multiple parallel attention mechanisms that use multiple attention heads facilitate greater performance of the Transformer model for various applications e.g., Neural Machine Translation (NMT), text classification. In multi-head attention mechanism, different heads attend to different parts of the input. However, the limitation is that multiple heads might attend to the same part of the input, resulting in multiple heads being redundant. Thus, the model resources are under-utilized. One approach to avoid this is to prune least important heads based on certain importance score. In this work, we focus on designing a Dynamic Head Importance Computation Mechanism (DHICM) to dynamically calculate the importance of a head with respect to the input. Our insight is to design an additional attention layer together with multi-head attention, and utilize the outputs of the multi-head attention along with the input, to compute the importance for each head. Additionally, we add an extra loss function to prevent the model from assigning same score to all heads, to identify more important heads and improvise performance. We analyzed performance of DHICM for NMT with different languages. Experiments on different datasets show that DHICM outperforms traditional Transformer-based approach by large margin, especially, when less training data is available.</abstract>
      <url hash="9b38a7cb">2021.ranlp-1.52</url>
      <bibkey>goindani-shrivastava-2021-dynamic</bibkey>
    </paper>
    <paper id="53">
      <title>Syntax and Themes: How Context Free Grammar Rules and Semantic Word Association Influence Book Success</title>
      <author><first>Henry</first><last>Gorelick</last></author>
      <author><first>Biddut Sarker</first><last>Bijoy</last></author>
      <author><first>Syeda</first><last>Jannatus Saba</last></author>
      <author><first>Sudipta</first><last>Kar</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last></author>
      <pages>463–474</pages>
      <abstract>In this paper, we attempt to improve upon the state-of-the-art in predicting a novel’s success by modeling the lexical semantic relationships of its contents. We created the largest dataset used in such a project containing lexical data from 17,962 books from Project Gutenberg. We utilized domain specific feature reduction techniques to implement the most accurate models to date for predicting book success, with our best model achieving an average accuracy of 94.0%. By analyzing the model parameters, we extracted the successful semantic relationships from books of 12 different genres. We finally mapped those semantic relations to a set of themes, as defined in Roget’s Thesaurus and discovered the themes that successful books of a given genre prioritize. At the end of the paper, we further showed that our model demonstrate similar performance for book success prediction even when Goodreads rating was used instead of download count to measure success.</abstract>
      <url hash="e7f9fc2e">2021.ranlp-1.53</url>
      <bibkey>gorelick-etal-2021-syntax</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>S</fixed-case>ocial<fixed-case>V</fixed-case>is<fixed-case>TUM</fixed-case>: An Interactive Visualization Toolkit for Correlated Neural Topic Models on Social Media Opinion Mining</title>
      <author><first>Gerhard</first><last>Hagerer</last></author>
      <author><first>Martin</first><last>Kirchhoff</last></author>
      <author><first>Hannah</first><last>Danner</last></author>
      <author><first>Robert</first><last>Pesch</last></author>
      <author><first>Mainak</first><last>Ghosh</last></author>
      <author><first>Archishman</first><last>Roy</last></author>
      <author><first>Jiaxi</first><last>Zhao</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>475–482</pages>
      <abstract>Recent research in opinion mining proposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social media texts using SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of large text collections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online.</abstract>
      <url hash="e217a9bc">2021.ranlp-1.54</url>
      <bibkey>hagerer-etal-2021-socialvistum</bibkey>
    </paper>
    <paper id="55">
      <title>Apples to Apples: A Systematic Evaluation of Topic Models</title>
      <author><first>Ismail</first><last>Harrando</last></author>
      <author><first>Pasquale</first><last>Lisena</last></author>
      <author><first>Raphael</first><last>Troncy</last></author>
      <pages>483–493</pages>
      <abstract>From statistical to neural models, a wide variety of topic modelling algorithms have been proposed in the literature. However, because of the diversity of datasets and metrics, there have not been many efforts to systematically compare their performance on the same benchmarks and under the same conditions. In this paper, we present a selection of 9 topic modelling techniques from the state of the art reflecting a diversity of approaches to the task, an overview of the different metrics used to compare their performance, and the challenges of conducting such a comparison. We empirically evaluate the performance of these models on different settings reflecting a variety of real-life conditions in terms of dataset size, number of topics, and distribution of topics, following identical preprocessing and evaluation processes. Using both metrics that rely on the intrinsic characteristics of the dataset (different coherence metrics), as well as external knowledge (word embeddings and ground-truth topic labels), our experiments reveal several shortcomings regarding the common practices in topic models evaluation.</abstract>
      <url hash="52778976">2021.ranlp-1.55</url>
      <bibkey>harrando-etal-2021-apples</bibkey>
      <pwccode url="https://github.com/d2klab/tomodapi" additional="false">d2klab/tomodapi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="56">
      <title>Claim Verification Using a Multi-<fixed-case>GAN</fixed-case> Based Model</title>
      <author><first>Amartya</first><last>Hatua</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <author><first>Rakesh</first><last>Verma</last></author>
      <pages>494–503</pages>
      <abstract>This article describes research on claim verification carried out using a multiple GAN-based model. The proposed model consists of three pairs of generators and discriminators. The generator and discriminator pairs are responsible for generating synthetic data for supported and refuted claims and claim labels. A theoretical discussion about the proposed model is provided to validate the equilibrium state of the model. The proposed model is applied to the FEVER dataset, and a pre-trained language model is used for the input text data. The synthetically generated data helps to gain information that improves classification performance over state of the art baselines. The respective F1 scores after applying the proposed method on FEVER 1.0 and FEVER 2.0 datasets are 0.65+-0.018 and 0.65+-0.051.</abstract>
      <url hash="d58d01a0">2021.ranlp-1.56</url>
      <bibkey>hatua-etal-2021-claim</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="57">
      <title>Semi-Supervised and Unsupervised Sense Annotation via Translations</title>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <author><first>Yixing</first><last>Luan</last></author>
      <author><first>Arnob</first><last>Mallik</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <pages>504–513</pages>
      <abstract>Acquisition of multilingual training data continues to be a challenge in word sense disambiguation (WSD). To address this problem, unsupervised approaches have been proposed to automatically generate sense annotations for training supervised WSD systems. We present three new methods for creating sense-annotated corpora which leverage translations, parallel bitexts, lexical resources, as well as contextual and synset embeddings. Our semi-supervised method applies machine translation to transfer existing sense annotations to other languages. Our two unsupervised methods refine sense annotations produced by a knowledge-based WSD system via lexical translations in a parallel corpus. We obtain state-of-the-art results on standard WSD benchmarks.</abstract>
      <url hash="1aff20b4">2021.ranlp-1.57</url>
      <bibkey>hauer-etal-2021-semi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="58">
      <title>Personality Predictive Lexical Cues and Their Correlations</title>
      <author><first>Xiaoli</first><last>He</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>514–523</pages>
      <abstract>In recent years, a number of studies have used linear models for personality prediction based on text. In this paper, we empirically analyze and compare the lexical signals captured in such models. We identify lexical cues for each dimension of the MBTI personality scheme in several different ways, considering different datasets, feature sets, and learning algorithms. We conduct a series of correlation analyses between the resulting MBTI data and explore their connection to other signals, such as for Big-5 traits, emotion, sentiment, age, and gender. The analysis shows intriguing correlation patterns between different personality dimensions and other traits, and also provides evidence for the robustness of the data.</abstract>
      <url hash="3d4bf8be">2021.ranlp-1.58</url>
      <bibkey>he-de-melo-2021-personality</bibkey>
    </paper>
    <paper id="59">
      <title>Evaluation Datasets for Cross-lingual Semantic Textual Similarity</title>
      <author><first>Tomáš</first><last>Hercig</last></author>
      <author><first>Pavel</first><last>Kral</last></author>
      <pages>524–529</pages>
      <abstract>Semantic textual similarity (STS) systems estimate the degree of the meaning similarity between two sentences. Cross-lingual STS systems estimate the degree of the meaning similarity between two sentences, each in a different language. State-of-the-art algorithms usually employ a strongly supervised, resource-rich approach difficult to use for poorly-resourced languages. However, any approach needs to have evaluation data to confirm the results. In order to simplify the evaluation process for poorly-resourced languages (in terms of STS evaluation datasets), we present new datasets for cross-lingual and monolingual STS for languages without this evaluation data. We also present the results of several state-of-the-art methods on these data which can be used as a baseline for further research. We believe that this article will not only extend the current STS research to other languages, but will also encourage competition on this new evaluation data.</abstract>
      <url hash="98ffb5ad">2021.ranlp-1.59</url>
      <bibkey>hercig-kral-2021-evaluation</bibkey>
    </paper>
    <paper id="60">
      <title>Relation Extraction Using Multiple Pre-Training Models in Biomedical Domain</title>
      <author><first>Satoshi</first><last>Hiai</last></author>
      <author><first>Kazutaka</first><last>Shimada</last></author>
      <author><first>Taiki</first><last>Watanabe</last></author>
      <author><first>Akiva</first><last>Miura</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>530–537</pages>
      <abstract>The number of biomedical documents is increasing rapidly. Accordingly, a demand for extracting knowledge from large-scale biomedical texts is also increasing. BERT-based models are known for their high performance in various tasks. However, it is often computationally expensive. A high-end GPU environment is not available in many situations. To attain both high accuracy and fast extraction speed, we propose combinations of simpler pre-trained models. Our method outperforms the latest state-of-the-art model and BERT-based models on the GAD corpus. In addition, our method shows approximately three times faster extraction speed than the BERT-based models on the ChemProt corpus and reduces the memory size to one sixth of the BERT ones.</abstract>
      <url hash="c37db836">2021.ranlp-1.60</url>
      <bibkey>hiai-etal-2021-relation</bibkey>
    </paper>
    <paper id="61">
      <title>Discussion Structure Prediction Based on a Two-step Method</title>
      <author><first>Takumi</first><last>Himeno</last></author>
      <author><first>Kazutaka</first><last>Shimada</last></author>
      <pages>538–546</pages>
      <abstract>Conversations are often held in laboratories and companies. A summary is vital to grasp the content of a discussion for people who did not attend the discussion. If the summary is illustrated as an argument structure, it is helpful to grasp the discussion’s essentials immediately. Our purpose in this paper is to predict a link structure between nodes that consist of utterances in a conversation: classification of each node pair into “linked” or “not-linked.” One approach to predict the structure is to utilize machine learning models. However, the result tends to over-generate links of nodes. To solve this problem, we introduce a two-step method to the structure prediction task. We utilize a machine learning-based approach as the first step: a link prediction task. Then, we apply a score-based approach as the second step: a link selection task. Our two-step methods dramatically improved the accuracy as compared with one-step methods based on SVM and BERT.</abstract>
      <url hash="d48b15cf">2021.ranlp-1.61</url>
      <bibkey>himeno-shimada-2021-discussion</bibkey>
    </paper>
    <paper id="62">
      <title>On the Usefulness of Personality Traits in Opinion-oriented Tasks</title>
      <author><first>Marjan</first><last>Hosseinia</last></author>
      <author><first>Eduard</first><last>Dragut</last></author>
      <author><first>Dainis</first><last>Boumber</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <pages>547–556</pages>
      <abstract>We use a deep bidirectional transformer to extract the Myers-Briggs personality type from user-generated data in a multi-label and multi-class classification setting. Our dataset is large and made up of three available personality datasets of various social media platforms including Reddit, Twitter, and Personality Cafe forum. We induce personality embeddings from our transformer-based model and investigate if they can be used for downstream text classification tasks. Experimental evidence shows that personality embeddings are effective in three classification tasks including authorship verification, stance, and hyperpartisan detection. We also provide novel and interpretable analysis for the third task: hyperpartisan news classification.</abstract>
      <url hash="8348bae8">2021.ranlp-1.62</url>
      <bibkey>hosseinia-etal-2021-usefulness</bibkey>
    </paper>
    <paper id="63">
      <title>Application of Deep Learning Methods to <fixed-case>SNOMED</fixed-case> <fixed-case>CT</fixed-case> Encoding of Clinical Texts: From Data Collection to Extreme Multi-Label Text-Based Classification</title>
      <author><first>Anton</first><last>Hristov</last></author>
      <author><first>Aleksandar</first><last>Tahchiev</last></author>
      <author><first>Hristo</first><last>Papazov</last></author>
      <author><first>Nikola</first><last>Tulechki</last></author>
      <author><first>Todor</first><last>Primov</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>557–565</pages>
      <abstract>Concept normalization of clinical texts to standard medical classifications and ontologies is a task with high importance for healthcare and medical research. We attempt to solve this problem through automatic SNOMED CT encoding, where SNOMED CT is one of the most widely used and comprehensive clinical term ontologies. Applying basic Deep Learning models, however, leads to undesirable results due to the unbalanced nature of the data and the extreme number of classes. We propose a classification procedure that features a multiple-step workflow consisting of label clustering, multi-cluster classification, and clusters-to-labels mapping. For multi-cluster classification, BioBERT is fine-tuned over our custom dataset. The clusters-to-labels mapping is carried out by a one-vs-all classifier (SVC) applied to every single cluster. We also present the steps for automatic dataset generation of textual descriptions annotated with SNOMED CT codes based on public data and linked open data. In order to cope with the problem that our dataset is highly unbalanced, some data augmentation methods are applied. The results from the conducted experiments show high accuracy and reliability of our approach for prediction of SNOMED CT codes relevant to a clinical text.</abstract>
      <url hash="3d24b69f">2021.ranlp-1.63</url>
      <bibkey>hristov-etal-2021-application</bibkey>
    </paper>
    <paper id="64">
      <title>Syntax Matters! Syntax-Controlled in Text Style Transfer</title>
      <author><first>Zhiqiang</first><last>Hu</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last></author>
      <author><first>Charu C.</first><last>Aggarwal</last></author>
      <pages>566–575</pages>
      <abstract>Existing text style transfer (TST) methods rely on style classifiers to disentangle the text’s content and style attributes for text style transfer. While the style classifier plays a critical role in existing TST methods, there is no known investigation on its effect on the TST methods. In this paper, we conduct an empirical study on the limitations of the style classifiers used in existing TST methods. We demonstrated that the existing style classifiers cannot learn sentence syntax effectively and ultimately worsen existing TST models’ performance. To address this issue, we propose a novel Syntax-Aware Controllable Generation (SACG) model, which includes a syntax-aware style classifier that ensures learned style latent representations effectively capture the sentence structure for TST. Through extensive experiments on two popular text style transfer tasks, we show that our proposed method significantly outperforms twelve state-of-the-art methods. Our case studies have also demonstrated SACG’s ability to generate fluent target-style sentences that preserved the original content.</abstract>
      <url hash="bcbbb8a3">2021.ranlp-1.64</url>
      <bibkey>hu-etal-2021-syntax</bibkey>
    </paper>
    <paper id="65">
      <title>Transfer Learning for <fixed-case>C</fixed-case>zech Historical Named Entity Recognition</title>
      <author><first>Helena</first><last>Hubková</last></author>
      <author><first>Pavel</first><last>Kral</last></author>
      <pages>576–582</pages>
      <abstract>Nowadays, named entity recognition (NER) achieved excellent results on the standard corpora. However, big issues are emerging with a need for an application in a specific domain, because it requires a suitable annotated corpus with adapted NE tag-set. This is particularly evident in the historical document processing field. The main goal of this paper consists of proposing and evaluation of several transfer learning methods to increase the score of the Czech historical NER. We study several information sources, and we use two neural nets for NE modeling and recognition. We employ two corpora for evaluation of our transfer learning methods, namely Czech named entity corpus and Czech historical named entity corpus. We show that BERT representation with fine-tuning and only the simple classifier trained on the union of corpora achieves excellent results.</abstract>
      <url hash="2589d962">2021.ranlp-1.65</url>
      <bibkey>hubkova-kral-2021-transfer</bibkey>
    </paper>
    <paper id="66">
      <title>Personality Trait Identification Using the <fixed-case>R</fixed-case>ussian Feature Extraction Toolkit</title>
      <author><first>James R.</first><last>Hull</last></author>
      <author><first>Valerie</first><last>Novak</last></author>
      <author><first>C. Anton</first><last>Rytting</last></author>
      <author><first>Paul</first><last>Rodrigues</last></author>
      <author><first>Victor M.</first><last>Frank</last></author>
      <author><first>Matthew</first><last>Swahn</last></author>
      <pages>583–592</pages>
      <abstract>Feature engineering is an important step in classical NLP pipelines, but machine learning engineers may not be aware of the signals to look for when processing foreign language text. The Russian Feature Extraction Toolkit (RFET) is a collection of feature extraction libraries bundled for ease of use by engineers who do not speak Russian. RFET’s current feature set includes features applicable to social media genres of text and to computational social science tasks. We demonstrate the effectiveness of the tool by using it in a personality trait identification task. We compare the performance of Support Vector Machines (SVMs) trained with and without the features provided by RFET; we also compare it to a SVM with neural embedding features generated by Sentence-BERT.</abstract>
      <url hash="53ccbe2e">2021.ranlp-1.66</url>
      <bibkey>hull-etal-2021-personality</bibkey>
    </paper>
    <paper id="67">
      <title>Semi-Supervised Learning Based on Auto-generated Lexicon Using <fixed-case>XAI</fixed-case> in Sentiment Analysis</title>
      <author><first>Hohyun</first><last>Hwang</last></author>
      <author><first>Younghoon</first><last>Lee</last></author>
      <pages>593–600</pages>
      <abstract>In this study, we proposed a novel Lexicon-based pseudo-labeling method utilizing explainable AI(XAI) approach. Existing approach have a fundamental limitation in their robustness because poor classifier leads to inaccurate soft-labeling, and it lead to poor classifier repetitively. Meanwhile, we generate the lexicon consists of sentiment word based on the explainability score. Then we calculate the confidence of unlabeled data with lexicon and add them into labeled dataset for the robust pseudo-labeling approach. Our proposed method has three contributions. First, the proposed methodology automatically generates a lexicon based on XAI and performs independent pseudo-labeling, thereby guaranteeing higher performance and robustness compared to the existing one. Second, since lexicon-based pseudo-labeling is performed without re-learning in most of models, time efficiency is considerably increased, and third, the generated high-quality lexicon can be available for sentiment analysis of data from similar domains. The effectiveness and efficiency of our proposed method were verified through quantitative comparison with the existing pseudo-labeling method and qualitative review of the generated lexicon.</abstract>
      <url hash="bde49842">2021.ranlp-1.67</url>
      <bibkey>hwang-lee-2021-semi</bibkey>
    </paper>
    <paper id="68">
      <title>Multiple Teacher Distillation for Robust and Greener Models</title>
      <author><first>Artur</first><last>Ilichev</last></author>
      <author><first>Nikita</first><last>Sorokin</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <pages>601–610</pages>
      <abstract>The language models nowadays are in the center of natural language processing progress. These models are mostly of significant size. There are successful attempts to reduce them, but at least some of these attempts rely on randomness. We propose a novel distillation procedure leveraging on multiple teachers usage which alleviates random seed dependency and makes the models more robust. We show that this procedure applied to TinyBERT and DistilBERT models improves their worst case results up to 2% while keeping almost the same best-case ones. The latter fact keeps true with a constraint on computational time, which is important to lessen the carbon footprint. In addition, we present the results of an application of the proposed procedure to a computer vision model ResNet, which shows that the statement keeps true in this totally different domain.</abstract>
      <url hash="1ac71d55">2021.ranlp-1.68</url>
      <bibkey>ilichev-etal-2021-multiple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="69">
      <title><fixed-case>BERT</fixed-case> Embeddings for Automatic Readability Assessment</title>
      <author><first>Joseph Marvin</first><last>Imperial</last></author>
      <pages>611–618</pages>
      <abstract>Automatic readability assessment (ARA) is the task of evaluating the level of ease or difficulty of text documents for a target audience. For researchers, one of the many open problems in the field is to make such models trained for the task show efficacy even for low-resource languages. In this study, we propose an alternative way of utilizing the information-rich embeddings of BERT models with handcrafted linguistic features through a combined method for readability assessment. Results show that the proposed method outperforms classical approaches in readability assessment using English and Filipino datasets, obtaining as high as 12.4% increase in F1 performance. We also show that the general information encoded in BERT embeddings can be used as a substitute feature set for low-resource languages like Filipino with limited semantic and syntactic NLP tools to explicitly extract feature values for the task.</abstract>
      <url hash="1f607fca">2021.ranlp-1.69</url>
      <bibkey>imperial-2021-bert</bibkey>
      <pwccode url="https://github.com/imperialite/BERT-Embeddings-For-ARA" additional="false">imperialite/BERT-Embeddings-For-ARA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="70">
      <title>Semantic-Based Opinion Summarization</title>
      <author><first>Marcio</first><last>Inácio</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>619–628</pages>
      <abstract>The amount of information available online can be overwhelming for users to digest, specially when dealing with other users’ comments when making a decision about buying a product or service. In this context, opinion summarization systems are of great value, extracting important information from the texts and presenting them to the user in a more understandable manner. It is also known that the usage of semantic representations can benefit the quality of the generated summaries. This paper aims at developing opinion summarization methods based on Abstract Meaning Representation of texts in the Brazilian Portuguese language. Four different methods have been investigated, alongside some literature approaches. The results show that a Machine Learning-based method produced summaries of higher quality, outperforming other literature techniques on manually constructed semantic graphs. We also show that using parsed graphs over manually annotated ones harmed the output. Finally, an analysis of how important different types of information are for the summarization process suggests that using Sentiment Analysis features did not improve summary quality.</abstract>
      <url hash="d91f4849">2021.ranlp-1.70</url>
      <bibkey>inacio-pardo-2021-semantic</bibkey>
      <pwccode url="https://github.com/superar/semopinions" additional="false">superar/semopinions</pwccode>
    </paper>
    <paper id="71">
      <title>Using Collaborative Filtering to Model Argument Selection</title>
      <author><first>Sagar</first><last>Indurkhya</last></author>
      <pages>629–639</pages>
      <abstract>This study evaluates whether model-based Collaborative Filtering (CF) algorithms, which have been extensively studied and widely used to build recommender systems, can be used to predict which common nouns a predicate can take as its complement. We find that, when trained on verb-noun co-occurrence data drawn from the Corpus of Contemporary American-English (COCA), two popular model-based CF algorithms, Singular Value Decomposition and Non-negative Matrix Factorization, perform well on this task, each achieving an AUROC of at least 0.89 and surpassing several different baselines. We then show that the embedding-vectors for verbs and nouns learned by the two CF models can be quantized (via application of k-means clustering) with minimal loss of performance on the prediction task while only using a small number of verb and noun clusters (relative to the number of distinct verbs and nouns). Finally we evaluate the alignment between the quantized embedding vectors for verbs and the Levin verb classes, finding that the alignment surpassed several randomized baselines. We conclude by discussing how model-based CF algorithms might be applied to learning restrictions on constituent selection between various lexical categories and how these (learned) models could then be used to augment a (rule-based) constituency grammar.</abstract>
      <url hash="0586f6b0">2021.ranlp-1.71</url>
      <bibkey>indurkhya-2021-using</bibkey>
    </paper>
    <paper id="72">
      <title>Domain-Specific <fixed-case>J</fixed-case>apanese <fixed-case>ELECTRA</fixed-case> Model Using a Small Corpus</title>
      <author><first>Youki</first><last>Itoh</last></author>
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <pages>640–646</pages>
      <abstract>Recently, domain shift, which affects accuracy due to differences in data between source and target domains, has become a serious issue when using machine learning methods to solve natural language processing tasks. With additional pretraining and fine-tuning using a target domain corpus, pretraining models such as BERT (Bidirectional Encoder Representations from Transformers) can address this issue. However, the additional pretraining of the BERT model is difficult because it requires significant computing resources. The efficiently learning an encoder that classifies token replacements accurately (ELECTRA) pretraining model replaces the BERT pretraining method’s masked language modeling with a method called replaced token detection, which improves the computational efficiency and allows the additional pretraining of the model to a practical extent. Herein, we propose a method for addressing the computational efficiency of pretraining models in domain shift by constructing an ELECTRA pretraining model on a Japanese dataset and additional pretraining this model in a downstream task using a corpus from the target domain. We constructed a pretraining model for ELECTRA in Japanese and conducted experiments on a document classification task using data from Japanese news articles. Results show that even a model smaller than the pretrained model performs equally well.</abstract>
      <url hash="028cd3c4">2021.ranlp-1.72</url>
      <bibkey>itoh-shinnou-2021-domain</bibkey>
    </paper>
    <paper id="73">
      <title><fixed-case>BERT</fixed-case>-<fixed-case>P</fixed-case>ers<fixed-case>NER</fixed-case>: A New Model for <fixed-case>P</fixed-case>ersian Named Entity Recognition</title>
      <author><first>Farane</first><last>Jalali Farahani</last></author>
      <author><first>Gholamreza</first><last>Ghassem-Sani</last></author>
      <pages>647–654</pages>
      <abstract>Named entity recognition (NER) is one of the major tasks in natural language processing. A named entity is often a word or expression that bears a valuable piece of information, which can be effectively employed by some major NLP tasks such as machine translation, question answering, and text summarization. In this paper, we introduce a new model called BERT-PersNER (BERT based Persian Named Entity Recognizer), in which we have applied transfer learning and active learning approaches to NER in Persian, which is regarded as a low-resource language. Like many others, we have used Conditional Random Field for tag decoding in our proposed architecture. BERT-PersNER has outperformed two available studies in Persian NER, in most cases of our experiments using the supervised learning approach on two Persian datasets called Arman and Peyma. Besides, as the very first effort to try active learning in the Persian NER, using only 30% of Arman and 20% of Peyma, we respectively achieved 92.15%, and 92.41% performance of the mentioned supervised learning experiments.</abstract>
      <url hash="f3659c63">2021.ranlp-1.73</url>
      <bibkey>jalali-farahani-ghassem-sani-2021-bert</bibkey>
    </paper>
    <paper id="74">
      <title>Cross-lingual Fine-tuning for Abstractive <fixed-case>A</fixed-case>rabic Text Summarization</title>
      <author><first>Mram</first><last>Kahla</last></author>
      <author><first>Zijian Győző</first><last>Yang</last></author>
      <author><first>Attila</first><last>Novák</last></author>
      <pages>655–663</pages>
      <abstract>While abstractive summarization in certain languages, like English, has already reached fairly good results due to the availability of trend-setting resources, like the CNN/Daily Mail dataset, and considerable progress in generative neural models, progress in abstractive summarization for Arabic, the fifth most-spoken language globally, is still in baby shoes. While some resources for extractive summarization have been available for some time, in this paper, we present the first corpus of human-written abstractive news summaries in Arabic, hoping to lay the foundation of this line of research for this important language. The dataset consists of more than 21 thousand items. We used this dataset to train a set of neural abstractive summarization systems for Arabic by fine-tuning pre-trained language models such as multilingual BERT, AraBERT, and multilingual BART-50. As the Arabic dataset is much smaller than e.g. the CNN/Daily Mail dataset, we also applied cross-lingual knowledge transfer to significantly improve the performance of our baseline systems. The setups included two M-BERT-based summarization models originally trained for Hungarian/English and a similar system based on M-BART-50 originally trained for Russian that were further fine-tuned for Arabic. Evaluation of the models was performed in terms of ROUGE, and a manual evaluation of fluency and adequacy of the models was also performed.</abstract>
      <url hash="de49f1fb">2021.ranlp-1.74</url>
      <bibkey>kahla-etal-2021-cross</bibkey>
    </paper>
    <paper id="75">
      <title>Behavior of Modern Pre-trained Language Models Using the Example of Probing Tasks</title>
      <author><first>Ekaterina</first><last>Kalyaeva</last></author>
      <author><first>Oleg</first><last>Durandin</last></author>
      <author><first>Alexey</first><last>Malafeev</last></author>
      <pages>664–670</pages>
      <abstract>Modern transformer-based language models are revolutionizing NLP. However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training. Thus, the aim of this study is to examine behavior of the model BERT in the task of masked language modelling and to provide linguistic interpretation to the unexpected effects and errors produced by the model. For this purpose, we used a new Russian-language dataset based on educational texts for learners of Russian and annotated with the help of the National Corpus of the Russian language. In terms of quality metrics (the proportion of words, semantically related to the target word), the multilingual BERT is recognized as the best model. Generally, each model has distinct strengths in relation to a certain linguistic phenomenon. These observations have meaningful implications for research into applied linguistics and pedagogy, contribute to dialogue system development, automatic exercise making, text generation and potentially could improve the quality of existing linguistic technologies</abstract>
      <url hash="c2ca0bf4">2021.ranlp-1.75</url>
      <bibkey>kalyaeva-etal-2021-behavior</bibkey>
    </paper>
    <paper id="76">
      <title>Towards Quantifying Magnitude of Political Bias in News Articles Using a Novel Annotation Schema</title>
      <author><first>Lalitha</first><last>Kameswari</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>671–678</pages>
      <abstract>Media bias is a predominant phenomenon present in most forms of print and electronic media such as news articles, blogs, tweets, etc. Since media plays a pivotal role in shaping public opinion towards political happenings, both political parties and media houses often use such sources as outlets to propagate their own prejudices to the public. There has been some research on detecting political bias in news articles. However, none of it attempts to analyse the nature of bias or quantify the magnitude ofthe bias in a given text. This paper presents a political bias annotated corpus viz. PoBiCo-21, which is annotated using a schema specifically designed with 10 labels to capture various techniques used to create political bias in news. We create a ranking of these techniques based on their contribution to bias. After validating the ranking, we propose methods to use it to quantify the magnitude of bias in political news articles.</abstract>
      <url hash="e5dbc708">2021.ranlp-1.76</url>
      <bibkey>kameswari-mamidi-2021-towards</bibkey>
    </paper>
    <paper id="77">
      <title>Application of Mix-Up Method in Document Classification Task Using <fixed-case>BERT</fixed-case></title>
      <author><first>Naoki</first><last>Kikuta</last></author>
      <author><first>Hiroyuki</first><last>Shinnou</last></author>
      <pages>679–683</pages>
      <abstract>The mix-up method (Zhang et al., 2017), one of the methods for data augmentation, is known to be easy to implement and highly effective. Although the mix-up method is intended for image identification, it can also be applied to natural language processing. In this paper, we attempt to apply the mix-up method to a document classification task using bidirectional encoder representations from transformers (BERT) (Devlin et al., 2018). Since BERT allows for two-sentence input, we concatenated word sequences from two documents with different labels and used the multi-class output as the supervised data with a one-hot vector. In an experiment using the livedoor news corpus, which is Japanese, we compared the accuracy of document classification using two methods for selecting documents to be concatenated with that of ordinary document classification. As a result, we found that the proposed method is better than the normal classification when the documents with labels shortages are mixed preferentially. This indicates that how to choose documents for mix-up has a significant impact on the results.</abstract>
      <url hash="2c00c21a">2021.ranlp-1.77</url>
      <bibkey>kikuta-shinnou-2021-application</bibkey>
    </paper>
    <paper id="78">
      <title>Translation Memory Retrieval Using Lucene</title>
      <author><first>Kwang-hyok</first><last>Kim</last></author>
      <author><first>Myong-ho</first><last>Cho</last></author>
      <author><first>Chol-ho</first><last>Ryang</last></author>
      <author><first>Ju-song</first><last>Im</last></author>
      <author><first>Song-yong</first><last>Cho</last></author>
      <author><first>Yong-jun</first><last>Han</last></author>
      <pages>684–691</pages>
      <abstract>Translation Memory (TM) system, a major component of computer-assisted translation (CAT), is widely used to improve human translators’ productivity by making effective use of previously translated resource. We propose a method to achieve high-speed retrieval from a large translation memory by means of similarity evaluation based on vector model, and present the experimental result. Through our experiment using Lucene, an open source information retrieval search engine, we conclude that it is possible to achieve real-time retrieval speed of about tens of microseconds even for a large translation memory with 5 million segment pairs.</abstract>
      <url hash="d98c1d4d">2021.ranlp-1.78</url>
      <bibkey>kim-etal-2021-translation</bibkey>
    </paper>
    <paper id="79">
      <title>Now, It’s Personal : The Need for Personalized Word Sense Disambiguation</title>
      <author><first>Milton</first><last>King</last></author>
      <author><first>Paul</first><last>Cook</last></author>
      <pages>692–700</pages>
      <abstract>Authors of text tend to predominantly use a single sense for a lemma that can differ among different authors. This might not be captured with an author-agnostic word sense disambiguation (WSD) model that was trained on multiple authors. Our work finds that WordNet’s first senses, the predominant senses of our dataset’s genre, and the predominant senses of an author can all be different and therefore, author-agnostic models could perform well over the entire dataset, but poorly on individual authors. In this work, we explore methods for personalizing WSD models by tailoring existing state-of-the-art models toward an individual by exploiting the author’s sense distributions. We propose a novel WSD dataset and show that personalizing a WSD system with knowledge of an author’s sense distributions or predominant senses can greatly increase its performance.</abstract>
      <url hash="c0a4be71">2021.ranlp-1.79</url>
      <bibkey>king-cook-2021-now</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="80">
      <title>Multilingual Image Corpus: Annotation Protocol</title>
      <author><first>Svetla</first><last>Koeva</last></author>
      <pages>701–707</pages>
      <abstract>In this paper, we present work in progress aimed at the development of a new image dataset with annotated objects. The Multilingual Image Corpus consists of an ontology of visual objects (based on WordNet) and a collection of thematically related images annotated with segmentation masks and object classes. We identified 277 dominant classes and 1,037 parent and attribute classes, and grouped them into 10 thematic domains such as sport, medicine, education, food, security, etc. For the selected classes a large-scale web image search is being conducted in order to compile a substantial collection of high-quality copyright free images. The focus of the paper is the annotation protocol which we established to facilitate the annotation process: the Ontology of visual objects and the conventions for image selection and for object segmentation. The dataset is designed both for image classification and object detection and for semantic segmentation. In addition, the object annotations will be supplied with multilingual descriptions by using freely available wordnets.</abstract>
      <url hash="98f815e5">2021.ranlp-1.80</url>
      <bibkey>koeva-2021-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/caltech-256">Caltech-256</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/labelme">LabelMe</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/modanet">ModaNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/silhouettes">Silhouettes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tiny-images">Tiny Images</pwcdataset>
    </paper>
    <paper id="81">
      <title><fixed-case>ELERRANT</fixed-case>: Automatic Grammatical Error Type Classification for <fixed-case>G</fixed-case>reek</title>
      <author><first>Katerina</first><last>Korre</last></author>
      <author><first>Marita</first><last>Chatzipanagiotou</last></author>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <pages>708–717</pages>
      <abstract>In this paper, we introduce the Greek version of the automatic annotation tool ERRANT (Bryant et al., 2017), which we named ELERRANT. ERRANT functions as a rule-based error type classifier and was used as the main evaluation tool of the systems participating in the BEA-2019 (Bryant et al., 2019) shared task. Here, we discuss grammatical and morphological differences between English and Greek and how these differences affected the development of ELERRANT. We also introduce the first Greek Native Corpus (GNC) and the Greek WikiEdits Corpus (GWE), two new evaluation datasets with errors from native Greek learners and Wikipedia Talk Pages edits respectively. These two datasets are used for the evaluation of ELERRANT. This paper is a sole fragment of a bigger picture which illustrates the attempt to solve the problem of low-resource languages in NLP, in our case Greek.</abstract>
      <url hash="be6d4c7f">2021.ranlp-1.81</url>
      <bibkey>korre-etal-2021-elerrant</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiconv">WikiConv</pwcdataset>
    </paper>
    <paper id="82">
      <title>Neural Machine Translation for <fixed-case>S</fixed-case>inhala-<fixed-case>E</fixed-case>nglish Code-Mixed Text</title>
      <author><first>Archchana</first><last>Kugathasan</last></author>
      <author><first>Sagara</first><last>Sumathipala</last></author>
      <pages>718–726</pages>
      <abstract>Code-mixing has become a moving method of communication among multilingual speakers. Most of the social media content of the multilingual societies are written in code-mixed text. However, most of the current translation systems neglect to convert code-mixed texts to a standard language. Most of the user written code-mixed content in social media remains unprocessed due to the unavailability of linguistic resource such as parallel corpus. This paper proposes a Neural Machine Translation(NMT) model to translate the Sinhala-English code-mixed text to the Sinhala language. Due to the limited resources available for Sinhala-English code-mixed(SECM) text, a parallel corpus is created with SECM sentences and Sinhala sentences. Srilankan social media sites contain SECM texts more frequently than the standard languages. The model proposed for code-mixed text translation in this study is a combination of Encoder-Decoder framework with LSTM units and Teachers Forcing Algorithm. The translated sentences from the model are evaluated using BLEU(Bilingual Evaluation Understudy) metric. Our model achieved a remarkable BLEU score for the translation.</abstract>
      <url hash="a211d1d9">2021.ranlp-1.82</url>
      <bibkey>kugathasan-sumathipala-2021-neural</bibkey>
    </paper>
    <paper id="83">
      <title>Multilingual Multi-Domain <fixed-case>NMT</fixed-case> for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Sourav</first><last>Kumar</last></author>
      <author><first>Salil</first><last>Aggarwal</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>727–733</pages>
      <abstract>India is known as the land of many tongues and dialects. Neural machine translation (NMT) is the current state-of-the-art approach for machine translation (MT) but performs better only with large datasets which Indian languages usually lack, making this approach infeasible. So, in this paper, we address the problem of data scarcity by efficiently training multilingual and multilingual multi domain NMT systems involving languages of the 𝐈𝐧𝐝𝐢𝐚𝐧 𝐬𝐮𝐛𝐜𝐨𝐧𝐭𝐢𝐧𝐞𝐧𝐭. We are proposing the technique for using the joint domain and language tags in a multilingual setup. We draw three major conclusions from our experiments: (i) Training a multilingual system via exploiting lexical similarity based on language family helps in achieving an overall average improvement of 𝟑.𝟐𝟓 𝐁𝐋𝐄𝐔 𝐩𝐨𝐢𝐧𝐭𝐬 over bilingual baselines, (ii) Technique of incorporating domain information into the language tokens helps multilingual multi-domain system in getting a significant average improvement of 𝟔 𝐁𝐋𝐄𝐔 𝐩𝐨𝐢𝐧𝐭𝐬 over the baselines, (iii) Multistage fine-tuning further helps in getting an improvement of 𝟏-𝟏.𝟓 𝐁𝐋𝐄𝐔 𝐩𝐨𝐢𝐧𝐭𝐬 for the language pair of interest.</abstract>
      <url hash="7ffffbd3">2021.ranlp-1.83</url>
      <bibkey>kumar-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="84">
      <title>Fiction in <fixed-case>R</fixed-case>ussian Translation: A Translationese Study</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>734–743</pages>
      <abstract>This paper presents a translationese study based on the parallel data from the Russian National Corpus (RNC). We explored differences between literary texts originally authored in Russian and fiction translated into Russian from 11 languages. The texts are represented with frequency-based features that capture structural and lexical properties of language. Binary classification results indicate that literary translations can be distinguished from non-translations with an accuracy ranging from 82 to 92% depending on the source language and feature set. Multiclass classification confirms that translations from distant languages are more distinct from non-translations than translations from languages that are typologically close to Russian. It also demonstrates that translations from same-family source languages share translationese properties. Structural features return more consistent results than features relying on external resources and capturing lexical properties of texts in both translationese detection and source language identification tasks.</abstract>
      <url hash="c36dff5b">2021.ranlp-1.84</url>
      <bibkey>kunilovskaya-etal-2021-fiction</bibkey>
    </paper>
    <paper id="85">
      <title>Corpus Creation and Language Identification in Low-Resource Code-Mixed <fixed-case>T</fixed-case>elugu-<fixed-case>E</fixed-case>nglish Text</title>
      <author><first>Siva Subrahamanyam Varma</first><last>Kusampudi</last></author>
      <author><first>Anudeep</first><last>Chaluvadi</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>744–752</pages>
      <abstract>Code-Mixing (CM) is a common phenomenon in multilingual societies. CM plays a significant role in technology and medical fields where terminologies in the native language are not available or known. Language Identification (LID) of the CM data will help solve NLP tasks such as Spell Checking, Named Entity Recognition, Part-Of-Speech tagging, and Semantic Parsing. In the current era of machine learning, a common problem to the above-mentioned tasks is the availability of Learning data to train models. In this paper, we introduce two Telugu-English CM manually annotated datasets (Twitter dataset and Blog dataset). The Twitter dataset contains more romanization variability and misspelled words than the blog dataset. We compare across various classification models and perform extensive bench-marking using both Classical and Deep Learning Models for LID compared to existing models. We propose two architectures for language classification (Telugu and English) in CM data: (1) Word Level Classification (2) Sentence Level word-by-word Classification and compare these approaches presenting two strong baselines for LID on these datasets.</abstract>
      <url hash="65065e86">2021.ranlp-1.85</url>
      <bibkey>kusampudi-etal-2021-corpus</bibkey>
    </paper>
    <paper id="86">
      <title>Sentiment Analysis in Code-Mixed <fixed-case>T</fixed-case>elugu-<fixed-case>E</fixed-case>nglish Text with Unsupervised Data Normalization</title>
      <author><first>Siva Subrahamanyam Varma</first><last>Kusampudi</last></author>
      <author><first>Preetham</first><last>Sathineni</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>753–760</pages>
      <abstract>In a multilingual society, people communicate in more than one language, leading to Code-Mixed data. Sentimental analysis on Code-Mixed Telugu-English Text (CMTET) poses unique challenges. The unstructured nature of the Code-Mixed Data is due to the informal language, informal transliterations, and spelling errors. In this paper, we introduce an annotated dataset for Sentiment Analysis in CMTET. Also, we report an accuracy of 80.22% on this dataset using novel unsupervised data normalization with a Multilayer Perceptron (MLP) model. This proposed data normalization technique can be extended to any NLP task involving CMTET. Further, we report an increase of 2.53% accuracy due to this data normalization approach in our best model.</abstract>
      <url hash="9bfe2ea8">2021.ranlp-1.86</url>
      <bibkey>kusampudi-etal-2021-sentiment</bibkey>
    </paper>
    <paper id="87">
      <title>From Constituency to <fixed-case>UD</fixed-case>-Style Dependency: Building the First Conversion Tool of <fixed-case>T</fixed-case>urkish</title>
      <author><first>Aslı</first><last>Kuzgun</last></author>
      <author><first>Oğuz Kerem</first><last>Yıldız</last></author>
      <author><first>Neslihan</first><last>Cesur</last></author>
      <author><first>Büşra</first><last>Marşan</last></author>
      <author><first>Arife Betül</first><last>Yenice</last></author>
      <author><first>Ezgi</first><last>Sanıyar</last></author>
      <author><first>Oguzhan</first><last>Kuyrukçu</last></author>
      <author><first>Bilge Nas</first><last>Arıcan</last></author>
      <author><first>Olcay Taner</first><last>Yıldız</last></author>
      <pages>761–769</pages>
      <abstract>This paper deliberates on the process of building the first constituency-to-dependency conversion tool of Turkish. The starting point of this work is a previous study in which 10,000 phrase structure trees were manually transformed into Turkish from the original PennTreebank corpus. Within the scope of this project, these Turkish phrase structure trees were automatically converted into UD-style dependency structures, using both a rule-based algorithm and a machine learning algorithm specific to the requirements of the Turkish language. The results of both algorithms were compared and the machine learning approach proved to be more accurate than the rule-based algorithm. The output was revised by a team of linguists. The refined versions were taken as gold standard annotations for the evaluation of the algorithms. In addition to its contribution to the UD Project with a large dataset of 10,000 Turkish dependency trees, this project also fulfills the important gap of a Turkish conversion tool, enabling the quick compilation of dependency corpora which can be used for the training of better dependency parsers.</abstract>
      <url hash="2115f3f5">2021.ranlp-1.87</url>
      <bibkey>kuzgun-etal-2021-constituency</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="88">
      <title>Making Your Tweets More Fancy: Emoji Insertion to Texts</title>
      <author><first>Jingun</first><last>Kwon</last></author>
      <author><first>Naoki</first><last>Kobayashi</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>770–779</pages>
      <abstract>In the social media, users frequently use small images called emojis in their posts. Although using emojis in texts plays a key role in recent communication systems, less attention has been paid on their positions in the given texts, despite that users carefully choose and put an emoji that matches their post. Exploring positions of emojis in texts will enhance understanding of the relationship between emojis and texts. We extend an emoji label prediction task taking into account the information of emoji positions, by jointly learning the emoji position in a tweet to predict the emoji label. The results demonstrate that the position of emojis in texts is a good clue to boost the performance of emoji label prediction. Human evaluation validates that there exists a suitable emoji position in a tweet, and our proposed task is able to make tweets more fancy and natural. In addition, considering emoji position can further improve the performance for the irony detection task compared to the emoji label prediction. We also report the experimental results for the modified dataset, due to the problem of the original dataset for the first shared task to predict an emoji label in SemEval2018.</abstract>
      <url hash="7b90d5ba">2021.ranlp-1.88</url>
      <bibkey>kwon-etal-2021-making</bibkey>
    </paper>
    <paper id="89">
      <title>Addressing Slot-Value Changes in Task-oriented Dialogue Systems through Dialogue Domain Adaptation</title>
      <author><first>Tiziano</first><last>Labruna</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>780–789</pages>
      <abstract>Recent task-oriented dialogue systems learn a model from annotated dialogues, and such dialogues are in turn collected and annotated so that they are consistent with certain domain knowledge. However, in real scenarios, domain knowledge is subject to frequent changes, and initial training dialogues may soon become obsolete, resulting in a significant decrease in the model performance. In this paper, we investigate the relationship between training dialogues and domain knowledge, and propose Dialogue Domain Adaptation, a methodology aiming at adapting initial training dialogues to changes intervened in the domain knowledge. We focus on slot-value changes (e.g., when new slot values are available to describe domain entities) and define an experimental setting for dialogue domain adaptation. First, we show that current state-of-the-art models for dialogue state tracking are still poorly robust to slot-value changes of the domain knowledge. Then, we compare different domain adaptation strategies, showing that simple techniques are effective to reduce the gap between training dialogues and domain knowledge.</abstract>
      <url hash="5bb8fd9f">2021.ranlp-1.89</url>
      <bibkey>labruna-magnini-2021-addressing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="90">
      <title>Developing a Clinical Language Model for <fixed-case>S</fixed-case>wedish: Continued Pretraining of Generic <fixed-case>BERT</fixed-case> with In-Domain Data</title>
      <author><first>Anastasios</first><last>Lamproudis</last></author>
      <author><first>Aron</first><last>Henriksson</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>790–797</pages>
      <abstract>The use of pretrained language models, fine-tuned to perform a specific downstream task, has become widespread in NLP. Using a generic language model in specialized domains may, however, be sub-optimal due to differences in language use and vocabulary. In this paper, it is investigated whether an existing, generic language model for Swedish can be improved for the clinical domain through continued pretraining with clinical text. The generic and domain-specific language models are fine-tuned and evaluated on three representative clinical NLP tasks: (i) identifying protected health information, (ii) assigning ICD-10 diagnosis codes to discharge summaries, and (iii) sentence-level uncertainty prediction. The results show that continued pretraining on in-domain data leads to improved performance on all three downstream tasks, indicating that there is a potential added value of domain-specific language models for clinical NLP.</abstract>
      <url hash="1f043155">2021.ranlp-1.90</url>
      <bibkey>lamproudis-etal-2021-developing</bibkey>
    </paper>
    <paper id="91">
      <title>Text Retrieval for Language Learners: Graded Vocabulary vs. Open Learner Model</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Chak Yan</first><last>Yeung</last></author>
      <pages>798–804</pages>
      <abstract>A text retrieval system for language learning returns reading materials at the appropriate difficulty level for the user. The system typically maintains a learner model on the user’s vocabulary knowledge, and identifies texts that best fit the model. As the user’s language proficiency increases, model updates are necessary to retrieve texts with the corresponding lexical complexity. We investigate an open learner model that allows user modification of its content, and evaluate its effectiveness with respect to the amount of user update effort. We compare this model with the graded approach, in which the system returns texts at the optimal grade. When the user makes at least half of the expected updates to the open learner model, simulation results show that it outperforms the graded approach in retrieving texts that fit user preference for new-word density.</abstract>
      <url hash="dbf71d26">2021.ranlp-1.91</url>
      <bibkey>lee-yeung-2021-text</bibkey>
    </paper>
    <paper id="92">
      <title>Transforming Multi-Conditioned Generation from Meaning Representation</title>
      <author><first>Joosung</first><last>Lee</last></author>
      <pages>805–813</pages>
      <abstract>Our study focuses on language generation by considering various information representing the meaning of utterances as multiple conditions of generation. Generating an utterance from a Meaning representation (MR) usually passes two steps: sentence planning and surface realization. However, we propose a simple one-stage framework to generate utterances directly from MR. Our model is based on GPT2 and generates utterances with flat conditions on slot and value pairs, which does not need to determine the structure of the sentence. We evaluate several systems in the E2E dataset with 6 automatic metrics. Our system is a simple method, but it demonstrates comparable performance to previous systems in automated metrics. In addition, using only 10% of the dataset without any other techniques, our model achieves comparable performance, and shows the possibility of performing zero-shot generation and expanding to other datasets.</abstract>
      <url hash="10bcee15">2021.ranlp-1.92</url>
      <bibkey>lee-2021-transforming</bibkey>
      <pwccode url="https://github.com/rungjoo/TransMC-data2text" additional="false">rungjoo/TransMC-data2text</pwccode>
    </paper>
    <paper id="93">
      <title>Frustration Level Annotation in <fixed-case>L</fixed-case>atvian Tweets with Non-Lexical Means of Expression</title>
      <author><first>Viktorija</first><last>Leonova</last></author>
      <author><first>Janis</first><last>Zuters</last></author>
      <pages>814–823</pages>
      <abstract>We present a neural-network-driven model for annotating frustration intensity in customer support tweets, based on representing tweet texts using a bag-of-words encoding after processing with subword segmentation together with non-lexical features. The model was evaluated on tweets in English and Latvian languages, focusing on aspects beyond the pure bag-of-words representations used in previous research. The experimental results show that the model can be successfully applied for texts in a non-English language, and that adding non-lexical features to tweet representations significantly improves performance, while subword segmentation has a moderate but positive effect on model accuracy. Our code and training data are publicly available.</abstract>
      <url hash="6de72a35">2021.ranlp-1.93</url>
      <bibkey>leonova-zuters-2021-frustration</bibkey>
    </paper>
    <paper id="94">
      <title>System Combination for Grammatical Error Correction Based on Integer Programming</title>
      <author><first>Ruixi</first><last>Lin</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>824–829</pages>
      <abstract>In this paper, we propose a system combination method for grammatical error correction (GEC), based on nonlinear integer programming (IP). Our method optimizes a novel F score objective based on error types, and combines multiple end-to-end GEC systems. The proposed IP approach optimizes the selection of a single best system for each grammatical error type present in the data. Experiments of the IP approach on combining state-of-the-art standalone GEC systems show that the combined system outperforms all standalone systems. It improves F0.5 score by 3.61% when combining the two best participating systems in the BEA 2019 shared task, and achieves F0.5 score of 73.08%. We also perform experiments to compare our IP approach with another state-of-the-art system combination method for GEC, demonstrating IP’s competitive combination capability.</abstract>
      <url hash="228f77b1">2021.ranlp-1.94</url>
      <bibkey>lin-ng-2021-system</bibkey>
      <pwccode url="https://github.com/nusnlp/gec_ip" additional="false">nusnlp/gec_ip</pwccode>
    </paper>
    <paper id="95">
      <title>Multilingual Learning for Mild Cognitive Impairment Screening from a Clinical Speech Task</title>
      <author><first>Hali</first><last>Lindsay</last></author>
      <author><first>Philipp</first><last>Müller</last></author>
      <author><first>Insa</first><last>Kröger</last></author>
      <author><first>Johannes</first><last>Tröger</last></author>
      <author><first>Nicklas</first><last>Linz</last></author>
      <author><first>Alexandra</first><last>Konig</last></author>
      <author><first>Radia</first><last>Zeghari</last></author>
      <author><first>Frans RJ</first><last>Verhey</last></author>
      <author><first>Inez HGB</first><last>Ramakers</last></author>
      <pages>830–838</pages>
      <abstract>The Semantic Verbal Fluency Task (SVF) is an efficient and minimally invasive speech-based screening tool for Mild Cognitive Impairment (MCI). In the SVF, testees have to produce as many words for a given semantic category as possible within 60 seconds. State-of-the-art approaches for automatic evaluation of the SVF employ word embeddings to analyze semantic similarities in these word sequences. While these approaches have proven promising in a variety of test languages, the small amount of data available for any given language limits the performance. In this paper, we for the first time investigate multilingual learning approaches for MCI classification from the SVF in order to combat data scarcity. To allow for cross-language generalisation, these approaches either rely on translation to a shared language, or make use of several distinct word embeddings. In evaluations on a multilingual corpus of older French, Dutch, and German participants (Controls=66, MCI=66), we show that our multilingual approaches clearly improve over single-language baselines.</abstract>
      <url hash="0341d459">2021.ranlp-1.95</url>
      <bibkey>lindsay-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="96">
      <title>Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues Using <fixed-case>BERT</fixed-case></title>
      <author><first>Ye</first><last>Liu</last></author>
      <author><first>Wolfgang</first><last>Maier</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>839–845</pages>
      <abstract>This paper presents an automatic method to evaluate the naturalness of natural language generation in dialogue systems. While this task was previously rendered through expensive and time-consuming human labor, we present this novel task of automatic naturalness evaluation of generated language. By fine-tuning the BERT model, our proposed naturalness evaluation method shows robust results and outperforms the baselines: support vector machines, bi-directional LSTMs, and BLEURT. In addition, the training speed and evaluation performance of naturalness model are improved by transfer learning from quality and informativeness linguistic knowledge.</abstract>
      <url hash="e059c241">2021.ranlp-1.96</url>
      <bibkey>liu-etal-2021-naturalness</bibkey>
      <revision id="1" href="2021.ranlp-1.96v1" hash="e10078ae"/>
      <revision id="2" href="2021.ranlp-1.96v2" hash="e059c241" date="2021-12-02">Revised author emails</revision>
    </paper>
    <paper id="97">
      <title>Towards the Application of Calibrated Transformers to the Unsupervised Estimation of Question Difficulty from Text</title>
      <author><first>Ekaterina</first><last>Loginova</last></author>
      <author><first>Luca</first><last>Benedetto</last></author>
      <author><first>Dries</first><last>Benoit</last></author>
      <author><first>Paolo</first><last>Cremonesi</last></author>
      <pages>846–855</pages>
      <abstract>Being able to accurately perform Question Difficulty Estimation (QDE) can improve the accuracy of students’ assessment and better their learning experience. Traditional approaches to QDE are either subjective or introduce a long delay before new questions can be used to assess students. Thus, recent work proposed machine learning-based approaches to overcome these limitations. They use questions of known difficulty to train models capable of inferring the difficulty of questions from their text. Once trained, they can be used to perform QDE of newly created questions. Existing approaches employ supervised models which are domain-dependent and require a large dataset of questions of known difficulty for training. Therefore, they cannot be used if such a dataset is not available ( for new courses on an e-learning platform). In this work, we experiment with the possibility of performing QDE from text in an unsupervised manner. Specifically, we use the uncertainty of calibrated question answering models as a proxy of human-perceived difficulty. Our experiments show promising results, suggesting that model uncertainty could be successfully leveraged to perform QDE from text, reducing both costs and elapsed time.</abstract>
      <url hash="78a2e15e">2021.ranlp-1.97</url>
      <bibkey>loginova-etal-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="98">
      <title><fixed-case>G</fixed-case>e<fixed-case>SERA</fixed-case>: General-domain Summary Evaluation by Relevance Analysis</title>
      <author><first>Jessica</first><last>López Espejel</last></author>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <author><first>Jorge</first><last>Garcia Flores</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <author><first>Ivan Vladimir</first><last>Meza Ruiz</last></author>
      <pages>856–867</pages>
      <abstract>We present GeSERA, an open-source improved version of SERA for evaluating automatic extractive and abstractive summaries from the general domain. SERA is based on a search engine that compares candidate and reference summaries (called queries) against an information retrieval document base (called index). SERA was originally designed for the biomedical domain only, where it showed a better correlation with manual methods than the widely used lexical-based ROUGE method. In this paper, we take out SERA from the biomedical domain to the general one by adapting its content-based method to successfully evaluate summaries from the general domain. First, we improve the query reformulation strategy with POS Tags analysis of general-domain corpora. Second, we replace the biomedical index used in SERA with two article collections from AQUAINT-2 and Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM datasets. Results show that, in most cases, GeSERA achieves higher correlations with manual evaluation methods than SERA, while it reduces its gap with ROUGE for general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases of TAC2009. Finally, we conduct extensive experiments and provide a comprehensive study of the impact of human annotators and the index size on summary evaluation with SERA and GeSERA.</abstract>
      <url hash="e0cb09db">2021.ranlp-1.98</url>
      <bibkey>lopez-espejel-etal-2021-gesera</bibkey>
    </paper>
    <paper id="99">
      <title>On the Interaction between Annotation Quality and Classifier Performance in Abusive Language Detection</title>
      <author><first>Holly</first><last>Lopez Long</last></author>
      <author><first>Alexandra</first><last>O’Neil</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>868–875</pages>
      <abstract>Abusive language detection has become an important tool for the cultivation of safe online platforms. We investigate the interaction of annotation quality and classifier performance. We use a new, fine-grained annotation scheme that allows us to distinguish between abusive language and colloquial uses of profanity that are not meant to harm. Our results show a tendency of crowd workers to overuse the abusive class, which creates an unrealistic class balance and affects classification accuracy. We also investigate different methods of distinguishing between explicit and implicit abuse and show lexicon-based approaches either over- or under-estimate the proportion of explicit abuse in data sets.</abstract>
      <url hash="9368fe61">2021.ranlp-1.99</url>
      <bibkey>lopez-long-etal-2021-interaction</bibkey>
    </paper>
    <paper id="100">
      <title><fixed-case>NEREL</fixed-case>: A <fixed-case>R</fixed-case>ussian Dataset with Nested Named Entities, Relations and Events</title>
      <author><first>Natalia</first><last>Loukachevitch</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Tatiana</first><last>Batura</last></author>
      <author><first>Pavel</first><last>Braslavski</last></author>
      <author><first>Ilia</first><last>Denisov</last></author>
      <author><first>Vladimir</first><last>Ivanov</last></author>
      <author><first>Suresh</first><last>Manandhar</last></author>
      <author><first>Alexander</first><last>Pugachev</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>876–885</pages>
      <abstract>In this paper, we present NEREL, a Russian dataset for named entity recognition and relation extraction. NEREL is significantly larger than existing Russian datasets: to date it contains 56K annotated named entities and 39K annotated relations. Its important difference from previous datasets is annotation of nested named entities, as well as relations within nested entities and at the discourse level. NEREL can facilitate development of novel models that can extract relations between nested named entities, as well as relations on both sentence and document levels. NEREL also contains the annotation of events involving named entities and their roles in the events. The NEREL collection is available via <url>https://github.com/nerel-ds/NEREL</url>.</abstract>
      <url hash="6d26cec5">2021.ranlp-1.100</url>
      <bibkey>loukachevitch-etal-2021-nerel</bibkey>
      <pwccode url="https://github.com/nerel-ds/nerel" additional="false">nerel-ds/nerel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nne">NNE</pwcdataset>
    </paper>
    <paper id="101">
      <title>Active Learning for Interactive Relation Extraction in a <fixed-case>F</fixed-case>rench Newspaper’s Articles</title>
      <author><first>Cyrielle</first><last>Mallart</last></author>
      <author><first>Michel</first><last>Le Nouy</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>886–894</pages>
      <abstract>Relation extraction is a subtask of natural langage processing that has seen many improvements in recent years, with the advent of complex pre-trained architectures. Many of these state-of-the-art approaches are tested against benchmarks with labelled sentences containing tagged entities, and require important pre-training and fine-tuning on task-specific data. However, in a real use-case scenario such as in a newspaper company mostly dedicated to local information, relations are of varied, highly specific type, with virtually no annotated data for such relations, and many entities co-occur in a sentence without being related. We question the use of supervised state-of-the-art models in such a context, where resources such as time, computing power and human annotators are limited. To adapt to these constraints, we experiment with an active-learning based relation extraction pipeline, consisting of a binary LSTM-based lightweight model for detecting the relations that do exist, and a state-of-the-art model for relation classification. We compare several choices for classification models in this scenario, from basic word embedding averaging, to graph neural networks and Bert-based ones, as well as several active learning acquisition strategies, in order to find the most cost-efficient yet accurate approach in our French largest daily newspaper company’s use case.</abstract>
      <url hash="2d5a218b">2021.ranlp-1.101</url>
      <bibkey>mallart-etal-2021-active</bibkey>
    </paper>
    <paper id="102">
      <title><fixed-case>ROFF</fixed-case> - A <fixed-case>R</fixed-case>omanian <fixed-case>T</fixed-case>witter Dataset for Offensive Language</title>
      <author><first>Mihai</first><last>Manolescu</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>895–900</pages>
      <abstract>This paper describes the annotation process of an offensive language data set for Romanian on social media. To facilitate comparable multi-lingual research on offensive language, the annotation guidelines follow some of the recent annotation efforts for other languages. The final corpus contains 5000 micro-blogging posts annotated by a large number of volunteer annotators. The inter-annotator agreement and the initial automatic discrimination results we present are in line with earlier annotation efforts.</abstract>
      <url hash="85427b26">2021.ranlp-1.102</url>
      <bibkey>manolescu-coltekin-2021-roff</bibkey>
    </paper>
    <paper id="103">
      <title>Monitoring Fact Preservation, Grammatical Consistency and Ethical Behavior of Abstractive Summarization Neural Models</title>
      <author><first>Iva</first><last>Marinova</last></author>
      <author><first>Yolina</first><last>Petrova</last></author>
      <author><first>Milena</first><last>Slavcheva</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Ivaylo</first><last>Radev</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <pages>901–909</pages>
      <abstract>The paper describes a system for automatic summarization in English language of online news data that come from different non-English languages. The system is designed to be used in production environment for media monitoring. Automatic summarization can be very helpful in this domain when applied as a helper tool for journalists so that they can review just the important information from the news channels. However, like every software solution, the automatic summarization needs performance monitoring and assured safe environment for the clients. In media monitoring environment the most problematic features to be addressed are: the copyright issues, the factual consistency, the style of the text and the ethical norms in journalism. Thus, the main contribution of our present work is that the above mentioned characteristics are successfully monitored in neural automatic summarization models and improved with the help of validation, fact-preserving and fact-checking procedures.</abstract>
      <url hash="3b8dfd1d">2021.ranlp-1.103</url>
      <bibkey>marinova-etal-2021-monitoring</bibkey>
    </paper>
    <paper id="104">
      <title>Cultural Topic Modelling over Novel <fixed-case>W</fixed-case>ikipedia Corpora for <fixed-case>S</fixed-case>outh-<fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Filip</first><last>Markoski</last></author>
      <author><first>Elena</first><last>Markoska</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Eftim</first><last>Zdravevski</last></author>
      <author><first>Ljupco</first><last>Kocarev</last></author>
      <pages>910–917</pages>
      <abstract>There is a shortage of high-quality corpora for South-Slavic languages. Such corpora are useful to computer scientists and researchers in social sciences and humanities alike, focusing on numerous linguistic, content analysis, and natural language processing applications. This paper presents a workflow for mining Wikipedia content and processing it into linguistically-processed corpora, applied on the Bosnian, Bulgarian, Croatian, Macedonian, Serbian, Serbo-Croatian and Slovenian Wikipedia. We make the resulting seven corpora publicly available. We showcase these corpora by comparing the content of the underlying Wikipedias, our assumption being that the content of the Wikipedias reflects broadly the interests in various topics in these Balkan nations. We perform the content comparison by using topic modelling algorithms and various distribution comparisons. The results show that all Wikipedias are topically rather similar, with all of them covering art, culture, and literature, whereas they contain differences in geography, politics, history and science.</abstract>
      <url hash="f213bb3a">2021.ranlp-1.104</url>
      <bibkey>markoski-etal-2021-cultural</bibkey>
    </paper>
    <paper id="105">
      <title>Discovery of Multiword Expressions with Loanwords and Their Equivalents in the <fixed-case>P</fixed-case>ersian Language</title>
      <author><first>Katarzyna</first><last>Marszałek-Kowalewska</last></author>
      <pages>918–928</pages>
      <abstract>This paper presents an attempt at multiword expressions (MWEs) discovery in the Persian language. It focuses on extracting MWEs containing lemmas of a particular group: loanwords in Persian and their equivalents proposed by the Academy of Persian Language and Literature. In order to discover such MWEs, four association measures (AMs) are used and evaluated. Finally, the list of extracted MWEs is analyzed, and a comparison between expressions with loanwords and equivalents is presented. To our knowledge, this is the first time such analysis was provided for the Persian language.</abstract>
      <url hash="ef47ea0b">2021.ranlp-1.105</url>
      <bibkey>marszalek-kowalewska-2021-discovery</bibkey>
    </paper>
    <paper id="106">
      <title>The Impact of Text Normalization on Multiword Expressions Discovery in <fixed-case>P</fixed-case>ersian</title>
      <author><first>Katarzyna</first><last>Marszałek-Kowalewska</last></author>
      <pages>929–939</pages>
      <abstract>This paper evaluates normalization procedures of Persian text for a downstream NLP task - multiword expressions (MWEs) discovery. We discuss the challenges the Persian language poses for NLP and evaluate open-source tools that try to address these difficulties. The best-performing tool is later used in the main task - MWEs discovery. In order to discover MWEs, we use association measures and a subpart of the MirasText corpus. The results show that an F-score is 26% higher in the case of normalized input data.</abstract>
      <url hash="d9b503fb">2021.ranlp-1.106</url>
      <bibkey>marszalek-kowalewska-2021-impact</bibkey>
    </paper>
    <paper id="107">
      <title>Improving Neural Language Processing with Named Entities</title>
      <author><first>Kyoumoto</first><last>Matsushita</last></author>
      <author><first>Takuya</first><last>Makino</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>940–949</pages>
      <abstract>Pretraining-based neural network models have demonstrated state-of-the-art (SOTA) performances on natural language processing (NLP) tasks. The most frequently used sentence representation for neural-based NLP methods is a sequence of subwords that is different from the sentence representation of non-neural methods that are created using basic NLP technologies, such as part-of-speech (POS) tagging, named entity (NE) recognition, and parsing. Most neural-based NLP models receive only vectors encoded from a sequence of subwords obtained from an input text. However, basic NLP information, such as POS tags, NEs, parsing results, etc, cannot be obtained explicitly from only the large unlabeled text used in pretraining-based models. This paper explores use of NEs on two Japanese tasks; document classification and headline generation using Transformer-based models, to reveal the effectiveness of basic NLP information. The experimental results with eight basic NEs and approximately 200 extended NEs show that NEs improve accuracy although a large pretraining-based model trained using 70 GB text data was used.</abstract>
      <url hash="dfaa62c8">2021.ranlp-1.107</url>
      <bibkey>matsushita-etal-2021-improving</bibkey>
    </paper>
    <paper id="108">
      <title><fixed-case>TREM</fixed-case>o<fixed-case>L</fixed-case>o-Tweets: A Multi-Label Corpus of <fixed-case>F</fixed-case>rench Tweets for Language Register Characterization</title>
      <author><first>Jade</first><last>Mekki</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Nicolas</first><last>Béchet</last></author>
      <pages>950–958</pages>
      <abstract>The casual, neutral, and formal language registers are highly perceptible in discourse productions. However, they are still poorly studied in Natural Language Processing (NLP), especially outside English, and for new textual types like tweets. To stimulate research, this paper introduces a large corpus of 228,505 French tweets (6M words) annotated in language registers. Labels are provided by a multi-label CamemBERT classifier trained and checked on a manually annotated subset of the corpus, while the tweets are selected to avoid undesired biases. Based on the corpus, an initial analysis of linguistic traits from either human annotators or automatic extractions is provided to describe the corpus and pave the way for various NLP tasks. The corpus, annotation guide and classifier are available on <url>http://tremolo.irisa.fr</url>.</abstract>
      <url hash="db4effdd">2021.ranlp-1.108</url>
      <bibkey>mekki-etal-2021-tremolo-tweets</bibkey>
    </paper>
    <paper id="109">
      <title>Ranking Online Reviews Based on Their Helpfulness: An Unsupervised Approach</title>
      <author><first>Alimuddin</first><last>Melleng</last></author>
      <author><first>Anna</first><last>Jurek-Loughrey</last></author>
      <author><first>Deepak</first><last>P</last></author>
      <pages>959–967</pages>
      <abstract>Online reviews are an essential aspect of online shopping for both customers and retailers. However, many reviews found on the Internet lack in quality, informativeness or helpfulness. In many cases, they lead the customers towards positive or negative opinions without providing any concrete details (e.g., very poor product, I would not recommend it). In this work, we propose a novel unsupervised method for quantifying helpfulness leveraging the availability of a corpus of reviews. In particular, our method exploits three characteristics of the reviews, viz., relevance, emotional intensity and specificity, towards quantifying helpfulness. We perform three rankings (one for each feature above), which are then combined to obtain a final helpfulness ranking. For the purpose of empirically evaluating our method, we use review of four product categories from Amazon review. The experimental evaluation demonstrates the effectiveness of our method in comparison to a recent and state-of-the-art baseline.</abstract>
      <url hash="91db477f">2021.ranlp-1.109</url>
      <bibkey>melleng-etal-2021-ranking</bibkey>
    </paper>
    <paper id="110">
      <title>incom.py 2.0 - Calculating Linguistic Distances and Asymmetries in Auditory Perception of Closely Related Languages</title>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <author><first>Bernd</first><last>Möbius</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>968–977</pages>
      <abstract>We present an extended version of a tool developed for calculating linguistic distances and asymmetries in auditory perception of closely related languages. Along with evaluating the metrics available in the initial version of the tool, we introduce word adaptation entropy as an additional metric of linguistic asymmetry. Potential predictors of speech intelligibility are validated with human performance in spoken cognate recognition experiments for Bulgarian and Russian. Special attention is paid to the possibly different contributions of vowels and consonants in oral intercomprehension. Using incom.py 2.0 it is possible to calculate, visualize, and validate three measurement methods of linguistic distances and asymmetries as well as carrying out regression analyses in speech intelligibility between related languages.</abstract>
      <url hash="4b08f894">2021.ranlp-1.110</url>
      <bibkey>mosbach-etal-2021-incom</bibkey>
    </paper>
    <paper id="111">
      <title>Not All Linearizations Are Equally Data-Hungry in Sequence Labeling Parsing</title>
      <author><first>Alberto</first><last>Muñoz-Ortiz</last></author>
      <author><first>Michalina</first><last>Strzyz</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <pages>978–988</pages>
      <abstract>Different linearizations have been proposed to cast dependency parsing as sequence labeling and solve the task as: (i) a head selection problem, (ii) finding a representation of the token arcs as bracket strings, or (iii) associating partial transition sequences of a transition-based parser to words. Yet, there is little understanding about how these linearizations behave in low-resource setups. Here, we first study their data efficiency, simulating data-restricted setups from a diverse set of rich-resource treebanks. Second, we test whether such differences manifest in truly low-resource setups. The results show that head selection encodings are more data-efficient and perform better in an ideal (gold) framework, but that such advantage greatly vanishes in favour of bracketing formats when the running setup resembles a real-world low-resource configuration.</abstract>
      <url hash="720e3c2b">2021.ranlp-1.111</url>
      <bibkey>munoz-ortiz-etal-2021-linearizations</bibkey>
    </paper>
    <paper id="112">
      <title>Pre-training a <fixed-case>BERT</fixed-case> with Curriculum Learning by Increasing Block-Size of Input Text</title>
      <author><first>Koichi</first><last>Nagatsuka</last></author>
      <author><first>Clifford</first><last>Broni-Bediako</last></author>
      <author><first>Masayasu</first><last>Atsumi</last></author>
      <pages>989–996</pages>
      <abstract>Recently, pre-trained language representation models such as BERT and RoBERTa have achieved significant results in a wide range of natural language processing (NLP) tasks, however, it requires extremely high computational cost. Curriculum Learning (CL) is one of the potential solutions to alleviate this problem. CL is a training strategy where training samples are given to models in a meaningful order instead of random sampling. In this work, we propose a new CL method which gradually increases the block-size of input text for training the self-attention mechanism of BERT and its variants using the maximum available batch-size. Experiments in low-resource settings show that our approach outperforms the baseline in terms of convergence speed and final performance on downstream tasks.</abstract>
      <url hash="961140b4">2021.ranlp-1.112</url>
      <bibkey>nagatsuka-etal-2021-pre</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="113">
      <title><fixed-case>COVID</fixed-case>-19 in <fixed-case>B</fixed-case>ulgarian Social Media: Factuality, Harmfulness, Propaganda, and Framing</title>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Shaden</first><last>Shaar</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Yifan</first><last>Zhang</last></author>
      <pages>997–1009</pages>
      <abstract>With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic is currently ranked very high on the list of priorities of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. With this in mind, we studied how COVID-19 is discussed in Bulgarian social media in terms of factuality, harmfulness, propaganda, and framing. We found that most Bulgarian tweets contain verifiable factual claims, are factually true, are of potential public interest, are not harmful, and are too trivial to fact-check; moreover, zooming into harmful tweets, we found that they spread not only rumors but also panic. We further analyzed articles shared in Bulgarian partisan pro/con-COVID-19 Facebook groups and found that propaganda is more prevalent in skeptical articles, which use doubt, flag waving, and slogans to convey their message; in contrast, concerned ones appeal to emotions, fear, and authority; moreover, skeptical articles frame the issue as one of quality of life, policy, legality, economy, and politics, while concerned articles focus on health &amp; safety. We release our manually and automatically analyzed datasets to enable further research.</abstract>
      <url hash="c01c91d0">2021.ranlp-1.113</url>
      <bibkey>nakov-etal-2021-covid</bibkey>
      <pwccode url="https://gitlab.com/sshaar/covid-19-in-bulgarian-social-media" additional="false">sshaar/covid-19-in-bulgarian-social-media</pwccode>
    </paper>
    <paper id="114">
      <title>A Second Pandemic? Analysis of Fake News about <fixed-case>COVID</fixed-case>-19 Vaccines in <fixed-case>Q</fixed-case>atar</title>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Shaden</first><last>Shaar</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Yifan</first><last>Zhang</last></author>
      <pages>1010–1021</pages>
      <abstract>While COVID-19 vaccines are finally becoming widely available, a second pandemic that revolves around the circulation of anti-vaxxer “fake news” may hinder efforts to recover from the first one. With this in mind, we performed an extensive analysis of Arabic and English tweets about COVID-19 vaccines, with focus on messages originating from Qatar. We found that Arabic tweets contain a lot of false information and rumors, while English tweets are mostly factual. However, English tweets are much more propagandistic than Arabic ones. In terms of propaganda techniques, about half of the Arabic tweets express doubt, and 1/5 use loaded language, while English tweets are abundant in loaded language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in terms of framing, Arabic tweets adopt a health and safety perspective, while in English economic concerns dominate.</abstract>
      <url hash="3e321faa">2021.ranlp-1.114</url>
      <bibkey>nakov-etal-2021-second</bibkey>
    </paper>
    <paper id="115">
      <title>A Hierarchical Entity Graph Convolutional Network for Relation Extraction across Documents</title>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>1022–1030</pages>
      <abstract>Distantly supervised datasets for relation extraction mostly focus on sentence-level extraction, and they cover very few relations. In this work, we propose cross-document relation extraction, where the two entities of a relation tuple appear in two different documents that are connected via a chain of common entities. Following this idea, we create a dataset for two-hop relation extraction, where each chain contains exactly two documents. Our proposed dataset covers a higher number of relations than the publicly available sentence-level datasets. We also propose a hierarchical entity graph convolutional network (HEGCN) model for this task that improves performance by 1.1% F1 score on our two-hop relation extraction dataset, compared to some strong neural baselines.</abstract>
      <url hash="e6651f2a">2021.ranlp-1.115</url>
      <bibkey>nayak-ng-2021-hierarchical</bibkey>
      <pwccode url="https://github.com/nusnlp/MHRE" additional="false">nusnlp/MHRE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/thred">THRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="116">
      <title>Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering</title>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>1031–1039</pages>
      <abstract>Distantly supervised models are very popular for relation extraction since we can obtain a large amount of training data using the distant supervision method without human annotation. In distant supervision, a sentence is considered as a source of a tuple if the sentence contains both entities of the tuple. However, this condition is too permissive and does not guarantee the presence of relevant relation-specific information in the sentence. As such, distantly supervised training data contains much noise which adversely affects the performance of the models. In this paper, we propose a self-ensemble filtering mechanism to filter out the noisy samples during the training process. We evaluate our proposed framework on the New York Times dataset which is obtained via distant supervision. Our experiments with multiple state-of-the-art neural relation extraction models show that our proposed filtering mechanism improves the robustness of the models and increases their F1 scores.</abstract>
      <url hash="698ef6ca">2021.ranlp-1.116</url>
      <bibkey>nayak-etal-2021-improving</bibkey>
      <pwccode url="https://github.com/nayakt/SENF4DSRE" additional="false">nayakt/SENF4DSRE</pwccode>
    </paper>
    <paper id="117">
      <title>Learning Entity-Likeness with Multiple Approximate Matches for Biomedical <fixed-case>NER</fixed-case></title>
      <author><first>An</first><last>Nguyen Le</last></author>
      <author><first>Hajime</first><last>Morita</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>1040–1049</pages>
      <abstract>Biomedical Named Entities are complex, so approximate matching has been used to improve entity coverage. However, the usual approximate matching approach fetches only one matching result, which is often noisy. In this work, we propose a method for biomedical NER that fetches multiple approximate matches for a given phrase to leverage their variations to estimate entity-likeness. The model uses pooling to discard the unnecessary information from the noisy matching results, and learn the entity-likeness of the phrase with multiple approximate matches. Experimental results on three benchmark datasets from the biomedical domain, BC2GM, NCBI-disease, and BC4CHEMD, demonstrate the effectiveness. Our model improves the average by up to +0.21 points compared to a BioBERT-based NER.</abstract>
      <url hash="3ea5a289">2021.ranlp-1.117</url>
      <bibkey>nguyen-le-etal-2021-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
    </paper>
    <paper id="118">
      <title>Extending a Text-to-Pictograph System to <fixed-case>F</fixed-case>rench and to Arasaac</title>
      <author><first>Magali</first><last>Norré</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>1050–1059</pages>
      <abstract>We present an adaptation of the Text-to-Picto system, initially designed for Dutch, and extended to English and Spanish. The original system, aimed at people with an intellectual disability, automatically translates text into pictographs (Sclera and Beta). We extend it to French and add a large set of Arasaac pictographs linked to WordNet 3.1. To carry out this adaptation, we automatically link the pictographs and their metadata to synsets of two French WordNets and leverage this information to translate words into pictographs. We automatically and manually evaluate our system with different corpora corresponding to different use cases, including one for medical communication between doctors and patients. The system is also compared to similar systems in other languages.</abstract>
      <url hash="918dcd2f">2021.ranlp-1.118</url>
      <bibkey>norre-etal-2021-extending</bibkey>
    </paper>
    <paper id="119">
      <title>Transfer-based Enrichment of a <fixed-case>H</fixed-case>ungarian Named Entity Dataset</title>
      <author><first>Attila</first><last>Novák</last></author>
      <author><first>Borbála</first><last>Novák</last></author>
      <pages>1060–1067</pages>
      <abstract>In this paper, we present a major update to the first Hungarian named entity dataset, the Szeged NER corpus. We used zero-shot cross-lingual transfer to initialize the enrichment of entity types annotated in the corpus using three neural NER models: two of them based on the English OntoNotes corpus and one based on the Czech Named Entity Corpus finetuned from multilingual neural language models. The output of the models was automatically merged with the original NER annotation, and automatically and manually corrected and further enriched with additional annotation, like qualifiers for various entity types. We present the evaluation of the zero-shot performance of the two OntoNotes-based models and a transformer-based new NER model trained on the training part of the final corpus. We release the corpus and the trained model.</abstract>
      <url hash="14c8d1e1">2021.ranlp-1.119</url>
      <bibkey>novak-novak-2021-transfer</bibkey>
    </paper>
    <paper id="120">
      <title>One Size Does Not Fit All: Finding the Optimal Subword Sizes for <fixed-case>F</fixed-case>ast<fixed-case>T</fixed-case>ext Models across Languages</title>
      <author><first>Vít</first><last>Novotný</last></author>
      <author><first>Eniafe Festus</first><last>Ayetiran</last></author>
      <author><first>Dalibor</first><last>Bačovský</last></author>
      <author><first>Dávid</first><last>Lupták</last></author>
      <author><first>Michal</first><last>Štefánik</last></author>
      <author><first>Petr</first><last>Sojka</last></author>
      <pages>1068–1074</pages>
      <abstract>Unsupervised representation learning of words from large multilingual corpora is useful for downstream tasks such as word sense disambiguation, semantic text similarity, and information retrieval. The representation precision of log-bilinear fastText models is mostly due to their use of subword information. In previous work, the optimization of fastText’s subword sizes has not been fully explored, and non-English fastText models were trained using subword sizes optimized for English and German word analogy tasks. In our work, we find the optimal subword sizes on the English, German, Czech, Italian, Spanish, French, Hindi, Turkish, and Russian word analogy tasks. We then propose a simple n-gram coverage model and we show that it predicts better-than-default subword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy tasks. We show that the optimization of fastText’s subword sizes matters and results in a 14% improvement on the Czech word analogy task. We also show that expensive parameter optimization can be replaced by a simple n-gram coverage model that consistently improves the accuracy of fastText models on the word analogy tasks by up to 3% compared to the default subword sizes, and that it is within 1% accuracy of the optimal subword sizes.</abstract>
      <url hash="c842e90a">2021.ranlp-1.120</url>
      <bibkey>novotny-etal-2021-one</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>CL</fixed-case>ex<fixed-case>IS</fixed-case>2: A New Corpus for Complex Word Identification Research in Computing Studies</title>
      <author><first>Jenny A.</first><last>Ortiz Zambrano</last></author>
      <author><first>Arturo</first><last>Montejo-Ráez</last></author>
      <pages>1075–1083</pages>
      <abstract>Reading is a complex process not only because of the words or sections that are difficult for the reader to understand. Complex word identification (CWI) is the task of detecting in the content of documents the words that are difficult or complex to understand by the people of a certain group. Annotated corpora for English learners are widely available, while they are less common for the Spanish language. In this article, we present CLexIS<tex-math>^2</tex-math>, a new corpus in Spanish to contribute to the advancement of research in the area of Lexical Simplification, specifically in the identification and prediction of complex words in computing studies. Several metrics used to evaluate the complexity of texts in Spanish were applied, such as LC, LDI, ILFW, SSR, SCI, ASL, CS. Furthermore, as a baseline of the primer, two experiments have been performed to predict the complexity of words: one using a supervised learning approach and the other using an unsupervised solution based on the frequency of words on a general corpus.</abstract>
      <url hash="993144d7">2021.ranlp-1.121</url>
      <bibkey>ortiz-zambrano-montejo-raez-2021-clexis2</bibkey>
    </paper>
    <paper id="122">
      <title>Towards Precise Lexicon Integration in Neural Machine Translation</title>
      <author><first>Ogün</first><last>Öz</last></author>
      <author><first>Maria</first><last>Sukhareva</last></author>
      <pages>1084–1095</pages>
      <abstract>Terminological consistency is an essential requirement for industrial translation. High-quality, hand-crafted terminologies contain entries in their nominal forms. Integrating such a terminology into machine translation is not a trivial task. The MT system must be able to disambiguate homographs on the source side and choose the correct wordform on the target side. In this work, we propose a simple but effective method for homograph disambiguation and a method of wordform selection by introducing multi-choice lexical constraints. We also propose a metric to measure the terminological consistency of the translation. Our results have a significant improvement over the current SOTA in terms of terminological consistency without any loss of the BLEU score. All the code used in this work will be published as open-source.</abstract>
      <url hash="d8241a53">2021.ranlp-1.122</url>
      <bibkey>oz-sukhareva-2021-towards</bibkey>
      <pwccode url="https://github.com/term-integration-mt/term-integration-mt" additional="false">term-integration-mt/term-integration-mt</pwccode>
    </paper>
    <paper id="123">
      <title><fixed-case>O</fixed-case>ffend<fixed-case>ES</fixed-case>: A New Corpus in <fixed-case>S</fixed-case>panish for Offensive Language Research</title>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last></author>
      <author><first>Arturo</first><last>Montejo-Ráez</last></author>
      <author><first>L. Alfonso</first><last>Ureña-López</last></author>
      <author><first>María-Teresa</first><last>Martín-Valdivia</last></author>
      <pages>1096–1108</pages>
      <abstract>Offensive language detection and analysis has become a major area of research in Natural Language Processing. The freedom of participation in social media has exposed online users to posts designed to denigrate, insult or hurt them according to gender, race, religion, ideology, or other personal characteristics. Focusing on young influencers from the well-known social platforms of Twitter, Instagram, and YouTube, we have collected a corpus composed of 47,128 Spanish comments manually labeled on offensive pre-defined categories. A subset of the corpus attaches a degree of confidence to each label, so both multi-class classification and multi-output regression studies are possible. In this paper, we introduce the corpus, discuss its building process, novelties, and some preliminary experiments with it to serve as a baseline for the research community.</abstract>
      <url hash="5ee36108">2021.ranlp-1.123</url>
      <bibkey>plaza-del-arco-etal-2021-offendes</bibkey>
    </paper>
    <paper id="124">
      <title>On Machine Translation of User Reviews</title>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Marija</first><last>Brkic</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>1109–1118</pages>
      <abstract>This work investigates neural machine translation (NMT) systems for translating English user reviews into Croatian and Serbian, two similar morphologically complex languages. Two types of reviews are used for testing the systems: IMDb movie reviews and Amazon product reviews. Two types of training data are explored: large out-of-domain bilingual parallel corpora, as well as small synthetic in-domain parallel corpus obtained by machine translation of monolingual English Amazon reviews into the target languages. Both automatic scores and human evaluation show that using the synthetic in-domain corpus together with a selected sub-set of out-of-domain data is the best option. Separated results on IMDb and Amazon reviews indicate that MT systems perform differently on different review types so that user reviews generally should not be considered as a homogeneous genre. Nevertheless, more detailed research on larger amount of different reviews covering different domains/topics is needed to fully understand these differences.</abstract>
      <url hash="d1c16114">2021.ranlp-1.124</url>
      <bibkey>popovic-etal-2021-machine</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="125">
      <title>Multilingual Coreference Resolution with Harmonized Annotations</title>
      <author><first>Ondřej</first><last>Pražák</last></author>
      <author><first>Miloslav</first><last>Konopík</last></author>
      <author><first>Jakub</first><last>Sido</last></author>
      <pages>1119–1123</pages>
      <abstract>In this paper, we present coreference resolution experiments with a newly created multilingual corpus CorefUD (Nedoluzhko et al.,2021). We focus on the following languages: Czech, Russian, Polish, German, Spanish, and Catalan. In addition to monolingual experiments, we combine the training data in multilingual experiments and train two joined models - for Slavic languages and for all the languages together. We rely on an end-to-end deep learning model that we slightly adapted for the CorefUD corpus. Our results show that we can profit from harmonized annotations, and using joined models helps significantly for the languages with smaller training data.</abstract>
      <url hash="75d3d4b2">2021.ranlp-1.125</url>
      <bibkey>prazak-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="126">
      <title>Predicting Informativeness of Semantic Triples</title>
      <author><first>Judita</first><last>Preiss</last></author>
      <pages>1124–1129</pages>
      <abstract>Many automatic semantic relation extraction tools extract subject-predicate-object triples from unstructured text. However, a large quantity of these triples merely represent background knowledge. We explore using full texts of biomedical publications to create a training corpus of informative and important semantic triples based on the notion that the main contributions of an article are summarized in its abstract. This corpus is used to train a deep learning classifier to identify important triples, and we suggest that an importance ranking for semantic triples could also be generated.</abstract>
      <url hash="49a01627">2021.ranlp-1.126</url>
      <bibkey>preiss-2021-predicting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
    </paper>
    <paper id="127">
      <title>Unknown Intent Detection Using Multi-Objective Optimization on Deep Learning Classifiers</title>
      <author><first>Prerna</first><last>Prem</last></author>
      <author><first>Zishan</first><last>Ahmad</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Shubhashis</first><last>Sengupta</last></author>
      <author><first>Sakshi C.</first><last>Jain</last></author>
      <author><first>Roshni</first><last>Ramnani</last></author>
      <pages>1130–1137</pages>
      <abstract>Modelling and understanding dialogues in a conversation depends on identifying the user intent from the given text. Unknown or new intent detection is a critical task, as in a realistic scenario a user intent may frequently change over time and divert even to an intent previously not encountered. This task of separating the unknown intent samples from known intents one is challenging as the unknown user intent can range from intents similar to the predefined intents to something completely different. Prior research on intent discovery often consider it as a classification task where an unknown intent can belong to a predefined set of known intent classes. In this paper we tackle the problem of detecting a completely unknown intent without any prior hints about the kind of classes belonging to unknown intents. We propose an effective post-processing method using multi-objective optimization to tune an existing neural network based intent classifier and make it capable of detecting unknown intents. We perform experiments using existing state-of-the-art intent classifiers and use our method on top of them for unknown intent detection. Our experiments across different domains and real-world datasets show that our method yields significant improvements compared with the state-of-the-art methods for unknown intent detection.</abstract>
      <url hash="80f84340">2021.ranlp-1.127</url>
      <bibkey>prem-etal-2021-unknown</bibkey>
    </paper>
    <paper id="128">
      <title>Are the Multilingual Models Better? Improving <fixed-case>C</fixed-case>zech Sentiment with Transformers</title>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Josef</first><last>Steinberger</last></author>
      <pages>1138–1149</pages>
      <abstract>In this paper, we aim at improving Czech sentiment with transformer-based models and their multilingual versions. More concretely, we study the task of polarity detection for the Czech language on three sentiment polarity datasets. We fine-tune and perform experiments with five multilingual and three monolingual models. We compare the monolingual and multilingual models’ performance, including comparison with the older approach based on recurrent neural networks. Furthermore, we test the multilingual models and their ability to transfer knowledge from English to Czech (and vice versa) with zero-shot cross-lingual classification. Our experiments show that the huge multilingual models can overcome the performance of the monolingual models. They are also able to detect polarity in another language without any training data, with performance not worse than 4.4 % compared to state-of-the-art monolingual trained models. Moreover, we achieved new state-of-the-art results on all three datasets.</abstract>
      <url hash="0db38fe1">2021.ranlp-1.128</url>
      <bibkey>priban-steinberger-2021-multilingual</bibkey>
      <pwccode url="https://github.com/pauli31/improving-czech-sentiment-transformers" additional="false">pauli31/improving-czech-sentiment-transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="129">
      <title>Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment</title>
      <author><first>Charith</first><last>Rajitha</last></author>
      <author><first>Lakmali</first><last>Piyarathna</last></author>
      <author><first>Dilan</first><last>Sachintha</last></author>
      <author><first>Surangika</first><last>Ranathunga</last></author>
      <pages>1150–1157</pages>
      <abstract>Document alignment techniques based on multilingual sentence representations have recently shown state of the art results. However, these techniques rely on unsupervised distance measurement techniques, which cannot be fined-tuned to the task at hand. In this paper, instead of these unsupervised distance measurement techniques, we employ Metric Learning to derive task-specific distance measurements. These measurements are supervised, meaning that the distance measurement metric is trained using a parallel dataset. Using a dataset belonging to English, Sinhala, and Tamil, which belong to three different language families, we show that these task-specific supervised distance learning metrics outperform their unsupervised counterparts, for document alignment.</abstract>
      <url hash="5beb5960">2021.ranlp-1.129</url>
      <bibkey>rajitha-etal-2021-metric</bibkey>
    </paper>
    <paper id="130">
      <title>Multi-label Diagnosis Classification of <fixed-case>S</fixed-case>wedish Discharge Summaries – <fixed-case>ICD</fixed-case>-10 Code Assignment Using <fixed-case>KB</fixed-case>-<fixed-case>BERT</fixed-case></title>
      <author><first>Sonja</first><last>Remmer</last></author>
      <author><first>Anastasios</first><last>Lamproudis</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>1158–1166</pages>
      <abstract>The International Classification of Diseases (ICD) is a system for systematically recording patients’ diagnoses. Clinicians or professional coders assign ICD codes to patients’ medical records to facilitate funding, research, and administration. In most health facilities, clinical coding is a manual, time-demanding task that is prone to errors. A tool that automatically assigns ICD codes to free-text clinical notes could save time and reduce erroneous coding. While many previous studies have focused on ICD coding, research on Swedish patient records is scarce. This study explored different approaches to pairing Swedish clinical notes with ICD codes. KB-BERT, a BERT model pre-trained on Swedish text, was compared to the traditional supervised learning models Support Vector Machines, Decision Trees, and K-nearest Neighbours used as the baseline. When considering ICD codes grouped into ten blocks, the KB-BERT was superior to the baseline models, obtaining an F1-micro of 0.80 and an F1-macro of 0.58. When considering the 263 full ICD codes, the KB-BERT was outperformed by all baseline models at an F1-micro and F1-macro of zero. Wilcoxon signed-rank tests showed that the performance differences between the KB-BERT and the baseline models were statistically significant.</abstract>
      <url hash="cdf4157c">2021.ranlp-1.130</url>
      <bibkey>remmer-etal-2021-multi</bibkey>
    </paper>
    <paper id="131">
      <title><fixed-case>S</fixed-case>iamese Networks for Inference in <fixed-case>M</fixed-case>alayalam Language Texts</title>
      <author><first>Sara</first><last>Renjit</last></author>
      <author><first>Sumam Mary</first><last>Idicula</last></author>
      <pages>1167–1173</pages>
      <abstract>Natural language inference is a method of finding inferences in language texts. Understanding the meaning of a sentence and its inference is essential in many language processing applications. In this context, we consider the inference problem for a Dravidian language, Malayalam. Siamese networks train the text hypothesis pairs with word embeddings and language agnostic embeddings, and the results are evaluated against classification metrics for binary classification into entailment and contradiction classes. XLM-R embeddings based Siamese architecture using gated recurrent units and bidirectional long short term memory networks provide promising results for this classification problem.</abstract>
      <url hash="c908650c">2021.ranlp-1.131</url>
      <bibkey>renjit-idicula-2021-siamese</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="132">
      <title>A Call for Clarity in Contemporary Authorship Attribution Evaluation</title>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Patrick</first><last>Juola</last></author>
      <pages>1174–1179</pages>
      <abstract>Recent research has documented that results reported in frequently-cited authorship attribution papers are difficult to reproduce. Inaccessible code and data are often proposed as factors which block successful reproductions. Even when original materials are available, problems remain which prevent researchers from comparing the effectiveness of different methods. To solve the remaining problems—the lack of fixed test sets and the use of inappropriately homogeneous corpora—our paper contributes materials for five closed-set authorship identification experiments. The five experiments feature texts from 106 distinct authors. Experiments involve a range of contemporary non-fiction American English prose. These experiments provide the foundation for comparable and reproducible authorship attribution research involving contemporary writing.</abstract>
      <url hash="a2649043">2021.ranlp-1.132</url>
      <bibkey>riddell-etal-2021-call</bibkey>
    </paper>
    <paper id="133">
      <title>Varieties of Plain Language</title>
      <author><first>Allen</first><last>Riddell</last></author>
      <author><first>Yohei</first><last>Igarashi</last></author>
      <pages>1180–1187</pages>
      <abstract>Many organizations seek or need to produce documents that are written plainly. In the United States, the “Plain Writing Act of 2010” requires that many federal agencies’ documents for the public are written in plain English. In particular, the government’s Plain Language Action and Information Network (“PLAIN”) recommends that writers use short sentences and everyday words, as does the Securities and Exchange Commission’s “Plain English Rule.” Since the 1970s, American plain language advocates have moved away from readability measures and favored usability testing and document design considerations. But in this paper we use quantitative measures of sentence length and word difficulty that (1) reveal stylistic variation among PLAIN’s exemplars of plain writing, and (2) help us position PLAIN’s exemplars relative to documents written in other kinds of accessible English (e.g., The New York Times, Voice of America Special English, and Wikipedia) and one academic document likely to be perceived as difficult. Uncombined measures for sentences and vocabulary—left separate, unlike in traditional readability formulas—can complement usability testing and document design considerations, and advance knowledge about different types of plainer English.</abstract>
      <url hash="a06c120d">2021.ranlp-1.133</url>
      <bibkey>riddell-igarashi-2021-varieties</bibkey>
    </paper>
    <paper id="134">
      <title>Word Discriminations for Vocabulary Inventory Prediction</title>
      <author><first>Frankie</first><last>Robertson</last></author>
      <pages>1188–1195</pages>
      <abstract>The aim of vocabulary inventory prediction is to predict a learner’s whole vocabulary based on a limited sample of query words. This paper approaches the problem starting from the 2-parameter Item Response Theory (IRT) model, giving each word in the vocabulary a difficulty and discrimination parameter. The discrimination parameter is evaluated on the sub-problem of question item selection, familiar from the fields of Computerised Adaptive Testing (CAT) and active learning. Next, the effect of the discrimination parameter on prediction performance is examined, both in a binary classification setting, and in an information retrieval setting. Performance is compared with baselines based on word frequency. A number of different generalisation scenarios are examined, including generalising word difficulty and discrimination using word embeddings with a predictor network and testing on out-of-dataset data.</abstract>
      <url hash="512a125b">2021.ranlp-1.134</url>
      <bibkey>robertson-2021-word</bibkey>
      <pwccode url="https://github.com/frankier/vocabirt" additional="false">frankier/vocabirt</pwccode>
    </paper>
    <paper id="135">
      <title><fixed-case>F</fixed-case>ren<fixed-case>L</fixed-case>y<fixed-case>S</fixed-case>: A Tool for the Automatic Simplification of <fixed-case>F</fixed-case>rench General Language Texts</title>
      <author><first>Eva</first><last>Rolin</last></author>
      <author><first>Quentin</first><last>Langlois</last></author>
      <author><first>Patrick</first><last>Watrin</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>1196–1205</pages>
      <abstract>Lexical simplification (LS) aims at replacing words considered complex in a sentence by simpler equivalents. In this paper, we present the first automatic LS service for French, FrenLys, which offers different techniques to generate, select and rank substitutes. The paper describes the different methods proposed by our tool, which includes both classical approaches (e.g. generation of candidates from lexical resources, frequency filter, etc.) and more innovative approaches such as the exploitation of CamemBERT, a model for French based on the RoBERTa architecture. To evaluate the different methods, a new evaluation dataset for French is introduced.</abstract>
      <url hash="9b946155">2021.ranlp-1.135</url>
      <bibkey>rolin-etal-2021-frenlys</bibkey>
    </paper>
    <paper id="136">
      <title>Spelling Correction for <fixed-case>R</fixed-case>ussian: A Comparative Study of Datasets and Methods</title>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>1206–1216</pages>
      <abstract>We develop a minimally-supervised model for spelling correction and evaluate its performance on three datasets annotated for spelling errors in Russian. The first corpus is a dataset of Russian social media data that was recently used in a shared task on Russian spelling correction. The other two corpora contain texts produced by learners of Russian as a foreign language. Evaluating on three diverse datasets allows for a cross-corpus comparison. We compare the performance of the minimally-supervised model to two baseline models that do not use context for candidate re-ranking, as well as to a character-level statistical machine translation system with context-based re-ranking. We show that the minimally-supervised model outperforms all of the other models. We also present an analysis of the spelling errors and discuss the difficulty of the task compared to the spelling correction problem in English.</abstract>
      <url hash="b3f1a9bc">2021.ranlp-1.136</url>
      <bibkey>rozovskaya-2021-spelling</bibkey>
    </paper>
    <paper id="137">
      <title>Sentiment-Aware Measure (<fixed-case>SAM</fixed-case>) for Evaluating Sentiment Transfer by Machine Translation Systems</title>
      <author><first>Hadeel</first><last>Saadany</last></author>
      <author><first>Constantin</first><last>Orăsan</last></author>
      <author><first>Emad</first><last>Mohamed</last></author>
      <author><first>Ashraf</first><last>Tantavy</last></author>
      <pages>1217–1226</pages>
      <abstract>In translating text where sentiment is the main message, human translators give particular attention to sentiment-carrying words. The reason is that an incorrect translation of such words would miss the fundamental aspect of the source text, i.e. the author’s sentiment. In the online world, MT systems are extensively used to translate User-Generated Content (UGC) such as reviews, tweets, and social media posts, where the main message is often the author’s positive or negative attitude towards the topic of the text. It is important in such scenarios to accurately measure how far an MT system can be a reliable real-life utility in transferring the correct affect message. This paper tackles an under-recognized problem in the field of machine translation evaluation which is judging to what extent automatic metrics concur with the gold standard of human evaluation for a correct translation of sentiment. We evaluate the efficacy of conventional quality metrics in spotting a mistranslation of sentiment, especially when it is the sole error in the MT output. We propose a numerical “sentiment-closeness” measure appropriate for assessing the accuracy of a translated affect message in UGC text by an MT system. We will show that incorporating this sentiment-aware measure can significantly enhance the correlation of some available quality metrics with the human judgement of an accurate translation of sentiment.</abstract>
      <url hash="30b66b8b">2021.ranlp-1.137</url>
      <bibkey>saadany-etal-2021-sentiment</bibkey>
    </paper>
    <paper id="138">
      <title>Multilingual Epidemic Event Extraction : From Simple Classification Methods to Open Information Extraction (<fixed-case>OIE</fixed-case>) and Ontology</title>
      <author><first>Sihem</first><last>Sahnoun</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>1227–1233</pages>
      <abstract>There is an incredible amount of information available in the form of textual documents due to the growth of information sources. In order to get the information into an actionable way, it is common to use information extraction and more specifically the event extraction, it became crucial in various domains even in public health. In this paper, we address the problem of the epidemic event extraction in potentially any language, so that we tested different corpuses on an existed multilingual system for tele-epidemiology: the Data Analysis for Information Extraction in any Language(DANIEL) system. We focused on the influence of the number of documents on the performance of the system, on average results show that it is able to achieve a precision and recall around 82%, but when we resorted to the evaluation by event by checking whether it has been really detected or not, the results are not satisfactory according to this paper’s evaluation. Our idea is to propose a system that uses an ontology which includes information in different languages and covers specific epidemiological concepts, it is also based on the multilingual open information extraction for the relation extraction step to reduce the expert intervention and to restrict the content for each text. We describe a methodology of five main stages: Pre-processing, relation extraction, named entity recognition (NER), event recognition and the matching between the information extracted and the ontology.</abstract>
      <url hash="97082ded">2021.ranlp-1.138</url>
      <bibkey>sahnoun-lejeune-2021-multilingual</bibkey>
    </paper>
    <paper id="139">
      <title>Exploiting Domain-Specific Knowledge for Judgment Prediction Is No Panacea</title>
      <author><first>Olivier</first><last>Salaün</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Karim</first><last>Benyekhlef</last></author>
      <pages>1234–1243</pages>
      <abstract>Legal judgment prediction (LJP) usually consists in a text classification task aimed at predicting the verdict on the basis of the fact description. The literature shows that the use of articles as input features helps improve the classification performance. In this work, we designed a verdict prediction task based on landlord-tenant disputes and we applied BERT-based models to which we fed different article-based features. Although the results obtained are consistent with the literature, the improvements with the articles are mostly obtained with the most frequent labels, suggesting that pre-trained and fine-tuned transformer-based models are not scalable as is for legal reasoning in real life scenarios as they would only excel in accurately predicting the most recurrent verdicts to the detriment of other legal outcomes.</abstract>
      <url hash="9df33d9e">2021.ranlp-1.139</url>
      <bibkey>salaun-etal-2021-exploiting</bibkey>
    </paper>
    <paper id="140">
      <title>Masking and Transformer-based Models for Hyperpartisanship Detection in News</title>
      <author><first>Javier</first><last>Sánchez-Junquera</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Manuel</first><last>Montes-y-Gómez</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <pages>1244–1251</pages>
      <abstract>Hyperpartisan news show an extreme manipulation of reality based on an underlying and extreme ideological orientation. Because of its harmful effects at reinforcing one’s bias and the posterior behavior of people, hyperpartisan news detection has become an important task for computational linguists. In this paper, we evaluate two different approaches to detect hyperpartisan news. First, a text masking technique that allows us to compare style vs. topic-related features in a different perspective from previous work. Second, the transformer-based models BERT, XLM-RoBERTa, and M-BERT, known for their ability to capture semantic and syntactic patterns in the same representation. Our results corroborate previous research on this task in that topic-related features yield better results than style-based ones, although they also highlight the relevance of using higher-length n-grams. Furthermore, they show that transformer-based models are more effective than traditional methods, but this at the cost of greater computational complexity and lack of transparency. Based on our experiments, we conclude that the beginning of the news show relevant information for the transformers at distinguishing effectively between left-wing, mainstream, and right-wing orientations.</abstract>
      <url hash="e43a0870">2021.ranlp-1.140</url>
      <bibkey>sanchez-junquera-etal-2021-masking</bibkey>
    </paper>
    <paper id="141">
      <title><fixed-case>S</fixed-case>erbian <fixed-case>NER</fixed-case>&amp;Beyond: The Archaic and the Modern Intertwinned</title>
      <author><first>Branislava</first><last>Šandrih Todorović</last></author>
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Milica</first><last>Ikonić Nešić</last></author>
      <pages>1252–1260</pages>
      <abstract>In this work, we present a Serbian literary corpus that is being developed under the umbrella of the “Distant Reading for European Literary History” COST Action CA16204. Using this corpus of novels written more than a century ago, we have developed and made publicly available a Named Entity Recognizer (NER) trained to recognize 7 different named entity types, with a Convolutional Neural Network (CNN) architecture, having F1 score of ≈91% on the test dataset. This model has been further assessed on a separate evaluation dataset. We wrap up with comparison of the developed model with the existing one, followed by a discussion of pros and cons of the both models.</abstract>
      <url hash="120b6b2b">2021.ranlp-1.141</url>
      <bibkey>sandrih-todorovic-etal-2021-serbian</bibkey>
    </paper>
    <paper id="142">
      <title>A Semi-Supervised Approach to Detect Toxic Comments</title>
      <author><first>Ghivvago Damas</first><last>Saraiva</last></author>
      <author><first>Rafael</first><last>Anchiêta</last></author>
      <author><first>Francisco Assis Ricarte</first><last>Neto</last></author>
      <author><first>Raimundo</first><last>Moura</last></author>
      <pages>1261–1267</pages>
      <abstract>Toxic comments contain forms of non-acceptable language targeted towards groups or individuals. These types of comments become a serious concern for government organizations, online communities, and social media platforms. Although there are some approaches to handle non-acceptable language, most of them focus on supervised learning and the English language. In this paper, we deal with toxic comment detection as a semi-supervised strategy over a heterogeneous graph. We evaluate the approach on a toxic dataset of the Portuguese language, outperforming several graph-based methods and achieving competitive results compared to transformer architectures.</abstract>
      <url hash="3954ad1d">2021.ranlp-1.142</url>
      <bibkey>saraiva-etal-2021-semi</bibkey>
      <pwccode url="https://github.com/rafaelanchieta/toxic" additional="false">rafaelanchieta/toxic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/told-br">ToLD-Br</pwcdataset>
    </paper>
    <paper id="143">
      <title>Graph-based Argument Quality Assessment</title>
      <author><first>Ekaterina</first><last>Saveleva</last></author>
      <author><first>Volha</first><last>Petukhova</last></author>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>1268–1280</pages>
      <abstract>The paper presents a novel discourse-based approach to argument quality assessment defined as a graph classification task, where the depth of reasoning (argumentation) is evident from the number and type of detected discourse units and relations between them. We successfully applied state-of-the-art discourse parsers and machine learning models to reconstruct argument graphs with the identified and classified discourse units as nodes and relations between them as edges. Then Graph Neural Networks were trained to predict the argument quality assessing its acceptability, relevance, sufficiency and overall cogency. The obtained accuracy ranges from 74.5% to 85.0% and indicates that discourse-based argument structures reflect qualitative properties of natural language arguments. The results open many interesting prospects for future research in the field of argumentation mining.</abstract>
      <url hash="c9f72892">2021.ranlp-1.143</url>
      <bibkey>saveleva-etal-2021-graph</bibkey>
    </paper>
    <paper id="144">
      <title>A Hybrid Approach of Opinion Mining and Comparative Linguistic Analysis of Restaurant Reviews</title>
      <author><first>Salim</first><last>Sazzed</last></author>
      <pages>1281–1288</pages>
      <abstract>The existing research on sentiment analysis mainly utilized data curated in limited geographical regions and demography (e.g., USA, UK, China) due to commercial interest and availability of review data. Since the user’s attitudes and preferences can be affected by numerous sociocultural factors and demographic characteristics, it is necessary to have annotated review datasets belong to various demography. In this work, we first construct a review dataset <i>BanglaRestaurant</i> that contains over 2300 customer reviews towards a number of Bangladeshi restaurants. Then, we present a hybrid methodology that yields improvement over the best performing lexicon-based and machine learning (ML) based classifier without using any labeled data. Finally, we investigate how the demography (i.e., geography and nativeness in English) of users affect the linguistic characteristics of the reviews by contrasting two datasets, <i>BanglaRestaurant</i> and <i>Yelp</i>. The comparative results demonstrate the efficacy of the proposed hybrid approach. The data analysis reveals that demography plays an influential role in the linguistic aspects of reviews.</abstract>
      <url hash="e3b4e220">2021.ranlp-1.144</url>
      <bibkey>sazzed-2021-hybrid</bibkey>
    </paper>
    <paper id="145">
      <title>A Lexicon for Profane and Obscene Text Identification in <fixed-case>B</fixed-case>engali</title>
      <author><first>Salim</first><last>Sazzed</last></author>
      <pages>1289–1296</pages>
      <abstract>Bengali is a low-resource language that lacks tools and resources for profane and obscene textual content detection. Until now, no lexicon exists for detecting obscenity in Bengali social media text. This study introduces a Bengali obscene lexicon consisting of over 200 Bengali terms, which can be considered filthy, slang, profane or obscene. A semi-automatic methodology is presented for developing the profane lexicon that leverages an obscene corpus, word embedding, and part-of-speech (POS) taggers. The developed lexicon achieves coverage of around 0.85 for obscene and profane content detection in an evaluation dataset. The experimental results imply that the developed lexicon is effective at identifying obscenity in Bengali social media content.</abstract>
      <url hash="f2e0fe9d">2021.ranlp-1.145</url>
      <bibkey>sazzed-2021-lexicon</bibkey>
    </paper>
    <paper id="146">
      <title>A Case Study of Deep Learning-Based Multi-Modal Methods for Labeling the Presence of Questionable Content in Movie Trailers</title>
      <author><first>Mahsa</first><last>Shafaei</last></author>
      <author><first>Christos</first><last>Smailis</last></author>
      <author><first>Ioannis</first><last>Kakadiaris</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>1297–1307</pages>
      <abstract>In this work, we explore different approaches to combine modalities for the problem of automated age-suitability rating of movie trailers. First, we introduce a new dataset containing videos of movie trailers in English downloaded from IMDB and YouTube, along with their corresponding age-suitability rating labels. Secondly, we propose a multi-modal deep learning pipeline addressing the movie trailer age suitability rating problem. This is the first attempt to combine video, audio, and speech information for this problem, and our experimental results show that multi-modal approaches significantly outperform the best mono and bimodal models in this task.</abstract>
      <url hash="0905a505">2021.ranlp-1.146</url>
      <bibkey>shafaei-etal-2021-case</bibkey>
    </paper>
    <paper id="147">
      <title>A Domain-Independent Holistic Approach to Deception Detection</title>
      <author><first>Sadat</first><last>Shahriar</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <author><first>Omprakash</first><last>Gnawali</last></author>
      <pages>1308–1317</pages>
      <abstract>The deception in the text can be of different forms in different domains, including fake news, rumor tweets, and spam emails. Irrespective of the domain, the main intent of the deceptive text is to deceit the reader. Although domain-specific deception detection exists, domain-independent deception detection can provide a holistic picture, which can be crucial to understand how deception occurs in the text. In this paper, we detect deception in a domain-independent setting using deep learning architectures. Our method outperforms the State-of-the-Art performance of most benchmark datasets with an overall accuracy of 93.42% and F1-Score of 93.22%. The domain-independent training allows us to capture subtler nuances of deceptive writing style. Furthermore, we analyze how much in-domain data may be helpful to accurately detect deception, especially for the cases where data may not be readily available to train. Our results and analysis indicate that there may be a universal pattern of deception lying in-between the text independent of the domain, which can create a novel area of research and open up new avenues in the field of deception detection.</abstract>
      <url hash="d366f906">2021.ranlp-1.147</url>
      <bibkey>shahriar-etal-2021-domain</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fakenewsnet">FakeNewsNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
    </paper>
    <paper id="148">
      <title>Towards Domain-Generalizable Paraphrase Identification by Avoiding the Shortcut Learning</title>
      <author><first>Xin</first><last>Shen</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>1318–1325</pages>
      <abstract>In this paper, we investigate the Domain Generalization (DG) problem for supervised Paraphrase Identification (PI). We observe that the performance of existing PI models deteriorates dramatically when tested in an out-of-distribution (OOD) domain. We conjecture that it is caused by shortcut learning, i.e., these models tend to utilize the cue words that are unique for a particular dataset or domain. To alleviate this issue and enhance the DG ability, we propose a PI framework based on Optimal Transport (OT). Our method forces the network to learn the necessary features for all the words in the input, which alleviates the shortcut learning problem. Experimental results show that our method improves the DG ability for the PI models.</abstract>
      <url hash="587cec72">2021.ranlp-1.148</url>
      <bibkey>shen-lam-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parade">PARADE</pwcdataset>
    </paper>
    <paper id="149">
      <title>Czert – <fixed-case>C</fixed-case>zech <fixed-case>BERT</fixed-case>-like Model for Language Representation</title>
      <author><first>Jakub</first><last>Sido</last></author>
      <author><first>Ondřej</first><last>Pražák</last></author>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Jan</first><last>Pašek</last></author>
      <author><first>Michal</first><last>Seják</last></author>
      <author><first>Miloslav</first><last>Konopík</last></author>
      <pages>1326–1338</pages>
      <abstract>This paper describes the training process of the first Czech monolingual language representation models based on BERT and ALBERT architectures. We pre-train our models on more than 340K of sentences, which is 50 times more than multilingual models that include Czech data. We outperform the multilingual models on 9 out of 11 datasets. In addition, we establish the new state-of-the-art results on nine datasets. At the end, we discuss properties of monolingual and multilingual models based upon our results. We publish all the pre-trained and fine-tuned models freely for the research community.</abstract>
      <url hash="380cf52a">2021.ranlp-1.149</url>
      <bibkey>sido-etal-2021-czert</bibkey>
      <pwccode url="https://github.com/kiv-air/Czert" additional="false">kiv-air/Czert</pwccode>
    </paper>
    <paper id="150">
      <title>Exploring <fixed-case>G</fixed-case>erman Multi-Level Text Simplification</title>
      <author><first>Nicolas</first><last>Spring</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Sarah</first><last>Ebling</last></author>
      <pages>1339–1349</pages>
      <abstract>We report on experiments in automatic text simplification (ATS) for German with multiple simplification levels along the Common European Framework of Reference for Languages (CEFR), simplifying standard German into levels A1, A2 and B1. For that purpose, we investigate the use of source labels and pretraining on standard German, allowing us to simplify standard language to a specific CEFR level. We show that these approaches are especially effective in low-resource scenarios, where we are able to outperform a standard transformer baseline. Moreover, we introduce copy labels, which we show can help the model make a distinction between sentences that require further modifications and sentences that can be copied as-is.</abstract>
      <url hash="e98f430e">2021.ranlp-1.150</url>
      <bibkey>spring-etal-2021-exploring</bibkey>
      <pwccode url="https://github.com/zurichnlp/ranlp2021-german-ats" additional="false">zurichnlp/ranlp2021-german-ats</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="151">
      <title>Exploring Reliability of Gold Labels for Emotion Detection in <fixed-case>T</fixed-case>witter</title>
      <author><first>Sanja</first><last>Stajner</last></author>
      <pages>1350–1359</pages>
      <abstract>Emotion detection from social media posts has attracted noticeable attention from natural language processing (NLP) community in recent years. The ways for obtaining gold labels for training and testing of the systems for automatic emotion detection differ significantly from one study to another, and pose the question of reliability of gold labels and obtained classification results. This study systematically explores several ways for obtaining gold labels for Ekman’s emotion model on Twitter data and the influence of the chosen strategy on the manual classification results.</abstract>
      <url hash="c77ca756">2021.ranlp-1.151</url>
      <bibkey>stajner-2021-exploring</bibkey>
    </paper>
    <paper id="152">
      <title>How to Obtain Reliable Labels for <fixed-case>MBTI</fixed-case> Classification from Texts?</title>
      <author><first>Sanja</first><last>Stajner</last></author>
      <author><first>Seren</first><last>Yenikent</last></author>
      <pages>1360–1368</pages>
      <abstract>Automatic detection of the Myers-Briggs Type Indicator (MBTI) from short posts attracted noticeable attention in the last few years. Recent studies showed that this is quite a difficult task, especially on commonly used Twitter data. Obtaining MBTI labels is also difficult, as human annotation requires trained psychologists, and automatic way of obtaining them is through long questionnaires of questionable usability for the task. In this paper, we present a method for collecting reliable MBTI labels via only four carefully selected questions that can be applied to any type of textual data.</abstract>
      <url hash="1041f3c2">2021.ranlp-1.152</url>
      <bibkey>stajner-yenikent-2021-obtain</bibkey>
    </paper>
    <paper id="153">
      <title>Watching a Language Model Learning Chess</title>
      <author><first>Andreas</first><last>Stöckl</last></author>
      <pages>1369–1379</pages>
      <abstract>We analyse how a transformer-based language model learns the rules of chess from text data of recorded games. We show how it is possible to investigate how the model capacity and the available number of training data influence the learning success of a language model with the help of chess-specific metrics. With these metrics, we show that more games used for training in the studied range offers significantly better results for the same training time. However, model size does not show such a clear influence. It is also interesting to observe that the usual evaluation metrics for language models, predictive accuracy and perplexity, give no indication of this here. Further examination of trained models reveals how they store information about board state in the activations of neuron groups, and how the overall sequence of previous moves influences the newly-generated moves.</abstract>
      <url hash="7eeb28e3">2021.ranlp-1.153</url>
      <bibkey>stockl-2021-watching</bibkey>
    </paper>
    <paper id="154">
      <title>Tackling Multilinguality and Internationality in Fake News</title>
      <author><first>Andrey</first><last>Tagarev</last></author>
      <author><first>Krasimira</first><last>Bozhanova</last></author>
      <author><first>Ivelina</first><last>Nikolova-Koleva</last></author>
      <author><first>Ivan</first><last>Ivanov</last></author>
      <pages>1380–1386</pages>
      <abstract>The last several years have seen a massive increase in the quantity and influence of disinformation being spread online. Various approaches have been developed to target the process at different stages from identifying sources to tracking distribution in social media to providing follow up debunks to people who have encountered the disinformation. One common conclusion in each of these approaches is that disinformation is too nuanced and subjective a topic for fully automated solutions to work but the quantity of data to process and cross-reference is too high for humans to handle unassisted. Ultimately, the problem calls for a hybrid approach of human experts with technological assistance. In this paper we will demonstrate the application of certain state-of-the-art NLP techniques in assisting expert debunkers and fact checkers as well as the role of these NLP algorithms within a more holistic approach to analyzing and countering the spread of disinformation. We will present a multilingual corpus of disinformation and debunks which contains text, concept tags, images and videos as well as various methods for searching and leveraging the content.</abstract>
      <url hash="2648dcec">2021.ranlp-1.154</url>
      <bibkey>tagarev-etal-2021-tackling</bibkey>
    </paper>
    <paper id="155">
      <title>Learning and Evaluating <fixed-case>C</fixed-case>hinese Idiom Embeddings</title>
      <author><first>Minghuan</first><last>Tan</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>1387–1396</pages>
      <abstract>We study the task of learning and evaluating Chinese idiom embeddings. We first construct a new evaluation dataset that contains idiom synonyms and antonyms. Observing that existing Chinese word embedding methods may not be suitable for learning idiom embeddings, we further present a BERT-based method that directly learns embedding vectors for individual idioms. We empirically compare representative existing methods and our method. We find that our method substantially outperforms existing methods on the evaluation dataset we have constructed.</abstract>
      <url hash="f175af3e">2021.ranlp-1.155</url>
      <bibkey>tan-jiang-2021-learning</bibkey>
      <pwccode url="https://github.com/VisualJoyce/ChengyuBERT" additional="false">VisualJoyce/ChengyuBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cos960">COS960</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chid">ChID</pwcdataset>
    </paper>
    <paper id="156">
      <title>Does <fixed-case>BERT</fixed-case> Understand Idioms? A Probing-Based Empirical Study of <fixed-case>BERT</fixed-case> Encodings of Idioms</title>
      <author><first>Minghuan</first><last>Tan</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>1397–1407</pages>
      <abstract>Understanding idioms is important in NLP. In this paper, we study to what extent pre-trained BERT model can encode the meaning of a potentially idiomatic expression (PIE) in a certain context. We make use of a few existing datasets and perform two probing tasks: PIE usage classification and idiom paraphrase identification. Our experiment results suggest that BERT indeed can separate the literal and idiomatic usages of a PIE with high accuracy. It is also able to encode the idiomatic meaning of a PIE to some extent.</abstract>
      <url hash="2f9b144b">2021.ranlp-1.156</url>
      <bibkey>tan-jiang-2021-bert</bibkey>
    </paper>
    <paper id="157">
      <title>An Empirical Analysis of Topic Models: Uncovering the Relationships between Hyperparameters, Document Length and Performance Measures</title>
      <author><first>Silvia</first><last>Terragni</last></author>
      <author><first>Elisabetta</first><last>Fersini</last></author>
      <pages>1408–1416</pages>
      <abstract>Neural Topic Models are recent neural models that aim at extracting the main themes from a collection of documents. The comparison of these models is usually limited because the hyperparameters are held fixed. In this paper, we present an empirical analysis and comparison of Neural Topic Models by finding the optimal hyperparameters of each model for four different performance measures adopting a single-objective Bayesian optimization. This allows us to determine the robustness of a topic model for several evaluation metrics. We also empirically show the effect of the length of the documents on different optimized metrics and discover which evaluation metrics are in conflict or agreement with each other.</abstract>
      <url hash="30599634">2021.ranlp-1.157</url>
      <bibkey>terragni-fersini-2021-empirical</bibkey>
    </paper>
    <paper id="158">
      <title><fixed-case>TR</fixed-case>-<fixed-case>SEQ</fixed-case>: Named Entity Recognition Dataset for <fixed-case>T</fixed-case>urkish Search Engine Queries</title>
      <author><first>Berkay</first><last>Topçu</last></author>
      <author><first>İlknur</first><last>Durgar El-Kahlout</last></author>
      <pages>1417–1422</pages>
      <abstract>Recognizing named entities in short search engine queries is a difficult task due to their weaker contextual information compared to long sentences. Standard named entity recognition (NER) systems that are trained on grammatically correct and long sentences fail to perform well on such queries. In this study, we share our efforts towards creating a cleaned and labeled dataset of real Turkish search engine queries (TR-SEQ) and introduce an extended label set to satisfy the search engine needs. A NER system is trained by applying the state-of-the-art deep learning method BERT to the collected data and its high performance on search engine queries is reported. Moreover, we compare our results with the state-of-the-art Turkish NER systems.</abstract>
      <url hash="c4f464ce">2021.ranlp-1.158</url>
      <bibkey>topcu-durgar-el-kahlout-2021-tr</bibkey>
    </paper>
    <paper id="159">
      <title>Opinion Prediction with User Fingerprinting</title>
      <author><first>Kishore</first><last>Tumarada</last></author>
      <author><first>Yifan</first><last>Zhang</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Eduard</first><last>Dragut</last></author>
      <author><first>Omprakash</first><last>Gnawali</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <pages>1423–1431</pages>
      <abstract>Opinion prediction is an emerging research area with diverse real-world applications, such as market research and situational awareness. We identify two lines of approaches to the problem of opinion prediction. One uses topic-based sentiment analysis with time-series modeling, while the other uses static embedding of text. The latter approaches seek user-specific solutions by generating user fingerprints. Such approaches are useful in predicting user’s reactions to unseen content. In this work, we propose a novel dynamic fingerprinting method that leverages contextual embedding of user’s comments conditioned on relevant user’s reading history. We integrate BERT variants with a recurrent neural network to generate predictions. The results show up to 13% improvement in micro F1-score compared to previous approaches. Experimental results show novel insights that were previously unknown such as better predictions for an increase in dynamic history length, the impact of the nature of the article on performance, thereby laying the foundation for further research.</abstract>
      <url hash="2527aad1">2021.ranlp-1.159</url>
      <bibkey>tumarada-etal-2021-opinion</bibkey>
      <pwccode url="https://github.com/kishoret04/opinionprediction" additional="false">kishoret04/opinionprediction</pwccode>
    </paper>
    <paper id="160">
      <title>Can Multilingual Transformers Fight the <fixed-case>COVID</fixed-case>-19 Infodemic?</title>
      <author><first>Lasitha</first><last>Uyangodage</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Hansi</first><last>Hettiarachchi</last></author>
      <pages>1432–1437</pages>
      <abstract>The massive spread of false information on social media has become a global risk especially in a global pandemic situation like COVID-19. False information detection has thus become a surging research topic in recent months. In recent years, supervised machine learning models have been used to automatically identify false information in social media. However, most of these machine learning models focus only on the language they were trained on. Given the fact that social media platforms are being used in different languages, managing machine learning models for each and every language separately would be chaotic. In this research, we experiment with multilingual models to identify false information in social media by using two recently released multilingual false information detection datasets. We show that multilingual models perform on par with the monolingual models and sometimes even better than the monolingual models to detect false information in social media making them more useful in real-world scenarios.</abstract>
      <url hash="5262fe1e">2021.ranlp-1.160</url>
      <bibkey>uyangodage-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="161">
      <title>Contextual-Lexicon Approach for Abusive Language Detection</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Fabiana</first><last>Rodrigues de Góes</last></author>
      <author><first>Isabelle</first><last>Carvalho</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>1438–1447</pages>
      <abstract>Since a lexicon-based approach is more elegant scientifically, explaining the solution components and being easier to generalize to other applications, this paper provides a new approach for offensive language and hate speech detection on social media, which embodies a lexicon of implicit and explicit offensive and swearing expressions annotated with contextual information. Due to the severity of the social media abusive comments in Brazil, and the lack of research in Portuguese, Brazilian Portuguese is the language used to validate the models. Nevertheless, our method may be applied to any other language. The conducted experiments show the effectiveness of the proposed approach, outperforming the current baseline methods for the Portuguese language.</abstract>
      <url hash="038bbcae">2021.ranlp-1.161</url>
      <bibkey>vargas-etal-2021-contextual</bibkey>
    </paper>
    <paper id="162">
      <title>Comparative Analysis of Fine-tuned Deep Learning Language Models for <fixed-case>ICD</fixed-case>-10 Classification Task for <fixed-case>B</fixed-case>ulgarian Language</title>
      <author><first>Boris</first><last>Velichkov</last></author>
      <author><first>Sylvia</first><last>Vassileva</last></author>
      <author><first>Simeon</first><last>Gerginov</last></author>
      <author><first>Boris</first><last>Kraychev</last></author>
      <author><first>Ivaylo</first><last>Ivanov</last></author>
      <author><first>Philip</first><last>Ivanov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>1448–1454</pages>
      <abstract>The task of automatic diagnosis encoding into standard medical classifications and ontologies, is of great importance in medicine - both to support the daily tasks of physicians in the preparation and reporting of clinical documentation, and for automatic processing of clinical reports. In this paper we investigate the application and performance of different deep learning transformers for automatic encoding in ICD-10 of clinical texts in Bulgarian. The comparative analysis attempts to find which approach is more efficient to be used for fine-tuning of pretrained BERT family transformer to deal with a specific domain terminology on a rare language as Bulgarian. On the one side are used SlavicBERT and MultiligualBERT, that are pretrained for common vocabulary in Bulgarian, but lack medical terminology. On the other hand in the analysis are used BioBERT, ClinicalBERT, SapBERT, BlueBERT, that are pretrained for medical terminology in English, but lack training for language models in Bulgarian, and more over for vocabulary in Cyrillic. In our research study all BERT models are fine-tuned with additional medical texts in Bulgarian and then applied to the classification task for encoding medical diagnoses in Bulgarian into ICD-10 codes. Big corpora of diagnosis in Bulgarian annotated with ICD-10 codes is used for the classification task. Such an analysis gives a good idea of which of the models would be suitable for tasks of a similar type and domain. The experiments and evaluation results show that both approaches have comparable accuracy.</abstract>
      <url hash="14d875db">2021.ranlp-1.162</url>
      <bibkey>velichkov-etal-2021-comparative</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
    </paper>
    <paper id="163">
      <title>Mistake Captioning: A Machine Learning Approach for Detecting Mistakes and Generating Instructive Feedback</title>
      <author><first>Anton</first><last>Vinogradov</last></author>
      <author><first>Andrew Miles</first><last>Byrd</last></author>
      <author><first>Brent</first><last>Harrison</last></author>
      <pages>1455–1462</pages>
      <abstract>Giving feedback to students is not just about marking their answers as correct or incorrect, but also finding mistakes in their thought process that led them to that incorrect answer. In this paper, we introduce a machine learning technique for mistake captioning, a task that attempts to identify mistakes and provide feedback meant to help learners correct these mistakes. We do this by training a sequence-to-sequence network to generate this feedback based on domain experts. To evaluate this system, we explore how it can be used on a Linguistics assignment studying Grimm’s Law. We show that our approach generates feedback that outperforms a baseline on a set of automated NLP metrics. In addition, we perform a series of case studies in which we examine successful and unsuccessful system outputs.</abstract>
      <url hash="c0482eca">2021.ranlp-1.163</url>
      <bibkey>vinogradov-etal-2021-mistake</bibkey>
    </paper>
    <paper id="164">
      <title>A Novel Machine Learning Based Approach for Post-<fixed-case>OCR</fixed-case> Error Detection</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <author><first>Azam</first><last>Sheikh Muhammad</last></author>
      <pages>1463–1470</pages>
      <abstract>Post processing is the most conventional approach for correcting errors that are caused by Optical Character Recognition(OCR) systems. Two steps are usually taken to correct OCR errors: detection and corrections. For the first task, supervised machine learning methods have shown state-of-the-art performances. Previously proposed approaches have focused most prominently on combining lexical, contextual and statistical features for detecting errors. In this study, we report a novel system to error detection which is based merely on the n-gram counts of a candidate token. In addition to being simple and computationally less expensive, our proposed system beats previous systems reported in the ICDAR2019 competition on OCR-error detection with notable margins. We achieved state-of-the-art F1-scores for eight out of the ten involved European languages. The maximum improvement is for Spanish which improved from 0.69 to 0.90, and the minimum for Polish from 0.82 to 0.84.</abstract>
      <url hash="e93275bd">2021.ranlp-1.164</url>
      <bibkey>virk-etal-2021-novel</bibkey>
    </paper>
    <paper id="165">
      <title>A Data-Driven Semi-Automatic Framenet Development Methodology</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Markus</first><last>Forsberg</last></author>
      <pages>1471–1479</pages>
      <abstract>FrameNet is a lexical semantic resource based on the linguistic theory of frame semantics. A number of framenet development strategies have been reported previously and all of them involve exploration of corpora and a fair amount of manual work. Despite previous efforts, there does not exist a well-thought-out automatic/semi-automatic methodology for frame construction. In this paper we propose a data-driven methodology for identification and semi-automatic construction of frames. As a proof of concept, we report on our initial attempts to build a wider-scale framenet for the legal domain (LawFN) using the proposed methodology. The constructed frames are stored in a lexical database and together with the annotated example sentences they have been made available through a web interface.</abstract>
      <url hash="c532124f">2021.ranlp-1.165</url>
      <bibkey>virk-etal-2021-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="166">
      <title>A Deep Learning System for Automatic Extraction of Typological Linguistic Information from Descriptive Grammars</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Daniel</first><last>Foster</last></author>
      <author><first>Azam</first><last>Sheikh Muhammad</last></author>
      <author><first>Raheela</first><last>Saleem</last></author>
      <pages>1480–1489</pages>
      <abstract>Linguistic typology is an area of linguistics concerned with analysis of and comparison between natural languages of the world based on their certain linguistic features. For that purpose, historically, the area has relied on manual extraction of linguistic feature values from textural descriptions of languages. This makes it a laborious and time expensive task and is also bound by human brain capacity. In this study, we present a deep learning system for the task of automatic extraction of linguistic features from textual descriptions of natural languages. First, textual descriptions are manually annotated with special structures called semantic frames. Those annotations are learned by a recurrent neural network, which is then used to annotate un-annotated text. Finally, the annotations are converted to linguistic feature values using a separate rule based module. Word embeddings, learned from general purpose text, are used as a major source of knowledge by the recurrent neural network. We compare the proposed deep learning system to a previously reported machine learning based system for the same task, and the deep learning system wins in terms of F1 scores with a fair margin. Such a system is expected to be a useful contribution for the automatic curation of typological databases, which otherwise are manually developed.</abstract>
      <url hash="f0d31c7a">2021.ranlp-1.166</url>
      <bibkey>virk-etal-2021-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="167">
      <title>Recognizing and Splitting Conditional Sentences for Automation of Business Processes Management</title>
      <author><first>Ngoc Phuoc An</first><last>Vo</last></author>
      <author><first>Irene</first><last>Manotas</last></author>
      <author><first>Octavian</first><last>Popescu</last></author>
      <author><first>Algimantas</first><last>Černiauskas</last></author>
      <author><first>Vadim</first><last>Sheinin</last></author>
      <pages>1490–1497</pages>
      <abstract>Business Process Management (BPM) is the discipline which is responsible for management of discovering, analyzing, redesigning, monitoring, and controlling business processes. One of the most crucial tasks of BPM is discovering and modelling business processes from text documents. In this paper, we present our system that resolves an end-to-end problem consisting of 1) recognizing conditional sentences from technical documents, 2) finding boundaries to extract conditional and resultant clauses from each conditional sentence, and 3) categorizing resultant clause as Action or Consequence which later helps to generate new steps in our business process model automatically. We created a new dataset and three models to solve this problem. Our best model achieved very promising results of 83.82, 87.84, and 85.75 for Precision, Recall, and F1, respectively, for extracting Condition, Action, and Consequence clauses using Exact Match metric.</abstract>
      <url hash="5018761c">2021.ranlp-1.167</url>
      <bibkey>vo-etal-2021-recognizing</bibkey>
    </paper>
    <paper id="168">
      <title>“Don’t discuss”: Investigating Semantic and Argumentative Features for Supervised Propagandist Message Detection and Classification</title>
      <author><first>Vorakit</first><last>Vorakitphan</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <pages>1498–1507</pages>
      <abstract>One of the mechanisms through which disinformation is spreading online, in particular through social media, is by employing propaganda techniques. These include specific rhetorical and psychological strategies, ranging from leveraging on emotions to exploiting logical fallacies. In this paper, our goal is to push forward research on propaganda detection based on text analysis, given the crucial role these methods may play to address this main societal issue. More precisely, we propose a supervised approach to classify textual snippets both as propaganda messages and according to the precise applied propaganda technique, as well as a detailed linguistic analysis of the features characterising propaganda information in text (e.g., semantic, sentiment and argumentation features). Extensive experiments conducted on two available propagandist resources (i.e., NLP4IF’19 and SemEval’20-Task 11 datasets) show that the proposed approach, leveraging different language models and the investigated linguistic features, achieves very promising results on propaganda classification, both at sentence- and at fragment-level.</abstract>
      <url hash="7d600c86">2021.ranlp-1.168</url>
      <bibkey>vorakitphan-etal-2021-dont</bibkey>
    </paper>
    <paper id="169">
      <title><fixed-case>C</fixed-case>ombo<fixed-case>NER</fixed-case>: A Lightweight All-In-One <fixed-case>POS</fixed-case> Tagger, Dependency Parser and <fixed-case>NER</fixed-case></title>
      <author><first>Aleksander</first><last>Wawer</last></author>
      <pages>1508–1514</pages>
      <abstract>The current natural language processing is strongly focused on raising accuracy. The progress comes at a cost of super-heavy models with hundreds of millions or even billions of parameters. However, simple syntactic tasks such as part-of-speech (POS) tagging, dependency parsing or named entity recognition (NER) do not require the largest models to achieve acceptable results. In line with this assumption we try to minimize the size of the model that jointly performs all three tasks. We introduce ComboNER: a lightweight tool, orders of magnitude smaller than state-of-the-art transformers. It is based on pre-trained subword embeddings and recurrent neural network architecture. ComboNER operates on Polish language data. The model has outputs for POS tagging, dependency parsing and NER. Our paper contains some insights from fine-tuning of the model and reports its overall results.</abstract>
      <url hash="afeb77ad">2021.ranlp-1.169</url>
      <bibkey>wawer-2021-comboner</bibkey>
    </paper>
    <paper id="170">
      <title>Investigating Annotator Bias in Abusive Language Datasets</title>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Christian</first><last>Widmer</last></author>
      <author><first>Gerhard</first><last>Hagerer</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>1515–1525</pages>
      <abstract>Nowadays, social media platforms use classification models to cope with hate speech and abusive language. The problem of these models is their vulnerability to bias. A prevalent form of bias in hate speech and abusive language datasets is annotator bias caused by the annotator’s subjective perception and the complexity of the annotation task. In our paper, we develop a set of methods to measure annotator bias in abusive language datasets and to identify different perspectives on abusive language. We apply these methods to four different abusive language datasets. Our proposed approach supports annotation processes of such datasets and future research addressing different perspectives on the perception of abusive language.</abstract>
      <url hash="fe3fbb54">2021.ranlp-1.170</url>
      <bibkey>wich-etal-2021-investigating</bibkey>
      <pwccode url="https://github.com/mawic/annotator-bias-abusive-language" additional="false">mawic/annotator-bias-abusive-language</pwccode>
    </paper>
    <paper id="171">
      <title>Rules Ruling Neural Networks - Neural vs. Rule-Based Grammar Checking for a Low Resource Language</title>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <author><first>Flammie</first><last>Pirinen</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Chiara</first><last>Argese</last></author>
      <pages>1526–1535</pages>
      <abstract>We investigate both rule-based and machine learning methods for the task of compound error correction and evaluate their efficiency for North Sámi, a low resource language. The lack of error-free data needed for a neural approach is a challenge to the development of these tools, which is not shared by bigger languages. In order to compensate for that, we used a rule-based grammar checker to remove erroneous sentences and insert compound errors by splitting correct compounds. We describe how we set up the error detection rules, and how we train a bi-RNN based neural network. The precision of the rule-based model tested on a corpus with real errors (81.0%) is slightly better than the neural model (79.4%). The rule-based model is also more flexible with regard to fixing specific errors requested by the user community. However, the neural model has a better recall (98%). The results suggest that an approach that combines the advantages of both models would be desirable in the future. Our tools and data sets are open-source and freely available on GitHub and Zenodo.</abstract>
      <url hash="d57f1f1b">2021.ranlp-1.171</url>
      <bibkey>wiechetek-etal-2021-rules</bibkey>
    </paper>
    <paper id="172">
      <title>Transformer with Syntactic Position Encoding for Machine Translation</title>
      <author><first>Yikuan</first><last>Xie</last></author>
      <author><first>Wenyong</first><last>Wang</last></author>
      <author><first>Mingqian</first><last>Du</last></author>
      <author><first>Qing</first><last>He</last></author>
      <pages>1536–1544</pages>
      <abstract>It has been widely recognized that syntax information can help end-to-end neural machine translation (NMT) systems to achieve better translation. In order to integrate dependency information into Transformer based NMT, existing approaches either exploit words’ local head-dependent relations, ignoring their non-local neighbors carrying important context; or approximate two words’ syntactic relation by their relative distance on the dependency tree, sacrificing exactness. To address these issues, we propose global positional encoding for dependency tree, a new scheme that facilitates syntactic relation modeling between any two words with keeping exactness and without immediate neighbor constraint. Experiment results on NC11 German→English, English→German and WMT English→German datasets show that our approach is more effective than the above two strategies. In addition, our experiments quantitatively show that compared with higher layers, lower layers of the model are more proper places to incorporate syntax information in terms of each layer’s preference to the syntactic pattern and the final performance.</abstract>
      <url hash="c2054ac3">2021.ranlp-1.172</url>
      <bibkey>xie-etal-2021-transformer</bibkey>
    </paper>
    <paper id="173">
      <title>Towards Sentiment Analysis of Tobacco Products’ Usage in Social Media</title>
      <author><first>Venkata Himakar</first><last>Yanamandra</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>1545–1552</pages>
      <abstract>Contemporary tobacco-related studies are mostly concerned with a single social media platform while missing out on a broader audience. Moreover, they are heavily reliant on labeled datasets, which are expensive to make. In this work, we explore sentiment and product identification on tobacco-related text from two social media platforms. We release SentiSmoke-Twitter and SentiSmoke-Reddit datasets, along with a comprehensive annotation schema for identifying tobacco products’ sentiment. We then perform benchmarking text classification experiments using state-of-the-art models, including BERT, RoBERTa, and DistilBERT. Our experiments show F1 scores as high as 0.72 for sentiment identification in the Twitter dataset, 0.46 for sentiment identification, and 0.57 for product identification using semi-supervised learning for Reddit.</abstract>
      <url hash="ea16cc3e">2021.ranlp-1.173</url>
      <bibkey>yanamandra-etal-2021-towards</bibkey>
    </paper>
    <paper id="174">
      <title>Improving Evidence Retrieval with Claim-Evidence Entailment</title>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Eduard</first><last>Dragut</last></author>
      <author><first>Arjun</first><last>Mukherjee</last></author>
      <pages>1553–1558</pages>
      <abstract>Claim verification is challenging because it requires first to find textual evidence and then apply claim-evidence entailment to verify a claim. Previous works evaluate the entailment step based on the retrieved evidence, whereas we hypothesize that the entailment prediction can provide useful signals for evidence retrieval, in the sense that if a sentence supports or refutes a claim, the sentence must be relevant. We propose a novel model that uses the entailment score to express the relevancy. Our experiments verify that leveraging entailment prediction improves ranking multiple pieces of evidence.</abstract>
      <url hash="cba32698">2021.ranlp-1.174</url>
      <bibkey>yang-etal-2021-improving-evidence</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="175">
      <title>Sentence Structure and Word Relationship Modeling for Emphasis Selection</title>
      <author><first>Haoran</first><last>Yang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>1559–1566</pages>
      <abstract>Emphasis Selection is a newly proposed task which focuses on choosing words for emphasis in short sentences. Traditional methods only consider the sequence information of a sentence while ignoring the rich sentence structure and word relationship information. In this paper, we propose a new framework that considers sentence structure via a sentence structure graph and word relationship via a word similarity graph. The sentence structure graph is derived from the parse tree of a sentence. The word similarity graph allows nodes to share information with their neighbors since we argue that in emphasis selection, similar words are more likely to be emphasized together. Graph neural networks are employed to learn the representation of each node of these two graphs. Experimental results demonstrate that our framework can achieve superior performance.</abstract>
      <url hash="ed2f11b3">2021.ranlp-1.175</url>
      <bibkey>yang-lam-2021-sentence</bibkey>
      <pwccode url="https://github.com/lhryang/emphasis-selection" additional="false">lhryang/emphasis-selection</pwccode>
    </paper>
    <paper id="176">
      <title>Utterance Position-Aware Dialogue Act Recognition</title>
      <author><first>Yuki</first><last>Yano</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <author><first>Hiroaki</first><last>Obayashi</last></author>
      <pages>1567–1574</pages>
      <abstract>This study proposes an utterance position-aware approach for a neural network-based dialogue act recognition (DAR) model, which incorporates positional encoding for utterance’s absolute or relative position. The proposed approach is inspired by the observation that some dialogue acts have tendencies of occurrence positions. The evaluations on the Switchboard corpus show that the proposed positional encoding of utterances statistically significantly improves the performance of DAR.</abstract>
      <url hash="fddce0ce">2021.ranlp-1.176</url>
      <bibkey>yano-etal-2021-utterance</bibkey>
    </paper>
    <paper id="177">
      <title>Tell Me What You Read: Automatic Expertise-Based Annotator Assignment for Text Annotation in Expert Domains</title>
      <author><first>Hiyori</first><last>Yoshikawa</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <author><first>Kimi</first><last>Kaneko</last></author>
      <author><first>Hiroaki</first><last>Yoshida</last></author>
      <author><first>Yasutaka</first><last>Kumano</last></author>
      <author><first>Kazutaka</first><last>Shimada</last></author>
      <author><first>Rafal</first><last>Rzepka</last></author>
      <author><first>Patrycja</first><last>Swieczkowska</last></author>
      <pages>1575–1585</pages>
      <abstract>This paper investigates the effectiveness of automatic annotator assignment for text annotation in expert domains. In the task of creating high-quality annotated corpora, expert domains often cover multiple sub-domains (e.g. organic and inorganic chemistry in the chemistry domain) either explicitly or implicitly. Therefore, it is crucial to assign annotators to documents relevant with their fine-grained domain expertise. However, most of existing methods for crowdsoucing estimate reliability of each annotator or annotated instance only after the annotation process. To address the issue, we propose a method to estimate the domain expertise of each annotator before the annotation process using information easily available from the annotators beforehand. We propose two measures to estimate the annotator expertise: an explicit measure using the predefined categories of sub-domains, and an implicit measure using distributed representations of the documents. The experimental results on chemical name annotation tasks show that the annotation accuracy improves when both explicit and implicit measures for annotator assignment are combined.</abstract>
      <url hash="446605ab">2021.ranlp-1.177</url>
      <bibkey>yoshikawa-etal-2021-tell</bibkey>
    </paper>
    <paper id="178">
      <title>Abstractive Document Summarization with Word Embedding Reconstruction</title>
      <author><first>Jingyi</first><last>You</last></author>
      <author><first>Chenlong</first><last>Hu</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>1586–1596</pages>
      <abstract>Neural sequence-to-sequence (Seq2Seq) models and BERT have achieved substantial improvements in abstractive document summarization (ADS) without and with pre-training, respectively. However, they sometimes repeatedly attend to unimportant source phrases while mistakenly ignore important ones. We present reconstruction mechanisms on two levels to alleviate this issue. The sequence-level reconstructor reconstructs the whole document from the hidden layer of the target summary, while the word embedding-level one rebuilds the average of word embeddings of the source at the target side to guarantee that as much critical information is included in the summary as possible. Based on the assumption that inverse document frequency (IDF) measures how important a word is, we further leverage the IDF weights in our embedding-level reconstructor. The proposed frameworks lead to promising improvements for ROUGE metrics and human rating on both the CNN/Daily Mail and Newsroom summarization datasets.</abstract>
      <url hash="a4ea0745">2021.ranlp-1.178</url>
      <bibkey>you-etal-2021-abstractive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="179">
      <title>Interpretable Propaganda Detection in News Articles</title>
      <author><first>Seunghak</first><last>Yu</last></author>
      <author><first>Giovanni</first><last>Da San Martino</last></author>
      <author><first>Mitra</first><last>Mohtarami</last></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1597–1605</pages>
      <abstract>Online users today are exposed to misleading and propagandistic news articles and media posts on a daily basis. To counter thus, a number of approaches have been designed aiming to achieve a healthier and safer online news and media consumption. Automatic systems are able to support humans in detecting such content; yet, a major impediment to their broad adoption is that besides being accurate, the decisions of such systems need also to be interpretable in order to be trusted and widely adopted by users. Since misleading and propagandistic content influences readers through the use of a number of deception techniques, we propose to detect and to show the use of such techniques as a way to offer interpretability. In particular, we define qualitatively descriptive features and we analyze their suitability for detecting deception techniques. We further show that our interpretable features can be easily combined with pre-trained language models, yielding state-of-the-art results.</abstract>
      <url hash="f1c8f4a1">2021.ranlp-1.179</url>
      <bibkey>yu-etal-2021-interpretable</bibkey>
    </paper>
    <paper id="180">
      <title>Generic Mechanism for Reducing Repetitions in Encoder-Decoder Models</title>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Tatsuya</first><last>Aoki</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>1606–1615</pages>
      <abstract>Encoder-decoder models have been commonly used for many tasks such as machine translation and response generation. As previous research reported, these models suffer from generating redundant repetition. In this research, we propose a new mechanism for encoder-decoder models that estimates the semantic difference of a source sentence before and after being fed into the encoder-decoder model to capture the consistency between two sides. This mechanism helps reduce repeatedly generated tokens for a variety of tasks. Evaluation results on publicly available machine translation and response generation datasets demonstrate the effectiveness of our proposal.</abstract>
      <url hash="e735f19e">2021.ranlp-1.180</url>
      <bibkey>zhang-etal-2021-generic</bibkey>
    </paper>
    <paper id="181">
      <title>Knowledge Distillation with <fixed-case>BERT</fixed-case> for Image Tag-Based Privacy Prediction</title>
      <author><first>Chenye</first><last>Zhao</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>1616–1625</pages>
      <abstract>Text in the form of tags associated with online images is often informative for predicting private or sensitive content from images. When using privacy prediction systems running on social networking sites that decide whether each uploaded image should get posted or be protected, users may be reluctant to share real images that may reveal their identity but may share image tags. In such cases, privacy-aware tags become good indicators of image privacy and can be utilized to generate privacy decisions. In this paper, our aim is to learn tag representations for images to improve tag-based image privacy prediction. To achieve this, we explore self-distillation with BERT, in which we utilize knowledge in the form of soft probability distributions (soft labels) from the teacher model to help with the training of the student model. Our approach effectively learns better tag representations with improved performance on private image identification and outperforms state-of-the-art models for this task. Moreover, we utilize the idea of knowledge distillation to improve tag representations in a semi-supervised learning task. Our semi-supervised approach with only 20% of annotated data achieves similar performance compared with its supervised learning counterpart. Last, we provide a comprehensive analysis to get a better understanding of our approach.</abstract>
      <url hash="20ee0943">2021.ranlp-1.181</url>
      <bibkey>zhao-caragea-2021-knowledge</bibkey>
    </paper>
    <paper id="182">
      <title>Delexicalized Cross-lingual Dependency Parsing for <fixed-case>X</fixed-case>ibe</title>
      <author><first>He</first><last>Zhou</last></author>
      <author><first>Sandra</first><last>Kübler</last></author>
      <pages>1626–1635</pages>
      <abstract>Manually annotating a treebank is time-consuming and labor-intensive. We conduct delexicalized cross-lingual dependency parsing experiments, where we train the parser on one language and test on our target language. As our test case, we use Xibe, a severely under-resourced Tungusic language. We assume that choosing a closely related language as the source language will provide better results than more distant relatives. However, it is not clear how to determine those closely related languages. We investigate three different methods: choosing the typologically closest language, using LangRank, and choosing the most similar language based on perplexity. We train parsing models on the selected languages using UDify and test on different genres of Xibe data. The results show that languages selected based on typology and perplexity scores outperform those predicted by LangRank; Japanese is the optimal source language. In determining the source language, proximity to the target language is more important than large training sizes. Parsing is also influenced by genre differences, but they have little influence as long as the training data is at least as complex as the target.</abstract>
      <url hash="753b5775">2021.ranlp-1.182</url>
      <bibkey>zhou-kubler-2021-delexicalized</bibkey>
    </paper>
    <paper id="183">
      <title><fixed-case>A</fixed-case>uto<fixed-case>C</fixed-case>hart: A Dataset for Chart-to-Text Generation Task</title>
      <author><first>Jiawen</first><last>Zhu</last></author>
      <author><first>Jinye</first><last>Ran</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last></author>
      <author><first>Zhi</first><last>Li</last></author>
      <author><first>Kenny</first><last>Choo</last></author>
      <pages>1636–1644</pages>
      <abstract>The analytical description of charts is an exciting and important research area with many applications in academia and industry. Yet, this challenging task has received limited attention from the computational linguistics research community. This paper proposes AutoChart, a large dataset for the analytical description of charts, which aims to encourage more research into this important area. Specifically, we offer a novel framework that generates the charts and their analytical description automatically. We conducted extensive human and machine evaluation on the generated charts and descriptions and demonstrate that the generated texts are informative, coherent, and relevant to the corresponding charts.</abstract>
      <url hash="d9891030">2021.ranlp-1.183</url>
      <bibkey>zhu-etal-2021-autochart</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/autochart">AutoChart</pwcdataset>
    </paper>
    <paper id="184">
      <title>A Comparative Study on Abstractive and Extractive Approaches in Summarization of <fixed-case>E</fixed-case>uropean Legislation Documents</title>
      <author><first>Valentin</first><last>Zmiycharov</last></author>
      <author><first>Milen</first><last>Chechev</last></author>
      <author><first>Gergana</first><last>Lazarova</last></author>
      <author><first>Todor</first><last>Tsonkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <pages>1645–1651</pages>
      <abstract>Extracting the most important part of legislation documents has great business value because the texts are usually very long and hard to understand. The aim of this article is to evaluate different algorithms for text summarization on EU legislation documents. The content contains domain-specific words. We collected a text summarization dataset of EU legal documents consisting of 1563 documents, in which the mean length of summaries is 424 words. Experiments were conducted with different algorithms using the new dataset. A simple extractive algorithm was selected as a baseline. Advanced extractive algorithms, which use encoders show better results than baseline. The best result measured by ROUGE scores was achieved by a fine-tuned abstractive T5 model, which was adapted to work with long texts.</abstract>
      <url hash="08f988f2">2021.ranlp-1.184</url>
      <bibkey>zmiycharov-etal-2021-comparative</bibkey>
    </paper>
    <paper id="185">
      <title>Not All Comments Are Equal: Insights into Comment Moderation from a Topic-Aware Model</title>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <author><first>Mladen</first><last>Karan</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>1652–1662</pages>
      <abstract>Moderation of reader comments is a significant problem for online news platforms. Here, we experiment with models for automatic moderation, using a dataset of comments from a popular Croatian newspaper. Our analysis shows that while comments that violate the moderation rules mostly share common linguistic and thematic features, their content varies across the different sections of the newspaper. We therefore make our models topic-aware, incorporating semantic features from a topic model into the classification decision. Our results show that topic information improves the performance of the model, increases its confidence in correct outputs, and helps us understand the model’s outputs.</abstract>
      <url hash="bf9505c7">2021.ranlp-1.185</url>
      <bibkey>zosa-etal-2021-comments</bibkey>
      <pwccode url="https://github.com/ezosa/topic-aware-moderation" additional="false">ezosa/topic-aware-moderation</pwccode>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2021-11-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Student Research Workshop Associated with RANLP 2021</booktitle>
      <editor><first>Souhila</first><last>Djabri</last></editor>
      <editor><first>Dinara</first><last>Gimadi</last></editor>
      <editor><first>Tsvetomila</first><last>Mihaylova</last></editor>
      <editor><first>Ivelina</first><last>Nikolova-Koleva</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Online</address>
      <month>September</month>
      <year>2021</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="d6c0aaa9">2021.ranlp-srw.0</url>
      <bibkey>ranlp-2021-student</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Humor Generation and Detection in Code-Mixed <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Kaustubh</first><last>Agarwal</last></author>
      <author><first>Rhythm</first><last>Narula</last></author>
      <pages>1–6</pages>
      <abstract>Computational humor generation is one of the hardest tasks in natural language generation, especially in code-mixed languages. Existing research has shown that humor generation in English is a promising avenue. However, studies have shown that bilingual speakers often appreciate humor more in code-mixed languages with unexpected transitions and clever word play. In this study, we propose several methods for generating and detecting humor in code-mixed Hindi-English. Of the experimented approaches, an Attention Based Bi-Directional LSTM with converting parts of text on a word2vec embedding gives the best results by generating 74.8% good jokes and IndicBERT used for detecting humor in code-mixed Hindi-English outperforms other humor detection methods with an accuracy of 96.98%.</abstract>
      <url hash="5401ab9d">2021.ranlp-srw.1</url>
      <bibkey>agarwal-narula-2021-humor-generation</bibkey>
    </paper>
    <paper id="2">
      <title>Towards Code-Mixed <fixed-case>H</fixed-case>inglish Dialogue Generation</title>
      <author><first>Vibhav</first><last>Agarwal</last></author>
      <author><first>Pooja</first><last>Rao</last></author>
      <author><first>Dinesh Babu</first><last>Jayagopi</last></author>
      <pages>7–15</pages>
      <abstract>Code-mixed language plays a crucial role in communication in multilingual societies. Though the recent growth of web users has greatly boosted the use of such mixed languages, the current generation of dialog systems is primarily monolingual. This increase in usage of code-mixed language has prompted dialog systems in a similar language. We present our work in Code-Mixed Dialog Generation, an unexplored task in code-mixed languages, generating utterances in code-mixed language rather than a single language that is more often just English. We present a new synthetic corpus in code-mix for dialogs, CM-DailyDialog, by converting an existing English-only dialog corpus to a mixed Hindi-English corpus. We then propose a baseline approach where we show the effectiveness of using mBART like multilingual sequence-to-sequence transformers for code-mixed dialog generation. Our best performing dialog models can conduct coherent conversations in Hindi-English mixed language as evaluated by human and automatic metrics setting new benchmarks for the Code-Mixed Dialog Generation task.</abstract>
      <url hash="d661856f">2021.ranlp-srw.2</url>
      <bibkey>agarwal-etal-2021-towards-code-mixed</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lince">LinCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/phinc">PHINC</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>H</fixed-case>inglish to <fixed-case>E</fixed-case>nglish Machine Translation using Multilingual Transformers</title>
      <author><first>Vibhav</first><last>Agarwal</last></author>
      <author><first>Pooja</first><last>Rao</last></author>
      <author><first>Dinesh Babu</first><last>Jayagopi</last></author>
      <pages>16–21</pages>
      <abstract>Code-Mixed language plays a very important role in communication in multilingual societies and with the recent increase in internet users especially in multilingual societies, the usage of such mixed language has also increased. However, the cross translation be- tween the Hinglish Code-Mixed and English and vice-versa has not been explored very extensively. With the recent success of large pretrained language models, we explore the possibility of using multilingual pretrained transformers like mBART and mT5 for exploring one such task of code-mixed Hinglish to English machine translation. Further, we compare our approach with the only baseline over the PHINC dataset and report a significant jump from 15.3 to 29.5 in BLEU scores, a 92.8% improvement over the same dataset.</abstract>
      <url hash="9285ccaa">2021.ranlp-srw.3</url>
      <bibkey>agarwal-etal-2021-hinglish-english</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/phinc">PHINC</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>S</fixed-case>i<fixed-case>POS</fixed-case>: A Benchmark Dataset for <fixed-case>S</fixed-case>indhi Part-of-Speech Tagging</title>
      <author><first>Wazir</first><last>Ali</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <author><first>Jay</first><last>Kumar</last></author>
      <pages>22–30</pages>
      <abstract>In this paper, we introduce the SiPOS dataset for part-of-speech tagging in the low-resource Sindhi language with quality baselines. The dataset consists of more than 293K tokens annotated with sixteen universal part-of-speech categories. Two experienced native annotators annotated the SiPOS using the Doccano text annotation tool with an inter-annotation agreement of 0.872. We exploit the conditional random field, the popular bidirectional long-short-term memory neural model, and self-attention mechanism with various settings to evaluate the proposed dataset. Besides pre-trained GloVe and fastText representation, the character-level representations are incorporated to extract character-level information using the bidirectional long-short-term memory encoder. The high accuracy of 96.25% is achieved with the task-specific joint word-level and character-level representations. The SiPOS dataset is likely to be a significant resource for the low-resource Sindhi language.</abstract>
      <url hash="3da6d5b1">2021.ranlp-srw.4</url>
      <bibkey>ali-etal-2021-sipos-benchmark</bibkey>
    </paper>
    <paper id="5">
      <title>Sarcasm Detection and Building an <fixed-case>E</fixed-case>nglish Language Corpus in Real Time</title>
      <author><first>Oliver</first><last>Cakebread-Andrews</last></author>
      <pages>31–35</pages>
      <abstract>This is a research proposal for doctoral research into sarcasm detection, and the real-time compilation of an English language corpus of sarcastic utterances. It details the previous research into similar topics, the potential research directions and the research aims.</abstract>
      <url hash="287e4650">2021.ranlp-srw.5</url>
      <bibkey>cakebread-andrews-2021-sarcasm-detection</bibkey>
    </paper>
    <paper id="6">
      <title>Correcting Texts Generated by Transformers using Discourse Features and Web Mining</title>
      <author><first>Alexander</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Boris</first><last>Galitsky</last></author>
      <pages>36–43</pages>
      <abstract>Recent transformer-based approaches to NLG like GPT-2 can generate syntactically coherent original texts. However, these generated texts have serious flaws: global discourse incoherence and meaninglessness of sentences in terms of entity values. We address both of these flaws: they are independent but can be combined to generate original texts that will be both consistent and truthful. This paper presents an approach to estimate the quality of discourse structure. Empirical results confirm that the discourse structure of currently generated texts is inaccurate. We propose the research directions to correct it using discourse features during the fine-tuning procedure. The suggested approach is universal and can be applied to different languages. Apart from that, we suggest a method to correct wrong entity values based on Web Mining and text alignment.</abstract>
      <url hash="2dbe80d2">2021.ranlp-srw.6</url>
      <bibkey>chernyavskiy-etal-2021-correcting-texts</bibkey>
    </paper>
    <paper id="7">
      <title>Introducing linguistic transformation to improve translation memory retrieval. Results of a professional translators’ survey for <fixed-case>S</fixed-case>panish, <fixed-case>F</fixed-case>rench and <fixed-case>A</fixed-case>rabic</title>
      <author><first>Souhila</first><last>Djabri</last></author>
      <author><first>Rocío</first><last>Caro Quintana</last></author>
      <pages>44–50</pages>
      <abstract>Translation memory systems (TMS) are the main component of computer-assisted translation (CAT) tools. They store translations allowing to save time by presenting translations on the database through matching of several types such as fuzzy matches, which are calculated by algorithms like the edit distance. However, studies have demonstrated the linguistic deficiencies of these systems and the difficulties in data retrieval or obtaining a high percentage of matching, especially after the application of syntactic and semantic transformations as the active/passive voice change, change of word order, substitution by a synonym or a personal pronoun, for instance. This paper presents the results of a pilot study where we analyze the qualitative and quantitative data of questionnaires conducted with professional translators of Spanish, French and Arabic in order to improve the effectiveness of TMS and explore all possibilities to integrate further linguistic processing from ten transformation types. The results are encouraging, and they allowed us to find out about the translation process itself; from which we propose a pre-editing processing tool to improve the matching and retrieving processes.</abstract>
      <url hash="090a5cd0">2021.ranlp-srw.7</url>
      <bibkey>djabri-caro-quintana-2021-introducing-linguistic</bibkey>
    </paper>
    <paper id="8">
      <title>Using Transfer Learning to Automatically Mark <fixed-case>L</fixed-case>2 Writing Texts</title>
      <author><first>Tim</first><last>Elks</last></author>
      <pages>51–57</pages>
      <abstract>The use of transfer learning in Natural Language Processing (NLP) has grown over the last few years. Large, pre-trained neural networks based on the Transformer architecture are one example of this, achieving state-of-theart performance on several commonly used performance benchmarks, often when finetuned on a downstream task. Another form of transfer learning, Multitask Learning, has also been shown to improve performance in Natural Language Processing tasks and increase model robustness. This paper outlines preliminary findings of investigations into the impact of using pretrained language models alongside multitask fine-tuning to create an automated marking system of second language learners’ written English. Using multiple transformer models and multiple datasets, this study compares different combinations of models and tasks and evaluates their impact on the performance of an automated marking system This presentation is a snap-shot of work being conducted as part of my dissertation for the University of Wolverhampton’s Computational Linguistics Masters’ programme.</abstract>
      <url hash="78036f6b">2021.ranlp-srw.8</url>
      <bibkey>elks-2021-using-transfer</bibkey>
    </paper>
    <paper id="9">
      <title>Bilingual Terminology Extraction Using Neural Word Embeddings on Comparable Corpora</title>
      <author><first>Darya</first><last>Filippova</last></author>
      <author><first>Burcu</first><last>Can</last></author>
      <author><first>Gloria</first><last>Corpas Pastor</last></author>
      <pages>58–64</pages>
      <abstract>Term and glossary management are vital steps of preparation of every language specialist, and they play a very important role at the stage of education of translation professionals. The growing trend of efficient time management and constant time constraints we may observe in every job sector increases the necessity of the automatic glossary compilation. Many well-performing bilingual AET systems are based on processing parallel data, however, such parallel corpora are not always available for a specific domain or a language pair. Domain-specific, bilingual access to information and its retrieval based on comparable corpora is a very promising area of research that requires a detailed analysis of both available data sources and the possible extraction techniques. This work focuses on domain-specific automatic terminology extraction from comparable corpora for the English – Russian language pair by utilizing neural word embeddings.</abstract>
      <url hash="be75b7e3">2021.ranlp-srw.9</url>
      <bibkey>filippova-etal-2021-bilingual-terminology</bibkey>
    </paper>
    <paper id="10">
      <title>Web-sentiment analysis of public comments (public reviews) for languages with limited resources such as the <fixed-case>K</fixed-case>azakh language</title>
      <author><first>Dinara</first><last>Gimadi</last></author>
      <pages>65–68</pages>
      <abstract>In the pandemic period, the stay-at-home trend forced businesses to switch their activities to digital mode, for example, app-based payment methods, social distancing via social media platforms, and other digital means have become an integral part of our lives. Sentiment analysis of textual information in user comments is a topical task in emotion AI because user comments or reviews are not homogeneous, they contain sparse context behind, and are misleading both for human and computer. Barriers arise from the emotional language enriched with slang, peculiar spelling, transliteration, use of emoji and their symbolic counterparts, and code-switching. For low resource languages sentiment analysis has not been worked upon extensively, because of an absence of ready-made tools and linguistic resources for sentiment analysis. This research focuses on developing a method for aspect-based sentiment analysis for Kazakh-language reviews in Android Google Play Market.</abstract>
      <url hash="f54b8418">2021.ranlp-srw.10</url>
      <bibkey>gimadi-2021-web-sentiment</bibkey>
    </paper>
    <paper id="11">
      <title>Disambiguating Grammatical Number and Gender With <fixed-case>BERT</fixed-case></title>
      <author><first>Annegret</first><last>Janzso</last></author>
      <pages>69–77</pages>
      <abstract>Accurately dealing with any type of ambiguity is a major task in Natural Language Processing, with great advances recently reached due to the development of context dependent language models and the use of word or sentence embeddings. In this context, our work aimed at determining how the popular language representation model BERT handle ambiguity of nouns in grammatical number and gender in different languages. We show that models trained on one specific language achieve better results for the disambiguation process than multilingual models. Also, ambiguity is generally better dealt with in grammatical number than it is in grammatical gender, reaching greater distance values from one to another in direct comparisons of individual senses. The overall results show also that the amount of data needed for training monolingual models as well as application should not be underestimated.</abstract>
      <url hash="f5a996c8">2021.ranlp-srw.11</url>
      <bibkey>janzso-2021-disambiguating-grammatical</bibkey>
    </paper>
    <paper id="12">
      <title>Towards a Language Model for Temporal Commonsense Reasoning</title>
      <author><first>Mayuko</first><last>Kimura</last></author>
      <author><first>Lis</first><last>Kanashiro Pereira</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <pages>78–84</pages>
      <abstract>Temporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicit in text. In this work, we propose an ensemble model for temporal commonsense reasoning. Our model relies on pre-trained contextual representations from transformer-based language models (i.e., BERT), and on a variety of training methods for enhancing model generalization: 1) multi-step fine-tuning using carefully selected auxiliary tasks and datasets, and 2) a specifically designed temporal masked language model task aimed to capture temporal commonsense knowledge. Our model greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.</abstract>
      <url hash="f00f65dd">2021.ranlp-srw.12</url>
      <bibkey>kimura-etal-2021-towards-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="13">
      <title>Text Preprocessing and its Implications in a Digital Humanities Project</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Alistair</first><last>Plum</last></author>
      <pages>85–93</pages>
      <abstract>This paper focuses on data cleaning as part of a preprocessing procedure applied to text data retrieved from the web. Although the importance of this early stage in a project using NLP methods is often highlighted by researchers, the details, general principles and techniques are usually left out due to consideration of space. At best, they are dismissed with a comment “The usual data cleaning and preprocessing procedures were applied”. More coverage is usually given to automatic text annotation such as lemmatisation, part-of-speech tagging and parsing, which is often included in preprocessing. In the literature, the term ‘preprocessing’ is used to refer to a wide range of procedures, from filtering and cleaning to data transformation such as stemming and numeric representation, which might create confusion. We argue that text preprocessing might skew original data distribution with regard to the metadata, such as types, locations and times of registered datapoints. In this paper we describe a systematic approach to cleaning text data mined by a data-providing company for a Digital Humanities (DH) project focused on cultural analytics. We reveal the types and amount of noise in the data coming from various web sources and estimate the changes in the size of the data associated with preprocessing. We also compare the results of a text classification experiment run on the raw and preprocessed data. We hope that our experience and approaches will help the DH community to diagnose the quality of textual data collected from the web and prepare it for further natural language processing.</abstract>
      <url hash="c6835384">2021.ranlp-srw.13</url>
      <bibkey>kunilovskaya-plum-2021-text-preprocessing</bibkey>
    </paper>
    <paper id="14">
      <title>Compiling a specialised corpus for translation research in the environmental domain</title>
      <author><first>Anastasiia</first><last>Laktionova</last></author>
      <pages>94–98</pages>
      <abstract>The present study is an ongoing research that aims to investigate lexico-grammatical and stylistic features of texts in the environmental domain in English, their implications for translation into Ukrainian as well as the translation of key terminological units based on a specialised parallel and comparable corpora.</abstract>
      <url hash="df82887e">2021.ranlp-srw.14</url>
      <bibkey>laktionova-2021-compiling-specialised</bibkey>
    </paper>
    <paper id="15">
      <title>Paragraph Similarity Matches for Generating Multiple-choice Test Items</title>
      <author><first>Halyna</first><last>Maslak</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>99–108</pages>
      <abstract>Multiple-choice questions (MCQs) are widely used in knowledge assessment in educational institutions, during work interviews, in entertainment quizzes and games. Although the research on the automatic or semi-automatic generation of multiple-choice test items has been conducted since the beginning of this millennium, most approaches focus on generating questions from a single sentence. In this research, a state-of-the-art method of creating questions based on multiple sentences is introduced. It was inspired by semantic similarity matches used in the translation memory component of translation management systems. The performance of two deep learning algorithms, doc2vec and SBERT, is compared for the paragraph similarity task. The experiments are performed on the ad-hoc corpus within the EU domain. For the automatic evaluation, a smaller corpus of manually selected matching paragraphs has been compiled. The results prove the good performance of Sentence Embeddings for the given task.</abstract>
      <url hash="af636f05">2021.ranlp-srw.15</url>
      <bibkey>maslak-mitkov-2021-paragraph-similarity</bibkey>
    </paper>
    <paper id="16">
      <title>Neural Borrowing Detection with Monolingual Lexical Models</title>
      <author><first>John</first><last>Miller</last></author>
      <author><first>Emanuel</first><last>Pariasca</last></author>
      <author><first>Cesar</first><last>Beltran Castañon</last></author>
      <pages>109–117</pages>
      <abstract>Identification of lexical borrowings, transfer of words between languages, is an essential practice of historical linguistics and a vital tool in analysis of language contact and cultural events in general. We seek to improve tools for automatic detection of lexical borrowings, focusing here on detecting borrowed words from monolingual wordlists. Starting with a recurrent neural lexical language model and competing entropies approach, we incorporate a more current Transformer based lexical model. From there we experiment with several different models and approaches including a lexical donor model with augmented wordlist. The Transformer model reduces execution time and minimally improves borrowing detection. The augmented donor model shows some promise. A substantive change in approach or model is needed to make significant gains in identification of lexical borrowings.</abstract>
      <url hash="b1b46fc5">2021.ranlp-srw.16</url>
      <bibkey>miller-etal-2021-neural-borrowing</bibkey>
    </paper>
    <paper id="17">
      <title>Does local pruning offer task-specific models to learn effectively ?</title>
      <author><first>Abhishek Kumar</first><last>Mishra</last></author>
      <author><first>Mohna</first><last>Chakraborty</last></author>
      <pages>118–125</pages>
      <abstract>The need to deploy large-scale pre-trained models on edge devices under limited computational resources has led to substantial research to compress these large models. However, less attention has been given to compress the task-specific models. In this work, we investigate the different methods of unstructured pruning on task-specific models for Aspect-based Sentiment Analysis (ABSA) tasks. Specifically, we analyze differences in the learning dynamics of pruned models by using the standard pruning techniques to achieve high-performing sparse networks. We develop a hypothesis to demonstrate the effectiveness of local pruning over global pruning considering a simple CNN model. Later, we utilize the hypothesis to demonstrate the efficacy of the pruned state-of-the-art model compared to the over-parameterized state-of-the-art model under two settings, the first considering the baselines for the same task used for generating the hypothesis, i.e., aspect extraction and the second considering a different task, i.e., sentiment analysis. We also provide discussion related to the generalization of the pruning hypothesis.</abstract>
      <url hash="0c3c3ac6">2021.ranlp-srw.17</url>
      <bibkey>mishra-chakraborty-2021-local-pruning</bibkey>
      <pwccode url="https://github.com/abhishekkumarm98/local_vs_global-pruning" additional="false">abhishekkumarm98/local_vs_global-pruning</pwccode>
    </paper>
    <paper id="18">
      <title>On Reducing Repetition in Abstractive Summarization</title>
      <author><first>Pranav</first><last>Nair</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>126–134</pages>
      <abstract>Repetition in natural language generation reduces the informativeness of text and makes it less appealing. Various techniques have been proposed to alleviate it. In this work, we explore and propose techniques to reduce repetition in abstractive summarization. First, we explore the application of unlikelihood training and embedding matrix regularizers from previous work on language modeling to abstractive summarization. Next, we extend the coverage and temporal attention mechanisms to the token level to reduce repetition. In our experiments on the CNN/Daily Mail dataset, we observe that these techniques reduce the amount of repetition and increase the informativeness of the summaries, which we confirm via human evaluation.</abstract>
      <url hash="e29ee0fa">2021.ranlp-srw.18</url>
      <bibkey>nair-singh-2021-reducing-repetition</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="19">
      <title>Improving Abstractive Summarization with Commonsense Knowledge</title>
      <author><first>Pranav</first><last>Nair</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>135–143</pages>
      <abstract>Large scale pretrained models have demonstrated strong performances on several natural language generation and understanding benchmarks. However, introducing commonsense into them to generate more realistic text remains a challenge. Inspired from previous work on commonsense knowledge generation and generative commonsense reasoning, we introduce two methods to add commonsense reasoning skills and knowledge into abstractive summarization models. Both methods beat the baseline on ROUGE scores, demonstrating the superiority of our models over the baseline. Human evaluation results suggest that summaries generated by our methods are more realistic and have fewer commonsensical errors.</abstract>
      <url hash="e22a129f">2021.ranlp-srw.19</url>
      <bibkey>nair-singh-2021-improving-abstractive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="20">
      <title>A Dataset for Research on Modelling Depression Severity in Online Forum Data</title>
      <author><first>Isuri Anuradha</first><last>Nanomi Arachchige</last></author>
      <author><first>Vihangi Himaya</first><last>Jayasuriya</last></author>
      <author><first>Ruvan</first><last>Weerasinghe</last></author>
      <pages>144–153</pages>
      <abstract>People utilize online forums to either look for information or to contribute it. Because of their growing popularity, certain online forums have been created specifically to provide support, assistance, and opinions for people suffering from mental illness. Depression is one of the most frequent psychological illnesses worldwide. People communicate more with online forums to find answers for their psychological disease. However, there is no mechanism to measure the severity of depression in each post and give higher importance to those who are diagnosed more severely depressed. Despite the fact that numerous researches based on online forum data and the identification of depression have been conducted, the severity of depression is rarely explored. In addition, the absence of datasets will stymie the development of novel diagnostic procedures for practitioners. From this study, we offer a dataset to support research on depression severity evaluation. The computational approach to measure an automatic process, identified severity of depression here is quite novel approach. Nonetheless, this elaborate measuring severity of depression in online forum posts is needed to ensure the measurement scales used in our research meets the expected norms of scientific research.</abstract>
      <url hash="fd7ddee0">2021.ranlp-srw.20</url>
      <bibkey>nanomi-arachchige-etal-2021-dataset-research</bibkey>
    </paper>
    <paper id="21">
      <title>Handling synset overgeneration: Sense Merging in <fixed-case>BTB</fixed-case>-<fixed-case>WN</fixed-case></title>
      <author><first>Ivaylo</first><last>Radev</last></author>
      <author><first>Zara</first><last>Kancheva</last></author>
      <pages>154–161</pages>
      <abstract>The paper reports on an effort to reconsider the representation of some cases of derivational paradigm patterns in Bulgarian. The new treatment implemented within BulTreeBank-WordNet (BTB-WN), a wordnet for Bulgarian, is the grouping together of related words that have a common main meaning in the same synset while the nuances in sense are to be encoded within the synset as a modification functions over the main meaning. In this way, we can solve the following challenges: (1) to avoid the influence of English Wordnet (EWN) synset distinctions over Bulgarian that was a result from the translation of some of the synsets from Core WordNet; (2) to represent the common meaning of such derivation patterns just once and to improve the management of BTB-WN, and (3) to encode idiosyncratic usages locally to the corresponding synsets instead of introducing new semantic relations.</abstract>
      <url hash="d2c48439">2021.ranlp-srw.21</url>
      <bibkey>radev-kancheva-2021-handling-synset</bibkey>
    </paper>
    <paper id="22">
      <title>On the Evolution of Word Order</title>
      <author><first>Idan</first><last>Rejwan</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <pages>162–166</pages>
      <abstract>Most natural languages have a predominant or fixed word order. For example in English the word order is usually Subject-Verb-Object. This work attempts to explain this phenomenon as well as other typological findings regarding word order from a functional perspective. In particular, we examine whether fixed word order provides a functional advantage, explaining why these languages are prevalent. To this end, we consider an evolutionary model of language and demonstrate, both theoretically and using genetic algorithms, that a language with a fixed word order is optimal. We also show that adding information to the sentence, such as case markers and noun-verb distinction, reduces the need for fixed word order, in accordance with the typological findings.</abstract>
      <url hash="0cda3dd2">2021.ranlp-srw.22</url>
      <bibkey>rejwan-caciularu-2021-evolution-word</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>E</fixed-case>mo<fixed-case>P</fixed-case>ars: A Collection of 30<fixed-case>K</fixed-case> Emotion-Annotated <fixed-case>P</fixed-case>ersian Social Media Texts</title>
      <author><first>Nazanin</first><last>Sabri</last></author>
      <author><first>Reyhane</first><last>Akhavan</last></author>
      <author><first>Behnam</first><last>Bahrak</last></author>
      <pages>167–173</pages>
      <abstract>The wide reach of social media platforms, such as Twitter, have enabled many users to share their thoughts, opinions and emotions on various topics online. The ability to detect these emotions automatically would allow social scientists, as well as, businesses to better understand responses from nations and costumers. In this study we introduce a dataset of 30,000 Persian Tweets labeled with Ekman’s six basic emotions (Anger, Fear, Happiness, Sadness, Hatred, and Wonder). This is the first publicly available emotion dataset in the Persian language. In this paper, we explain the data collection and labeling scheme used for the creation of this dataset. We also analyze the created dataset, showing the different features and characteristics of the data. Among other things, we investigate co-occurrence of different emotions in the dataset, and the relationship between sentiment and emotion of textual instances. The dataset is publicly available at <url>https://github.com/nazaninsbr/Persian-Emotion-Detection</url>.</abstract>
      <url hash="25c566a6">2021.ranlp-srw.23</url>
      <bibkey>sabri-etal-2021-emopars-collection</bibkey>
      <pwccode url="https://github.com/nazaninsbr/persian-emotion-detection" additional="false">nazaninsbr/persian-emotion-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emopars">EmoPars</pwcdataset>
    </paper>
    <paper id="24">
      <title>A Review on Document Information Extraction Approaches</title>
      <author><first>Kanishka</first><last>Silva</last></author>
      <author><first>Thushari</first><last>Silva</last></author>
      <pages>174–179</pages>
      <abstract>Information extraction from documents has become great use of novel natural language processing areas. Most of the entity extraction methodologies are variant in a context such as medical area, financial area, also come even limited to the given language. It is better to have one generic approach applicable for any document type to extract entity information regardless of language, context, and structure. Also, another issue in such research is structural analysis while keeping the hierarchical, semantic, and heuristic features. Another problem identified is that usually, it requires a massive training corpus. Therefore, this research focus on mitigating such barriers. Several approaches have been identifying towards building document information extractors focusing on different disciplines. This research area involves natural language processing, semantic analysis, information extraction, and conceptual modelling. This paper presents a review of the information extraction mechanism to construct a generic framework for document extraction with aim of providing a solid base for upcoming research.</abstract>
      <url hash="44a35513">2021.ranlp-srw.24</url>
      <bibkey>silva-silva-2021-review-document</bibkey>
    </paper>
    <paper id="25">
      <title>Towards New Generation Translation Memory Systems</title>
      <author><first>Nikola</first><last>Spasovski</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>180–183</pages>
      <abstract>Despite the enormous popularity of Translation Memory systems and the active research in the field, their language processing features still suffer from certain limitations. While many recent papers focus on semantic matching capabilities of TMs, this planned study will address how these tools perform when dealing with longer segments and whether this could be a cause of lower match scores. An experiment will be carried out on corpora from two different (repetitive) domains. Following the results, recommendations for future developments of new TMs will be made.</abstract>
      <url hash="1699fa53">2021.ranlp-srw.25</url>
      <bibkey>spasovski-mitkov-2021-towards-new</bibkey>
    </paper>
    <paper id="26">
      <title>Question answering in Natural Language: the Special Case of Temporal Expressions</title>
      <author><first>Armand</first><last>Stricker</last></author>
      <pages>184–192</pages>
      <abstract>Although general question answering has been well explored in recent years, temporal question answering is a task which has not received as much focus. Our work aims to leverage a popular approach used for general question answering, answer extraction, in order to find answers to temporal questions within a paragraph. To train our model, we propose a new dataset, inspired by SQuAD, a state-of-the-art question answering corpus, specifically tailored to provide rich temporal information by adapting the corpus WikiWars, which contains several documents on history’s greatest conflicts. Our evaluation shows that a pattern matching deep learning model, often used in general question answering, can be adapted to temporal question answering, if we accept to ask questions whose answers must be directly present within a text.</abstract>
      <url hash="afe63dc8">2021.ranlp-srw.26</url>
      <bibkey>stricker-2021-question-answering</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="27">
      <title>Building A Corporate Corpus For Threads Constitution</title>
      <author><first>Lionel</first><last>Tadonfouet Tadjou</last></author>
      <author><first>Fabrice</first><last>Bourge</last></author>
      <author><first>Tiphaine</first><last>Marie</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Éric</first><last>de la Clergerie</last></author>
      <pages>193–202</pages>
      <abstract>In this paper we describe the process of build-ing a corporate corpus that will be used as a ref-erence for modelling and computing threadsfrom conversations generated using commu-nication and collaboration tools. The overallgoal of the reconstruction of threads is to beable to provide value to the collorator in var-ious use cases, such as higlighting the impor-tant parts of a running discussion, reviewingthe upcoming commitments or deadlines, etc. Since, to our knowledge, there is no avail-able corporate corpus for the French languagewhich could allow us to address this prob-lem of thread constitution, we present here amethod for building such corpora includingdifferent aspects and steps which allowed thecreation of a pipeline to pseudo-anonymisedata. Such a pipeline is a response to theconstraints induced by the General Data Pro-tection Regulation GDPR in Europe and thecompliance to the secrecy of correspondence.</abstract>
      <url hash="1ab62445">2021.ranlp-srw.27</url>
      <bibkey>tadonfouet-tadjou-etal-2021-building-corporate</bibkey>
    </paper>
    <paper id="28">
      <title>Generating Answer Candidates for Quizzes and Answer-Aware Question Generators</title>
      <author><first>Kristiyan</first><last>Vachev</last></author>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Georgi</first><last>Karadzhov</last></author>
      <author><first>Georgi</first><last>Georgiev</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>203–209</pages>
      <abstract>In education, quiz questions have become an important tool for assessing the knowledge of students. Yet, manually preparing such questions is a tedious task, and thus automatic question generation has been proposed as a possible alternative. So far, the vast majority of research has focused on generating the question text, relying on question answering datasets with readily picked answers, and the problem of how to come up with answer candidates in the first place has been largely ignored. Here, we aim to bridge this gap. In particular, we propose a model that can generate a specified number of answer candidates for a given passage of text, which can then be used by instructors to write questions manually or can be passed as an input to automatic answer-aware question generators. Our experiments show that our proposed answer candidate generation model outperforms several baselines.</abstract>
      <url hash="a7047ccf">2021.ranlp-srw.28</url>
      <bibkey>vachev-etal-2021-generating-answer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/exams">EXAMS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="29">
      <title>Toward Discourse-Aware Models for Multilingual Fake News Detection</title>
      <author><first>Francielle</first><last>Vargas</last></author>
      <author><first>Fabrício</first><last>Benevenuto</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>210–218</pages>
      <abstract>Statements that are intentionally misstated (or manipulated) are of considerable interest to researchers, government, security, and financial systems. According to deception literature, there are reliable cues for detecting deception and the belief that liars give off cues that may indicate their deception is near-universal. Therefore, given that deceiving actions require advanced cognitive development that honesty simply does not require, as well as people’s cognitive mechanisms have promising guidance for deception detection, in this Ph.D. ongoing research, we propose to examine discourse structure patterns in multilingual deceptive news corpora using the Rhetorical Structure Theory framework. Considering that our work is the first to exploit multilingual discourse-aware strategies for fake news detection, the research community currently lacks multilingual deceptive annotated corpora. Accordingly, this paper describes the current progress in this thesis, including (i) the construction of the first multilingual deceptive corpus, which was annotated by specialists according to the Rhetorical Structure Theory framework, and (ii) the introduction of two new proposed rhetorical relations: INTERJECTION and IMPERATIVE, which we assume to be relevant for the fake news detection task.</abstract>
      <url hash="694dd7a0">2021.ranlp-srw.29</url>
      <bibkey>vargas-etal-2021-toward-discourse</bibkey>
    </paper>
    <paper id="30">
      <title>Automatic Transformation of Clinical Narratives into Structured Format</title>
      <author><first>Sylvia</first><last>Vassileva</last></author>
      <author><first>Gergana</first><last>Todorova</last></author>
      <author><first>Kristina</first><last>Ivanova</last></author>
      <author><first>Boris</first><last>Velichkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <author><first>Galia</first><last>Angelova</last></author>
      <author><first>Svetla</first><last>Boytcheva</last></author>
      <pages>219–227</pages>
      <abstract>Vast amounts of data in healthcare are available in unstructured text format, usually in the local language of the countries. These documents contain valuable information. Secondary use of clinical narratives and information extraction of key facts and relations from them about the patient disease history can foster preventive medicine and improve healthcare. In this paper, we propose a hybrid method for the automatic transformation of clinical text into a structured format. The documents are automatically sectioned into the following parts: diagnosis, patient history, patient status, lab results. For the “Diagnosis” section a deep learning text-based encoding into ICD-10 codes is applied using MBG-ClinicalBERT - a fine-tuned ClinicalBERT model for Bulgarian medical text. From the “Patient History” section, we identify patient symptoms using a rule-based approach enhanced with similarity search based on MBG-ClinicalBERT word embeddings. We also identify symptom relations like negation. For the “Patient Status” description, binary classification is used to determine the status of each anatomic organ. In this paper, we demonstrate different methods for adapting NLP tools for English and other languages to a low resource language like Bulgarian.</abstract>
      <url hash="67a272ca">2021.ranlp-srw.30</url>
      <bibkey>vassileva-etal-2021-automatic-transformation</bibkey>
    </paper>
  </volume>
  <event id="ranlp-2021">
    <colocated>
      <volume-id>2021.bucc-1</volume-id>
      <volume-id>2021.mmtlrl-1</volume-id>
    </colocated>
  </event>
</collection>
