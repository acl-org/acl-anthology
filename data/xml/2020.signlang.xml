<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.signlang">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</booktitle>
      <editor><first>Eleni</first><last>Efthimiou</last></editor>
      <editor><first>Stavroula-Evita</first><last>Fotinea</last></editor>
      <editor><first>Thomas</first><last>Hanke</last></editor>
      <editor><first>Julie A.</first><last>Hochgesang</last></editor>
      <editor><first>Jette</first><last>Kristoffersen</last></editor>
      <editor><first>Johanna</first><last>Mesch</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-54-2</isbn>
      <venue>signlang</venue>
    </meta>
    <frontmatter>
      <url hash="c53a1350">2020.signlang-1.0</url>
      <bibkey>signlang-2020-lrec2020</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Back and Forth between Theory and Application: Shared Phonological Coding Between <fixed-case>ASL</fixed-case> <fixed-case>S</fixed-case>ignbank and <fixed-case>ASL</fixed-case>-<fixed-case>LEX</fixed-case></title>
      <author><first>Amelia</first><last>Becker</last></author>
      <author><first>Donovan</first><last>Catt</last></author>
      <author><first>Julie A.</first><last>Hochgesang</last></author>
      <pages>1–6</pages>
      <abstract>The development of signed language lexical databases, digital organizations that describe different phonological features of and attempt to establish relationships between signs has resulted in a renewed interest in the phonological descriptions used to uniquely identify and organize the lexicons of respective sign languages (van der Kooij, 2002; Fenlon et al., 2016; Brentari et al., 2018). Throughout the mutually shared coding process involved in organizing two lexical databases, ASL Signbank (Hochgesang, Crasborn and Lillo-Martin, 2020) and ASL-LEX (Caselli et al., 2016), issues have arisen that require revisiting how phonological features and categories are to be applied and even decided upon, and which would adequately distinguish lexical contrast for respective sign languages. The paper concludes by exploring the inverse of the theory-to-database relationship. Examples are given of theoretical implications and research questions that arise from consequences of language resource building. These are presented as evidence that not only does theory impact organization of databases but that the process of database creation can also inform our theories.</abstract>
      <url hash="e3e77ee8">2020.signlang-1.1</url>
      <language>eng</language>
      <bibkey>becker-etal-2020-back</bibkey>
    </paper>
    <paper id="2">
      <title>Improving and Extending Continuous Sign Language Recognition: Taking Iconicity and Spatial Language into account</title>
      <author><first>Valentin</first><last>Belissen</last></author>
      <author><first>Michèle</first><last>Gouiffès</last></author>
      <author><first>Annelies</first><last>Braffort</last></author>
      <pages>7–12</pages>
      <abstract>In a lot of recent research, attention has been drawn to recognizing sequences of lexical signs in continuous Sign Language corpora, often artificial. However, as SLs are structured through the use of space and iconicity, focusing on lexicon only prevents the field of Continuous Sign Language Recognition (CSLR) from extending to Sign Language Understanding and Translation. In this article, we propose a new formulation of the CSLR problem and discuss the possibility of recognizing higher-level linguistic structures in SL videos, like classifier constructions. These structures show much more variability than lexical signs, and are fundamentally different than them in the sense that form and meaning can not be disentangled. Building on the recently published French Sign Language corpus Dicta-Sign-LSF-v2, we discuss the performance and relevance of a simple recurrent neural network trained to recognize illustrative structures.</abstract>
      <url hash="371ec968">2020.signlang-1.2</url>
      <language>eng</language>
      <bibkey>belissen-etal-2020-improving</bibkey>
    </paper>
    <paper id="3">
      <title>Utterance-Unit Annotation for the <fixed-case>JSL</fixed-case> Dialogue Corpus: Toward a Multimodal Approach to Corpus Linguistics</title>
      <author><first>Mayumi</first><last>Bono</last></author>
      <author><first>Rui</first><last>Sakaida</last></author>
      <author><first>Tomohiro</first><last>Okada</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>13–20</pages>
      <abstract>This paper describes a method for annotating the Japanese Sign Language (JSL) dialogue corpus. We developed a way to identify interactional boundaries and define a ‘utterance unit’ in sign language using various multimodal features accompanying signing. The utterance unit is an original concept for segmenting and annotating sign language dialogue referring to signer’s native sense from the perspectives of Conversation Analysis (CA) and Interaction Studies. First of all, we postulated that we should identify a fundamental concept of interaction-specific unit for understanding interactional mechanisms, such as turn-taking (Sacks et al. 1974), in sign-language social interactions. Obviously, it does should not relying on a spoken language writing system for storing signings in corpora and making translations. We believe that there are two kinds of possible applications for utterance units: one is to develop corpus linguistics research for both signed and spoken corpora; the other is to build an informatics system that includes, but is not limited to, a machine translation system for sign languages.</abstract>
      <url hash="9a91342d">2020.signlang-1.3</url>
      <language>eng</language>
      <bibkey>bono-etal-2020-utterance</bibkey>
    </paper>
    <paper id="4">
      <title>Measuring Lexical Similarity across Sign Languages in <fixed-case>G</fixed-case>lobal <fixed-case>S</fixed-case>ignbank</title>
      <author><first>Carl</first><last>Börstell</last></author>
      <author><first>Onno</first><last>Crasborn</last></author>
      <author><first>Lori</first><last>Whynot</last></author>
      <pages>21–26</pages>
      <abstract>Lexicostatistics is the main method used in previous work measuring linguistic distances between sign languages. As a method, it disregards any possible structural/grammatical similarity, instead focusing exclusively on lexical items, but it is time consuming as it requires some comparable phonological coding (i.e. form description) as well as concept matching (i.e. meaning description) of signs across the sign languages to be compared. In this paper, we present a novel approach for measuring lexical similarity across any two sign languages using the Global Signbank platform, a lexical database of uniformly coded signs. The method involves a feature-by-feature comparison of all matched phonological features. This method can be used in two distinct ways: 1) automatically comparing the amount of lexical overlap between two sign languages (with a more detailed feature-description than previous lexicostatistical methods); 2) finding exact form-matches across languages that are either matched or mismatched in meaning (i.e. true or false friends). We show the feasability of this method by comparing three languages (datasets) in Global Signbank, and are currently expanding both the size of these three as well as the total number of datasets.</abstract>
      <url hash="5785ec2e">2020.signlang-1.4</url>
      <language>eng</language>
      <bibkey>borstell-etal-2020-measuring</bibkey>
    </paper>
    <paper id="5">
      <title>Optimised Preprocessing for Automatic Mouth Gesture Classification</title>
      <author><first>Maren</first><last>Brumm</last></author>
      <author><first>Rolf-Rainer</first><last>Grigat</last></author>
      <pages>27–32</pages>
      <abstract>Mouth gestures are facial expressions in sign language, that do not refer to lip patterns of a spoken language. Research on this topic has been limited so far. The aim of this work is to automatically classify mouth gestures from video material by training a neural network. This could render time-consuming manual annotation unnecessary and help advance the field of automatic sign language translation. However, it is a challenging task due to the little data available as training material and the similarity of different mouth gesture classes. In this paper we focus on the preprocessing of the data, such as finding the area of the face important for mouth gesture recognition. Furthermore we analyse the duration of mouth gestures and determine the optimal length of video clips for classification. Our experiments show, that this can improve the classification results significantly and helps to reach a near human accuracy.</abstract>
      <url hash="6050e203">2020.signlang-1.5</url>
      <language>eng</language>
      <bibkey>brumm-grigat-2020-optimised</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>PE</fixed-case>2<fixed-case>LGP</fixed-case> Animator: A Tool To Animate A <fixed-case>P</fixed-case>ortuguese <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Avatar</title>
      <author><first>Pedro</first><last>Cabral</last></author>
      <author><first>Matilde</first><last>Gonçalves</last></author>
      <author><first>Hugo</first><last>Nicolau</last></author>
      <author><first>Luísa</first><last>Coheur</last></author>
      <author><first>Ruben</first><last>Santos</last></author>
      <pages>33–38</pages>
      <abstract>Software for the production of sign languages is much less common than for spoken languages. Such software usually relies on 3D humanoid avatars to produce signs which, inevitably, necessitates the use of animation. One barrier to the use of popular animation tools is their complexity and steep learning curve, which can be hard to master for inexperienced users. Here, we present PE2LGP, an authoring system that features a 3D avatar that signs Portuguese Sign Language. Our Animator is designed specifically to craft sign language animations using a key frame method, and is meant to be easy to use and learn to users without animation skills. We conducted a preliminary evaluation of the Animator, where we animated seven Portuguese Sign Language sentences and asked four sign language users to evaluate their quality. This evaluation revealed that the system, in spite of its simplicity, is indeed capable of producing comprehensible messages.</abstract>
      <url hash="40e84936">2020.signlang-1.6</url>
      <language>eng</language>
      <bibkey>cabral-etal-2020-pe2lgp</bibkey>
    </paper>
    <paper id="7">
      <title>Translating an <fixed-case>A</fixed-case>esop’s Fable to <fixed-case>F</fixed-case>ilipino <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage through 3<fixed-case>D</fixed-case> Animation</title>
      <author><first>Mark</first><last>Cueto</last></author>
      <author><first>Winnie</first><last>He</last></author>
      <author><first>Rei</first><last>Untiveros</last></author>
      <author><first>Josh</first><last>Zuñiga</last></author>
      <author><first>Joanna Pauline</first><last>Rivera</last></author>
      <pages>39–44</pages>
      <abstract>According to the National Statistics Office (2003) in the 2000 Population Census, the deaf community in the Philippines numbered to about 121,000 deaf and hard of hearing Filipinos. Deaf and hard of hearing Filipinos in these communities use the Filipino Sign Language (FSL) as the main method of manual communication. Deaf and hard of hearing children experience difficulty in developing reading and writing skills through traditional methods of teaching used primarily for hearing children. This study aims to translate an Aesop’s fable to Filipino Sign Language with the use of 3D animation resulting to a video output. The video created contains a 3D animated avatar performing the sign translations to FSL (mainly focusing on hand gestures which includes hand shape, palm orientation, location, and movement) on screen beside their English text equivalent and related images. The final output was then evaluated by FSL deaf signers. Evaluation results showed that the final output can potentially be used as a learning material. In order to make it more effective as a learning material, it is very important to consider the animation’s appearance, speed, naturalness, and accuracy. In this paper, the common action units were also listed for easier construction of animations of the signs.</abstract>
      <url hash="46955659">2020.signlang-1.7</url>
      <language>eng</language>
      <bibkey>cueto-etal-2020-translating</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>LSE</fixed-case>_<fixed-case>UVIGO</fixed-case>: A Multi-source Database for <fixed-case>S</fixed-case>panish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Recognition</title>
      <author><first>Laura</first><last>Docío-Fernández</last></author>
      <author><first>José Luis</first><last>Alba-Castro</last></author>
      <author><first>Soledad</first><last>Torres-Guijarro</last></author>
      <author><first>Eduardo</first><last>Rodríguez-Banga</last></author>
      <author><first>Manuel</first><last>Rey-Area</last></author>
      <author><first>Ania</first><last>Pérez-Pérez</last></author>
      <author><first>Sonia</first><last>Rico-Alonso</last></author>
      <author><first>Carmen</first><last>García-Mateo</last></author>
      <pages>45–52</pages>
      <abstract>This paper presents LSE_UVIGO, a multi-source database designed to foster research on Sign Language Recognition. It is being recorded and compiled for Spanish Sign Language (LSE acronym in Spanish) and contains also spoken Galician language, so it is very well fitted to research on these languages, but also quite useful for fundamental research in any other sign language. LSE_UVIGO is composed of two datasets: LSE_Lex40_UVIGO, a multi-sensor and multi-signer dataset acquired from scratch, designed as an incremental dataset, both in complexity of the visual content and in the variety of signers. It contains static and co-articulated sign recordings, fingerspelled and gloss-based isolated words, and sentences. Its acquisition is done in a controlled lab environment in order to obtain good quality videos with sharp video frames and RGB and depth information, making them suitable to try different approaches to automatic recognition. The second subset, LSE_TVGWeather_UVIGO is being populated from the regional television weather forecasts interpreted to LSE, as a faster way to acquire high quality, continuous LSE recordings with a domain-restricted vocabulary and with a correspondence to spoken sentences.</abstract>
      <url hash="d3ee1bf4">2020.signlang-1.8</url>
      <language>eng</language>
      <bibkey>docio-fernandez-etal-2020-lse</bibkey>
    </paper>
    <paper id="9">
      <title>Elicitation and Corpus of Spontaneous Sign Language Discourse Representation Diagrams</title>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>53–60</pages>
      <abstract>While Sign Languages have no standard written form, many signers do capture their language in some form of spontaneous graphical form. We list a few use cases (discourse preparation, deverbalising for translation, etc.) and give examples of diagrams. After hypothesising that they contain regular patterns of significant value, we propose to build a corpus of such productions. The main contribution of this paper is the specification of the elicitation protocol, explaining the variables that are likely to affect the diagrams collected. We conclude with a report on the current state of a collection following this protocol, and a few observations on the collected contents. A first prospect is the standardisation of a scheme to represent SL discourse in a way that would make them sharable. A subsequent longer-term prospect is for this scheme to be owned by users and with time be shaped into a script for their language.</abstract>
      <url hash="98b9400b">2020.signlang-1.9</url>
      <language>eng</language>
      <bibkey>filhol-2020-elicitation</bibkey>
    </paper>
    <paper id="10">
      <title>The Synthesis of Complex Shape Deployments in Sign Language</title>
      <author><first>Michael</first><last>Filhol</last></author>
      <author><first>John C.</first><last>McDonald</last></author>
      <pages>61–68</pages>
      <abstract>Proform constructs such as classifier predicates and size and shape specifiers are essential elements of Sign Language communication, but have remained a challenge for synthesis due to their highly variable nature. In contrast to frozen signs, which may be pre-animated or recorded, their variability necessitates a new approach both to their linguistic description and to their synthesis in animation. Though the specification and animation of classifier predicates was covered in previous works, size and shape specifiers have to this date remain unaddressed. This paper presents an efficient method for linguistically describing such specifiers using a small number of rules that cover a large range of possible constructs. It continues to show that with a small number of services in a signing avatar, these descriptions can be synthesized in a natural way that captures the essential gestural actions while also including the subtleties of human motion that make the signing legible.</abstract>
      <url hash="5fc27b0d">2020.signlang-1.10</url>
      <language>eng</language>
      <bibkey>filhol-mcdonald-2020-synthesis</bibkey>
    </paper>
    <paper id="11">
      <title>Signing as Input for a Dictionary Query: Matching Signs Based on Joint Positions of the Dominant Hand</title>
      <author><first>Manolis</first><last>Fragkiadakis</last></author>
      <author><first>Victoria</first><last>Nyst</last></author>
      <author><first>Peter</first><last>van der Putten</last></author>
      <pages>69–74</pages>
      <abstract>This study presents a new methodology to search sign language lexica, using a full sign as input for a query. Thus, a dictionary user can look up information about a sign by signing the sign to a webcam. The recorded sign is then compared to potential matching signs in the lexicon. As such, it provides a new way of searching sign language dictionaries to complement existing methods based on (spoken language) glosses or phonological features, like handshape or location. The method utilizes OpenPose to extract the body and finger joint positions. Dynamic Time Warping (DTW) is used to quantify the variation of the trajectory of the dominant hand and the average trajectories of the fingers. Ten people with various degrees of sign language proficiency have participated in this study. Each subject viewed a set of 20 signs from the newly compiled Ghanaian sign language lexicon and was asked to replicate the signs. The results show that DTW can predict the matching sign with 87% and 74% accuracy at the Top-10 and Top-5 ranking level respectively by using only the trajectory of the dominant hand. Additionally, more proficient signers obtain 90% accuracy at the Top-10 ranking. The methodology has the potential to be used also as a variation measurement tool to quantify the difference in signing between different signers or sign languages in general.</abstract>
      <url hash="da33243a">2020.signlang-1.11</url>
      <language>eng</language>
      <bibkey>fragkiadakis-etal-2020-signing</bibkey>
    </paper>
    <paper id="12">
      <title>Extending the <fixed-case>P</fixed-case>ublic <fixed-case>DGS</fixed-case> <fixed-case>C</fixed-case>orpus in Size and Depth</title>
      <author><first>Thomas</first><last>Hanke</last></author>
      <author><first>Marc</first><last>Schulder</last></author>
      <author><first>Reiner</first><last>Konrad</last></author>
      <author><first>Elena</first><last>Jahn</last></author>
      <pages>75–82</pages>
      <abstract>In 2018 the DGS-Korpus project published the first full release of the Public DGS Corpus. This event marked a change of focus for the project. While before most attention had been on increasing the size of the corpus, now an increase in its depth became the priority. New data formats were added, corpus annotation conventions were released and OpenPose pose information was published for all transcripts. The community and research portal websites of the corpus also received upgrades, including persistent identifiers, archival copies of previous releases and improvements to their usability on mobile devices.The research portal was enhanced even further, improving its transcript web viewer, adding a KWIC concordance view, introducing cross-references to other linguistic resources of DGS and making its entire interface available in German in addition to English. This article provides an overview of these changes, chronicling the evolution of the Public DGS Corpus from its first release in 2018, through its second release in 2019 until its third release in 2020.</abstract>
      <url hash="9e3f6982">2020.signlang-1.12</url>
      <language>eng</language>
      <bibkey>hanke-etal-2020-extending</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>S</fixed-case>ign<fixed-case>H</fixed-case>unter – A Sign Elicitation Tool Suitable for Deaf Events</title>
      <author><first>Thomas</first><last>Hanke</last></author>
      <author><first>Elena</first><last>Jahn</last></author>
      <author><first>Sabrina</first><last>Wähl</last></author>
      <author><first>Oliver</first><last>Böse</last></author>
      <author><first>Lutz</first><last>König</last></author>
      <pages>83–88</pages>
      <abstract>This paper presents SignHunter, a tool for collecting isolated signs, and discusses application possibilities. SignHunter is successfully used within the DGS-Korpus project to collect name signs for places and cities. The data adds to the content of a German Sign Language (DGS) – German dictionary which is currently being developed, as well as a freely accessible subset of the DGS Corpus, the Public DGS Corpus. We discuss reasons to complement a natural language corpus by eliciting concepts without context and present an application example of SignHunter.</abstract>
      <url hash="9c9b012d">2020.signlang-1.13</url>
      <language>eng</language>
      <bibkey>hanke-etal-2020-signhunter</bibkey>
    </paper>
    <paper id="14">
      <title>An Isolated-Signing <fixed-case>RGBD</fixed-case> Dataset of 100 <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Signs Produced by Fluent <fixed-case>ASL</fixed-case> Signers</title>
      <author><first>Saad</first><last>Hassan</last></author>
      <author><first>Larwan</first><last>Berke</last></author>
      <author><first>Elahe</first><last>Vahdani</last></author>
      <author><first>Longlong</first><last>Jing</last></author>
      <author><first>Yingli</first><last>Tian</last></author>
      <author><first>Matt</first><last>Huenerfauth</last></author>
      <pages>89–94</pages>
      <abstract>We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor. This directed dataset had originally been collected as part of an ongoing collaborative project, to aid in the development of a sign-recognition system for identifying occurrences of these 100 signs in video. The set of words consist of vocabulary items that would commonly be learned in a first-year ASL course offered at a university, although the specific set of signs selected for inclusion in the dataset had been motivated by project-related factors. Given increasing interest among sign-recognition and other computer-vision researchers in red-green-blue-depth (RBGD) video, we release this dataset for use by the research community. In addition to the RGB video files, we share depth and HD face data as well as additional features of face, hands, and body produced through post-processing of this data.</abstract>
      <url hash="12d76e40">2020.signlang-1.14</url>
      <language>eng</language>
      <bibkey>hassan-etal-2020-isolated</bibkey>
    </paper>
    <paper id="15">
      <title>Approaches to the Anonymisation of Sign Language Corpora</title>
      <author><first>Amy</first><last>Isard</last></author>
      <pages>95–100</pages>
      <abstract>In this paper we survey the state of the art for the anonymisation of sign language corpora. We begin by exploring the motivations behind anonymisation and the close connection with the issue of ethics and informed consent for corpus participants. We detail how the the names which should be anonymised can be identified. We then describe the processes which can be used to anonymise both the video and the annotations belonging to a corpus, and the variety of ways in which these can be carried out. We provide examples for all of these processes from three sign language corpora in which anonymisation of the data has been performed.</abstract>
      <url hash="e0a794b9">2020.signlang-1.15</url>
      <language>eng</language>
      <bibkey>isard-2020-approaches</bibkey>
    </paper>
    <paper id="16">
      <title>Sign Language Motion Capture Dataset for Data-driven Synthesis</title>
      <author><first>Pavel</first><last>Jedlička</last></author>
      <author><first>Zdeněk</first><last>Krňoul</last></author>
      <author><first>Jakub</first><last>Kanis</last></author>
      <author><first>Miloš</first><last>Železný</last></author>
      <pages>101–106</pages>
      <abstract>This paper presents a new 3D motion capture dataset of Czech Sign Language (CSE). Its main purpose is to provide the data for further analysis and data-based automatic synthesis of CSE utterances. The content of the data in the given limited domain of weather forecasts was carefully selected by the CSE linguists to provide the necessary utterances needed to produce any new weather forecast. The dataset was recorded using the state-of-the-art motion capture (MoCap) technology to provide the most precise trajectories of the motion. In general, MoCap is a device capable of accurate recording of motion directly in 3D space. The data contains trajectories of body, arms, hands and face markers recorded at once to provide consistent data without the need for the time alignment.</abstract>
      <url hash="b43c520f">2020.signlang-1.16</url>
      <language>eng</language>
      <bibkey>jedlicka-etal-2020-sign</bibkey>
    </paper>
    <paper id="17">
      <title>A survey of Shading Techniques for Facial Deformations on Sign Language Avatars</title>
      <author><first>Ronan</first><last>Johnson</last></author>
      <author><first>Rosalee</first><last>Wolfe</last></author>
      <pages>107–112</pages>
      <abstract>Of the five phonemic parameters in sign language (handshape, location, palm orientation, movement and nonmanual expressions), the one that still poses the most challenges for effective avatar display is nonmanual signals. Facial nonmanual signals carry a rich combination of linguistic and pragmatic information, but current techniques have yet to portray these in a satisfactory manner. Due to the complexity of facial movements, additional considerations must be taken into account for rendering in real time. Of particular interest is the shading areas of facial deformations to improve legibility. In contrast to more physically-based, compute-intensive techniques that more closely mimic nature, we propose using a simple, classic, Phong illumination model with a dynamically modified layered texture. To localize and control the desired shading, we utilize an opacity channel within the texture. The new approach, when applied to our avatar “Paula”, results in much quicker render times than more sophisticated, computationally intensive techniques.</abstract>
      <url hash="20c23903">2020.signlang-1.17</url>
      <language>eng</language>
      <bibkey>johnson-wolfe-2020-survey</bibkey>
    </paper>
    <paper id="18">
      <title>Use Cases for a Sign Language Concordancer</title>
      <author><first>Marion</first><last>Kaczmarek</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>113–116</pages>
      <abstract>This article treats about a Sign Language concordancer. In the past years, the need for content translated into Sign Language has been growing, and is still growing nowadays. Yet, unlike their text-to-text counterparts, Sign Language translators are not equipped with computer-assisted translation software. As we aim to provide them with such software, we explore the possibilities offered by a first tool: a Sign Language concordancer. It includes designing an alignments database as well as a search function to browse it. Testing sessions with professionals highlight relevant use cases for their professional practices. It can either comfort the translator when the results are identical, or show the importance of context when the results are different for a same expression. This concordancer is available online, and aim to be a collaborative tool. Though our current database is small, we hope for translators to invest themselves and help us to keep it expanding.</abstract>
      <url hash="5d2fa0cc">2020.signlang-1.18</url>
      <language>eng</language>
      <bibkey>kaczmarek-filhol-2020-use</bibkey>
    </paper>
    <paper id="19">
      <title>Towards <fixed-case>K</fixed-case>urdish Text to Sign Translation</title>
      <author><first>Zina</first><last>Kamal</last></author>
      <author><first>Hossein</first><last>Hassani</last></author>
      <pages>117–122</pages>
      <abstract>The resources and technologies for Sign language processing of resourceful languages are emerging, while the low-resource languages are falling behind. Kurdish is a multi-dialect language, and it is considered a low-resource language. It is spoken by approximately 30 million people in several countries, which denotes that it has a large community with hearing-impairments as well. This paper reports on a project which aims to develop the necessary data and tools to process the Sign language for Sorani as one of the spoken Kurdish dialects. We present the results of developing a dataset in HamNoSys and its corresponding SiGML form for the Kurdish Sign lexicon. We use this dataset to implement a sign-supported Kurdish tool to check the accuracy of the Sign lexicon. We tested the tool by presenting it to hearing-impaired individuals. The experiment showed that 100% of the translated letters were understandable by a hearing-impaired person. The percentages were 65% for isolated words, and approximately 30% for the words in sentences. The data is publicly available at <url>https://github.com/KurdishBLARK/KurdishSignLanguage</url> for non-commercial use under the CC BY-NC-SA 4.0 licence</abstract>
      <url hash="55707465">2020.signlang-1.19</url>
      <language>eng</language>
      <bibkey>kamal-hassani-2020-towards</bibkey>
      <pwccode url="https://github.com/KurdishBLARK/KurdishSignLanguage" additional="false">KurdishBLARK/KurdishSignLanguage</pwccode>
    </paper>
    <paper id="20">
      <title>Recognition of Static Features in Sign Language Using Key-Points</title>
      <author><first>Ioannis</first><last>Koulierakis</last></author>
      <author><first>Georgios</first><last>Siolas</last></author>
      <author><first>Eleni</first><last>Efthimiou</last></author>
      <author><first>Evita</first><last>Fotinea</last></author>
      <author><first>Andreas-Georgios</first><last>Stafylopatis</last></author>
      <pages>123–126</pages>
      <abstract>In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for handshape, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software. The models have been applied to a Danish and a Greek Sign Language dataset, providing results around 96%. Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.</abstract>
      <url hash="ba17a60c">2020.signlang-1.20</url>
      <language>eng</language>
      <bibkey>koulierakis-etal-2020-recognition</bibkey>
    </paper>
    <paper id="21">
      <title>Collocations in Sign Language Lexicography: Towards Semantic Abstractions for Word Sense Discrimination</title>
      <author><first>Gabriele</first><last>Langer</last></author>
      <author><first>Marc</first><last>Schulder</last></author>
      <pages>127–134</pages>
      <abstract>In general monolingual lexicography a corpus-based approach to word sense discrimination (WSD) is the current standard. Automatically generated lexical profiles such as Word Sketches provide an overview on typical uses in the form of collocate lists grouped by their part of speech categories and their syntactic dependency relations to the base item. Collocates are sorted by their typicality according to frequency-based rankings. With the advancement of sign language (SL) corpora, SL lexicography can finally be based on actual language use as reflected in corpus data. In order to use such data effectively and gain new insights on sign usage, automatically generated collocation profiles need to be developed under the special conditions and circumstances of the SL data available. One of these conditions is that many of the prerequesites for the automatic syntactic parsing of corpora are not yet available for SL. In this article we describe a collocation summary generated from DGS Corpus data which is used for WSD as well as in entry-writing. The summary works based on the glosses used for lemmatisation. In addition, we explore how other resources can be utilised to add an additional layer of semantic grouping to the collocation analysis. For this experimental approach we use glosses, concepts, and wordnet supersenses.</abstract>
      <url hash="6c09cdb5">2020.signlang-1.21</url>
      <language>eng</language>
      <bibkey>langer-schulder-2020-collocations</bibkey>
    </paper>
    <paper id="22">
      <title>Machine Learning for Enhancing Dementia Screening in Ageing Deaf Signers of <fixed-case>B</fixed-case>ritish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Xing</first><last>Liang</last></author>
      <author><first>Bencie</first><last>Woll</last></author>
      <author><first>Kapetanios</first><last>Epaminondas</last></author>
      <author><first>Anastasia</first><last>Angelopoulou</last></author>
      <author><first>Reda</first><last>Al-Batat</last></author>
      <pages>135–138</pages>
      <abstract>Ageing trend in populations is correlated with increased prevalence of acquired cognitive impairments such as dementia. Although there is no cure for dementia, a timely diagnosis helps in obtaining necessary support and appropriate medication. With this in mind, researchers are working urgently to develop effective technological tools that can help doctors undertake early identification of cognitive disorder. In this paper, we introduce an automatic dementia screening system for ageing Deaf signers of British Sign Language (BSL), using Convolutional Neural Networks (CNN), by analysing the sign space envelope and facial expression of BSL signers using normal 2D videos from BSL corpus. Our approach firstly establishes an accurate real-time hand trajectory tracking model together with a real-time landmark facial motion analysis model to identify differences in sign space envelope and facial movement as the keys to identifying language changes associated with dementia. Based on the differences in patterns obtained from facial and trajectory motion data, CNN models (ResNet50/VGG16) are fine-tuned using Keras deep learning models to incrementally identify and improve dementia recognition rates. We report the results for two methods using different modalities (sign trajectory and facial motion), together with the performance comparisons between different deep learning CNN models in ResNet50 and VGG16. The experiments show the effectiveness of our deep learning based approach in terms of sign space tracking, facial motion tracking and early stage dementia performance assessment tasks. The results are validated against cognitive assessment scores as of our ground truth data with a test set performance of 87.88%. The proposed system has potential for economical, simple, flexible, and adaptable assessment of other acquired neurological impairments associated with motor changes, such as stroke and Parkinson’s disease in both hearing and Deaf people.</abstract>
      <url hash="c6fb2ac4">2020.signlang-1.22</url>
      <language>eng</language>
      <bibkey>liang-etal-2020-machine</bibkey>
    </paper>
    <paper id="23">
      <title>Machine Translation from Spoken Language to Sign Language using Pre-trained Language Model as Encoder</title>
      <author><first>Taro</first><last>Miyazaki</last></author>
      <author><first>Yusuke</first><last>Morita</last></author>
      <author><first>Masanori</first><last>Sano</last></author>
      <pages>139–144</pages>
      <abstract>Sign language is the first language for those who were born deaf or lost their hearing in early childhood, so such individuals require services provided with sign language. To achieve flexible open-domain services with sign language, machine translations into sign language are needed. Machine translations generally require large-scale training corpora, but there are only small corpora for sign language. To overcome this data-shortage scenario, we developed a method that involves using a pre-trained language model of spoken language as the initial model of the encoder of the machine translation model. We evaluated our method by comparing it to baseline methods, including phrase-based machine translation, using only 130,000 phrase pairs of training data. Our method outperformed the baseline method, and we found that one of the reasons of translation error is from pointing, which is a special feature used in sign language. We also conducted trials to improve the translation quality for pointing. The results are somewhat disappointing, so we believe that there is still room for improving translation quality, especially for pointing.</abstract>
      <url hash="e68807df">2020.signlang-1.23</url>
      <language>eng</language>
      <bibkey>miyazaki-etal-2020-machine</bibkey>
    </paper>
    <paper id="24">
      <title>Towards Large-Scale Data Mining for Data-Driven Analysis of Sign Languages</title>
      <author><first>Boris</first><last>Mocialov</last></author>
      <author><first>Graham</first><last>Turner</last></author>
      <author><first>Helen</first><last>Hastie</last></author>
      <pages>145–150</pages>
      <abstract>Access to sign language data is far from adequate. We show that it is possible to collect the data from social networking services such as TikTok, Instagram, and YouTube by applying data filtering to enforce quality standards and by discovering patterns in the filtered data, making it easier to analyse and model. Using our data collection pipeline, we collect and examine the interpretation of songs in both the American Sign Language (ASL) and the Brazilian Sign Language (Libras). We explore their differences and similarities by looking at the co-dependence of the orientation and location phonological parameters.</abstract>
      <url hash="e96a70c5">2020.signlang-1.24</url>
      <language>eng</language>
      <bibkey>mocialov-etal-2020-towards</bibkey>
    </paper>
    <paper id="25">
      <title>Extending a Model for Animating Adverbs of Manner in <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Robyn</first><last>Moncrief</last></author>
      <pages>151–156</pages>
      <abstract>The goal of this work is to show that a model produced to characterize adverbs of manner can be applied to a variety of neutral animated signs to be used towards avatar sign language synthesis. This case study presents the extension of a new approach that was first presented at SLTAT 2019 in Hamburg for modeling language processes that manifest themselves as modifications to the manual channel. This work discusses additions to the model to be effective for one-handed and two-handed signs, repeating and non-repeating signs, and signs with contact.</abstract>
      <url hash="e922803b">2020.signlang-1.25</url>
      <language>eng</language>
      <bibkey>moncrief-2020-extending</bibkey>
    </paper>
    <paper id="26">
      <title>From Dictionary to Corpus and Back Again – Linking Heterogeneous Language Resources for <fixed-case>DGS</fixed-case></title>
      <author><first>Anke</first><last>Müller</last></author>
      <author><first>Thomas</first><last>Hanke</last></author>
      <author><first>Reiner</first><last>Konrad</last></author>
      <author><first>Gabriele</first><last>Langer</last></author>
      <author><first>Sabrina</first><last>Wähl</last></author>
      <pages>157–164</pages>
      <abstract>The Public DGS Corpus is published in two different formats, that is subtitled videos for lay persons and lemmatized and annotated transcripts and videos for experts. In addition, a draft version with the first set of preliminary entries of the DGS dictionary (DW-DGS) to be completed in 2023 is now online. The Public DGS Corpus and the DW-DGS are conceived of as stand-alone products, but are nevertheless closely interconnected to offer additional and complementary informative functions. In this paper we focus on linking the published products in order to provide users access to corpus and corpus-based dictionary in various, interrelated ways. We discuss which links are thought to be useful and what challenges the linking of the products poses. In addition we address the inclusion of links to other, older lexical resources (LSP dictionaries).</abstract>
      <url hash="988f3bbe">2020.signlang-1.26</url>
      <language>eng</language>
      <bibkey>muller-etal-2020-dictionary</bibkey>
    </paper>
    <paper id="27">
      <title>Automatic Classification of Handshapes in <fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Alfarabi</first><last>Imashev</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>165–170</pages>
      <abstract>Handshapes are one of the basic parameters of signs, and any phonological or phonetic analysis of a sign language must account for handshapes. Many sign languages have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as it is important due to the need of generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, it is a very time-consuming process, thus only a handful of sign languages have such inventories. This work proposes a process of automatically generating such inventories for sign languages by applying automatic hand detection, cropping, and clustering techniques. We applied our proposed method to a commonly used resource: the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verified the data to be able to perform classification. Thus, the proposed pipeline can serve as an alternative approach to manual annotation, and can help linguists in answering numerous research questions in relation to handshape frequencies in sign languages.</abstract>
      <url hash="a96553f5">2020.signlang-1.27</url>
      <language>eng</language>
      <bibkey>mukushev-etal-2020-automatic</bibkey>
    </paper>
    <paper id="28">
      <title>Design and Evaluation for a Prototype of an Online Tool to Access Mathematics Notions in Sign Language</title>
      <author><first>Camille</first><last>Nadal</last></author>
      <author><first>Christophe</first><last>Collet</last></author>
      <pages>171–176</pages>
      <abstract>The Sign’Maths project aims at giving access to pedagogical resources in Sign Language (SL). It will provide Deaf students and teachers with mathematics vocabulary in SL, this in order to contribute to the standardisation of the vocabulary used at school. The work conducted led to Sign’Maths, an online interactive tool that gives Deaf students access to mathematics definitions in SL. A group of mathematics teachers for Deafs and teachers experts in SL collaborated to create signs to express mathematics concepts, and to produce videos of definitions, examples and illustrations for these concepts. In parallel, we are working on the conception and the design of Sign’Maths software and user interface. Our research work investigated ways to include SL in pedagogical resources in order to present information but also to navigate through the content. User tests revealed that users appreciate the use of SL in a pedagogical resource. However, they pointed out that SL content should be complemented with French to support bilingual education. Our final solution takes advantage of the complementarity of SL, French and visual content to provide an interface that will suit users no matter what their education background is. Future work will investigate a tool for text and signs’ search within Sign’Maths.</abstract>
      <url hash="3c0ad942">2020.signlang-1.28</url>
      <language>eng</language>
      <bibkey>nadal-collet-2020-design</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>STS</fixed-case>-korpus: A Sign Language Web Corpus Tool for Teaching and Public Use</title>
      <author><first>Zrajm</first><last>Öqvist</last></author>
      <author><first>Nikolaus</first><last>Riemer Kankkonen</last></author>
      <author><first>Johanna</first><last>Mesch</last></author>
      <pages>177–180</pages>
      <abstract>In this paper we describe STS-korpus, a web corpus tool for Swedish Sign Language (STS) which we have built during the past year, and which is now publicly available on the internet. STS-korpus uses the data of Swedish Sign Language Corpus (SSLC) and is primarily intended for teachers and students of sign language. As such it is created to be simple and user-friendly with no download or setup required. The user interface allows for searching – with search results displayed as a simple concordance – and viewing of videos with annotations. Each annotation also provides additional data and links to the corresponding entry in the online Swedish Sign Language Dictionary. We describe the corpus, its appearance and search syntax, as well as more advanced features like access control and dynamic content. Finally we say a word or two about the role we hope it will play in the classroom, and something about the development process and the software used. STS-korpus is available here: https://teckensprakskorpus.su.se</abstract>
      <url hash="b2296fd8">2020.signlang-1.29</url>
      <language>eng</language>
      <bibkey>oqvist-etal-2020-sts</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>B</fixed-case>osphorus<fixed-case>S</fixed-case>ign22k Sign Language Recognition Dataset</title>
      <author><first>Oğulcan</first><last>Özdemir</last></author>
      <author><first>Ahmet Alp</first><last>Kındıroğlu</last></author>
      <author><first>Necati</first><last>Cihan Camgöz</last></author>
      <author><first>Lale</first><last>Akarun</last></author>
      <pages>181–188</pages>
      <abstract>Sign Language Recognition is a challenging research domain. It has recently seen several advancements with the increased availability of data. In this paper, we introduce the BosphorusSign22k, a publicly available large scale sign language dataset aimed at computer vision, video recognition and deep learning research communities. The primary objective of this dataset is to serve as a new benchmark in Turkish Sign Language Recognition for its vast lexicon, the high number of repetitions by native signers, high recording quality, and the unique syntactic properties of the signs it encompasses. We also provide state-of-the-art human pose estimates to encourage other tasks such as Sign Language Production. We survey other publicly available datasets and expand on how BosphorusSign22k can contribute to future research that is being made possible through the widespread availability of similar Sign Language resources. We have conducted extensive experiments and present baseline results to underpin future research on our dataset.</abstract>
      <url hash="dc25c0b2">2020.signlang-1.30</url>
      <language>eng</language>
      <bibkey>ozdemir-etal-2020-bosphorussign22k</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bosphorussign22k">BosphorusSign22k</pwcdataset>
    </paper>
    <paper id="31">
      <title>Unsupervised Term Discovery for Continuous Sign Language</title>
      <author><first>Korhan</first><last>Polat</last></author>
      <author><first>Murat</first><last>Saraçlar</last></author>
      <pages>189–196</pages>
      <abstract>Most of the sign language recognition (SLR) systems rely on supervision for training and available annotated sign language resources are scarce due to the difficulties of manual labeling. Unsupervised discovery of lexical units would facilitate the annotation process and thus lead to better SLR systems. Inspired by the unsupervised spoken term discovery in speech processing field, we investigate whether a similar approach can be applied in sign language to discover repeating lexical units. We adapt an algorithm that is designed for spoken term discovery by using hand shape and pose features instead of speech features. The experiments are run on a large scale continuous sign corpus and the performance is evaluated using gloss level annotations. This work introduces a new task for sign language processing that has not been addressed before.</abstract>
      <url hash="546b2a98">2020.signlang-1.31</url>
      <language>eng</language>
      <bibkey>polat-saraclar-2020-unsupervised</bibkey>
    </paper>
    <paper id="32">
      <title>The Corpus of <fixed-case>F</fixed-case>innish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Juhana</first><last>Salonen</last></author>
      <author><first>Antti</first><last>Kronqvist</last></author>
      <author><first>Tommi</first><last>Jantunen</last></author>
      <pages>197–202</pages>
      <abstract>This paper presents the Corpus of Finnish Sign Language (Corpus FinSL), a structured and annotated collection of Finnish Sign Language (FinSL) videos published in May 2019 in FIN-CLARIN’s Language Bank of Finland. The corpus is divided into two subcorpora, one of which comprises elicited narratives and the other conversations. All of the FinSL material has been annotated using ELAN and the lexical database Finnish Signbank. Basic annotation includes ID-glosses and translations into Finnish. The anonymized metadata of Corpus FinSL has been organized in accordance with the IMDI standard. Altogether, Corpus FinSL contains nearly 15 hours of video material from 21 FinSL users. Corpus FinSL has already been exploited in FinSL research and teaching, and it is predicted that in the future it will have a significant positive impact on these fields as well as on the status of the sign language community in Finland. Keywords: Corpus of Finnish Sign Language, Language Bank of Finland, Finnish Signbank, annotation, metadata, research, teaching</abstract>
      <url hash="f260991a">2020.signlang-1.32</url>
      <language>eng</language>
      <bibkey>salonen-etal-2020-corpus</bibkey>
    </paper>
    <paper id="33">
      <title>Tools for the Use of <fixed-case>S</fixed-case>ign<fixed-case>W</fixed-case>riting as a Language Resource</title>
      <author><first>Antonio F. G.</first><last>Sevilla</last></author>
      <author><first>Alberto</first><last>Díaz Esteban</last></author>
      <author><first>José María</first><last>Lahoz-Bengoechea</last></author>
      <pages>203–208</pages>
      <abstract>Representation of linguistic data is an issue of utmost importance when developing language resources, but the lack of a standard written form in sign languages presents a challenge. Different notation systems exist, but only SignWriting seems to have some use in the native signer community. It is, however, a difficult system to use computationally, not based on a linear sequence of characters. We present the project “VisSE”, which aims to develop tools for the effective use of SignWriting in the computer. The first of these is an application which uses computer vision to interpret SignWriting, understanding the meaning of new or existing transcriptions, or even hand-written images. Two additional tools will be able to consume the result of this recognizer: first, a textual description of the features of the transcription will make it understandable for non-signers. Second, a three-dimensional avatar will be able to reproduce the configurations and movements contained within the transcription, making it understandable for signers even if not familiar with SignWriting. Additionally, the project will result in a corpus of annotated SignWriting data which will also be of use to the computational linguistics community.</abstract>
      <url hash="ed620195">2020.signlang-1.33</url>
      <language>eng</language>
      <bibkey>sevilla-etal-2020-tools</bibkey>
    </paper>
    <paper id="34">
      <title>Video-to-<fixed-case>H</fixed-case>am<fixed-case>N</fixed-case>o<fixed-case>S</fixed-case>ys Automated Annotation System</title>
      <author><first>Victor</first><last>Skobov</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>209–216</pages>
      <abstract>The Hamburg Notation System (HamNoSys) was developed for movement annotation of any sign language (SL) and can be used to produce signing animations for a virtual avatar with the JASigning platform. This provides the potential to use HamNoSys, i.e., strings of characters, as a representation of an SL corpus instead of video material. Processing strings of characters instead of images can significantly contribute to sign language research. However, the complexity of HamNoSys makes it difficult to annotate without a lot of time and effort. Therefore annotation has to be automatized. This work proposes a conceptually new approach to this problem. It includes a new tree representation of the HamNoSys grammar that serves as a basis for the generation of grammatical training data and classification of complex movements using machine learning. Our automatic annotation system relies on HamNoSys grammar structure and can potentially be used on already existing SL corpora. It is retrainable for specific settings such as camera angles, speed, and gestures. Our approach is conceptually different from other SL recognition solutions and offers a developed methodology for future research.</abstract>
      <url hash="9448fa21">2020.signlang-1.34</url>
      <language>eng</language>
      <bibkey>skobov-lepage-2020-video</bibkey>
    </paper>
    <paper id="35">
      <title>Cross-Lingual Keyword Search for Sign Language</title>
      <author><first>Nazif Can</first><last>Tamer</last></author>
      <author><first>Murat</first><last>Saraçlar</last></author>
      <pages>217–223</pages>
      <abstract>Sign language research most often relies on exhaustively annotated and segmented data, which is scarce even for the most studied sign languages. However, parallel corpora consisting of sign language interpreting are rarely explored. By utilizing such data for the task of keyword search, this work aims to enable information retrieval from sign language with the queries from the translated written language. With the written language translations as labels, we train a weakly supervised keyword search model for sign language and further improve the retrieval performance with two context modeling strategies. In our experiments, we compare the gloss retrieval and cross language retrieval performance on RWTH-PHOENIX-Weather 2014T dataset.</abstract>
      <url hash="6b2680c3">2020.signlang-1.35</url>
      <language>eng</language>
      <bibkey>tamer-saraclar-2020-cross</bibkey>
    </paper>
    <paper id="36">
      <title>One Side of the Coin: Development of an <fixed-case>ASL</fixed-case>-<fixed-case>E</fixed-case>nglish Parallel Corpus by Leveraging <fixed-case>SRT</fixed-case> Files</title>
      <author><first>Rafael</first><last>Treviño</last></author>
      <author><first>Julie A.</first><last>Hochgesang</last></author>
      <author><first>Emily P.</first><last>Shaw</last></author>
      <author><first>Nic</first><last>Willow</last></author>
      <pages>224–230</pages>
      <abstract>We report on a method used to develop a sizable parallel corpus of English and American Sign Language (ASL). The effort is part of the Gallaudet University Documentation of ASL (GUDA) project, which is currently coordinated by an interdisciplinary team from the Department of Linguistics and the Department of Interpretation and Translation at Gallaudet University. Creation of the parallel corpus makes use of the available SRT (SubRip Subtitle) files of ASL videos that have been interpreted into or from English, or captioned into English. The corpus allows for one-way searches based on the English translation or interpretation, which is useful for translators, interpreters, and those conducting comparative analyses. We conclude with a discussion of important considerations for this method of constructing a parallel corpus, as well as next steps that will help to refine the development and utility of this type of corpus.</abstract>
      <url hash="9da9783f">2020.signlang-1.36</url>
      <language>eng</language>
      <bibkey>trevino-etal-2020-one</bibkey>
    </paper>
  </volume>
</collection>
