<?xml version='1.0' encoding='UTF-8'?>
<collection id="2006.jeptalnrecital">
  <volume id="invite" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 13ème conférence sur le Traitement Automatique des Langues Naturelles. Conférences invitées</booktitle>
      <editor><first>Piet</first><last>Mertens</last></editor>
      <editor><first>Cédrick</first><last>Fairon</last></editor>
      <editor><first>Anne</first><last>Dister</last></editor>
      <editor><first>Patrick</first><last>Watrin</last></editor>
      <publisher>ATALA</publisher>
      <address>Leuven, Belgique</address>
      <month>April</month>
      <year>2006</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="591cc4e4">2006.jeptalnrecital-invite.0</url>
      <bibkey>jep-taln-recital-2006-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Les nouveaux outils de correction linguistique de <fixed-case>M</fixed-case>icrosoft</title>
      <author><first>Thierry</first><last>Fontenelle</last></author>
      <pages>3–19</pages>
      <abstract>De nouveaux outils de correction linguistique sont disponibles pour le français depuis quelques mois. Mis à la disposition des utilisateurs de Microsoft Office 2003, un nouveau correcteur orthographique et un nouveau correcteur grammatical permettent d’améliorer le processus de rédaction de documents. En partant d’évaluations externes effectuées récemment, nous présentons les diverses facettes de ces améliorations et de ces outils, en abordant la question de l’évaluation des outils de correction linguistique (qu’évaluer ? quels critères appliquer ? pourquoi développer une nouvelle version ?). La réforme de l’orthographe, la féminisation des noms de métier, l’évolution de la langue figurent parmi les thèmes abordés dans cet article.</abstract>
      <url hash="26593f62">2006.jeptalnrecital-invite.1</url>
      <language>fra</language>
      <bibkey>fontenelle-2006-les</bibkey>
    </paper>
    <paper id="2">
      <title>At Last Parsing Is Now Operational</title>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>20–42</pages>
      <abstract>Natural language analysis systems which combine knowledge-based and corpus-based methods are now becoming accurate enough to be used in various applications. We describe one such parsing system for Dutch, known as Alpino, and we show how corpus-based methods are essential to obtain accurate knowledge-based parsers. In particular we show a variety of cases where large amounts of parser output are used to improve the parser.</abstract>
      <url hash="7be0eae2">2006.jeptalnrecital-invite.2</url>
      <bibkey>van-noord-2006-last</bibkey>
    </paper>
  </volume>
  <volume id="long" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 13ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Piet</first><last>Mertens</last></editor>
      <editor><first>Cédrick</first><last>Fairon</last></editor>
      <editor><first>Anne</first><last>Dister</last></editor>
      <editor><first>Patrick</first><last>Watrin</last></editor>
      <publisher>ATALA</publisher>
      <address>Leuven, Belgique</address>
      <month>April</month>
      <year>2006</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="aedb2db0">2006.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2006-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Increased Retrieval Performance using Word Sense Discrimination</title>
      <author><first>Atelach</first><last>Alemu Argaw</last></author>
      <author><first>Lars</first><last>Asker</last></author>
      <pages>43–52</pages>
      <abstract>We show that Mutual Information between word pairs can be successfully used to discriminate between word senses in the query translation step of Cross Language Information Retrieval. The experiment is conducted in the context of Amharic to French Cross Language Information Retrieval. We have performed a number of retrieval experiments in which we compare the performance of the sense discriminated and non-discriminated set of query terms against a ranked document collection. The results show an increased performance for the discriminated queries compared to the alternative approach, which uses the fully expanded set of terms.</abstract>
      <url hash="c4940665">2006.jeptalnrecital-long.1</url>
      <bibkey>alemu-argaw-asker-2006-increased</bibkey>
    </paper>
    <paper id="2">
      <title>Traitement de la polysémie lexicale dans un but de traduction</title>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <pages>53–62</pages>
      <abstract>La désambiguïsation lexicale a une place centrale dans les applications de Traitement Automatique des Langues relatives à la traduction. Le travail présenté ici fait partie d’une étude sur les recouvrements et les divergences entre les espaces sémantiques occupés par des unités polysémiques de deux langues. Les correspondances entre ces unités sont rarement biunivoques et l’étude de ces correspondances aide à tirer des conclusions sur les possibilités et les limites d’utilisation d’une autre langue pour la désambiguïsation des unités d’une langue source. Le but de ce travail est l’établissement de correspondances d’une granularité optimale entre les unités de deux langues entretenant des relations de traduction. Ces correspondances seraient utilisables pour la prédiction des équivalents de traduction les plus adéquats de nouvelles occurrences des éléments polysémiques.</abstract>
      <url hash="992528c0">2006.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>apidianaki-2006-traitement</bibkey>
    </paper>
    <paper id="3">
      <title>Un analyseur morphologique multi-niveaux utilisant la jointure</title>
      <author><first>François</first><last>Barthélemy</last></author>
      <pages>63–72</pages>
      <abstract>Dans cet article nous présentons un analyseur morphologique pour le verbe akkadien. Cette langue est de la famille des langues sémitiques. Les flexions du verbe font intervenir des changements internes à la racine. L’analyseur présenté ici illustre l’utilisation d’un formalisme multi-niveaux et d’opérateurs relationnels puissants, notamment la jointure. La multiplicité de niveaux intermédiaires entre les formes profondes et de surface, ainsi que les opérateurs de compositions permettent de diviser la description en contraintes relativement simples qui sont ensuite rassemblées pour s’exercer soit simultanément, soit en cascade, soit encore d’une façon mixte, c’est-à-dire simultanément pour certains des niveaux et en cascade pour d’autres. Ce mécanisme nous permet de décrire la vocalisation du radical comme un processus d’insertions successives de voyelles. Cela présente l’intérêt d’être plus simple que l’utilisation d’un schéma vocalique figé soumis à interdigitation. De plus, cela semble expliquer de façon plus économique les formes des verbes faibles.</abstract>
      <url hash="5f12fe54">2006.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>barthelemy-2006-un</bibkey>
    </paper>
    <paper id="4">
      <title>Analyse quantitative et statistique de la sémantique dans un corpus technique</title>
      <author><first>Ann</first><last>Bertels</last></author>
      <author><first>Dirk</first><last>Speelman</last></author>
      <author><first>Dirk</first><last>Geeraerts</last></author>
      <pages>73–82</pages>
      <abstract>Cet article présente la méthodologie et les résultats d’une analyse sémantique quantitative d’environ 5000 spécificités dans le domaine technique des machines-outils pour l’usinage des métaux. Les spécificités seront identifiées avec la méthode des mots-clés (KeyWords Method). Ensuite, elles seront soumises à une analyse sémantique quantitative, à partir du recouvrement des cooccurrences des cooccurrences, permettant de déterminer le degré de monosémie des spécificités. Finalement, les données quantitatives de spécificité et de monosémie feront l’objet d’analyses de régression. Nous avançons l’hypothèse que les mots (les plus) spécifiques du corpus technique ne sont pas (les plus) monosémiques. Nous présenterons ici les résultats statistiques, ainsi qu’une interprétation linguistique. Le but de cette étude est donc de vérifier si et dans quelle mesure les spécificités du corpus technique sont monosémiques ou polysémiques et quels sont les facteurs déterminants.</abstract>
      <url hash="3b69d57f">2006.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>bertels-etal-2006-analyse</bibkey>
    </paper>
    <paper id="5">
      <title>Outilex, plate-forme logicielle de traitement de textes écrits</title>
      <author><first>Olivier</first><last>Blanc</last></author>
      <author><first>Matthieu</first><last>Constant</last></author>
      <author><first>Éric</first><last>Laporte</last></author>
      <pages>83–92</pages>
      <abstract>La plate-forme logicielle Outilex, qui sera mise à la disposition de la recherche, du développement et de l’industrie, comporte des composants logiciels qui effectuent toutes les opérations fondamentales du traitement automatique du texte écrit : traitements sans lexiques, exploitation de lexiques et de grammaires, gestion de ressources linguistiques. Les données manipulées sont structurées dans des formats XML, et également dans d’autres formats plus compacts, soit lisibles soit binaires, lorsque cela est nécessaire ; les convertisseurs de formats nécessaires sont inclus dans la plate-forme ; les formats de grammaires permettent de combiner des méthodes statistiques avec des méthodes fondées sur des ressources linguistiques. Enfin, des lexiques du français et de l’anglais issus du LADL, construits manuellement et d’une couverture substantielle seront distribués avec la plate-forme sous licence LGPL-LR.</abstract>
      <url hash="098d7718">2006.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>blanc-etal-2006-outilex</bibkey>
    </paper>
    <paper id="6">
      <title>Une grammaire multilingue partagée pour la traduction automatique de la parole</title>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Bruna</first><last>Novellas</last></author>
      <author><first>Yukie</first><last>Nakao</last></author>
      <author><first>Marianne</first><last>Santaholma</last></author>
      <author><first>Marianne</first><last>Starlander</last></author>
      <author><first>Nikos</first><last>Chatzichrisafis</last></author>
      <pages>93–102</pages>
      <abstract>Aujourd’hui, l’approche la plus courante en traitement de la parole consiste à combiner un reconnaisseur statistique avec un analyseur robuste. Pour beaucoup d’applications cependant, les reconnaisseurs linguistiques basés sur les grammaires offrent de nombreux avantages. Dans cet article, nous présentons une méthodologie et un ensemble de logiciels libres (appelé Regulus) pour dériver rapidement des reconnaisseurs linguistiquement motivés à partir d’une grammaire générale partagée pour le catalan et le français.</abstract>
      <url hash="ab9ce86c">2006.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>bouillon-etal-2006-une</bibkey>
    </paper>
    <paper id="7">
      <title>Prise en compte des disfluences dans un système d’analyse syntaxique automatique de l’oral</title>
      <author><first>Rémi</first><last>Bove</last></author>
      <author><first>Christine</first><last>Chardenon</last></author>
      <author><first>Jean</first><last>Véronis</last></author>
      <pages>103–111</pages>
      <abstract>Nous présentons dans cette étude un essai de prise en compte des disfluences dans un système d’analyse linguistique initialement prévu pour l’écrit, en vue de la réalisation d’un prototype de traduction parole-parole. À partir d’une étude approfondie sur corpus, nous montrons comment des modifications du lexique et de la grammaire ont permis de traiter les cas les plus simples (pauses remplies, répétitions de mots isolés, etc.). D’autres cas plus complexes comme répétitions et auto-corrections de syntagmes ont nécessité la mise au point d’un mécanisme de contrôle sémantique permettant de limiter la combinatoire. Cette étude a mis également en évidence la difficulté de traitement de phénomènes tels que les amorces (mots interrompus) et les constructions inachevées, qui pour l’instant restent sans solution satisfaisante.</abstract>
      <url hash="e2c3368a">2006.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>bove-etal-2006-prise</bibkey>
    </paper>
    <paper id="8">
      <title>Acquisition de concepts bilingues à partir du Web</title>
      <author><first>Olivier</first><last>Collin</last></author>
      <author><first>Émmanuelle</first><last>Pétrier</last></author>
      <pages>112–120</pages>
      <abstract>Nous montrons une utilisation du Web, corpus multilingue de grande taille, pour effectuer une acquisition supervisée de concepts bilingue français/anglais. Cette acquisition utilise comme point initial un verbe français. Nous apparions ensuite des phrases provenant des deux langues à partir de couples de noms propres possédant la même forme dans les deux langues. Cet appariement automatique mais sommaire ne garantit pas l’alignement des phrases. Nous montrons qu’il nous permet cependant d’extraire des termes français et anglais équivalents dans leur contexte d’utilisation. Ces termes constituent des ressources multilingues particulièrement adaptées au Web, notamment pour les applications question réponse « crosslingue ».</abstract>
      <url hash="3a6f35ce">2006.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>collin-petrier-2006-acquisition</bibkey>
    </paper>
    <paper id="9">
      <title>Extraction de relations sémantiques entre noms et verbes au-delà des liens morphologiques</title>
      <author><first>Cécile</first><last>Fabre</last></author>
      <author><first>Didier</first><last>Bourigault</last></author>
      <pages>121–129</pages>
      <abstract>Nous étudions les relations de proximité sémantique entre les noms et les verbes à partir de données calculées sur un corpus de 200 millions de mots par un programme d’analyse distributionnelle automatique. Nous exposons les résultats d’une méthode d’extraction de couples Nom/Verbe, qui combine un indice de proximité distributionnelle et un indice de cooccurrence : un couple est extrait si le nom et le verbe apparaissent avec les mêmes arguments sur l’ensemble du corpus, d’une part, et s’ils apparaissent au moins une fois dans un même paragraphe munis du même argument, d’autre part. L’article élabore une typologie des 1441 couples extraits et démontre l’intérêt de prendre en compte les couples non liés morphologiquement, qui constituent 70 % des données.</abstract>
      <url hash="0dbb2bda">2006.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>fabre-bourigault-2006-extraction</bibkey>
    </paper>
    <paper id="10">
      <title>Résumé multidocuments orienté par une requête complexe</title>
      <author><first>Atefeh</first><last>Farzindar</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>130–138</pages>
      <abstract>Nous présentons un système de synthèse d’information pour la production de résumés multidocuments orientés par une requête complexe. Après une analyse du profil de l’utilisateur exprimé par des questions complexes, nous comparons la similarité entre les documents à résumer avec les questions à deux niveaux : global et détaillé. Cette étude démontre l’importance d’étudier pour une requête la pertinence d’une phrase à l’intérieur de la structure thématique du document. Cette méthodologie a été appliquée lors de notre participation à la campagne d’évaluation DUC 2005 où notre système a été classé parmi les meilleurs.</abstract>
      <url hash="d2fe6bbc">2006.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>farzindar-lapalme-2006-resume</bibkey>
    </paper>
    <paper id="11">
      <title>Extraction d’information de sous-catégorisation à partir des tables du <fixed-case>LADL</fixed-case></title>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <author><first>Ingrid</first><last>Falk</last></author>
      <pages>139–148</pages>
      <abstract>Les tables du LADL (Laboratoire d’Automatique Documentaire et Linguistique) contiennent des données électroniques extensives sur les propriétés morphosyntaxiques et syntaxiques des foncteurs syntaxiques du français (verbes, noms, adjectifs). Ces données, dont on sait qu’elles sont nécessaires pour le bon fonctionnement des systèmes de traitement automatique des langues, ne sont cependant que peu utilisées par les systèmes actuels. Dans cet article, nous identifions les raisons de cette lacune et nous proposons une méthode de conversion des tables vers un format mieux approprié au traitement automatique des langues.</abstract>
      <url hash="e3d13557">2006.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>gardent-etal-2006-extraction</bibkey>
    </paper>
    <paper id="12">
      <title>Intégration d’une dimension sémantique dans les grammaires d’arbres adjoints</title>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>149–158</pages>
      <abstract>Dans cet article, nous considérons un formalisme linguistique pour lequel l’intégration d’information sémantique dans une grammaire à large couverture n’a pas encore été réalisée à savoir, les grammaires d’arbres adjoints (Tree Adjoining Grammar ou TAG). Nous proposons une méthode permettant cette intégration et décrivons sa mise en oeuvre dans une grammaire noyau pour le français. Nous montrons en particulier que le formalisme de spécification utilisé, XMG, (Duchier et al., 2004) permet une factorisation importante des données sémantiques facilitant ainsi le développement, la maintenance et le déboggage de la grammaire.</abstract>
      <url hash="af09dc69">2006.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>gardent-2006-integration</bibkey>
    </paper>
    <paper id="13">
      <title>Questions Booléennes : Oui ou Non, des Questions et des Réponses</title>
      <author><first>Laurent</first><last>Gillard</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Marc</first><last>El-Bèze</last></author>
      <pages>159–166</pages>
      <abstract>Dans cet article, nous présentons une approche afin de traiter les questions booléennes, c’est-à-dire des questions dont la réponse peut être un Oui ou un Non, cela, dans le cadre d’un système de Questions-Réponses. En effet, la campagne Technolangue-EQueR, première campagne francophone de Questions-Réponses (QR) utilisant des questions et un corpus en français, a également été la première campagne QR à introduire une évaluation pour ce type de questions. Nous détaillons, parallèlement à notre approche, des pistes de réflexion sur les aspects sous-jacents à ces questions booléennes, notamment au travers d’une analyse des résultats obtenus par notre système dans un contexte similaire à celui de notre participation à la campagne officielle.</abstract>
      <url hash="25d136f5">2006.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>gillard-etal-2006-questions</bibkey>
    </paper>
    <paper id="14">
      <title>Productivité quantitative des suffixations par -ité et -Able dans un corpus journalistique moderne</title>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Delphine</first><last>Tribout</last></author>
      <author><first>Georgette</first><last>Dal</last></author>
      <author><first>Bernard</first><last>Fradin</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Stéphanie</first><last>Lignon</last></author>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <author><first>Clément</first><last>Plancq</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>167–177</pages>
      <abstract>Dans ce travail, nous étudions en corpus la productivité quantitative des suffixations par -Able et par -ité du français, d’abord indépendamment l’une de l’autre, puis lorsqu’elles s’enchaînent dérivationnellement (la suffixation en -ité s’applique à des bases en -Able dans environ 15 % des cas). Nous estimons la productivité de ces suffixations au moyen de mesures statistiques dont nous suivons l’évolution par rapport à la taille du corpus. Ces deux suffixations sont productives en français moderne : elles forment de nouveaux lexèmes tout au long des corpus étudiés sans qu’on n’observe de saturation, leurs indices de productivité montrent une évolution stable bien qu’étant dépendante des calculs qui leur sont appliqués. On note cependant que, de façon générale, de ces deux suffixations, c’est la suffixation par -ité qui est la plus fréquente en corpus journalistique, sauf précisément quand -ité s’applique à un adjectif en -Able. Étant entendu qu’un adjectif en -Able et le nom en -ité correspondant expriment la même propriété, ce résultat indique que la complexité de la base est un paramètre à prendre en considération dans la formation du lexique possible.</abstract>
      <url hash="acff7c74">2006.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>grabar-etal-2006-productivite</bibkey>
    </paper>
    <paper id="15">
      <title>La coordination considérée comme un entassement paradigmatique : description, représentation et intégration</title>
      <author><first>Marie-Laure</first><last>Guénot</last></author>
      <pages>178–187</pages>
      <abstract>Nous proposons de traiter la coordination comme un entassement paradigmatique, établissant une relation de parataxe entre ses constituants. Par cette considération et ses implications sur la description et l’analyse, on s’éloigne des assomptions les plus fréquentes en linguistique formelle sur le traitement de la coordination. Nous introduisons une description des caractéristiques syntaxiques de cette proposition, ainsi que sa représentation formelle et son intégration au sein d’une grammaire du français qui a pour objet d’être utilisée en traitement automatique. Cette description strictement syntaxique a vocation à être complétée par des informations provenant d’autres domaines, ce qui nous permet d’illustrer quelques spécificités notables de notre modèle.</abstract>
      <url hash="83c515e9">2006.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>guenot-2006-la</bibkey>
    </paper>
    <paper id="16">
      <title>Systèmes question-réponse et <fixed-case>E</fixed-case>uro<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Christine</first><last>Jacquin</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Emmanuel</first><last>Desmontils</last></author>
      <pages>188–197</pages>
      <abstract>Pour améliorer l’efficacité des systèmes de recherche d’informations précises, l’utilisation de connaissances sémantiques est nécessaire. Cependant pour le français, les outils de connaissances sémantiques telles les thesaurus sur domaine ouvert ne sont d’une part pas très nombreux et d’autre part pas suffisamment complets. Dans cet article, nous expliquons premièrement, l’intérêt de l’utilisation de connaissances sémantiques pour un système de question réponse. Puis, nous présentons le thesaurus EuroWordNet, notamment ses limites et les améliorations que nous avons effectuées pour la base française dans un souci de le rendre plus satisfaisant pour notre application par l’ajout de relations inexistantes entre concepts et de définitions par le biais de l’encyclopédieWikipedia (2006).</abstract>
      <url hash="759b3317">2006.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>jacquin-etal-2006-systemes</bibkey>
    </paper>
    <paper id="17">
      <title>Reconnaissance automatique de formes dérivées dans les textes grecs</title>
      <author><first>Tita</first><last>Kyriacopoulou</last></author>
      <author><first>Claude</first><last>Martineau</last></author>
      <author><first>Anastasia</first><last>Yannacopoulou</last></author>
      <pages>198–206</pages>
      <abstract>Notre objectif est la reconnaissance automatique de certaines formes dérivées, i.e. des diminutifs et des augmentatifs des noms et des adjectifs simples, ainsi que des comparatifs et des superlatifs des adjectifs simples du grec moderne. Il s’agit de formes qui sont généralement produites par l’adjonction d’un suffixe à la forme standard correspondante. Nous justifions notre choix de les ajouter dans le dictionnaire électronique. Leur traitement a nécessité une nouvelle représentation du dictionnaire qui utilise désormais un système de règles permettant de générer aisément les formes fléchies dérivées, de les étiqueter en tant que telles, et de les mettre en relation avec leur forme de base. Il en résulte une meilleure structuration des ressources lexicales et une production de dictionnaires flexible.</abstract>
      <url hash="512516be">2006.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>kyriacopoulou-etal-2006-reconnaissance</bibkey>
    </paper>
    <paper id="18">
      <title>Influence de la situation lors de la résolution des anaphores dans le dialogue</title>
      <author><first>Frédéric</first><last>Landragin</last></author>
      <pages>207–216</pages>
      <abstract>La résolution des anaphores dans les systèmes de dialogue homme-machine s’inspire généralement des modèles et des algorithmes développés pour le texte. Or le dialogue met en jeu une situation, c’est-à-dire un environnement physique immédiat et des événements dont la perception est partagée par les interlocuteurs. Cette situation peut servir d’ancrage à des expressions référentielles dites « anaphores à antécédents non linguistiques ». L’attribution de référents à de telles expressions s’avère difficile pour deux raisons : premièrement les facteurs situationnels sont nombreux et peu explicites ; deuxièmement des ambiguïtés peuvent apparaître entre de possibles antécédents situationnels et de possibles antécédents linguistiques. Nous proposons ici un modèle clarifiant l’intervention des facteurs situationnels et permettant leur prise en compte lors de la compréhension des expressions référentielles potentiellement anaphoriques. En intégrant la notion de saillance valable à la fois pour les aspects situationnels et linguistiques, nous montrons comment utiliser des scores numériques pour gérer les interférences entre hypothèses situationnelles et linguistiques.</abstract>
      <url hash="8fa984a2">2006.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>landragin-2006-influence</bibkey>
    </paper>
    <paper id="19">
      <title>De la Chambre des communes à la chambre d’isolement : adaptabilité d’un système de traduction basé sur les segments de phrases</title>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <author><first>Alexandre</first><last>Patry</last></author>
      <pages>217–226</pages>
      <abstract>Nous présentons notre participation à la deuxième campagne d’évaluation de CESTA, un projet EVALDA de l’action Technolangue. Le but de cette campagne consistait à tester l’aptitude des systèmes de traduction à s’adapter rapidement à une tâche spécifique. Nous analysons la fragilité d’un système de traduction probabiliste entraîné sur un corpus hors-domaine et dressons la liste des expériences que nous avons réalisées pour adapter notre système au domaine médical.</abstract>
      <url hash="b9adcc31">2006.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>langlais-etal-2006-de</bibkey>
    </paper>
    <paper id="20">
      <title>L’extraction des réponses dans un système de question-réponse</title>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>227–236</pages>
      <abstract>Les systèmes de question-réponse sont la plupart du temps composés de trois grands modules : l’analyse de la question, la sélection des documents et l’extraction de la réponse. Dans cet article, nous nous intéressons au troisième module, plus particulièrement dans le cas plus délicat où la réponse attendue n’est pas du type entitée nommée. Nous décrivons comment l’analyseur Cass est employé pour marquer la réponse dans les phrases candidates et nous évaluons les résultats de cette approche. Au préalable, nous décrivons et évaluons le module dédié à l’analyse de la question, car les informations qui en sont issues sont nécessaires à notre étape finale d’extraction.</abstract>
      <url hash="34321894">2006.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>ligozat-etal-2006-lextraction</bibkey>
    </paper>
    <paper id="21">
      <title>Les transducteurs à sorties variables</title>
      <author><first>Denis</first><last>Maurel</last></author>
      <author><first>Jan</first><last>Daciuk</last></author>
      <pages>237–245</pages>
      <abstract>Dans le traitement automatique du langage naturel, les dictionnaires électroniques associent à chaque mot de l’information. La représentation informatique la plus efficace de ces dictionnaires utilise des machines à nombre fini d’états (automates ou transducteurs). Dans cet article, nous nous inspirons des algorithmes de construction directe d’un automate déterministe minimal pour proposer une nouvelle forme de transducteur. Cette nouvelle forme permet un calcul rapide des sorties associées aux mots, tout en étant plus compacte quant au nombre de transitions et de sorties distinctes, comme le montrent nos expérimentations.</abstract>
      <url hash="9b051aec">2006.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>maurel-daciuk-2006-les</bibkey>
    </paper>
    <paper id="22">
      <title>Une expérience de sémantique inférentielle</title>
      <author><first>Farid</first><last>Nouioua</last></author>
      <author><first>Daniel</first><last>Kayser</last></author>
      <pages>246–256</pages>
      <abstract>Nous développons un système qui doit être capable d’effectuer les mêmes inférences que le lecteur humain d’un constat d’accident de la route, et plus particulièrement de déterminer les causes apparentes de l’accident. Nous décrivons les niveaux linguistiques et sémantiques de l’analyse, et les règles d’inférence utilisées par ce système.</abstract>
      <url hash="5dcdff63">2006.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>nouioua-kayser-2006-une</bibkey>
    </paper>
    <paper id="23">
      <title>Résolution des références aux documents dans un corpus de dialogues humains</title>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <pages>257–266</pages>
      <abstract>Cet article étudie la résolution des références à des entités lorsqu’une représentation informatique de ces entités est disponible. Nous nous intéressons à un corpus de dialogues entre humains, portant sur les grands titres de la presse francophone du jour, et proposons une méthode pour détecter et résoudre les références faites par les locuteurs aux articles des journaux. La détection des expressions nominales qui réfèrent à ces documents est réalisée grâce à une grammaire, alors que le problème de la détection des pronoms qui réfèrent aux documents est abordé par des moyens statistiques. La résolution de ces expressions, à savoir l’attribution des référents, fait quant à elle l’objet d’un algorithme inspiré de la résolution des coréférences. Ces propositions sont évaluées par le biais de mesures quantitatives spécifiques.</abstract>
      <url hash="cdd31ba4">2006.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>popescu-belis-2006-resolution</bibkey>
    </paper>
    <paper id="24">
      <title>Mise au jour semi-automatique de nuances sémantiques entre mots de sens proches</title>
      <author><first>Mathias</first><last>Rossignol</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>267–276</pages>
      <abstract>L’acquisition automatique sur corpus d’informations lexicales sémantiques donne une place importante à la constitution de classes sémantiques rassemblant des mots de sens proches. Or, l’intérêt pratique de celles-ci reste limité en l’absence d’information sur les distinctions individualisant les sens des mots qu’elles rassemblent. Nous présentons dans cet article un premier système permettant de mettre au jour, de manière semi-automatique et à partir des seules données textuelles rassemblées dans un corpus, des éléments de distinction sémantique fine entre mots appartenant à une même classe, atteignant ainsi un degré de définition du sens encore inédit en acquisition automatique d’informations sémantiques lexicales. La technique mise au point regroupe, en s’appuyant sur l’étude de grands voisinages autour des occurrences des mots comparés, des paires de mots distingués par des nuances similaires. Cette approche présente la faiblesse de ne permettre qu’une représentation implicite des nuances découvertes : les listes de paires de mots rapprochées doivent être interprétées afin de « comprendre » l’élément de distinction commun. En revanche, elle permet une automatisation importante du processus de recherche de nuances, suffisante pour assurer que le travail humain de validation des résultats n’introduise dans ceux-ci de biais interprétatif trop important.</abstract>
      <url hash="db976dd5">2006.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>rossignol-sebillot-2006-mise</bibkey>
    </paper>
    <paper id="25">
      <title>Groupes Nominaux Prédicatifs : utilisation d’une grammaire de liens pour l’extraction d’information</title>
      <author><first>Jean</first><last>Royauté</last></author>
      <author><first>Élisabeth</first><last>Godbert</last></author>
      <author><first>Mohamed</first><last>Madhi Malik</last></author>
      <pages>277–287</pages>
      <abstract>L’identification des structures prédicatives présente un grand intérêt quand on se situe dans une problématique d’extraction d’information. Si une littérature abondante existe à ce sujet, particulièrement dans le domaine de la génomique, la plupart des travaux portent sur les relations autour du verbe. Peu s’intéressent à la relation qui peut unir une nominalisation et ses actants dans un groupe nominal à tête prédicative (GNP). Nous montrons la complexité des différents types de GNP et des relations paraphrastiques qui les unissent avec les formes verbales, afin de donner une vue unifiée des structures prédicatives nomino-verbales. Nous montrons ensuite comment nous avons conçu une grammaire de liens permettant l’identification de chacun des actants dans les GNP. Nous en décrivons la mise en oeuvre avec le Link Parser, pour l’extraction d’information dans des articles scientifiques du domaine de la Biologie.</abstract>
      <url hash="ebf4209a">2006.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>royaute-etal-2006-groupes</bibkey>
    </paper>
    <paper id="26">
      <title>Trouver le coupable : Fouille d’erreurs sur des sorties d’analyseurs syntaxiques</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>288–297</pages>
      <abstract>Nous présentons une méthode de fouille d’erreurs pour détecter automatiquement des erreurs dans les ressources utilisées par les systèmes d’analyse syntaxique. Nous avons mis en oeuvre cette méthode sur le résultat de l’analyse de plusieurs millions de mots par deux systèmes d’analyse différents qui ont toutefois en commun le lexique syntaxique et la chaîne de traitement pré-syntaxique. Nous avons pu identifier ainsi des inexactitudes et des incomplétudes dans les ressources utilisées. En particulier, la comparaison des résultats obtenus sur les sorties des deux analyseurs sur un même corpus nous a permis d’isoler les problèmes issus des ressources partagées de ceux issus des grammaires.</abstract>
      <url hash="01eba9ec">2006.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>sagot-villemonte-de-la-clergerie-2006-trouver</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>V</fixed-case>1<fixed-case>Ω</fixed-case> a=able ou Normaliser des lexiques syntaxiques est délectable</title>
      <author><first>Susanne</first><last>Salmon-Alt</last></author>
      <pages>298–307</pages>
      <abstract>Partant des lexiques TAL syntaxiques existants, cet article propose une représentation lexicale unifiée et normalisée, préalable et nécessaire à toute exploitation des lexiques syntaxiques hors de leur propre contexte de conception. Ce travail s’inscrit dans un cadre de modélisation privilégié − le Lexical Markup Framework − qui a été conçu dès le départ comme un modèle lexicographique intégrant les différents niveaux de description. Ce modèle permet d’articuler des descriptions extensionnelles et intensionnelles et fait référence à un jeu de descripteurs normalisés, garantissant la rigueur de la description des faits linguistiques et assurant, à terme, la compatibilité avec des formats de données utilisés pour l’annotation de corpus.</abstract>
      <url hash="d4a9553a">2006.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>salmon-alt-2006-v1o</bibkey>
    </paper>
    <paper id="28">
      <title>Identifying Genres of Web Pages</title>
      <author><first>Marina</first><last>Santini</last></author>
      <pages>308–317</pages>
      <abstract>In this paper, we present an inferential model for text type and genre identification of Web pages, where text types are inferred using a modified form of Bayes’ theorem, and genres are derived using a few simple if-then rules. As the genre system on the Web is a complex phenomenon, and Web pages are usually more unpredictable and individualized than paper documents, we propose this approach as an alternative to unsupervised and supervised techniques. The inferential model allows a classification that can accommodate genres that are not entirely standardized, and is more capable of reading a Web page, which is mixed, rarely corresponding to an ideal type and often showing a mixture of genres or no genre at all. A proper evaluation of such a model remains an open issue.</abstract>
      <url hash="85ff661f">2006.jeptalnrecital-long.28</url>
      <bibkey>santini-2006-identifying</bibkey>
    </paper>
    <paper id="29">
      <title>Using Stemming in Morphological Analysis to Improve <fixed-case>A</fixed-case>rabic Information Retrieval</title>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>Meriama</first><last>Laib</last></author>
      <author><first>Christian</first><last>Fluhr</last></author>
      <pages>318–327</pages>
      <abstract>Information retrieval (IR) consists in finding all relevant documents for a user query in a collection of documents. These documents are ordered by the probability of being relevant to the user’s query. The highest ranked document is considered to be the most likely relevant document. Natural Language Processing (NLP) for IR aims to transform the potentially ambiguous words of queries and documents into unambiguous internal representations on which matching and retrieval can take place. This transformation is generally achieved by several levels of linguistic analysis, morphological, syntactic and so forth. In this paper, we present the Arabic linguistic analyzer used in the LIC2M cross-lingual search engine. We focus on the morphological analyzer and particularly the clitic stemmer which segments the input words into proclitics, simple forms and enclitics. We demonstrate that stemming improves search engine recall and precision.</abstract>
      <url hash="3c85c3eb">2006.jeptalnrecital-long.29</url>
      <bibkey>semmar-etal-2006-using</bibkey>
    </paper>
    <paper id="30">
      <title>Décodage conceptuel et apprentissage automatique : application au corpus de dialogue Homme-Machine <fixed-case>MEDIA</fixed-case></title>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <pages>328–337</pages>
      <abstract>Cette étude présente les travaux du LIA effectués sur le corpus de dialogue homme-machine MEDIA et visant à proposer des méthodes d’analyse robuste permettant d’extraire d’un message audio une séquence de concepts élémentaires. Le modèle de décodage conceptuel présenté est basé sur une approche stochastique qui intègre directement le processus de compréhension au processus de Reconnaissance Automatique de la Parole (RAP). Cette approche permet de garder l’espace probabiliste des phrases produit en sortie du module de RAP et de le projeter vers un espace probabiliste de séquences de concepts. Les expériences menées sur le corpus MEDIA montrent que les performances atteintes par notre modèle sont au niveau des meilleurs systèmes ayant participé à l’évaluation sur des transcriptions manuelles de dialogues. En détaillant les performances du système en fonction de la taille du corpus d’apprentissage on peut mesurer le nombre minimal ainsi que le nombre optimal de dialogues nécessaires à l’apprentissage des modèles. Enfin nous montrons comment des connaissances a priori peuvent être intégrées dans nos modèles afin d’augmenter significativement leur couverture en diminuant, à performance égale, l’effort de constitution et d’annotation du corpus d’apprentissage.</abstract>
      <url hash="8fd2ecab">2006.jeptalnrecital-long.30</url>
      <language>fra</language>
      <bibkey>servan-bechet-2006-decodage</bibkey>
    </paper>
    <paper id="31">
      <title>Vers une prédiction automatique de la difficulté d’une question en langue naturelle</title>
      <author><first>Laurianne</first><last>Sitbon</last></author>
      <author><first>Jens</first><last>Grivolla</last></author>
      <author><first>Laurent</first><last>Gillard</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>338–347</pages>
      <abstract>Nous proposons et testons deux méthodes de prédiction de la capacité d’un système à répondre à une question factuelle. Une telle prédiciton permet de déterminer si l’on doit initier un dialogue afin de préciser ou de reformuler la question posée par l’utilisateur. La première approche que nous proposons est une adaptation d’une méthode de prédiction dans le domaine de la recherche documentaire, basée soit sur des machines à vecteurs supports (SVM) soit sur des arbres de décision, avec des critères tels que le contenu des questions ou des documents, et des mesures de cohésion entre les documents ou passages de documents d’où sont extraits les réponses. L’autre approche vise à utiliser le type de réponse attendue pour décider de la capacité du système à répondre. Les deux approches ont été testées sur les données de la campagne Technolangue EQUER des systèmes de questions-réponses en français. L’approche à base de SVM est celle qui obtient les meilleurs résultats. Elle permet de distinguer au mieux les questions faciles, celles auxquelles notre système apporte une bonne réponse, des questions difficiles, celles restées sans réponses ou auxquelles le système a répondu de manière incorrecte. A l’opposé on montre que pour notre système, le type de réponse attendue (personnes, quantités, lieux...) n’est pas un facteur déterminant pour la difficulté d’une question.</abstract>
      <url hash="8cdeaae0">2006.jeptalnrecital-long.31</url>
      <language>fra</language>
      <bibkey>sitbon-etal-2006-vers</bibkey>
    </paper>
    <paper id="32">
      <title>Variation terminologique et analyse diachronique</title>
      <author><first>Annie</first><last>Tartier</last></author>
      <pages>348–357</pages>
      <abstract>Cet article présente un travail destiné à automatiser l’étude de l’évolution terminologique à partir de termes datés extraits de corpus diachroniques de textes scientifiques ou techniques. Les apparitions et disparitions d’attestations de termes au cours du temps constituent la manifestation la plus simple de l’évolution. Mais la prise en compte des formes variantes apporte une information de meilleure qualité sur le suivi des termes. Une distance entre termes complexes permet de rendre opérationnelle l’intégration de la variation terminologique à l’analyse diachronique. Des résultats montrant la prise en compte des variantes sont présentés et commentés à la fin de l’article.</abstract>
      <url hash="f3c946b4">2006.jeptalnrecital-long.32</url>
      <language>fra</language>
      <bibkey>tartier-2006-variation</bibkey>
    </paper>
    <paper id="33">
      <title>Éléments pour la génération de classes sémantiques à partir de définitions lexicographiques Pour une approche sémique du sens</title>
      <author><first>Mathieu</first><last>Valette</last></author>
      <author><first>Alexander</first><last>Estacio-Moreno</last></author>
      <author><first>Étienne</first><last>Petitjean</last></author>
      <author><first>Évelyne</first><last>Jacquey</last></author>
      <pages>358–367</pages>
      <abstract>Ce papier expose une expérience de classification menée sur un corpus de définitions dictionnairiques. Le cadre général de cette recherche est la constitution d’une ressource lexico-sémantique fondée sur une conception structuraliste du sens (le contenu sémantique d’une unité lexicale est structuré en sèmes ; le sens d’un texte émerge de faisceaux de regroupements sémiques stabilisés). L’objectif de l’expérience rapportée est de découvrir des classes sémantiques à partir de définitions dictionnairiques avec la méthode CAH. Les classes sémantiques regroupent des unités lexicales en fonction de sèmes génériques (i.e. communs à toutes les unités lexicales de la classe) et s’organisent différentiellement en fonction de sèmes spécifiques. À partir d’une sélection d’entrées dictionnairiques partageant le sème générique /arbre/, nous étudions la distribution et l’organisation d’une hypothétique classe sémantique liée au domaine de la sylviculture.</abstract>
      <url hash="902a9c27">2006.jeptalnrecital-long.33</url>
      <language>fra</language>
      <bibkey>valette-etal-2006-elements</bibkey>
    </paper>
    <paper id="34">
      <title>Analyse par contraintes de l’organisation du discours</title>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>368–377</pages>
      <abstract>Nous abordons ici la question de l’analyse de la structure du discours, du point de vue de sa description formelle et de son traitement automatique. Nous envisageons l’hypothèse selon laquelle une approche par contraintes pourrait permettre la prise en charge de structures discursives variées d’une part, et de différents types d’indices de leur manifestation d’autre part. Le formalisme CDML que nous introduisons vise précisément une telle approche.</abstract>
      <url hash="0b22cd27">2006.jeptalnrecital-long.34</url>
      <language>fra</language>
      <bibkey>widlocher-2006-analyse</bibkey>
    </paper>
  </volume>
  <volume id="poster" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 13ème conférence sur le Traitement Automatique des Langues Naturelles. Posters</booktitle>
      <editor><first>Piet</first><last>Mertens</last></editor>
      <editor><first>Cédrick</first><last>Fairon</last></editor>
      <editor><first>Anne</first><last>Dister</last></editor>
      <editor><first>Patrick</first><last>Watrin</last></editor>
      <publisher>ATALA</publisher>
      <address>Leuven, Belgique</address>
      <month>April</month>
      <year>2006</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="cf454c60">2006.jeptalnrecital-poster.0</url>
      <bibkey>jep-taln-recital-2006-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Étude et analyse de la phrase nominale arabe en <fixed-case>HPSG</fixed-case></title>
      <author><first>Abdelkarim</first><last>Abdelkader</last></author>
      <author><first>Kais</first><last>Haddar</last></author>
      <author><first>Abdelmajid</first><last>Ben Hamadou</last></author>
      <pages>379–388</pages>
      <abstract>Dans cet article, nous proposons une démarche d’analyse syntaxique pour les phrases nominales arabes à l’aide du formalisme des grammaires syntagmatiques guidées par les têtes HPSG. Pour ce faire, nous commençons par étudier la typologie de la phrase nominale arabe en précisant ses différentes formes. Puis, nous élaborons une grammaire HPSG traitant ce type de phrase et qui respecte la spécificité de la langue arabe. Ensuite, nous présentons une démarche d’analyse syntaxique se basant sur une approche ascendante et sur le mécanisme d’unification. Enfin, nous donnons une idée sur l’implémentation et l’expérimentation du prototype réalisé.</abstract>
      <url hash="b9210013">2006.jeptalnrecital-poster.1</url>
      <language>fra</language>
      <bibkey>abdelkader-etal-2006-etude</bibkey>
    </paper>
    <paper id="2">
      <title>Corpus-based extraction and identification of <fixed-case>P</fixed-case>ortuguese Multiword Expressions</title>
      <author><first>Sandra</first><last>Antunes</last></author>
      <author><first>Maria</first><last>Fernanda Bacelar do Nascimento</last></author>
      <author><first>João</first><last>Miguel Casteleiro</last></author>
      <author><first>Amália</first><last>Mendes</last></author>
      <author><first>Luísa</first><last>Pereira</last></author>
      <author><first>Tiago</first><last>Sá</last></author>
      <pages>389–397</pages>
      <abstract>This presentation reports on an on-going project aimed at building a large lexical database of corpus-extracted multiword (MW) expressions for the Portuguese language. MW expressions were automatically extracted from a balanced 50 million word corpus compiled for this project, furthermore these were statistically interpreted using lexical association measures, followed by a manual validation process. The lexical database covers different types of MW expressions, from named entities to lexical associations with different degrees of cohesion, ranging from totally frozen idioms to favoured co-occurring forms, such as collocations. We aim to achieve two main objectives with this resource. Firstly to build on the large set of data of different types of MW expressions, thus revising existing typologies of collocations and integrating them in a larger theory of MW units. Secondly, to use the extensive hand-checked data as training data to evaluate existing statistical lexical association measures.</abstract>
      <url hash="656a0969">2006.jeptalnrecital-poster.2</url>
      <bibkey>antunes-etal-2006-corpus</bibkey>
    </paper>
    <paper id="3">
      <title>Ambiguous Turn-Taking Games in Conversations</title>
      <author><first>Gemma</first><last>Bel-Enguix</last></author>
      <author><first>Maria Dolores</first><last>Jiménez-López</last></author>
      <pages>398–406</pages>
      <abstract>Human-computer interfaces require models of dialogue structure that capture the variability and unpredictability within dialogue. Semantic and pragmatic context are continuously evolving during conversation, especially by the distribution of turns that have a direct effect in dialogue exchanges. In this paper we use a formal language paradigm for modelling multi-agent system conversations. Our computational model combines pragmatic minimal units –speech acts– for constructing dialogues. In this framework, we show how turn-taking distribution can be ambiguous and propose an algorithm for solving it, considering turn coherence, trajectories and turn pairing. Finally, we suggest overlapping as one of the possible phenomena emerging from an unresolved turn-taking.</abstract>
      <url hash="1a23586a">2006.jeptalnrecital-poster.3</url>
      <bibkey>bel-enguix-dolores-jimenez-lopez-2006-ambiguous</bibkey>
    </paper>
    <paper id="4">
      <title>Comment évaluer les algorithmes de segmentation automatique ? Essai de construction d’un matériel de référence.</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <author><first>Sophie</first><last>Piérard</last></author>
      <pages>407–414</pages>
      <abstract>L’objectif de cette recherche est d’évaluer l’efficacité d’algorithmes lors de l’identification des ruptures thématiques dans des textes. Pour ce faire, 32 articles de journaux ont été segmentés par des groupes de 15 juges. L’analyse de leurs réponses indique que chaque juge, pris individuellement, est peu fiable contrairement à l’indice global de segmentation, qui peut être dérivé des réponses de l’ensemble des juges. Si les deux algorithmes testés sont capables de retrouver le début des articles lorsque ceux-ci sont concaténés, ils échouent dans la détection des changements de thème perçus par la majorité des juges. Il faut toutefois noter que les juges, pris individuellement, sont eux-mêmes inefficaces dans l’identification des changements de thème. Dans la conclusion, nous évaluons différentes explications du faible niveau de performance observé.</abstract>
      <url hash="1103dd20">2006.jeptalnrecital-poster.4</url>
      <language>fra</language>
      <bibkey>bestgen-pierard-2006-comment</bibkey>
    </paper>
    <paper id="5">
      <title>Mécanismes de contrôle pour l’analyse en Grammaires de Propriétés</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Stéphane</first><last>Rauzy</last></author>
      <pages>415–424</pages>
      <abstract>Les méthodes d’analyse syntaxiques hybrides, reposant à la fois sur des techniques statistiques et symboliques, restent peu exploitées. Dans la plupart des cas, les informations statistiques sont intégrées à un squelette contextfree et sont utilisées pour contrôler le choix des règles ou des structures. Nous proposons dans cet article une méthode permettant de calculer un indice de corrélation entre deux objets linguistiques (catégories, propriétés). Nous décrivons une utilisation de cette notion dans le cadre de l’analyse des Grammaires de Propriétés. L’indice de corrélation nous permet dans ce cas de contrôler à la fois la sélection des constituants d’une catégorie, mais également la satisfaction des propriétés qui la décrivent.</abstract>
      <url hash="779e362d">2006.jeptalnrecital-poster.5</url>
      <language>fra</language>
      <bibkey>blache-rauzy-2006-mecanismes</bibkey>
    </paper>
    <paper id="6">
      <title>Exploration et utilisation d’informations distantes dans les modèles de langage statistiques</title>
      <author><first>Armelle</first><last>Brun</last></author>
      <author><first>David</first><last>Langlois</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <pages>425–434</pages>
      <abstract>Dans le cadre de la modélisation statistique du langage, nous montrons qu’il est possible d’utiliser un modèle n-grammes avec un historique qui n’est pas nécessairement celui avec lequel il a été appris. Par exemple, un adverbe présent dans l’historique peut ne pas avoir d’importance pour la prédiction, et devrait donc être ignoré en décalant l’historique utilisé pour la prédiction. Notre étude porte sur les modèles n-grammes classiques et les modèles n-grammes distants et est appliquée au cas des bigrammes. Nous présentons quatre cas d’utilisation pour deux modèles bigrammes : distants et non distants. Nous montrons que la combinaison linéaire dépendante de l’historique de ces quatre cas permet d’améliorer de 14 % la perplexité du modèle bigrammes classique. Par ailleurs, nous nous intéressons à quelques cas de combinaison qui permettent de mettre en valeur les historiques pour lesquels les modèles que nous proposons sont performants.</abstract>
      <url hash="453d8aae">2006.jeptalnrecital-poster.6</url>
      <language>fra</language>
      <bibkey>brun-etal-2006-exploration</bibkey>
    </paper>
    <paper id="7">
      <title>Création d’une base terminologique juridique multilingue à l’aide de la plateforme générique Jibiki : le projet <fixed-case>L</fixed-case>ex<fixed-case>ALP</fixed-case></title>
      <author><first>Francis</first><last>Brunet-Manquat</last></author>
      <author><first>Gilles</first><last>Sérasset</last></author>
      <pages>435–444</pages>
      <abstract>Cet article présente l’utilisation de « Jibiki » (la plateforme de développement du serveur Web Papillon) dans le cadre du projet LexALP1. Le but de ce projet est d’harmoniser la terminologie des quatre langues (français, allemand, italien et slovène) de la Convention Alpine2 de sorte que les états membres puissent coopérer efficacement. Pour cela, le projet utilise la plateforme Jibiki afin de construire une banque terminologique permettant de comparer la terminologie spécialisée de sept systèmes légaux dans quatre langues, et de l’harmoniser, optimisant ainsi la compréhension entre les états alpins sur des questions environnementales au niveau supranational. Dans cet article, nous présentons comment peut être employée la plateforme générique Jibiki afin de gérer un dictionnaire particulier.</abstract>
      <url hash="f709c0ec">2006.jeptalnrecital-poster.7</url>
      <language>fra</language>
      <bibkey>brunet-manquat-serasset-2006-creation</bibkey>
    </paper>
    <paper id="8">
      <title>Traitement des incompréhensions et des malentendus en dialogue homme-machine</title>
      <author><first>Jean</first><last>Caelen</last></author>
      <author><first>Hoá</first><last>Nguyen</last></author>
      <pages>445–454</pages>
      <abstract>Traiter les erreurs en dialogue homme-machine est un problème difficile compte-tenu des multiples sources possibles depuis la reconnaissance de la parole jusqu’à la génération en passant par d’autres modules comme l’analyse sémantique, l’interprétation pragmatique ou la gestion du dialogue. Dans cet article, ce problème est envisagé dans le but d’apporter de la généricité et de la robustesse au système ; il est traité au niveau du contrôleur de dialogue. Les différents types d’erreurs sont d’abord identifiés et regroupés en deux catégories qui seules ont un sens vis-à-vis de l’utilisateur : les incompréhensions et les malentendus. Puis, ces deux catégories d’erreur sont traitées de manière spécifique pour que le système puisse générer une réponse convenable et intelligente à l’utilisateur, sans rupture de dialogue. L’expérimentation effectuée en appliquant cette approche au système de dialogue Mélina présente des résultats prometteurs pour traiter les erreurs en dialogue.</abstract>
      <url hash="bee7282e">2006.jeptalnrecital-poster.8</url>
      <language>fra</language>
      <bibkey>caelen-nguyen-2006-traitement</bibkey>
    </paper>
    <paper id="9">
      <title>Un modèle pour unifier la gestion de ressources linguistiques en contexte multilingue</title>
      <author><first>Frederik</first><last>Cailliau</last></author>
      <pages>455–461</pages>
      <abstract>Le bon fonctionnement d’Intuition, plate-forme de recherche d’information, repose sur le développement et l’intégration d’un grand nombre de ressources linguistiques. Dans un souci de cohérence et de meilleure gestion, l’unification de ressources contenant des connaissances hétérogènes s’impose. Comme Intuition est disponible dans la plupart des langues européennes, cette unification se heurte au facteur multilingue. Pour surmonter les problèmes causés par les différences structurelles entre les langues, une nouvelle architecture linguistique a été conçue et exprimée en UML. Ce méta-modèle est le point de départ pour la nouvelle base de données qui sera le noyau d’un nouvel environnement de travail centré sur son utilisateur, l’expert linguistique. Cet environnement centralisera la gestion de toutes les ressources linguistiques d’Intuition.</abstract>
      <url hash="9932dc67">2006.jeptalnrecital-poster.9</url>
      <language>fra</language>
      <bibkey>cailliau-2006-un</bibkey>
    </paper>
    <paper id="10">
      <title>Reconnaissance de la métrique des poèmes arabes par les réseaux de neurones artificiels</title>
      <author><first>Hafedh</first><last>El Ayech</last></author>
      <author><first>Amine</first><last>Mahfouf</last></author>
      <author><first>Adnane</first><last>Zribi</last></author>
      <pages>462–472</pages>
      <abstract>Nous avons construit un système capable de reconnaître les modes de composition pour les poèmes arabes, nous décrivons dans cet article les différents modules du système. Le recours à une technique d’apprentissage artificiel pour classer une séquence phonétique de syllabes est justifiable par le fait que nous avons imité le processus d’apprentissage naturel humain suivi par les poètes pendant des siècles. Les réseaux de neurones artificiels de type Perceptron multicouches ont montré un pouvoir très puissant de classification.</abstract>
      <url hash="3a219a11">2006.jeptalnrecital-poster.10</url>
      <language>fra</language>
      <bibkey>el-ayech-etal-2006-reconnaissance</bibkey>
    </paper>
    <paper id="11">
      <title>Annotation automatique de relations de contrôle dans des spécifications des besoins informatiques</title>
      <author><first>Jorge</first><last>García-Flores</last></author>
      <author><first>Elena</first><last>Ivanova</last></author>
      <author><first>Jean-Pierre</first><last>Desclés</last></author>
      <author><first>Brahim</first><last>Djioua</last></author>
      <pages>473–482</pages>
      <abstract>La conception de logiciels est un processus technologique complexe, qui nécessite d’être assisté par des outils de traitement automatique des langues. Cet article présente une méthode pour l’annotation de relations discursives de contrôle dans des textes de spécification de besoins informatiques (SBI). La méthode vise à distinguer les actions contrôlées par le système de celles contrôlées par son environnement, ce qui permet d’établir de façon claire les limites et les responsabilités d’un système informatique. Notre méthode fait appel à la sémantique discursive pour analyser les moyens d’expression du contrôle dans un corpus de SBI industrielles ; l’expression du contrôle est identifiable par la présence, dans un certain contexte, de marqueurs linguistiques exprimés par des règles dites d’Exploration Contextuelle. La dernière partie montre le processus d’annotation automatique de la notion de contrôle par le système EXCOM et termine par la présentation d’un début d’évaluation de cette méthodologie.</abstract>
      <url hash="27000f24">2006.jeptalnrecital-poster.11</url>
      <language>fra</language>
      <bibkey>garcia-flores-etal-2006-annotation</bibkey>
    </paper>
    <paper id="12">
      <title>Vers l’intégration du contexte dans une mémoire de traduction sous-phrastique : détection du domaine de traduction</title>
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Claude</first><last>Coulombe</last></author>
      <pages>483–492</pages>
      <abstract>Nous présentons dans cet article une mémoire de traduction sous-phrastique sensible au domaine de traduction, une première étape vers l’intégration du contexte. Ce système est en mesure de recycler les traductions déjà « vues » par la mémoire, non seulement pour des phrases complètes, mais également pour des sous-séquences contiguës de ces phrases, via un aligneur de mots. Les séquences jugées intéressantes sont proposées au traducteur. Nous expliquons également la création d’un utilisateur artificiel, indispensable pour tester les performances du système en l’absence d’intervention humaine. Nous le testons lors de la traduction d’un ensemble disparate de corpus. Ces performances sont exprimées par un ensemble de métriques que nous définissons. Enfin, nous démontrons que la détection automatique du contexte de traduction peut s’avérer bénéfique et prometteuse pour améliorer le fonctionnement d’une telle mémoire, en agissant comme un filtre sur le matériel cible suggéré.</abstract>
      <url hash="c7cf07fd">2006.jeptalnrecital-poster.12</url>
      <language>fra</language>
      <bibkey>gotti-etal-2006-vers</bibkey>
    </paper>
    <paper id="13">
      <title>Analyse et désambiguïsation morphologiques de textes arabes non voyellés</title>
      <author><first>Lamia</first><last>Hadrich Belguith</last></author>
      <author><first>Nouha</first><last>Chaâben</last></author>
      <pages>493–501</pages>
      <abstract>Dans ce papier nous proposons d’abord une méthode d’analyse et de désambiguïsation morphologiques de textes arabes non voyellés permettant de lever l’ambiguïté morphologique due à l’absence des marques de voyelles et aussi à l’irrégularité des formes dérivées de certains mots arabes (e.g. formes irrégulières du pluriel des noms et des adjectifs). Ensuite, nous présentons le système MORPH2, un analyseur morphologique de textes arabes non voyellés basé sur la méthode proposée. Ce système est évalué sur un livre scolaire et des articles de journaux. Les résultats obtenus son et très encourageants. En effet, les mesures de rappel et de précision globales sont respectivement de 69,77 % et 68,51 %.</abstract>
      <url hash="904ed4c2">2006.jeptalnrecital-poster.13</url>
      <language>fra</language>
      <bibkey>hadrich-belguith-chaaben-2006-analyse</bibkey>
    </paper>
    <paper id="14">
      <title>Génération automatique des représentations ontologiques</title>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <pages>502–511</pages>
      <abstract>Depuis la conception du Web sémantique une tâche importante se pose au niveau de traitement automatique du langage : rendre accessible le contenu existant duWeb dit classique aux traitements et raisonnements ontologiques. Comme la plupart du contenu est composé de textes, on a besoin de générer des représentations ontologiques de ces informations textuelles. Dans notre article nous proposons une méthode afin d’automatiser cette traduction en utilisant des ontologies et une analyse syntaxico-sémantique profonde.</abstract>
      <url hash="b0902298">2006.jeptalnrecital-poster.14</url>
      <language>fra</language>
      <bibkey>heinecke-2006-generation</bibkey>
    </paper>
    <paper id="15">
      <title>Approche évolutive des notions de base pour une représentation thématique des connaissances générales</title>
      <author><first>Alain</first><last>Joubert</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>512–521</pages>
      <abstract>Dans le domaine du Traitement Automatique du Langage Naturel, pour élaborer un système de représentation thématique des connaissances générales, des méthodes s’appuyant sur des thésaurus sont utilisées depuis une quinzaine d’années. Un thésaurus est constitué d’un ensemble de concepts qui définissent un système générateur d’un espace vectoriel modélisant les connaissances générales. Ces concepts, souvent organisés en une hiérarchie arborescente, constituent un instrument fondamental, mais totalement figé. Même si les notions évoluent (nous pensons par exemple aux domaines techniques), un thésaurus ne peut quant à lui être modifié que lors d’un processus particulièrement lourd, car nécessitant la collaboration d’experts humains. C’est à ce problème que nous nous attaquons ici. Après avoir détaillé les caractéristiques que doit posséder un système générateur de l’espace vectoriel de modélisation des connaissances, nous définissons les « notions de base ». Celles-ci, dont la construction s’appuie initialement sur les concepts d’un thésaurus, constituent un autre système générateur de cet espace vectoriel. Nous abordons la détermination des acceptions exprimant les notions de base, ce qui nous amène naturellement à nous poser la question de leur nombre. Enfin, nous explicitons comment, s’affranchissant des concepts du thésaurus, ces notions de base évoluent par un processus itératif au fur et à mesure de l’analyse de nouveaux textes.</abstract>
      <url hash="549ce6ce">2006.jeptalnrecital-poster.15</url>
      <language>fra</language>
      <bibkey>joubert-etal-2006-approche</bibkey>
    </paper>
    <paper id="16">
      <title>Relever des critères pour la distinction automatique entre les documents médicaux scientifiques et vulgarisés en russe et en japonais</title>
      <author><first>Sonia</first><last>Krivine</last></author>
      <author><first>Masaru</first><last>Tomimitsu</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Monique</first><last>Slodzian</last></author>
      <pages>522–531</pages>
      <abstract>Dans cet article, nous cherchons à affiner la notion de comparabilité des corpus. Nous étudions en particulier la distinction entre les documents scientifiques et vulgarisés dans le domaine médical. Nous supposons que cette distinction peut apporter des informations importantes, par exemple en recherche d’information. Nous supposons par là même que les documents, étant le reflet de leur contexte de production, fournissent des critères nécessaires à cette distinction. Nous étudions plusieurs critères linguistiques, typographiques, lexicaux et autres pour la caractérisation des documents médicaux scientifiques et vulgarisés. Les résultats présentés sont acquis sur les données en russe et en japonais. Certains des critères étudiés s’avèrent effectivement pertinents. Nous faisons également quelques réflexions et propositions quant à la distinction des catégories scientifique et vulgarisée et aux questionnements théoriques.</abstract>
      <url hash="7137f0bd">2006.jeptalnrecital-poster.16</url>
      <language>fra</language>
      <bibkey>krivine-etal-2006-relever</bibkey>
    </paper>
    <paper id="17">
      <title>Graphes paramétrés et outils de lexicalisation</title>
      <author><first>Éric</first><last>Laporte</last></author>
      <author><first>Sébastien</first><last>Paumier</last></author>
      <pages>532–540</pages>
      <abstract>La lexicalisation des grammaires réduit le nombre des erreurs d’analyse syntaxique et améliore les résultats des applications. Cependant, cette modification affecte un système d’analyse syntaxique dans tous ses aspects. Un de nos objectifs de recherche est de mettre au point un modèle réaliste pour la lexicalisation des grammaires. Nous avons réalisé des expériences en ce sens avec une grammaire très simple par son contenu et son formalisme, et un lexique syntaxique très informatif, le lexique-grammaire du français élaboré au LADL. La méthode de lexicalisation est celle des graphes paramétrés. Nos résultats tendent à montrer que la plupart des informations contenues dans le lexique-grammaire peuvent être transférées dans une grammaire et exploitées avec succès dans l’analyse syntaxique de phrases.</abstract>
      <url hash="7b7f0d7c">2006.jeptalnrecital-poster.17</url>
      <language>fra</language>
      <bibkey>laporte-paumier-2006-graphes</bibkey>
    </paper>
    <paper id="18">
      <title>Traitement des clitiques dans un environnement multilingue</title>
      <author><first>Jorge</first><last>Antonio Leoni de León</last></author>
      <author><first>Athina</first><last>Michou</last></author>
      <pages>541–550</pages>
      <abstract>Cet article décrit le traitement automatique des pronoms clitiques en espagnol et en grec moderne, deux langues de familles distinctes, dans le cadre de l’analyseur syntaxique FIPS multilingue, développé au Laboratoire d’Analyse et de Technologie de Langage (LATL). Nous abordons la distribution des pronoms clitiques, leurs similarités ainsi que leurs particularités par rapport à leur usage général. Ensuite nous présentons la méthode appliquée pour leur traitement, commune aux deux langues. Nous montrons que l’algorithme proposé peut facilement s’étendre à d’autres langues traitées par Fips qui partagent le phénomène de la cliticisation.</abstract>
      <url hash="fcf53f3e">2006.jeptalnrecital-poster.18</url>
      <language>fra</language>
      <bibkey>antonio-leoni-de-leon-michou-2006-traitement</bibkey>
    </paper>
    <paper id="19">
      <title>Détection des propositions syntaxiques du français en vue de l’alignement des propositions de textes parallèles français-japonais</title>
      <author><first>Yayoi</first><last>Nakamura-Delloye</last></author>
      <pages>551–560</pages>
      <abstract>Nous présentons dans cet article SIGLé (Système d’Identification de propositions avec Grammaire Légère), un système réalisant la détection des propositions françaises. Ce système détecte les propositions – à partir de phrases en entrée ségmentées et étiquetées en chunk par un analyseur extérieur –, analyse leurs relations et leur attribue une étiquette indiquant leur nature syntaxique. Il est caractérisé d’une part par sa grammaire de type CFG proposant un ensemble d’étiquettes adaptées à notre analyse pour les mots dits en « qu- », et d’autre part par l’utilisation du formalisme DCG et du langage PROLOG.</abstract>
      <url hash="c14e7939">2006.jeptalnrecital-poster.19</url>
      <language>fra</language>
      <bibkey>nakamura-delloye-2006-detection</bibkey>
    </paper>
    <paper id="20">
      <title>Word Segmentation for <fixed-case>V</fixed-case>ietnamese Text Categorization An <fixed-case>I</fixed-case>nternet-based Statistic and Genetic Algorithm Approach</title>
      <author><first>Hung</first><last>Nguyen Thanh</last></author>
      <author><first>Khanh</first><last>Bui Doan</last></author>
      <pages>561–570</pages>
      <abstract>This paper suggests a novel Vietnamese segmentation approach for text categorization. Instead of using an annotated training corpus or a lexicon which are still lacking in Vietnamese, we use both statistical information extracted directly from a commercial search engine and a genetic algorithm to find the optimal routes to segmentation. The extracted information includes document frequency and n-gram mutual information. Our experiment results obtained on the segmentation and categorization of online news abstracts are very promising. It matches near 80 % human judgment on segmentation and over 90 % micro-averaging F1 in categorization. The processing time is less than one second per document when statistical information is cached.</abstract>
      <url hash="45d80dd6">2006.jeptalnrecital-poster.20</url>
      <bibkey>nguyen-thanh-bui-doan-2006-word</bibkey>
    </paper>
    <paper id="21">
      <title>Extraction de grammaires <fixed-case>TAG</fixed-case> lexicalisées avec traits à partir d’un corpus arboré pour le coréen</title>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>571–579</pages>
      <abstract>Nous présentons, ici, une implémentation d’un système qui n’extrait pas seulement une grammaire lexicalisée (LTAG), mais aussi une grammaire LTAG avec traits (FB-LTAG) à partir d’un corpus arboré. Nous montrons les expérimentations pratiques où nous extrayons les grammaires TAG à partir du Sejong Treebank pour le coréen. Avant tout, les 57 étiquettes syntaxiques et les analyses morphologiques dans le corpus SJTree nous permettent d’extraire les traits syntaxiques automatiquement. De plus, nous modifions le corpus pour l’extraction d’une grammaire lexicalisée et convertissons les grammaires lexicalisées en schémas d’arbre pour résoudre le problème de la couverture lexicale limitée des grammaires lexicalisées extraites.</abstract>
      <url hash="e954112d">2006.jeptalnrecital-poster.21</url>
      <language>fra</language>
      <bibkey>park-2006-extraction-de</bibkey>
    </paper>
    <paper id="22">
      <title>Étude de métaphores conceptuelles à l’aide de vues globales et temporelles sur un corpus</title>
      <author><first>Thibault</first><last>Roy</last></author>
      <author><first>Stéphane</first><last>Ferrari</last></author>
      <author><first>Pierre</first><last>Beust</last></author>
      <pages>580–589</pages>
      <abstract>Cet article présente des expériences récentes menées dans le cadre d’un projet de recherche consacré à l’étude de métaphores conceptuelles. Ces expériences consistent à appréhender visuellement la répartition de trois domaines pouvant être à l’origine de métaphores conceptuelles dans un corpus d’articles boursiers. Les trois domaines étudiés sont la météorologie, la guerre et la santé, un grand nombre d’emplois métaphoriques du lexique de ces trois domaines ayant été observés dans le corpus d’étude. Afin de visualiser la répartition de ces domaines en corpus, nous exploitons la plate-forme ProxiDocs dédiée à la cartographie et à la catégorisation de corpus. Les cartes construites à partir du corpus et des domaines d’étude nous ont ainsi permis de localiser certaines métaphores conceptuelles dans des articles et des groupes d’articles du corpus. Des articles contenant des emplois non métaphoriques des domaines étudiés ont également été distingués sur les cartes. Des représentations cartographiques du corpus mettant dynamiquement en évidence l’évolution des trois domaines d’étude au fil du temps nous ont permis d’amorcer une étude sur le lien entre la présence de certaines métaphores conceptuelles et des faits d’actualité.</abstract>
      <url hash="dbf0220a">2006.jeptalnrecital-poster.22</url>
      <language>fra</language>
      <bibkey>roy-etal-2006-etude</bibkey>
    </paper>
    <paper id="23">
      <title>Système de traduction automatique statistique combinant différentes ressources</title>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Roland</first><last>Kuhn</last></author>
      <pages>590–599</pages>
      <abstract>Cet article décrit une approche combinant différents modèles statistiques pour la traduction automatique basée sur les segments. Pour ce faire, différentes ressources sont utilisées, dont deux corpus parallèles aux caractéristiques différentes et un dictionnaire de terminologie bilingue et ce, afin d’améliorer la performance quantitative et qualitative du système de traduction. Nous évaluons notre approche sur la paire de langues français-anglais et montrons comment la combinaison des ressources proposées améliore de façon significative les résultats.</abstract>
      <url hash="ec2d07fe">2006.jeptalnrecital-poster.23</url>
      <language>fra</language>
      <bibkey>sadat-etal-2006-systeme</bibkey>
    </paper>
    <paper id="24">
      <title>Computer Tools for the Management of Lexicon-Grammar Databases</title>
      <author><first>Javier</first><last>M. Sastre Martínez</last></author>
      <pages>600–608</pages>
      <abstract>Lexicon grammar is a systematic method for the analysis and the representation of the elementary sentence structures of a natural language producing large collections of syntactic electronic dictionaries or lexicongrammar tables (LGTs). In order to describe a language, very long term collaborative work is required. However, the current computer tools for the management of LGTs do not fulfill key requirements including automatic integration of multisource data, data coherence and version control, filtering and sorting, exchange formats, coupled management of data and documentation, dedicated graphical interfaces (GUIs) and user management and access control. In this paper we propose a solution based on PostgreSQL and/or MySQL (open source database management systems), Swing (a GUI toolkit for Java), JDBC (the API for Java database connectivity) and StAX (an API for the analysis and generation of XML documents).</abstract>
      <url hash="ac45d5fa">2006.jeptalnrecital-poster.24</url>
      <bibkey>m-sastre-martinez-2006-computer</bibkey>
    </paper>
    <paper id="25">
      <title>Modélisation et analyse des coordinations elliptiques par l’exploitation dynamique des forêts de dérivation</title>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>609–618</pages>
      <abstract>Nous présentons dans cet article une approche générale pour la modélisation et l’analyse syntaxique des coordinations elliptiques. Nous montrons que les lexèmes élidés peuvent être remplacés, au cours de l’analyse, par des informations qui proviennent de l’autre membre de la coordination, utilisé comme guide au niveau des dérivations. De plus, nous montrons comment cette approche peut être effectivement mise en oeuvre par une légère extension des Grammaires d’Arbres Adjoints Lexicalisées (LTAG) à travers une opération dite de fusion. Nous décrivons les algorithmes de dérivation nécessaires pour l’analyse de constructions coordonnées pouvant comporter un nombre quelconque d’ellipses.</abstract>
      <url hash="c264411d">2006.jeptalnrecital-poster.25</url>
      <language>fra</language>
      <bibkey>seddah-sagot-2006-modelisation</bibkey>
    </paper>
    <paper id="26">
      <title>Analyse spectrale des textes : détection automatique des frontières de langue et de discours</title>
      <author><first>Pascal</first><last>Vaillant</last></author>
      <author><first>Richard</first><last>Nock</last></author>
      <author><first>Claudia</first><last>Henry</last></author>
      <pages>619–629</pages>
      <abstract>Nous proposons un cadre théorique qui permet, à partir de matrices construites sur la base des données statistiques d’un corpus, d’extraire par des procédés mathématiques simples des informations sur les mots du vocabulaire de ce corpus, et sur la syntaxe des langues qui l’ont engendré. À partir des mêmes données initiales, on peut construire une matrice de similarité syntagmatique (probabilités de transition d’un mot à un autre), ou une matrice de similarité paradigmatique (probabilité de partager des contextes identiques). Pour ce qui concerne la première de ces deux possibilités, les résultats obtenus sont interprétés dans le cadre d’une modélisation du processus génératif par chaînes de Markov. Nous montrons que les résultats d’une analyse spectrale de la matrice de transition peuvent être interprétés comme des probabilités d’appartenance de mots à des classes. Cette méthode nous permet d’obtenir une classification continue des mots du vocabulaire dans des sous-systèmes génératifs contribuant à la génération de textes composites. Une application pratique est la segmentation de textes hétérogènes en segments homogènes d’un point de vue linguistique, notamment dans le cas de langues proches par le degré de recouvrement de leurs vocabulaires.</abstract>
      <url hash="b0349f39">2006.jeptalnrecital-poster.26</url>
      <language>fra</language>
      <bibkey>vaillant-etal-2006-analyse</bibkey>
    </paper>
    <paper id="27">
      <title>Adaptation de modèles de langage à l’utilisateur et au registre de langage : expérimentations dans le domaine de l’aide au handicap</title>
      <author><first>Tonio</first><last>Wandmacher</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <pages>630–639</pages>
      <abstract>Les modèles markoviens de langage sont très dépendants des données d’entraînement sur lesquels ils sont appris. Cette dépendance, qui rend difficile l’interprétation des performances, a surtout un fort impact sur l’adaptation à chaque utilisateur de ces modèles. Cette question a déjà été largement étudiée par le passé. En nous appuyant sur un domaine d’application spécifique (prédiction de texte pour l’aide à la communication pour personnes handicapées), nous voudrions l’étendre à la problématique de l’influence du registre de langage. En considérant des corpus relevant de cinq genres différents, nous avons étudié la réduction de cette influence par trois modèles adaptatifs différents : (a) un modèle cache classique favorisant les n derniers mots rencontrés, (b) l’intégration au modèle d’un dictionnaire dynamique de l’utilisateur et enfin (c) un modèle de langage interpolé combinant un modèle général et un modèle utilisateur mis à jour dynamiquement au fil des saisies. Cette évaluation porte un système de prédiction de texte basé sur un modèle trigramme.</abstract>
      <url hash="c5d15825">2006.jeptalnrecital-poster.27</url>
      <language>fra</language>
      <bibkey>wandmacher-antoine-2006-adaptation</bibkey>
    </paper>
    <paper id="28">
      <title>L’influence du contexte sur la compréhension de la parole arabe spontanée</title>
      <author><first>Anis</first><last>Zouaghi</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <author><first>Mohamed</first><last>Ben Ahmed</last></author>
      <pages>640–648</pages>
      <abstract>Notre travail s’intègre dans le cadre du projet intitulé « Oréodule » : un système de reconnaissance, de traduction et de synthèse de la langue arabe. L’objectif de cet article est d’essayer d’améliorer le modèle probabiliste sur lequel est basé notre décodeur sémantique de la parole arabe spontanée. Pour atteindre cet objectif, nous avons décidé de tester l’influence de l’utilisation du contexte pertinent, et de l’intégration de différents types de données contextuelles sur la performance du décodeur sémantique employé. Les résultats sont satisfaisants.</abstract>
      <url hash="815b7531">2006.jeptalnrecital-poster.28</url>
      <language>fra</language>
      <bibkey>zouaghi-etal-2006-linfluence</bibkey>
    </paper>
  </volume>
  <volume id="tutoriel" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 13ème conférence sur le Traitement Automatique des Langues Naturelles. Tutoriels</booktitle>
      <editor><first>Piet</first><last>Mertens</last></editor>
      <editor><first>Cédrick</first><last>Fairon</last></editor>
      <editor><first>Anne</first><last>Dister</last></editor>
      <editor><first>Patrick</first><last>Watrin</last></editor>
      <publisher>ATALA</publisher>
      <address>Leuven, Belgique</address>
      <month>April</month>
      <year>2006</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="e514b95f">2006.jeptalnrecital-tutoriel.0</url>
      <bibkey>jep-taln-recital-2006-actes-de-la-13eme</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analogie en traitement automatique des langues. Application à la traduction automatique</title>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>781–791</pages>
      <abstract>On se place ici dans la tendance actuelle en traitement automatique des langues, celle à base de corpus et aussi dans une perspective que l’on peut qualifier d’approche à moindre effort : il s’agit d’examiner les limites des possibilités de traitement à partir de données textuelles brutes, c’est-à-dire non pré-traitées. L’interrogation théorique présente en arrière-plan est la suivante : quelles sont les opérations fondamentales en langue ? L’analogie proportionnelle a été mentionnée par de nombreux grammairiens et linguistes. On se propose de montrer l’efficacité d’une telle opération en la testant sur une tâche dure du traitement automatique des langues : la traduction automatique. On montrera aussi les bonnes conséquences de la formalisation d’une telle opération avec des résultats théoriques en théorie des langages en relation avec leur adéquation à la description des langues. De cette façon, une opération fondamentale en langue, l’analogie proportionnelle, se verra illustrée tant par ses aspects théoriques que par ses performances en pratique.</abstract>
      <url hash="9266aad9">2006.jeptalnrecital-tutoriel.1</url>
      <language>fra</language>
      <bibkey>lepage-2006-analogie</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 13ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Piet</first><last>Mertens</last></editor>
      <editor><first>Cédrick</first><last>Fairon</last></editor>
      <editor><first>Anne</first><last>Dister</last></editor>
      <editor><first>Patrick</first><last>Watrin</last></editor>
      <publisher>ATALA</publisher>
      <address>Leuven, Belgique</address>
      <month>April</month>
      <year>2006</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="4a20e5ab">2006.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2006-actes-de-la-13eme-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Acquisition semi-automatique de collocations à partir de corpus monolingues et multilingues comparables</title>
      <author><first>Vincent</first><last>Archer</last></author>
      <pages>651–660</pages>
      <abstract>Cet article présente une méthode d’acquisition semi-automatique de collocations. Notre extraction monolingue estime pour chaque co-occurrence sa capacité à être une collocation, d’après une mesure statistique modélisant une caractéristique essentielle (le fait qu’une collocation se produit plus souvent que par hasard), effectue ensuite un filtrage automatique (en utilisant les vecteurs conceptuels) pour ne retenir que des collocations d’un certain type sémantique, puis effectue enfin un nouveau filtrage à partir de données entrées manuellement. Notre extraction bilingue est effectuée à partir de corpus comparables, et a pour but d’extraire des collocations qui ne soient pas forcément traductions mot à mot l’une de l’autre. Notre évaluation démontre l’intérêt de mêler extraction automatique et intervention manuelle pour acquérir des collocations et ainsi permettre de compléter les bases lexicales multilingues.</abstract>
      <url hash="02067949">2006.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>archer-2006-acquisition</bibkey>
    </paper>
    <paper id="2">
      <title>Constance et variabilité de l’incomplétude lexicale</title>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <pages>661–669</pages>
      <abstract>Cet article propose, au travers des résultats de différentes expériences sur la couverture des lexiques informatisés, de montrer que l’incomplétude lexicale est un phénomène constant dans tous les lexiques de TAL, mais que les mots inconnus eux-mêmes varient grandement selon les outils. Nous montrons également que la constance de cette incomplétude est étroitement liée à la créativité lexicale de la langue.</abstract>
      <url hash="49b18648">2006.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>cartoni-2006-constance</bibkey>
    </paper>
    <paper id="3">
      <title>Problèmes de représentation de la Langue des Signes Française en vue du traitement automatique</title>
      <author><first>Loïc</first><last>Kervajan</last></author>
      <pages>670–679</pages>
      <abstract>Nous proposons dans cet article une description de la Langue des Signes Française dans le but de traduire des énoncés courts du français et de les faire signer par un personnage de synthèse. Cette description pose en préalable la question de la transcription des éléments d’une langue dont le signal n’est pas linéaire. Il s’agit ensuite de repérer les différentes couches linguistiques et la forme de leurs unités constitutives en vue de la répartition des tâches informatiques : la synthèse de gestes nécessite un traitement des éléments constitutifs du geste et la génération syntaxique doit pouvoir manipuler des morphèmes.</abstract>
      <url hash="6d938d56">2006.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>kervajan-2006-problemes</bibkey>
    </paper>
    <paper id="4">
      <title>L’information biographique : modélisation, extraction et organisation en base de connaissances</title>
      <author><first>Laurent</first><last>Kevers</last></author>
      <pages>680–689</pages>
      <abstract>L’extraction et la valorisation de données biographiques contenues dans les dépêches de presse est un processus complexe. Pour l’appréhender correctement, une définition complète, précise et fonctionnelle de cette information est nécessaire. Or, la difficulté que l’on rencontre lors de l’analyse préalable de la tâche d’extraction réside dans l’absence d’une telle définition. Nous proposons ici des conventions dans le but d’en développer une. Le principal concept utilisé pour son expression est la structuration de l’information sous forme de triplets sujet, relation, objet. Le début de définition ainsi construit est exploité lors de l’étape d’extraction d’informations par transducteurs à états finis. Il permet également de suggérer une solution d’implémentation pour l’organisation des données extraites en base de connaissances.</abstract>
      <url hash="636c3126">2006.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>kevers-2006-linformation</bibkey>
    </paper>
    <paper id="5">
      <title>Repérage de segments d’information évolutive dans des documents de type encyclopédique</title>
      <author><first>Marion</first><last>Laignelet</last></author>
      <pages>690–699</pages>
      <abstract>Dans cet article, nous cherchons à caractériser linguistiquement des segments textuels définis pragmatiquement, relativement à des besoins de réédition de documents et au sein desquels l’information est susceptible d’évoluer dans le temps. Sur la base d’un corpus de textes encyclopédiques en français, nous analysons la distribution de marqueurs textuels et discursifs et leur pertinence en nous focalisant principalement sur un traitement sémantique particulier de la temporalité.</abstract>
      <url hash="df414217">2006.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>laignelet-2006-reperage</bibkey>
    </paper>
    <paper id="6">
      <title>Acquisition automatique de traductions de termes complexes par comparaison de « mondes lexicaux » sur le Web</title>
      <author><first>Stéphanie</first><last>Léon</last></author>
      <pages>700–708</pages>
      <abstract>Nous présentons une méthode de traduction automatique de termes complexes pour la construction de ressources bilingues français/anglais, basée principalement sur une comparaison entre « mondes lexicaux » (ensemble de co-occurrents), à partir du Web. Nous construisons les mondes lexicaux des termes français sur le Web. Puis, nous générons leurs traductions candidates via un dictionnaire bilingue électronique et constituons les mondes lexicaux de toutes les traductions candidates. Nous comparons enfin les mondes lexicaux français et anglais afin de valider la traduction adéquate par filtres statistiques. Notre évaluation sur 10 mots français très polysémiques montre que l’exploitation des mondes lexicaux des termes complexes sur le Web permet une acquisition automatique de traductions avec une excellente précision.</abstract>
      <url hash="1a500be7">2006.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>leon-2006-acquisition</bibkey>
    </paper>
    <paper id="7">
      <title>Unsupervised approaches to metonymy recognition</title>
      <author><first>Yves</first><last>Peirsman</last></author>
      <pages>709–718</pages>
      <abstract>To this day, the automatic recognition of metonymies has generally been addressed with supervised approaches. However, these require the annotation of a large number of training instances and hence, hinder the development of a wide-scale metonymy recognition system. This paper investigates if this knowledge acquisition bottleneck in metonymy recognition can be resolved by the application of unsupervised learning. Although the investigated technique, Schütze’s (1998) algorithm, enjoys considerable popularity in Word Sense Disambiguation, I will show that it is not yet robust enough to tackle the specific case of metonymy recognition. In particular, I will study the influence on its performance of four variables—the type of data set, the size of the context window, the application of SVD and the type of feature selection.</abstract>
      <url hash="79cc45b2">2006.jeptalnrecital-recital.7</url>
      <bibkey>peirsman-2006-unsupervised</bibkey>
    </paper>
    <paper id="8">
      <title>Une première approche de l’utilisation des chaînes coréférentielles pour la détection des variantes anaphoriques de termes</title>
      <author><first>Sarah</first><last>Trichet-Allaire</last></author>
      <pages>719–728</pages>
      <abstract>Cet article traite de l’utilité à détecter une chaîne coréférentielle de termes complexes afin d’améliorer la détection de variations de ce même terme complexe. Nous implémentons pour cela un programme permettant de détecter le nombre de variantes anaphoriques d’un terme complexe ainsi que le nombre de variantes anaphoriques de termes dans un texte scientifique. Ces deux fonctionnalités sont développées avec une ancrage dans une chaîne coréférentielle et en dehors de toute chaîne coréférentielle, afin de pouvoir évaluer l’efficacité de cette méthode.</abstract>
      <url hash="e5dd4732">2006.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>trichet-allaire-2006-une</bibkey>
    </paper>
  </volume>
  <volume id="recitalposter" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 13ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues (Posters)</booktitle>
      <editor><first>Piet</first><last>Mertens</last></editor>
      <editor><first>Cédrick</first><last>Fairon</last></editor>
      <editor><first>Anne</first><last>Dister</last></editor>
      <editor><first>Patrick</first><last>Watrin</last></editor>
      <publisher>ATALA</publisher>
      <address>Leuven, Belgique</address>
      <month>April</month>
      <year>2006</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="a06ba9e9">2006.jeptalnrecital-recitalposter.0</url>
      <bibkey>jep-taln-recital-2006-actes-de-la-13eme-sur-le</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Construction automatique d’une interface syntaxe / sémantique utilisant des ressources de large couverture en langue anglaise</title>
      <author><first>François-Régis</first><last>Chaumartin</last></author>
      <pages>729–735</pages>
      <abstract>Nous décrivons ici une approche pour passer d’une représentation syntaxique (issue d’une analyse grammaticale) à une représentation sémantique (sous forme de prédicats). Nous montrons ensuite que la construction de cette interface est automatisable. Nous nous appuyons sur l’interopérabilité de plusieurs ressources couvrant des aspects d’ordre syntaxique (Link Grammar Parser), lexical (WordNet) et syntaxico-sémantique (VerbNet) de la langue anglaise. L’utilisation conjointe de ces ressources de large couverture permet d’obtenir une désambiguïsation syntaxique et lexicale au moins partielle.</abstract>
      <url hash="52bfd1b4">2006.jeptalnrecital-recitalposter.1</url>
      <language>fra</language>
      <bibkey>chaumartin-2006-construction</bibkey>
    </paper>
    <paper id="2">
      <title>Une approche géometrique pour la modélisation des lexiques en langues signées</title>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>736–741</pages>
      <abstract>Le contexte est celui d’une plateforme de génération automatique d’énoncés en langue signée, réalisés par un avatar 3D. Il existe quelques uns de ces systèmes aujourd’hui, par exemple le projet VisiCast (Hanke, 2002). Nous revenons ici sur les systèmes de description utilisés pour les unités gestuelles impliquées dans les énoncés, fondés sur un langage peu flexible et guère adaptatif. Nous proposons ensuite une nouvelle approche, constructiviste et géométrique, avec l’objectif de rendre la description des signes des lexiques signés plus adéquate, et par là améliorer leur intégration dans les discours générés.</abstract>
      <url hash="9b9326d8">2006.jeptalnrecital-recitalposter.2</url>
      <language>fra</language>
      <bibkey>filhol-2006-une</bibkey>
    </paper>
    <paper id="3">
      <title>Une proposition de représentation normalisée des lexiques <fixed-case>HPSG</fixed-case></title>
      <author><first>Noureddine</first><last>Loukil</last></author>
      <pages>742–747</pages>
      <abstract>L’interopérabilité entre les lexiques des grammaires d’unification passe par l’adoption d’une représentation normalisée de ces ressources. Dans ce papier, nous proposons l’utilisation de LMF pour établir la standardisation des ressources lexicales en HPSG. Nous présentons LMF d’une manière sommaire et nous détaillons son utilisation pour coder les entrées lexicales d’un lexique HPSG.</abstract>
      <url hash="15c0eaf2">2006.jeptalnrecital-recitalposter.3</url>
      <language>fra</language>
      <bibkey>loukil-2006-une</bibkey>
    </paper>
    <paper id="4">
      <title>Analyse lexicale et morphologique de l’arabe standard utilisant la plateforme linguistique <fixed-case>N</fixed-case>oo<fixed-case>J</fixed-case></title>
      <author><first>Slim</first><last>Mesfar</last></author>
      <pages>748–754</pages>
      <abstract>Cet article décrit un système de construction du lexique et d’analyse morphologique pour l’arabe standard. Ce système profite des apports des modèles à états finis au sein de l’environnement linguistique de développement NooJ pour traiter aussi bien les textes voyellés que les textes partiellement ou non voyellés. Il se base sur une analyse morphologique faisant appel à des règles grammaticales à large couverture.</abstract>
      <url hash="45eb2633">2006.jeptalnrecital-recitalposter.4</url>
      <language>fra</language>
      <bibkey>mesfar-2006-analyse</bibkey>
    </paper>
    <paper id="5">
      <title>Contrôle dynamique multicritère des résultats d’une chaîne de <fixed-case>TAL</fixed-case></title>
      <author><first>Grégory</first><last>Smits</last></author>
      <pages>755–760</pages>
      <abstract>Le traitement linguistique d’un énoncé écrit conduit le plus souvent à la prise en compte d’interprétations concurrentes, ainsi qu’à la création d’ambiguïtés artificielles. Le contrôle de ces points d’embarras est indispensable pour garantir une efficacité et une précision convenable du processus d’analyse. L’approche décrite dans ce document exploite le paradigme de l’aide multicritère à la décision dans un contexte de TALN. Elle consiste à optimiser l’apport des méthodes spécifiques de contrôle dans une chaîne de traitement.</abstract>
      <url hash="59356ecb">2006.jeptalnrecital-recitalposter.5</url>
      <language>fra</language>
      <bibkey>smits-2006-controle</bibkey>
    </paper>
    <paper id="6">
      <title>Résoudre la coréférence à l’aide d’un classifieur bayésien naïf</title>
      <author><first>Olivier</first><last>Tardif</last></author>
      <pages>761–766</pages>
      <abstract>Nous présentons ici les bases d’une méthode de résolution de la coréférence entre les expressions nominales désignant des entités nommées. Nous comptons appliquer cet algorithme sur un corpus de textes journalistiques ; certains aspects de ce que l’on pourrait nommer les « facteurs de coréférence » dans ces textes nous amènent à favoriser l’utilisation de méthodes statistiques pour accomplir cette tâche. Nous décrivons l’algorithme de résolution de la coréférence mis en oeuvre, constitué d’un classifieur bayésien naïf.</abstract>
      <url hash="e1d17680">2006.jeptalnrecital-recitalposter.6</url>
      <language>fra</language>
      <bibkey>tardif-2006-resoudre</bibkey>
    </paper>
    <paper id="7">
      <title>The Application of Singular Value Decomposition to <fixed-case>D</fixed-case>utch Noun-Adjective Matrices</title>
      <author><first>Tim</first><last>Van de Cruys</last></author>
      <pages>767–772</pages>
      <abstract>Automatic acquisition of semantics from text has received quite some attention in natural language processing. A lot of research has been done by looking at syntactically similar contexts. For example, semantically related nouns can be clustered by looking at the collocating adjectives. There are, however, two major problems with this approach : computational complexity and data sparseness. This paper describes the application of a mathematical technique called singular value decomposition, which has been succesfully applied in Information Retrieval to counter these problems. It is investigated whether this technique is also able to cluster nouns according to latent semantic dimensions in a reduced adjective space.</abstract>
      <url hash="5645832a">2006.jeptalnrecital-recitalposter.7</url>
      <bibkey>van-de-cruys-2006-application</bibkey>
    </paper>
    <paper id="8">
      <title>Calcul du sens des mots arabes ambigus</title>
      <author><first>Anis</first><last>Zouaghi</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <author><first>Mohamed</first><last>Ben Ahmed</last></author>
      <pages>773–778</pages>
      <abstract>Nous présentons dans cet article un analyseur sémantique pour la langue arabe. Cet analyseur contribue à la sélection du sens adéquat parmi l’ensemble des sens possibles que peut recevoir un mot hors contexte. Pour atteindre cet objectif, nous proposons un modèle vectoriel qui permet de lever les ambiguïtés locales au niveau de la phrase et celles relevant du domaine. Ce modèle est inspiré des modèles vectoriels très utilisés dans le domaine de la recherche documentaire.</abstract>
      <url hash="a4534bbf">2006.jeptalnrecital-recitalposter.8</url>
      <language>fra</language>
      <bibkey>zouaghi-etal-2006-calcul</bibkey>
    </paper>
  </volume>
</collection>
