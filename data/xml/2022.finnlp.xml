<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.finnlp">
  <volume id="1" ingest-date="2022-12-13">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Financial Technology and Natural Language Processing (FinNLP)</booktitle>
      <editor><first>Chung-Chi</first><last>Chen</last></editor>
      <editor><first>Hen-Hsen</first><last>Huang</last></editor>
      <editor><first>Hiroya</first><last>Takamura</last></editor>
      <editor><first>Hsin-Hsi</first><last>Chen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates (Hybrid)</address>
      <month>December</month>
      <year>2022</year>
      <url hash="e8128158">2022.finnlp-1</url>
      <venue>finnlp</venue>
    </meta>
    <frontmatter>
      <url hash="799559ba">2022.finnlp-1.0</url>
      <bibkey>finnlp-2022-financial</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Contextualizing Emerging Trends in Financial News Articles</title>
      <author><first>Nhu Khoa</first><last>Nguyen</last><affiliation>L3i Laboratory, La Rochelle University</affiliation></author>
      <author><first>Thierry</first><last>Delahaut</last><affiliation>La Banque Postale - Asset Management</affiliation></author>
      <author><first>Emanuela</first><last>Boros</last><affiliation>University of La Rochelle</affiliation></author>
      <author><first>Antoine</first><last>Doucet</last><affiliation>University of La Rochelle</affiliation></author>
      <author><first>Gaël</first><last>Lejeune</last><affiliation>STIH, Sorbonne Université</affiliation></author>
      <pages>1-9</pages>
      <abstract>Identifying and exploring emerging trends in news is becoming more essential than ever with many changes occurring around the world due to the global health crises. However, most of the recent research has focused mainly on detecting trends in social media, thus, benefiting from social features (e.g. likes and retweets on Twitter) which helped the task as they can be used to measure the engagement and diffusion rate of content. Yet, formal text data, unlike short social media posts, comes with a longer, less restricted writing format, and thus, more challenging. In this paper, we focus our study on emerging trends detection in financial news articles about Microsoft, collected before and during the start of the COVID-19 pandemic (July 2019 to July 2020). We make the dataset freely available and we also propose a strong baseline (Contextual Leap2Trend) for exploring the dynamics of similarities between pairs of keywords based on topic modeling and term frequency. Finally, we evaluate against a gold standard (Google Trends) and present noteworthy real-world scenarios regarding the influence of the pandemic on Microsoft.</abstract>
      <url hash="febc2b15">2022.finnlp-1.1</url>
      <bibkey>nguyen-etal-2022-contextualizing</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>A</fixed-case>st<fixed-case>BERT</fixed-case>: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees</title>
      <author><first>Rong</first><last>Liang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Tiehua</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yujie</first><last>Lu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yuze</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zhen</first><last>Huang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xin</first><last>Chen</last><affiliation>Ant Group</affiliation></author>
      <pages>10-17</pages>
      <abstract>Using the pre-trained language models to understand source codes has attracted increasing attention from financial institutions owing to the great potential to uncover financial risks. However, there are several challenges in applying these language models to solve programming language related problems directly. For instance, the shift of domain knowledge between natural language (NL) and programming language (PL) requires understanding the semantic and syntactic information from the data from different perspectives. To this end, we propose the AstBERT model, a pre-trained PL model aiming to better understand the financial codes using the abstract syntax tree (AST). Specifically, we collect a sheer number of source codes (both Java and Python) from the Alipay code repository and incorporate both syntactic and semantic code knowledge into our model through the help of code parsers, in which AST information of the source codes can be interpreted and integrated. We evaluate the performance of the proposed model on three tasks, including code question answering, code clone detection and code refinement. Experiment results show that our AstBERT achieves promising performance on three different downstream tasks.</abstract>
      <url hash="a8992d67">2022.finnlp-1.2</url>
      <bibkey>liang-etal-2022-astbert</bibkey>
    </paper>
    <paper id="3">
      <title>Disentangled Variational Topic Inference for Topic-Accurate Financial Report Generation</title>
      <author><first>Sixing</first><last>Yan</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>18-24</pages>
      <abstract>Automatic generating financial report from a set of news is important but challenging. The financial reports is composed of key points of the news and corresponding inferring and reasoning from specialists in financial domain with professional knowledge. The challenges lie in the effective learning of the extra knowledge that is not well presented in the news, and the misalignment between topic of input news and output knowledge in target reports. In this work, we introduce a disentangled variational topic inference approach to learn two latent variables for news and report, respectively. We use a publicly available dataset to evaluate the proposed approach. The results demonstrate its effectiveness of enhancing the language informativeness and the topic accuracy of the generated financial reports.</abstract>
      <url hash="635b6597">2022.finnlp-1.3</url>
      <bibkey>yan-2022-disentangled</bibkey>
    </paper>
    <paper id="4">
      <title>Toward Privacy-preserving Text Embedding Similarity with Homomorphic Encryption</title>
      <author><first>Donggyu</first><last>Kim</last><affiliation>-</affiliation></author>
      <author><first>Garam</first><last>Lee</last><affiliation>CryptoLab</affiliation></author>
      <author><first>Sungwoo</first><last>Oh</last><affiliation>KB Kookmin Bank</affiliation></author>
      <pages>25-36</pages>
      <abstract>Text embedding is an essential component to build efficient natural language applications based on text similarities such as search engines and chatbots. Certain industries like finance and healthcare demand strict privacy-preserving conditions that user’s data should not be exposed to any potential malicious users even including service providers. From a privacy standpoint, text embeddings seem impossible to be interpreted but there is still a privacy risk that they can be recovered to original texts through inversion attacks. To satisfy such privacy requirements, in this paper, we study a Homomorphic Encryption (HE) based text similarity inference. To validate our method, we perform extensive experiments on two vital text similarity tasks. Through text embedding inversion tests, we prove that the benchmark datasets are vulnerable to inversion attacks and another privacy preserving approach, dχ-privacy, a relaxed version of Local Differential Privacy method fails to prevent them. We show that our approach preserves the performance of models compared to that the baseline has degradation up to 10% of scores for the minimum security.</abstract>
      <url hash="81648cc0">2022.finnlp-1.4</url>
      <bibkey>kim-etal-2022-toward</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>T</fixed-case>weet<fixed-case>F</fixed-case>in<fixed-case>S</fixed-case>ent: A Dataset of Stock Sentiments on <fixed-case>T</fixed-case>witter</title>
      <author><first>Yulong</first><last>Pei</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Amarachi</first><last>Mbakwe</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Akshat</first><last>Gupta</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Salwa</first><last>Alamir</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Hanxuan</first><last>Lin</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Xiaomo</first><last>Liu</last><affiliation>JP Morgan AI Research</affiliation></author>
      <author><first>Sameena</first><last>Shah</last><affiliation>JP Morgan AI Research</affiliation></author>
      <pages>37-47</pages>
      <abstract>Stock sentiment has strong correlations with the stock market but traditional sentiment analysis task classifies sentiment according to having feelings and emotions of good or bad. This definition of sentiment is not an accurate indicator of public opinion about specific stocks. To bridge this gap, we introduce a new task of stock sentiment analysis and present a new dataset for this task named TweetFinSent. In TweetFinSent, tweets are annotated based on if one gained or expected to gain positive or negative return from a stock. Experiments on TweetFinSent with several sentiment analysis models from lexicon-based to transformer-based have been conducted. Experimental results show that TweetFinSent dataset constitutes a challenging problem and there is ample room for improvement on the stock sentiment analysis task. TweetFinSent is available at <url>https://github.com/jpmcair/tweetfinsent</url>.</abstract>
      <url hash="5bbfc137">2022.finnlp-1.5</url>
      <bibkey>pei-etal-2022-tweetfinsent</bibkey>
    </paper>
    <paper id="6">
      <title>Stock Price Volatility Prediction: A Case Study with <fixed-case>A</fixed-case>uto<fixed-case>ML</fixed-case></title>
      <author><first>Hilal</first><last>Pataci</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Yunyao</first><last>Li</last><affiliation>Apple</affiliation></author>
      <author><first>Yannis</first><last>Katsis</last><affiliation>IBM Research - Almaden</affiliation></author>
      <author><first>Yada</first><last>Zhu</last><affiliation>MIT-IBM Watson AI Lab, IBM Research</affiliation></author>
      <author><first>Lucian</first><last>Popa</last><affiliation>IBM Research - Almaden</affiliation></author>
      <pages>48-57</pages>
      <abstract>Accurate prediction of the stock price volatility, the rate at which the price of a stock increases or decreases over a particular period, is an important problem in finance. Inaccurate prediction of stock price volatility might lead to investment risk and financial loss, while accurate prediction might generate significant returns for investors. Several studies investigated stock price volatility prediction in a regression task by using the transcripts of earning calls (quarterly conference calls held by public companies) with Natural Language Processing (NLP) techniques. Existing studies use the entire transcript and this degrades the performance due to noise caused by irrelevant information that might not have a significant impact on stock price volatility. In order to overcome these limitations, by considering stock price volatility prediction as a classification task, we explore several denoising approaches, ranging from general-purpose approaches to techniques specific to finance to remove the noise, and leverage AutoML systems that enable auto-exploration of a wide variety of models. Our preliminary findings indicate that domain-specific denoising approaches provide better results than general-purpose approaches, moreover AutoML systems provide promising results.</abstract>
      <url hash="a9bde3b0">2022.finnlp-1.6</url>
      <bibkey>pataci-etal-2022-stock</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>D</fixed-case>igi<fixed-case>C</fixed-case>all: A Benchmark for Measuring the Maturity of Digital Strategy through Company Earning Calls</title>
      <author><first>Hilal</first><last>Pataci</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Kexuan</first><last>Sun</last><affiliation>University of Southern California</affiliation></author>
      <author><first>T.</first><last>Ravichandran</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>58-67</pages>
      <abstract>Digital transformation reinvents companies, their vision and strategy, organizational structure, processes, capabilities, and culture, and enables the development of new or enhanced products and services delivered to customers more efficiently. Organizations, by formalizing their digital strategy attempt to plan for their digital transformations and accelerate their company growth. Understanding how successful a company is in its digital transformation starts with accurate measurement of its digital maturity levels. However, existing approaches to measuring organizations’ digital strategy have low accuracy levels and this leads to inconsistent results, and also does not provide resources (data) for future research to improve. In order to measure the digital strategy maturity of companies, we leverage the state-of-the-art NLP models on unstructured data (earning call transcripts), and reach the state-of-the-art levels (94%) for this task. We release 3.691 earning call transcripts and also annotated data set, labeled particularly for the digital strategy maturity by linguists. Our work provides an empirical baseline for research in industry and management science.</abstract>
      <url hash="a03677ba">2022.finnlp-1.7</url>
      <bibkey>pataci-etal-2022-digicall</bibkey>
    </paper>
    <paper id="8">
      <title>Learning Better Intent Representations for Financial Open Intent Classification</title>
      <author><first>Xianzhi</first><last>Li</last><affiliation>Queens University</affiliation></author>
      <author><first>Will</first><last>Aitken</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Stephen W.</first><last>Thomas</last><affiliation>Queen’s University</affiliation></author>
      <pages>68-77</pages>
      <abstract>With the recent surge of NLP technologies in the financial domain, banks and other financial entities have adopted virtual agents (VA) to assist customers. A challenging problem for VAs in this domain is determining a user’s reason or intent for contacting the VA, especially when the intent was unseen or open during the VA’s training. One method for handling open intents is adaptive decision boundary (ADB) post-processing, which learns tight decision boundaries from intent representations to separate known and open intents. We propose incorporating two methods for supervised pre-training of intent representations: prefix tuning and fine-tuning just the last layer of a large language model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher than the prior state-of-the-art ADB method for open intent classification on the banking77 benchmark amongst others. Notably, we only supplement the original ADB model with 0.1% additional trainable parameters. Ablation studies also determine that our method yields better results than full fine-tuning the entire model. We hypothesize that our findings could stimulate a new optimal method of downstream tuning that combines parameter efficient tuning modules with fine-tuning a subset of the base model’s layers.</abstract>
      <url hash="35fe93b6">2022.finnlp-1.8</url>
      <bibkey>li-etal-2022-learning-better</bibkey>
    </paper>
    <paper id="9">
      <title>Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis</title>
      <author><first>Sudhandar</first><last>Balakrishnan</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Yihao</first><last>Fang</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <pages>78-88</pages>
      <abstract>The invention of transformer-based models such as BERT, GPT, and RoBERTa has enabled researchers and financial companies to finetune these powerful models and use them in different downstream tasks to achieve state-of-the-art performance. Recently, a lightweight alternative (approximately 0.1% - 3% of the original model parameters) to fine-tuning, known as prefix tuning has been introduced. This method freezes the model parameters and only updates the prefix to achieve performance comparable to full fine-tuning. Prefix tuning enables researchers and financial practitioners to achieve similar results with much fewer parameters. In this paper, we explore the robustness of prefix tuning when facing noisy data. Our experiments demonstrate that fine-tuning is more robust to noise than prefix tuning—the latter method faces a significant decrease in performance on most corrupted data sets with increasing noise levels. Furthermore, prefix tuning has high variances on the F1 scores compared to fine-tuning in many corruption methods. We strongly advocate that caution should be carefully taken when applying the state-of-the-art prefix tuning method to noisy data.</abstract>
      <url hash="424af464">2022.finnlp-1.9</url>
      <bibkey>balakrishnan-etal-2022-exploring</bibkey>
    </paper>
    <paper id="10">
      <title>A Taxonomical <fixed-case>NLP</fixed-case> Blueprint to Support Financial Decision Making through Information-Centred Interactions</title>
      <author><first>Siavash</first><last>Kazemian</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Cosmin</first><last>Munteanu</last><affiliation>University of Toronto Mississauga</affiliation></author>
      <author><first>Gerald</first><last>Penn</last><affiliation>University of Toronto</affiliation></author>
      <pages>89-98</pages>
      <abstract>Investment management professionals (IMPs) often make decisions after manual analysis of text transcripts of central banks’ conferences or companies’ earning calls. Their current software tools, while interactive, largely leave users unassisted in using these transcripts. A key component to designing speech and NLP techniques for this community is to qualitatively characterize their perceptions of AI as well as their legitimate needs so as to (1) better apply existing NLP methods, (2) direct future research and (3) correct IMPs’ perceptions of what AI is capable of. This paper presents such a study, through a contextual inquiry with eleven IMPs, uncovering their information practices when using such transcripts. We then propose a taxonomy of user requirements and usability criteria to support IMP decision making, and validate the taxonomy through participatory design workshops with four IMPs. Our investigation suggests that: (1) IMPs view visualization methods and natural language processing algorithms primarily as time-saving tools that are incapable of enhancing either discovery or interpretation and (2) their existing software falls well short of the state of the art in both visualization and NLP.</abstract>
      <url hash="ebb2613d">2022.finnlp-1.10</url>
      <bibkey>kazemian-etal-2022-taxonomical</bibkey>
    </paper>
    <paper id="11">
      <title>Overview of the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Evaluating the Rationales of Amateur Investors</title>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>National Institute of Advanced Industrial Science and Technology, Japan</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>The National Institute of Advanced Industrial Science and Technology (AIST)</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>99-103</pages>
      <abstract>This paper provides an overview of the shared task, Evaluating the Rationales of Amateur Investors (ERAI), in FinNLP-2022 at EMNLP-2022. This shared task aims to sort out investment opinions that would lead to higher profit from social platforms. We obtained 19 registered teams; 9 teams submitted their results for final evaluation, and 8 teams submitted papers to share their methods. The discussed directions are various: prompting, fine-tuning, translation system comparison, and tailor-made neural network architectures. We provide details of the task settings, data statistics, participants’ results, and fine-grained analysis.</abstract>
      <url hash="9c7ec0a5">2022.finnlp-1.11</url>
      <bibkey>chen-etal-2022-overview</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>S</fixed-case>hots at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Pairwise Comparison and Unsupervised Ranking</title>
      <author><first>Peratham</first><last>Wiriyathammabhum</last><affiliation>-</affiliation></author>
      <pages>104-110</pages>
      <abstract>This report describes our PromptShots submissions to a shared task on Evaluating the Rationales of Amateur Investors (ERAI). We participated in both pairwise comparison and unsupervised ranking tasks. For pairwise comparison, we employed instruction-based models based on T5-small and OpenAI InstructGPT language models. Surprisingly, we observed OpenAI InstructGPT language model few-shot trained on Chinese data works best in our submissions, ranking 3rd on the maximal loss (ML) pairwise accuracy. This model works better than training on the Google translated English data by a large margin, where the English few-shot trained InstructGPT model even performs worse than an instruction-based T5-small model finetuned on the English data. However, all instruction-based submissions do not perform well on the maximal potential profit (MPP) pairwise accuracy where there are more data and learning signals. The Chinese few-shot trained InstructGPT model still performs best in our setting. For unsupervised ranking, we utilized many language models, including many financial-specific ones, and Bayesian lexicons unsupervised-learned on both Chinese and English words using a method-of-moments estimator. All our submissions rank best in the MPP ranking, from 1st to 3rd. However, they all do not perform well for ML scoring. Therefore, both MPP and ML scores need different treatments since we treated MPP and ML using the same formula. Our only difference is the treatment of market sentiment lexicons.</abstract>
      <url hash="7414f661">2022.finnlp-1.12</url>
      <bibkey>wiriyathammabhum-2022-promptshots</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>LIPI</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Ensembling Sentence Transformers for Assessing Maximum Possible Profit and Loss from Online Financial Posts</title>
      <author><first>Sohom</first><last>Ghosh</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Sudip Kumar</first><last>Naskar</last><affiliation>Jadavpur University</affiliation></author>
      <pages>111-115</pages>
      <abstract>Using insights from social media for making investment decisions has become mainstream. However, in the current era of information ex- plosion, it is essential to mine high-quality so- cial media posts. The FinNLP-2022 ERAI task deals with assessing Maximum Possible Profit (MPP) and Maximum Loss (ML) from social me- dia posts relating to finance. In this paper, we present our team LIPI’s approach. We ensem- bled a range of Sentence Transformers to quan- tify these posts. Unlike other teams with vary- ing performances across different metrics, our system performs consistently well. Our code is available here https://github.com/sohomghosh/LIPI_ERAI_ FinNLP_EMNLP- 2022/</abstract>
      <url hash="bb490c8d">2022.finnlp-1.13</url>
      <bibkey>ghosh-naskar-2022-lipi-finnlp</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>DCU</fixed-case>-<fixed-case>ML</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Investigating the Transferability of Sentiment Analysis Data for Evaluating Rationales of Investors</title>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Tianbo</first><last>Ji</last><affiliation>ADAPT Centre</affiliation></author>
      <author><first>Liting</first><last>Zhou</last><affiliation>Dublin City University</affiliation></author>
      <pages>116-121</pages>
      <abstract>In this paper, we describe our system for the FinNLP-2022 shared task: Evaluating the Rationales of Amateur Investors (ERAI). The ERAI shared tasks focuses on mining profitable information from financial texts by predicting the possible Maximal Potential Profit (MPP) and Maximal Loss (ML) based on the posts from amateur investors. There are two sub-tasks in ERAI: Pairwise Comparison and Unsupervised Rank, both target on the prediction of MPP and ML. To tackle the two tasks, we frame this task as a text-pair classification task where the input consists of two documents and the output is the label of whether the first document will lead to higher MPP or lower ML. Specifically, we propose to take advantage of the transferability of Sentiment Analysis data with an assumption that a more positive text will lead to higher MPP or higher ML to facilitate the prediction of MPP and ML. In experiment on the ERAI blind test set, our systems trained on Sentiment Analysis data and ERAI training data ranked 1st and 8th in ML and MPP pairwise comparison respectively. Code available in this link.</abstract>
      <url hash="cb8cde0f">2022.finnlp-1.14</url>
      <bibkey>lyu-etal-2022-dcu-ml</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>UOA</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Leveraging the Class Label Description for Financial Opinion Mining</title>
      <author><first>Jinan</first><last>Zou</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Haiyao</first><last>Cao</last><affiliation>Australian Institute for Machine Learning, University of Adelaide</affiliation></author>
      <author><first>Yanxi</first><last>Liu</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Lingqiao</first><last>Liu</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Ehsan</first><last>Abbasnejad</last><affiliation>Aiml</affiliation></author>
      <author><first>Javen Qinfeng</first><last>Shi</last><affiliation>The University of Adelaide</affiliation></author>
      <pages>122-126</pages>
      <abstract>Evaluating the Rationales of Amateur Investors (ERAI) is a task about mining expert-like viewpoints from social media. This paper summarizes our solutions to the ERAI shared task, which is co-located with the FinNLP workshop at EMNLP 2022. There are 2 sub-tasks in ERAI. Sub-task 1 is a pair-wised comparison task, where we propose a BERT-based pre-trained model projecting opinion pairs in a common space for classification. Sub-task 2 is an unsupervised learning task ranking the opinions’ maximal potential profit (MPP) and maximal loss (ML), where our model leverages the regression method and multi-layer perceptron to rank the MPP and ML values. The proposed approaches achieve competitive accuracy of 54.02% on ML Accuracy and 51.72% on MPP Accuracy for pairwise tasks, also 12.35% and -9.39% regression unsupervised ranking task for MPP and ML.</abstract>
      <url hash="a13a9b64">2022.finnlp-1.15</url>
      <bibkey>zou-etal-2022-uoa</bibkey>
    </paper>
    <paper id="16">
      <title>ai<fixed-case>ML</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Combining Classification and Regression Tasks for Financial Opinion Mining</title>
      <author><first>Zhaoxuan</first><last>Qin</last><affiliation>The University of Adelaide</affiliation></author>
      <author><first>Jinan</first><last>Zou</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Qiaoyang</first><last>Luo</last><affiliation>The Australian Institute for Machine Learning</affiliation></author>
      <author><first>Haiyao</first><last>Cao</last><affiliation>Australian Institute for Machine Learning, University of Adelaide</affiliation></author>
      <author><first>Yang</first><last>Jiao</last><affiliation>University of Adelaide at South Australia</affiliation></author>
      <pages>127-131</pages>
      <abstract>Identifying posts of high financial quality from opinions is of extraordinary significance for investors. Hence, this paper focuses on evaluating the rationales of amateur investors (ERAI) in a shared task, and we present our solutions. The pairwise comparison task aims at extracting the post that will trigger higher MPP and ML values from pairs of posts. The goal of the unsupervised ranking task is to find the top 10% of posts with higher MPP and ML values. We initially model the shared task as text classification and regression problems. We then propose a multi-learning approach applied by financial domain pre-trained models and multiple linear classifiers for factor combinations to integrate better relationships and information between training data. The official results have proved that our method achieves 48.28% and 52.87% for MPP and ML accuracy on pairwise tasks, 14.02% and -4.17% regarding unsupervised ranking tasks for MPP and ML. Our source code is available.</abstract>
      <url hash="28d237ed">2022.finnlp-1.16</url>
      <bibkey>qin-etal-2022-aiml</bibkey>
    </paper>
    <paper id="17">
      <title>Yet at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Modified models for evaluating the Rationales of Amateur Investors</title>
      <author><first>Yan</first><last>Zhuang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Fuji</first><last>Ren</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <pages>132-135</pages>
      <abstract>The financial reports usually reveal the recent development of the company and often cause the volatility in the company’s share price. The opinions causing higher maximal potential profit and lower maximal loss can help the amateur investors choose rational strategies. FinNLP-2022 ERAI task aims to quantify the opinions’ potentials of leading higher maximal potential profit and lower maximal loss. In this paper, different strategies were applied to solve the ERAI tasks. Valinna ‘RoBERTa-wwm’ showed excellent performance and helped us rank second in ‘MPP’ label prediction task. After integrating some tricks, the modified ‘RoBERTa-wwm’ outperformed all other models in ‘ML’ ranking task.</abstract>
      <url hash="94011573">2022.finnlp-1.17</url>
      <bibkey>zhuang-ren-2022-yet</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>LDPP</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: Determinantal Point Processes and Variational Auto-encoders for Identifying High-Quality Opinions from a pool of Social Media Posts</title>
      <author><first>Paul</first><last>Trust</last><affiliation>University College Cork</affiliation></author>
      <author><first>Rosane</first><last>Minghim</last><affiliation>University College Cork</affiliation></author>
      <pages>136-140</pages>
      <abstract>Social media and online forums have made it easier for people to share their views and opinions on various topics in society. In this paper, we focus on posts discussing investment related topics. When it comes to investment , people can now easily share their opinions about online traded items and also provide rationales to support their arguments on social media. However, there are millions of posts to read with potential of having some posts from amateur investors or completely unrelated posts. Identifying the most important posts that could lead to higher maximal potential profit (MPP) and lower maximal loss for investment is not a trivial task. In this paper, propose to use determinantal point processes and variational autoencoders to identify high quality posts from the given rationales. Experimental results suggest that our method mines quality posts compared to random selection and also latent variable modeling improves improves the quality of selected posts.</abstract>
      <url hash="2cf574e2">2022.finnlp-1.18</url>
      <bibkey>trust-minghim-2022-ldpp</bibkey>
    </paper>
    <paper id="19">
      <title>Jetsons at the <fixed-case>F</fixed-case>in<fixed-case>NLP</fixed-case>-2022 <fixed-case>ERAI</fixed-case> Task: <fixed-case>BERT</fixed-case>-<fixed-case>C</fixed-case>hinese for mining high <fixed-case>MPP</fixed-case> posts</title>
      <author><first>Alolika</first><last>Gon</last><affiliation>Fidelity Investments, LLC</affiliation></author>
      <author><first>Sihan</first><last>Zha</last><affiliation>Fidelity Investments, LLC</affiliation></author>
      <author><first>Sai Krishna</first><last>Rallabandi</last><affiliation>Fidelity Investments, LLC</affiliation></author>
      <author><first>Parag Pravin</first><last>Dakle</last><affiliation>Fidelity Investments, LLC</affiliation></author>
      <author><first>Preethi</first><last>Raghavan</last><affiliation>IBM Research</affiliation></author>
      <pages>141-146</pages>
      <abstract>In this paper, we discuss the various approaches by the <i>Jetsons</i> team for the “Pairwise Comparison” sub-task of the ERAI shared task to compare financial opinions for profitability and loss. Our BERT-Chinese model considers a pair of opinions and predicts the one with a higher maximum potential profit (MPP) with 62.07% accuracy. We analyze the performance of our approaches on both the MPP and maximal loss (ML) problems and deeply dive into why BERT-Chinese outperforms other models.</abstract>
      <url hash="cad9e83e">2022.finnlp-1.19</url>
      <bibkey>gon-etal-2022-jetsons</bibkey>
    </paper>
    <paper id="20">
      <title>No Stock is an Island: Learning Internal and Relational Attributes of Stocks with Contrastive Learning</title>
      <author><first>Shicheng</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Li</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <author><first>Zhiyuan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ruihan</first><last>Bao</last><affiliation>Mizuho Bank</affiliation></author>
      <author><first>Keiko</first><last>Harimoto</last><affiliation>Mizuho Bank</affiliation></author>
      <author><first>Keiko</first><last>Harimoto</last><affiliation>Mizuho Bank</affiliation></author>
      <pages>147-153</pages>
      <abstract>Previous work has demonstrated the viability of applying deep learning techniques in the financial area. Recently, the task of stock embedding learning has been drawing attention from the research community, which aims to represent the characteristics of stocks with distributed vectors that can be used in various financial analysis scenarios. Existing approaches for learning stock embeddings either require expert knowledge, or mainly focus on the textual part of information corresponding to individual temporal movements. In this paper, we propose to model stock properties as the combination of internal attributes and relational attributes, which takes into consideration both the time-invariant properties of individual stocks and their movement patterns in relation to the market. To learn the two types of attributes from financial news and transaction data, we design several training objectives based on contrastive learning to extract and separate the long-term and temporary information in the data that are able to counter the inherent randomness of the stock market. Experiments and further analyses on portfolio optimization reveal the effectiveness of our method in extracting comprehensive stock information from various data sources.</abstract>
      <url hash="09f0c7d6">2022.finnlp-1.20</url>
      <bibkey>li-etal-2022-stock</bibkey>
    </paper>
    <paper id="21">
      <title>Prospectus Language and <fixed-case>IPO</fixed-case> Performance</title>
      <author><first>Jared</first><last>Sharpe</last><affiliation>University of Delaware</affiliation></author>
      <author><first>Keith</first><last>Decker</last><affiliation>University of Delaware</affiliation></author>
      <pages>154-162</pages>
      <abstract>Pricing a firm’s Initial Public Offering (IPO) has historically been very difficult, with high average returns on the first-day of trading. Furthermore, IPO withdrawal, the event in which companies who file to go public ultimately rescind the application before the offering, is an equally challenging prediction problem. This research utilizes word embedding techniques to evaluate existing theories concerning firm sentiment on first-day trading performance and the probability of withdrawal, which has not yet been explored empirically. The results suggest that firms attempting to go public experience a decreased probability of withdrawal with the increased presence of positive, litigious, and uncertain language in their initial prospectus, while the increased presence of strong modular language leads to an increased probability of withdrawal. The results also suggest that frequent or large adjustments in the strong modular language of subsequent filings leads to smaller first-day returns.</abstract>
      <url hash="369a951f">2022.finnlp-1.21</url>
      <bibkey>sharpe-decker-2022-prospectus</bibkey>
    </paper>
    <paper id="22">
      <title>It’s Time to Reason: Annotating Argumentation Structures in Financial Earnings Calls: The <fixed-case>F</fixed-case>in<fixed-case>A</fixed-case>rg Dataset</title>
      <author><first>Alaa</first><last>Alhamzeh</last><affiliation>INSA de Lyon and University of Passau</affiliation></author>
      <author><first>Romain</first><last>Fonck</last><affiliation>Insa De Lyon</affiliation></author>
      <author><first>Erwan</first><last>Versmée</last><affiliation>Insa De Lyon</affiliation></author>
      <author><first>Elöd</first><last>Egyed-Zsigmond</last><affiliation>INSA Lyon</affiliation></author>
      <author><first>Harald</first><last>Kosch</last><affiliation>University of Passau</affiliation></author>
      <author><first>Lionel</first><last>Brunie</last><affiliation>INSA de Lyon</affiliation></author>
      <pages>163-169</pages>
      <abstract>With the goal of reasoning on the financial textual data, we present in this paper, a novel approach for annotating arguments, their components and relations in the transcripts of earnings conference calls (ECCs). The proposed scheme is driven from the argumentation theory at the micro-structure level of discourse. We further conduct a manual annotation study with four annotators on 136 documents. We obtained inter-annotator agreement of <tex-math>lpha_{U}</tex-math> = 0.70 for argument components and <tex-math>lpha</tex-math> = 0.81 for argument relations. The final created corpus, with the size of 804 documents, as well as the annotation guidelines are publicly available for researchers in the domains of computational argumentation, finance and FinNLP.</abstract>
      <url hash="e660384c">2022.finnlp-1.22</url>
      <bibkey>alhamzeh-etal-2022-time</bibkey>
    </paper>
    <paper id="23">
      <title>How Can a Teacher Make Learning From Sparse Data Softer? Application to Business Relation Extraction</title>
      <author><first>Hadjer</first><last>Khaldi</last><affiliation>IRIT - University of Paul Sabatier/ Geotrend</affiliation></author>
      <author><first>Farah</first><last>Benamara</last><affiliation>University of Toulouse</affiliation></author>
      <author><first>Camille</first><last>Pradel</last><affiliation>Geotrend</affiliation></author>
      <author><first>Nathalie</first><last>Aussenac-Gilles</last><affiliation>Cnrs - Irit</affiliation></author>
      <pages>170-177</pages>
      <abstract>Business Relation Extraction between market entities is a challenging information extraction task that suffers from data imbalance due to the over-representation of negative relations (also known as No-relation or Others) compared to positive relations that corresponds to the taxonomy of relations of interest. This paper proposes a novel solution to tackle this problem, relying on binary soft labels supervision generated by an approach based on knowledge distillation. When evaluated on a business relation extraction dataset, the results suggest that the proposed approach improves the overall performance, beating state-of-the art solutions for data imbalance. In particular, it improves the extraction of under-represented relations as well as the detection of false negatives.</abstract>
      <url hash="0e26c77a">2022.finnlp-1.23</url>
      <bibkey>khaldi-etal-2022-teacher</bibkey>
    </paper>
    <paper id="24">
      <title>Astock: A New Dataset and Automated Stock Trading based on Stock-specific News Analyzing Model</title>
      <author><first>Jinan</first><last>Zou</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Haiyao</first><last>Cao</last><affiliation>Australian Institute for Machine Learning, University of Adelaide</affiliation></author>
      <author><first>Lingqiao</first><last>Liu</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Yuhao</first><last>Lin</last><affiliation>Uni Adelaide</affiliation></author>
      <author><first>Ehsan</first><last>Abbasnejad</last><affiliation>Aiml</affiliation></author>
      <author><first>Javen Qinfeng</first><last>Shi</last><affiliation>The University of Adelaide</affiliation></author>
      <pages>178-186</pages>
      <abstract>Natural Language Processing (NLP) demonstrates a great potential to support financial decision-making by analyzing the text from social media or news outlets. In this work, we build a platform to study the NLP-aided stock auto-trading algorithms systematically. In contrast to the previous work, our platform is characterized by three features: (1) We provide financial news for each specific stock. (2) We provide various stock factors for each stock. (3) We evaluate performance from more financial-relevant metrics. Such a design allows us to develop and evaluate NLP-aided stock auto-trading algorithms in a more realistic setting. In addition to designing an evaluation platform and dataset collection, we also made a technical contribution by proposing a system to automatically learn a good feature representation from various input information. The key to our algorithm is a method called semantic role labeling Pooling (SRLP), which leverages Semantic Role Labeling (SRL) to create a compact representation of each news paragraph. Based on SRLP, we further incorporate other stock factors to make the final prediction. In addition, we propose a self-supervised learning strategy based on SRLP to enhance the out-of-distribution generalization performance of our system. Through our experimental study, we show that the proposed method achieves better performance and outperforms all the baselines’ annualized rate of return as well as the maximum drawdown of the CSI300 index and XIN9 index on real trading. Our Astock dataset and code are available at https://github.com/JinanZou/Astock.</abstract>
      <url hash="b2e9e926">2022.finnlp-1.24</url>
      <bibkey>zou-etal-2022-astock</bibkey>
    </paper>
    <paper id="25">
      <title>Next-Year Bankruptcy Prediction from Textual Data: Benchmark and Baselines</title>
      <author><first>Henri</first><last>Arno</last><affiliation>Ghent University</affiliation></author>
      <author><first>Klaas</first><last>Mulier</last><affiliation>Ghent University</affiliation></author>
      <author><first>Joke</first><last>Baeck</last><affiliation>Ghent University</affiliation></author>
      <author><first>Thomas</first><last>Demeester</last><affiliation>Ghent University - imec</affiliation></author>
      <pages>187-195</pages>
      <abstract>Models for bankruptcy prediction are useful in several real-world scenarios, and multiple research contributions have been devoted to the task, based on structured (numerical) as well as unstructured (textual) data. However, the lack of a common benchmark dataset and evaluation strategy impedes the objective comparison between models. This paper introduces such a benchmark for the unstructured data scenario, based on novel and established datasets, in order to stimulate further research into the task. We describe and evaluate several classical and neural baseline models, and discuss benefits and flaws of different strategies. In particular, we find that a lightweight bag-of-words model based on static in-domain word representations obtains surprisingly good results, especially when taking textual data from several years into account. These results are critically assessed, and discussed in light of particular aspects of the data and the task. All code to replicate the data and experimental results will be released.</abstract>
      <url hash="00c6db51">2022.finnlp-1.25</url>
      <bibkey>arno-etal-2022-next</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>A</fixed-case>da<fixed-case>K</fixed-case>-<fixed-case>NER</fixed-case>: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations</title>
      <author><first>Hongtao</first><last>Ruan</last><affiliation>Ant Group</affiliation></author>
      <author><first>Liying</first><last>Zheng</last><affiliation>Ebay Inc.</affiliation></author>
      <author><first>Peixian</first><last>Hu</last><affiliation>Huaneng Guicheng Trust Corp.,Ltd.</affiliation></author>
      <pages>196-202</pages>
      <abstract>State-of-the-art Named Entity Recognition (NER) models rely heavily on large amounts of fully annotated training data. However, accessible data are often incompletely annotated since the annotators usually lack comprehensive knowledge in the target domain. Normally the unannotated tokens are regarded as non-entities by default, while we underline that these tokens could either be non-entities or part of any entity. Here, we study NER modeling with incomplete annotated data where only a fraction of the named entities are labeled, and the unlabeled tokens are equivalently multi-labeled by every possible label. Taking multi-labeled tokens into account, the numerous possible paths can distract the training model from the gold path (ground truth label sequence), and thus hinders the learning ability. In this paper, we propose AdaK-NER, named the adaptive top-K approach, to help the model focus on a smaller feasible region where the gold path is more likely to be located. We demonstrate the superiority of our approach through extensive experiments on both English and Chinese datasets, averagely improving 2% in F-score on the CoNLL-2003 and over 10% on two Chinese datasets compared with the prior state-of-the-art works.</abstract>
      <url hash="11716df0">2022.finnlp-1.26</url>
      <bibkey>ruan-etal-2022-adak</bibkey>
    </paper>
    <paper id="27">
      <title>A Sentiment and Emotion Annotated Dataset for Bitcoin Price Forecasting Based on <fixed-case>R</fixed-case>eddit Posts</title>
      <author><first>Pavlo</first><last>Seroyizhko</last><affiliation>University of Studies of Bologna</affiliation></author>
      <author><first>Zhanel</first><last>Zhexenova</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Muhammad Zohaib</first><last>Shafiq</last><affiliation>-</affiliation></author>
      <author><first>Fabio</first><last>Merizzi</last><affiliation>Alma Mater Studiorum</affiliation></author>
      <author><first>Andrea</first><last>Galassi</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Federico</first><last>Ruggeri</last><affiliation>University of Bologna</affiliation></author>
      <pages>203-210</pages>
      <abstract>Cryptocurrencies have gained enormous momentum in finance and are nowadays commonly adopted as a medium of exchange for online payments. After recent events during which GameStop’s stocks were believed to be influenced by WallStreetBets subReddit, Reddit has become a very hot topic on the cryptocurrency market. The influence of public opinions on cryptocurrency price trends has inspired researchers on exploring solutions that integrate such information in crypto price change forecasting. A popular integration technique regards representing social media opinions via sentiment features. However, this research direction is still in its infancy, where a limited number of publicly available datasets with sentiment annotations exists. We propose a novel Bitcoin Reddit Sentiment Dataset, a ready-to-use dataset annotated with state-of-the-art sentiment and emotion recognition. The dataset contains pre-processed Reddit posts and comments about Bitcoin from several domain-related subReddits along with Bitcoin’s financial data. We evaluate several widely adopted neural architectures for crypto price change forecasting. Our results show controversial benefits of sentiment and emotion features advocating for more sophisticated social media integration techniques. We make our dataset publicly available for research.</abstract>
      <url hash="9cce55ea">2022.finnlp-1.27</url>
      <bibkey>seroyizhko-etal-2022-sentiment</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>F</fixed-case>in<fixed-case>S</fixed-case>im4-<fixed-case>ESG</fixed-case> Shared Task: Learning Semantic Similarities for the Financial Domain. Extended edition to <fixed-case>ESG</fixed-case> insights</title>
      <author><first>Juyeon</first><last>Kang</last><affiliation>Fortia Financial Solutions</affiliation></author>
      <author><first>Ismail</first><last>El Maarouf</last><affiliation>Imprevicible</affiliation></author>
      <pages>211-217</pages>
      <abstract>This paper describes FinSim4-ESG 1 shared task organized in the 4th FinNLP workshopwhich is held in conjunction with the IJCAI-ECAI-2022 confer- enceThis year, the FinSim4 is extended to the Environment, Social and Government (ESG) insights and proposes two subtasks, one for ESG Taxonomy Enrichment and the other for Sustainable Sentence Prediction. Among the 28 teams registered to the shared task, a total of 8 teams submitted their systems results and 6 teams also submitted a paper to describe their method. The winner of each subtask shows good performance results of 0.85% and 0.95% in terms of accuracy, respectively.</abstract>
      <url hash="082c15d1">2022.finnlp-1.28</url>
      <bibkey>kang-el-maarouf-2022-finsim4</bibkey>
    </paper>
    <paper id="29">
      <title>Using Contextual Sentence Analysis Models to Recognize <fixed-case>ESG</fixed-case> Concepts</title>
      <author><first>Elvys</first><last>Linhares Pontes</last><affiliation>University of La Rochelle</affiliation></author>
      <author><first>Mohamed</first><last>Ben Jannet</last><affiliation>Laboratoire d’Informatique pour la Mécanique et les Sciences de l’Ingénieur</affiliation></author>
      <author><first>Jose G.</first><last>Moreno</last><affiliation>Paul Sabatier University - IRIT</affiliation></author>
      <author><first>Antoine</first><last>Doucet</last><affiliation>University of La Rochelle</affiliation></author>
      <pages>218-223</pages>
      <abstract>This paper summarizes the joint participation of the Trading Central Labs and the L3i laboratory of the University of La Rochelle on both sub-tasks of the <i>Shared Task FinSim-4</i> evaluation campaign. The first sub-task aims to enrich the ‘Fortia ESG taxonomy’ with new lexicon entries while the second one aims to classify sentences to either ‘sustainable’ or ‘unsustainable’ with respect to ESG (Environment, Social and Governance) related factors. For the first sub-task, we proposed a model based on pre-trained Sentence-BERT models to project sentences and concepts in a common space in order to better represent ESG concepts. The official task results show that our system yields a significant performance improvement compared to the baseline and outperforms all other submissions on the first sub-task. For the second sub-task, we combine the RoBERTa model with a feed-forward multi-layer perceptron in order to extract the context of sentences and classify them. Our model achieved high accuracy scores (over 92%) and was ranked among the top 5 systems.</abstract>
      <url hash="e23f8306">2022.finnlp-1.29</url>
      <bibkey>linhares-pontes-etal-2022-using</bibkey>
    </paper>
    <paper id="30">
      <title>Automatic Term and Sentence Classification Via Augmented Term and Pre-trained language model in <fixed-case>ESG</fixed-case> Taxonomy texts</title>
      <author><first>Ke</first><last>Tian</last><affiliation>Rakuten Group, Inc</affiliation></author>
      <author><first>Zepeng</first><last>Zhang</last><affiliation>Jiangxi Normal University</affiliation></author>
      <author><first>Hua</first><last>Chen</last><affiliation>Jiangxi Normal University</affiliation></author>
      <pages>224-227</pages>
      <abstract>In this paper, we present our solutions to the FinSim4 Shared Task which is co-located with the FinNLP workshop at IJCAI-2022. This new edition of FinSim4-ESG is extended to the “Environment, Social and Governance (ESG)” related issues in the financial domain. There are two sub-tasks in the FinSim4 shared task. The goal of sub-task1 is to develop a model to predict correctly a list of given terms from ESG taxonomy domain into the most relevant concepts. The aim of subtask2 is to design a system that can automatically classify the ESG Taxonomy text sentence into sustainable or unsustainable class. We have developed different classifiers to automatically classify the terms and sentences with augmented term and pre-trained language models: tf-idf vector, word2vec, Bert, Distill-Bert, Albert, Roberta. The result dashboard shows that our proposed methods yield a significant performance improvement compared to the baseline which ranked 1st in the subtask2 and 2rd of mean rank in the subtask1.</abstract>
      <url hash="949db7a8">2022.finnlp-1.30</url>
      <bibkey>tian-etal-2022-automatic</bibkey>
    </paper>
    <paper id="31">
      <title>Knowledge informed sustainability detection from short financial texts</title>
      <author><first>Boshko</first><last>Koloski</last><affiliation>Jožef Stefan Institute (IJS)</affiliation></author>
      <author><first>Syrielle</first><last>Montariol</last><affiliation>Jožef Stefan Institute</affiliation></author>
      <author><first>Matthew</first><last>Purver</last><affiliation>Queen Mary University of London and Jožef Stefan Institute</affiliation></author>
      <author><first>Senja</first><last>Pollak</last><affiliation>Jožef Stefan Institute</affiliation></author>
      <pages>228-234</pages>
      <abstract>There is a global trend for responsible investing and the need for developing automated methods for analyzing and Environmental, Social and Governance (ESG) related elements in financial texts is raising. In this work we propose a solution to the FinSim4-ESG task, consisting of binary classification of sentences into sustainable or unsustainable. We propose a novel knowledge-based latent heterogeneous representation that is based on knowledge from taxonomies and knowledge graphs and multiple contemporary document representations. We hypothesize that an approach based on a combination of knowledge and document representations can introduce significant improvement over conventional document representation approaches. We consider ensembles on classifier as well on representation level late-fusion and early fusion. The proposed approaches achieve competitive accuracy of 89 and are 5.85 behind the best achieved score.</abstract>
      <url hash="a9edb178">2022.finnlp-1.31</url>
      <bibkey>koloski-etal-2022-knowledge</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>TCS</fixed-case> <fixed-case>WITM</fixed-case> 2022@<fixed-case>F</fixed-case>in<fixed-case>S</fixed-case>im4-<fixed-case>ESG</fixed-case>: Augmenting <fixed-case>BERT</fixed-case> with Linguistic and Semantic features for <fixed-case>ESG</fixed-case> data classification</title>
      <author><first>Tushar</first><last>Goel</last><affiliation>TCS Research</affiliation></author>
      <author><first>Vipul</first><last>Chauhan</last><affiliation>TCS Research</affiliation></author>
      <author><first>Suyash</first><last>Sangwan</last><affiliation>TCS Research</affiliation></author>
      <author><first>Ishan</first><last>Verma</last><affiliation>TCS Research</affiliation></author>
      <author><first>Tirthankar</first><last>Dasgupta</last><affiliation>TCS Research</affiliation></author>
      <author><first>Lipika</first><last>Dey</last><affiliation>TCS Research</affiliation></author>
      <pages>235-242</pages>
      <abstract>Advanced neural network architectures have provided several opportunities to develop systems to automatically capture information from domain-specific unstructured text sources. The FinSim4-ESG shared task, collocated with the FinNLP workshop, proposed two sub-tasks. In sub-task1, the challenge was to design systems that could utilize contextual word embeddings along with sustainability resources to elaborate an ESG taxonomy. In the second sub-task, participants were asked to design a system that could classify sentences into sustainable or unsustainable sentences. In this paper, we utilize semantic similarity features along with BERT embeddings to segregate domain terms into a fixed number of class labels. The proposed model not only considers the contextual BERT embeddings but also incorporates Word2Vec, cosine, and Jaccard similarity which gives word-level importance to the model. For sentence classification, several linguistic elements along with BERT embeddings were used as classification features. We have shown a detailed ablation study for the proposed models.</abstract>
      <url hash="2bbc67f1">2022.finnlp-1.32</url>
      <bibkey>goel-etal-2022-tcs</bibkey>
    </paper>
    <paper id="33">
      <title>Ranking Environment, Social And Governance Related Concepts And Assessing Sustainability Aspect of Financial Texts</title>
      <author><first>Sohom</first><last>Ghosh</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Sudip Kumar</first><last>Naskar</last><affiliation>Jadavpur University</affiliation></author>
      <pages>243-249</pages>
      <abstract>Understanding Environmental, Social, and Governance (ESG) factors related to financial products has become extremely important for investors. However, manually screening through the corporate policies and reports to understand their sustainability aspect is extremely tedious. In this paper, we propose solutions to two such problems which were released as shared tasks of the FinNLP workshop of the IJCAI-2022 conference. Firstly, we train a Sentence Transformers based model which automatically ranks ESG related concepts for a given unknown term. Secondly, we fine-tune a RoBERTa model to classify financial texts as sustainable or not. Out of 26 registered teams, our team ranked 4th in sub-task 1 and 3rd in sub-task 2. The source code can be accessed from https://github.com/sohomghosh/Finsim4_ESG</abstract>
      <url hash="7e7d25c0">2022.finnlp-1.33</url>
      <bibkey>ghosh-naskar-2022-ranking</bibkey>
    </paper>
    <paper id="34">
      <title>Using Transformer-based Models for Taxonomy Enrichment and Sentence Classification</title>
      <author><first>Parag Pravin</first><last>Dakle</last><affiliation>Fidelity Investments, LLC</affiliation></author>
      <author><first>Shrikumar</first><last>Patil</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Sai Krishna</first><last>Rallabandi</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Chaitra</first><last>Hegde</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Preethi</first><last>Raghavan</last><affiliation>IBM Research</affiliation></author>
      <pages>250-258</pages>
      <abstract>In this paper, we present a system that addresses the taxonomy enrichment problem for Environment, Social and Governance issues in the financial domain, as well as classifying sentences as sustainable or unsustainable, for FinSim4-ESG, a shared task for the FinNLP workshop at IJCAI-2022. We first created a derived dataset for taxonomy enrichment by using a sentence-BERT-based paraphrase detector (Reimers and Gurevych, 2019) (on the train set) to create positive and negative term-concept pairs. We then model the problem by fine-tuning the sentence-BERT-based paraphrase detector on this derived dataset, and use it as the encoder, and use a Logistic Regression classifier as the decoder, resulting in test Accuracy: 0.6 and Avg. Rank: 1.97. In case of the sentence classification task, the best-performing classifier (Accuracy: 0.92) consists of a pre-trained RoBERTa model (Liu et al., 2019a) as the encoder and a Feed Forward Neural Network classifier as the decoder.</abstract>
      <url hash="c873da98">2022.finnlp-1.34</url>
      <bibkey>dakle-etal-2022-using</bibkey>
    </paper>
  </volume>
</collection>
