<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.nejlt">
  <volume id="1" ingest-date="2023-03-03">
    <meta>
      <booktitle>Northern European Journal of Language Technology, Volume 8</booktitle>
      <editor><first>Leon</first><last>Derczynski</last></editor>
      <publisher>Northern European Association of Language Technology</publisher>
      <address>Copenhagen, Denmark</address>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.8.1</doi>
      <url hash="b4654b54">2022.nejlt-1</url>
      <year>2022</year>
      <venue>nejlt</venue>
    </meta>
    <paper id="1">
      <title>Foreword to <fixed-case>NEJLT</fixed-case> Volume 8, 2022</title>
      <author><first>Leon</first><last>Derczynski</last></author>
      <abstract>An introduction to the Northern European Journal of Language Technology in 2022</abstract>
      <url hash="e8b71a2c">2022.nejlt-1.1</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.4617</doi>
      <bibkey>derczynski-2022-foreword</bibkey>
    </paper>
    <paper id="2">
      <title>Task-dependent Optimal Weight Combinations for Static Embeddings</title>
      <author><first>Nathaniel</first><last>Robinson</last></author>
      <author><first>Nathaniel</first><last>Carlson</last></author>
      <author><first>David</first><last>Mortensen</last></author>
      <author><first>Elizabeth</first><last>Vargas</last></author>
      <author><first>Thomas</first><last>Fackrell</last></author>
      <author><first>Nancy</first><last>Fulda</last></author>
      <abstract>A variety of NLP applications use word2vec skip-gram, GloVe, and fastText word embeddings. These models learn two sets of embedding vectors, but most practitioners use only one of them, or alternately an unweighted sum of both. This is the first study to systematically explore a range of linear combinations between the first and second embedding sets. We evaluate these combinations on a set of six NLP benchmarks including IR, POS-tagging, and sentence similarity. We show that the default embedding combinations are often suboptimal and demonstrate 1.0-8.0% improvements. Notably, GloVes default unweighted sum is its least effective combination across tasks. We provide a theoretical basis for weighting one set of embeddings more than the other according to the algorithm and task. We apply our findings to improve accuracy in applications of cross-lingual alignment and navigational knowledge by up to 15.2%.</abstract>
      <url hash="ec6f2a64">2022.nejlt-1.2</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.4438</doi>
      <bibkey>robinson-etal-2022-task</bibkey>
    </paper>
    <paper id="3">
      <title>Building Analyses from Syntactic Inference in Local Languages: An <fixed-case>HPSG</fixed-case> Grammar Inference System</title>
      <author><first>Kristen</first><last>Howell</last></author>
      <author><first>Emily M.</first><last>Bender</last></author>
      <abstract>We present a grammar inference system that leverages linguistic knowledge recorded in the form of annotations in interlinear glossed text (IGT) and in a meta-grammar engineering system (the LinGO Grammar Matrix customization system) to automatically produce machine-readable HPSG grammars. Building on prior work to handle the inference of lexical classes, stems, affixes and position classes, and preliminary work on inferring case systems and word order, we introduce an integrated grammar inference system that covers a wide range of fundamental linguistic phenomena. System development was guided by 27 geneologically and geographically diverse languages, and we test the system’s cross-linguistic generalizability on an additional 5 held-out languages, using datasets provided by field linguists. Our system out-performs three baseline systems in increasing coverage while limiting ambiguity and producing richer semantic representations, while also producing richer representations than previous work in grammar inference.</abstract>
      <url hash="da808536">2022.nejlt-1.3</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.4017</doi>
      <bibkey>howell-bender-2022-building</bibkey>
    </paper>
    <paper id="4">
      <title>Bias Identification and Attribution in <fixed-case>NLP</fixed-case> Models With Regression and Effect Sizes</title>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <abstract>In recent years, there has been an increasing awareness that many NLP systems incorporate biases of various types (e.g., regarding gender or race) which can have significant negative consequences. At the same time, the techniques used to statistically analyze such biases are still relatively simple. Typically, studies test for the presence of a significant difference between two levels of a single bias variable (e.g., male vs. female) without attention to potential confounders, and do not quantify the importance of the bias variable. This article proposes to analyze bias in the output of NLP systems using multivariate regression models. They provide a robust and more informative alternative which (a) generalizes to multiple bias variables, (b) can take covariates into account, (c) can be combined with measures of effect size to quantify the size of bias. Jointly, these effects contribute to a more robust statistical analysis of bias that can be used to diagnose system behavior and extract informative examples. We demonstrate the benefits of our method by analyzing a range of current NLP models on one regression and one classification tasks (emotion intensity prediction and coreference resolution, respectively).</abstract>
      <url hash="d70ddadb">2022.nejlt-1.4</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.3505</doi>
      <bibkey>dayanik-etal-2022-analysis</bibkey>
    </paper>
    <paper id="5">
      <title>Policy-focused Stance Detection in Parliamentary Debate Speeches</title>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <abstract>Legislative debate transcripts provide citizens with information about the activities of their elected representatives, but are difficult for people to process. We propose the novel task of policy-focused stance detection, in which both the policy proposals under debate and the position of the speakers towards those proposals are identified. We adapt a previously existing dataset to include manual annotations of policy preferences, an established schema from political science. We evaluate a range of approaches to the automatic classification of policy preferences and speech sentiment polarity, including transformer-based text representations and a multi-task learning paradigm. We find that it is possible to identify the policies under discussion using features derived from the speeches, and that incorporating motion-dependent debate modelling, previously used to classify speech sentiment, also improves performance in the classification of policy preferences. We analyse the output of the best performing system, finding that discriminating features for the task are highly domain-specific, and that speeches that address policy preferences proposed by members of the same party can be among the most difficult to predict.</abstract>
      <url hash="45e6c84c">2022.nejlt-1.5</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.3454</doi>
      <bibkey>abercrombie-batista-navarro-2022-policy</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>S</fixed-case>panish <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation: Annotation of a General Corpus</title>
      <author><first>Shira</first><last>Wein</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Ethan</first><last>Ricker</last></author>
      <author><first>Calvin</first><last>Engstrom</last></author>
      <author><first>Alex</first><last>Nelson</last></author>
      <author><first>Leonie</first><last>Harter</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <abstract>Abstract Meaning Representation (AMR), originally designed for English, has been adapted to a number of languages to facilitate cross-lingual semantic representation and analysis. We build on previous work and present the first sizable, general annotation project for Spanish AMR. We release a detailed set of annotation guidelines and a corpus of 486 gold-annotated sentences spanning multiple genres from an existing, cross-lingual AMR corpus. Our work constitutes the second largest non-English gold AMR corpus to date. Fine-tuning an AMR to-Spanish generation model with our annotations results in a BERTScore improvement of 8.8%, demonstrating initial utility of our work.</abstract>
      <url hash="b8ab1c04">2022.nejlt-1.6</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.4462</doi>
      <bibkey>wein-2022-spanish</bibkey>
    </paper>
    <paper id="7">
      <title>Part-of-Speech and Morphological Tagging of <fixed-case>A</fixed-case>lgerian <fixed-case>J</fixed-case>udeo-<fixed-case>A</fixed-case>rabic</title>
      <author><first>Ofra</first><last>Tirosh-Becker</last></author>
      <author><first>Michal</first><last>Kessler</last></author>
      <author><first>Oren</first><last>Becker</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <abstract>Most linguistic studies of Judeo-Arabic, the ensemble of dialects spoken and written by Jews in Arab lands, are qualitative in nature and rely on laborious manual annotation work, and are therefore limited in scale. In this work, we develop automatic methods for morpho-syntactic tagging of Algerian Judeo-Arabic texts published by Algerian Jews in the 19th–20th centuries, based on a linguistically tagged corpus. First, we describe our semi-automatic approach for preprocessing these texts. Then, we experiment with both an off-the-shelf morphological tagger and several specially designed neural network taggers. Finally, we perform a real-world evaluation of new texts that were never tagged before in comparison with human expert annotators. Our experimental results demonstrate that these methods can dramatically speed up and improve the linguistic research pipeline, enabling linguists to study these dialects on a much greater scale.</abstract>
      <url hash="7988c04e">2022.nejlt-1.7</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.4315</doi>
      <bibkey>tirosh-becker-etal-2022-part</bibkey>
    </paper>
    <paper id="8">
      <title>Lexical variation in <fixed-case>E</fixed-case>nglish language podcasts, editorial media, and social media</title>
      <author><first>Jussi</first><last>Karlgren</last></author>
      <abstract>The study presented in this paper demonstrates how transcribed podcast material differs with respect to lexical content from other collections of English language data: editorial text, social media, both long form and microblogs, dialogue from movie scripts, and transcribed phone conversations. Most of the recorded differences are as might be expected, reflecting known or assumed difference between spoken and written language, between dialogue and soliloquy, and between scripted formal and unscripted informal language use. Most notably, podcast material, compared to the hitherto typical training sets from editorial media, is characterised by being in the present tense, and with a much higher incidence of pronouns, interjections, and negations. These characteristics are, unsurprisingly, largely shared with social media texts. Where podcast material differs from social media material is in its attitudinal content, with many more amplifiers and much less negative attitude than in blog texts. This variation, besides being of philological interest, has ramifications for computational work. Information access for material which is not primarily topical should be designed to be sensitive to such variation that defines the data set itself and discriminates items within it. In general, training sets for language models are a non-trivial parameter which are likely to show effects both expected and unexpected when applied to data from other sources and the characteristics and provenance of data used to train a model should be listed on the label as a minimal form of downstream consumer protection.</abstract>
      <url hash="6b2c2093">2022.nejlt-1.8</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.3566</doi>
      <bibkey>karlgren-2022-lexical</bibkey>
    </paper>
    <paper id="9">
      <title>Contextualized embeddings for semantic change detection: Lessons learned</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <abstract>We present a qualitative analysis of the (potentially erroneous) outputs of contextualized embedding-based methods for detecting diachronic semantic change. First, we introduce an ensemble method outperforming previously described contextualized approaches. This method is used as a basis for an in-depth analysis of the degrees of semantic change predicted for English words across 5 decades. Our findings show that contextualized methods can often predict high change scores for words which are not undergoing any real diachronic semantic shift in the lexicographic sense of the term (or at least the status of these shifts is questionable). Such challenging cases are discussed in detail with examples, and their linguistic categorization is proposed. Our conclusion is that pre-trained contextualized language models are prone to confound changes in lexicographic senses and changes in contextual variance, which naturally stem from their distributional nature, but is different from the types of issues observed in methods based on static embeddings. Additionally, they often merge together syntactic and semantic aspects of lexical entities. We propose a range of possible future solutions to these issues.</abstract>
      <url hash="db015b96">2022.nejlt-1.9</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2022.3478</doi>
      <bibkey>kutuzov-etal-2022-contextualized</bibkey>
    </paper>
  </volume>
</collection>
