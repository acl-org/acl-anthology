<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.cl">
  <volume id="1">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 1 - March 2021</booktitle>
      <month>March</month>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2021</year>
      <venue>cl</venue>
    </meta>
    <paper id="1">
      <title>Kathy <fixed-case>M</fixed-case>c<fixed-case>K</fixed-case>eown Interviews Bonnie Webber</title>
      <author><first>Bonnie</first><last>Webber</last></author>
      <doi>10.1162/coli_a_00393</doi>
      <abstract>Abstract Because the 2020 ACL Lifetime Achievement Award presentation could not be done in person, we replaced the usual LTA talk with an interview between Professor Kathy McKeown (Columbia University) and the recipient, Bonnie Webber. The following is an edited version of the interview, with added citations.</abstract>
      <pages>1–7</pages>
      <url hash="0f568337">2021.cl-1.1</url>
      <bibkey>webber-2021-kathy</bibkey>
    </paper>
    <paper id="2">
      <title>Formal Basis of a Language Universal</title>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/coli_a_00394</doi>
      <abstract>Abstract Steedman (2020) proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the “separable” permutations. This class of permutations is exactly the class that can be expressed in combinatory categorial grammars (CCGs). The excluded non-separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in word order in nominal and verbal constructions. The number of permutations that are separable grows in the number n of lexical elements in the construction as the Large Schröder Number Sn−1. Because that number grows much more slowly than the n! number of all permutations, this generalization is also of considerable practical interest for computational applications such as parsing and machine translation. The present article examines the mathematical and computational origins of this restriction, and the reason it is exactly captured in CCG without the imposition of any further constraints.</abstract>
      <pages>9–42</pages>
      <url hash="f68d3f5d">2021.cl-1.2</url>
      <bibkey>stanojevic-steedman-2021-formal</bibkey>
      <video href="2021.cl-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Comparing Knowledge-Intensive and Data-Intensive Models for <fixed-case>E</fixed-case>nglish Resource Semantic Parsing</title>
      <author><first>Junjie</first><last>Cao</last></author>
      <author><first>Zi</first><last>Lin</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <doi>10.1162/coli_a_00395</doi>
      <abstract>Abstract In this work, we present a phenomenon-oriented comparative analysis of the two dominant approaches in English Resource Semantic (ERS) parsing: classic, knowledge-intensive and neural, data-intensive models. To reflect state-of-the-art neural NLP technologies, a factorization-based parser is introduced that can produce Elementary Dependency Structures much more accurately than previous data-driven parsers. We conduct a suite of tests for different linguistic phenomena to analyze the grammatical competence of different parsers, where we show that, despite comparable performance overall, knowledge- and data-intensive models produce different types of errors, in a way that can be explained by their theoretical properties. This analysis is beneficial to in-depth evaluation of several representative parsing techniques and leads to new directions for parser development.</abstract>
      <pages>43–68</pages>
      <url hash="aee207f1">2021.cl-1.3</url>
      <bibkey>cao-etal-2021-comparing</bibkey>
    </paper>
    <paper id="4">
      <title>Semantic Data Set Construction from Human Clustering and Spatial Arrangement</title>
      <author><first>Olga</first><last>Majewska</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Jasper J. F.</first><last>van den Bosch</last></author>
      <author><first>Nikolaus</first><last>Kriegeskorte</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/coli_a_00396</doi>
      <abstract>Abstract Research into representation learning models of lexical semantics usually utilizes some form of intrinsic evaluation to ensure that the learned representations reflect human semantic judgments. Lexical semantic similarity estimation is a widely used evaluation method, but efforts have typically focused on pairwise judgments of words in isolation, or are limited to specific contexts and lexical stimuli. There are limitations with these approaches that either do not provide any context for judgments, and thereby ignore ambiguity, or provide very specific sentential contexts that cannot then be used to generate a larger lexical resource. Furthermore, similarity between more than two items is not considered. We provide a full description and analysis of our recently proposed methodology for large-scale data set construction that produces a semantic classification of a large sample of verbs in the first phase, as well as multi-way similarity judgments made within the resultant semantic classes in the second phase. The methodology uses a spatial multi-arrangement approach proposed in the field of cognitive neuroscience for capturing multi-way similarity judgments of visual stimuli. We have adapted this method to handle polysemous linguistic stimuli and much larger samples than previous work. We specifically target verbs, but the method can equally be applied to other parts of speech. We perform cluster analysis on the data from the first phase and demonstrate how this might be useful in the construction of a comprehensive verb resource. We also analyze the semantic information captured by the second phase and discuss the potential of the spatially induced similarity judgments to better reflect human notions of word similarity. We demonstrate how the resultant data set can be used for fine-grained analyses and evaluation of representation learning models on the intrinsic tasks of semantic clustering and semantic similarity. In particular, we find that stronger static word embedding methods still outperform lexical representations emerging from more recent pre-training methods, both on word-level similarity and clustering. Moreover, thanks to the data set’s vast coverage, we are able to compare the benefits of specializing vector representations for a particular type of external knowledge by evaluating FrameNet- and VerbNet-retrofitted models on specific semantic domains such as “Heat” or “Motion.”</abstract>
      <pages>69–116</pages>
      <url hash="d83daca3">2021.cl-1.4</url>
      <bibkey>majewska-etal-2021-semantic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="5">
      <title>Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve</title>
      <author><first>Oshin</first><last>Agarwal</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Byron C.</first><last>Wallace</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <doi>10.1162/coli_a_00397</doi>
      <abstract>Abstract Named entity recognition systems achieve remarkable performance on domains such as English news. It is natural to ask: What are these models actually learning to achieve this? Are they merely memorizing the names themselves? Or are they capable of interpreting the text and inferring the correct entity type from the linguistic context? We examine these questions by contrasting the performance of several variants of architectures for named entity recognition, with some provided only representations of the context as features. We experiment with GloVe-based BiLSTM-CRF as well as BERT. We find that context does influence predictions, but the main factor driving high performance is learning the named tokens themselves. Furthermore, we find that BERT is not always better at recognizing predictive contexts compared to a BiLSTM-CRF model. We enlist human annotators to evaluate the feasibility of inferring entity types from context alone and find that humans are also mostly unable to infer entity types for the majority of examples on which the context-only system made errors. However, there is room for improvement: A system should be able to recognize any named entity in a predictive context correctly and our experiments indicate that current systems may be improved by such capability. Our human study also revealed that systems and humans do not always learn the same contextual clues, and context-only systems are sometimes correct even when humans fail to recognize the entity type from the context. Finally, we find that one issue contributing to model errors is the use of “entangled” representations that encode both contextual and local token information into a single vector, which can obscure clues. Our results suggest that designing models that explicitly operate over representations of local inputs and context, respectively, may in some cases improve performance. In light of these and related findings, we highlight directions for future work.</abstract>
      <pages>117–140</pages>
      <url hash="cda87812">2021.cl-1.5</url>
      <bibkey>agarwal-etal-2021-interpretability</bibkey>
      <video href="2021.cl-1.5.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="6">
      <title>Supervised and Unsupervised Neural Approaches to Text Readability</title>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <doi>10.1162/coli_a_00398</doi>
      <abstract>Abstract We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the unsupervised setting, we leverage neural language models, whereas in the supervised setting, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages, and allows adaptation to a specific readability task and data set. By systematic comparison of several neural architectures on a number of benchmark and new labeled readability data sets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to readability, which in most cases still rely on extensive feature engineering, and propose possibilities for improvements.</abstract>
      <pages>141–179</pages>
      <url hash="266db4eb">2021.cl-1.6</url>
      <bibkey>martinc-etal-2021-supervised</bibkey>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="7">
      <title>Depth-Bounded Statistical <fixed-case>PCFG</fixed-case> Induction as a Model of Human Grammar Acquisition</title>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Lane</first><last>Schwartz</last></author>
      <author><first>Finale</first><last>Doshi-Velez</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>William</first><last>Schuler</last></author>
      <doi>10.1162/coli_a_00399</doi>
      <abstract>Abstract This article describes a simple PCFG induction model with a fixed category domain that predicts a large majority of attested constituent boundaries, and predicts labels consistent with nearly half of attested constituent labels on a standard evaluation data set of child-directed speech. The article then explores the idea that the difference between simple grammars exhibited by child learners and fully recursive grammars exhibited by adult learners may be an effect of increasing working memory capacity, where the shallow grammars are constrained images of the recursive grammars. An implementation of these memory bounds as limits on center embedding in a depth-specific transform of a recursive grammar yields a significant improvement over an equivalent but unbounded baseline, suggesting that this arrangement may indeed confer a learning advantage.</abstract>
      <pages>181–216</pages>
      <url hash="189ce55d">2021.cl-1.7</url>
      <bibkey>jin-etal-2021-depth</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="8">
      <title>Python for Linguists</title>
      <author><first>Benjamin</first><last>Roth</last></author>
      <author><first>Michael</first><last>Wiegand</last></author>
      <doi>10.1162/coli_r_00400</doi>
      <pages>217–220</pages>
      <url hash="a83157b8">2021.cl-1.8</url>
      <bibkey>roth-wiegand-2021-python</bibkey>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 2 - June 2021</booktitle>
      <month>June</month>
      <year>2021</year>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <venue>cl</venue>
    </meta>
    <paper id="9">
      <title>Approximating Probabilistic Models as Weighted Finite Automata</title>
      <author><first>Ananda Theertha</first><last>Suresh</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Michael</first><last>Riley</last></author>
      <author><first>Vlad</first><last>Schogol</last></author>
      <doi>10.1162/coli_a_00401</doi>
      <abstract>Abstract Weighted finite automata (WFAs) are often used to represent probabilistic models, such as n-gram language models, because among other things, they are efficient for recognition tasks in time and space. The probabilistic source to be represented as a WFA, however, may come in many forms. Given a generic probabilistic model over sequences, we propose an algorithm to approximate it as a WFA such that the Kullback-Leibler divergence between the source model and the WFA target model is minimized. The proposed algorithm involves a counting step and a difference of convex optimization step, both of which can be performed efficiently. We demonstrate the usefulness of our approach on various tasks, including distilling n-gram models from neural models, building compact language models, and building open-vocabulary character models. The algorithms used for these experiments are available in an open-source software library.</abstract>
      <pages>221–254</pages>
      <url hash="390ee738">2021.cl-2.9</url>
      <bibkey>suresh-etal-2021-approximating</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <doi>10.1162/coli_a_00402</doi>
      <abstract>Abstract Universal dependencies (UD) is a framework for morphosyntactic annotation of human language, which to date has been used to create treebanks for more than 100 languages. In this article, we outline the linguistic theory of the UD framework, which draws on a long tradition of typologically oriented grammatical theories. Grammatical relations between words are centrally used to explain how predicate–argument structures are encoded morphosyntactically in different languages while morphological features and part-of-speech classes give the properties of words. We argue that this theory is a good basis for crosslinguistically consistent annotation of typologically diverse languages in a way that supports computational natural language understanding as well as broader linguistic studies.</abstract>
      <pages>255–308</pages>
      <url hash="a0ea83b3">2021.cl-2.11</url>
      <bibkey>de-marneffe-etal-2021-universal</bibkey>
      <video href="2021.cl-2.11.mp4"/>
    </paper>
    <paper id="12">
      <title><fixed-case>RYANSQL</fixed-case>: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-<fixed-case>SQL</fixed-case> in Cross-Domain Databases</title>
      <author><first>DongHyun</first><last>Choi</last></author>
      <author><first>Myeong Cheol</first><last>Shin</last></author>
      <author><first>EungGyun</first><last>Kim</last></author>
      <author><first>Dong Ryeol</first><last>Shin</last></author>
      <doi>10.1162/coli_a_00403</doi>
      <abstract>Abstract Text-to-SQL is the problem of converting a user question into an SQL query, when the question and database are given. In this article, we present a neural network approach called RYANSQL (Recursively Yielding Annotation Network for SQL) to solve complex Text-to-SQL tasks for cross-domain databases. Statement Position Code (SPC) is defined to transform a nested SQL query into a set of non-nested SELECT statements; a sketch-based slot-filling approach is proposed to synthesize each SELECT statement for its corresponding SPC. Additionally, two input manipulation methods are presented to improve generation performance further. RYANSQL achieved competitive result of 58.2% accuracy on the challenging Spider benchmark. At the time of submission (April 2020), RYANSQL v2, a variant of original RYANSQL, is positioned at 3rd place among all systems and 1st place among the systems not using database content with 60.6% exact matching accuracy. The source code is available at https://github.com/kakaoenterprise/RYANSQL.</abstract>
      <pages>309–332</pages>
      <url hash="69a55600">2021.cl-2.12</url>
      <bibkey>choi-etal-2021-ryansql</bibkey>
      <video href="2021.cl-2.12.mp4"/>
      <pwccode url="https://github.com/kakaoenterprise/RYANSQL" additional="false">kakaoenterprise/RYANSQL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>C</fixed-case>ausa<fixed-case>LM</fixed-case>: Causal Model Explanation Through Counterfactual Language Models</title>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Nadav</first><last>Oved</last></author>
      <author><first>Uri</first><last>Shalit</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/coli_a_00404</doi>
      <abstract>Abstract Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning–based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1</abstract>
      <pages>333–386</pages>
      <url hash="5bd824ae">2021.cl-2.13</url>
      <bibkey>feder-etal-2021-causalm</bibkey>
      <video href="2021.cl-2.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Analysis and Evaluation of Language Models for Word Sense Disambiguation</title>
      <author><first>Daniel</first><last>Loureiro</last></author>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <doi>10.1162/coli_a_00405</doi>
      <abstract>Abstract Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.</abstract>
      <pages>387–443</pages>
      <url hash="20cc9d3c">2021.cl-2.14</url>
      <bibkey>loureiro-etal-2021-analysis</bibkey>
      <video href="2021.cl-2.14.mp4"/>
      <pwccode url="https://github.com/danlou/bert-disambiguation" additional="false">danlou/bert-disambiguation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coarsewsd-20">CoarseWSD-20</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="15">
      <title>Universal Discourse Representation Structure Parsing</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <doi>10.1162/coli_a_00406</doi>
      <abstract>Abstract We consider the task of crosslingual semantic parsing in the style of Discourse Representation Theory (DRT) where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide learning in other languages. We introduce 𝕌niversal Discourse Representation Theory (𝕌DRT), a variant of DRT that explicitly anchors semantic representations to tokens in the linguistic input. We develop a semantic parsing framework based on the Transformer architecture and utilize it to obtain semantic resources in multiple languages following two learning schemes. The many-to-one approach translates non-English text to English, and then runs a relatively accurate English parser on the translated text, while the one-to-many approach translates gold standard English to non-English text and trains multiple parsers (one per language) on the translations. Experimental results on the Parallel Meaning Bank show that our proposal outperforms strong baselines by a wide margin and can be used to construct (silver-standard) meaning banks for 99 languages.</abstract>
      <pages>445–476</pages>
      <url hash="b741f589">2021.cl-2.15</url>
      <bibkey>liu-etal-2021-universal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/groningen-meaning-bank">Groningen Meaning Bank</pwcdataset>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 3 - November 2021</booktitle>
      <month>November</month>
      <year>2021</year>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <venue>cl</venue>
    </meta>
    <paper id="16">
      <title>The Taxonomy of Writing Systems: How to Measure How Logographic a System Is</title>
      <author><first>Richard</first><last>Sproat</last></author>
      <author><first>Alexander</first><last>Gutkin</last></author>
      <doi>10.1162/coli_a_00409</doi>
      <abstract>Taxonomies of writing systems since Gelb (1952) have classified systems based on what the written symbols represent: if they represent words or morphemes, they are logographic; if syllables, syllabic; if segments, alphabetic; and so forth. Sproat (2000) and Rogers (2005) broke with tradition by splitting the logographic and phonographic aspects into two dimensions, with logography being graded rather than a categorical distinction. A system could be syllabic, and highly logographic; or alphabetic, and mostly non-logographic. This accords better with how writing systems actually work, but neither author proposed a method for measuring logography. In this article we propose a novel measure of the degree of logography that uses an attention-based sequence-to-sequence model trained to predict the spelling of a token from its pronunciation in context. In an ideal phonographic system, the model should need to attend to only the current token in order to compute how to spell it, and this would show in the attention matrix activations. In contrast, with a logographic system, where a given pronunciation might correspond to several different spellings, the model would need to attend to a broader context. The ratio of the activation outside the token and the total activation forms the basis of our measure. We compare this with a simple lexical measure, and an entropic measure, as well as several other neural models, and argue that on balance our attention-based measure accords best with intuition about how logographic various systems are. Our work provides the first quantifiable measure of the notion of logography that accords with linguistic intuition and, we argue, provides better insight into what this notion means.</abstract>
      <pages>477–528</pages>
      <url hash="e822a20d">2021.cl-3.16</url>
      <bibkey>sproat-gutkin-2021-taxonomy</bibkey>
      <video href="2021.cl-3.16.mp4"/>
    </paper>
    <paper id="17">
      <title>Syntax Role for Neural Semantic Role Labeling</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Shexia</first><last>He</last></author>
      <author><first>Jiaxun</first><last>Cai</last></author>
      <doi>10.1162/coli_a_00408</doi>
      <abstract>Semantic role labeling (SRL) is dedicated to recognizing the semantic predicate-argument structure of a sentence. Previous studies in terms of traditional models have shown syntactic information can make remarkable contributions to SRL performance; however, the necessity of syntactic information was challenged by a few recent neural SRL studies that demonstrate impressive performance without syntactic backbones and suggest that syntax information becomes much less important for neural semantic role labeling, especially when paired with recent deep neural network and large-scale pre-trained language models. Despite this notion, the neural SRL field still lacks a systematic and full investigation on the relevance of syntactic information in SRL, for both dependency and both monolingual and multilingual settings. This paper intends to quantify the importance of syntactic information for neural SRL in the deep learning framework. We introduce three typical SRL frameworks (baselines), sequence-based, tree-based, and graph-based, which are accompanied by two categories of exploiting syntactic information: syntax pruning-based and syntax feature-based. Experiments are conducted on the CoNLL-2005, -2009, and -2012 benchmarks for all languages available, and results show that neural SRL models can still benefit from syntactic information under certain conditions. Furthermore, we show the quantitative significance of syntax to neural SRL models together with a thorough empirical survey using existing models.</abstract>
      <pages>529–574</pages>
      <url hash="558ef3d1">2021.cl-3.17</url>
      <bibkey>li-etal-2021-syntax</bibkey>
      <video href="2021.cl-3.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Generalizing Cross-Document Event Coreference Resolution Across Multiple Corpora</title>
      <author><first>Michael</first><last>Bugert</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/coli_a_00407</doi>
      <abstract>Cross-document event coreference resolution (CDCR) is an NLP task in which mentions of events need to be identified and clustered throughout a collection of documents. CDCR aims to benefit downstream multidocument applications, but despite recent progress on corpora and system development, downstream improvements from applying CDCR have not been shown yet. We make the observation that every CDCR system to date was developed, trained, and tested only on a single respective corpus. This raises strong concerns on their generalizability—a must-have for downstream applications where the magnitude of domains or event mentions is likely to exceed those found in a curated corpus. To investigate this assumption, we define a uniform evaluation setup involving three CDCR corpora: ECB+, the Gun Violence Corpus, and the Football Coreference Corpus (which we reannotate on token level to make our analysis possible). We compare a corpus-independent, feature-based system against a recent neural system developed for ECB+. Although being inferior in absolute numbers, the feature-based system shows more consistent performance across all corpora whereas the neural system is hit-or-miss. Via model introspection, we find that the importance of event actions, event time, and so forth, for resolving coreference in practice varies greatly between the corpora. Additional analysis shows that several systems overfit on the structure of the ECB+ corpus. We conclude with recommendations on how to achieve generally applicable CDCR systems in the future—the most important being that evaluation on multiple CDCR corpora is strongly necessary. To facilitate future research, we release our dataset, annotation guidelines, and system implementation to the public.1</abstract>
      <pages>575–614</pages>
      <url hash="fe7cb632">2021.cl-3.18</url>
      <bibkey>bugert-etal-2021-generalizing</bibkey>
      <video href="2021.cl-3.18.mp4"/>
      <pwccode url="https://github.com/UKPLab/cdcr-beyond-corpus-tailored" additional="false">UKPLab/cdcr-beyond-corpus-tailored</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="19">
      <title>Toward Gender-Inclusive Coreference Resolution: An Analysis of Gender and Bias Throughout the Machine Learning Lifecycle*</title>
      <author><first>Yang Trista</first><last>Cao</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <doi>10.1162/coli_a_00413</doi>
      <abstract>Abstract Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systematic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and investigate where in the machine learning pipeline such biases can enter a coreference resolution system. We inspect many existing data sets for trans-exclusionary biases, and develop two new data sets for interrogating bias in both crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we will build systems that fail for: quality of service, stereotyping, and over- or under-representation, especially for binary and non-binary trans users.</abstract>
      <pages>615–661</pages>
      <url hash="46e69dc3">2021.cl-3.19</url>
      <bibkey>cao-daume-iii-2021-toward</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gicoref">GICoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/map">MAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/agender">aGender</pwcdataset>
    </paper>
    <paper id="20">
      <title>Decoding Word Embeddings with Brain-Based Semantic Features</title>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <doi>10.1162/coli_a_00412</doi>
      <abstract>Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these representations have been criticized for lacking interpretable dimensions. This property of word embeddings limits our understanding of the semantic features they actually encode. Moreover, it contributes to the “black box” nature of the tasks in which they are used, since the reasons for word embedding performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. 2016). Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the mapping in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the embedding performance and how they encode those features. This study sets itself as a step forward in understanding which aspects of meaning are captured by vector spaces, by proposing a new and simple method to carve human-interpretable semantic representations from distributional vectors.</abstract>
      <pages>663–698</pages>
      <url hash="945a1b8b">2021.cl-3.20</url>
      <bibkey>chersoni-etal-2021-decoding</bibkey>
      <video href="2021.cl-3.20.mp4"/>
    </paper>
    <paper id="22">
      <title>Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning</title>
      <author><first>Marcos</first><last>Garcia</last></author>
      <doi>10.1162/coli_r_00410</doi>
      <pages>699–701</pages>
      <url hash="4de0b165">2021.cl-3.22</url>
      <bibkey>garcia-2021-embeddings</bibkey>
    </paper>
    <paper id="23">
      <title>Understanding Dialogue: Language Use and Social Interaction</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <doi>10.1162/coli_r_00411</doi>
      <pages>703–705</pages>
      <url hash="8d1ef68e">2021.cl-3.23</url>
      <bibkey>bawden-2021-understanding</bibkey>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Computational Linguistics, Volume 47, Issue 4 - December 2021</booktitle>
      <month>December</month>
      <year>2021</year>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <venue>cl</venue>
    </meta>
    <paper id="24">
      <title>Natural Language Processing and Computational Linguistics</title>
      <author><first>Junichi</first><last>Tsujii</last></author>
      <doi>10.1162/coli_a_00420</doi>
      <pages>707–727</pages>
      <url hash="fa7d783a">2021.cl-4.24</url>
      <bibkey>tsujii-2021-natural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="25">
      <title>Detecting Local Insights from Global Labels: Supervised and Zero-Shot Sequence Labeling via a Convolutional Decomposition</title>
      <author><first>Allen</first><last>Schmaltz</last></author>
      <doi>10.1162/coli_a_00416</doi>
      <abstract>Abstract We propose a new, more actionable view of neural network interpretability and data analysis by leveraging the remarkable matching effectiveness of representations derived from deep networks, guided by an approach for class-conditional feature detection. The decomposition of the filter-n-gram interactions of a convolutional neural network (CNN) and a linear layer over a pre-trained deep network yields a strong binary sequence labeler, with flexibility in producing predictions at—and defining loss functions for—varying label granularities, from the fully supervised sequence labeling setting to the challenging zero-shot sequence labeling setting, in which we seek token-level predictions but only have document-level labels for training. From this sequence-labeling layer we derive dense representations of the input that can then be matched to instances from training, or a support set with known labels. Such introspection with inference-time decision rules provides a means, in some settings, of making local updates to the model by altering the labels or instances in the support set without re-training the full model. Finally, we construct a particular K-nearest neighbors (K-NN) model from matched exemplar representations that approximates the original model’s predictions and is at least as effective a predictor with respect to the ground-truth labels. This additionally yields interpretable heuristics at the token level for determining when predictions are less likely to be reliable, and for screening input dissimilar to the support set. In effect, we show that we can transform the deep network into a simple weighting over exemplars and associated labels, yielding an introspectable—and modestly updatable—version of the original model.</abstract>
      <pages>729–773</pages>
      <url hash="2bcf4c06">2021.cl-4.25</url>
      <bibkey>schmaltz-2021-detecting</bibkey>
      <video href="2021.cl-4.25.mp4"/>
      <pwccode url="https://github.com/allenschmaltz/exa" additional="false">allenschmaltz/exa</pwccode>
    </paper>
    <paper id="26">
      <title>Variational Deep Logic Network for Joint Inference of Entities and Relations</title>
      <author><first>Wenya</first><last>Wang</last></author>
      <author><first>Sinno Jialin</first><last>Pan</last></author>
      <doi>10.1162/coli_a_00415</doi>
      <abstract>Abstract Currently, deep learning models have been widely adopted and achieved promising results on various application domains. Despite their intriguing performance, most deep learning models function as black boxes, lacking explicit reasoning capabilities and explanations, which are usually essential for complex problems. Take joint inference in information extraction as an example. This task requires the identification of multiple structured knowledge from texts, which is inter-correlated, including entities, events, and the relationships between them. Various deep neural networks have been proposed to jointly perform entity extraction and relation prediction, which only propagate information implicitly via representation learning. However, they fail to encode the intensive correlations between entity types and relations to enforce their coexistence. On the other hand, some approaches adopt rules to explicitly constrain certain relational facts, although the separation of rules with representation learning usually restrains the approaches with error propagation. Moreover, the predefined rules are inflexible and might result in negative effects when data is noisy. To address these limitations, we propose a variational deep logic network that incorporates both representation learning and relational reasoning via the variational EM algorithm. The model consists of a deep neural network to learn high-level features with implicit interactions via the self-attention mechanism and a relational logic network to explicitly exploit target interactions. These two components are trained interactively to bring the best of both worlds. We conduct extensive experiments ranging from fine-grained sentiment terms extraction, end-to-end relation prediction, to end-to-end event extraction to demonstrate the effectiveness of our proposed method.</abstract>
      <pages>775–812</pages>
      <url hash="de9caca8">2021.cl-4.26</url>
      <bibkey>wang-pan-2021-variational</bibkey>
      <video href="2021.cl-4.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization</title>
      <author><first>Panagiotis</first><last>Kouris</last></author>
      <author><first>Georgios</first><last>Alexandridis</last></author>
      <author><first>Andreas</first><last>Stafylopatis</last></author>
      <doi>10.1162/coli_a_00417</doi>
      <abstract>Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.</abstract>
      <pages>813–859</pages>
      <url hash="57d76055">2021.cl-4.27</url>
      <bibkey>kouris-etal-2021-abstractive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="28">
      <title>The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification</title>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <doi>10.1162/coli_a_00418</doi>
      <abstract>Abstract In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on metrics that moderately correlate with human judgments on the simplicity achieved by executing specific operations (e.g., simplicity gain based on lexical replacements). In this article, we investigate how well existing metrics can assess sentence-level simplifications where multiple operations may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable data set for evaluating the correlation of metrics and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics’ scores and human judgments across three dimensions: the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.</abstract>
      <pages>861–889</pages>
      <url hash="64db7533">2021.cl-4.28</url>
      <bibkey>alva-manchego-etal-2021-un</bibkey>
      <video href="2021.cl-4.28.mp4"/>
      <pwccode url="https://github.com/feralvam/metaeval-simplification" additional="false">feralvam/metaeval-simplification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
    </paper>
    <paper id="29">
      <title>Sequence-Level Training for Non-Autoregressive Neural Machine Translation</title>
      <author><first>Chenze</first><last>Shao</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <doi>10.1162/coli_a_00421</doi>
      <abstract>Abstract In recent years, Neural Machine Translation (NMT) has achieved notable results in various translation tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency applications. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup by generating target words independently and simultaneously. Nevertheless, NAT still takes the word-level cross-entropy loss as the training objective, which is not optimal because the output of NAT cannot be properly evaluated due to the multimodality problem. In this article, we propose using sequence-level training objectives to train NAT models, which evaluate the NAT outputs as a whole and correlates well with the real translation quality. First, we propose training NAT models to optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel reinforcement algorithms customized for NAT, which outperform the conventional method by reducing the variance of gradient estimation. Second, we introduce a novel training objective for NAT models, which aims to minimize the Bag-of-N-grams (BoN) difference between the model output and the reference sentence. The BoN training objective is differentiable and can be calculated efficiently without doing any approximations. Finally, we apply a three-stage training strategy to combine these two methods to train the NAT model. We validate our approach on four translation tasks (WMT14 En↔De, WMT16 En↔Ro), which shows that our approach largely outperforms NAT baselines and achieves remarkable performance on all translation tasks. The source code is available at https://github.com/ictnlp/Seq-NAT.</abstract>
      <pages>891–925</pages>
      <url hash="0300e2fc">2021.cl-4.29</url>
      <bibkey>shao-etal-2021-sequence</bibkey>
      <pwccode url="https://github.com/ictnlp/Seq-NAT" additional="false">ictnlp/Seq-NAT</pwccode>
    </paper>
    <paper id="30">
      <title>Are Ellipses Important for Machine Translation?</title>
      <author><first>Payal</first><last>Khullar</last></author>
      <doi>10.1162/coli_a_00414</doi>
      <abstract>Abstract This article describes an experiment to evaluate the impact of different types of ellipses discussed in theoretical linguistics on Neural Machine Translation (NMT), using English to Hindi/Telugu as source and target languages. Evaluation with manual methods shows that most of the errors made by Google NMT are located in the clause containing the ellipsis, the frequency of such errors is slightly more in Telugu than Hindi, and the translation adequacy shows improvement when ellipses are reconstructed with their antecedents. These findings not only confirm the importance of ellipses and their resolution for MT, but also hint toward a possible correlation between the translation of discourse devices like ellipses with the morphological incongruity of the source and target. We also observe that not all ellipses are translated poorly and benefit from reconstruction, advocating for a disparate treatment of different ellipses in MT research.</abstract>
      <pages>927–937</pages>
      <url hash="770090b8">2021.cl-4.30</url>
      <bibkey>khullar-2021-ellipses</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>LFG</fixed-case> Generation from Acyclic <fixed-case>F</fixed-case>-Structures is <fixed-case>NP</fixed-case>-Hard</title>
      <author><first>Jürgen</first><last>Wedekind</last></author>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <doi>10.1162/coli_a_00419</doi>
      <abstract>Abstract The universal generation problem for LFG grammars is the problem of determining whether a given grammar derives any terminal string with a given f-structure. It is known that this problem is decidable for acyclic f-structures. In this brief note, we show that for those f-structures the problem is nonetheless intractable. This holds even for grammars that are off-line parsable.</abstract>
      <pages>939–946</pages>
      <url hash="3f8a673f">2021.cl-4.31</url>
      <bibkey>wedekind-kaplan-2021-lfg</bibkey>
    </paper>
  </volume>
</collection>
