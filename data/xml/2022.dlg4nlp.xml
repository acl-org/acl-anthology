<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.dlg4nlp">
  <volume id="1" ingest-date="2022-06-29" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022)</booktitle>
      <editor><first>Lingfei</first><last>Wu</last></editor>
      <editor><first>Bang</first><last>Liu</last></editor>
      <editor><first>Rada</first><last>Mihalcea</last></editor>
      <editor><first>Jian</first><last>Pei</last></editor>
      <editor><first>Yue</first><last>Zhang</last></editor>
      <editor><first>Yunyao</first><last>Li</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, Washington</address>
      <month>July</month>
      <year>2022</year>
      <url hash="1b6955ef">2022.dlg4nlp-1</url>
      <venue>dlg4nlp</venue>
    </meta>
    <frontmatter>
      <url hash="0ec815e5">2022.dlg4nlp-1.0</url>
      <bibkey>dlg4nlp-2022-deep</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>1-11</pages>
      <abstract>Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.</abstract>
      <url hash="0c161066">2022.dlg4nlp-1.1</url>
      <bibkey>yu-etal-2022-diversifying-content</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.1</doi>
      <pwccode url="https://github.com/DM2-ND/MoKGE" additional="false">DM2-ND/MoKGE</pwccode>
    </paper>
    <paper id="2">
      <title>Improving Neural Machine Translation with the <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation by Combining Graph and Sequence Transformers</title>
      <author><first>Changmao</first><last>Li</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <pages>12-21</pages>
      <abstract>Previous studies have shown that the Abstract Meaning Representation (AMR) can improve Neural Machine Translation (NMT). However, there has been little work investigating incorporating AMR graphs into Transformer models. In this work, we propose a novel encoder-decoder architecture which augments the Transformer model with a Heterogeneous Graph Transformer (Yao et al., 2020) which encodes source sentence AMR graphs. Experimental results demonstrate the proposed model outperforms the Transformer model and previous non-Transformer based models on two different language pairs in both the high resource setting and low resource setting. Our source code, training corpus and released models are available at <url>https://github.com/jlab-nlp/amr-nmt</url>.</abstract>
      <url hash="cb658ae4">2022.dlg4nlp-1.2</url>
      <bibkey>li-flanigan-2022-improving</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.2</doi>
      <pwccode url="https://github.com/jlab-nlp/amr-nmt" additional="false">jlab-nlp/amr-nmt</pwccode>
    </paper>
    <paper id="3">
      <title>Continuous Temporal Graph Networks for Event-Based Graph Data</title>
      <author><first>Jin</first><last>Guo</last></author>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Su</first><last>Zhou</last></author>
      <author><first>Jiliang</first><last>Li</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <author><first>Yuyi</first><last>Wang</last></author>
      <pages>22-29</pages>
      <abstract>There has been an increasing interest in modeling continuous-time dynamics of temporal graph data. Previous methods encode time-evolving relational information into a low-dimensional representation by specifying discrete layers of neural networks, while real-world dynamic graphs often vary continuously over time. Hence, we propose Continuous Temporal Graph Networks (CTGNs) to capture continuous dynamics of temporal graph data. We use both the link starting timestamps and link duration as evolving information to model continuous dynamics of nodes. The key idea is to use neural ordinary differential equations (ODE) to characterize the continuous dynamics of node representations over dynamic graphs. We parameterize ordinary differential equations using a novel graph neural network. The existing dynamic graph networks can be considered as a specific discretization of CTGNs. Experiment results on both transductive and inductive tasks demonstrate the effectiveness of our proposed approach over competitive baselines.</abstract>
      <url hash="97dd4a8b">2022.dlg4nlp-1.3</url>
      <bibkey>guo-etal-2022-continuous</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit">Reddit</pwcdataset>
    </paper>
    <paper id="4">
      <title>Scene Graph Parsing via <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation in Pre-trained Language Models</title>
      <author><first>Woo Suk</first><last>Choi</last></author>
      <author><first>Yu-Jung</first><last>Heo</last></author>
      <author><first>Dharani</first><last>Punithan</last></author>
      <author><first>Byoung-Tak</first><last>Zhang</last></author>
      <pages>30-35</pages>
      <abstract>In this work, we propose the application of abstract meaning representation (AMR) based semantic parsing models to parse textual descriptions of a visual scene into scene graphs, which is the first work to the best of our knowledge. Previous works examined scene graph parsing from textual descriptions using dependency parsing and left the AMR parsing approach as future work since sophisticated methods are required to apply AMR. Hence, we use pre-trained AMR parsing models to parse the region descriptions of visual scenes (i.e. images) into AMR graphs and pre-trained language models (PLM), BART and T5, to parse AMR graphs into scene graphs. The experimental results show that our approach explicitly captures high-level semantics from textual descriptions of visual scenes, such as objects, attributes of objects, and relationships between objects. Our textual scene graph parsing approach outperforms the previous state-of-the-art results by 9.3% in the SPICE metric score.</abstract>
      <url hash="fcc4bc61">2022.dlg4nlp-1.4</url>
      <bibkey>choi-etal-2022-scene</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="5">
      <title>Graph Neural Networks for Adapting Off-the-shelf General Domain Language Models to Low-Resource Specialised Domains</title>
      <author><first>Merieme</first><last>Bouhandi</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <pages>36-42</pages>
      <abstract>Language models encode linguistic proprieties and are used as input for more specific models. Using their word representations as-is for specialised and low-resource domains might be less efficient. Methods of adapting them exist, but these models often overlook global information about how words, terms, and concepts relate to each other in a corpus due to their strong reliance on attention. We consider that global information can influence the results of the downstream tasks, and combination with contextual information is performed using graph convolution networks or GCN built on vocabulary graphs. By outperforming baselines, we show that this architecture is profitable for domain-specific tasks.</abstract>
      <url hash="7747fea7">2022.dlg4nlp-1.5</url>
      <bibkey>bouhandi-etal-2022-graph</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>G</fixed-case>ra<fixed-case>DA</fixed-case>: Graph Generative Data Augmentation for Commonsense Reasoning</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>43-59</pages>
      <abstract>Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions. Our approach improves performance for SocialIQA, CODAH, HellaSwag and CommonsenseQA, and works well for generative tasks like ProtoQA. We show improvement in robustness to semantic adversaries after training with GraDA and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.</abstract>
      <url hash="c6d9c65c">2022.dlg4nlp-1.6</url>
      <bibkey>maharana-bansal-2022-grada</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>L</fixed-case>i<fixed-case>GCN</fixed-case>: Label-interpretable Graph Convolutional Networks for Multi-label Text Classification</title>
      <author><first>Irene</first><last>Li</last></author>
      <author><first>Aosong</first><last>Feng</last></author>
      <author><first>Hao</first><last>Wu</last></author>
      <author><first>Tianxiao</first><last>Li</last></author>
      <author><first>Toyotaro</first><last>Suzumura</last></author>
      <author><first>Ruihai</first><last>Dong</last></author>
      <pages>60-70</pages>
      <abstract>Multi-label text classification (MLTC) is an attractive and challenging task in natural language processing (NLP). Compared with single-label text classification, MLTC has a wider range of applications in practice. In this paper, we propose a label-interpretable graph convolutional network model to solve the MLTC problem by modeling tokens and labels as nodes in a heterogeneous graph. In this way, we are able to take into account multiple relationships including token-level relationships. Besides, the model allows better interpretability for predicted labels as the token-label edges are exposed. We evaluate our method on four real-world datasets and it achieves competitive scores against selected baseline methods. Specifically, this model achieves a gain of 0.14 on the F1 score in the small label set MLTC, and 0.07 in the large label set scenario.</abstract>
      <url hash="14920186">2022.dlg4nlp-1.7</url>
      <bibkey>li-etal-2022-ligcn</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.7</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
    </paper>
    <paper id="8">
      <title>Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering</title>
      <author><first>Zhenyun</first><last>Deng</last></author>
      <author><first>Yonghua</first><last>Zhu</last></author>
      <author><first>Qianqian</first><last>Qi</last></author>
      <author><first>Michael</first><last>Witbrock</last></author>
      <author><first>Patricia</first><last>Riddle</last></author>
      <pages>71-80</pages>
      <abstract>Current graph-neural-network-based (GNN-based) approaches to multi-hop questions integrate clues from scattered paragraphs in an entity graph, achieving implicit reasoning by synchronous update of graph node representations using information from neighbours; this is poorly suited for explaining how clues are passed through the graph in hops. In this paper, we describe a structured Knowledge and contextual Information Fusion GNN (KIFGraph) whose explicit multi-hop graph reasoning mimics human step by step reasoning. Specifically, we first integrate clues at multiple levels of granularity (question, paragraph, sentence, entity) as nodes in the graph, connected by edges derived using structured semantic knowledge, then use a contextual encoder to obtain the initial node representations, followed by step-by-step two-stage graph reasoning that asynchronously updates node representations. Each node can be related to its neighbour nodes through fused structured knowledge and contextual information, reliably integrating their answer clues. Moreover, a masked attention mechanism (MAM) filters out noisy or redundant nodes and edges, to avoid ineffective clue propagation in graph reasoning. Experimental results show performance competitive with published models on the HotpotQA dataset.</abstract>
      <url hash="b564e7f5">2022.dlg4nlp-1.8</url>
      <bibkey>deng-etal-2022-explicit</bibkey>
      <doi>10.18653/v1/2022.dlg4nlp-1.8</doi>
      <pwccode url="https://github.com/tswinggg/kifgraph" additional="false">tswinggg/kifgraph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
  </volume>
</collection>
