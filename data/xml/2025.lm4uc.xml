<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.lm4uc">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Language Models for Underserved Communities (LM4UC 2025)</booktitle>
      <editor><first>Duc</first><last>Nguyen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="9665b6fe">2025.lm4uc-1</url>
      <venue>lm4uc</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-242-8</isbn>
    </meta>
    <frontmatter>
      <url hash="f5904f22">2025.lm4uc-1.0</url>
      <bibkey>lm4uc-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Enhance Contextual Learning in <fixed-case>ASR</fixed-case> for Endangered Low-resource Languages</title>
      <author><first>Zhaolin</first><last>Li</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <pages>1-7</pages>
      <abstract>Automatic Speech Recognition (ASR) facilitates documenting endangered low-resource languages. While recent advances in acoustic modelling have been substantial, contextual learning remains underexplored. This study investigates the main factors that influence the integration of knowledge from language models (LMs) into state-of-the-art ASR models for endangered low-resource languages. Through experiments on five diverse low-resource languages, we find: 1) Fine-grained tokenization effectively improves ASR performance by addressing the prevalent unknown words and improving data usage efficiency; 2) The integration of transformer-based LMs into ASR systems surpasses that of N-gram LMs only in one language, even though they consistently achieve better results in language modelling tasks. 3) ASR performance is highly sensitive to language-specific optimization, as shown by a 43% performance degradation in one language due to parameter transfer across languages. We open-source our scripts to support further research and applications.</abstract>
      <url hash="57fd5c7f">2025.lm4uc-1.1</url>
      <bibkey>li-niehues-2025-enhance</bibkey>
    </paper>
    <paper id="2">
      <title>Empowering Low-Resource Languages: <fixed-case>T</fixed-case>ra<fixed-case>S</fixed-case>e Architecture for Enhanced Retrieval-Augmented Generation in <fixed-case>B</fixed-case>angla</title>
      <author><first>Atia Shahnaz</first><last>Ipa</last><affiliation>Khulna University of Engineering &amp; Technology</affiliation></author>
      <author><first>Mohammad Abu Tareq</first><last>Rony</last><affiliation>Noakhali Science &amp; Technology University</affiliation></author>
      <author><first>Mohammad Shariful</first><last>Islam</last><affiliation>Noakhali Science &amp; Technology University</affiliation></author>
      <pages>8-15</pages>
      <abstract>Research on Retrieval-Augmented Generation for low-resource languages has been sparse because of limited resources. To address this, we focus on Bangla, a low-resource language, and have created a dataset of 200 question-answer pairs as a basis for our study from Bangla Wikipedia dumps data. This paper introduces the TraSe architecture, which enhances RAG for Bangla using Translative prompting. Our experiments demonstrate that TraSe improves answer selection accuracy, achieving 34% with automatic retrieval and 63% with Human-in-the-Loop retrieval, outperforming baseline methods. The TraSe architecture marks a significant advancement in RAG for low-resource languages and has the potential to enhance question-answering systems for Bangla and similar languages. Future research could explore additional low-resource languages. The code is available at the following GitHub repository: https://github.com/Atia6/TraSe-Bangla-RAG.</abstract>
      <url hash="86712547">2025.lm4uc-1.2</url>
      <bibkey>ipa-etal-2025-empowering</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>ABDUL</fixed-case>: A New Approach to Build Language Models for Dialects Using Formal Language Corpora Only</title>
      <author><first>Yassine</first><last>Toughrai</last><affiliation>Université de Lorraine</affiliation></author>
      <author><first>Kamel</first><last>Smaïli</last><affiliation>Université de Lorraine</affiliation></author>
      <author><first>David</first><last>Langlois</last><affiliation>Université de Lorraine</affiliation></author>
      <pages>16-21</pages>
      <abstract>Arabic dialects present major challenges for natural language processing (NLP) due to their diglossic nature, phonetic variability, and the scarcity of resources. To address this, we introduce a phoneme-like transcription approach that enables the training of robust language models for North African Dialects (NADs) using only formal language data, without the need for dialect-specific corpora.Our key insight is that Arabic dialects are highly phonetic, with NADs particularly influenced by European languages. This motivated us to develop a novel approach in which we convert Arabic script into a Latin-based representation, allowing our language model, ABDUL, to benefit from existing Latin-script corpora.Our method demonstrates strong performance in multi-label emotion classification and named entity recognition (NER) across various Arabic dialects. ABDUL achieves results comparable to or better than specialized and multilingual models such as DarijaBERT, DziriBERT, and mBERT. Notably, in the NER task, ABDUL outperforms mBERT by 5% in F1-score for Modern Standard Arabic (MSA), Moroccan, and Algerian Arabic, despite using a vocabulary four times smaller than mBERT.</abstract>
      <url hash="90f66e4f">2025.lm4uc-1.3</url>
      <bibkey>toughrai-etal-2025-abdul</bibkey>
    </paper>
    <paper id="4">
      <title>Untangling the Influence of Typology, Data, and Model Architecture on Ranking Transfer Languages for Cross-Lingual <fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Enora</first><last>Rice</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Ali</first><last>Marashian</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Hannah</first><last>Haynie</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Katharina</first><last>Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>22-31</pages>
      <abstract>Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge. The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood. We take a holistic approach, examining how both dataset-specific and fine-grained typological features influence transfer language selection for part-of-speech tagging, considering two different sources for morphosyntactic features. While previous work examines these dynamics in the context of bilingual biLSTMS, we extend our analysis to a more modern transfer learning pipeline: zero-shot prediction with pretrained multilingual models. We train a series of transfer language ranking systems and examine how different feature inputs influence ranker performance across architectures. Word overlap, type-token ratio, and genealogical distance emerge as top features across all architectures. Our findings reveal that a combination of typological and dataset-dependent features leads to the best rankings, and that good performance can be obtained with either feature group on its own.</abstract>
      <url hash="9a67214c">2025.lm4uc-1.4</url>
      <bibkey>rice-etal-2025-untangling</bibkey>
    </paper>
    <paper id="5">
      <title>Serving the Underserved: Leveraging <fixed-case>BARTB</fixed-case>ahnar Language Model for Bahnaric-<fixed-case>V</fixed-case>ietnamese Translation</title>
      <author><first>Long</first><last>Nguyen</last><affiliation>Ho Chi Minh City University of Technology - VNU-HCM</affiliation></author>
      <author><first>Tran</first><last>Le</last><affiliation>Ho Chi Minh City University of Technology - VNU-HCM</affiliation></author>
      <author><first>Huong</first><last>Nguyen</last><affiliation>Ho Chi Minh City University of Technology - VNU-HCM</affiliation></author>
      <author><first>Quynh</first><last>Vo</last><affiliation>Ho Chi Minh City University of Technology - VNU-HCM</affiliation></author>
      <author><first>Phong</first><last>Nguyen</last><affiliation>Ho Chi Minh City University of Technology - VNU-HCM</affiliation></author>
      <author><first>Tho</first><last>Quan</last><affiliation>Ho Chi Minh City University of Technology - VNU-HCM</affiliation></author>
      <pages>32-41</pages>
      <abstract>The Bahnar people, one of Vietnam’s ethnic minorities, represent an underserved community with limited access to modern technologies. Developing an effective Bahnaric-Vietnamese translation system is essential for fostering linguistic exchange, preserving cultural heritage, and empowering local communities by bridging communication barriers. With advancements in Artificial Intelligence (AI), Neural Machine Translation (NMT) has achieved remarkable success across various language pairs. However, the low-resource nature of Bahnaric, characterized by data scarcity, vocabulary constraints, and the lack of parallel corpora, poses significant challenges to building an accurate and efficient translation system. To address these challenges, we propose a novel hybrid architecture for Bahnaric-Vietnamese translation, with BARTBahnar as its core language model. BARTBahnar is developed by continually training a pre-trained Vietnamese model, BARTPho, on augmented monolingual Bahnaric data, followed by fine-tuning on bilingual datasets. This transfer learning approach reduces training costs while effectively capturing linguistic similarities between the two languages. Additionally, we implement advanced data augmentation techniques to enrich and diversify training data, further enhancing BARTBahnar’s robustness and translation accuracy. Beyond leveraging the language model, our hybrid system integrates rule-based and statistical methods to improve translation quality. Experimental results show substantial improvements on bilingual Bahnaric-Vietnamese datasets, validating the effectiveness of our approach for low-resource translation. To support further research, we open-source our code and related materials at https://github.com/ura-hcmut/BARTBahnar.</abstract>
      <url hash="5fbb5417">2025.lm4uc-1.5</url>
      <bibkey>nguyen-etal-2025-serving</bibkey>
    </paper>
    <paper id="6">
      <title>Caption Generation in Cultural Heritage: Crowdsourced Data and Tuning Multimodal Large Language Models</title>
      <author><first>Artem</first><last>Reshetnikov</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Maria-Cristina</first><last>Marinescu</last><affiliation>Universitat Ramon Llull</affiliation></author>
      <pages>42-50</pages>
      <abstract>Automated caption generation for paintings enables enhanced access and understanding of visual artworks. This work introduces a novel caption dataset, obtained by manual annotation of about 7500 images from the publicly available DEArt dataset for object detection and pose estimation. Our focus is on describing the visual scenes rather than the context or style of the artwork - more common in other existing captioning datasets. The dataset is the result of a crowdsourcing initiative spanning 13 months, with volunteers adhering to explicit captioning guidelines reflecting our requirements. We provide each artwork in the dataset with five captions, created independently by volunteers to ensure diversity of interpretation and increase the robustness of the captioning model. In addition, we explore using the crowdsourced dataset for fine-tuning Large Language Models with vision encoders for domain-specific caption generation. The goal is to improve the performance of multimodal LLMs in the context of cultural heritage, a domain with “small data” which often struggles with the nuanced visual analysis and interpretation required for cultural objects such as paintings. The use of crowdsourced data in the domain adaptation process enables us to incorporate the collective perceptual insights of diverse annotators, resulting in an exploration of visual narratives and observing a reduction in hallucinations otherwise produced by these large language models.</abstract>
      <url hash="92596a1f">2025.lm4uc-1.6</url>
      <bibkey>reshetnikov-marinescu-2025-caption</bibkey>
    </paper>
    <paper id="7">
      <title>Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent <fixed-case>AI</fixed-case> Systems</title>
      <author><first>Mahfuz</first><last>Anik</last><affiliation>hahjalal University of Science and Technology</affiliation></author>
      <author><first>Abdur</first><last>Rahman</last><affiliation>hahjalal University of Science and Technology</affiliation></author>
      <author><first>Azmine</first><last>Wasi</last><affiliation>Shahjalal University of Science and Technology</affiliation></author>
      <author><first>Md</first><last>Ahsan</last><affiliation>University of Oklahoma</affiliation></author>
      <pages>51-60</pages>
      <abstract>Language is a cornerstone of cultural identity, yet globalization and the dominance of major languages have placed nearly 3,000 languages at risk of extinction. Existing AI-driven translation models prioritize efficiency but often fail to capture cultural nuances, idiomatic expressions, and historical significance, leading to translations that marginalize linguistic diversity. To address these challenges, we propose a multi-agent AI framework designed for culturally adaptive translation in underserved language communities. Our approach leverages specialized agents for translation, interpretation, content synthesis, and bias evaluation, ensuring that linguistic accuracy and cultural relevance are preserved. Using CrewAI and LangChain, our system enhances contextual fidelity while mitigating biases through external validation. Comparative analysis shows that our framework outperforms GPT-4o, producing contextually rich and culturally embedded translations—a critical advancement for Indigenous, regional, and low-resource languages. This research underscores the potential of multi-agent AI in fostering equitable, sustainable, and culturally sensitive NLP technologies, aligning with the AI Governance, Cultural NLP, and Sustainable NLP pillars of Language Models for Underserved Communities. Our full experimental codebase is publicly avail able at: github.com/ciol-researchlab/Context-Aware_Translation_MAS.</abstract>
      <url hash="9e501108">2025.lm4uc-1.7</url>
      <bibkey>anik-etal-2025-preserving</bibkey>
    </paper>
    <paper id="8">
      <title>Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</title>
      <author><first>Fred</first><last>Philippy</last><affiliation>Zortify Labs</affiliation></author>
      <author><first>Siwen</first><last>Guo</last><affiliation>Zortify Labs</affiliation></author>
      <author><first>Cedric</first><last>Lothritz</last><affiliation>Luxembourg Institute of Science and Technology</affiliation></author>
      <author><first>Jacques</first><last>Klein</last><affiliation>University of Luxembourg</affiliation></author>
      <author><first>Tegawendé</first><last>Bissyandé</last><affiliation>University of Luxembourg</affiliation></author>
      <pages>61-75</pages>
      <abstract>In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios.Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from related classification tasks, especially when these datasets originate from different languages or distributions. Moreover, existing prompt-based methods typically rely on manually crafted prompts in a specific language, limiting their adaptability and effectiveness in cross-lingual settings.To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts. RoSPrompt is designed for small multilingual PLMs, enabling them to leverage high-resource languages to improve performance in low-resource settings without requiring extensive fine-tuning or high computational costs. We evaluate our approach on multiple multilingual PLMs across datasets covering 106 languages, demonstrating strong cross-lingual transfer performance and robust generalization capabilities over unseen classes.</abstract>
      <url hash="65235208">2025.lm4uc-1.8</url>
      <bibkey>philippy-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="9">
      <title>Cognate and Contact-Induced Transfer Learning for Hamshentsnag: A Low-Resource and Endangered Language</title>
      <author><first>Onur</first><last>Keleş</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Baran</first><last>Günay</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Berat</first><last>Doğan</last><affiliation>Boğaziçi University</affiliation></author>
      <pages>76-85</pages>
      <abstract>This study investigates zero-shot and few-shot cross-lingual transfer effects in Part-of-Speech (POS) tagging and Named Entity Recognition (NER) for Hamshentsnag, an endangered Western Armenian dialect. We examine how different source languages, Western Armenian (contact cognate), Eastern Armenian (ancestral cognate), Turkish (substrate or contact-induced), and English (non-cognate), affect the task performance using multilingual BERT and BERTurk. Results show that cognate varieties improved POS tagging by 8% F1, while the substrate source enhanced NER by 15% F1. BERTurk outperformed mBERT on NER but not on POS. We attribute this to task-specific advantages of different source languages. We also used script conversion and phonetic alignment with the target for non-Latin scripts, which alleviated transfer.</abstract>
      <url hash="f50afd2c">2025.lm4uc-1.9</url>
      <bibkey>keles-etal-2025-cognate</bibkey>
    </paper>
    <paper id="11">
      <title>Nayana <fixed-case>OCR</fixed-case>: A Scalable Framework for Document <fixed-case>OCR</fixed-case> in Low-Resource Languages</title>
      <author><first>Adithya</first><last>Kolavi</last><affiliation>CognitiveLab</affiliation></author>
      <author><first>Samarth</first><last>P</last><affiliation>CognitiveLab</affiliation></author>
      <author><first>Vyoman</first><last>Jain</last><affiliation>CognitiveLab</affiliation></author>
      <pages>86-103</pages>
      <abstract>We introduce Nayana, a scalable and efficient framework for adapting Vision-Language Models (VLMs) to low-resource languages. Despite significant advances, modern VLMs remain constrained by the scarcity of training data in non-English languages, limiting their global applicability. Our framework addresses this fundamental challenge through a novel layout-aware synthetic data generation pipeline combined with parameter-efficient adaptation techniques. Instead of requiring extensive manually annotated datasets, Nayana enables existing models to learn new languages effectively using purely synthetic data. Using Low-Rank Adaptation (LoRA), we demonstrate this capability across ten Indic languages: Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, and Telugu. Through extensive experiments in OCR tasks, we show that models can achieve strong performance in new languages without the traditional requirements of large-scale annotated datasets or extensive model modifications. Nayana’s success in adapting VLMs to new languages with synthetic data establishes a practical pathway for extending AI capabilities to underserved languages, particularly in scenarios where annotated data is scarce or unavailable.</abstract>
      <url hash="eae135e8">2025.lm4uc-1.11</url>
      <bibkey>kolavi-etal-2025-nayana</bibkey>
    </paper>
    <paper id="12">
      <title>On Tables with Numbers, with Numbers</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last><affiliation>Aalto University</affiliation></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last><affiliation>University of Crete</affiliation></author>
      <pages>104-115</pages>
      <abstract>This paper is a critical reflection on the epistemic culture of contemporary computational linguistics, framed in the context of its growing obsession with tables with numbers. We argue against tables with numbers on the basis of their epistemic irrelevance, their environmental impact, their role in enabling and exacerbating social inequalities, and their deep ties to commercial applications and profit-driven research. We substantiate our arguments with empirical evidence drawn from a meta-analysis of computational linguistics research over the last decade.</abstract>
      <url hash="a5829acf">2025.lm4uc-1.12</url>
      <bibkey>kogkalidis-chatzikyriakidis-2025-tables</bibkey>
    </paper>
  </volume>
</collection>
