<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.climatenlp">
  <volume id="1" ingest-date="2024-07-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024)</booktitle>
      <editor><first>Dominik</first><last>Stammbach</last></editor>
      <editor><first>Jingwei</first><last>Ni</last></editor>
      <editor><first>Tobias</first><last>Schimanski</last></editor>
      <editor><first>Kalyan</first><last>Dutia</last></editor>
      <editor><first>Alok</first><last>Singh</last></editor>
      <editor><first>Julia</first><last>Bingler</last></editor>
      <editor><first>Christophe</first><last>Christiaen</last></editor>
      <editor><first>Neetu</first><last>Kushwaha</last></editor>
      <editor><first>Veruska</first><last>Muccione</last></editor>
      <editor><first>Saeid</first><last>A. Vaghefi</last></editor>
      <editor><first>Markus</first><last>Leippold</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="0f25cbce">2024.climatenlp-1</url>
      <venue>climatenlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="136233ad">2024.climatenlp-1.0</url>
      <bibkey>climatenlp-1-2024</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Climate Policy Transformer: Utilizing <fixed-case>NLP</fixed-case> to track the coherence of Climate Policy Documents in the Context of the <fixed-case>P</fixed-case>aris Agreement</title>
      <author><first>Prashant</first><last>Singh</last><affiliation>Deutsche Gesellschaft für Internationale Zusammenarbeit</affiliation></author>
      <author><first>Erik</first><last>Lehmann</last><affiliation>Gesellschaft für Internationale Zusammenarbeit</affiliation></author>
      <author><first>Mark</first><last>Tyrrell</last><affiliation>Gesellschaft für Internationale Zusammenarbeit</affiliation></author>
      <pages>1-11</pages>
      <abstract>Climate policy implementation is pivotal inglobal efforts to mitigate and adapt to climatechange. In this context, this paper explores theuse of Natural Language Processing (NLP) as atool for policy advisors to efficiently track andassess climate policy and strategies, such asNationally Determined Contributions (NDCs).These documents are essential for monitoringcoherence with the Paris Agreement, yet theiranalysis traditionally demands significant la-bor and time. We demonstrate how to leverageNLP on existing climate policy databases totransform this process by structuring informa-tion extracted from these otherwise unstruc-tured policy documents and opening avenuesfor a more in-depth analysis of national and re-gional policies. Central to our approach is thecreation of a machine-learning (ML) dataset’CPo-CD’, based on data provided by the Inter-national Climate Initiative (IKI) and ClimateWatch (CW). The CPo-CD dataset is utilizedto fine-tune Transformer Models on classify-ing climate targets, actions, policies, and plans,along with their sector, mitigation-adaptation,and greenhouse gas (GHG) components. Wepublish our model and dataset on a HuggingFace repository.</abstract>
      <url hash="00343d35">2024.climatenlp-1.1</url>
      <bibkey>singh-etal-2024-climate</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Informing climate risk analysis using textual information - A research agenda</title>
      <author><first>Andreas</first><last>Dimmelmeier</last><affiliation>Ludwig-Maximilians-Universitüt München</affiliation></author>
      <author><first>Hendrik</first><last>Doll</last><affiliation>Deutsche Bundesbank</affiliation></author>
      <author><first>Malte</first><last>Schierholz</last><affiliation>Ludwig-Maximilians-University Munich</affiliation></author>
      <author><first>Emily</first><last>Kormanyos</last><affiliation>Deutsche Bundesbank</affiliation></author>
      <author><first>Maurice</first><last>Fehr</last><affiliation>Deutsche Bundesbank</affiliation></author>
      <author><first>Bolei</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universitüt München</affiliation></author>
      <author><first>Jacob</first><last>Beck</last><affiliation>Ludwig-Maximilians-Universitüt München</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Frauke</first><last>Kreuter</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>12-26</pages>
      <abstract>We present a research agenda focused on efficiently extracting, assuring quality, and consolidating textual company sustainability information to address urgent climate change decision-making needs. Starting from the goal to create integrated FAIR (Findable, Accessible, Interoperable, Reusable) climate-related data, we identify research needs pertaining to the technical aspects of information extraction as well as to the design of the integrated sustainability datasets that we seek to compile. Regarding extraction, we leverage technological advancements, particularly in large language models (LLMs) and Retrieval-Augmented Generation (RAG) pipelines, to unlock the underutilized potential of unstructured textual information contained in corporate sustainability reports. In applying these techniques, we review key challenges, which include the retrieval and extraction of CO<tex-math>_2</tex-math> emission values from PDF documents, especially from unstructured tables and graphs therein, and the validation of automatically extracted data through comparisons with human-annotated values. We also review how existing use cases and practices in climate risk analytics relate to choices of what textual information should be extracted and how it could be linked to existing structured data.</abstract>
      <url hash="53d2d1ca">2024.climatenlp-1.2</url>
      <bibkey>dimmelmeier-etal-2024-informing</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>My Climate Advisor: An Application of <fixed-case>NLP</fixed-case> in Climate Adaptation for Agriculture</title>
      <author><first>Vincent</first><last>Nguyen</last></author>
      <author><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <author><first>Willow</first><last>Hallgren</last><affiliation>CSIRO</affiliation></author>
      <author><first>Ashley</first><last>Harkin</last></author>
      <author><first>Mahesh</first><last>Prakash</last></author>
      <pages>27-45</pages>
      <abstract>Climate adaptation in the agricultural sector necessitates tools that equip farmers and farm advisors with relevant and trustworthy information to help increase their resiliency to climate change. We introduce <i>My Climate Advisor</i>, a question-answering (QA) prototype that synthesises information from different data sources, such as peer-reviewed scientific literature and high-quality, industry-relevant grey literature to generate answers, with references, to a given user’s question. Our prototype uses open-source generative models for data privacy and intellectual property protection, and retrieval augmented generation for answer generation, grounding and provenance. While there are standard evaluation metrics for QA systems, no existing evaluation framework suits our LLM-based QA application in the climate adaptation domain. We design an evaluation framework with seven metrics based on the requirements of the domain experts to judge the generated answers from 12 different LLM-based models. Our initial evaluations through a user study via domain experts show promising usability results.</abstract>
      <url hash="f16f2b60">2024.climatenlp-1.3</url>
      <bibkey>nguyen-etal-2024-climate</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Generative Debunking of Climate Misinformation</title>
      <author><first>Francisco</first><last>Zanartu</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Yulia</first><last>Otmakhova</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>John</first><last>Cook</last></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>University of Melbourne</affiliation></author>
      <pages>46-62</pages>
      <abstract>Misinformation about climate change causes numerous negative impacts, necessitating corrective responses. Psychological research has offered various strategies for reducing the influence of climate misinformation, such as the fact-myth-fallacy-fact-structure. However, practically implementing corrective interventions at scale represents a challenge. Automatic detection and correction of misinformation offers a solution to the misinformation problem. This study documents the development of large language models that accept as input a climate myth and produce a debunking that adheres to the fact-myth-fallacy-fact (“truth sandwich”) structure, by incorporating contrarian claim classification and fallacy detection into an LLM prompting framework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with prompting strategies of varying complexity. Experiments reveal promising performance of GPT-4 and Mixtral if combined with structured prompts. We identify specific challenges of debunking generation and human evaluation, and map out avenues for future work. We release a dataset of high-quality truth-sandwich debunkings, source code and a demo of the debunking system.</abstract>
      <url hash="3f891a7d">2024.climatenlp-1.4</url>
      <bibkey>zanartu-etal-2024-generative</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics</title>
      <author><first>Ruiran</first><last>Su</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Janet</first><last>Pierrehumbert</last><affiliation>University of Oxford</affiliation></author>
      <pages>63-81</pages>
      <abstract>This paper presents the ClimateSent-GAT Model, a novel approach that combines Graph Attention Networks (GATs) with natural language processing techniques to accurately identify and predict disagreements within Reddit comment-reply pairs. Our model classifies disagreements into three categories: agree, disagree, and neutral. Leveraging the inherent graph structure of Reddit comment-reply pairs, the model significantly outperforms existing benchmarks by capturing complex interaction patterns and sentiment dynamics. This research advances graph-based NLP methodologies and provides actionable insights for policymakers and educators in climate science communication.</abstract>
      <url hash="7fc38721">2024.climatenlp-1.5</url>
      <bibkey>su-pierrehumbert-2024-decoding</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Evaluating <fixed-case>C</fixed-case>hat<fixed-case>N</fixed-case>et<fixed-case>Z</fixed-case>ero, an <fixed-case>LLM</fixed-case>-Chatbot to Demystify Climate Pledges</title>
      <author><first>Angel</first><last>Hsu</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Mason</first><last>Laney</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Arboretica</affiliation></author>
      <author><first>Diego</first><last>Manya</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Linda</first><last>Farczadi</last><affiliation>Arboretica</affiliation></author>
      <pages>82-92</pages>
      <abstract>This paper introduces and evaluates ChatNetZero, a large-language model (LLM) chatbot developed through Retrieval-Augmented Generation (RAG), which uses generative AI to produce answers grounded in verified, climate-domain specific information. We describe ChatNetZero’s design, particularly the innovation of anti-hallucination and reference modules designed to enhance the accuracy and credibility of generated responses. To evaluate ChatNetZero’s performance against other LLMs, including GPT-4, Gemini, Coral, and ChatClimate, we conduct two types of validation: comparing LLMs’ generated responses to original source documents to verify their factual accuracy, and employing an expert survey to evaluate the overall quality, accuracy and relevance of each response. We find that while ChatNetZero responses show higher factual accuracy when compared to original source data, experts surveyed prefer lengthier responses that provide more context. Our results highlight the importance of prioritizing information presentation in the design of domain-specific LLMs to ensure that scientific information is effectively communicated, especially as even expert audiences find it challenging to assess the credibility of AI-generated content.</abstract>
      <url hash="fa539e58">2024.climatenlp-1.6</url>
      <bibkey>hsu-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Using <fixed-case>LLM</fixed-case>s to Build a Database of Climate Extreme Impacts</title>
      <author><first>Ni</first><last>Li</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Shorouq</first><last>Zahra</last><affiliation>RISE Research Institutes of Sweden AB</affiliation></author>
      <author><first>Mariana</first><last>Brito</last><affiliation>Helmholtz Zentrum München</affiliation></author>
      <author><first>Clare</first><last>Flynn</last></author>
      <author><first>Olof</first><last>Görnerup</last></author>
      <author><first>Koffi</first><last>Worou</last></author>
      <author><first>Murathan</first><last>Kurfali</last></author>
      <author><first>Chanjuan</first><last>Meng</last></author>
      <author><first>Wim</first><last>Thiery</last></author>
      <author><first>Jakob</first><last>Zscheischler</last><affiliation>Helmholtz Centre for Environmental Research - UFZ</affiliation></author>
      <author><first>Gabriele</first><last>Messori</last><affiliation>Uppsala University and Stockholm University</affiliation></author>
      <author><first>Joakim</first><last>Nivre</last><affiliation>Uppsala University</affiliation></author>
      <pages>93-110</pages>
      <abstract>To better understand how extreme climate events impact society, we need to increase the availability of accurate and comprehensive information about these impacts. We propose a method for building large-scale databases of climate extreme impacts from online textual sources, using LLMs for information extraction in combination with more traditional NLP techniques to improve accuracy and consistency. We evaluate the method against a small benchmark database created by human experts and find that extraction accuracy varies for different types of information. We compare three different LLMs and find that, while the commercial GPT-4 model gives the best performance overall, the open-source models Mistral and Mixtral are competitive for some types of information.</abstract>
      <url hash="317754ec">2024.climatenlp-1.7</url>
      <bibkey>li-etal-2024-using-llms</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Envisioning <fixed-case>NLP</fixed-case> for intercultural climate communication</title>
      <author><first>Steven</first><last>Bird</last><affiliation>Charles Darwin University</affiliation></author>
      <author><first>Angelina</first><last>Aquino</last><affiliation>Charles Darwin University</affiliation></author>
      <author><first>Ian</first><last>Gumbula</last><affiliation>Charles Darwin University</affiliation></author>
      <pages>111-122</pages>
      <abstract>Climate communication is often seen by the NLP community as an opportunity for machine translation, applied to ever smaller languages. However, over 90% the world’s linguistic diversity comes from languages with ‘primary orality’ and mostly spoken in non-Western oral societies. A case in point is the Aboriginal communities of Northern Australia, where we have been conducting workshops on climate communication, revealing shortcomings in existing communication practices along with new opportunities for improving intercultural communication. We present a case study of climate communication in an oral society, including the voices of many local people, and draw several lessons for the research program of NLP in the climate space.</abstract>
      <url hash="d6cf34c8">2024.climatenlp-1.8</url>
      <bibkey>bird-etal-2024-envisioning</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>E</fixed-case>n<fixed-case>C</fixed-case>laim: A Style Augmented Transformer Architecture for Environmental Claim Detection</title>
      <author><first>Diya</first><last>Saha</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manjira</first><last>Sinha</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <pages>123-132</pages>
      <abstract>Across countries, a noteworthy paradigm shift towards a more sustainable and environmentally responsible economy is underway. However, this positive transition is accompanied by an upsurge in greenwashing, where companies make exaggerated claims about their environmental commitments. To address this challenge and protect consumers, initiatives have emerged to substantiate green claims. With the proliferation of environmental and scientific assertions, a critical need arises for automated methods to detect and validate these claims at scale. In this paper, we introduce EnClaim, a transformer network architecture augmented with stylistic features for automatically detecting claims from open web documents or social media posts. The proposed model considers various linguistic stylistic features in conjunction with language models to predict whether a given statement constitutes a claim. We have rigorously evaluated the model using multiple open datasets. Our initial findings indicate that incorporating stylistic vectors alongside the BERT-based language model enhances the overall effectiveness of environmental claim detection.</abstract>
      <url hash="a9db0ec5">2024.climatenlp-1.9</url>
      <bibkey>saha-etal-2024-enclaim</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>LEAF</fixed-case>: Predicting the Environmental Impact of Food Products based on their Name</title>
      <author><first>Bas</first><last>Krahmer</last></author>
      <pages>133-142</pages>
      <abstract>Although food consumption represents a sub- stantial global source of greenhouse gas emis- sions, assessing the environmental impact of off-the-shelf products remains challenging. Currently, this information is often unavailable, hindering informed consumer decisions when grocery shopping. The present work introduces a new set of models called LEAF, which stands for Linguistic Environmental Analysis of Food Products. LEAF models predict the life-cycle environmental impact of food products based on their name. It is shown that LEAF models can accurately predict the environmental im- pact based on just the product name in a multi- lingual setting, greatly outperforming zero-shot classification methods. Models of varying sizes and capabilities are released, along with the code and dataset to fully reproduce the study.</abstract>
      <url hash="84f56680">2024.climatenlp-1.10</url>
      <bibkey>krahmer-2024-leaf</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Large Scale Narrative Messaging around Climate Change: A Cross-Cultural Comparison</title>
      <author><first>Haiqi</first><last>Zhou</last></author>
      <author><first>David</first><last>Hobson</last><affiliation>McGill University, McGill University</affiliation></author>
      <author><first>Derek</first><last>Ruths</last><affiliation>McGill University</affiliation></author>
      <author><first>Andrew</first><last>Piper</last><affiliation>McGill University</affiliation></author>
      <pages>143-155</pages>
      <abstract>In this study, we explore the use of Large Language Models (LLMs) such as GPT-4 to extract and analyze the latent narrative messaging in climate change-related news articles from North American and Chinese media. By defining “narrative messaging” as the intrinsic moral or lesson of a story, we apply our model to a dataset of approximately 15,000 news articles in English and Mandarin, categorized by climate-related topics and ideological groupings. Our findings reveal distinct differences in the narrative values emphasized by different cultural and ideological contexts, with North American sources often focusing on individualistic and crisis-driven themes, while Chinese sources emphasize developmental and cooperative narratives. This work demonstrates the potential of LLMs in understanding and influencing climate communication, offering new insights into the collective belief systems that shape public discourse on climate change across different cultures.</abstract>
      <url hash="d0fadf8f">2024.climatenlp-1.11</url>
      <bibkey>zhou-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Challenges in End-to-End Policy Extraction from Climate Action Plans</title>
      <author><first>Nupoor</first><last>Gandhi</last></author>
      <author><first>Tom</first><last>Corringham</last><affiliation>University California San Diego</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Allen Institute for Artificial Intelligence and Carnegie Mellon University</affiliation></author>
      <pages>156-167</pages>
      <abstract>Gray policy literature such as climate action plans (CAPs) provide an information-rich resource with potential to inform analysis and decision-making. However, these corpora are currently underutilized due to the substantial manual effort and expertise required to sift through long and detailed documents. Automatically structuring relevant information using information extraction (IE) would be useful for assisting policy scientists in synthesizing vast gray policy corpora to identify relevant entities, concepts and themes. LLMs have demonstrated strong performance on IE tasks in the few-shot setting, but it is unclear whether these gains transfer to gray policy literature which differs significantly to traditional benchmark datasets in several aspects, such as format of information content, length of documents, and inconsistency of document structure. We perform a case study on end-to-end IE with California CAPs, inspecting the performance of state-of-the-art tools for: (1) extracting content from CAPs into structured markup segments; (2) few-shot IE with LLMs; and (3) the utility of extracted entities for downstream analyses. We identify challenges at several points of the end-to-end IE pipeline for CAPs, and we provide recommendations for open problems centered around representing rich non-textual elements, document structure, flexible annotation schemes, and global information. Tackling these challenges would make it possible to realize the potential of LLMs for IE with gray policy literature.</abstract>
      <url hash="51fc2ce5">2024.climatenlp-1.12</url>
      <bibkey>gandhi-etal-2024-challenges</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Structuring Sustainability Reports for Environmental Standards with <fixed-case>LLM</fixed-case>s guided by Ontology</title>
      <author><first>Aida</first><last>Usmanova</last><affiliation>Leuphana Universitüt Lüneburg</affiliation></author>
      <author><first>Ricardo</first><last>Usbeck</last><affiliation>Leuphana Universitüt Lüneburg</affiliation></author>
      <pages>168-177</pages>
      <abstract>Following the introduction of the European Sustainability Reporting Standard (ESRS), companies will have to adapt to a new policy and provide mandatory sustainability reports. However, implementing such reports entails a challenge, such as the comprehension of a large number of textual information from various sources. This task can be accelerated by employing Large Language Models (LLMs) and ontologies to effectively model the domain knowledge. In this study, we extended an existing ontology to model ESRS Topical Standard for disclosure. The developed ontology would enable automated reasoning over the data and assist in constructing Knowledge Graphs (KGs). Moreover, the proposed ontology extension would also help to identify gaps in companies’ sustainability reports with regard to the ESRS requirements.Additionally, we extracted knowledge from corporate sustainability reports via LLMs guided with a proposed ontology and developed their KG representation.</abstract>
      <url hash="213e80ee">2024.climatenlp-1.13</url>
      <bibkey>usmanova-usbeck-2024-structuring</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Unlearning Climate Misinformation in Large Language Models</title>
      <author><first>Michael</first><last>Fore</last></author>
      <author><first>Simranjit</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chaehong</first><last>Lee</last></author>
      <author><first>Amritanshu</first><last>Pandey</last><affiliation>University of Vermont</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <author><first>Dimitrios</first><last>Stamoulis</last><affiliation>Microsoft</affiliation></author>
      <pages>178-192</pages>
      <abstract>Misinformation regarding climate change is a key roadblock in addressing one of the most serious threats to humanity. This paper investigates factual accuracy in large language models (LLMs) regarding climate information. Using true/false labeled Q&amp;A data for fine-tuning and evaluating LLMs on climate-related claims, we compare open-source models, assessing their ability to generate truthful responses to climate change questions. We investigate the detectability of models intentionally poisoned with false climate information, finding that such poisoning may not affect the accuracy of a model’s responses in other domains. Furthermore, we compare the effectiveness of unlearning algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually grounding LLMs on climate change topics. Our evaluation reveals that unlearning algorithms can be effective for nuanced conceptual claims, despite previous findings suggesting their inefficacy in privacy contexts. These insights aim to guide the development of more factually reliable LLMs and highlight the need for additional work to secure LLMs against misinformation attacks.</abstract>
      <url hash="c79fdb3b">2024.climatenlp-1.14</url>
      <bibkey>fore-etal-2024-unlearning</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Statements: Universal Information Extraction from Tables with Large Language Models for <fixed-case>ESG</fixed-case> <fixed-case>KPI</fixed-case>s</title>
      <author><first>Lokesh</first><last>Mishra</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sohayl</first><last>Dhibi</last></author>
      <author><first>Yusik</first><last>Kim</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Cesar</first><last>Berrospi Ramis</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Shubham</first><last>Gupta</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Michele</first><last>Dolfi</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Peter</first><last>Staar</last></author>
      <pages>193-214</pages>
      <abstract>Environment, Social, and Governance (ESG) KPIs assess an organization’s performance on issues such as climate change, greenhouse gas emissions, water consumption, waste management, human rights, diversity, and policies. ESG reports convey this valuable quantitative information through tables. Unfortunately, extracting this information is difficult due to high variability in the table structure as well as content. We propose Statements, a novel domain agnostic data structure for extracting quantitative facts and related information. We propose translating tables to statements as a new supervised deep-learning universal information extraction task. We introduce SemTabNet - a dataset of over 100K annotated tables. Investigating a family of T5-based Statement Extraction Models, our best model generates statements which are 82% similar to the ground-truth (compared to baseline of 21%). We demonstrate the advantages of statements by applying our model to over 2700 tables from ESG reports. The homogeneous nature of statements permits exploratory data analysis on expansive information found in large collections of ESG reports.</abstract>
      <url hash="bd6da0ea">2024.climatenlp-1.15</url>
      <bibkey>mishra-etal-2024-statements</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>CLIMATELI</fixed-case>: Evaluating Entity Linking on Climate Change Data</title>
      <author><first>Shijia</first><last>Zhou</last><affiliation>Ludwig-Maximilians-Universitüt München</affiliation></author>
      <author><first>Siyao</first><last>Peng</last><affiliation>Ludwig-Maximilians-Universitüt München</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universitüt München and IT University of Copenhagen</affiliation></author>
      <pages>215-222</pages>
      <abstract>Climate Change (CC) is a pressing topic of global importance, attracting increasing attention across research fields, from social sciences to Natural Language Processing (NLP). CC is also discussed in various settings and communication platforms, from academic publications to social media forums. Understanding who and what is mentioned in such data is a first critical step to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking), the first manually annotated CC dataset that links 3,087 entity spans to Wikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing entity linking (EL) systems on the CC topic across various genres and propose automated filtering methods for CC entities. We find that the performance of EL models notably lags behind humans at both token and entity levels. Testing within the scope of retaining or excluding non-nominal and/or non-CC entities particularly impacts the models’ performances.</abstract>
      <url hash="f7fbd044">2024.climatenlp-1.16</url>
      <bibkey>zhou-etal-2024-climateli</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Aligning Unstructured <fixed-case>P</fixed-case>aris Agreement Climate Plans with Sustainable Development Goals</title>
      <author><first>Daniel</first><last>Spokoyny</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Janelle</first><last>Cai</last></author>
      <author><first>Tom</first><last>Corringham</last><affiliation>UC San Diego</affiliation></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>223-232</pages>
      <abstract>Aligning unstructured climate policy documents according to a particular classification taxonomy with little to no labeled examples is challenging and requires manual effort of climate policy researchers. In this work we examine whether large language models (LLMs) can act as an effective substitute or assist in the annotation process. Utilizing a large set of text spans from Paris Agreement Nationally Determined Contributions (NDCs) linked to United Nations Sustainable Development Goals (SDGs) and targets contained in the Climate Watch dataset from the World Resources Institute in combination with our own annotated data, we validate our approaches and establish a benchmark for model performance evaluation on this task. With our evaluation benchmarking we quantify the effectiveness of using zero-shot or few-shot prompted LLMs to align these documents.</abstract>
      <url hash="c003d785">2024.climatenlp-1.17</url>
      <bibkey>spokoyny-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Granular Analysis of Social Media Users’ Truthfulness Stances Toward Climate Change Factual Claims</title>
      <author><first>Haiqi</first><last>Zhang</last></author>
      <author><first>Zhengyuan</first><last>Zhu</last></author>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Jacob</first><last>Devasier</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>233-240</pages>
      <abstract>Climate change poses an urgent global problem that requires efficient data analysis mechanisms to provide insights into climate-related discussions on social media platforms. This paper presents a framework aimed at understanding social media users’ perceptions of various climate change topics and uncovering the insights behind these perceptions. Our framework employs large language model to develop a taxonomy of factual claims related to climate change and build a classification model that detects the truthfulness stance of tweets toward the factual claims. The findings reveal two key conclusions: (1) The public tends to believe the claims are true, regardless of the actual claim veracity; (2) The public shows a lack of discernment between facts and misinformation across different topics, particularly in areas related to politics, economy, and environment.</abstract>
      <url hash="547494cc">2024.climatenlp-1.18</url>
      <bibkey>zhang-etal-2024-granular</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>SDG</fixed-case> target detection in environmental reports using Retrieval-augmented Generation with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Dario</first><last>Garigliotti</last><affiliation>University of Bergen</affiliation></author>
      <pages>241-250</pages>
      <abstract>With the consolidation of Large Language Models (LLM) as a dominant component in approaches for multiple linguistic tasks, the interest in these technologies has greatly increased within a variety of areas and domains. A particular scenario of information needs where to exploit these approaches is climate-aware NLP. Paradigmatically, the vast manual labour of inspecting long, heterogeneous documents to find environment-relevant expressions and claims suits well within a recently established Retrieval-augmented Generation (RAG) framework. In this paper, we tackle two dual problems within environment analysis dealing with the common goal of detecting a Sustainable Developmental Goal (SDG) target being addressed in a textual passage of an environmental assessment report.We develop relevant test collections, and propose and evaluate a series of methods within the general RAG pipeline, in order to assess the current capabilities of LLMs for the tasks of SDG target evidence identification and SDG target detection.</abstract>
      <url hash="8570eab8">2024.climatenlp-1.19</url>
      <bibkey>garigliotti-2024-sdg</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Assessing the Effectiveness of <fixed-case>GPT</fixed-case>-4o in Climate Change Evidence Synthesis and Systematic Assessments: Preliminary Insights</title>
      <author><first>Elphin</first><last>Joe</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Sai</first><last>Koneru</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Christine</first><last>Kirchhoff</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>251-257</pages>
      <abstract>In this research short, we examine the potential of using GPT-4o, a state-of-the-art large language model (LLM) to undertake evidence synthesis and systematic assessment tasks. Traditional workflows for such tasks involve large groups of domain experts who manually review and synthesize vast amounts of literature. The exponential growth of scientific literature and recent advances in LLMs provide an opportunity to complementing these traditional workflows with new age tools. We assess the efficacy of GPT-4o to do these tasks on a sample from the dataset created by the Global Adaptation Mapping Initiative (GAMI) where we check the accuracy of climate change adaptation related feature extraction from the scientific literature across three levels of expertise. Our results indicate that while GPT-4o can achieve high accuracy in low-expertise tasks like geographic location identification, their performance in intermediate and high-expertise tasks, such as stakeholder identification and assessment of depth of the adaptation response, is less reliable. The findings motivate the need for designing assessment workflows that utilize the strengths of models like GPT-4o while also providing refinements to improve their performance on these tasks.</abstract>
      <url hash="1c69c3e8">2024.climatenlp-1.20</url>
      <bibkey>joe-etal-2024-assessing</bibkey>
      <doi>10.18653/v1/2024.climatenlp-1.20</doi>
    </paper>
  </volume>
</collection>
