<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.cogalex">
  <volume id="1" ingest-date="2024-05-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop on Cognitive Aspects of the Lexicon @ LREC-COLING 2024</booktitle>
      <editor><first>Michael</first><last>Zock</last></editor>
      <editor><first>Emmanuele</first><last>Chersoni</last></editor>
      <editor><first>Yu-Yin</first><last>Hsu</last></editor>
      <editor><first>Simon</first><last>de Deyne</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="2ce4db09">2024.cogalex-1</url>
      <venue>cogalex</venue>
    </meta>
    <frontmatter>
      <url hash="2b012ec0">2024.cogalex-1.0</url>
      <bibkey>cogalex-2024-cognitive</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>CLAVELL</fixed-case> - Cognitive Linguistic Annotation and Visualization Environment for Language Learning</title>
      <author><first>Werner</first><last>Winiwarter</last></author>
      <pages>1–13</pages>
      <abstract>In this paper we introduce a novel sentence annotation based on radical construction grammar and Uniform Meaning Representation, which covers all levels of linguistic analysis, from interlinear morphemic glossing to PropBank rolesets, WordNet synsets, and Wikipedia page titles as concept identifiers. We visually enhance our annotation by using images to represent concepts, emojis for thematic roles, and color-coding for constructions. The meaning representation is embedded into the syntactic parse by aligning all concepts with the surface tokens in the sentence. The main motivation for developing this type of representation was its use in second language acquisition as part of a Web-based language learning environment. In entertaining and engaging annotation tasks language students assemble the representation step-by-step following a bottom-up strategy. Based on language exposure while performing these exercises, we populate personal idiolectal constructicons representing the students’ current status of second language comprehension. As first use case, we have implemented a solution for Japanese due to its soaring popularity in our language education program and the particular challenges involved with trying to master this language.</abstract>
      <url hash="51612b86">2024.cogalex-1.1</url>
      <bibkey>winiwarter-2024-clavell</bibkey>
    </paper>
    <paper id="2">
      <title>Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education</title>
      <author><first>Markus J.</first><last>Hofmann</last></author>
      <author><first>Markus T.</first><last>Jansen</last></author>
      <author><first>Christoph</first><last>Wigbels</last></author>
      <author><first>Benny</first><last>Briesemeister</last></author>
      <author><first>Arthur M.</first><last>Jacobs</last></author>
      <pages>14–25</pages>
      <abstract>Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model with the same architecture often provided slightly more stable predictions for intellectual interests, knowledge in humanities and level of education. Finally, a learning curve analysis suggested that around 500 training participants are required for generalizable predictions. We discuss ICs as a complement or replacement of survey-based psychodiagnostics.</abstract>
      <url hash="556474f8">2024.cogalex-1.2</url>
      <bibkey>hofmann-etal-2024-individual</bibkey>
    </paper>
    <paper id="3">
      <title>An Empirical Study on Vague Deictic Temporal Adverbials</title>
      <author><first>Svenja</first><last>Kenneweg</last></author>
      <author><first>Brendan Balcerak</first><last>Jackson</last></author>
      <author><first>Joerg</first><last>Deigmoeller</last></author>
      <author><first>Julian</first><last>Eggert</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>26–31</pages>
      <abstract>Temporal adverbial phrases such as recently and some time ago have a special function in communication and temporal cognition. These adverbials are deictic, in that their meaning is tied to their time of utterance; and they are vague, in that the time periods to which they apply are under-specified in comparison to expressions such as yesterday, which precisely indicates the day before the day of utterance. Despite their vagueness, conversational participants have a mental image of when events described using these adverbials take place. We present a study that aims to quantify this mental model in terms of fuzzy or graded membership. To achieve this, we investigated the four English temporal adverbials recently, just, some time ago and long time ago as applied to types of events with different durations and frequencies, by conducting surveys to measure how speakers judge the different adverbials to apply in different time ranges. Our results suggest that it is possible to represent the meanings of deictic vague temporal adverbials geometrically in terms of graded membership within a temporal conceptual space.</abstract>
      <url hash="722201b5">2024.cogalex-1.3</url>
      <bibkey>kenneweg-etal-2024-empirical</bibkey>
    </paper>
    <paper id="4">
      <title>Symbolic Learning of Rules for Semantic Relation Types Identification in <fixed-case>F</fixed-case>rench Genitive Postnominal Prepositional Phrases</title>
      <author><first>Hani</first><last>Guenoune</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>32–41</pages>
      <abstract>We are interested in the semantic relations conveyed by polylexical entities in the postnominal prepositional noun phrases form “A de B” (A of B). After identifying a relevant set of semantic relations types, we proceed, using generative AI, to build a collection of phrases, for each semantic relation type identified. We propose an algorithm for creating rules that allow the selection of the relation between A and B in noun phrases of each type. These rules correspond to selecting from a knowledge base the appropriate neighborhood of a given term. For the phrase “désert d’Algérie” carrying the location relation, the term “désert” is identified as a geographical location, and “Algérie” as a country. These constraints are used to automatically learn a set of rules for selecting the location relation for this type of example. Rules are not exclusive as there may be instances that fall under multiple relations. In the phrase “portrait de sa mère - the portrait of his/her mother”, all of depiction, possession, and producer types are a possible match.</abstract>
      <url hash="18dc2de3">2024.cogalex-1.4</url>
      <bibkey>guenoune-lafourcade-2024-symbolic</bibkey>
    </paper>
    <paper id="5">
      <title>How Human-Like Are Word Associations in Generative Models? An Experiment in <fixed-case>S</fixed-case>lovene</title>
      <author><first>Špela</first><last>Vintar</last></author>
      <author><first>Mojca</first><last>Brglez</last></author>
      <author><first>Aleš</first><last>Žagar</last></author>
      <pages>42–48</pages>
      <abstract>Large language models (LLMs) show extraordinary performance in a broad range of cognitive tasks, yet their capability to reproduce human semantic similarity judgements remains disputed. We report an experiment in which we fine-tune two LLMs for Slovene, a monolingual SloT5 and a multilingual mT5, as well as an mT5 for English, to generate word associations. The models are fine-tuned on human word association norms created within the Small World of Words project, which recently started to collect data for Slovene. Since our aim was to explore differences between human and model-generated outputs, the model parameters were minimally adjusted to fit the association task. We perform automatic evaluation using a set of methods to measure the overlap and ranking, and in addition a subset of human and model-generated responses were manually classified into four categories (meaning-, positionand form-based, and erratic). Results show that human-machine overlap is very small, but that the models produce a similar distribution of association categories as humans.</abstract>
      <url hash="eb0e9653">2024.cogalex-1.5</url>
      <bibkey>vintar-etal-2024-human</bibkey>
    </paper>
    <paper id="6">
      <title>Idiom Complexity in Apple-Pie Order: The Disentanglement of Decomposability and Transparency</title>
      <author><first>Irene</first><last>Pagliai</last></author>
      <pages>49–55</pages>
      <abstract>Both decomposability and transparency investigate the interplay between literality and figurativity in idioms. For this reason, they have often been merged. This study argues that idiom decomposability and transparency are related but conceptually different constructs, thus advocating for their distinction. Leveraging a normed lexicon of Italian and English idioms, the respective effects of decomposability and transparency on idiom meaning recognition are explored via statistical modeling. Results show the two variables contribute differently to idiom meaning recognition in the two languages, while the absence of collinearity underscores their distinct contributions. Based on this empirical evidence, the study finally proposes FrameNet and MetaNet as computational tools for modeling idiom decomposability and transparency. This study thus not only substantiates the separation of idiom decomposability and transparency, but also sets a foundation for future interdisciplinary research to bridge the gap in idiom research between empirical psycholinguistics, cognitive linguistics and computational applications.</abstract>
      <url hash="45379a74">2024.cogalex-1.6</url>
      <bibkey>pagliai-2024-idiom</bibkey>
    </paper>
    <paper id="7">
      <title>What <fixed-case>GPT</fixed-case>-4 Knows about Aspectual Coercion: Focused on “Begin the Book”</title>
      <author><first>Seohyun</first><last>Im</last></author>
      <author><first>Chungmin</first><last>Lee</last></author>
      <pages>56–67</pages>
      <abstract>This paper explores whether Pre-trained Large Language Models (PLLMs) like GPT-4 can grasp profound linguistic insights into language phenomena such as Aspectual Coercion through interaction with Microsoft’s Copilot, which integrates GPT-4. Firstly, we examined Copilot’s understanding of the co-occurrence constraints of the aspectual verb “begin” and the complex-type noun “book” using the classic illustration of Aspectual Coercion, “begin the book.” Secondly, we verified Copilot’s awareness of both the default interpretation of “begin the book” with no specific context and the contextually preferred interpretation. Ultimately, Copilot provided appropriate responses regarding potential interpretations of “begin the book” based on its distributional properties and context-dependent preferred interpretations. However, it did not furnish sophisticated explanations concerning these interpretations from a linguistic theoretical perspective. On the other hand, by offering diverse interpretations grounded in distributional properties, language models like GPT-4 demonstrated their potential contribution to the refinement of linguistic theories. Furthermore, we suggested the feasibility of employing Language Models to construct language resources associated with language phenomena including Aspectual Coercion.</abstract>
      <url hash="e3af633f">2024.cogalex-1.7</url>
      <bibkey>im-lee-2024-gpt</bibkey>
    </paper>
    <paper id="8">
      <title>Can <fixed-case>GPT</fixed-case>-4 Recover Latent Semantic Relational Information from Word Associations? A Detailed Analysis of Agreement with Human-annotated Semantic Ontologies.</title>
      <author><first>Simon</first><last>De Deyne</last></author>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>68–78</pages>
      <abstract>Word associations, i.e., spontaneous responses to a cue word, provide not only a window into the human mental lexicon but have also been shown to be a repository of common-sense knowledge and can underpin efforts in lexicography and the construction of dictionaries. Especially the latter tasks require knowledge about the relations underlying the associations (e.g., Taxonomic vs. Situational); however, to date, there is neither an established ontology of relations nor an effective labelling paradigm. Here, we test GPT-4’s ability to infer semantic relations for human-produced word associations. We use four human-labelled data sets of word associations and semantic features, with differing relation inventories and various levels of annotator agreement. We directly prompt GPT-4 with detailed relation definitions without further fine-tuning or training. Our results show that while GPT-4 provided a good account of higher-level classifications (e.g. Taxonomic vs Situational), prompting instructions alone cannot obtain similar performance for detailed classifications (e.g. superordinate, subordinate or coordinate relations) despite high agreement among human annotators. This suggests that latent relations can at least be partially recovered from word associations and highlights ways in which LLMs could be improved and human annotation protocols could adapted to reduce coding ambiguity.</abstract>
      <url hash="b3d61ec5">2024.cogalex-1.8</url>
      <bibkey>de-deyne-etal-2024-gpt</bibkey>
    </paper>
    <paper id="9">
      <title>What’s in a Name? Electrophysiological Differences in Processing Proper Nouns in <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese</title>
      <author><first>Bernard A. J.</first><last>Jap</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <author><first>Lavinia</first><last>Salicchi</last></author>
      <author><first>Yu Xi</first><last>Li</last></author>
      <pages>79–85</pages>
      <abstract>The current study examines how proper names and common nouns in Chinese are cognitively processed during sentence comprehension. EEG data was recorded when participants were presented with neutral contexts followed by either a proper name or a common noun. Proper names in Chinese often consist of characters that can function independently as words or be combined with other characters to form words, potentially benefiting from the semantic features carried by each character. Using cluster-based permutation tests, we found a larger N400 for common nouns when compared to proper names. Our results suggest that the semantics of characters do play a role in facilitating the processing of proper names. This is consistent with previous behavioral findings on noun processing in Chinese, indicating that common nouns require more cognitive resources to process than proper names. Moreover, our results suggest that proper names are processed differently between alphabetic languages and Chinese language.</abstract>
      <url hash="ce57eaa4">2024.cogalex-1.9</url>
      <bibkey>jap-etal-2024-whats</bibkey>
    </paper>
    <paper id="10">
      <title>Cross-Linguistic Processing of Non-Compositional Expressions in <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Iuliia</first><last>Zaitova</last></author>
      <author><first>Irina</first><last>Stenger</last></author>
      <author><first>Muhammad Umer</first><last>Butt</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>86–97</pages>
      <abstract>This study focuses on evaluating and predicting the intelligibility of non-compositional expressions within the context of five closely related Slavic languages: Belarusian, Bulgarian, Czech, Polish, and Ukrainian, as perceived by native speakers of Russian. Our investigation employs a web-based experiment where native Russian respondents take part in free-response and multiple-choice translation tasks. Based on the previous studies in mutual intelligibility and non-compositionality, we propose two predictive factors for reading comprehension of unknown but closely related languages: 1) linguistic distances, which include orthographic and phonological distances; 2) surprisal scores obtained from monolingual Language Models (LMs). Our primary objective is to explore the relationship of these two factors with the intelligibility scores and response times of our web-based experiment. Our findings reveal that, while intelligibility scores from the experimental tasks exhibit a stronger correlation with phonological distances, LM surprisal scores appear to be better predictors of the time participants invest in completing the translation tasks.</abstract>
      <url hash="828ef0bf">2024.cogalex-1.10</url>
      <bibkey>zaitova-etal-2024-cross</bibkey>
    </paper>
    <paper id="11">
      <title>Using Language Models to Unravel Semantic Development in Children’s Use of Perception Verbs</title>
      <author><first>Bram</first><last>van Dijk</last></author>
      <author><first>Max J.</first><last>van Duijn</last></author>
      <author><first>Li</first><last>Kloostra</last></author>
      <author><first>Marco</first><last>Spruit</last></author>
      <author><first>Barend</first><last>Beekhuizen</last></author>
      <pages>98–106</pages>
      <abstract>In this short paper we employ a Language Model (LM) to gain insight into how complex semantics of a Perception Verb (PV) emerge in children. Using a Dutch LM as representation of mature language use, we find that for all ages 1) the LM accurately predicts PV use in children’s freely-told narratives; 2) children’s PV use is close to mature use; 3) complex PV meanings with attentional and cognitive aspects can be found. Our approach illustrates how LMs can be meaningfully employed in studying language development, hence takes a constructive position in the debate on the relevance of LMs in this context.</abstract>
      <url hash="ab4e956d">2024.cogalex-1.11</url>
      <bibkey>van-dijk-etal-2024-using</bibkey>
    </paper>
    <paper id="12">
      <title>Representing Abstract Concepts with Images: An Investigation with Large Language Models</title>
      <author><first>Ludovica</first><last>Cerini</last></author>
      <author><first>Alessandro</first><last>Bondielli</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <pages>107–113</pages>
      <abstract>Multimodal metaphorical interpretation of abstract concepts has always been a debated problem in many research fields, including cognitive linguistics and NLP. With the dramatic improvements of Large Language Models (LLMs) and the increasing attention toward multimodal Vision-Language Models (VLMs), there has been pronounced attention on the conceptualization of abstracts. Nevertheless, a systematic scientific investigation is still lacking. This work introduces a framework designed to shed light on the indirect grounding mechanisms that anchor the meaning of abstract concepts to concrete situations (e.g. ability - a person skating), following the idea that abstracts acquire meaning from embodied and situated simulation. We assessed human and LLMs performances by a situation generation task. Moreover, we assess the figurative richness of images depicting concrete scenarios, via a text-to-image retrieval task performed on LAION-400M.</abstract>
      <url hash="14e3ba60">2024.cogalex-1.12</url>
      <bibkey>cerini-etal-2024-representing</bibkey>
    </paper>
    <paper id="13">
      <title>Big-Five Backstage: A Dramatic Dataset for Characters Personality Traits &amp; Gender Analysis</title>
      <author><first>Marina</first><last>Tiuleneva</last></author>
      <author><first>Vadim A.</first><last>Porvatov</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>114–119</pages>
      <abstract>This paper introduces a novel textual dataset comprising fictional characters’ lines with annotations based on their gender and Big-Five personality traits. Using psycholinguistic findings, we compared texts attributed to fictional characters and real people with respect to their genders and personality traits. Our results indicate that imagined personae mirror most of the language categories observed in real people while demonstrating them in a more expressive manner.</abstract>
      <url hash="41c47413">2024.cogalex-1.13</url>
      <bibkey>tiuleneva-etal-2024-big</bibkey>
    </paper>
    <paper id="14">
      <title>Interaction of Semantics and Morphology in <fixed-case>R</fixed-case>ussian Word Vectors</title>
      <author><first>Yulia</first><last>Zinova</last></author>
      <author><first>Ruben</first><last>van de Vijver</last></author>
      <author><first>Anastasia</first><last>Yablokova</last></author>
      <pages>120–128</pages>
      <abstract>In this paper we explore how morphological information can be extracted from fastText embeddings for Russian nouns. We investigate the negative effects of syncretism and propose ways of modifying the vectors that can help to find better representations for morphological functions and thus for out of vocabulary words. In particular, we look at the effect of analysing shift vectors instead of original vectors, discuss various possibilities of finding base forms to create shift vectors, and show that using only the high frequency data is beneficial when looking for structure with respect to the morphosyntactic functions in the embeddings.</abstract>
      <url hash="fe77015f">2024.cogalex-1.14</url>
      <bibkey>zinova-etal-2024-interaction</bibkey>
    </paper>
    <paper id="15">
      <title>Listen, Repeat, Decide: Investigating Pronunciation Variation in Spoken Word Recognition among <fixed-case>R</fixed-case>ussian Speakers</title>
      <author><first>Vladislav Ivanovich</first><last>Zubov</last></author>
      <author><first>Elena</first><last>Riekhakaynen</last></author>
      <pages>129–132</pages>
      <abstract>Variability is one of the important features of natural speech and a challenge for spoken word recognition models and automatic speech recognition systems. We conducted two preliminary experiments aimed at finding out whether native Russian speakers regard differently certain types of pronunciation variation when the variants are equally possible according to orthoepic norms. In the first experiment, the participants had to repeat the words with three different types of pronunciation variability. In the second experiment, we focused on the assessment of words with variable and only one standard stress. Our results support the hypothesis that listeners pay the most attention to words with variable stress, less to the variability of soft and hard consonants, and even less to the presence / absence of /j/. Assessing the correct pronunciation of words with variable stress takes significantly more time than assessing words which have only one correct pronunciation variant. These preliminary results show that pronunciation variants can provide new evidence on how a listener access the mental lexicon during natural speech processing and chooses among the variants stored in it.</abstract>
      <url hash="b719769f">2024.cogalex-1.15</url>
      <bibkey>zubov-riekhakaynen-2024-listen</bibkey>
    </paper>
    <paper id="16">
      <title>The Mental Lexicon of Communicative Fragments and Contours: The Remix N-gram Method</title>
      <author><first>Emese</first><last>K. Molnár</last></author>
      <author><first>Andrea</first><last>Dömötör</last></author>
      <pages>133–139</pages>
      <abstract>The classical mental lexicon models represented the lexicon as a list of words. Usage-based models describe the mental lexicon more dynamically, but they do not capture the real-time operation of speech production. In the linguistic model of Boris Gasparov, the notions of communicative fragment and contour can provide a comprehensive description of the diversity of linguistic experience. Fragments and contours form larger linguistic structures than words and they are recognized as a whole unit by speakers through their communicative profile. Fragments are prefabricated units that can be added to or merged with each other during speech production. The contours serve as templates for the utterances by combining specific and abstract linguistic elements. Based on this theoretical framework, our tool applies remix n-grams (combination of word forms, lemmas and POS-tags) to identify similar linguistic structures in different texts that form the basic units of the mental lexicon.</abstract>
      <url hash="27a03850">2024.cogalex-1.16</url>
      <bibkey>k-molnar-domotor-2024-mental</bibkey>
    </paper>
    <paper id="17">
      <title>Three Studies on Predicting Word Concreteness with Embedding Vectors</title>
      <author><first>Michael</first><last>Flor</last></author>
      <pages>140–150</pages>
      <abstract>Human-assigned concreteness ratings for words are commonly used in psycholinguistic and computational linguistic studies. Previous research has shown that such ratings can be modeled and extrapolated by using dense word-embedding representations. However, due to rater disagreement, considerable amounts of human ratings in published datasets are not reliable. We investigate how such unreliable data influences modeling of concreteness with word embeddings. Study 1 compares fourteen embedding models over three datasets of concreteness ratings, showing that most models achieve high correlations with human ratings, and exhibit low error rates on predictions. Study 2 investigates how exclusion of the less reliable ratings influences the modeling results. It indicates that improved results can be achieved when data is cleaned. Study 3 adds additional conditions over those of study 2 and indicates that the improved results hold only for the cleaned data, and that in the general case removing the less reliable data points is not useful.</abstract>
      <url hash="4ba37983">2024.cogalex-1.17</url>
      <bibkey>flor-2024-three</bibkey>
    </paper>
    <paper id="18">
      <title>Combining Neo-Structuralist and Cognitive Approaches to Semantics to Build Wordnets for Ancient Languages: Challenges and Perspectives</title>
      <author><first>Erica</first><last>Biagetti</last></author>
      <author><first>Martina</first><last>Giuliani</last></author>
      <author><first>Silvia</first><last>Zampetta</last></author>
      <author><first>Silvia</first><last>Luraghi</last></author>
      <author><first>Chiara</first><last>Zanchi</last></author>
      <pages>151–161</pages>
      <abstract>This paper addresses challenges encountered in constructing lexical databases, specifically WordNets, for three ancient Indo-European languages: Ancient Greek, Latin, and Sanskrit. The difficulties partly arise from adapting concepts and methodologies designed for modern languages to the construction of lexical resources for ancient ones. A further significant challenge arises from the goal of creating WordNets that not only adhere to a neo-structuralist relational view of meaning but also integrate Cognitive Semantics concepts, aiming for a more realistic representation of meaning. This integration is crucial for facilitating studies in diachronic semantics and lexicology, and representing meaning in such a nuanced manner becomes paramount when constructing language resources for theoretical research, rather than for applied tasks, as is the case with lexical resources for ancient languages. The paper delves into these challenges through a case study focused on the TEMPERATURE conceptual domain in the three languages. It outlines difficulties in distinguishing prototypical and non-prototypical senses, literal and non-literal ones, and, within non-literal meanings, between metaphorical and metonymic ones. Solutions adopted to address these challenges are presented, highlighting the necessity of achieving maximum granularity in meaning representation while maintaining a sustainable workflow for annotators.</abstract>
      <url hash="ec345b9b">2024.cogalex-1.18</url>
      <bibkey>biagetti-etal-2024-combining</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>S</fixed-case>ensory<fixed-case>T</fixed-case>5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification</title>
      <author><first>Yuhan</first><last>Xia</last></author>
      <author><first>Qingqing</first><last>Zhao</last></author>
      <author><first>Yunfei</first><last>Long</last></author>
      <author><first>Ge</first><last>Xu</last></author>
      <author><first>Jia</first><last>Wang</last></author>
      <pages>162–174</pages>
      <abstract>In traditional research approaches, sensory perception and emotion classification have traditionally been considered separate domains. Yet, the significant influence of sensory experiences on emotional responses is undeniable. The natural language processing (NLP) community has often missed the opportunity to merge sensory knowledge with emotion classification. To address this gap, we propose SensoryT5, a neurocognitive approach that integrates sensory information into the T5 (Text-to-Text Transfer Transformer) model, designed specifically for fine-grained emotion classification. This methodology incorporates sensory cues into the T5’s attention mechanism, enabling a harmonious balance between contextual understanding and sensory awareness. The resulting model amplifies the richness of emotional representations. In rigorous tests across various detailed emotion classification datasets, SensoryT5 showcases improved performance, surpassing both the foundational T5 model and current state-of-the-art works. Notably, SensoryT5’s success signifies a pivotal change in the NLP domain, highlighting the potential influence of neurocognitive data in refining machine learning models’ emotional sensitivity.</abstract>
      <url hash="e1598bf3">2024.cogalex-1.19</url>
      <bibkey>xia-etal-2024-sensoryt5</bibkey>
    </paper>
  </volume>
</collection>
