<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.in2writing">
  <volume id="1" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)</booktitle>
      <editor><first>Ting-Hao 'Kenneth'</first><last>Huang</last></editor>
      <editor><first>Vipul</first><last>Raheja</last></editor>
      <editor><first>Dongyeop</first><last>Kang</last></editor>
      <editor><first>John Joon Young</first><last>Chung</last></editor>
      <editor><first>Daniel</first><last>Gissin</last></editor>
      <editor><first>Mina</first><last>Lee</last></editor>
      <editor><first>Katy Ilonka</first><last>Gero</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="4c2fd404">2022.in2writing-1</url>
      <venue>in2writing</venue>
    </meta>
    <frontmatter>
      <url hash="8933180f">2022.in2writing-1.0</url>
      <bibkey>in2writing-2022-intelligent</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Data-to-text systems as writing environment</title>
      <author><first>Adela</first><last>Schneider</last></author>
      <author><first>Andreas</first><last>Madsack</last></author>
      <author><first>Johanna</first><last>Heininger</last></author>
      <author><first>Ching-Yi</first><last>Chen</last></author>
      <author><first>Robert</first><last>Weißgraeber</last></author>
      <pages>1-10</pages>
      <abstract>Today, data-to-text systems are used as commercial solutions for automated text productionof large quantities of text. Therefore, they already represent a new technology of writing.This new technology requires the author, asan act of writing, both to configure a systemthat then takes over the transformation into areal text, but also to maintain strategies of traditional writing. What should an environmentlook like, where a human guides a machineto write texts? Based on a comparison of theNLG pipeline architecture with the results ofthe research on the human writing process, thispaper attempts to take an overview of whichtasks need to be solved and which strategiesare necessary to produce good texts in this environment. From this synopsis, principles for thedesign of data-to-text systems as a functioningwriting environment are then derived.</abstract>
      <url hash="1b38adb1">2022.in2writing-1.1</url>
      <bibkey>schneider-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.1</doi>
      <video href="2022.in2writing-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>A Design Space for Writing Support Tools Using a Cognitive Process Model of Writing</title>
      <author><first>Katy</first><last>Gero</last></author>
      <author><first>Alex</first><last>Calderwood</last></author>
      <author><first>Charlotte</first><last>Li</last></author>
      <author><first>Lydia</first><last>Chilton</last></author>
      <pages>11-24</pages>
      <abstract>Improvements in language technology have led to an increasing interest in writing support tools. In this paper we propose a design space for such tools based on a cognitive process model of writing. We conduct a systematic review of recent computer science papers that present and/or study such tools, analyzing 30 papers from the last five years using the design space. Tools are plotted according to three distinct cognitive processes–planning, translating, and reviewing–and the level of constraint each process entails. Analyzing recent work with the design space shows that highly constrained planning and reviewing are under-studied areas that recent technology improvements may now be able to serve. Finally, we propose shared evaluation methodologies and tasks that may help the field mature.</abstract>
      <url hash="418e4dd8">2022.in2writing-1.2</url>
      <bibkey>gero-etal-2022-design</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.2</doi>
      <video href="2022.in2writing-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>A Selective Summary of Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence</title>
      <author><first>Nikhil</first><last>Singh</last></author>
      <author><first>Guillermo</first><last>Bernal</last></author>
      <author><first>Daria</first><last>Savchenko</last></author>
      <author><first>Elena</first><last>Glassman</last></author>
      <pages>25-26</pages>
      <abstract>While developing a story, novices and published writers alike have had to look outside themselves for inspiration. Language models have recently been able to generate text fluently, producing new stochastic narratives upon request. However, effectively integrating such capabilities with human cognitive faculties and creative processes remains challenging. We propose to investigate this integration with a multimodal writing support interface that offers writing suggestions textually, visually, and aurally. We conduct an extensive study that combines elicitation of prior expectations before writing, observation and semi-structured interviews during writing, and outcome evaluations after writing. Our results illustrate individual and situational variation in machine-in-the-loop writing approaches, suggestion acceptance, and ways the system is helpful. Centrally, we report how participants perform integrative leaps, by which they do cognitive work to integrate suggestions of varying semantic relevance into their developing stories. We interpret these findings, offering modeling and design recommendations for future creative writing support technologies.</abstract>
      <url hash="1d0ae1ae">2022.in2writing-1.3</url>
      <bibkey>singh-etal-2022-selective</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.3</doi>
      <video href="2022.in2writing-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>A text-writing system for Easy-to-Read <fixed-case>G</fixed-case>erman evaluated with low-literate users with cognitive impairment</title>
      <author><first>Ina</first><last>Steinmetz</last></author>
      <author><first>Karin</first><last>Harbusch</last></author>
      <pages>27-38</pages>
      <abstract>Low-literate users with intellectual or developmental disabilities (IDD) and/or complex communication needs (CCN) require specific writing support. We present a system that interactively supports fast and correct writing of a variant of Leichte Sprache (LS; German term for easy-to-read German), slightly extended within and beyond the inner-sentential syntactic level. The system provides simple and intuitive dialogues for selecting options from a natural-language paraphrase generator. Moreover, it reminds the user to add text elements enhancing understandability, audience design, and text coherence. In earlier development phases, the system was evaluated with different groups of substitute users. Here, we report a case study with seven low-literate users with IDD.</abstract>
      <url hash="4b1daf4e">2022.in2writing-1.4</url>
      <bibkey>steinmetz-harbusch-2022-text</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.4</doi>
      <video href="2022.in2writing-1.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="5">
      <title>Language Models as Context-sensitive Word Search Engines</title>
      <author><first>Matti</first><last>Wiegmann</last></author>
      <author><first>Michael</first><last>Völske</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>39-45</pages>
      <abstract>Context-sensitive word search engines are writing assistants that support word choice, phrasing, and idiomatic language use by indexing large-scale n-gram collections and implementing a wildcard search. However, search results become unreliable with increasing context size (e.g., n&gt;=5), when observations become sparse. This paper proposes two strategies for word search with larger n, based on masked and conditional language modeling. We build such search engines using BERT and BART and compare their capabilities in answering English context queries with those of the n-gram-based word search engine Netspeak. Our proposed strategies score within 5 percentage points MRR of n-gram collections while answering up to 5 times as many queries.</abstract>
      <url hash="defd0328">2022.in2writing-1.5</url>
      <bibkey>wiegmann-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.5</doi>
      <video href="2022.in2writing-1.5.mp4"/>
      <pwccode url="https://github.com/webis-de/in2writing22-language-models-as-context-sensitive-word-search-engines" additional="false">webis-de/in2writing22-language-models-as-context-sensitive-word-search-engines</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cloth">CLOTH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="6">
      <title>Plug-and-Play Controller for Story Completion: A Pilot Study toward Emotion-aware Story Writing Assistance</title>
      <author><first>Yusuke</first><last>Mori</last></author>
      <author><first>Hiroaki</first><last>Yamane</last></author>
      <author><first>Ryohei</first><last>Shimizu</last></author>
      <author><first>Tatsuya</first><last>Harada</last></author>
      <pages>46-57</pages>
      <abstract>Emotions are essential for storytelling and narrative generation, and as such, the relationship between stories and emotions has been extensively studied. The authors of this paper, including a professional novelist, have examined the use of natural language processing to address the problems of novelists from the perspective of practical creative writing. In particular, the story completion task, which requires understanding the existing unfinished context, was studied from the perspective of creative support for human writers, to generate appropriate content to complete the unfinished parts. It was found that unsupervised pre-trained large neural models of the sequence-to-sequence type are useful for this task. Furthermore, based on the plug-and-play module for controllable text generation using GPT-2, an additional module was implemented to consider emotions. Although this is a preliminary study, and the results leave room for improvement before incorporating the model into a practical system, this effort is an important step in complementing the emotional trajectory of the story.</abstract>
      <url hash="89d1a968">2022.in2writing-1.6</url>
      <bibkey>mori-etal-2022-plug</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.6</doi>
      <video href="2022.in2writing-1.6.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="7">
      <title>Text Revision by On-the-Fly Representation Optimization</title>
      <author><first>Jingjing</first><last>Li</last></author>
      <author><first>Zichao</first><last>Li</last></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Irwin</first><last>King</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <pages>58-59</pages>
      <abstract>Text revision refers to a family of natural language generation tasks, where the source and target sequences share moderate resemblance in surface form but differentiate in attributes, such as text formality and simplicity. Current state-of-the-art methods formulate these tasks as sequence-to-sequence learning problems, which rely on large-scale parallel training corpus. In this paper, we present an iterative inplace editing approach for text revision, which requires no parallel data. In this approach, we simply fine-tune a pre-trained Transformer with masked language modeling and attribute classification. During inference, the editing at each iteration is realized by two-step span replacement. At the first step, the distributed representation of the text optimizes on the fly towards an attribute function. At the second step, a text span is masked and another new one is proposed conditioned on the optimized representation. The empirical experiments on two typical and important text revision tasks, text formalization and text simplification, show the effectiveness of our approach. It achieves competitive and even better performance than state-of-the-art supervised methods on text simplification, and gains better performance than strong unsupervised methods on text formalization.</abstract>
      <url hash="8124855f">2022.in2writing-1.7</url>
      <bibkey>li-etal-2022-text</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.7</doi>
      <video href="2022.in2writing-1.7.mp4"/>
      <pwccode url="https://github.com/jingjingli01/oreo" additional="false">jingjingli01/oreo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="8">
      <title>The Pure Poet: How Good is the Subjective Credibility and Stylistic Quality of Literary Short Texts Written with an Artificial Intelligence Tool as Compared to Texts Written by Human Authors?</title>
      <author><first>Vivian Emily</first><last>Gunser</last></author>
      <author><first>Steffen</first><last>Gottschling</last></author>
      <author><first>Birgit</first><last>Brucker</last></author>
      <author><first>Sandra</first><last>Richter</last></author>
      <author><first>Dîlan Canan</first><last>Çakir</last></author>
      <author><first>Peter</first><last>Gerjets</last></author>
      <pages>60-61</pages>
      <abstract>The application of artificial intelligence (AI) for text generation in creative domains raises questions regarding the credibility of AI-generated content. In two studies, we explored if readers can differentiate between AI-based and human-written texts (generated based on the first line of texts and poems of classic authors) and how the stylistic qualities of these texts are rated. Participants read 9 AI-based continuations and either 9 human-written continuations (Study 1, N=120) or 9 original continuations (Study 2, N=302). Participants’ task was to decide whether a continuation was written with an AI-tool or not, to indicate their confidence in each decision, and to assess the stylistic text quality. Results showed that participants generally had low accuracy for differentiating between text types but were overconfident in their decisions. Regarding the assessment of stylistic quality, AI-continuations were perceived as less well-written, inspiring, fascinating, interesting, and aesthetic than both human-written and original continuations.</abstract>
      <url hash="c27b3cb2">2022.in2writing-1.8</url>
      <bibkey>gunser-etal-2022-pure</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.8</doi>
      <video href="2022.in2writing-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Interactive Children’s Story Rewriting Through Parent-Children Interaction</title>
      <author><first>Yoonjoo</first><last>Lee</last></author>
      <author><first>Tae Soo</first><last>Kim</last></author>
      <author><first>Minsuk</first><last>Chang</last></author>
      <author><first>Juho</first><last>Kim</last></author>
      <pages>62-71</pages>
      <abstract>Storytelling in early childhood provides significant benefits in language and literacy development, relationship building, and entertainment. To maximize these benefits, it is important to empower children with more agency. Interactive story rewriting through parent-children interaction can boost children’s agency and help build the relationship between parent and child as they collaboratively create changes to an original story. However, for children with limited proficiency in reading and writing, parents must carry out multiple tasks to guide the rewriting process, which can incur a high cognitive load. In this work, we introduce an interface design that aims to support children and parents to rewrite stories together with the help of AI techniques. We describe three design goals determined by a review of prior literature in interactive storytelling and existing educational activities. We also propose a preliminary prompt-based pipeline that uses GPT-3 to realize the design goals and enable the interface.</abstract>
      <url hash="35f22f92">2022.in2writing-1.9</url>
      <bibkey>lee-etal-2022-interactive</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.9</doi>
      <video href="2022.in2writing-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>News Article Retrieval in Context for Event-centric Narrative Creation</title>
      <author><first>Nikos</first><last>Voskarides</last></author>
      <author><first>Edgar</first><last>Meij</last></author>
      <author><first>Sabrina</first><last>Sauer</last></author>
      <author><first>Maarten</first><last>de Rijke</last></author>
      <pages>72-73</pages>
      <abstract>Writers such as journalists often use automatic tools to find relevant content to include in their narratives. In this paper, we focus on supporting writers in the news domain to develop event-centric narratives. Given an incomplete narrative that specifies a main event and a context, we aim to retrieve news articles that discuss relevant events that would enable the continuation of the narrative. We formally define this task and propose a retrieval dataset construction procedure that relies on existing news articles to simulate incomplete narratives and relevant articles. Experiments on two datasets derived from this procedure show that state-of-the-art lexical and semantic rankers are not sufficient for this task. We show that combining those with a ranker that ranks articles by reverse chronological order outperforms those rankers alone. We also perform an in-depth quantitative and qualitative analysis of the results that sheds light on the characteristics of this task.</abstract>
      <url hash="f34aaaa3">2022.in2writing-1.10</url>
      <bibkey>voskarides-etal-2022-news</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.10</doi>
      <video href="2022.in2writing-1.10.mp4"/>
      <pwccode url="https://github.com/nickvosk/ictir2021-news-retrieval-in-context" additional="false">nickvosk/ictir2021-news-retrieval-in-context</pwccode>
    </paper>
    <paper id="11">
      <title>Unmet Creativity Support Needs in Computationally Supported Creative Writing</title>
      <author><first>Max</first><last>Kreminski</last></author>
      <author><first>Chris</first><last>Martens</last></author>
      <pages>74-82</pages>
      <abstract>Large language models (LLMs) enabled by the datasets and computing power of the last decade have recently gained popularity for their capacity to generate plausible natural language text from human-provided prompts. This ability makes them appealing to fiction writers as prospective co-creative agents, addressing the common challenge of writer’s block, or getting unstuck. However, creative writers face additional challenges, including maintaining narrative consistency, developing plot structure, architecting reader experience, and refining their expressive intent, which are not well-addressed by current LLM-backed tools. In this paper, we define these needs by grounding them in cognitive and theoretical literature, then survey previous computational narrative research that holds promise for supporting each of them in a co-creative setting.</abstract>
      <url hash="06bd8d1d">2022.in2writing-1.11</url>
      <bibkey>kreminski-martens-2022-unmet</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.11</doi>
      <video href="2022.in2writing-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Sparks: Inspiration for Science Writing using Language Models</title>
      <author><first>Katy</first><last>Gero</last></author>
      <author><first>Vivian</first><last>Liu</last></author>
      <author><first>Lydia</first><last>Chilton</last></author>
      <pages>83-84</pages>
      <abstract>Large-scale language models are rapidly improving, performing well on a variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating “sparks”, sentences related to a scientific concept intended to inspire writers. We run a user study with 13 STEM graduate students and find three main use cases of sparks—inspiration, translation, and perspective—each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the overall quality of sparks seen by a given participant did not correlate with their satisfaction with the tool.</abstract>
      <url hash="78e0a5c7">2022.in2writing-1.12</url>
      <bibkey>gero-etal-2022-sparks</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.12</doi>
      <video href="2022.in2writing-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title><fixed-case>C</fixed-case>hip<fixed-case>S</fixed-case>ong: A Controllable Lyric Generation System for <fixed-case>C</fixed-case>hinese Popular Song</title>
      <author><first>Nayu</first><last>Liu</last></author>
      <author><first>Wenjing</first><last>Han</last></author>
      <author><first>Guangcan</first><last>Liu</last></author>
      <author><first>Da</first><last>Peng</last></author>
      <author><first>Ran</first><last>Zhang</last></author>
      <author><first>Xiaorui</first><last>Wang</last></author>
      <author><first>Huabin</first><last>Ruan</last></author>
      <pages>85-95</pages>
      <abstract>In this work, we take a further step towards satisfying practical demands in Chinese lyric generation from musical short-video creators, in respect of the challenges on songs’ format constraints, creating specific lyrics from open-ended inspiration inputs, and language rhyme grace. One representative detail in these demands is to control lyric format at word level, that is, for Chinese songs, creators even expect fix-length words on certain positions in a lyric to match a special melody, while previous methods lack such ability. Although recent lyric generation community has made gratifying progress, most methods are not comprehensive enough to simultaneously meet these demands. As a result, we propose ChipSong, which is an assisted lyric generation system built based on a Transformer-based autoregressive language model architecture, and generates controlled lyric paragraphs fit for musical short-video display purpose, by designing 1) a novel Begin-Internal-End (BIE) word-granularity embedding sequence with its guided attention mechanism for word-level length format control, and an explicit symbol set for sentence-level length format control; 2) an open-ended trigger word mechanism to guide specific lyric contents generation; 3) a paradigm of reverse order training and shielding decoding for rhyme control. Extensive experiments show that our ChipSong generates fluent lyrics, with assuring the high consistency to pre-determined control conditions.</abstract>
      <url hash="8ea21d80">2022.in2writing-1.13</url>
      <bibkey>liu-etal-2022-chipsong</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.13</doi>
      <video href="2022.in2writing-1.13.mp4"/>
      <pwccode url="https://github.com/korokes/chipsong" additional="false">korokes/chipsong</pwccode>
    </paper>
    <paper id="14">
      <title>Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision</title>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Zae Myung</first><last>Kim</last></author>
      <author><first>Vipul</first><last>Raheja</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <pages>96-108</pages>
      <abstract>Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at <url>https://github.com/vipulraheja/IteraTeR</url>. Our system demonstration is available at <url>https://youtu.be/lK08tIpEoaE</url>.</abstract>
      <url hash="8c6086b5">2022.in2writing-1.14</url>
      <bibkey>du-etal-2022-read</bibkey>
      <doi>10.18653/v1/2022.in2writing-1.14</doi>
      <video href="2022.in2writing-1.14.mp4"/>
      <pwccode url="https://github.com/vipulraheja/iterater" additional="false">vipulraheja/iterater</pwccode>
    </paper>
  </volume>
</collection>
