<?xml version='1.0' encoding='UTF-8'?>
<collection id="W19">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
      <editor><first>Gaja</first><last>Jarosz</last></editor>
      <editor><first>Max</first><last>Nelson</last></editor>
      <editor><first>Brendan</first><last>O’Connor</last></editor>
      <editor><first>Joe</first><last>Pater</last></editor>
      <venue>scil</venue>
    </meta>
    <frontmatter>
      <pages>i-ii</pages>
      <doi>10.7275/ntf6-xx21</doi>
      <url hash="de7f1400">W19-0100</url>
      <bibkey>ws-2019-society</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Can Entropy Explain Successor Surprisal Effects in Reading?</title>
      <author><first>Marten</first><last>van Schijndel</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>1-7</pages>
      <doi>10.7275/qtbb-9d05</doi>
      <url hash="5e23b704">W19-0101</url>
      <bibkey>van-schijndel-linzen-2019-entropy</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-stories">Natural Stories</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>R</fixed-case>ed<fixed-case>T</fixed-case>yp: A Database of Reduplication with Computational Models</title>
      <author><first>Hossep</first><last>Dolatian</last></author>
      <author><first>Jeffrey</first><last>Heinz</last></author>
      <pages>8-18</pages>
      <doi>10.7275/ckx7-s770</doi>
      <url hash="8cf39a1f">W19-0102</url>
      <bibkey>dolatian-heinz-2019-redtyp</bibkey>
      <pwccode url="https://github.com/jhdeov/RedTyp" additional="false">jhdeov/RedTyp</pwccode>
    </paper>
    <paper id="3">
      <title>Unsupervised Learning of Cross-Lingual Symbol Embeddings Without Parallel Data</title>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <author><first>Hannu</first><last>Toivonen</last></author>
      <pages>19-28</pages>
      <doi>10.7275/wx64-ea83</doi>
      <url hash="6c68f454">W19-0103</url>
      <bibkey>granroth-wilding-toivonen-2019-unsupervised</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>Q</fixed-case>-Theory Representations are Logically Equivalent to Autosegmental Representations</title>
      <author><first>Nick</first><last>Danis</last></author>
      <author><first>Adam</first><last>Jardine</last></author>
      <pages>29-38</pages>
      <doi>10.7275/tvj1-k306</doi>
      <url hash="ae60b288">W19-0104</url>
      <bibkey>danis-jardine-2019-q</bibkey>
    </paper>
    <paper id="5">
      <title>Modeling Clausal Complementation for a Grammar Engineering Resource</title>
      <author><first>Olga</first><last>Zamaraeva</last></author>
      <author><first>Kristen</first><last>Howell</last></author>
      <author><first>Emily M.</first><last>Bender</last></author>
      <pages>39-49</pages>
      <doi>10.7275/dygn-c796</doi>
      <url hash="f8d3d0e2">W19-0105</url>
      <bibkey>zamaraeva-etal-2019-modeling</bibkey>
    </paper>
    <paper id="6">
      <title>Do <fixed-case>RNN</fixed-case>s learn human-like abstract word order preferences?</title>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Roger P.</first><last>Levy</last></author>
      <pages>50-59</pages>
      <doi>10.7275/jb34-9986</doi>
      <url hash="58be6481">W19-0106</url>
      <bibkey>futrell-levy-2019-rnns</bibkey>
      <pwccode url="https://github.com/langprocgroup/rnn_soft_constraints" additional="false">langprocgroup/rnn_soft_constraints</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="7">
      <title>Segmentation and <fixed-case>UR</fixed-case> Acquisition with <fixed-case>UR</fixed-case> Constraints</title>
      <author><first>Max</first><last>Nelson</last></author>
      <pages>60-68</pages>
      <doi>10.7275/zc9d-pn56</doi>
      <url hash="b4612029">W19-0107</url>
      <bibkey>nelson-2019-segmentation</bibkey>
    </paper>
    <paper id="8">
      <title>Constraint breeding during on-line incremental learning</title>
      <author><first>Elliot</first><last>Moreton</last></author>
      <pages>69-80</pages>
      <doi>10.7275/6f9x-6411</doi>
      <url hash="63f81efe">W19-0108</url>
      <bibkey>moreton-2019-constraint</bibkey>
    </paper>
    <paper id="9">
      <title>An Incremental Iterated Response Model of Pragmatics</title>
      <author><first>Reuben</first><last>Cohn-Gordon</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>81-90</pages>
      <doi>10.7275/cprc-8x17</doi>
      <url hash="502c7b8a">W19-0109</url>
      <bibkey>cohn-gordon-etal-2019-incremental</bibkey>
    </paper>
    <paper id="10">
      <title>Learning Exceptionality and Variation with Lexically Scaled <fixed-case>M</fixed-case>ax<fixed-case>E</fixed-case>nt</title>
      <author><first>Coral</first><last>Hughto</last></author>
      <author><first>Andrew</first><last>Lamont</last></author>
      <author><first>Brandon</first><last>Prickett</last></author>
      <author><first>Gaja</first><last>Jarosz</last></author>
      <pages>91-101</pages>
      <doi>10.7275/y68s-kh12</doi>
      <url hash="bb83c9c7">W19-0110</url>
      <bibkey>hughto-etal-2019-learning</bibkey>
    </paper>
    <paper id="11">
      <title>Learning complex inflectional paradigms through blended gradient inputs</title>
      <author><first>Eric R.</first><last>Rosen</last></author>
      <pages>102-112</pages>
      <doi>10.7275/ccwf-j606</doi>
      <url hash="cc0bae14">W19-0111</url>
      <bibkey>rosen-2019-learning</bibkey>
    </paper>
    <paper id="12">
      <title>Jabberwocky Parsing: Dependency Parsing with Lexical Noise</title>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <pages>113-123</pages>
      <doi>10.7275/h12q-k754</doi>
      <url hash="6cf40249">W19-0112</url>
      <bibkey>kasai-frank-2019-jabberwocky</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="13">
      <title>Learnability and Overgeneration in Computational Syntax</title>
      <author><first>Yiding</first><last>Hao</last></author>
      <pages>124-134</pages>
      <doi>10.7275/1qmz-bg76</doi>
      <url hash="382eba02">W19-0113</url>
      <bibkey>hao-2019-learnability</bibkey>
    </paper>
    <paper id="14">
      <title>A Conceptual Spaces Model of Socially Motivated Language Change</title>
      <author><first>Heather</first><last>Burnett</last></author>
      <author><first>Olivier</first><last>Bonami</last></author>
      <pages>135-145</pages>
      <doi>10.7275/5vmp-cs05</doi>
      <url hash="67ecf790">W19-0114</url>
      <bibkey>burnett-bonami-2019-conceptual</bibkey>
    </paper>
    <paper id="15">
      <title>Identifying Participation of Individual Verbs or <fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et Classes in the Causative Alternation</title>
      <author><first>Esther</first><last>Seyffarth</last></author>
      <pages>146-155</pages>
      <doi>10.7275/efvz-jy59</doi>
      <url hash="99a292e5">W19-0115</url>
      <bibkey>seyffarth-2019-identifying</bibkey>
    </paper>
    <paper id="16">
      <title>Using Sentiment Induction to Understand Variation in Gendered Online Communities</title>
      <author><first>Li</first><last>Lucy</last></author>
      <author><first>Julia</first><last>Mendelsohn</last></author>
      <pages>156-166</pages>
      <doi>10.7275/11wq-ep51</doi>
      <url hash="f9b51a1b">W19-0116</url>
      <bibkey>lucy-mendelsohn-2019-using</bibkey>
      <pwccode url="https://github.com/lucy3/reddit-sent" additional="false">lucy3/reddit-sent</pwccode>
    </paper>
    <paper id="17">
      <title>On the difficulty of a distributional semantics of spoken language</title>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <author><first>Lieke</first><last>Gelderloos</last></author>
      <author><first>Ákos</first><last>Kádár</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <pages>167-173</pages>
      <doi>10.7275/extq-7546</doi>
      <url hash="d009dec9">W19-0117</url>
      <bibkey>chrupala-etal-2019-difficulty</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="18">
      <title>Distributional Effects of Gender Contrasts Across Categories</title>
      <author><first>Timothee</first><last>Mickus</last></author>
      <author><first>Olivier</first><last>Bonami</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <pages>174-184</pages>
      <doi>10.7275/g11b-3s25</doi>
      <url hash="d3025f7c">W19-0118</url>
      <bibkey>mickus-etal-2019-distributional</bibkey>
    </paper>
    <paper id="19">
      <title>Guess Who’s Coming (and Who’s Going): Bringing Perspective to the Rational Speech Acts Framework</title>
      <author><first>Carolyn Jane</first><last>Anderson</last></author>
      <author><first>Brian W.</first><last>Dillon</last></author>
      <pages>185-194</pages>
      <doi>10.7275/9bn3-8x38</doi>
      <url hash="c4d1ded4">W19-0119</url>
      <bibkey>anderson-dillon-2019-guess</bibkey>
    </paper>
    <paper id="20">
      <title>The organization of sound inventories: A study on obstruent gaps</title>
      <author><first>Sheng-Fu</first><last>Wang</last></author>
      <pages>195-204</pages>
      <doi>10.7275/pbe8-zf60</doi>
      <url hash="8344b449">W19-0120</url>
      <bibkey>wang-2019-organization</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>C</fixed-case>-Command Dependencies as <fixed-case>TSL</fixed-case> String Constraints</title>
      <author><first>Thomas</first><last>Graf</last></author>
      <author><first>Nazila</first><last>Shafiei</last></author>
      <pages>205-215</pages>
      <doi>10.7275/4rrx-x488</doi>
      <url hash="fca14cfa">W19-0121</url>
      <bibkey>graf-shafiei-2019-c</bibkey>
    </paper>
    <paper id="22">
      <title>Modeling the Acquisition of Words with Multiple Meanings</title>
      <author><first>Libby</first><last>Barak</last></author>
      <author><first>Sammy</first><last>Floyd</last></author>
      <author><first>Adele</first><last>Goldberg</last></author>
      <pages>216-225</pages>
      <doi>10.7275/tr21-m273</doi>
      <url hash="ff464bca">W19-0122</url>
      <bibkey>barak-etal-2019-modeling</bibkey>
    </paper>
    <paper id="23">
      <title>Evaluation Order Effects in Dynamic Continuized <fixed-case>CCG</fixed-case>: From Negative Polarity Items to Balanced Punctuation</title>
      <author><first>Michael</first><last>White</last></author>
      <pages>226-235</pages>
      <doi>10.7275/kpch-rk05</doi>
      <url hash="d1c525a4">W19-0123</url>
      <bibkey>white-2019-evaluation</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation for Human-Robot Dialogue</title>
      <author><first>Claire N.</first><last>Bonial</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Jessica</first><last>Ervin</last></author>
      <author><first>Clare R.</first><last>Voss</last></author>
      <pages>236-246</pages>
      <doi>10.7275/v3c5-yd35</doi>
      <url hash="4659169f">W19-0124</url>
      <bibkey>bonial-etal-2019-abstract</bibkey>
    </paper>
    <paper id="25">
      <title>A Logical and Computational Methodology for Exploring Systems of Phonotactic Constraints</title>
      <author><first>Dakotah</first><last>Lambert</last></author>
      <author><first>James</first><last>Rogers</last></author>
      <pages>247-256</pages>
      <doi>10.7275/t0dv-9t05</doi>
      <url hash="15200ace">W19-0125</url>
      <bibkey>lambert-rogers-2019-logical</bibkey>
    </paper>
    <paper id="26">
      <title>Augmentic Compositional Models for Knowledge Base Completion Using Gradient Representations</title>
      <author><first>Matthias R.</first><last>Lalisse</last></author>
      <author><first>Paul</first><last>Smolensky</last></author>
      <pages>257-266</pages>
      <doi>10.7275/8et8-qd83</doi>
      <url hash="29a65256">W19-0126</url>
      <bibkey>lalisse-smolensky-2019-augmentic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
    </paper>
    <paper id="27">
      <title>Case assignment in <fixed-case>TSL</fixed-case> syntax: a case study</title>
      <author><first>Mai Ha</first><last>Vu</last></author>
      <author><first>Nazila</first><last>Shafiei</last></author>
      <author><first>Thomas</first><last>Graf</last></author>
      <pages>267-276</pages>
      <doi>10.7275/sywz-xw23</doi>
      <url hash="7c56a282">W19-0127</url>
      <bibkey>vu-etal-2019-case</bibkey>
    </paper>
    <paper id="28">
      <title>On Evaluating the Generalization of <fixed-case>LSTM</fixed-case> Models in Formal Languages</title>
      <author><first>Mirac</first><last>Suzgun</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Stuart M.</first><last>Shieber</last></author>
      <pages>277-286</pages>
      <doi>10.7275/s02b-4d91</doi>
      <url hash="cf4fee51">W19-0128</url>
      <bibkey>suzgun-etal-2019-evaluating</bibkey>
      <pwccode url="https://github.com/suzgunmirac/lstm-eval" additional="false">suzgunmirac/lstm-eval</pwccode>
    </paper>
    <paper id="29">
      <title>Verb Argument Structure Alternations in Word and Sentence Embeddings</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Alex</first><last>Warstadt</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>287-297</pages>
      <doi>10.7275/q5js-4y86</doi>
      <url hash="4fd2696f">W19-0129</url>
      <bibkey>kann-etal-2019-verb</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
      <url hash="57ba8ec6">W19-03</url>
      <editor><first>Tommi A.</first><last>Pirinen</last></editor>
      <editor><first>Heiki-Jaan</first><last>Kaalep</last></editor>
      <editor><first>Francis M.</first><last>Tyers</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Tartu, Estonia</address>
      <month>January</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="d2a59d94">W19-0300</url>
      <bibkey>ws-2019-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Data-Driven Morphological Analysis for <fixed-case>U</fixed-case>ralic Languages</title>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>1–14</pages>
      <url hash="93b20deb">W19-0301</url>
      <doi>10.18653/v1/W19-0301</doi>
      <bibkey>silfverberg-tyers-2019-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>N</fixed-case>orth <fixed-case>S</fixed-case>ámi morphological segmentation with low-resource semi-supervised sequence labeling</title>
      <author><first>Stig-Arne</first><last>Grönroos</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>15–26</pages>
      <url hash="3c4c6379">W19-0302</url>
      <doi>10.18653/v1/W19-0302</doi>
      <bibkey>gronroos-etal-2019-north</bibkey>
    </paper>
    <paper id="3">
      <title>What does the Nom say? An algorithm for case disambiguation in <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Noémi</first><last>Ligeti-Nagy</last></author>
      <author><first>Andrea</first><last>Dömötör</last></author>
      <author><first>Noémi</first><last>Vadász</last></author>
      <pages>27–41</pages>
      <url hash="546234ee">W19-0303</url>
      <doi>10.18653/v1/W19-0303</doi>
      <bibkey>ligeti-nagy-etal-2019-nom</bibkey>
    </paper>
    <paper id="4">
      <title>A Contrastive Evaluation of Word Sense Disambiguation Systems for <fixed-case>F</fixed-case>innish</title>
      <author><first>Frankie</first><last>Robertson</last></author>
      <pages>42–54</pages>
      <url hash="003d9a1d">W19-0304</url>
      <doi>10.18653/v1/W19-0304</doi>
      <bibkey>robertson-2019-contrastive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="5">
      <title>Elliptical Constructions in <fixed-case>E</fixed-case>stonian <fixed-case>UD</fixed-case> Treebank</title>
      <author><first>Kadri</first><last>Muischnek</last></author>
      <author><first>Liisi</first><last>Torga</last></author>
      <pages>55–65</pages>
      <url hash="b85ab146">W19-0305</url>
      <doi>10.18653/v1/W19-0305</doi>
      <bibkey>muischnek-torga-2019-elliptical</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>F</fixed-case>i<fixed-case>ST</fixed-case> – towards a free Semantic Tagger of modern standard <fixed-case>F</fixed-case>innish</title>
      <author><first>Kimmo</first><last>Kettunen</last></author>
      <pages>66–76</pages>
      <url hash="0cc3ed94">W19-0306</url>
      <doi>10.18653/v1/W19-0306</doi>
      <bibkey>kettunen-2019-fist</bibkey>
    </paper>
    <paper id="7">
      <title>An <fixed-case>OCR</fixed-case> system for the Unified Northern Alphabet</title>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Michael</first><last>Rießler</last></author>
      <pages>77–89</pages>
      <url hash="368fe8dc">W19-0307</url>
      <doi>10.18653/v1/W19-0307</doi>
      <bibkey>partanen-riessler-2019-ocr</bibkey>
      <pwccode url="https://github.com/langdoc/iwclul2019" additional="false">langdoc/iwclul2019</pwccode>
    </paper>
    <paper id="8">
      <title><fixed-case>ELAN</fixed-case> as a search engine for hierarchically structured, tagged corpora</title>
      <author><first>Joshua</first><last>Wilbur</last></author>
      <pages>90–103</pages>
      <url hash="b5714270">W19-0308</url>
      <doi>10.18653/v1/W19-0308</doi>
      <bibkey>wilbur-2019-elan</bibkey>
    </paper>
    <paper id="9">
      <title>Neural and rule-based <fixed-case>F</fixed-case>innish <fixed-case>NLP</fixed-case> models—expectations, experiments and experiences</title>
      <author><first>Tommi A</first><last>Pirinen</last></author>
      <pages>104–114</pages>
      <url hash="0329ccd0">W19-0309</url>
      <doi>10.18653/v1/W19-0309</doi>
      <bibkey>pirinen-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="10">
      <title><fixed-case>U</fixed-case>ralic multimedia corpora: <fixed-case>ISO</fixed-case>/<fixed-case>TEI</fixed-case> corpus data in the project <fixed-case>INEL</fixed-case></title>
      <author><first>Timofey</first><last>Arkhangelskiy</last></author>
      <author><first>Anne</first><last>Ferger</last></author>
      <author><first>Hanna</first><last>Hedeland</last></author>
      <pages>115–124</pages>
      <url hash="851d6bfc">W19-0310</url>
      <doi>10.18653/v1/W19-0310</doi>
      <bibkey>arkhangelskiy-etal-2019-uralic</bibkey>
    </paper>
    <paper id="11">
      <title>Corpora of social media in minority <fixed-case>U</fixed-case>ralic languages</title>
      <author><first>Timofey</first><last>Arkhangelskiy</last></author>
      <pages>125–140</pages>
      <url hash="c392ede3">W19-0311</url>
      <doi>10.18653/v1/W19-0311</doi>
      <bibkey>arkhangelskiy-2019-corpora</bibkey>
    </paper>
    <paper id="12">
      <title>Is this the end? Two-step tokenization of sentence boundaries</title>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <author><first>Sjur Nørstebø</first><last>Moshagen</last></author>
      <author><first>Thomas</first><last>Omma</last></author>
      <pages>141–153</pages>
      <url hash="251057f2">W19-0312</url>
      <doi>10.18653/v1/W19-0312</doi>
      <bibkey>wiechetek-etal-2019-end</bibkey>
    </paper>
    <paper id="13">
      <title>Learning multilingual topics through aspect extraction from monolingual texts</title>
      <author><first>Johannes</first><last>Huber</last></author>
      <author><first>Myra</first><last>Spiliopoulou</last></author>
      <pages>154–183</pages>
      <url hash="78670268">W19-0313</url>
      <doi>10.18653/v1/W19-0313</doi>
      <bibkey>huber-spiliopoulou-2019-learning</bibkey>
    </paper>
    <paper id="14">
      <title>Electronical resources for <fixed-case>L</fixed-case>ivonian</title>
      <author><first>Valts</first><last>Ernštreits</last></author>
      <pages>184–191</pages>
      <url hash="cc3d95d5">W19-0314</url>
      <doi>10.18653/v1/W19-0314</doi>
      <bibkey>ernstreits-2019-electronical</bibkey>
    </paper>
    <paper id="15">
      <title>The use of Extract Morphology for Automatic Generation of Language Technology for <fixed-case>V</fixed-case>otic</title>
      <author><first>Kristian</first><last>Kankainen</last></author>
      <pages>192–204</pages>
      <url hash="77cdb0e6">W19-0315</url>
      <doi>10.18653/v1/W19-0315</doi>
      <bibkey>kankainen-2019-use</bibkey>
    </paper>
  </volume>
  <volume id="4">
    <meta>
      <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
      <url hash="1f24a1f3">W19-04</url>
      <editor><first>Simon</first><last>Dobnik</last></editor>
      <editor><first>Stergios</first><last>Chatzikyriakidis</last></editor>
      <editor><first>Vera</first><last>Demberg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>iwcs</venue>
    </meta>
    <frontmatter>
      <url hash="b7fe59fe">W19-0400</url>
      <bibkey>ws-2019-international-semantics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Projecting Temporal Properties, Events and Actions</title>
      <author><first>Tim</first><last>Fernando</last></author>
      <pages>1–12</pages>
      <abstract>Temporal notions based on a finite set <i>A</i> of properties are represented in strings, on which projections are defined that vary the granularity <i>A</i>. The structure of properties in <i>A</i> is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The projections proposed are deployed as labels for records and record types amenable to finite-state methods.</abstract>
      <url hash="b7067a58">W19-0401</url>
      <doi>10.18653/v1/W19-0401</doi>
      <bibkey>fernando-2019-projecting</bibkey>
    </paper>
    <paper id="2">
      <title>A Type-coherent, Expressive Representation as an Initial Step to Language Understanding</title>
      <author><first>Gene Louis</first><last>Kim</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>13–30</pages>
      <abstract>A growing interest in tasks involving language understanding by the NLP community has led to the need for effective semantic parsing and inference. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms (ULF) for Episodic Logic (EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, word sense, and anaphora unresolved; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers.</abstract>
      <url hash="b41318e3">W19-0402</url>
      <doi>10.18653/v1/W19-0402</doi>
      <bibkey>kim-schubert-2019-type</bibkey>
    </paper>
    <paper id="3">
      <title>A Semantic Annotation Scheme for Quantification</title>
      <author><first>Harry</first><last>Bunt</last></author>
      <pages>31–42</pages>
      <abstract>This paper describes in brief the proposal called ‘QuantML’ which was accepted by the International Organisation for Standards (ISO) last February as a starting point for developing a standard for the interoperable annotation of quantification phenomena in natural language, as part of the ISO 24617 Semantic Annotation Framework. The proposal, firmly rooted in the theory of generalised quantifiers, neo-Davidsonian semantics, and DRT, covers a wide range of quantification phenomena. The QuantML scheme consists of (1) an abstract syntax which defines ‘annotation structures’ as triples and other set-theoretic constructs; (b) a compositional semantics of annotation structures; (3) an XML representation of annotation structures.</abstract>
      <url hash="0ceab1b5">W19-0403</url>
      <doi>10.18653/v1/W19-0403</doi>
      <bibkey>bunt-2019-semantic</bibkey>
    </paper>
    <paper id="4">
      <title>Re-Ranking Words to Improve Interpretability of Automatically Generated Topics</title>
      <author><first>Areej</first><last>Alokaili</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>43–54</pages>
      <abstract>Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.</abstract>
      <url hash="c5e9cf9c">W19-0404</url>
      <doi>10.18653/v1/W19-0404</doi>
      <bibkey>alokaili-etal-2019-ranking</bibkey>
      <pwccode url="https://github.com/areejokaili/topic_reranking" additional="false">areejokaili/topic_reranking</pwccode>
    </paper>
    <paper id="5">
      <title>An Improved Approach for Semantic Graph Composition with <fixed-case>CCG</fixed-case></title>
      <author><first>Austin</first><last>Blodgett</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>55–70</pages>
      <abstract>This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new semantics for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators: these require that the two constituents being combined overlap in one AMR relation. We also provide a new semantics for type raising, which is necessary for certain constructions. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.</abstract>
      <url hash="6b676d51">W19-0405</url>
      <doi>10.18653/v1/W19-0405</doi>
      <bibkey>blodgett-schneider-2019-improved</bibkey>
    </paper>
    <paper id="6">
      <title>A Semantic Ontology of <fixed-case>D</fixed-case>anish Adjectives</title>
      <author><first>Eckhard</first><last>Bick</last></author>
      <pages>71–78</pages>
      <abstract>This paper presents a semantic annotation scheme for Danish adjectives, focusing both on prototypical semantic content and semantic collocational restrictions on an adjective’s head noun. The core type set comprises about 110 categories ordered in a shallow hierarchy with 14 primary and 25 secondary umbrella categories. In addition, domain information and binary sentiment tags are provided, as well as VerbNet-derived frames and semantic roles for those adjectives governing arguments. The scheme has been almost fully implemented on the lexicon of the Danish VISL parser, DanGram, containing 14,000 adjectives. We discuss the annotation scheme and its applicational perspectives, and present a statistical breakdown and coverage evaluation for three Danish reference corpora.</abstract>
      <url hash="bd69b6af">W19-0406</url>
      <doi>10.18653/v1/W19-0406</doi>
      <bibkey>bick-2019-semantic</bibkey>
    </paper>
    <paper id="7">
      <title>Towards a Compositional Analysis of <fixed-case>G</fixed-case>erman Light Verb Constructions (<fixed-case>LVC</fixed-case>s) Combining <fixed-case>L</fixed-case>exicalized <fixed-case>T</fixed-case>ree <fixed-case>A</fixed-case>djoining <fixed-case>G</fixed-case>rammar (<fixed-case>LTAG</fixed-case>) with Frame Semantics</title>
      <author><first>Jens</first><last>Fleischhauer</last></author>
      <author><first>Thomas</first><last>Gamerschlag</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Simon</first><last>Petitjean</last></author>
      <pages>79–90</pages>
      <abstract>Complex predicates formed of a semantically ‘light’ verbal head and a noun or verb which contributes the major part of the meaning are frequently referred to as ‘light verb constructions’ (LVCs). In the paper, we present a case study of LVCs with the German posture verb stehen ‘stand’. In our account, we model the syntactic as well as semantic composition of such LVCs by combining Lexicalized Tree Adjoining Grammar (LTAG) with frames. Starting from the analysis of the literal uses of posture verbs, we show how the meaning components of the literal uses are systematically exploited in the interpretation of stehen-LVCs. The paper constitutes an important step towards a compositional and computational analysis of LVCs. We show that LTAG allows us to separate constructional from lexical meaning components and that frames enable elegant generalizations over event types and related constraints.</abstract>
      <url hash="821174a0">W19-0407</url>
      <doi>10.18653/v1/W19-0407</doi>
      <bibkey>fleischhauer-etal-2019-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Words are Vectors, Dependencies are Matrices: Learning Word Embeddings from Dependency Graphs</title>
      <author><first>Paula</first><last>Czarnowska</last></author>
      <author><first>Guy</first><last>Emerson</last></author>
      <author><first>Ann</first><last>Copestake</last></author>
      <pages>91–102</pages>
      <abstract>Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts. Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations. In this work, we propose a new dependency-based DSM. The novelty of our model lies in associating an independent meaning representation, a matrix, with each dependency-label. This allows it to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks. In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word.</abstract>
      <url hash="b10269ae">W19-0408</url>
      <doi>10.18653/v1/W19-0408</doi>
      <bibkey>czarnowska-etal-2019-words</bibkey>
    </paper>
    <paper id="9">
      <title>Temporal and Aspectual Entailment</title>
      <author><first>Thomas</first><last>Kober</last></author>
      <author><first>Sander</first><last>Bijl de Vroe</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>103–119</pages>
      <abstract>Inferences regarding “Jane’s arrival in London” from predications such as “Jane is going to London” or “Jane has gone to London” depend on tense and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of “going to London” is completed and whether its consequences hold at that time or not. While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern embedding models capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform inference on predications across aspectual types and tenses. We show that they encode a substantial amount of information relating to tense and aspect, but fail to consistently model inferences that require reasoning with these semantic properties.</abstract>
      <url hash="7421d30a">W19-0409</url>
      <doi>10.18653/v1/W19-0409</doi>
      <bibkey>kober-etal-2019-temporal</bibkey>
      <pwccode url="https://github.com/tttthomasssss/iwcs2019" additional="false">tttthomasssss/iwcs2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="10">
      <title>Don’t Blame Distributional Semantics if it can’t do Entailment</title>
      <author><first>Matthijs</first><last>Westera</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <pages>120–133</pages>
      <abstract>Distributional semantics has had enormous empirical success in Computational Linguistics and Cognitive Science in modeling various semantic phenomena, such as semantic similarity, and distributional models are widely used in state-of-the-art Natural Language Processing systems. However, the theoretical status of distributional semantics within a broader theory of language and cognition is still unclear: What does distributional semantics model? Can it be, on its own, a fully adequate model of the meanings of linguistic expressions? The standard answer is that distributional semantics is not fully adequate in this regard, because it falls short on some of the central aspects of formal semantic approaches: truth conditions, entailment, reference, and certain aspects of compositionality. We argue that this standard answer rests on a misconception: These aspects do not belong in a theory of expression meaning, they are instead aspects of speaker meaning, i.e., communicative intentions in a particular context. In a slogan: words do not refer, speakers do. Clearing this up enables us to argue that distributional semantics on its own is an adequate model of expression meaning. Our proposal sheds light on the role of distributional semantics in a broader theory of language and cognition, its relationship to formal semantics, and its place in computational models.</abstract>
      <url hash="34052f75">W19-0410</url>
      <doi>10.18653/v1/W19-0410</doi>
      <bibkey>westera-boleda-2019-dont</bibkey>
    </paper>
    <paper id="11">
      <title>Ambiguity in Explicit Discourse Connectives</title>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Rashmi</first><last>Prasad</last></author>
      <author><first>Alan</first><last>Lee</last></author>
      <pages>134–141</pages>
      <abstract>Discourse connectives are known to be subject to both usage and sense ambiguity, as has already been discussed in the literature. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four are illustrated and discussed here.</abstract>
      <url hash="f7d69e5f">W19-0411</url>
      <doi>10.18653/v1/W19-0411</doi>
      <bibkey>webber-etal-2019-ambiguity</bibkey>
    </paper>
    <paper id="12">
      <title>Aligning Open <fixed-case>IE</fixed-case> Relations and <fixed-case>KB</fixed-case> Relations using a <fixed-case>S</fixed-case>iamese Network Based on Word Embedding</title>
      <author><first>Rifki Afina</first><last>Putri</last></author>
      <author><first>Giwon</first><last>Hong</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>142–153</pages>
      <abstract>Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text. Given a triple, the relation expresses the type of semantic relation between the entities. Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning. As a way to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB. Our approach is to use a Siamese network that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation. In order to make the approach practical, we automatically generate a training dataset using a distant supervision approach instead of relying on a hand-labeled dataset. Our experiment shows that the proposed method can capture the relational semantics better than the recent approaches.</abstract>
      <url hash="0b875732">W19-0412</url>
      <doi>10.18653/v1/W19-0412</doi>
      <bibkey>putri-etal-2019-aligning</bibkey>
    </paper>
    <paper id="13">
      <title>Language-Agnostic Model for Aspect-Based Sentiment Analysis</title>
      <author><first>Md Shad</first><last>Akhtar</last></author>
      <author><first>Abhishek</first><last>Kumar</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>154–164</pages>
      <abstract>In this paper, we propose a language-agnostic deep neural network architecture for aspect-based sentiment analysis. The proposed approach is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network, which is further assisted with extra hand-crafted features. We define three different architectures for the successful combination of word embeddings and hand-crafted features. We evaluate the proposed approach for six languages (i.e. English, Spanish, French, Dutch, German and Hindi) and two problems (i.e. aspect term extraction and aspect sentiment classification). Experiments show that the proposed model attains state-of-the-art performance in most of the settings.</abstract>
      <url hash="435bc3ed">W19-0413</url>
      <doi>10.18653/v1/W19-0413</doi>
      <bibkey>akhtar-etal-2019-language</bibkey>
    </paper>
    <paper id="14">
      <title>The Effect of Context on Metaphor Paraphrase Aptness Judgments</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Shalom</first><last>Lappin</last></author>
      <pages>165–175</pages>
      <abstract>We conduct two experiments to study the effect of context on metaphor paraphrase aptness judgments. The first is an AMT crowd source task in which speakers rank metaphor-paraphrase candidate sentence pairs in short document contexts for paraphrase aptness. In the second we train a composite DNN to predict these human judgments, first in binary classifier mode, and then as gradient ratings. We found that for both mean human judgments and our DNN’s predictions, adding document context compresses the aptness scores towards the center of the scale, raising low out-of-context ratings and decreasing high out-of-context scores. We offer a provisional explanation for this compression effect.</abstract>
      <url hash="829fe7f1">W19-0414</url>
      <doi>10.18653/v1/W19-0414</doi>
      <bibkey>bizzoni-lappin-2019-effect</bibkey>
      <pwccode url="https://github.com/yuri-bizzoni/Metaphor-Paraphrase" additional="false">yuri-bizzoni/Metaphor-Paraphrase</pwccode>
    </paper>
    <paper id="15">
      <title>Predicting Word Concreteness and Imagery</title>
      <author><first>Jean</first><last>Charbonnier</last></author>
      <author><first>Christian</first><last>Wartena</last></author>
      <pages>176–187</pages>
      <abstract>Concreteness of words has been studied extensively in psycholinguistic literature. A number of datasets have been created with average values for perceived concreteness of words. We show that we can train a regression model on these data, using word embeddings and morphological features, that can predict these concreteness values with high accuracy. We evaluate the model on 7 publicly available datasets. Only for a few small subsets of these datasets prediction of concreteness values are found in the literature. Our results clearly outperform the reported results for these datasets.</abstract>
      <url hash="6964b15c">W19-0415</url>
      <doi>10.18653/v1/W19-0415</doi>
      <bibkey>charbonnier-wartena-2019-predicting</bibkey>
    </paper>
    <paper id="16">
      <title>Learning to Explicitate Connectives with <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Network for Implicit Discourse Relation Classification</title>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>188–199</pages>
      <abstract>Implicit discourse relation classification is one of the most difficult steps in discourse parsing. The difficulty stems from the fact that the coherence relation must be inferred based on the content of the discourse relational arguments. Therefore, an effective encoding of the relational arguments is of crucial importance. We here propose a new model for implicit discourse relation classification, which consists of a classifier, and a sequence-to-sequence model which is trained to generate a representation of the discourse relational arguments by trying to predict the relational arguments including a suitable implicit connective. Training is possible because such implicit connectives have been annotated as part of the PDTB corpus. Along with a memory network, our model could generate more refined representations for the task. And on the now standard 11-way classification, our method outperforms the previous state of the art systems on the PDTB benchmark on multiple settings including cross validation.</abstract>
      <url hash="cae9abf7">W19-0416</url>
      <doi>10.18653/v1/W19-0416</doi>
      <bibkey>shi-demberg-2019-learning</bibkey>
    </paper>
    <paper id="17">
      <title>Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles</title>
      <author><first>Maryam</first><last>Aminian</last></author>
      <author><first>Mohammad Sadegh</first><last>Rasooli</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>200–210</pages>
      <abstract>We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.</abstract>
      <url hash="26f68cb0">W19-0417</url>
      <doi>10.18653/v1/W19-0417</doi>
      <bibkey>aminian-etal-2019-cross</bibkey>
    </paper>
    <paper id="18">
      <title>Evaluating the Representational Hub of Language and Vision Models</title>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <author><first>Ece</first><last>Takmaz</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>211–222</pages>
      <abstract>The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the “Hub and Spoke” architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs. In particular, the Hub is implemented as a neural network encoder. We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue. To measure the quality of the representations learned by the encoder, we use two kinds of analyses. First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing “diagnostic task” designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities.</abstract>
      <url hash="b995aa04">W19-0418</url>
      <doi>10.18653/v1/W19-0418</doi>
      <bibkey>shekhar-etal-2019-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqg">VQG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="19">
      <title>The Fast and the Flexible: Training Neural Networks to Learn to Follow Instructions from Small Data</title>
      <author><first>Rezka</first><last>Leonandya</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <author><first>Germán</first><last>Kruszewski</last></author>
      <pages>223–234</pages>
      <abstract>Learning to follow human instructions is a long-pursued goal in artificial intelligence. The task becomes particularly challenging if no prior knowledge of the employed language is assumed while relying only on a handful of examples to learn from. Work in the past has relied on hand-coded components or manually engineered features to provide strong inductive biases that make learning in such situations possible. In contrast, here we seek to establish whether this knowledge can be acquired automatically by a neural network system through a two phase training procedure: A (slow) offline learning stage where the network learns about the general structure of the task and a (fast) online adaptation phase where the network learns the language of a new given speaker. Controlled experiments show that when the network is exposed to familiar instructions but containing novel words, the model adapts very efficiently to the new vocabulary. Moreover, even for human speakers whose language usage can depart significantly from our artificial training language, our network can still make use of its automatically acquired inductive bias to learn to follow instructions more effectively.</abstract>
      <url hash="9166a81a">W19-0419</url>
      <doi>10.18653/v1/W19-0419</doi>
      <bibkey>leonandya-etal-2019-fast</bibkey>
      <pwccode url="https://github.com/rezkaaufar/fast-and-flexible" additional="false">rezkaaufar/fast-and-flexible</pwccode>
    </paper>
    <paper id="20">
      <title>Fast and Discriminative Semantic Embedding</title>
      <author><first>Rob</first><last>Koopman</last></author>
      <author><first>Shenghui</first><last>Wang</last></author>
      <author><first>Gwenn</first><last>Englebienne</last></author>
      <pages>235–246</pages>
      <abstract>The embedding of words and documents in compact, semantically meaningful vector spaces is a crucial part of modern information systems. Deep Learning models are powerful but their hyperparameter selection is often complex and they are expensive to train, and while pre-trained models are available, embeddings trained on general corpora are not necessarily well-suited to domain specific tasks. We propose a novel embedding method which extends random projection by weighting and projecting raw term embeddings orthogonally to an average language vector, thus improving the discriminating power of resulting term embeddings, and build more meaningful document embeddings by assigning appropriate weights to individual terms. We describe how updating the term embeddings online as we process the training data results in an extremely efficient method, in terms of both computational and memory requirements. Our experiments show highly competitive results with various state-of-the-art embedding methods on different tasks, including the standard STS benchmark and a subject prediction task, at a fraction of the computational cost.</abstract>
      <url hash="0b64978d">W19-0420</url>
      <doi>10.18653/v1/W19-0420</doi>
      <bibkey>koopman-etal-2019-fast</bibkey>
    </paper>
    <paper id="21">
      <title>Using Multi-Sense Vector Embeddings for Reverse Dictionaries</title>
      <author><first>Michael A.</first><last>Hedderich</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>247–258</pages>
      <abstract>Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.</abstract>
      <url hash="1a5010cb">W19-0421</url>
      <doi>10.18653/v1/W19-0421</doi>
      <bibkey>hedderich-etal-2019-using</bibkey>
      <pwccode url="https://github.com/uds-lsv/Multi-Sense-Embeddings-Reverse-Dictionaries" additional="false">uds-lsv/Multi-Sense-Embeddings-Reverse-Dictionaries</pwccode>
    </paper>
    <paper id="22">
      <title>Using <fixed-case>W</fixed-case>iktionary as a resource for <fixed-case>WSD</fixed-case> : the case of <fixed-case>F</fixed-case>rench verbs</title>
      <author><first>Vincent</first><last>Segonne</last></author>
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>259–270</pages>
      <abstract>As opposed to word sense induction, word sense disambiguation (WSD) has the advantage of us-ing interpretable senses, but requires annotated data, which are quite rare for most languages except English (Miller et al. 1993; Fellbaum, 1998). In this paper, we investigate which strategy to adopt to achieve WSD for languages lacking data that was annotated specifically for the task, focusing on the particular case of verb disambiguation in French. We first study the usability of Eurosense (Bovi et al. 2017) , a multilingual corpus extracted from Europarl (Kohen, 2005) and automatically annotated with BabelNet (Navigli and Ponzetto, 2010) senses. Such a resource opened up the way to supervised and semi-supervised WSD for resourceless languages like French. While this perspective looked promising, our evaluation on French verbs was inconclusive and showed the annotated senses’ quality was not sufficient for supervised WSD on French verbs. Instead, we propose to use Wiktionary, a collaboratively edited, multilingual online dictionary, as a resource for WSD. Wiktionary provides both sense inventory and manually sense tagged examples which can be used to train supervised and semi-supervised WSD systems. Yet, because senses’ distribution differ in lexicographic examples found in Wiktionary with respect to natural text, we then focus on studying the impact on WSD of the training data size and senses’ distribution. Using state-of-the art semi-supervised systems, we report experiments of Wiktionary-based WSD for French verbs, evaluated on FrenchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses.</abstract>
      <url hash="b6100c85">W19-0422</url>
      <doi>10.18653/v1/W19-0422</doi>
      <bibkey>segonne-etal-2019-using</bibkey>
    </paper>
    <paper id="23">
      <title>A Comparison of Context-sensitive Models for Lexical Substitution</title>
      <author><first>Aina</first><last>Garí Soler</last></author>
      <author><first>Anne</first><last>Cocos</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>271–282</pages>
      <abstract>Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks. Embedding approaches differ as to whether and how they account for the context surrounding a word. We present a comparison of different word and context representations on the task of proposing substitutes for a target word in context (lexical substitution). We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word. We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution. This is better handled by models trained with this objective in mind, where the inter-dependence between word and context representations is explicitly modeled during training.</abstract>
      <url hash="d4957e18">W19-0423</url>
      <doi>10.18653/v1/W19-0423</doi>
      <bibkey>gari-soler-etal-2019-comparison</bibkey>
    </paper>
    <paper id="24">
      <title>Natural Language Semantics With Pictures: Some Language &amp; Vision Datasets and Potential Uses for Computational Semantics</title>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>283–294</pages>
      <abstract>Propelling, and propelled by, the “deep learning revolution”, recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the “linked to same image” relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability.</abstract>
      <url hash="edf55d34">W19-0424</url>
      <doi>10.18653/v1/W19-0424</doi>
      <bibkey>schlangen-2019-natural</bibkey>
    </paper>
    <paper id="25">
      <title>Frame Identification as Categorization: Exemplars vs Prototypes in Embeddingland</title>
      <author><first>Jennifer</first><last>Sikos</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>295–306</pages>
      <abstract>Categorization is a central capability of human cognition, and a number of theories have been developed to account for properties of categorization. Even though many tasks in semantics also involve categorization of some kind, theories of categorization do not play a major role in contemporary research in computational linguistics. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two design variables: (a) the choice of prototype vs. exemplar categorization, corresponding to different degrees of generalization applied to the input; and (b) the presence vs. absence of a fine-tuning step, corresponding to generic vs. task-adaptive categorization. We find that for frame identification, generalization and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification.</abstract>
      <url hash="3b7267e6">W19-0425</url>
      <doi>10.18653/v1/W19-0425</doi>
      <bibkey>sikos-pado-2019-frame</bibkey>
    </paper>
  </volume>
  <volume id="5">
    <meta>
      <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
      <url hash="9cb49636">W19-05</url>
      <editor><first>Simon</first><last>Dobnik</last></editor>
      <editor><first>Stergios</first><last>Chatzikyriakidis</last></editor>
      <editor><first>Vera</first><last>Demberg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>iwcs</venue>
    </meta>
    <frontmatter>
      <url hash="fd2b6176">W19-0500</url>
      <bibkey>ws-2019-international-semantics-short</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Distributional Model of Affordances in Semantic Type Coercion</title>
      <author><first>Stephen</first><last>McGregor</last></author>
      <author><first>Elisabetta</first><last>Jezek</last></author>
      <pages>1–7</pages>
      <abstract>We explore a novel application for interpreting semantic type coercions, motivated by insight into the role that perceptual affordances play in the selection of the semantic roles of artefactual nouns which are observed as arguments for verbs which would stereotypically select for objects of a different type. In order to simulate affordances, which we take to be direct perceptions of context-specific opportunities for action, we preform a distributional analysis dependency relationships between target words and their modifiers and adjuncts. We use these relationships as the basis for generating on-line transformations which project semantic subspaces in which the interpretations of coercive compositions are expected to emerge as salient word-vectors. We offer some preliminary examples of how this model operates on a dataset of sentences involving coercive interactions between verbs and objects specifically designed to evaluate this work.</abstract>
      <url hash="aa594914">W19-0501</url>
      <doi>10.18653/v1/W19-0501</doi>
      <bibkey>mcgregor-jezek-2019-distributional</bibkey>
    </paper>
    <paper id="2">
      <title>Natural Language Inference with Monotonicity</title>
      <author><first>Hai</first><last>Hu</last></author>
      <author><first>Qi</first><last>Chen</last></author>
      <author><first>Larry</first><last>Moss</last></author>
      <pages>8–15</pages>
      <abstract>This paper describes a working system which performs natural language inference using polarity-marked parse trees. The system handles all of the instances of monotonicity inference in the FraCaS data set. Except for the initial parse, it is entirely deterministic. It handles multi-premise arguments, and the kind of inference performed is essentially “logical”, but it goes beyond what is representable in first-order logic. In any case, the system works on surface forms rather than on representations of any kind.</abstract>
      <url hash="4f9dcc1d">W19-0502</url>
      <doi>10.18653/v1/W19-0502</doi>
      <bibkey>hu-etal-2019-natural</bibkey>
    </paper>
    <paper id="3">
      <title>Distributional Semantics in the Real World: Building Word Vector Representations from a Truth-Theoretic Model</title>
      <author><first>Elizaveta</first><last>Kuzmenko</last></author>
      <author><first>Aurélie</first><last>Herbelot</last></author>
      <pages>16–23</pages>
      <abstract>Distributional semantics models (DSMs) are known to produce excellent representations of word meaning, which correlate with a range of behavioural data. As lexical representations, they have been said to be fundamentally different from truth-theoretic models of semantics, where meaning is defined as a correspondence relation to the world. There are two main aspects to this difference: a) DSMs are built over corpus data which may or may not reflect ‘what is in the world’; b) they are built from word co-occurrences, that is, from lexical types rather than entities and sets. In this paper, we inspect the properties of a distributional model built over a set-theoretic approximation of ‘the real world’. To achieve this, we take the annotation a large database of images marked with objects, attributes and relations, convert the data into a representation akin to first-order logic and build several distributional models using various combinations of features. We evaluate those models over both relatedness and similarity datasets, demonstrating their effectiveness in standard evaluations. This allows us to conclude that, despite prior claims, truth-theoretic models are good candidates for building graded lexical representations of meaning.</abstract>
      <url hash="cdfd4fc5">W19-0503</url>
      <doi>10.18653/v1/W19-0503</doi>
      <bibkey>kuzmenko-herbelot-2019-distributional</bibkey>
    </paper>
    <paper id="4">
      <title>Linguistic Information in Neural Semantic Parsing with Multiple Encoders</title>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>24–31</pages>
      <abstract>Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.</abstract>
      <url hash="8e1e9fdf">W19-0504</url>
      <doi>10.18653/v1/W19-0504</doi>
      <bibkey>van-noord-etal-2019-linguistic</bibkey>
    </paper>
    <paper id="5">
      <title>Making Sense of Conflicting (Defeasible) Rules in the Controlled Natural Language <fixed-case>ACE</fixed-case>: Design of a System with Support for Existential Quantification Using Skolemization</title>
      <author><first>Martin</first><last>Diller</last></author>
      <author><first>Adam</first><last>Wyner</last></author>
      <author><first>Hannes</first><last>Strass</last></author>
      <pages>32–37</pages>
      <abstract>We present the design of a system for making sense of conflicting rules expressed in a fragment of the prominent controlled natural language ACE, yet extended with means of expressing defeasible rules in the form of normality assumptions. The approach we describe is ultimately based on answer-set-programming (ASP); simulating existential quantification by using skolemization in a manner resembling a translation for ASP recently formalized in the context of ∃-ASP. We discuss the advantages of this approach to building on the existing ACE interface to rule-systems, ACERules.</abstract>
      <url hash="d00bcec7">W19-0505</url>
      <doi>10.18653/v1/W19-0505</doi>
      <bibkey>diller-etal-2019-making</bibkey>
    </paper>
    <paper id="6">
      <title>Distributional Interaction of Concreteness and Abstractness in Verb–Noun Subcategorisation</title>
      <author><first>Diego</first><last>Frassinelli</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>38–43</pages>
      <abstract>In recent years, both cognitive and computational research has provided empirical analyses of contextual co-occurrence of concrete and abstract words, partially resulting in inconsistent pictures. In this work we provide a more fine-grained description of the distributional nature in the corpus-based interaction of verbs and nouns within subcategorisation, by investigating the concreteness of verbs and nouns that are in a specific syntactic relationship with each other, i.e., subject, direct object, and prepositional object. Overall, our experiments show consistent patterns in the distributional representation of subcategorising and subcategorised concrete and abstract words. At the same time, the studies reveal empirical evidence why contextual abstractness represents a valuable indicator for automatic non-literal language identification.</abstract>
      <url hash="eb0ec29b">W19-0506</url>
      <doi>10.18653/v1/W19-0506</doi>
      <bibkey>frassinelli-schulte-im-walde-2019-distributional</bibkey>
    </paper>
    <paper id="7">
      <title>Generating a Novel Dataset of Multimodal Referring Expressions</title>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>44–51</pages>
      <abstract>Referring expressions and definite descriptions of objects in space exploit information both about object characteristics and locations. To resolve potential ambiguity, referencing strategies in language can rely on increasingly abstract concepts to distinguish an object in a given location from similar ones elsewhere, yet the description of the intended location may still be imprecise or difficult to interpret. Meanwhile, modalities such as gesture may communicate spatial information such as locations in a more concise manner. In real peer-to-peer communication, humans use language and gesture together to reference entities, with a capacity for mixing and changing modalities where needed. While recent progress in AI and human-computer interaction has created systems where a human can interact with a computer multimodally, computers often lack the capacity to intelligently mix modalities when generating referring expressions. We present a novel dataset of referring expressions combining natural language and gesture, describe its creation and evaluation, and its uses to train computational models for generating and interpreting multimodal referring expressions.</abstract>
      <url hash="0578f6a6">W19-0507</url>
      <doi>10.18653/v1/W19-0507</doi>
      <bibkey>krishnaswamy-pustejovsky-2019-generating</bibkey>
    </paper>
    <paper id="8">
      <title>On Learning Word Embeddings From Linguistically Augmented Text Corpora</title>
      <author><first>Amila</first><last>Silva</last></author>
      <author><first>Chathurika</first><last>Amarathunga</last></author>
      <pages>52–58</pages>
      <abstract>Word embedding is a technique in Natural Language Processing (NLP) to map words into vector space representations. Since it has boosted the performance of many NLP downstream tasks, the task of learning word embeddings has been addressing significantly. Nevertheless, most of the underlying word embedding methods such as word2vec and GloVe fail to produce high-quality embeddings if the text corpus is small and sparse. This paper proposes a method to generate effective word embeddings from limited data. Through experiments, we show that our proposed model outperforms existing works for the classical word similarity task and for a domain-specific application.</abstract>
      <url hash="2eac49c4">W19-0508</url>
      <doi>10.18653/v1/W19-0508</doi>
      <bibkey>silva-amarathunga-2019-learning</bibkey>
    </paper>
    <paper id="9">
      <title>Sentiment Independent Topic Detection in Rated Hospital Reviews</title>
      <author><first>Christian</first><last>Wartena</last></author>
      <author><first>Uwe</first><last>Sander</last></author>
      <author><first>Christiane</first><last>Patzelt</last></author>
      <pages>59–64</pages>
      <abstract>We present a simple method to find topics in user reviews that accompany ratings for products or services. Standard topic analysis will perform sub-optimal on such data since the word distributions in the documents are not only determined by the topics but by the sentiment as well. We reduce the influence of the sentiment on the topic selection by adding two explicit topics, representing positive and negative sentiment. We evaluate the proposed method on a set of over 15,000 hospital reviews. We show that the proposed method, Latent Semantic Analysis with explicit word features, finds topics with a much smaller bias for sentiments than other similar methods.</abstract>
      <url hash="366f25fc">W19-0509</url>
      <doi>10.18653/v1/W19-0509</doi>
      <bibkey>wartena-etal-2019-sentiment</bibkey>
    </paper>
    <paper id="10">
      <title>Investigating the Stability of Concrete Nouns in Word Embeddings</title>
      <author><first>Bénédicte</first><last>Pierrejean</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <pages>65–70</pages>
      <abstract>We know that word embeddings trained using neural-based methods (such as word2vec SGNS) are sensitive to stability problems and that across two models trained using the exact same set of parameters, the nearest neighbors of a word are likely to change. All words are not equally impacted by this internal instability and recent studies have investigated features influencing the stability of word embeddings. This stability can be seen as a clue for the reliability of the semantic representation of a word. In this work, we investigate the influence of the degree of concreteness of nouns on the stability of their semantic representation. We show that for English generic corpora, abstract words are more affected by stability problems than concrete words. We also found that to a certain extent, the difference between the degree of concreteness of a noun and its nearest neighbors can partly explain the stability or instability of its neighbors.</abstract>
      <url hash="28765b64">W19-0510</url>
      <doi>10.18653/v1/W19-0510</doi>
      <bibkey>pierrejean-tanguy-2019-investigating</bibkey>
    </paper>
  </volume>
  <volume id="6">
    <meta>
      <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
      <url hash="2cb1515d">W19-06</url>
      <editor><first>Simon</first><last>Dobnik</last></editor>
      <editor><first>Stergios</first><last>Chatzikyriakidis</last></editor>
      <editor><first>Vera</first><last>Demberg</last></editor>
      <editor><first>Kathrein</first><last>Abu Kwaik</last></editor>
      <editor><first>Vladislav</first><last>Maraev</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>iwcs</venue>
    </meta>
    <frontmatter>
      <url hash="147a2ad7">W19-0600</url>
      <bibkey>ws-2019-international-semantics-student</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Dynamic Semantics for Causal Counterfactuals</title>
      <author><first>Kenneth</first><last>Lai</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>1–8</pages>
      <abstract>Under the standard approach to counterfactuals, to determine the meaning of a counterfactual sentence, we consider the “closest” possible world(s) where the antecedent is true, and evaluate the consequent. Building on the standard approach, some researchers have found that the set of worlds to be considered is dependent on context; it evolves with the discourse. Others have focused on how to define the “distance” between possible worlds, using ideas from causal modeling. This paper integrates the two ideas. We present a semantics for counterfactuals that uses a distance measure based on causal laws, that can also change over time. We show how our semantics can be implemented in the Haskell programming language.</abstract>
      <url hash="7afedb96">W19-0601</url>
      <doi>10.18653/v1/W19-0601</doi>
      <bibkey>lai-pustejovsky-2019-dynamic</bibkey>
    </paper>
    <paper id="2">
      <title>Visual <fixed-case>TTR</fixed-case> - Modelling Visual Question Answering in Type Theory with Records</title>
      <author><first>Ronja</first><last>Utescher</last></author>
      <pages>9–14</pages>
      <abstract>In this paper, I will describe a system that was developed for the task of Visual Question Answering. The system uses the rich type universe of Type Theory with Records (TTR) to model both the utterances about the image, the image itself and classifications made related to the two. At its most basic, the decision of whether any given predicate can be assigned to an object in the image is delegated to a CNN. Consequently, images can be judged as evidence for propositions. The end result is a model whose application of perceptual classifiers to a given image is guided by the accompanying utterance.</abstract>
      <url hash="bcc18d3f">W19-0602</url>
      <doi>10.18653/v1/W19-0602</doi>
      <bibkey>utescher-2019-visual</bibkey>
    </paper>
    <paper id="3">
      <title>The Lexical Gap: An Improved Measure of Automated Image Description Quality</title>
      <author><first>Austin</first><last>Kershaw</last></author>
      <author><first>Miroslaw</first><last>Bober</last></author>
      <pages>15–23</pages>
      <abstract>The challenge of automatically describing images and videos has stimulated much research in Computer Vision and Natural Language Processing. In order to test the semantic abilities of new algorithms, we need reliable and objective ways of measuring progress. We show that standard evaluation measures do not take into account the semantic richness of a description, and give the impression that sparse machine descriptions outperform rich human descriptions. We introduce and test a new measure of semantic ability based on relative lexical diversity. We show how our measure can work alongside existing measures to achieve state of the art correlation with human judgement of quality. We also introduce a new dataset: Rich-Sparse Descriptions, which provides 2K human and machine descriptions to stimulate interest into the semantic evaluation of machine descriptions.</abstract>
      <url hash="5a6f8922">W19-0603</url>
      <doi>10.18653/v1/W19-0603</doi>
      <bibkey>kershaw-bober-2019-lexical</bibkey>
    </paper>
    <paper id="4">
      <title>Modeling language constructs with fuzzy sets: some approaches, examples and interpretations</title>
      <author><first>Pavlo</first><last>Kapustin</last></author>
      <author><first>Michael</first><last>Kapustin</last></author>
      <pages>24–33</pages>
      <abstract>We present and discuss a couple of approaches, including different types of projections, and some examples, discussing the use of fuzzy sets for modeling meaning of certain types of language constructs. We are mostly focusing on words other than adjectives and linguistic hedges as these categories are the most studied from before. We discuss logical and linguistic interpretations of membership functions. We argue that using fuzzy sets for modeling meaning of words and other natural language constructs, along with situations described with natural language is interesting both from purely linguistic perspective, and also as a knowledge representation for problems of computational linguistics and natural language processing.</abstract>
      <url hash="f645574e">W19-0604</url>
      <doi>10.18653/v1/W19-0604</doi>
      <bibkey>kapustin-kapustin-2019-modeling</bibkey>
    </paper>
    <paper id="5">
      <title>Topological Data Analysis for Discourse Semantics?</title>
      <author><first>Ketki</first><last>Savle</last></author>
      <author><first>Wlodek</first><last>Zadrozny</last></author>
      <author><first>Minwoo</first><last>Lee</last></author>
      <pages>34–43</pages>
      <abstract>In this paper we present new results on applying topological data analysis to discourse structures. We show that topological information, extracted from the relationships between sentences can be used in inference, namely it can be applied to the very difficult legal entailment given in the COLIEE 2018 data set. Previous results of Doshi and Zadrozny (2018) and Gholizadeh et al. (2018) show that topological features are useful for classification. The applications of computational topology to entailment are novel in our view provide a new set of tools for discourse semantics: computational topology can perhaps provide a bridge between the brittleness of logic and the regression of neural networks. We discuss the advantages and disadvantages of using topological information, and some open problems such as explainability of the classifier decisions.</abstract>
      <url hash="b4b8d8b5">W19-0605</url>
      <doi>10.18653/v1/W19-0605</doi>
      <bibkey>savle-etal-2019-topological</bibkey>
    </paper>
    <paper id="6">
      <title>Semantic Frame Embeddings for Detecting Relations between Software Requirements</title>
      <author><first>Waad</first><last>Alhoshan</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <author><first>Liping</first><last>Zhao</last></author>
      <pages>44–51</pages>
      <abstract>The early phases of requirements engineering (RE) deal with a vast amount of software requirements (i.e., requirements that define characteristics of software systems), which are typically expressed in natural language. Analysing such unstructured requirements, usually obtained from users’ inputs, is considered a challenging task due to the inherent ambiguity and inconsistency of natural language. To support such a task, methods based on natural language processing (NLP) can be employed. One of the more recent advances in NLP is the use of word embeddings for capturing contextual information, which can then be applied in word analogy tasks. In this paper, we describe a new resource, i.e., embedding-based representations of semantic frames in FrameNet, which was developed to support the detection of relations between software requirements. Our embeddings, which encapsulate contextual information at the semantic frame level, were trained on a large corpus of requirements (i.e., a collection of more than three million mobile application reviews). The similarity between these frame embeddings is then used as a basis for detecting semantic relatedness between software requirements. Compared with existing resources underpinned by word-level embeddings alone, and frame embeddings built upon pre-trained vectors, our proposed frame embeddings obtained better performance against judgements of an RE expert. These encouraging results demonstrate the strong potential of the resource in supporting RE analysis tasks (e.g., traceability), which we plan to investigate as part of our future work.</abstract>
      <url hash="a8229c34">W19-0606</url>
      <doi>10.18653/v1/W19-0606</doi>
      <bibkey>alhoshan-etal-2019-semantic</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>R</fixed-case>-grams: Unsupervised Learning of Semantic Units in Natural Language</title>
      <author><first>Amaru Cuba</first><last>Gyllensten</last></author>
      <author><first>Ariel</first><last>Ekgren</last></author>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <pages>52–62</pages>
      <abstract>This paper investigates data-driven segmentation using Re-Pair or Byte Pair Encoding-techniques. In contrast to previous work which has primarily been focused on subword units for machine translation, we are interested in the general properties of such segments above the word level. We call these segments r-grams, and discuss their properties and the effect they have on the token frequency distribution. The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings. We also provide a number of qualitative examples of the proposed methodology, demonstrating its viability as a language-invariant segmentation procedure.</abstract>
      <url hash="f6d6e739">W19-0607</url>
      <doi>10.18653/v1/W19-0607</doi>
      <bibkey>gyllensten-etal-2019-r</bibkey>
    </paper>
  </volume>
  <volume id="8">
    <meta>
      <booktitle><fixed-case>RELATIONS</fixed-case> - Workshop on meaning relations between phrases and sentences</booktitle>
      <url hash="820b37e6">W19-08</url>
      <editor><first>Venelin</first><last>Kovatchev</last></editor>
      <editor><first>Darina</first><last>Gold</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>iwcs</venue>
    </meta>
    <frontmatter>
      <url hash="111ee401">W19-0800</url>
      <bibkey>ws-2019-relations</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Assessing the Difficulty of Classifying <fixed-case>C</fixed-case>oncept<fixed-case>N</fixed-case>et Relations in a Multi-Label Classification Setting</title>
      <author><first>Maria</first><last>Becker</last></author>
      <author><first>Michael</first><last>Staniek</last></author>
      <author><first>Vivi</first><last>Nastase</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <abstract>Commonsense knowledge relations are crucial for advanced NLU tasks. We examine the learnability of such relations as represented in ConceptNet, taking into account their specific properties, which can make relation classification difficult: a given concept pair can be linked by multiple relation types, and relations can have multi-word arguments of diverse semantic types. We explore a neural open world multi-label classification approach that focuses on the evaluation of classification accuracy for individual relations. Based on an in-depth study of the specific properties of the ConceptNet resource, we investigate the impact of different relation representations and model variations. Our analysis reveals that the complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work.</abstract>
      <url hash="cd3060a4">W19-0801</url>
      <doi>10.18653/v1/W19-0801</doi>
      <bibkey>becker-etal-2019-assessing</bibkey>
    </paper>
    <paper id="2">
      <title>Detecting Collocations Similarity via Logical-Linguistic Model</title>
      <author><first>Nina</first><last>Khairova</last></author>
      <author><first>Svitlana</first><last>Petrasova</last></author>
      <author><first>Orken</first><last>Mamyrbayev</last></author>
      <author><first>Kuralay</first><last>Mukhsina</last></author>
      <abstract>Semantic similarity between collocations, along with words similarity, is one of the main issues of NLP, which must be addressed, in particular, in order to facilitate the automatic thesaurus generation. In the paper, we consider the logical-linguistic model that allows defining the relation of semantic similarity of collocations via the logical-algebraic equations. We provide the model for English, Ukrainian and Russian text corpora. The implementation for each language is slightly different in the equations of the finite predicates algebra and used linguistic resources. As a dataset for our experiment, we use 5801 pairs of sentences of Microsoft Research Paraphrase Corpus for English and more than 1 000 texts of scientific papers for Russian and Ukrainian.</abstract>
      <url hash="c965486d">W19-0802</url>
      <doi>10.18653/v1/W19-0802</doi>
      <bibkey>khairova-etal-2019-detecting</bibkey>
    </paper>
    <paper id="3">
      <title>Detecting Paraphrases of Standard Clause Titles in Insurance Contracts</title>
      <author><first>Frieda</first><last>Josi</last></author>
      <author><first>Christian</first><last>Wartena</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <abstract>For the analysis of contract texts, validated model texts, such as model clauses, can be used to identify reused contract clauses. This paper investigates how to calculate the similarity between titles of model clauses and headings extracted from contracts, and which similarity measure is most suitable for this. For the calculation of the similarities between title pairs we tested various variants of string similarity and token based similarity. We also compare two more semantic similarity measures based on word embeddings using pretrained embeddings and word embeddings trained on contract texts. The identification of the model clause title can be used as a starting point for the mapping of clauses found in contracts to verified clauses.</abstract>
      <url hash="48d25a7b">W19-0803</url>
      <doi>10.18653/v1/W19-0803</doi>
      <bibkey>josi-etal-2019-detecting</bibkey>
    </paper>
    <paper id="4">
      <title>Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications</title>
      <author><first>Mark-Christoph</first><last>Mueller</last></author>
      <abstract>We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.</abstract>
      <url hash="0a2aa0a2">W19-0804</url>
      <doi>10.18653/v1/W19-0804</doi>
      <bibkey>mueller-2019-semantic</bibkey>
    </paper>
  </volume>
  <volume id="9">
    <meta>
      <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> Workshop Vector Semantics for Discourse and Dialogue</booktitle>
      <url hash="21dfd3bd">W19-09</url>
      <editor><first>Mehrnoosh</first><last>Sadrzadeh</last></editor>
      <editor><first>Matthew</first><last>Purver</last></editor>
      <editor><first>Arash</first><last>Eshghi</last></editor>
      <editor><first>Julian</first><last>Hough</last></editor>
      <editor><first>Ruth</first><last>Kempson</last></editor>
      <editor><first>Patrick G. T.</first><last>Healey</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>iwcs</venue>
    </meta>
    <frontmatter>
      <url hash="21dfd3bd">W19-0900</url>
      <bibkey>ws-2019-iwcs</bibkey>
    </frontmatter>
  </volume>
  <volume id="10">
    <meta>
      <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
      <url hash="ef14f324">W19-10</url>
      <editor><first>Rainer</first><last>Osswald</last></editor>
      <editor><first>Christian</first><last>Retoré</last></editor>
      <editor><first>Peter</first><last>Sutton</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>June</month>
      <year>2019</year>
      <venue>cstfrs</venue>
    </meta>
    <frontmatter>
      <url hash="f901d643">W19-1000</url>
      <bibkey>ws-2019-iwcs-2019</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Underspecification and interpretive parallelism in Dependent Type Semantics</title>
      <author><first>Yusuke</first><last>Kubota</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Robert</first><last>Levine</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <pages>1–9</pages>
      <url hash="6e1af318">W19-1001</url>
      <doi>10.18653/v1/W19-1001</doi>
      <bibkey>kubota-etal-2019-underspecification</bibkey>
    </paper>
    <paper id="2">
      <title>Translating a Fragment of Natural Deduction System for Natural Language into Modern Type Theory</title>
      <author><first>Ivo</first><last>Pezlar</last></author>
      <pages>10–18</pages>
      <url hash="03a3e1c6">W19-1002</url>
      <doi>10.18653/v1/W19-1002</doi>
      <bibkey>pezlar-2019-translating</bibkey>
    </paper>
    <paper id="3">
      <title>Modeling the Induced Action Alternation and the Caused-Motion Construction with <fixed-case>T</fixed-case>ree <fixed-case>A</fixed-case>djoining <fixed-case>G</fixed-case>rammar (<fixed-case>TAG</fixed-case>) and Semantic Frames</title>
      <author><first>Esther</first><last>Seyffarth</last></author>
      <pages>19–27</pages>
      <url hash="964f8760">W19-1003</url>
      <doi>10.18653/v1/W19-1003</doi>
      <bibkey>seyffarth-2019-modeling</bibkey>
      <pwccode url="https://github.com/ojahnn/caused-motion-xmg" additional="false">ojahnn/caused-motion-xmg</pwccode>
    </paper>
    <paper id="4">
      <title>Complex event representation in a typed feature structure implementation of Role and Reference Grammar</title>
      <author><first>Erika</first><last>Bellingham</last></author>
      <pages>28–36</pages>
      <url hash="ce3a8cda">W19-1004</url>
      <doi>10.18653/v1/W19-1004</doi>
      <bibkey>bellingham-2019-complex</bibkey>
    </paper>
    <paper id="5">
      <title>Computational Syntax-Semantics Interface with Type-Theory of Acyclic Recursion for Underspecified Semantics</title>
      <author><first>Roussanka</first><last>Loukanova</last></author>
      <pages>37–48</pages>
      <url hash="9c8e8efa">W19-1005</url>
      <doi>10.18653/v1/W19-1005</doi>
      <bibkey>loukanova-2019-computational</bibkey>
    </paper>
    <paper id="6">
      <title>Modeling language constructs with compatibility intervals</title>
      <author><first>Pavlo</first><last>Kapustin</last></author>
      <author><first>Michael</first><last>Kapustin</last></author>
      <pages>49–54</pages>
      <url hash="fd33d10d">W19-1006</url>
      <revision id="1" href="W19-1006v1" hash="0208a7ce"/>
      <revision id="2" href="W19-1006v2" hash="fd33d10d">The list of changes:1. Changed incorrect "mere: (0 -- 0.10.3]" in the example to correct "mere: (0 -- 0.1-0.3]" (a hyphen between 0.1 and 0.3 was missing).2. This example now starts from a new page for better readability (total number of pages is unchanged).3. Added numbering of sections.4. Minor language improvements.</revision>
      <doi>10.18653/v1/W19-1006</doi>
      <bibkey>kapustin-kapustin-2019-modeling-language</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>I</fixed-case>mage<fixed-case>TTR</fixed-case>: Grounding Type Theory with Records in Image Classification for Visual Question Answering</title>
      <author><first>Arild</first><last>Matsson</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <author><first>Staffan</first><last>Larsson</last></author>
      <pages>55–64</pages>
      <url hash="89186da2">W19-1007</url>
      <doi>10.18653/v1/W19-1007</doi>
      <bibkey>matsson-etal-2019-imagettr</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="8">
      <title>Enthymemetic Conditionals: Topoi as a guide for acceptability</title>
      <author><first>Eimear</first><last>Maguire</last></author>
      <pages>65–74</pages>
      <url hash="11b9014b">W19-1008</url>
      <doi>10.18653/v1/W19-1008</doi>
      <bibkey>maguire-2019-enthymemetic-conditionals</bibkey>
    </paper>
  </volume>
  <volume id="11">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on Natural Language and Computer Science</booktitle>
      <url hash="1a3c1474">W19-11</url>
      <editor><first>Robin</first><last>Cooper</last></editor>
      <editor><first>Valeria</first><last>de Paiva</last></editor>
      <editor><first>Lawrence S.</first><last>Moss</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="e7d95e5e">W19-1100</url>
      <bibkey>ws-2019-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Distribution is not enough: going Firther</title>
      <author><first>Andy</first><last>Lücking</last></author>
      <author><first>Robin</first><last>Cooper</last></author>
      <author><first>Staffan</first><last>Larsson</last></author>
      <author><first>Jonathan</first><last>Ginzburg</last></author>
      <pages>1–10</pages>
      <abstract>Much work in contemporary computational semantics follows the distributional hypothesis (DH), which is understood as an approach to semantics according to which the meaning of a word is a function of its distribution over contexts which is represented as vectors (word embeddings) within a multi-dimensional semantic space. In practice, use is identified with occurrence in text corpora, though there are some efforts to use corpora containing multi-modal information. In this paper we argue that the distributional hypothesis is intrinsically misguided as a self-supporting basis for semantics, as Firth was entirely aware. We mention philosophical arguments concerning the lack of normativity within DH data. Furthermore, we point out the shortcomings of DH as a model of learning, by discussing a variety of linguistic classes that cannot be learnt on a distributional basis, including indexicals, proper names, and wh-phrases. Instead of pursuing DH, we sketch an account of the problematic learning cases by integrating a rich, Firthian notion of dialogue context with interactive learning in signalling games backed by in probabilistic Type Theory with Records. We conclude that the success of the DH in computational semantics rests on a post hoc effect: DS presupposes a referential semantics on the basis of which utterances can be produced, comprehended and analysed in the first place.</abstract>
      <url hash="feda9ee8">W19-1101</url>
      <doi>10.18653/v1/W19-1101</doi>
      <bibkey>lucking-etal-2019-distribution</bibkey>
    </paper>
    <paper id="2">
      <title>Towards Natural Language Story Understanding with Rich Logical Schemas</title>
      <author><first>Lane</first><last>Lawley</last></author>
      <author><first>Gene Louis</first><last>Kim</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>11–22</pages>
      <abstract>Generating “commonsense’’ knowledge for intelligent understanding and reasoning is a difficult, long-standing problem, whose scale challenges the capacity of any approach driven primarily by human input. Furthermore, approaches based on mining statistically repetitive patterns fail to produce the rich representations humans acquire, and fall far short of human efficiency in inducing knowledge from text. The idea of our approach to this problem is to provide a learning system with a “head start” consisting of a semantic parser, some basic ontological knowledge, and most importantly, a small set of very general schemas about the kinds of patterns of events (often purposive, causal, or socially conventional) that even a one- or two-year-old could reasonably be presumed to possess. We match these initial schemas to simple children’s stories, obtaining concrete instances, and combining and abstracting these into new candidate schemas. Both the initial and generated schemas are specified using a rich, expressive logical form. While modern approaches to schema reasoning often only use slot-and-filler structures, this logical form allows us to specify complex relations and constraints over the slots. Though formal, the representations are language-like, and as such readily relatable to NL text. The agents, objects, and other roles in the schemas are represented by typed variables, and the event variables can be related through partial temporal ordering and causal relations. To match natural language stories with existing schemas, we first parse the stories into an underspecified variant of the logical form used by the schemas, which is suitable for most concrete stories. We include a walkthrough of matching a children’s story to these schemas and generating inferences from these matches.</abstract>
      <url hash="ffc9b833">W19-1102</url>
      <doi>10.18653/v1/W19-1102</doi>
      <bibkey>lawley-etal-2019-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Questions in Dependent Type Semantics</title>
      <author><first>Kazuki</first><last>Watanabe</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <pages>23–33</pages>
      <abstract>Dependent Type Semantics (DTS; Bekki and Mineshima, 2017) is a proof-theoretic compositional dynamic semantics based on Dependent Type Theory. The semantic representations for declarative sentences in DTS are types, based on the propositions-as-types paradigm. While type-theoretic semantics for natural language based on dependent type theory has been developed by many authors, how to assign semantic representations to interrogative sentences has been a non-trivial problem. In this study, we show how to provide the semantics of interrogative sentences in DTS. The basic idea is to assign the same type to both declarative sentences and interrogative sentences, partly building on the recent proposal in Inquisitive Semantics. We use Combinatory Categorial Grammar (CCG) as a syntactic component of DTS and implement our compositional semantics for interrogative sentences using ccg2lambda, a semantic parsing platform based on CCG. Based on the idea that the relationship between questions and answers can be formulated as the task of Recognizing Textual Entailment (RTE), we implement our inference system using proof assistant Coq and show that our system can deal with a wide range of question-answer relationships discussed in the formal semantics literature, including those with polar questions, alternative questions, and wh-questions.</abstract>
      <url hash="09e599b7">W19-1103</url>
      <doi>10.18653/v1/W19-1103</doi>
      <bibkey>watanabe-etal-2019-questions</bibkey>
    </paper>
    <paper id="4">
      <title>Monads for hyperintensionality? A situation semantics for hyperintensional side effects</title>
      <author><first>Luke</first><last>Burke</last></author>
      <pages>34–43</pages>
      <abstract>We outline a hyperintensional situation semantics in which hyperintensionality is modelled as a ‘side effect’, as this term has been understood in natural language semantics and in functional programming. We use monads from category theory in order to ‘upgrade’ an ordinary intensional semantics to a possible hyperintensional counterpart.</abstract>
      <url hash="9d2e03dc">W19-1104</url>
      <doi>10.18653/v1/W19-1104</doi>
      <bibkey>burke-2019-monads</bibkey>
    </paper>
  </volume>
  <volume id="12">
    <meta>
      <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> Shared Task on Semantic Parsing</booktitle>
      <url hash="ef01560c">W19-12</url>
      <editor><first>Lasha</first><last>Abzianidze</last></editor>
      <editor><first>Rik</first><last>van Noord</last></editor>
      <editor><first>Hessel</first><last>Haagsma</last></editor>
      <editor><first>Johan</first><last>Bos</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gothenburg, Sweden</address>
      <month>May</month>
      <year>2019</year>
      <venue>iwcs</venue>
    </meta>
    <frontmatter>
      <url hash="9b378c04">W19-1200</url>
      <bibkey>ws-2019-iwcs-shared</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The First Shared Task on Discourse Representation Structure Parsing</title>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Rik</first><last>van Noord</last></author>
      <author><first>Hessel</first><last>Haagsma</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <abstract>The paper presents the IWCS 2019 shared task on semantic parsing where the goal is to produce Discourse Representation Structures (DRSs) for English sentences. DRSs originate from Discourse Representation Theory and represent scoped meaning representations that capture the semantics of negation, modals, quantification, and presupposition triggers. Additionally, concepts and event-participants in DRSs are described with WordNet synsets and the thematic roles from VerbNet. To measure similarity between two DRSs, they are represented in a clausal form, i.e. as a set of tuples. Participant systems were expected to produce DRSs in this clausal form. Taking into account the rich lexical information, explicit scope marking, a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser.</abstract>
      <url hash="b8525c4b">W19-1201</url>
      <doi>10.18653/v1/W19-1201</doi>
      <bibkey>abzianidze-etal-2019-first</bibkey>
    </paper>
    <paper id="2">
      <title>Transition-based <fixed-case>DRS</fixed-case> Parsing Using Stack-<fixed-case>LSTM</fixed-case>s</title>
      <author><first>Kilian</first><last>Evang</last></author>
      <abstract>We present our submission to the IWCS 2019 shared task on semantic parsing, a transition-based parser that uses explicit word-meaning pairings, but no explicit representation of syntax. Parsing decisions are made based on vector representations of parser states, encoded via stack-LSTMs (Ballesteros et al., 2017), as well as some heuristic rules. Our system reaches 70.88% f-score in the competition.</abstract>
      <url hash="f8f711c1">W19-1202</url>
      <doi>10.18653/v1/W19-1202</doi>
      <bibkey>evang-2019-transition</bibkey>
    </paper>
    <paper id="3">
      <title>Discourse Representation Structure Parsing with Recurrent Neural Networks and the Transformer Model</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <abstract>We describe the systems we developed for Discourse Representation Structure (DRS) parsing as part of the IWCS-2019 Shared Task of DRS Parsing.1 Our systems are based on sequence-to-sequence modeling. To implement our model, we use the open-source neural machine translation system implemented in PyTorch, OpenNMT-py. We experimented with a variety of encoder-decoder models based on recurrent neural networks and the Transformer model. We conduct experiments on the standard benchmark of the Parallel Meaning Bank (PMB 2.2). Our best system achieves a score of 84.8% F1 in the DRS parsing shared task.</abstract>
      <url hash="09a36943">W19-1203</url>
      <doi>10.18653/v1/W19-1203</doi>
      <bibkey>liu-etal-2019-discourse-representation</bibkey>
    </paper>
    <paper id="4">
      <title>Neural Boxer at the <fixed-case>IWCS</fixed-case> Shared Task on <fixed-case>DRS</fixed-case> Parsing</title>
      <author><first>Rik</first><last>van Noord</last></author>
      <abstract>This paper describes our participation in the shared task of Discourse Representation Structure parsing. It follows the work of Van Noord et al. (2018), who employed a neural sequence-to-sequence model to produce DRSs, also exploiting linguistic information with multiple encoders. We provide a detailed look in the performance of this model and show that (i) the benefit of the linguistic features is evident across a number of experiments which vary the amount of training data and (ii) the model can be improved by applying a number of postprocessing methods to fix ill-formed output. Our model ended up in second place in the competition, with an F-score of 84.5.</abstract>
      <url hash="4d28977e">W19-1204</url>
      <doi>10.18653/v1/W19-1204</doi>
      <bibkey>van-noord-2019-neural</bibkey>
    </paper>
  </volume>
  <volume id="13">
    <meta>
      <booktitle>Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
      <url hash="5d635f79">W19-13</url>
      <editor><first>Alexandra</first><last>Balahur</last></editor>
      <editor><first>Roman</first><last>Klinger</last></editor>
      <editor><first>Veronique</first><last>Hoste</last></editor>
      <editor><first>Carlo</first><last>Strapparava</last></editor>
      <editor><first>Orphee</first><last>De Clercq</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, USA</address>
      <month>June</month>
      <year>2019</year>
      <venue>wassa</venue>
    </meta>
    <frontmatter>
      <url hash="d90c2b10">W19-1300</url>
      <bibkey>ws-2019-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Stance Detection in Code-Mixed <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Social Media Data using Multi-Task Learning</title>
      <author><first>Sushmitha Reddy</first><last>Sane</last></author>
      <author><first>Suraj</first><last>Tripathi</last></author>
      <author><first>Koushik Reddy</first><last>Sane</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>1–5</pages>
      <abstract>Social media sites like Facebook, Twitter, and other microblogging forums have emerged as a platform for people to express their opinions and views on different issues and events. It is often observed that people tend to take a stance; in favor, against or neutral towards a particular topic. The task of assessing the stance taken by the individual became significantly important with the emergence in the usage of online social platforms. Automatic stance detection system understands the user’s stance by analyzing the standalone texts against a target entity. Due to the limited contextual information a single sentence provides, it is challenging to solve this task effectively. In this paper, we introduce a Multi-Task Learning (MTL) based deep neural network architecture for automatically detecting stance present in the code-mixed corpus. We apply our approach on Hindi-English code-mixed corpus against the target entity - “Demonetisation.” Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.</abstract>
      <url hash="d0335705">W19-1301</url>
      <doi>10.18653/v1/W19-1301</doi>
      <bibkey>sane-etal-2019-stance</bibkey>
    </paper>
    <paper id="2">
      <title>A Soft Label Strategy for Target-Level Sentiment Classification</title>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Xiuyu</first><last>Wu</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <pages>6–15</pages>
      <abstract>In this paper, we propose a soft label approach to target-level sentiment classification task, in which a history-based soft labeling model is proposed to measure the possibility of a context word as an opinion word. We also apply a convolution layer to extract local active features, and introduce positional weights to take relative distance information into consideration. In addition, we obtain more informative target representation by training with context tokens together to make deeper interaction between target and context tokens. We conduct experiments on SemEval 2014 datasets and the experimental results show that our approach significantly outperforms previous models and gives state-of-the-art results on these datasets.</abstract>
      <url hash="6a5d7300">W19-1302</url>
      <doi>10.18653/v1/W19-1302</doi>
      <bibkey>yin-etal-2019-soft</bibkey>
    </paper>
    <paper id="3">
      <title>Online abuse detection: the value of preprocessing and neural attention models</title>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Robin</first><last>Cohen</last></author>
      <author><first>Lukasz</first><last>Golab</last></author>
      <pages>16–24</pages>
      <abstract>We propose an attention-based neural network approach to detect abusive speech in online social networks. Our approach enables more effective modeling of context and the semantic relationships between words. We also empirically evaluate the value of text pre-processing techniques in addressing the challenge of out-of-vocabulary words in toxic content. Finally, we conduct extensive experiments on the Wikipedia Talk page datasets, showing improved predictive power over the previous state-of-the-art.</abstract>
      <url hash="f976ff62">W19-1303</url>
      <doi>10.18653/v1/W19-1303</doi>
      <bibkey>kumar-etal-2019-online</bibkey>
      <pwccode url="https://github.com/ddhruvkr/Online_Abuse_Detection" additional="false">ddhruvkr/Online_Abuse_Detection</pwccode>
    </paper>
    <paper id="4">
      <title>Exploring Fine-Tuned Embeddings that Model Intensifiers for Emotion Analysis</title>
      <author><first>Laura Ana Maria</first><last>Bostan</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>25–34</pages>
      <abstract>Adjective phrases like “a little bit surprised”, “completely shocked”, or “not stunned at all” are not handled properly by current state-of-the-art emotion classification and intensity prediction systems. Based on this finding, we analyze differences between embeddings used by these systems in regard to their capability of handling such cases and argue that intensifiers in context of emotion words need special treatment, as is established for sentiment polarity classification, but not for more fine-grained emotion prediction. To resolve this issue, we analyze different aspects of a post-processing pipeline which enriches the word representations of such phrases. This includes expansion of semantic spaces at the phrase level and sub-word level followed by retrofitting to emotion lexicons. We evaluate the impact of these steps with ‘A La Carte and Bag-of-Substrings extensions based on pretrained GloVe,Word2vec, and fastText embeddings against a crowd-sourced corpus of intensity annotations for tweets containing our focus phrases. We show that the fastText-based models do not gain from handling these specific phrases under inspection. For Word2vec embeddings, we show that our post-processing pipeline improves the results by up to 8% on a novel dataset densly populated with intensifiers while it does not decrease the performance on the established EmoInt dataset.</abstract>
      <url hash="7966d916">W19-1304</url>
      <doi>10.18653/v1/W19-1304</doi>
      <bibkey>bostan-klinger-2019-exploring</bibkey>
    </paper>
    <paper id="5">
      <title>Enhancing the Measurement of Social Effects by Capturing Morality</title>
      <author><first>Rezvaneh</first><last>Rezapour</last></author>
      <author><first>Saumil H.</first><last>Shah</last></author>
      <author><first>Jana</first><last>Diesner</last></author>
      <pages>35–45</pages>
      <abstract>We investigate the relationship between basic principles of human morality and the expression of opinions in user-generated text data. We assume that people’s backgrounds, culture, and values are associated with their perceptions and expressions of everyday topics, and that people’s language use reflects these perceptions. While personal values and social effects are abstract and complex concepts, they have practical implications and are relevant for a wide range of NLP applications. To extract human values (in this paper, morality) and measure social effects (morality and stance), we empirically evaluate the usage of a morality lexicon that we expanded via a quality controlled, human in the loop process. As a result, we enhanced the Moral Foundations Dictionary in size (from 324 to 4,636 syntactically disambiguated entries) and scope. We used both lexica for feature-based and deep learning classification (SVM, RF, and LSTM) to test their usefulness for measuring social effects. We find that the enhancement of the original lexicon led to measurable improvements in prediction accuracy for the selected NLP tasks.</abstract>
      <url hash="cf0c0358">W19-1305</url>
      <doi>10.18653/v1/W19-1305</doi>
      <bibkey>rezapour-etal-2019-enhancing</bibkey>
    </paper>
    <paper id="6">
      <title>Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations</title>
      <author><first>Amita</first><last>Misra</last></author>
      <author><first>Mansurul</first><last>Bhuiyan</last></author>
      <author><first>Jalal</first><last>Mahmud</last></author>
      <author><first>Saurabh</first><last>Tripathy</last></author>
      <pages>46–56</pages>
      <abstract>Twitter customer service interactions have recently emerged as an effective platform to respond and engage with customers. In this work, we explore the role of ”negation” in customer service interactions, particularly applied to sentiment analysis. We define rules to identify true negation cues and scope more suited to conversational data than existing general review data. Using semantic knowledge and syntactic structure from constituency parse trees, we propose an algorithm for scope detection that performs comparable to state of the art BiLSTM. We further investigate the results of negation scope detection for the sentiment prediction task on customer service conversation data using both a traditional SVM and a Neural Network. We propose an antonym dictionary based method for negation applied to a combination CNN-LSTM for sentiment analysis. Experimental results show that the antonym-based method outperforms the previous lexicon-based and Neural Network methods.</abstract>
      <url hash="e07b83d6">W19-1306</url>
      <doi>10.18653/v1/W19-1306</doi>
      <bibkey>misra-etal-2019-using</bibkey>
    </paper>
    <paper id="7">
      <title>Deep Learning Techniques for Humor Detection in <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Code-Mixed Tweets</title>
      <author><first>Sushmitha Reddy</first><last>Sane</last></author>
      <author><first>Suraj</first><last>Tripathi</last></author>
      <author><first>Koushik Reddy</first><last>Sane</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>57–61</pages>
      <abstract>We propose bilingual word embeddings based on word2vec and fastText models (CBOW and Skip-gram) to address the problem of Humor detection in Hindi-English code-mixed tweets in combination with deep learning architectures. We focus on deep learning approaches which are not widely used on code-mixed data and analyzed their performance by experimenting with three different neural network models. We propose convolution neural network (CNN) and bidirectional long-short term memory (biLSTM) (with and without Attention) models which take the generated bilingual embeddings as input. We make use of Twitter data to create bilingual word embeddings. All our proposed architectures outperform the state-of-the-art results, and Attention-based bidirectional LSTM model achieved an accuracy of 73.6% which is an increment of more than 4% compared to the current state-of-the-art results.</abstract>
      <url hash="cadbf589">W19-1307</url>
      <doi>10.18653/v1/W19-1307</doi>
      <bibkey>sane-etal-2019-deep</bibkey>
    </paper>
    <paper id="8">
      <title>How do we feel when a robot dies? Emotions expressed on <fixed-case>T</fixed-case>witter before and after hitch<fixed-case>BOT</fixed-case>’s destruction</title>
      <author><first>Kathleen C.</first><last>Fraser</last></author>
      <author><first>Frauke</first><last>Zeller</last></author>
      <author><first>David Harris</first><last>Smith</last></author>
      <author><first>Saif</first><last>Mohammad</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>62–71</pages>
      <abstract>In 2014, a chatty but immobile robot called hitchBOT set out to hitchhike across Canada. It similarly made its way across Germany and the Netherlands, and had begun a trip across the USA when it was destroyed by vandals. In this work, we analyze the emotions and sentiments associated with words in tweets posted before and after hitchBOT’s destruction to answer two questions: Were there any differences in the emotions expressed across the different countries visited by hitchBOT? And how did the public react to the demise of hitchBOT? Our analyses indicate that while there were few cross-cultural differences in sentiment towards hitchBOT, there was a significant negative emotional reaction to its destruction, suggesting that people had formed an emotional connection with hitchBOT and perceived its destruction as morally wrong. We discuss potential implications of anthropomorphism and emotional attachment to robots from the perspective of robot ethics.</abstract>
      <url hash="b69707ab">W19-1308</url>
      <doi>10.18653/v1/W19-1308</doi>
      <bibkey>fraser-etal-2019-feel</bibkey>
    </paper>
    <paper id="9">
      <title>“When Numbers Matter!”: Detecting Sarcasm in Numerical Portions of Text</title>
      <author><first>Abhijeet</first><last>Dubey</last></author>
      <author><first>Lakshya</first><last>Kumar</last></author>
      <author><first>Arpan</first><last>Somani</last></author>
      <author><first>Aditya</first><last>Joshi</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>72–80</pages>
      <abstract>Research in sarcasm detection spans almost a decade. However a particular form of sarcasm remains unexplored: sarcasm expressed through numbers, which we estimate, forms about 11% of the sarcastic tweets in our dataset. The sentence ‘Love waking up at 3 am’ is sarcastic because of the number. In this paper, we focus on detecting sarcasm in tweets arising out of numbers. Initially, to get an insight into the problem, we implement a rule-based and a statistical machine learning-based (ML) classifier. The rule-based classifier conveys the crux of the numerical sarcasm problem, namely, incongruity arising out of numbers. The statistical ML classifier uncovers the indicators i.e., features of such sarcasm. The actual system in place, however, are two deep learning (DL) models, CNN and attention network that obtains an F-score of 0.93 and 0.91 on our dataset of tweets containing numbers. To the best of our knowledge, this is the first line of research investigating the phenomenon of sarcasm arising out of numbers, culminating in a detector thereof.</abstract>
      <url hash="314af0c7">W19-1309</url>
      <doi>10.18653/v1/W19-1309</doi>
      <bibkey>dubey-etal-2019-numbers</bibkey>
    </paper>
    <paper id="10">
      <title>Cross-lingual Subjectivity Detection for Resource Lean Languages</title>
      <author><first>Ida</first><last>Amini</last></author>
      <author><first>Samane</first><last>Karimi</last></author>
      <author><first>Azadeh</first><last>Shakery</last></author>
      <pages>81–90</pages>
      <abstract>Wide and universal changes in the web content due to the growth of web 2 applications increase the importance of user-generated content on the web. Therefore, the related research areas such as sentiment analysis, opinion mining and subjectivity detection receives much attention from the research community. Due to the diverse languages that web-users use to express their opinions and sentiments, research areas like subjectivity detection should present methods which are practicable on all languages. An important prerequisite to effectively achieve this aim is considering the limitations in resource-lean languages. In this paper, cross-lingual subjectivity detection on resource lean languages is investigated using two different approaches: a language-model based and a learning-to-rank approach. Experimental results show the impact of different factors on the performance of subjectivity detection methods using English resources to detect the subjectivity score of Persian documents. The experiments demonstrate that the proposed learning-to-rank method outperforms the baseline method in ranking documents based on their subjectivity degree.</abstract>
      <url hash="ad672449">W19-1310</url>
      <doi>10.18653/v1/W19-1310</doi>
      <bibkey>amini-etal-2019-cross</bibkey>
    </paper>
    <paper id="11">
      <title>Analyzing Incorporation of Emotion in Emoji Prediction</title>
      <author><first>Shirley Anugrah</first><last>Hayati</last></author>
      <author><first>Aldrian Obaja</first><last>Muis</last></author>
      <pages>91–99</pages>
      <abstract>In this work, we investigate the impact of incorporating emotion classes on the task of predicting emojis from Twitter texts. More specifically, we first show that there is a correlation between the emotion expressed in the text and the emoji choice of Twitter users. Based on this insight we propose a few simple methods to incorporate emotion information in traditional classifiers. Through automatic metrics, human evaluation, and error analysis, we show that the improvement obtained by incorporating emotion is significant and correlate better with human preferences compared to the baseline models. Through the human ratings that we obtained, we also argue for preference metric to better evaluate the usefulness of an emoji prediction system.</abstract>
      <url hash="e1b2a831">W19-1311</url>
      <doi>10.18653/v1/W19-1311</doi>
      <bibkey>hayati-muis-2019-analyzing</bibkey>
    </paper>
  </volume>
  <volume id="14">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on <fixed-case>NLP</fixed-case> for Similar Languages, Varieties and Dialects</booktitle>
      <url hash="eeff0e97">W19-14</url>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Nikola</first><last>Ljubešić</last></editor>
      <editor><first>Jörg</first><last>Tiedemann</last></editor>
      <editor><first>Ahmed</first><last>Ali</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Ann Arbor, Michigan</address>
      <month>June</month>
      <year>2019</year>
      <venue>vardial</venue>
    </meta>
    <frontmatter>
      <url hash="3ec27765">W19-1400</url>
      <bibkey>ws-2019-nlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Report on the Third <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial Evaluation Campaign</title>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Tanja</first><last>Samardžić</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Natalia</first><last>Klyueva</last></author>
      <author><first>Tung-Le</first><last>Pan</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <author><first>Andrei M.</first><last>Butnaru</last></author>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <pages>1–16</pages>
      <abstract>In this paper, we present the findings of the Third VarDial Evaluation Campaign organized as part of the sixth edition of the workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with NAACL 2019. This year, the campaign included five shared tasks, including one task re-run – German Dialect Identification (GDI) – and four new tasks – Cross-lingual Morphological Analysis (CMA), Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT), Moldavian vs. Romanian Cross-dialect Topic identification (MRC), and Cuneiform Language Identification (CLI). A total of 22 teams submitted runs across the five shared tasks. After the end of the competition, we received 14 system description papers, which are published in the VarDial workshop proceedings and referred to in this report.</abstract>
      <url hash="051a1243">W19-1401</url>
      <doi>10.18653/v1/W19-1401</doi>
      <bibkey>zampieri-etal-2019-report</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="2">
      <title>Improving Cuneiform Language Identification with <fixed-case>BERT</fixed-case></title>
      <author><first>Gabriel</first><last>Bernier-Colborne</last></author>
      <author><first>Cyril</first><last>Goutte</last></author>
      <author><first>Serge</first><last>Léger</last></author>
      <pages>17–25</pages>
      <abstract>We describe the systems developed by the National Research Council Canada for the Cuneiform Language Identification (CLI) shared task at the 2019 VarDial evaluation campaign. We compare a state-of-the-art baseline relying on character n-grams and a traditional statistical classifier, a voting ensemble of classifiers, and a deep learning approach using a Transformer network. We describe how these systems were trained, and analyze the impact of some preprocessing and model estimation decisions. The deep neural network achieved 77% accuracy on the test data, which turned out to be the best performance at the CLI evaluation, establishing a new state-of-the-art for cuneiform language identification.</abstract>
      <url hash="7a95c187">W19-1402</url>
      <doi>10.18653/v1/W19-1402</doi>
      <bibkey>bernier-colborne-etal-2019-improving</bibkey>
    </paper>
    <paper id="3">
      <title>Joint Approach to Deromanization of Code-mixed Texts</title>
      <author><first>Rashed Rubby</first><last>Riyadh</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>26–34</pages>
      <abstract>The conversion of romanized texts back to the native scripts is a challenging task because of the inconsistent romanization conventions and non-standard language use. This problem is compounded by code-mixing, i.e., using words from more than one language within the same discourse. In this paper, we propose a novel approach for handling these two problems together in a single system. Our approach combines three components: language identification, back-transliteration, and sequence prediction. The results of our experiments on Bengali and Hindi datasets establish the state of the art for the task of deromanization of code-mixed texts.</abstract>
      <url hash="06163fdb">W19-1403</url>
      <doi>10.18653/v1/W19-1403</doi>
      <bibkey>riyadh-kondrak-2019-joint</bibkey>
    </paper>
    <paper id="4">
      <title>Char-<fixed-case>RNN</fixed-case> for Word Stress Detection in <fixed-case>E</fixed-case>ast <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Ekaterina</first><last>Chernyak</last></author>
      <author><first>Maria</first><last>Ponomareva</last></author>
      <author><first>Kirill</first><last>Milintsevich</last></author>
      <pages>35–41</pages>
      <abstract>We explore how well a sequence labeling approach, namely, recurrent neural network, is suited for the task of resource-poor and POS tagging free word stress detection in the Russian, Ukranian, Belarusian languages. We present new datasets, annotated with the word stress, for the three languages and compare several RNN models trained on three languages and explore possible applications of the transfer learning for the task. We show that it is possible to train a model in a cross-lingual setting and that using additional languages improves the quality of the results.</abstract>
      <url hash="4a58276f">W19-1404</url>
      <doi>10.18653/v1/W19-1404</doi>
      <bibkey>chernyak-etal-2019-char</bibkey>
    </paper>
    <paper id="5">
      <title>Modeling Global Syntactic Variation in <fixed-case>E</fixed-case>nglish Using Dialect Classification</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <pages>42–53</pages>
      <abstract>This paper evaluates global-scale dialect identification for 14 national varieties of English on both web-crawled data and Twitter data. The paper makes three main contributions: (i) introducing data-driven language mapping as a method for selecting the inventory of national varieties to include in the task; (ii) producing a large and dynamic set of syntactic features using grammar induction rather than focusing on a few hand-selected features such as function words; and (iii) comparing models across both web corpora and social media corpora in order to measure the robustness of syntactic variation across registers.</abstract>
      <url hash="22cf227e">W19-1405</url>
      <attachment type="software" hash="5d97e455">W19-1405.Software.zip</attachment>
      <doi>10.18653/v1/W19-1405</doi>
      <bibkey>dunn-2019-modeling</bibkey>
    </paper>
    <paper id="6">
      <title>Language Discrimination and Transfer Learning for Similar Languages: Experiments with Feature Combinations and Adaptation</title>
      <author><first>Nianheng</first><last>Wu</last></author>
      <author><first>Eric</first><last>DeMattos</last></author>
      <author><first>Kwok Him</first><last>So</last></author>
      <author><first>Pin-zhen</first><last>Chen</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>54–63</pages>
      <abstract>This paper describes the work done by team tearsofjoy participating in the VarDial 2019 Evaluation Campaign. We developed two systems based on Support Vector Machines: SVM with a flat combination of features and SVM ensembles. We participated in all language/dialect identification tasks, as well as the Moldavian vs. Romanian cross-dialect topic identification (MRC) task. Our team achieved first place in German Dialect identification (GDI) and MRC subtasks 2 and 3, second place in the simplified variant of Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT) as well as Cuneiform Language Identification (CLI), and third and fifth place in DMT traditional and MRC subtask 1 respectively. In most cases, the SVM with a flat combination of features performed better than SVM ensembles. Besides describing the systems and the results obtained by them, we provide a tentative comparison between the feature combination methods, and present additional experiments with a method of adaptation to the test set, which may indicate potential pitfalls with some of the data sets.</abstract>
      <url hash="8378e1d8">W19-1406</url>
      <doi>10.18653/v1/W19-1406</doi>
      <bibkey>wu-etal-2019-language</bibkey>
    </paper>
    <paper id="7">
      <title>Variation between Different Discourse Types: Literate vs. Oral</title>
      <author><first>Katrin</first><last>Ortmann</last></author>
      <author><first>Stefanie</first><last>Dipper</last></author>
      <pages>64–79</pages>
      <abstract>This paper deals with the automatic identification of literate and oral discourse in German texts. A range of linguistic features is selected and their role in distinguishing between literate- and oral-oriented registers is investigated, using a decision-tree classifier. It turns out that all of the investigated features are related in some way to oral conceptuality. Especially simple measures of complexity (average sentence and word length) are prominent indicators of oral and literate discourse. In addition, features of reference and deixis (realized by different types of pronouns) also prove to be very useful in determining the degree of orality of different registers.</abstract>
      <url hash="46f4c86f">W19-1407</url>
      <doi>10.18653/v1/W19-1407</doi>
      <bibkey>ortmann-dipper-2019-variation</bibkey>
    </paper>
    <paper id="8">
      <title>Neural Machine Translation between <fixed-case>M</fixed-case>yanmar (<fixed-case>B</fixed-case>urmese) and <fixed-case>R</fixed-case>akhine (<fixed-case>A</fixed-case>rakanese)</title>
      <author><first>Thazin</first><last>Myint Oo</last></author>
      <author><first>Ye</first><last>Kyaw Thu</last></author>
      <author><first>Khin</first><last>Mar Soe</last></author>
      <pages>80–88</pages>
      <abstract>This work explores neural machine translation between Myanmar (Burmese) and Rakhine (Arakanese). Rakhine is a language closely related to Myanmar, often considered a dialect. We implemented three prominent neural machine translation (NMT) systems: recurrent neural networks (RNN), transformer, and convolutional neural networks (CNN). The systems were evaluated on a Myanmar-Rakhine parallel text corpus developed by us. In addition, two types of word segmentation schemes for word embeddings were studied: Word-BPE and Syllable-BPE segmentation. Our experimental results clearly show that the highest quality NMT and statistical machine translation (SMT) performances are obtained with Syllable-BPE segmentation for both types of translations. If we focus on NMT, we find that the transformer with Word-BPE segmentation outperforms CNN and RNN for both Myanmar-Rakhine and Rakhine-Myanmar translation. However, CNN with Syllable-BPE segmentation obtains a higher score than the RNN and transformer.</abstract>
      <url hash="447ccb3c">W19-1408</url>
      <doi>10.18653/v1/W19-1408</doi>
      <bibkey>myint-oo-etal-2019-neural</bibkey>
    </paper>
    <paper id="9">
      <title>Language and Dialect Identification of Cuneiform Texts</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <author><first>Tero</first><last>Alstola</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <pages>89–98</pages>
      <abstract>This article introduces a corpus of cuneiform texts from which the dataset for the use of the Cuneiform Language Identification (CLI) 2019 shared task was derived as well as some preliminary language identification experiments conducted using that corpus. We also describe the CLI dataset and how it was derived from the corpus. In addition, we provide some baseline language identification results using the CLI dataset. To the best of our knowledge, the experiments detailed here represent the first time that automatic language identification methods have been used on cuneiform data.</abstract>
      <url hash="71544f85">W19-1409</url>
      <doi>10.18653/v1/W19-1409</doi>
      <bibkey>jauhiainen-etal-2019-language</bibkey>
    </paper>
    <paper id="10">
      <title>Leveraging Pretrained Word Embeddings for Part-of-Speech Tagging of Code Switching Data</title>
      <author><first>Fahad</first><last>AlGhamdi</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>99–109</pages>
      <abstract>Linguistic Code Switching (CS) is a phenomenon that occurs when multilingual speakers alternate between two or more languages/dialects within a single conversation. Processing CS data is especially challenging in intra-sentential data given state-of-the-art monolingual NLP technologies since such technologies are geared toward the processing of one language at a time. In this paper, we address the problem of Part-of-Speech tagging (POS) in the context of linguistic code switching (CS). We explore leveraging multiple neural network architectures to measure the impact of different pre-trained embeddings methods on POS tagging CS data. We investigate the landscape in four CS language pairs, Spanish-English, Hindi-English, Modern Standard Arabic- Egyptian Arabic dialect (MSA-EGY), and Modern Standard Arabic- Levantine Arabic dialect (MSA-LEV). Our results show that multilingual embedding (e.g., MSA-EGY and MSA-LEV) helps closely related languages (EGY/LEV) but adds noise to the languages that are distant (SPA/HIN). Finally, we show that our proposed models outperform state-of-the-art CS taggers for MSA-EGY language pair.</abstract>
      <url hash="ff479d45">W19-1410</url>
      <doi>10.18653/v1/W19-1410</doi>
      <bibkey>alghamdi-diab-2019-leveraging</bibkey>
    </paper>
    <paper id="11">
      <title>Toward a deep dialectological representation of <fixed-case>I</fixed-case>ndo-<fixed-case>A</fixed-case>ryan</title>
      <author><first>Chundra</first><last>Cathcart</last></author>
      <pages>110–119</pages>
      <abstract>This paper presents a new approach to disentangling inter-dialectal and intra-dialectal relationships within one such group, the Indo-Aryan subgroup of Indo-European. We draw upon admixture models and deep generative models to tease apart historic language contact and language-specific behavior in the overall patterns of sound change displayed by Indo-Aryan languages. We show that a “deep” model of Indo-Aryan dialectology sheds some light on questions regarding inter-relationships among the Indo-Aryan languages, and performs better than a “shallow” model in terms of certain qualities of the posterior distribution (e.g., entropy of posterior distributions), and outline future pathways for model development.</abstract>
      <url hash="7e4eaf0e">W19-1411</url>
      <doi>10.18653/v1/W19-1411</doi>
      <bibkey>cathcart-2019-toward</bibkey>
    </paper>
    <paper id="12">
      <title>Naive <fixed-case>B</fixed-case>ayes and <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Ensemble for Discriminating between Mainland and <fixed-case>T</fixed-case>aiwan Variation of <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese</title>
      <author><first>Li</first><last>Yang</last></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <pages>120–127</pages>
      <abstract>Automatic dialect identification is a more challengingctask than language identification, as it requires the ability to discriminate between varieties of one language. In this paper, we propose an ensemble based system, which combines traditional machine learning models trained on bag of n-gram fetures, with deep learning models trained on word embeddings, to solve the Discriminating between Mainland and Taiwan Variation of Mandarin Chinese (DMT) shared task at VarDial 2019. Our experiments show that a character bigram-trigram combination based Naive Bayes is a very strong model for identifying varieties of Mandarin Chinense. Through further ensemble of Navie Bayes and BiLSTM, our system (team: itsalexyang) achived an macro-averaged F1 score of 0.8530 and 0.8687 in two tracks.</abstract>
      <url hash="4b88202a">W19-1412</url>
      <doi>10.18653/v1/W19-1412</doi>
      <bibkey>yang-xiang-2019-naive</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>BAM</fixed-case>: A combination of deep and shallow models for <fixed-case>G</fixed-case>erman Dialect Identification.</title>
      <author><first>Andrei M.</first><last>Butnaru</last></author>
      <pages>128–137</pages>
      <abstract>*This is a submission for the Third VarDial Evaluation Campaign* In this paper, we present a machine learning approach for the German Dialect Identification (GDI) Closed Shared Task of the DSL 2019 Challenge. The proposed approach combines deep and shallow models, by applying a voting scheme on the outputs resulted from a Character-level Convolutional Neural Networks (Char-CNN), a Long Short-Term Memory (LSTM) network, and a model based on String Kernels. The first model used is the Char-CNN model that merges multiple convolutions computed with kernels of different sizes. The second model is the LSTM network which applies a global max pooling over the returned sequences over time. Both models pass the activation maps to two fully-connected layers. The final model is based on String Kernels, computed on character p-grams extracted from speech transcripts. The model combines two blended kernel functions, one is the presence bits kernel, and the other is the intersection kernel. The empirical results obtained in the shared task prove that the approach can achieve good results. The system proposed in this paper obtained the fourth place with a macro-F1 score of 62.55%</abstract>
      <url hash="59dcb274">W19-1413</url>
      <doi>10.18653/v1/W19-1413</doi>
      <bibkey>butnaru-2019-bam</bibkey>
    </paper>
    <paper id="14">
      <title>The <fixed-case>R</fixed-case>2<fixed-case>I</fixed-case>_<fixed-case>LIS</fixed-case> Team Proposes Majority Vote for <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial’s <fixed-case>MRC</fixed-case> Task</title>
      <author><first>Adrian-Gabriel</first><last>Chifu</last></author>
      <pages>138–143</pages>
      <abstract>This article presents the model that generated the runs submitted by the R2I_LIS team to the VarDial2019 evaluation campaign, more particularly, to the binary classification by dialect sub-task of the Moldavian vs. Romanian Cross-dialect Topic identification (MRC) task. The team proposed a majority vote-based model, between five supervised machine learning models, trained on forty manually-crafted features. One of the three submitted runs was ranked second at the binary classification sub-task, with a performance of 0.7963, in terms of macro-F1 measure. The other two runs were ranked third and fourth, respectively.</abstract>
      <url hash="72843452">W19-1414</url>
      <doi>10.18653/v1/W19-1414</doi>
      <bibkey>chifu-2019-r2i</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="15">
      <title>Initial Experiments In Cross-Lingual Morphological Analysis Using Morpheme Segmentation</title>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Lorenzo</first><last>Tosi</last></author>
      <author><first>Anastasia</first><last>Khorosheva</last></author>
      <author><first>Oleg</first><last>Serikov</last></author>
      <pages>144–152</pages>
      <abstract>The paper describes initial experiments in data-driven cross-lingual morphological analysis of open-category words using a combination of unsupervised morpheme segmentation, annotation projection and an LSTM encoder-decoder model with attention. Our algorithm provides lemmatisation and morphological analysis generation for previously unseen low-resource language surface forms with only annotated data on the related languages given. Despite the inherently lossy annotation projection, we achieved the best lemmatisation F1-score in the VarDial 2019 Shared Task on Cross-Lingual Morphological Analysis for both Karachay-Balkar (Turkic languages, agglutinative morphology) and Sardinian (Romance languages, fusional morphology).</abstract>
      <url hash="d8bf32a4">W19-1415</url>
      <doi>10.18653/v1/W19-1415</doi>
      <bibkey>mikhailov-etal-2019-initial</bibkey>
    </paper>
    <paper id="16">
      <title>Neural and Linear Pipeline Approaches to Cross-lingual Morphological Analysis</title>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <pages>153–164</pages>
      <abstract>This paper describes Tübingen-Oslo team’s participation in the cross-lingual morphological analysis task in the VarDial 2019 evaluation campaign. We participated in the shared task with a standard neural network model. Our model achieved analysis F1-scores of 31.48 and 23.67 on test languages Karachay-Balkar (Turkic) and Sardinian (Romance) respectively. The scores are comparable to the scores obtained by the other participants in both language families, and the analysis score on the Romance data set was also the best result obtained in the shared task. Besides describing the system used in our shared task participation, we describe another, simpler, model based on linear classifiers, and present further analyses using both models. Our analyses, besides revealing some of the difficult cases, also confirm that the usefulness of a source language in this task is highly correlated with the similarity of source and target languages.</abstract>
      <url hash="7d7391f4">W19-1416</url>
      <doi>10.18653/v1/W19-1416</doi>
      <bibkey>coltekin-barnes-2019-neural</bibkey>
    </paper>
    <paper id="17">
      <title>Ensemble Methods to Distinguish Mainland and <fixed-case>T</fixed-case>aiwan <fixed-case>C</fixed-case>hinese</title>
      <author><first>Hai</first><last>Hu</last></author>
      <author><first>Wen</first><last>Li</last></author>
      <author><first>He</first><last>Zhou</last></author>
      <author><first>Zuoyu</first><last>Tian</last></author>
      <author><first>Yiwen</first><last>Zhang</last></author>
      <author><first>Liang</first><last>Zou</last></author>
      <pages>165–171</pages>
      <abstract>This paper describes the IUCL system at VarDial 2019 evaluation campaign for the task of discriminating between Mainland and Taiwan variation of mandarin Chinese. We first build several base classifiers, including a Naive Bayes classifier with word n-gram as features, SVMs with both character and syntactic features, and neural networks with pre-trained character/word embeddings. Then we adopt ensemble methods to combine output from base classifiers to make final predictions. Our ensemble models achieve the highest F1 score (0.893) in simplified Chinese track and the second highest (0.901) in traditional Chinese track. Our results demonstrate the effectiveness and robustness of the ensemble methods.</abstract>
      <url hash="2683e73f">W19-1417</url>
      <doi>10.18653/v1/W19-1417</doi>
      <bibkey>hu-etal-2019-ensemble</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>SC</fixed-case>-<fixed-case>UPB</fixed-case> at the <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial 2019 Evaluation Campaign: <fixed-case>M</fixed-case>oldavian vs. <fixed-case>R</fixed-case>omanian Cross-Dialect Topic Identification</title>
      <author><first>Cristian</first><last>Onose</last></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last></author>
      <author><first>Stefan</first><last>Trausan-Matu</last></author>
      <pages>172–177</pages>
      <abstract>This paper describes our models for the Moldavian vs. Romanian Cross-Topic Identification (MRC) evaluation campaign, part of the VarDial 2019 workshop. We focus on the three subtasks for MRC: binary classification between the Moldavian (MD) and the Romanian (RO) dialects and two cross-dialect multi-class classification between six news topics, MD to RO and RO to MD. We propose several deep learning models based on long short-term memory cells, Bidirectional Gated Recurrent Unit (BiGRU) and Hierarchical Attention Networks (HAN). We also employ three word embedding models to represent the text as a low dimensional vector. Our official submission includes two runs of the BiGRU and HAN models for each of the three subtasks. The best submitted model obtained the following macro-averaged F1 scores: 0.708 for subtask 1, 0.481 for subtask 2 and 0.480 for the last one. Due to a read error caused by the quoting behaviour over the test file, our final submissions contained a smaller number of items than expected. More than 50% of the submission files were corrupted. Thus, we also present the results obtained with the corrected labels for which the HAN model achieves the following results: 0.930 for subtask 1, 0.590 for subtask 2 and 0.687 for the third one.</abstract>
      <url hash="5a585629">W19-1418</url>
      <doi>10.18653/v1/W19-1418</doi>
      <bibkey>onose-etal-2019-sc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="19">
      <title>Discriminating between <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese and <fixed-case>S</fixed-case>wiss-<fixed-case>G</fixed-case>erman varieties using adaptive language models</title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <author><first>Heidi</first><last>Jauhiainen</last></author>
      <pages>178–187</pages>
      <abstract>This paper describes the language identification systems used by the SUKI team in the Discriminating between the Mainland and Taiwan variation of Mandarin Chinese (DMT) and the German Dialect Identification (GDI) shared tasks which were held as part of the third VarDial Evaluation Campaign. The DMT shared task included two separate tracks, one for the simplified Chinese script and one for the traditional Chinese script. We submitted three runs on both tracks of the DMT task as well as on the GDI task. We won the traditional Chinese track using Naive Bayes with language model adaptation, came second on GDI with an adaptive version of the HeLI 2.0 method, and third on the simplified Chinese track using again the adaptive Naive Bayes.</abstract>
      <url hash="e075d303">W19-1419</url>
      <doi>10.18653/v1/W19-1419</doi>
      <bibkey>jauhiainen-etal-2019-discriminating</bibkey>
    </paper>
    <paper id="20">
      <title>Investigating Machine Learning Methods for Language and Dialect Identification of Cuneiform Texts</title>
      <author><first>Ehsan</first><last>Doostmohammadi</last></author>
      <author><first>Minoo</first><last>Nassajian</last></author>
      <pages>188–193</pages>
      <abstract>Identification of the languages written using cuneiform symbols is a difficult task due to the lack of resources and the problem of tokenization. The Cuneiform Language Identification task in VarDial 2019 addresses the problem of identifying seven languages and dialects written in cuneiform; Sumerian and six dialects of Akkadian language: Old Babylonian, Middle Babylonian Peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian, and Neo-Assyrian. This paper describes the approaches taken by SharifCL team to this problem in VarDial 2019. The best result belongs to an ensemble of Support Vector Machines and a naive Bayes classifier, both working on character-level features, with macro-averaged F1-score of 72.10%.</abstract>
      <url hash="662f21ec">W19-1420</url>
      <doi>10.18653/v1/W19-1420</doi>
      <bibkey>doostmohammadi-nassajian-2019-investigating</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>T</fixed-case>wist<fixed-case>B</fixed-case>ytes - Identification of Cuneiform Languages and <fixed-case>G</fixed-case>erman Dialects at <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial 2019</title>
      <author><first>Fernando</first><last>Benites</last></author>
      <author><first>Pius</first><last>von Däniken</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <pages>194–201</pages>
      <abstract>We describe our approaches for the German Dialect Identification (GDI) and the Cuneiform Language Identification (CLI) tasks at the VarDial Evaluation Campaign 2019. The goal was to identify dialects of Swiss German in GDI and Sumerian and Akkadian in CLI. In GDI, the system should distinguish four dialects from the German-speaking part of Switzerland. Our system for GDI achieved third place out of 6 teams, with a macro averaged F-1 of 74.6%. In CLI, the system should distinguish seven languages written in cuneiform script. Our system achieved third place out of 8 teams, with a macro averaged F-1 of 74.7%.</abstract>
      <url hash="a7bfbe86">W19-1421</url>
      <doi>10.18653/v1/W19-1421</doi>
      <bibkey>benites-etal-2019-twistbytes</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>DT</fixed-case>eam @ <fixed-case>V</fixed-case>ar<fixed-case>D</fixed-case>ial 2019: Ensemble based on skip-gram and triplet loss neural networks for <fixed-case>M</fixed-case>oldavian vs. <fixed-case>R</fixed-case>omanian cross-dialect topic identification</title>
      <author><first>Diana</first><last>Tudoreanu</last></author>
      <pages>202–208</pages>
      <abstract>This paper presents the solution proposed by DTeam in the VarDial 2019 Evaluation Campaign for the Moldavian vs. Romanian cross-topic identification task. The solution proposed is a Support Vector Machines (SVM) ensemble composed of a two character-level neural networks. The first network is a skip-gram classification model formed of an embedding layer, three convolutional layers and two fully-connected layers. The second network has a similar architecture, but is trained using the triplet loss function.</abstract>
      <url hash="12eb925b">W19-1422</url>
      <doi>10.18653/v1/W19-1422</doi>
      <bibkey>tudoreanu-2019-dteam</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/moroco">MOROCO</pwcdataset>
    </paper>
    <paper id="23">
      <title>Experiments in Cuneiform Language Identification</title>
      <author><first>Gustavo Henrique</first><last>Paetzold</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>209–213</pages>
      <abstract>This paper presents methods to discriminate between languages and dialects written in Cuneiform script, one of the first writing systems in the world. We report the results obtained by the PZ team in the Cuneiform Language Identification (CLI) shared task organized within the scope of the VarDial Evaluation Campaign 2019. The task included two languages, Sumerian and Akkadian. The latter is divided into six dialects: Old Babylonian, Middle Babylonian peripheral, Standard Babylonian, Neo Babylonian, Late Babylonian, and Neo Assyrian. We approach the task using a meta-classifier trained on various SVM models and we show the effectiveness of the system for this task. Our submission achieved 0.738 F1 score in discriminating between the seven languages and dialects and it was ranked fourth in the competition among eight teams.</abstract>
      <url hash="cec61263">W19-1423</url>
      <doi>10.18653/v1/W19-1423</doi>
      <bibkey>paetzold-zampieri-2019-experiments</bibkey>
    </paper>
    <paper id="24">
      <title>Comparing Pipelined and Integrated Approaches to Dialectal <fixed-case>A</fixed-case>rabic Neural Machine Translation</title>
      <author><first>Pamela</first><last>Shapiro</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>214–222</pages>
      <abstract>When translating diglossic languages such as Arabic, situations may arise where we would like to translate a text but do not know which dialect it is. A traditional approach to this problem is to design dialect identification systems and dialect-specific machine translation systems. However, under the recent paradigm of neural machine translation, shared multi-dialectal systems have become a natural alternative. Here we explore under which conditions it is beneficial to perform dialect identification for Arabic neural machine translation versus using a general system for all dialects.</abstract>
      <url hash="d736e74f">W19-1424</url>
      <doi>10.18653/v1/W19-1424</doi>
      <bibkey>shapiro-duh-2019-comparing</bibkey>
    </paper>
    <paper id="25">
      <title>Cross-lingual Annotation Projection Is Effective for Neural Part-of-Speech Tagging</title>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Diana</first><last>Dutka</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>223–233</pages>
      <abstract>We tackle the important task of part-of-speech tagging using a neural model in the zero-resource scenario, where we have no access to gold-standard POS training data. We compare this scenario with the low-resource scenario, where we have access to a small amount of gold-standard POS training data. Our experiments focus on Ukrainian as a representative of under-resourced languages. Russian is highly related to Ukrainian, so we exploit gold-standard Russian POS tags. We consider four techniques to perform Ukrainian POS tagging: zero-shot tagging and cross-lingual annotation projection (for the zero-resource scenario), and compare these with self-training and multilingual learning (for the low-resource scenario). We find that cross-lingual annotation projection works particularly well in the zero-resource scenario.</abstract>
      <url hash="946ae874">W19-1425</url>
      <doi>10.18653/v1/W19-1425</doi>
      <bibkey>huck-etal-2019-cross</bibkey>
    </paper>
  </volume>
  <volume id="15">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Structured Prediction for <fixed-case>NLP</fixed-case></booktitle>
      <url hash="6d754522">W19-15</url>
      <editor><first>Andre</first><last>Martins</last></editor>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Sujith</first><last>Ravi</last></editor>
      <editor><first>Gerasimos</first><last>Lampouras</last></editor>
      <editor><first>Vlad</first><last>Niculae</last></editor>
      <editor><first>Julia</first><last>Kreutzer</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="32e7bdec">W19-1500</url>
      <bibkey>ws-2019-structured</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Parallelizable Stack Long Short-Term Memory</title>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>1–6</pages>
      <abstract>Stack Long Short-Term Memory (StackLSTM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation.</abstract>
      <url hash="ad8d62de">W19-1501</url>
      <attachment type="presentation" hash="a1f48fd7">W19-1501.Presentation.pdf</attachment>
      <doi>10.18653/v1/W19-1501</doi>
      <bibkey>ding-koehn-2019-parallelizable</bibkey>
      <pwccode url="https://github.com/shuoyangd/hoolock" additional="false">shuoyangd/hoolock</pwccode>
    </paper>
    <paper id="2">
      <title>Tracking Discrete and Continuous Entity State for Process Understanding</title>
      <author><first>Aditya</first><last>Gupta</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>7–12</pages>
      <abstract>Procedural text, which describes entities and their interactions as they undergo some process, depicts entities in a uniquely nuanced way. First, each entity may have some observable discrete attributes, such as its state or location; modeling these involves imposing global structure and enforcing consistency. Second, an entity may have properties which are not made explicit but can be effectively induced and tracked by neural networks. In this paper, we propose a structured neural architecture that reflects this dual nature of entity evolution. The model tracks each entity recurrently, updating its hidden continuous representation at each step to contain relevant state information. The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the ProPara dataset and find that our model achieves state-of-the-art results.</abstract>
      <url hash="ce4162f5">W19-1502</url>
      <doi>10.18653/v1/W19-1502</doi>
      <bibkey>gupta-durrett-2019-tracking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/propara">ProPara</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>SPARSE</fixed-case>: Structured Prediction using Argument-Relative Structured Encoding</title>
      <author><first>Rishi</first><last>Bommasani</last></author>
      <author><first>Arzoo</first><last>Katiyar</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>13–17</pages>
      <abstract>We propose structured encoding as a novel approach to learning representations for relations and events in neural structured prediction. Our approach explicitly leverages the structure of available relation and event metadata to generate these representations, which are parameterized by both the attribute structure of the metadata as well as the learned representation of the arguments of the relations and events. We consider affine, biaffine, and recurrent operators for building hierarchical representations and modelling underlying features. We apply our approach to the second-order structured prediction task studied in the 2016/2017 Belief and Sentiment analysis evaluations (BeSt): given a document and its entities, relations, and events (including metadata and mentions), determine the sentiment of each entity towards every relation and event in the document. Without task-specific knowledge sources or domain engineering, we significantly improve over systems and baselines that neglect the available metadata or its hierarchical structure. We observe across-the-board improvements on the BeSt 2016/2017 sentiment analysis task of at least 2.3 (absolute) and 10.6% (relative) F-measure over the previous state-of-the-art.</abstract>
      <url hash="6db4a934">W19-1503</url>
      <attachment type="software" hash="ca648251">W19-1503.Software.zip</attachment>
      <attachment type="supplementary" hash="728417bf">W19-1503.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-1503</doi>
      <bibkey>bommasani-etal-2019-sparse</bibkey>
    </paper>
    <paper id="4">
      <title>Lightly-supervised Representation Learning with Global Interpretability</title>
      <author><first>Andrew</first><last>Zupon</last></author>
      <author><first>Maria</first><last>Alexeeva</last></author>
      <author><first>Marco</first><last>Valenzuela-Escárcega</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>18–28</pages>
      <abstract>We propose a lightly-supervised approach for information extraction, in particular named entity classification, which combines the benefits of traditional bootstrapping, i.e., use of limited annotations and interpretability of extraction patterns, with the robust learning approaches proposed in representation learning. Our algorithm iteratively learns custom embeddings for both the multi-word entities to be extracted and the patterns that match them from a few example entities per category. We demonstrate that this representation-based approach outperforms three other state-of-the-art bootstrapping approaches on two datasets: CoNLL-2003 and OntoNotes. Additionally, using these embeddings, our approach outputs a globally-interpretable model consisting of a decision list, by ranking patterns based on their proximity to the average entity embedding in a given class. We show that this interpretable model performs close to our complete bootstrapping model, proving that representation learning can be used to produce interpretable models with small loss in performance. This decision list can be edited by human experts to mitigate some of that loss and in some cases outperform the original model.</abstract>
      <url hash="d358e9b7">W19-1504</url>
      <doi>10.18653/v1/W19-1504</doi>
      <bibkey>zupon-etal-2019-lightly</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="5">
      <title>Semi-Supervised Teacher-Student Architecture for Relation Extraction</title>
      <author><first>Fan</first><last>Luo</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Rebecca</first><last>Sharp</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>29–37</pages>
      <abstract>Generating a large amount of training data for information extraction (IE) is either costly (if annotations are created manually), or runs the risk of introducing noisy instances (if distant supervision is used). On the other hand, semi-supervised learning (SSL) is a cost-efficient solution to combat lack of training data. In this paper, we adapt Mean Teacher (Tarvainen and Valpola, 2017), a denoising SSL framework to extract semantic relations between pairs of entities. We explore the sweet spot of amount of supervision required for good performance on this binary relation extraction task. Additionally, different syntax representations are incorporated into our models to enhance the learned representation of sentences. We evaluate our approach on the Google-IISc Distant Supervision (GDS) dataset, which removes test data noise present in all previous distance supervision datasets, which makes it a reliable evaluation benchmark (Jat et al., 2017). Our results show that the SSL Mean Teacher approach nears the performance of fully-supervised approaches even with only 10% of the labeled corpus. Further, the syntax-aware model outperforms other syntax-free approaches across all levels of supervision.</abstract>
      <url hash="18c9ef27">W19-1505</url>
      <doi>10.18653/v1/W19-1505</doi>
      <bibkey>luo-etal-2019-semi</bibkey>
    </paper>
  </volume>
  <volume id="16">
    <meta>
      <booktitle>Proceedings of the Combined Workshop on Spatial Language Understanding (<fixed-case>S</fixed-case>p<fixed-case>LU</fixed-case>) and Grounded Communication for Robotics (<fixed-case>R</fixed-case>obo<fixed-case>NLP</fixed-case>)</booktitle>
      <url hash="a21e0d5a">W19-16</url>
      <editor><first>Archna</first><last>Bhatia</last></editor>
      <editor><first>Yonatan</first><last>Bisk</last></editor>
      <editor><first>Parisa</first><last>Kordjamshidi</last></editor>
      <editor><first>Jesse</first><last>Thomason</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>robonlp</venue>
    </meta>
    <frontmatter>
      <url hash="d5a3d0f4">W19-1600</url>
      <bibkey>ws-2019-combined</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Corpus of Multimodal Interaction for Collaborative Planning</title>
      <author><first>Miltiadis Marios</first><last>Katsakioris</last></author>
      <author><first>Helen</first><last>Hastie</last></author>
      <author><first>Ioannis</first><last>Konstas</last></author>
      <author><first>Atanas</first><last>Laskov</last></author>
      <pages>1–6</pages>
      <abstract>As autonomous systems become more commonplace, we need a way to easily and naturally communicate to them our goals and collaboratively come up with a plan on how to achieve these goals. To this end, we conducted a Wizard of Oz study to gather data and investigate the way operators would collaboratively make plans via a conversational ‘planning assistant’ for remote autonomous systems. We present here a corpus of 22 dialogs from expert operators, which can be used to train such a system. Data analysis shows that multimodality is key to successful interaction, measured both quantitatively and qualitatively via user feedback.</abstract>
      <url hash="9f41eff1">W19-1601</url>
      <doi>10.18653/v1/W19-1601</doi>
      <bibkey>katsakioris-etal-2019-corpus</bibkey>
    </paper>
    <paper id="2">
      <title>¿<fixed-case>E</fixed-case>s un plátano? Exploring the Application of a Physically Grounded Language Acquisition System to <fixed-case>S</fixed-case>panish</title>
      <author><first>Caroline</first><last>Kery</last></author>
      <author><first>Francis</first><last>Ferraro</last></author>
      <author><first>Cynthia</first><last>Matuszek</last></author>
      <pages>7–17</pages>
      <abstract>In this paper we describe a multilingual grounded language learning system adapted from an English-only system. This system learns the meaning of words used in crowd-sourced descriptions by grounding them in the physical representations of the objects they are describing. Our work presents a framework to compare the performance of the system when applied to a new language and to identify modifications necessary to attain equal performance, with the goal of enhancing the ability of robots to learn language from a more diverse range of people. We then demonstrate this system with Spanish, through first analyzing the performance of translated Spanish, and then extending this analysis to a new corpus of crowd-sourced Spanish language data. We find that with small modifications, the system is able to learn color, object, and shape words with comparable performance between languages.</abstract>
      <url hash="7673e415">W19-1602</url>
      <doi>10.18653/v1/W19-1602</doi>
      <bibkey>kery-etal-2019-es</bibkey>
    </paper>
    <paper id="3">
      <title>From Virtual to Real: A Framework for Verbal Interaction with Robots</title>
      <author><first>Eugene</first><last>Joseph</last></author>
      <pages>18–28</pages>
      <abstract>A Natural Language Understanding (NLU) pipeline integrated with a 3D physics-based scene is a flexible way to develop and test language-based human-robot interaction, by virtualizing people, robot hardware and the target 3D environment. Here, interaction means both controlling robots using language and conversing with them about the user’s physical environment and her daily life. Such a virtual development framework was initially developed for the Bot Colony videogame launched on Steam in June 2014, and has been undergoing improvements since. The framework is focused of developing intuitive verbal interaction with various types of robots. Key robot functions (robot vision and object recognition, path planning and obstacle avoidance, task planning and constraints, grabbing and inverse kinematics), the human participants in the interaction, and the impact of gravity and other forces on the environment are all simulated using commercial 3D tools. The framework can be used as a robotics testbed: the results of our simulations can be compared with the output of algorithms in real robots, to validate such algorithms. A novelty of our framework is support for social interaction with robots - enabling robots to converse about people and objects in the user’s environment, as well as learning about human needs and everyday life topics from their owner.</abstract>
      <url hash="da933a3c">W19-1603</url>
      <doi>10.18653/v1/W19-1603</doi>
      <bibkey>joseph-2019-virtual</bibkey>
    </paper>
    <paper id="4">
      <title>Learning from Implicit Information in Natural Language Instructions for Robotic Manipulations</title>
      <author><first>Ozan Arkan</first><last>Can</last></author>
      <author><first>Pedro</first><last>Zuidberg Dos Martires</last></author>
      <author><first>Andreas</first><last>Persson</last></author>
      <author><first>Julian</first><last>Gaal</last></author>
      <author><first>Amy</first><last>Loutfi</last></author>
      <author><first>Luc</first><last>De Raedt</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <author><first>Alessandro</first><last>Saffiotti</last></author>
      <pages>29–39</pages>
      <abstract>Human-robot interaction often occurs in the form of instructions given from a human to a robot. For a robot to successfully follow instructions, a common representation of the world and objects in it should be shared between humans and the robot so that the instructions can be grounded. Achieving this representation can be done via learning, where both the world representation and the language grounding are learned simultaneously. However, in robotics this can be a difficult task due to the cost and scarcity of data. In this paper, we tackle the problem by separately learning the world representation of the robot and the language grounding. While this approach can address the challenges in getting sufficient data, it may give rise to inconsistencies between both learned components. Therefore, we further propose Bayesian learning to resolve such inconsistencies between the natural language grounding and a robot’s world representation by exploiting spatio-relational information that is implicitly present in instructions given by a human. Moreover, we demonstrate the feasibility of our approach on a scenario involving a robotic arm in the physical world.</abstract>
      <url hash="260af9a8">W19-1604</url>
      <doi>10.18653/v1/W19-1604</doi>
      <bibkey>can-etal-2019-learning</bibkey>
    </paper>
    <paper id="5">
      <title>Multi-modal Discriminative Model for Vision-and-Language Navigation</title>
      <author><first>Haoshuo</first><last>Huang</last></author>
      <author><first>Vihan</first><last>Jain</last></author>
      <author><first>Harsh</first><last>Mehta</last></author>
      <author><first>Jason</first><last>Baldridge</last></author>
      <author><first>Eugene</first><last>Ie</last></author>
      <pages>40–49</pages>
      <abstract>Vision-and-Language Navigation (VLN) is a natural language grounding task where agents have to interpret natural language instructions in the context of visual scenes in a dynamic environment to achieve prescribed navigation goals. Successful agents must have the ability to parse natural language of varying linguistic styles, ground them in potentially unfamiliar scenes, plan and react with ambiguous environmental feedback. Generalization ability is limited by the amount of human annotated data. In particular, paired vision-language sequence data is expensive to collect. We develop a discriminator that evaluates how well an instruction explains a given path in VLN task using multi-modal alignment. Our study reveals that only a small fraction of the high-quality augmented data from Fried et al., as scored by our discriminator, is useful for training VLN agents with similar performance. We also show that a VLN agent warm-started with pre-trained components from the discriminator outperforms the benchmark success rates of 35.5 by 10% relative measure.</abstract>
      <url hash="8d848fd7">W19-1605</url>
      <doi>10.18653/v1/W19-1605</doi>
      <bibkey>huang-etal-2019-multi-modal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/touchdown-dataset">Touchdown Dataset</pwcdataset>
    </paper>
    <paper id="6">
      <title>Semantic Spatial Representation: a unique representation of an environment based on an ontology for robotic applications</title>
      <author><first>Guillaume</first><last>Sarthou</last></author>
      <author><first>Aurélie</first><last>Clodic</last></author>
      <author><first>Rachid</first><last>Alami</last></author>
      <pages>50–60</pages>
      <abstract>It is important, for human-robot interaction, to endow the robot with the knowledge necessary to understand human needs and to be able to respond to them. We present a formalized and unified representation for indoor environments using an ontology devised for a route description task in which a robot must provide explanations to a person. We show that this representation can be used to choose a route to explain to a human as well as to verbalize it using a route perspective. Based on ontology, this representation has a strong possibility of evolution to adapt to many other applications. With it, we get the semantics of the environment elements while keeping a description of the known connectivity of the environment. This representation and the illustration algorithms, to find and verbalize a route, have been tested in two environments of different scales.</abstract>
      <url hash="6cc7c432">W19-1606</url>
      <doi>10.18653/v1/W19-1606</doi>
      <bibkey>sarthou-etal-2019-semantic</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>S</fixed-case>patial<fixed-case>N</fixed-case>et: A Declarative Resource for Spatial Relations</title>
      <author><first>Morgan</first><last>Ulinski</last></author>
      <author><first>Bob</first><last>Coyne</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>61–70</pages>
      <abstract>This paper introduces SpatialNet, a novel resource which links linguistic expressions to actual spatial configurations. SpatialNet is based on FrameNet (Ruppenhofer et al., 2016) and VigNet (Coyne et al., 2011), two resources which use frame semantics to encode lexical meaning. SpatialNet uses a deep semantic representation of spatial relations to provide a formal description of how a language expresses spatial information. This formal representation of the lexical semantics of spatial language also provides a consistent way to represent spatial meaning across multiple languages. In this paper, we describe the structure of SpatialNet, with examples from English and German. We also show how SpatialNet can be combined with other existing NLP tools to create a text-to-scene system for a language.</abstract>
      <url hash="7f072af1">W19-1607</url>
      <doi>10.18653/v1/W19-1607</doi>
      <bibkey>ulinski-etal-2019-spatialnet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="8">
      <title>What a neural language model tells us about spatial relations</title>
      <author><first>Mehdi</first><last>Ghanimifard</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>71–81</pages>
      <abstract>Understanding and generating spatial descriptions requires knowledge about what objects are related, their functional interactions, and where the objects are geometrically located. Different spatial relations have different functional and geometric bias. The wide usage of neural language models in different areas including generation of image description motivates the study of what kind of knowledge is encoded in neural language models about individual spatial relations. With the premise that the functional bias of relations is expressed in their word distributions, we construct multi-word distributional vector representations and show that these representations perform well on intrinsic semantic reasoning tasks, thus confirming our premise. A comparison of our vector representations to human semantic judgments indicates that different bias (functional or geometric) is captured in different data collection tasks which suggests that the contribution of the two meaning modalities is dynamic, related to the context of the task.</abstract>
      <url hash="7f39b67c">W19-1608</url>
      <attachment type="supplementary" hash="e7454776">W19-1608.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-1608</doi>
      <bibkey>ghanimifard-dobnik-2019-neural</bibkey>
      <pwccode url="https://github.com/GU-CLASP/what_nlm_srels" additional="false">GU-CLASP/what_nlm_srels</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
  </volume>
  <volume id="17">
    <meta>
      <booktitle>Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies</booktitle>
      <url hash="7f873b79">W19-17</url>
      <editor><first>Heidi</first><last>Christensen</last><affiliation>University of Sheffield</affiliation></editor>
      <editor><first>Kristy</first><last>Hollingshead</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></editor>
      <editor><first>Emily</first><last>Prud’hommeaux</last><affiliation>Boston College</affiliation></editor>
      <editor><first>Frank</first><last>Rudzicz</last><affiliation>University of Toronto</affiliation></editor>
      <editor><first>Keith</first><last>Vertanen</last><affiliation>Michigan Technological University</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>slpat</venue>
    </meta>
    <frontmatter>
      <url hash="f4d0b6a5">W19-1700</url>
      <bibkey>ws-2019-speech</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A user study to compare two conversational assistants designed for people with hearing impairments</title>
      <author><first>Anja</first><last>Virkkunen</last></author>
      <author><first>Juri</first><last>Lukkarila</last></author>
      <author><first>Kalle</first><last>Palomäki</last></author>
      <author><first>Mikko</first><last>Kurimo</last></author>
      <pages>1–8</pages>
      <abstract>Participating in conversations can be difficult for people with hearing loss, especially in acoustically challenging environments. We studied the preferences the hearing impaired have for a personal conversation assistant based on automatic speech recognition (ASR) technology. We created two prototypes which were evaluated by hearing impaired test users. This paper qualitatively compares the two based on the feedback obtained from the tests. The first prototype was a proof-of-concept system running real-time ASR on a laptop. The second prototype was developed for a mobile device with the recognizer running on a separate server. In the mobile device, augmented reality (AR) was used to help the hearing impaired observe gestures and lip movements of the speaker simultaneously with the transcriptions. Several testers found the systems useful enough to use in their daily lives, with majority preferring the mobile AR version. The biggest concern of the testers was the accuracy of the transcriptions and the lack of speaker identification.</abstract>
      <url hash="67fa3116">W19-1701</url>
      <doi>10.18653/v1/W19-1701</doi>
      <bibkey>virkkunen-etal-2019-user</bibkey>
    </paper>
    <paper id="2">
      <title>Modeling Acoustic-Prosodic Cues for Word Importance Prediction in Spoken Dialogues</title>
      <author><first>Sushant</first><last>Kafle</last></author>
      <author><first>Cissi Ovesdotter</first><last>Alm</last></author>
      <author><first>Matt</first><last>Huenerfauth</last></author>
      <pages>9–16</pages>
      <abstract>Prosodic cues in conversational speech aid listeners in discerning a message. We investigate whether acoustic cues in spoken dialogue can be used to identify the importance of individual words to the meaning of a conversation turn. Individuals who are Deaf and Hard of Hearing often rely on real-time captions in live meetings. Word error rate, a traditional metric for evaluating automatic speech recognition (ASR), fails to capture that some words are more important for a system to transcribe correctly than others. We present and evaluate neural architectures that use acoustic features for 3-class word importance prediction. Our model performs competitively against state-of-the-art text-based word-importance prediction models, and it demonstrates particular benefits when operating on imperfect ASR output.</abstract>
      <url hash="0a3d81e5">W19-1702</url>
      <doi>10.18653/v1/W19-1702</doi>
      <bibkey>kafle-etal-2019-modeling</bibkey>
    </paper>
    <paper id="3">
      <title>Permanent Magnetic Articulograph (<fixed-case>PMA</fixed-case>) vs Electromagnetic Articulograph (<fixed-case>EMA</fixed-case>) in Articulation-to-Speech Synthesis for Silent Speech Interface</title>
      <author><first>Beiming</first><last>Cao</last></author>
      <author><first>Nordine</first><last>Sebkhi</last></author>
      <author><first>Ted</first><last>Mau</last></author>
      <author><first>Omer T.</first><last>Inan</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <pages>17–23</pages>
      <abstract>Silent speech interfaces (SSIs) are devices that enable speech communication when audible speech is unavailable. Articulation-to-speech (ATS) synthesis is a software design in SSI that directly converts articulatory movement information into audible speech signals. Permanent magnetic articulograph (PMA) is a wireless articulator motion tracking technology that is similar to commercial, wired Electromagnetic Articulograph (EMA). PMA has shown great potential for practical SSI applications, because it is wireless. The ATS performance of PMA, however, is unknown when compared with current EMA. In this study, we compared the performance of ATS using a PMA we recently developed and a commercially available EMA (NDI Wave system). Datasets with same stimuli and size that were collected from tongue tip were used in the comparison. The experimental results indicated the performance of PMA was close to, although not as equally good as that of EMA. Furthermore, in PMA, converting the raw magnetic signals to positional signals did not significantly affect the performance of ATS, which support the future direction in PMA-based ATS can be focused on the use of positional signals to maximize the benefit of spatial analysis.</abstract>
      <url hash="3ce9ed4a">W19-1703</url>
      <doi>10.18653/v1/W19-1703</doi>
      <bibkey>cao-etal-2019-permanent</bibkey>
    </paper>
    <paper id="4">
      <title>Speech-based Estimation of Bulbar Regression in Amyotrophic Lateral Sclerosis</title>
      <author><first>Alan</first><last>Wisler</last></author>
      <author><first>Kristin</first><last>Teplansky</last></author>
      <author><first>Jordan</first><last>Green</last></author>
      <author><first>Yana</first><last>Yunusova</last></author>
      <author><first>Thomas</first><last>Campbell</last></author>
      <author><first>Daragh</first><last>Heitzman</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <pages>24–31</pages>
      <abstract>Amyotrophic Lateral Sclerosis (ALS) is a progressive neurological disease that leads to degeneration of motor neurons and, as a result, inhibits the ability of the brain to control muscle movements. Monitoring the progression of ALS is of fundamental importance due to the wide variability in disease outlook that exists across patients. This progression is typically tracked using the ALS functional rating scale - revised (ALSFRS-R), which is the current clinical assessment of a patient’s level of functional impairment including speech and other motor tasks. In this paper, we investigated automatic estimation of the ALSFRS-R bulbar subscore from acoustic and articulatory movement samples. Experimental results demonstrated the AFSFRS-R bulbar subscore can be predicted from speech samples, which has clinical implication for automatic monitoring of the disease progression of ALS using speech information.</abstract>
      <url hash="de018f9c">W19-1704</url>
      <doi>10.18653/v1/W19-1704</doi>
      <bibkey>wisler-etal-2019-speech</bibkey>
    </paper>
    <paper id="5">
      <title>A Blissymbolics Translation System</title>
      <author><first>Usman</first><last>Sohail</last></author>
      <author><first>David</first><last>Traum</last></author>
      <pages>32–36</pages>
      <abstract>Blissymbolics (Bliss) is a pictographic writing system that is used by people with communication disorders. Bliss attempts to create a writing system that makes words easier to distinguish by using pictographic symbols that encapsulate meaning rather than sound, as the English alphabet does for example. Users of Bliss rely on human interpreters to use Bliss. We created a translation system from Bliss to natural English with the hopes of decreasing the reliance on human interpreters by the Bliss community. We first discuss the basic rules of Blissymbolics. Then we point out some of the challenges associated with developing computer assisted tools for Blissymbolics. Next we talk about our ongoing work in developing a translation system, including current limitations, and future work. We conclude with a set of examples showing the current capabilities of our translation system.</abstract>
      <url hash="5894dd7c">W19-1705</url>
      <doi>10.18653/v1/W19-1705</doi>
      <bibkey>sohail-traum-2019-blissymbolics</bibkey>
    </paper>
    <paper id="6">
      <title>Investigating Speech Recognition for Improving Predictive <fixed-case>AAC</fixed-case></title>
      <author><first>Jiban</first><last>Adhikary</last></author>
      <author><first>Robbie</first><last>Watling</last></author>
      <author><first>Crystal</first><last>Fletcher</last></author>
      <author><first>Alex</first><last>Stanage</last></author>
      <author><first>Keith</first><last>Vertanen</last></author>
      <pages>37–43</pages>
      <abstract>Making good letter or word predictions can help accelerate the communication of users of high-tech AAC devices. This is particularly important for real-time person-to-person conversations. We investigate whether per forming speech recognition on the speaking-side of a conversation can improve language model based predictions. We compare the accuracy of three plausible microphone deployment options and the accuracy of two commercial speech recognition engines (Google and IBM Watson). We found that despite recognition word error rates of 7-16%, our ensemble of N-gram and recurrent neural network language models made predictions nearly as good as when they used the reference transcripts.</abstract>
      <url hash="5d596199">W19-1706</url>
      <doi>10.18653/v1/W19-1706</doi>
      <bibkey>adhikary-etal-2019-investigating</bibkey>
    </paper>
    <paper id="7">
      <title>Noisy Neural Language Modeling for Typing Prediction in <fixed-case>BCI</fixed-case> Communication</title>
      <author><first>Rui</first><last>Dong</last></author>
      <author><first>David</first><last>Smith</last></author>
      <author><first>Shiran</first><last>Dudy</last></author>
      <author><first>Steven</first><last>Bedrick</last></author>
      <pages>44–51</pages>
      <abstract>Language models have broad adoption in predictive typing tasks. When the typing history contains numerous errors, as in open-vocabulary predictive typing with brain-computer interface (BCI) systems, we observe significant performance degradation in both n-gram and recurrent neural network language models trained on clean text. In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified. We also propose an effective strategy for combining evidence from multiple ambiguous histories of BCI electroencephalogram measurements.</abstract>
      <url hash="21f92cf9">W19-1707</url>
      <doi>10.18653/v1/W19-1707</doi>
      <bibkey>dong-etal-2019-noisy</bibkey>
    </paper>
  </volume>
  <volume id="18">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Shortcomings in Vision and Language</booktitle>
      <url hash="348565b8">W19-18</url>
      <editor><first>Raffaella</first><last>Bernardi</last></editor>
      <editor><first>Raquel</first><last>Fernandez</last></editor>
      <editor><first>Spandana</first><last>Gella</last></editor>
      <editor><first>Kushal</first><last>Kafle</last></editor>
      <editor><first>Christopher</first><last>Kanan</last></editor>
      <editor><first>Stefan</first><last>Lee</last></editor>
      <editor><first>Moin</first><last>Nabi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="9198a764">W19-1800</url>
      <bibkey>ws-2019-shortcomings</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects</title>
      <author><first>Gabriel</first><last>Grand</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>1–13</pages>
      <abstract>Visual question answering (VQA) models have been shown to over-rely on linguistic biases in VQA datasets, answering questions “blindly” without considering visual context. Adversarial regularization (AdvReg) aims to address this issue via an adversary sub-network that encourages the main model to learn a bias-free representation of the question. In this work, we investigate the strengths and shortcomings of AdvReg with the goal of better understanding how it affects inference in VQA models. Despite achieving a new state-of-the-art on VQA-CP, we find that AdvReg yields several undesirable side-effects, including unstable gradients and sharply reduced performance on in-domain examples. We demonstrate that gradual introduction of regularization during training helps to alleviate, but not completely solve, these issues. Through error analyses, we observe that AdvReg improves generalization to binary questions, but impairs performance on questions with heterogeneous answer distributions. Qualitatively, we also find that regularized models tend to over-rely on visual features, while ignoring important linguistic cues in the question. Our results suggest that AdvReg requires further refinement before it can be considered a viable bias mitigation technique for VQA.</abstract>
      <url hash="5749eecb">W19-1801</url>
      <doi>10.18653/v1/W19-1801</doi>
      <bibkey>grand-belinkov-2019-adversarial</bibkey>
      <pwccode url="https://github.com/gabegrand/adversarial-vqa" additional="false">gabegrand/adversarial-vqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="2">
      <title>Referring to Objects in Videos Using Spatio-Temporal Identifying Descriptions</title>
      <author><first>Peratham</first><last>Wiriyathammabhum</last></author>
      <author><first>Abhinav</first><last>Shrivastava</last></author>
      <author><first>Vlad</first><last>Morariu</last></author>
      <author><first>Larry</first><last>Davis</last></author>
      <pages>14–25</pages>
      <abstract>This paper presents a new task, the grounding of spatio-temporal identifying descriptions in videos. Previous work suggests potential bias in existing datasets and emphasizes the need for a new data creation schema to better model linguistic structure. We introduce a new data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos. We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion. We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules. Finally, we propose a future challenge and a need for a robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.</abstract>
      <url hash="69406820">W19-1802</url>
      <attachment type="supplementary" hash="1af24fe0">W19-1802.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-1802</doi>
      <bibkey>wiriyathammabhum-etal-2019-referring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tempo">TEMPO</pwcdataset>
    </paper>
    <paper id="3">
      <title>A Survey on Biomedical Image Captioning</title>
      <author><first>John</first><last>Pavlopoulos</last></author>
      <author><first>Vasiliki</first><last>Kougia</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <pages>26–36</pages>
      <abstract>Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians. This article is the first survey of biomedical image captioning, discussing datasets, evaluation measures, and state of the art methods. Additionally, we suggest two baselines, a weak and a stronger one; the latter outperforms all current state of the art systems on one of the datasets.</abstract>
      <url hash="9f357265">W19-1803</url>
      <doi>10.18653/v1/W19-1803</doi>
      <bibkey>pavlopoulos-etal-2019-survey</bibkey>
      <pwccode url="https://github.com/nlpaueb/bio_image_caption" additional="true">nlpaueb/bio_image_caption</pwccode>
    </paper>
    <paper id="4">
      <title>Revisiting Visual Grounding</title>
      <author><first>Erik</first><last>Conser</last></author>
      <author><first>Kennedy</first><last>Hahn</last></author>
      <author><first>Chandler</first><last>Watson</last></author>
      <author><first>Melanie</first><last>Mitchell</last></author>
      <pages>37–46</pages>
      <abstract>We revisit a particular visual grounding method: the “Image Retrieval Using Scene Graphs” (IRSG) system of Johnson et al. Our experiments indicate that the system does not effectively use its learned object-relationship models. We also look closely at the IRSG dataset, as well as the widely used Visual Relationship Dataset (VRD) that is adapted from it. We find that these datasets exhibit bias that allows methods that ignore relationships to perform relatively well. We also describe several other problems with the IRSG dataset, and report on experiments using a subset of the dataset in which the biases and other problems are removed. Our studies contribute to a more general effort: that of better understanding what machine-learning methods that combine language and vision actually learn and what popular datasets actually test.</abstract>
      <url hash="ef8b475a">W19-1804</url>
      <doi>10.18653/v1/W19-1804</doi>
      <bibkey>conser-etal-2019-revisiting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vrd">VRD</pwcdataset>
    </paper>
    <paper id="5">
      <title>The Steep Road to Happily Ever after: an Analysis of Current Visual Storytelling Models</title>
      <author><first>Yatri</first><last>Modi</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>47–57</pages>
      <abstract>Visual storytelling is an intriguing and complex task that only recently entered the research arena. In this work, we survey relevant work to date, and conduct a thorough error analysis of three very recent approaches to visual storytelling. We categorize and provide examples of common types of errors, and identify key shortcomings in current work. Finally, we make recommendations for addressing these limitations in the future.</abstract>
      <url hash="16f3b92c">W19-1805</url>
      <doi>10.18653/v1/W19-1805</doi>
      <bibkey>modi-parde-2019-steep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="6">
      <title>“Caption” as a Coherence Relation: Evidence and Implications</title>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <author><first>Matthew</first><last>Stone</last></author>
      <pages>58–67</pages>
      <abstract>We study verbs in image–text corpora, contrasting <i>caption</i> corpora, where texts are explicitly written to characterize image content, with <i>depiction</i> corpora, where texts and images may stand in more general relations. Captions show a distinctively limited distribution of verbs, with strong preferences for specific tense, aspect, lexical aspect, and semantic field. These limitations, which appear in data elicited by a range of methods, restrict the utility of caption corpora to inform image retrieval, multimodal document generation, and perceptually-grounded semantic models. We suggest that these limitations reflect the discourse constraints in play when subjects write texts to accompany imagery, so we argue that future development of image–text corpora should work to increase the diversity of event descriptions, while looking explicitly at the different ways text and imagery can be coherently related.</abstract>
      <url hash="53995372">W19-1806</url>
      <doi>10.18653/v1/W19-1806</doi>
      <bibkey>alikhani-stone-2019-caption</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="7">
      <title>Learning Multilingual Word Embeddings Using Image-Text Data</title>
      <author><first>Karan</first><last>Singhal</last></author>
      <author><first>Karthik</first><last>Raman</last></author>
      <author><first>Balder</first><last>ten Cate</last></author>
      <pages>68–77</pages>
      <abstract>There has been significant interest recently in learning multilingual word embeddings – in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.</abstract>
      <url hash="ad851d84">W19-1807</url>
      <doi>10.18653/v1/W19-1807</doi>
      <bibkey>singhal-etal-2019-learning</bibkey>
    </paper>
    <paper id="8">
      <title>Grounded Word Sense Translation</title>
      <author><first>Chiraag</first><last>Lala</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>78–85</pages>
      <abstract>Recent work on visually grounded language learning has focused on broader applications of grounded representations, such as visual question answering and multimodal machine translation. In this paper we consider grounded word sense translation, i.e. the task of correctly translating an ambiguous source word given the corresponding textual and visual context. Our main objective is to investigate the extent to which images help improve word-level (lexical) translation quality. We do so by first studying the dataset for this task to understand the scope and challenges of the task. We then explore different data settings, image features, and ways of grounding to investigate the gain from using images in each of the combinations. We find that grounding on the image is specially beneficial in weaker unidirectional recurrent translation models. We observe that adding structured image information leads to stronger gains in lexical translation accuracy.</abstract>
      <url hash="5d24812c">W19-1808</url>
      <doi>10.18653/v1/W19-1808</doi>
      <bibkey>lala-etal-2019-grounded</bibkey>
    </paper>
  </volume>
  <volume id="19">
    <meta>
      <booktitle>Proceedings of the 2nd Clinical Natural Language Processing Workshop</booktitle>
      <url hash="4daac6be">W19-19</url>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <editor><first>Kirk</first><last>Roberts</last></editor>
      <editor><first>Steven</first><last>Bethard</last></editor>
      <editor><first>Tristan</first><last>Naumann</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota, USA</address>
      <month>June</month>
      <year>2019</year>
      <venue>clinicalnlp</venue>
    </meta>
    <frontmatter>
      <url hash="f8f6a070">W19-1900</url>
      <bibkey>ws-2019-clinical</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Effective Feature Representation for Clinical Text Concept Extraction</title>
      <author><first>Yifeng</first><last>Tao</last></author>
      <author><first>Bruno</first><last>Godefroy</last></author>
      <author><first>Guillaume</first><last>Genthial</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>1–14</pages>
      <abstract>Crucial information about the practice of healthcare is recorded only in free-form text, which creates an enormous opportunity for high-impact NLP. However, annotated healthcare datasets tend to be small and expensive to obtain, which raises the question of how to make maximally efficient uses of the available data. To this end, we develop an LSTM-CRF model for combining unsupervised word representations and hand-built feature representations derived from publicly available healthcare ontologies. We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text (clinical, social, scientific, commercial). Each involves the labeling of complex, multi-word spans that pick out different healthcare concepts. We also introduce a new labeled dataset for identifying the treatment relations between drugs and diseases.</abstract>
      <url hash="b6c093a7">W19-1901</url>
      <attachment type="software" hash="03cc6c09">W19-1901.Software.txt</attachment>
      <doi>10.18653/v1/W19-1901</doi>
      <bibkey>tao-etal-2019-effective</bibkey>
    </paper>
    <paper id="2">
      <title>An Analysis of Attention over Clinical Notes for Predictive Tasks</title>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Ramin</first><last>Mohammadi</last></author>
      <author><first>Byron C.</first><last>Wallace</last></author>
      <pages>15–21</pages>
      <abstract>The shift to electronic medical records (EMRs) has engendered research into machine learning and natural language technologies to analyze patient records, and to predict from these clinical outcomes of interest. Two observations motivate our aims here. First, unstructured notes contained within EMR often contain key information, and hence should be exploited by models. Second, while strong predictive performance is important, interpretability of models is perhaps equally so for applications in this domain. Together, these points suggest that neural models for EMR may benefit from incorporation of attention over notes, which one may hope will both yield performance gains and afford transparency in predictions. In this work we perform experiments to explore this question using two EMR corpora and four different predictive tasks, that: (i) inclusion of attention mechanisms is critical for neural encoder modules that operate over notes fields in order to yield competitive performance, but, (ii) unfortunately, while these boost predictive performance, it is decidedly less clear whether they provide meaningful support for predictions.</abstract>
      <url hash="3c19c285">W19-1902</url>
      <doi>10.18653/v1/W19-1902</doi>
      <bibkey>jain-etal-2019-analysis</bibkey>
    </paper>
    <paper id="3">
      <title>Extracting Adverse Drug Event Information with Minimal Engineering</title>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Alon</first><last>Geva</last></author>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <pages>22–27</pages>
      <abstract>In this paper we describe an evaluation of the potential of classical information extraction methods to extract drug-related attributes, including adverse drug events, and compare to more recently developed neural methods. We use the 2018 N2C2 shared task data as our gold standard data set for training. We train support vector machine classifiers to detect drug and drug attribute spans, and pair these detected entities as training instances for an SVM relation classifier, with both systems using standard features. We compare to baseline neural methods that use standard contextualized embedding representations for entity and relation extraction. The SVM-based system and a neural system obtain comparable results, with the SVM system doing better on concepts and the neural system performing better on relation extraction tasks. The neural system obtains surprisingly strong results compared to the system based on years of research in developing features for information extraction.</abstract>
      <url hash="d30af476">W19-1903</url>
      <doi>10.18653/v1/W19-1903</doi>
      <bibkey>miller-etal-2019-extracting</bibkey>
    </paper>
    <paper id="4">
      <title>Hierarchical Nested Named Entity Recognition</title>
      <author><first>Zita</first><last>Marinho</last></author>
      <author><first>Afonso</first><last>Mendes</last></author>
      <author><first>Sebastião</first><last>Miranda</last></author>
      <author><first>David</first><last>Nogueira</last></author>
      <pages>28–34</pages>
      <abstract>In the medical domain and other scientific areas, it is often important to recognize different levels of hierarchy in mentions, such as those related to specific symptoms or diseases associated with different anatomical regions. Unlike previous approaches, we build a transition-based parser that explicitly models an arbitrary number of hierarchical and nested mentions, and propose a loss that encourages correct predictions of higher-level mentions. We further introduce a set of modifier classes which introduces certain concepts that change the meaning of an entity, such as absence, or uncertainty about a given disease. Our proposed model achieves state-of-the-art results in medical entity recognition datasets, using both nested and hierarchical mentions.</abstract>
      <url hash="b8a3c4ce">W19-1904</url>
      <attachment type="supplementary" hash="2bf0ec68">W19-1904.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-1904</doi>
      <bibkey>marinho-etal-2019-hierarchical</bibkey>
    </paper>
    <paper id="5">
      <title>Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models</title>
      <author><first>Oren</first><last>Melamud</last></author>
      <author><first>Chaitanya</first><last>Shivade</last></author>
      <pages>35–45</pages>
      <abstract>Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding patient privacy hinder the open dissemination of such data and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in clinical notes and propose to automatically generate synthetic clinical notes that are more amenable to sharing using generative models trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as utility in training clinical NLP models. Experiments using neural language models yield notes whose utility is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements.</abstract>
      <url hash="cd5778b2">W19-1905</url>
      <doi>10.18653/v1/W19-1905</doi>
      <bibkey>melamud-shivade-2019-towards</bibkey>
      <pwccode url="https://github.com/orenmel/synth-clinical-notes" additional="false">orenmel/synth-clinical-notes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="6">
      <title>A Novel System for Extractive Clinical Note Summarization using <fixed-case>EHR</fixed-case> Data</title>
      <author><first>Jennifer</first><last>Liang</last></author>
      <author><first>Ching-Huei</first><last>Tsou</last></author>
      <author><first>Ananya</first><last>Poddar</last></author>
      <pages>46–54</pages>
      <abstract>While much data within a patient’s electronic health record (EHR) is coded, crucial information concerning the patient’s care and management remain buried in unstructured clinical notes, making it difficult and time-consuming for physicians to review during their usual clinical workflow. In this paper, we present our clinical note processing pipeline, which extends beyond basic medical natural language processing (NLP) with concept recognition and relation detection to also include components specific to EHR data, such as structured data associated with the encounter, sentence-level clinical aspects, and structures of the clinical notes. We report on the use of this pipeline in a disease-specific extractive text summarization task on clinical notes, focusing primarily on progress notes by physicians and nurse practitioners. We show how the addition of EHR-specific components to the pipeline resulted in an improvement in our overall system performance and discuss the potential impact of EHR-specific components on other higher-level clinical NLP tasks.</abstract>
      <url hash="fe9eca00">W19-1906</url>
      <revision id="1" href="W19-1906v1" hash="57ffe67f"/>
      <revision id="2" href="W19-1906v2" hash="fe9eca00">No description of the changes were recorded.</revision>
      <doi>10.18653/v1/W19-1906</doi>
      <bibkey>liang-etal-2019-novel-system</bibkey>
    </paper>
    <paper id="7">
      <title>Study of lexical aspect in the <fixed-case>F</fixed-case>rench medical language. Development of a lexical resource</title>
      <author><first>Agathe</first><last>Pierson</last></author>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <pages>55–64</pages>
      <abstract>This paper details the development of a linguistic resource designed to improve temporal information extraction systems and to integrate aspectual values. After a brief review of recent works in temporal information extraction for the medical area, we discuss the linguistic notion of aspect and how it got a place in the NLP field. Then, we present our clinical data and describe the five-step approach adopted in this study. Finally, we represent the linguistic resource itself and explain how we elaborated it and which properties were selected for the creation of the tables.</abstract>
      <url hash="a4debe2d">W19-1907</url>
      <doi>10.18653/v1/W19-1907</doi>
      <bibkey>pierson-fairon-2019-study</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="8">
      <title>A <fixed-case>BERT</fixed-case>-based Universal Model for Both Within- and Cross-sentence Clinical Temporal Relation Extraction</title>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Guergana</first><last>Savova</last></author>
      <pages>65–71</pages>
      <abstract>Classic methods for clinical temporal relation extraction focus on relational candidates within a sentence. On the other hand, break-through Bidirectional Encoder Representations from Transformers (BERT) are trained on large quantities of arbitrary spans of contiguous text instead of sentences. In this study, we aim to build a sentence-agnostic framework for the task of CONTAINS temporal relation extraction. We establish a new state-of-the-art result for the task, 0.684F for in-domain (0.055-point improvement) and 0.565F for cross-domain (0.018-point improvement), by fine-tuning BERT and pre-training domain-specific BERT models on sentence-agnostic temporal relation instances with WordPiece-compatible encodings, and augmenting the labeled data with automatically generated “silver” instances.</abstract>
      <url hash="2af928ba">W19-1908</url>
      <doi>10.18653/v1/W19-1908</doi>
      <bibkey>lin-etal-2019-bert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="9">
      <title>Publicly Available Clinical <fixed-case>BERT</fixed-case> Embeddings</title>
      <author><first>Emily</first><last>Alsentzer</last></author>
      <author><first>John</first><last>Murphy</last></author>
      <author><first>William</first><last>Boag</last></author>
      <author><first>Wei-Hung</first><last>Weng</last></author>
      <author><first>Di</first><last>Jindi</last></author>
      <author><first>Tristan</first><last>Naumann</last></author>
      <author><first>Matthew</first><last>McDermott</last></author>
      <pages>72–78</pages>
      <abstract>Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.</abstract>
      <url hash="0d5393f7">W19-1909</url>
      <doi>10.18653/v1/W19-1909</doi>
      <bibkey>alsentzer-etal-2019-publicly</bibkey>
      <pwccode url="https://github.com/EmilyAlsentzer/clinicalBERT" additional="true">EmilyAlsentzer/clinicalBERT</pwccode>
    </paper>
    <paper id="10">
      <title>A General-Purpose Annotation Model for Knowledge Discovery: Case Study in <fixed-case>S</fixed-case>panish Clinical Text</title>
      <author><first>Alejandro</first><last>Piad-Morffis</last></author>
      <author><first>Yoan</first><last>Guitérrez</last></author>
      <author><first>Suilan</first><last>Estevez-Velarde</last></author>
      <author><first>Rafael</first><last>Muñoz</last></author>
      <pages>79–88</pages>
      <abstract>Knowledge discovery from text in natural language is a task usually aided by the manual construction of annotated corpora. Specifically in the clinical domain, several annotation models are used depending on the characteristics of the task to solve (e.g., named entity recognition, relation extraction, etc.). However, few general-purpose annotation models exist, that can support a broad range of knowledge extraction tasks. This paper presents an annotation model designed to capture a large portion of the semantics of natural language text. The structure of the annotation model is presented, with examples of annotated sentences and a brief description of each semantic role and relation defined. This research focuses on an application to clinical texts in the Spanish language. Nevertheless, the presented annotation model is extensible to other domains and languages. An example of annotated sentences, guidelines, and suitable configuration files for an annotation tool are also provided for the research community.</abstract>
      <url hash="790d2e5a">W19-1910</url>
      <doi>10.18653/v1/W19-1910</doi>
      <bibkey>piad-morffis-etal-2019-general</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="11">
      <title>Predicting <fixed-case>ICU</fixed-case> transfers using text messages between nurses and doctors</title>
      <author><first>Faiza</first><last>Khan Khattak</last></author>
      <author><first>Chloé</first><last>Pou-Prom</last></author>
      <author><first>Robert</first><last>Wu</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>89–94</pages>
      <abstract>We explore the use of real-time clinical information, i.e., text messages sent between nurses and doctors regarding patient conditions in order to predict transfer to the intensive care unit(ICU). Preliminary results, in data from five hospitals, indicate that, despite being short and full of noise, text messages can augment other visit information to improve the performance of ICU transfer prediction.</abstract>
      <url hash="4a56d9db">W19-1911</url>
      <doi>10.18653/v1/W19-1911</doi>
      <bibkey>khan-khattak-etal-2019-predicting</bibkey>
    </paper>
    <paper id="12">
      <title>Medical Entity Linking using Triplet Network</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <author><first>Sukannya</first><last>Purkayastha</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <author><first>Jitesh</first><last>Pillai</last></author>
      <author><first>Amitava</first><last>Bhattacharyya</last></author>
      <author><first>Mahanandeeshwar</first><last>Gattu</last></author>
      <pages>95–100</pages>
      <abstract>Entity linking (or Normalization) is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base (KB). This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.</abstract>
      <url hash="152cc6f2">W19-1912</url>
      <doi>10.18653/v1/W19-1912</doi>
      <bibkey>mondal-etal-2019-medical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="13">
      <title>Annotating and Characterizing Clinical Sentences with Explicit Why-<fixed-case>QA</fixed-case> Cues</title>
      <author><first>Jungwei</first><last>Fan</last></author>
      <pages>101–106</pages>
      <abstract>Many clinical information needs can be stated as why-questions. The answers to them represent important clinical reasoning and justification. Clinical notes are a rich source for such why-question answering (why-QA). However, there are few dedicated corpora, and little is known about the characteristics of clinical why-QA narratives. To address this gap, the study performed manual annotation of 277 sentences containing explicit why-QA cues and summarized their quantitative and qualitative properties. The contributions are: 1) sharing a seed corpus that can be used for various QA-related training purposes, 2) adding to our knowledge about the diversity and distribution of clinical why-QA contents.</abstract>
      <url hash="9da7d28b">W19-1913</url>
      <doi>10.18653/v1/W19-1913</doi>
      <bibkey>fan-2019-annotating</bibkey>
      <pwccode url="https://github.com/Jung-wei/ClinicalWhyQA" additional="false">Jung-wei/ClinicalWhyQA</pwccode>
    </paper>
    <paper id="14">
      <title>Extracting Factual <fixed-case>M</fixed-case>in/Max Age Information from Clinical Trial Studies</title>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Debasis</first><last>Ganguly</last></author>
      <author><first>Léa</first><last>Deleris</last></author>
      <author><first>Francesca</first><last>Bonin</last></author>
      <pages>107–116</pages>
      <abstract>Population age information is an essential characteristic of clinical trials. In this paper, we focus on extracting minimum and maximum (min/max) age values for the study samples from clinical research articles. Specifically, we investigate the use of a neural network model for question answering to address this information extraction task. The min/max age QA model is trained on the massive structured clinical study records from ClinicalTrials.gov. For each article, based on multiple min and max age values extracted from the QA model, we predict both actual min/max age values for the study samples and filter out non-factual age expressions. Our system improves the results over (i) a passage retrieval based IE system and (ii) a CRF-based system by a large margin when evaluated on an annotated dataset consisting of 50 research papers on smoking cessation.</abstract>
      <url hash="f736ded0">W19-1914</url>
      <doi>10.18653/v1/W19-1914</doi>
      <bibkey>hou-etal-2019-extracting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/clicr">CliCR</pwcdataset>
    </paper>
    <paper id="15">
      <title>Distinguishing Clinical Sentiment: The Importance of Domain Adaptation in Psychiatric Patient Health Records</title>
      <author><first>Eben</first><last>Holderness</last></author>
      <author><first>Philip</first><last>Cawkwell</last></author>
      <author><first>Kirsten</first><last>Bolton</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Mei-Hua</first><last>Hall</last></author>
      <pages>117–123</pages>
      <abstract>Recently natural language processing (NLP) tools have been developed to identify and extract salient risk indicators in electronic health records (EHRs). Sentiment analysis, although widely used in non-medical areas for improving decision making, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of sentiment analysis to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining sentiment analysis methods for clinical use. Our long-term objective is to incorporate the results of this project as part of a machine learning model that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of sentiment analysis to the clinical setting.</abstract>
      <url hash="67c7cb40">W19-1915</url>
      <doi>10.18653/v1/W19-1915</doi>
      <bibkey>holderness-etal-2019-distinguishing</bibkey>
    </paper>
    <paper id="16">
      <title>Medical Word Embeddings for <fixed-case>S</fixed-case>panish: Development and Evaluation</title>
      <author><first>Felipe</first><last>Soares</last></author>
      <author><first>Marta</first><last>Villegas</last></author>
      <author><first>Aitor</first><last>Gonzalez-Agirre</last></author>
      <author><first>Martin</first><last>Krallinger</last></author>
      <author><first>Jordi</first><last>Armengol-Estapé</last></author>
      <pages>124–133</pages>
      <abstract>Word embeddings are representations of words in a dense vector space. Although they are not recent phenomena in Natural Language Processing (NLP), they have gained momentum after the recent developments of neural methods and Word2Vec. Regarding their applications in medical and clinical NLP, they are invaluable resources when training in-domain named entity recognition systems, classifiers or taggers, for instance. Thus, the development of tailored word embeddings for medical NLP is of great interest. However, we identified a gap in the literature which we aim to fill in this paper: the availability of embeddings for medical NLP in Spanish, as well as a standardized form of intrinsic evaluation. Since most work has been done for English, some established datasets for intrinsic evaluation are already available. In this paper, we show the steps we employed to adapt such datasets for the first time to Spanish, of particular relevance due to the considerable volume of EHRs in this language, as well as the creation of in-domain medical word embeddings for the Spanish using the state-of-the-art FastText model. We performed intrinsic evaluation with our adapted datasets, as well as extrinsic evaluation with a named entity recognition systems using a baseline embedding of general-domain. Both experiments proved that our embeddings are suitable for use in medical NLP in the Spanish language, and are more accurate than general-domain ones.</abstract>
      <url hash="0d4f5aac">W19-1916</url>
      <doi>10.18653/v1/W19-1916</doi>
      <bibkey>soares-etal-2019-medical</bibkey>
    </paper>
    <paper id="17">
      <title>Attention Neural Model for Temporal Relation Extraction</title>
      <author><first>Sijia</first><last>Liu</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <author><first>Vipin</first><last>Chaudhary</last></author>
      <author><first>Hongfang</first><last>Liu</last></author>
      <pages>134–139</pages>
      <abstract>Neural network models have shown promise in the temporal relation extraction task. In this paper, we present the attention based neural network model to extract the containment relations within sentences from clinical narratives. The attention mechanism used on top of GRU model outperforms the existing state-of-the-art neural network models on THYME corpus in intra-sentence temporal relation extraction.</abstract>
      <url hash="3a680a0e">W19-1917</url>
      <doi>10.18653/v1/W19-1917</doi>
      <bibkey>liu-etal-2019-attention</bibkey>
    </paper>
    <paper id="18">
      <title>Automatically Generating Psychiatric Case Notes From Digital Transcripts of Doctor-Patient Conversations</title>
      <author><first>Nazmul</first><last>Kazi</last></author>
      <author><first>Indika</first><last>Kahanda</last></author>
      <pages>140–148</pages>
      <abstract>Electronic health records (EHRs) are notorious for reducing the face-to-face time with patients while increasing the screen-time for clinicians leading to burnout. This is especially problematic for psychiatry care in which maintaining consistent eye-contact and non-verbal cues are just as important as the spoken words. In this ongoing work, we explore the feasibility of automatically generating psychiatric EHR case notes from digital transcripts of doctor-patient conversation using a two-step approach: (1) predicting semantic topics for segments of transcripts using supervised machine learning, and (2) generating formal text of those segments using natural language processing. Through a series of preliminary experimental results obtained through a collection of synthetic and real-life transcripts, we demonstrate the viability of this approach.</abstract>
      <url hash="2aea8c35">W19-1918</url>
      <doi>10.18653/v1/W19-1918</doi>
      <bibkey>kazi-kahanda-2019-automatically</bibkey>
    </paper>
    <paper id="19">
      <title>Clinical Data Classification using Conditional Random Fields and Neural Parsing for Morphologically Rich Languages</title>
      <author><first>Razieh</first><last>Ehsani</last></author>
      <author><first>Tyko</first><last>Niemi</last></author>
      <author><first>Gaurav</first><last>Khullar</last></author>
      <author><first>Tiina</first><last>Leivo</last></author>
      <pages>149–155</pages>
      <abstract>Past prescriptions constitute a central element in patient records. These are often written in an unstructured and brief form. Extracting information from such prescriptions enables the development of automated processes in the medical data mining field. This paper presents a Conditional Random Fields (CRFs) based approach to extract relevant information from prescriptions. We focus on Finnish language prescriptions and make use of Finnish language specific features. Our labeling accuracy is 95%, which compares favorably to the current state-of-the-art in English language prescriptions. This, to the best of our knowledge, is the first such work for the Finnish language.</abstract>
      <url hash="cd1850cc">W19-1919</url>
      <doi>10.18653/v1/W19-1919</doi>
      <bibkey>ehsani-etal-2019-clinical</bibkey>
    </paper>
  </volume>
  <volume id="20">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for <fixed-case>NLP</fixed-case></booktitle>
      <url hash="16b84c9f">W19-20</url>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>Aleksandr</first><last>Drozd</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <editor><first>Yoav</first><last>Goldberg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, USA</address>
      <month>June</month>
      <year>2019</year>
      <venue>repeval</venue>
    </meta>
    <frontmatter>
      <url hash="2605b31b">W19-2000</url>
      <bibkey>ws-2019-evaluating</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Neural Vector Conceptualization for Word Vector Space Interpretation</title>
      <author><first>Robert</first><last>Schwarzenberg</last></author>
      <author><first>Lisa</first><last>Raithel</last></author>
      <author><first>David</first><last>Harbecke</last></author>
      <pages>1–7</pages>
      <abstract>Distributed word vector spaces are considered hard to interpret which hinders the understanding of natural language processing (NLP) models. In this work, we introduce a new method to interpret arbitrary samples from a word vector space. To this end, we train a neural model to conceptualize word vectors, which means that it activates higher order concepts it recognizes in a given vector. Contrary to prior approaches, our model operates in the original vector space and is capable of learning non-linear relations between word vectors and concepts. Furthermore, we show that it produces considerably less entropic concept activation profiles than the popular cosine similarity.</abstract>
      <url hash="bd10bead">W19-2001</url>
      <doi>10.18653/v1/W19-2001</doi>
      <bibkey>schwarzenberg-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/dfki-nlp/nvc" additional="false">dfki-nlp/nvc</pwccode>
    </paper>
    <paper id="2">
      <title>Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance</title>
      <author><first>Brendan</first><last>Whitaker</last></author>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Aparajita</first><last>Haldar</last></author>
      <author><first>Hakan</first><last>Ferhatosmanoglu</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <pages>8–17</pages>
      <abstract>Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing nearest neighbors. However, geometric properties of the continuous feature space contribute directly to the use of embedding features in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely: position relative to the origin, distribution of features in the vector space, global pairwise distances, and local pairwise distances. We define a sequence of transformations to generate new embeddings that expose subsets of these properties to downstream models and evaluate change in task performance to understand the contribution of each property to NLP models. We transform publicly available pretrained embeddings from three popular toolkits (word2vec, GloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model linguistic information in the vector space, and extrinsic tasks, which use vectors as input to machine learning models. We find that intrinsic evaluations are highly sensitive to absolute position, while extrinsic tasks rely primarily on local similarity. Our findings suggest that future embedding models and post-processing techniques should focus primarily on similarity to nearby points in vector space.</abstract>
      <url hash="75c67ace">W19-2002</url>
      <attachment type="supplementary" hash="c1602241">W19-2002.Supplementary.zip</attachment>
      <attachment type="presentation" hash="30e102f8">W19-2002.Presentation.pdf</attachment>
      <doi>10.18653/v1/W19-2002</doi>
      <bibkey>whitaker-etal-2019-characterizing</bibkey>
      <pwccode url="https://github.com/OSU-slatelab/geometric-embedding-properties" additional="true">OSU-slatelab/geometric-embedding-properties</pwccode>
    </paper>
    <paper id="3">
      <title>The Influence of Down-Sampling Strategies on <fixed-case>SVD</fixed-case> Word Embedding Stability</title>
      <author><first>Johannes</first><last>Hellrich</last></author>
      <author><first>Bernd</first><last>Kampe</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>18–26</pages>
      <abstract>The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD-PPMI-type embeddings. This finding seems to explain diverging reports on their stability and lead us to a simple modification which provides superior stability as well as accuracy on par with skip-gram embedding</abstract>
      <url hash="2423ccc5">W19-2003</url>
      <doi>10.18653/v1/W19-2003</doi>
      <bibkey>hellrich-etal-2019-influence</bibkey>
    </paper>
    <paper id="4">
      <title>How Well Do Embedding Models Capture Non-compositionality? A View from Multiword Expressions</title>
      <author><first>Navnita</first><last>Nandakumar</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Bahar</first><last>Salehi</last></author>
      <pages>27–34</pages>
      <abstract>In this paper, we apply various embedding methods on multiword expressions to study how well they capture the nuances of non-compositional data. Our results from a pool of word-, character-, and document-level embbedings suggest that Word2vec performs the best, followed by FastText and Infersent. Moreover, we find that recently-proposed contextualised embedding models such as Bert and ELMo are not adept at handling non-compositionality in multiword expressions.</abstract>
      <url hash="072d61d0">W19-2004</url>
      <doi>10.18653/v1/W19-2004</doi>
      <bibkey>nandakumar-etal-2019-well</bibkey>
    </paper>
    <paper id="5">
      <title>Measuring Semantic Abstraction of Multilingual <fixed-case>NMT</fixed-case> with Paraphrase Recognition and Generation Tasks</title>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <pages>35–42</pages>
      <abstract>In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language. The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. This is further supported by a study on paraphrase generation that we also include at the end of the paper.</abstract>
      <url hash="30761bcc">W19-2005</url>
      <attachment type="supplementary" hash="3235b149">W19-2005.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-2005</doi>
      <bibkey>tiedemann-scherrer-2019-measuring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
    </paper>
    <paper id="6">
      <title><fixed-case>SWOW</fixed-case>-8500: Word Association task for Intrinsic Evaluation of Word Embeddings</title>
      <author><first>Avijit</first><last>Thawani</last></author>
      <author><first>Biplav</first><last>Srivastava</last></author>
      <author><first>Anil</first><last>Singh</last></author>
      <pages>43–51</pages>
      <abstract>Downstream evaluation of pretrained word embeddings is expensive, more so for tasks where current state of the art models are very large architectures. Intrinsic evaluation using word similarity or analogy datasets, on the other hand, suffers from several disadvantages. We propose a novel intrinsic evaluation task employing large word association datasets (particularly the Small World of Words dataset). We observe correlations not just between performances on SWOW-8500 and previously proposed intrinsic tasks of word similarity prediction, but also with downstream tasks (eg. Text Classification and Natural Language Inference). Most importantly, we report better confidence intervals for scores on our word association task, with no fall in correlation with downstream performance.</abstract>
      <url hash="c39e59c2">W19-2006</url>
      <attachment type="poster" hash="94a3ac2d">W19-2006.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-2006</doi>
      <bibkey>thawani-etal-2019-swow</bibkey>
      <pwccode url="https://github.com/avi-jit/SWOW-eval" additional="false">avi-jit/SWOW-eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="7">
      <title>Classification of Semantic Paraphasias: Optimization of a Word Embedding Model</title>
      <author><first>Katy</first><last>McKinney-Bock</last></author>
      <author><first>Steven</first><last>Bedrick</last></author>
      <pages>52–62</pages>
      <abstract>In clinical assessment of people with aphasia, impairment in the ability to recall and produce words for objects (anomia) is assessed using a confrontation naming task, where a target stimulus is viewed and a corresponding label is spoken by the participant. Vector space word embedding models have had inital results in assessing semantic similarity of target-production pairs in order to automate scoring of this task; however, the resulting models are also highly dependent upon training parameters. To select an optimal family of models, we fit a beta regression model to the distribution of performance metrics on a set of 2,880 grid search models and evaluate the resultant first- and second-order effects to explore how parameterization affects model performance. Comparing to SimLex-999, we show that clinical data can be used in an evaluation task with comparable optimal parameter settings as standard NLP evaluation datasets.</abstract>
      <url hash="38628807">W19-2007</url>
      <doi>10.18653/v1/W19-2007</doi>
      <bibkey>mckinney-bock-bedrick-2019-classification</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>CODAH</fixed-case>: An Adversarially-Authored Question Answering Dataset for Common Sense</title>
      <author><first>Michael</first><last>Chen</last></author>
      <author><first>Mike</first><last>D’Arcy</last></author>
      <author><first>Alisa</first><last>Liu</last></author>
      <author><first>Jared</first><last>Fernandez</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <pages>63–69</pages>
      <abstract>Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging datasets that test common sense. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the datasets to achieve human-level scores. We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing common sense. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset. We observe a significant gap between human performance, which is 95.3%, and the performance of the best baseline accuracy of 65.3% by the OpenAI GPT model.</abstract>
      <url hash="0e2d8914">W19-2008</url>
      <doi>10.18653/v1/W19-2008</doi>
      <bibkey>chen-etal-2019-codah</bibkey>
      <pwccode url="https://github.com/Websail-NU/CODAH" additional="false">Websail-NU/CODAH</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codah">CODAH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="9">
      <title>Syntactic Interchangeability in Word Embedding Models</title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Assaf</first><last>Toledo</last></author>
      <author><first>Alon</first><last>Halfon</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>70–76</pages>
      <abstract>Nearest neighbors in word embedding models are commonly observed to be semantically similar, but the relations between them can vary greatly. We investigate the extent to which word embedding models preserve syntactic interchangeability, as reflected by distances between word vectors, and the effect of hyper-parameters—context window size in particular. We use part of speech (POS) as a proxy for syntactic interchangeability, as generally speaking, words with the same POS are syntactically valid in the same contexts. We also investigate the relationship between interchangeability and similarity as judged by commonly-used word similarity benchmarks, and correlate the result with the performance of word embedding models on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case.</abstract>
      <url hash="177656ea">W19-2009</url>
      <attachment type="software" hash="6baebbed">W19-2009.Software.zip</attachment>
      <attachment type="poster" hash="97919e42">W19-2009.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-2009</doi>
      <bibkey>hershcovich-etal-2019-syntactic</bibkey>
      <pwccode url="https://github.com/danielhers/interchangeability" additional="false">danielhers/interchangeability</pwccode>
    </paper>
    <paper id="10">
      <title>Evaluation of Morphological Embeddings for <fixed-case>E</fixed-case>nglish and <fixed-case>R</fixed-case>ussian Languages</title>
      <author><first>Vitaly</first><last>Romanov</last></author>
      <author><first>Albina</first><last>Khusainova</last></author>
      <pages>77–81</pages>
      <abstract>This paper evaluates morphology-based embeddings for English and Russian languages. Despite the interest and introduction of several morphology based word embedding models in the past and acclaimed performance improvements on word similarity and language modeling tasks, in our experiments, we did not observe any stable preference over two of our baseline models - SkipGram and FastText. The performance exhibited by morphological embeddings is the average of the two baselines mentioned above.</abstract>
      <url hash="fd0b9cbd">W19-2010</url>
      <doi>10.18653/v1/W19-2010</doi>
      <bibkey>romanov-khusainova-2019-evaluation</bibkey>
    </paper>
    <paper id="11">
      <title>Probing Biomedical Embeddings from Language Models</title>
      <author><first>Qiao</first><last>Jin</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <author><first>Xinghua</first><last>Lu</last></author>
      <pages>82–89</pages>
      <abstract>Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al. 2018), ELMo (Peters et al., 2018), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.</abstract>
      <url hash="f37212a4">W19-2011</url>
      <doi>10.18653/v1/W19-2011</doi>
      <bibkey>jin-etal-2019-probing</bibkey>
      <pwccode url="https://github.com/Andy-jqa/bioelmo" additional="false">Andy-jqa/bioelmo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="12">
      <title>Dyr Bul Shchyl. Proxying Sound Symbolism With Word Embeddings</title>
      <author><first>Ivan P.</first><last>Yamshchikov</last></author>
      <author><first>Viascheslav</first><last>Shibaev</last></author>
      <author><first>Alexey</first><last>Tikhonov</last></author>
      <pages>90–94</pages>
      <abstract>This paper explores modern word embeddings in the context of sound symbolism. Using basic properties of the representations space one can construct semantic axes. A method is proposed to measure if the presence of individual sounds in a given word shifts its semantics of that word along a specific axis. It is shown that, in accordance with several experimental and statistical results, word embeddings capture symbolism for certain sounds.</abstract>
      <url hash="f1bc9ee4">W19-2012</url>
      <doi>10.18653/v1/W19-2012</doi>
      <bibkey>yamshchikov-etal-2019-dyr</bibkey>
    </paper>
    <paper id="13">
      <title>Multi-Context Term Embeddings: the Use Case of Corpus-based Term Set Expansion</title>
      <author><first>Jonathan</first><last>Mamou</last></author>
      <author><first>Oren</first><last>Pereg</last></author>
      <author><first>Moshe</first><last>Wasserblat</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>95–101</pages>
      <abstract>In this paper, we present a novel algorithm that combines multi-context term embeddings using a neural classifier and we test this approach on the use case of corpus-based term set expansion. In addition, we present a novel and unique dataset for intrinsic evaluation of corpus-based term set expansion algorithms. We show that, over this dataset, our algorithm provides up to 5 mean average precision points over the best baseline.</abstract>
      <url hash="30acf3e0">W19-2013</url>
      <doi>10.18653/v1/W19-2013</doi>
      <bibkey>mamou-etal-2019-multi</bibkey>
    </paper>
  </volume>
  <volume id="21">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science</booktitle>
      <url hash="4414553d">W19-21</url>
      <editor><first>Svitlana</first><last>Volkova</last></editor>
      <editor><first>David</first><last>Jurgens</last></editor>
      <editor><first>Dirk</first><last>Hovy</last></editor>
      <editor><first>David</first><last>Bamman</last></editor>
      <editor><first>Oren</first><last>Tsur</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>nlpcss</venue>
    </meta>
    <frontmatter>
      <url hash="370ec610">W19-2100</url>
      <bibkey>ws-2019-natural-language</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Not My President: How Names and Titles Frame Political Figures</title>
      <author><first>Esther</first><last>van den Berg</last></author>
      <author><first>Katharina</first><last>Korfhage</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Michael</first><last>Wiegand</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>1–6</pages>
      <abstract>Naming and titling have been discussed in sociolinguistics as markers of status or solidarity. However, these functions have not been studied on a larger scale or for social media data. We collect a corpus of tweets mentioning presidents of six G20 countries by various naming forms. We show that naming variation relates to stance towards the president in a way that is suggestive of a framing effect mediated by respectfulness. This confirms sociolinguistic theory of naming and titling as markers of status.</abstract>
      <url hash="50d76706">W19-2101</url>
      <doi>10.18653/v1/W19-2101</doi>
      <bibkey>van-den-berg-etal-2019-president</bibkey>
    </paper>
    <paper id="2">
      <title>Identification, Interpretability, and <fixed-case>B</fixed-case>ayesian Word Embeddings</title>
      <author><first>Adam</first><last>Lauretig</last></author>
      <pages>7–17</pages>
      <abstract>Social scientists have recently turned to analyzing text using tools from natural language processing like word embeddings to measure concepts like ideology, bias, and affinity. However, word embeddings are difficult to use in the regression framework familiar to social scientists: embeddings are are neither identified, nor directly interpretable. I offer two advances on standard embedding models to remedy these problems. First, I develop Bayesian Word Embeddings with Automatic Relevance Determination priors, relaxing the assumption that all embedding dimensions have equal weight. Second, I apply work identifying latent variable models to anchor embeddings, identifying them, and making them interpretable and usable in a regression. I then apply this model and anchoring approach to two cases, the shift in internationalist rhetoric in the American presidents’ inaugural addresses, and the relationship between bellicosity in American foreign policy decision-makers’ deliberations. I find that inaugural addresses became less internationalist after 1945, which goes against the conventional wisdom, and that an increase in bellicosity is associated with an increase in hostile actions by the United States, showing that elite deliberations are not cheap talk, and helping confirm the validity of the model.</abstract>
      <url hash="4419fc4a">W19-2102</url>
      <doi>10.18653/v1/W19-2102</doi>
      <bibkey>lauretig-2019-identification</bibkey>
      <pwccode url="https://github.com/adamlauretig/bwe" additional="true">adamlauretig/bwe</pwccode>
    </paper>
    <paper id="3">
      <title>Tweet Classification without the Tweet: An Empirical Examination of User versus Document Attributes</title>
      <author><first>Veronica</first><last>Lynn</last></author>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>18–28</pages>
      <abstract>NLP naturally puts a primary focus on leveraging document language, occasionally considering user attributes as supplemental. However, as we tackle more social scientific tasks, it is possible user attributes might be of primary importance and the document supplemental. Here, we systematically investigate the predictive power of user-level features alone versus document-level features for document-level tasks. We first show user attributes can sometimes carry more task-related information than the document itself. For example, a tweet-level stance detection model using only 13 user-level attributes (i.e. features that did not depend on the specific tweet) was able to obtain a higher F1 than the top-performing SemEval participant. We then consider multiple tasks and a wider range of user attributes, showing the performance of strong document-only models can often be improved (as in stance, sentiment, and sarcasm) with user attributes, particularly benefiting tasks with stable “trait-like” outcomes (e.g. stance) most relative to frequently changing “state-like” outcomes (e.g. sentiment). These results not only support the growing work on integrating user factors into predictive systems, but that some of our NLP tasks might be better cast primarily as user-level (or human) tasks.</abstract>
      <url hash="61f790bf">W19-2103</url>
      <doi>10.18653/v1/W19-2103</doi>
      <bibkey>lynn-etal-2019-tweet</bibkey>
    </paper>
    <paper id="4">
      <title>Geolocating Political Events in Text</title>
      <author><first>Andrew</first><last>Halterman</last></author>
      <pages>29–39</pages>
      <abstract>This work introduces a general method for automatically finding the locations where political events in text occurred. Using a novel set of 8,000 labeled sentences, I create a method to link automatically extracted events and locations in text. The model achieves human level performance on the annotation task and outperforms previous event geolocation systems. It can be applied to most event extraction systems across geographic contexts. I formalize the event–location linking task, describe the neural network model, describe the potential uses of such a system in political science, and demonstrate a workflow to answer an open question on the role of conventional military offensives in causing civilian casualties in the Syrian civil war.</abstract>
      <url hash="21d55bbd">W19-2104</url>
      <doi>10.18653/v1/W19-2104</doi>
      <bibkey>halterman-2019-geolocating</bibkey>
      <pwccode url="https://github.com/ahalterman/event_location" additional="false">ahalterman/event_location</pwccode>
    </paper>
    <paper id="5">
      <title>Neural Network Prediction of Censorable Language</title>
      <author><first>Kei Yin</first><last>Ng</last></author>
      <author><first>Anna</first><last>Feldman</last></author>
      <author><first>Jing</first><last>Peng</last></author>
      <author><first>Chris</first><last>Leberknight</last></author>
      <pages>40–46</pages>
      <abstract>Internet censorship imposes restrictions on what information can be publicized or viewed on the Internet. According to Freedom House’s annual Freedom on the Net report, more than half the world’s Internet users now live in a place where the Internet is censored or restricted. China has built the world’s most extensive and sophisticated online censorship system. In this paper, we describe a new corpus of censored and uncensored social media tweets from a Chinese microblogging website, Sina Weibo, collected by tracking posts that mention ‘sensitive’ topics or authored by ‘sensitive’ users. We use this corpus to build a neural network classifier to predict censorship. Our model performs with a 88.50% accuracy using only linguistic features. We discuss these features in detail and hypothesize that they could potentially be used for censorship circumvention.</abstract>
      <url hash="639279c5">W19-2105</url>
      <doi>10.18653/v1/W19-2105</doi>
      <bibkey>ng-etal-2019-neural</bibkey>
    </paper>
    <paper id="6">
      <title>Modeling performance differences on cognitive tests using <fixed-case>LSTM</fixed-case>s and skip-thought vectors trained on reported media consumption.</title>
      <author><first>Maury</first><last>Courtland</last></author>
      <author><first>Aida</first><last>Davani</last></author>
      <author><first>Melissa</first><last>Reyes</last></author>
      <author><first>Leigh</first><last>Yeh</last></author>
      <author><first>Jun</first><last>Leung</last></author>
      <author><first>Brendan</first><last>Kennedy</last></author>
      <author><first>Morteza</first><last>Dehghani</last></author>
      <author><first>Jason</first><last>Zevin</last></author>
      <pages>47–53</pages>
      <abstract>Cognitive tests have traditionally resorted to standardizing testing materials in the name of equality and because of the onerous nature of creating test items. This approach ignores participants’ diverse language experiences that potentially significantly affect testing outcomes. Here, we seek to explain our prior finding of significant performance differences on two cognitive tests (reading span and SPiN) between clusters of participants based on their media consumption. Here, we model the language contained in these media sources using an LSTM trained on corpora of each cluster’s media sources to predict target words. We also model semantic similarity of test items with each cluster’s corpus using skip-thought vectors. We find robust, significant correlations between performance on the SPiN test and the LSTMs and skip-thought models we present here, but not the reading span test.</abstract>
      <url hash="d736e8fd">W19-2106</url>
      <doi>10.18653/v1/W19-2106</doi>
      <bibkey>courtland-etal-2019-modeling</bibkey>
    </paper>
    <paper id="7">
      <title>Using time series and natural language processing to identify viral moments in the 2016 <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. Presidential Debate</title>
      <author><first>Josephine</first><last>Lukito</last></author>
      <author><first>Prathusha</first><last>K Sarma</last></author>
      <author><first>Jordan</first><last>Foley</last></author>
      <author><first>Aman</first><last>Abhishek</last></author>
      <pages>54–64</pages>
      <abstract>This paper proposes a method for identifying and studying viral moments or highlights during a political debate. Using a combined strategy of time series analysis and domain adapted word embeddings, this study provides an in-depth analysis of several key moments during the 2016 U.S. Presidential election. First, a time series outlier analysis is used to identify key moments during the debate. These moments had to result in a long-term shift in attention towards either Hillary Clinton or Donald Trump (i.e., a transient change outlier or an intervention, resulting in a permanent change in the time series). To assess whether these moments also resulted in a discursive shift, two corpora are produced for each potential viral moment (a pre-viral corpus and post-viral corpus). A domain adaptation layer learns weights to combine a generic and domain-specific (DS) word embedding into a domain adapted (DA) embedding. Words are then classified using a generic encoder+ classifier framework that relies on these word embeddings as inputs. Results suggest that both Clinton and Trump were able to induce discourse-shifting viral moments, though the former is much better at producing a topically-specific discursive shift.</abstract>
      <url hash="88b33b9f">W19-2107</url>
      <doi>10.18653/v1/W19-2107</doi>
      <bibkey>lukito-etal-2019-using</bibkey>
    </paper>
    <paper id="8">
      <title>Stance Classification, Outcome Prediction, and Impact Assessment: <fixed-case>NLP</fixed-case> Tasks for Studying Group Decision-Making</title>
      <author><first>Elijah</first><last>Mayfield</last></author>
      <author><first>Alan</first><last>Black</last></author>
      <pages>65–77</pages>
      <abstract>In group decision-making, the nuanced process of conflict and resolution that leads to consensus formation is closely tied to the quality of decisions made. Behavioral scientists rarely have rich access to process variables, though, as unstructured discussion transcripts are difficult to analyze. Here, we define ways for NLP researchers to contribute to the study of groups and teams. We introduce three tasks alongside a large new corpus of over 400,000 group debates on Wikipedia. We describe the tasks and their importance, then provide baselines showing that BERT contextualized word embeddings consistently outperform other language representations.</abstract>
      <url hash="9d163252">W19-2108</url>
      <doi>10.18653/v1/W19-2108</doi>
      <bibkey>mayfield-black-2019-stance</bibkey>
    </paper>
    <paper id="9">
      <title>A Sociolinguistic Study of Online Echo Chambers on <fixed-case>T</fixed-case>witter</title>
      <author><first>Nikita</first><last>Duseja</last></author>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <pages>78–83</pages>
      <abstract>Online social media platforms such as Facebook and Twitter are increasingly facing criticism for polarization of users. One particular aspect which has caught the attention of various critics is presence of users in echo chambers - a situation wherein users are exposed mostly to the opinions which are in sync with their own views. In this paper, we perform a sociolinguistic study by comparing the tweets of users in echo chambers with the tweets of users not in echo chambers with similar levels of polarity on a broad topic. Specifically, we carry out a comparative analysis of tweet structure, lexical choices, and focus issues, and provide possible explanations for the results.</abstract>
      <url hash="a0152e10">W19-2109</url>
      <doi>10.18653/v1/W19-2109</doi>
      <bibkey>duseja-jhamtani-2019-sociolinguistic</bibkey>
    </paper>
    <paper id="10">
      <title>Uphill from here: Sentiment patterns in videos from left- and right-wing <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube news channels</title>
      <author><first>Felix</first><last>Soldner</last></author>
      <author><first>Justin Chun-ting</first><last>Ho</last></author>
      <author><first>Mykola</first><last>Makhortykh</last></author>
      <author><first>Isabelle W.J.</first><last>van der Vegt</last></author>
      <author><first>Maximilian</first><last>Mozes</last></author>
      <author><first>Bennett</first><last>Kleinberg</last></author>
      <pages>84–93</pages>
      <abstract>News consumption exhibits an increasing shift towards online sources, which bring platforms such as YouTube more into focus. Thus, the distribution of politically loaded news is easier, receives more attention, but also raises the concern of forming isolated ideological communities. Understanding how such news is communicated and received is becoming increasingly important. To expand our understanding in this domain, we apply a linguistic temporal trajectory analysis to analyze sentiment patterns in English-language videos from news channels on YouTube. We examine transcripts from videos distributed through eight channels with pro-left and pro-right political leanings. Using unsupervised clustering, we identify seven different sentiment patterns in the transcripts. We found that the use of two sentiment patterns differed significantly depending on political leaning. Furthermore, we used predictive models to examine how different sentiment patterns relate to video popularity and if they differ depending on the channel’s political leaning. No clear relations between sentiment patterns and popularity were found. However, results indicate, that videos from pro-right news channels are more popular and that a negative sentiment further increases that popularity, when sentiments are averaged for each video.</abstract>
      <url hash="eaaf82d4">W19-2110</url>
      <doi>10.18653/v1/W19-2110</doi>
      <bibkey>soldner-etal-2019-uphill</bibkey>
    </paper>
    <paper id="11">
      <title>Simple dynamic word embeddings for mapping perceptions in the public sphere</title>
      <author><first>Nabeel</first><last>Gillani</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>94–99</pages>
      <abstract>Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.</abstract>
      <url hash="ce2b5028">W19-2111</url>
      <doi>10.18653/v1/W19-2111</doi>
      <revision id="1" href="W19-2111v1" hash="2444524a"/>
      <revision id="2" href="W19-2111v2" hash="ce2b5028" date="2020-11-05">Corrects errors found in the original version of the paper; a new corrigendum at the end summarizes the original errors and how we corrected them.</revision>
      <bibkey>gillani-levy-2019-simple</bibkey>
    </paper>
    <paper id="12">
      <title>Modeling Behavioral Aspects of Social Media Discourse for Moral Classification</title>
      <author><first>Kristen</first><last>Johnson</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>100–109</pages>
      <abstract>Political discourse on social media microblogs, specifically Twitter, has become an undeniable part of mainstream U.S. politics. Given the length constraint of tweets, politicians must carefully word their statements to ensure their message is understood by their intended audience. This constraint often eliminates the context of the tweet, making automatic analysis of social media political discourse a difficult task. To overcome this challenge, we propose simultaneous modeling of high-level abstractions of political language, such as political slogans and framing strategies, with abstractions of how politicians behave on Twitter. These behavioral abstractions can be further leveraged as forms of supervision in order to increase prediction accuracy, while reducing the burden of annotation. In this work, we use Probabilistic Soft Logic (PSL) to build relational models to capture the similarities in language and behavior that obfuscate political messages on Twitter. When combined, these descriptors reveal the moral foundations underlying the discourse of U.S. politicians online, <i>across</i> differing governing administrations, showing how party talking points remain cohesive or change over time.</abstract>
      <url hash="a571ec6c">W19-2112</url>
      <doi>10.18653/v1/W19-2112</doi>
      <bibkey>johnson-goldwasser-2019-modeling</bibkey>
    </paper>
  </volume>
  <volume id="22">
    <meta>
      <booktitle>Proceedings of the Natural Legal Language Processing Workshop 2019</booktitle>
      <url hash="e36472a6">W19-22</url>
      <editor><first>Nikolaos</first><last>Aletras</last></editor>
      <editor><first>Elliott</first><last>Ash</last></editor>
      <editor><first>Leslie</first><last>Barrett</last></editor>
      <editor><first>Daniel</first><last>Chen</last></editor>
      <editor><first>Adam</first><last>Meyers</last></editor>
      <editor><first>Daniel</first><last>Preotiuc-Pietro</last></editor>
      <editor><first>David</first><last>Rosenberg</last></editor>
      <editor><first>Amanda</first><last>Stent</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="f82fd7d4">W19-2200</url>
      <bibkey>ws-2019-natural-legal</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Plain <fixed-case>E</fixed-case>nglish Summarization of Contracts</title>
      <author><first>Laura</first><last>Manor</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <pages>1–11</pages>
      <abstract>Unilateral legal contracts, such as terms of service, play a substantial role in modern digital life. However, few read these documents before accepting the terms within, as they are too long and the language too complicated. We propose the task of summarizing such legal documents in plain English, which would enable users to have a better understanding of the terms they are accepting. We propose an initial dataset of legal text snippets paired with summaries written in plain English. We verify the quality of these summaries manually, and show that they involve heavy abstraction, compression, and simplification. Initial experiments show that unsupervised extractive summarization methods do not perform well on this task due to the level of abstraction and style differences. We conclude with a call for resource and technique development for simplification and style transfer for legal language.</abstract>
      <url hash="f201bd61">W19-2201</url>
      <attachment type="supplementary" hash="db07d93e">W19-2201.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-2201</doi>
      <bibkey>manor-li-2019-plain</bibkey>
      <pwccode url="https://github.com/lauramanor/legal_summarization" additional="false">lauramanor/legal_summarization</pwccode>
    </paper>
    <paper id="2">
      <title>Scalable Methods for Annotating Legal-Decision Corpora</title>
      <author><first>Lisa</first><last>Ferro</last></author>
      <author><first>John</first><last>Aberdeen</last></author>
      <author><first>Karl</first><last>Branting</last></author>
      <author><first>Craig</first><last>Pfeifer</last></author>
      <author><first>Alexander</first><last>Yeh</last></author>
      <author><first>Amartya</first><last>Chakraborty</last></author>
      <pages>12–20</pages>
      <abstract>Recent research has demonstrated that judicial and administrative decisions can be predicted by machine-learning models trained on prior decisions. However, to have any practical application, these predictions must be explainable, which in turn requires modeling a rich set of features. Such approaches face a roadblock if the knowledge engineering required to create these features is not scalable. We present an approach to developing a feature-rich corpus of administrative rulings about domain name disputes, an approach which leverages a small amount of manual annotation and prototypical patterns present in the case documents to automatically extend feature labels to the entire corpus. To demonstrate the feasibility of this approach, we report results from systems trained on this dataset.</abstract>
      <url hash="35d67316">W19-2202</url>
      <doi>10.18653/v1/W19-2202</doi>
      <bibkey>ferro-etal-2019-scalable</bibkey>
    </paper>
    <paper id="3">
      <title>The Extent of Repetition in Contract Language</title>
      <author><first>Dan</first><last>Simonson</last></author>
      <author><first>Daniel</first><last>Broderick</last></author>
      <author><first>Jonathan</first><last>Herr</last></author>
      <pages>21–30</pages>
      <abstract>Contract language is repetitive (Anderson and Manns, 2017), but so is all language (Zipf, 1949). In this paper, we measure the extent to which contract language in English is repetitive compared with the language of other English language corpora. Contracts have much smaller vocabulary sizes compared with similarly sized non-contract corpora across multiple contract types, contain 1/5th as many hapax legomena, pattern differently on a log-log plot, use fewer pronouns, and contain sentences that are about 20% more similar to one another than in other corpora. These suggest that the study of contracts in natural language processing controls for some linguistic phenomena and allows for more in depth study of others.</abstract>
      <url hash="900c840a">W19-2203</url>
      <doi>10.18653/v1/W19-2203</doi>
      <bibkey>simonson-etal-2019-extent</bibkey>
    </paper>
    <paper id="4">
      <title>Sentence Boundary Detection in Legal Text</title>
      <author><first>George</first><last>Sanchez</last></author>
      <pages>31–38</pages>
      <abstract>In this paper, we examined several algorithms to detect sentence boundaries in legal text. Legal text presents challenges for sentence tokenizers because of the variety of punctuations and syntax of legal text. Out-of-the-box algorithms perform poorly on legal text affecting further analysis of the text. A novel and domain-specific approach is needed to detect sentence boundaries to further analyze legal text. We present the results of our investigation in this paper.</abstract>
      <url hash="1dad1ac6">W19-2204</url>
      <doi>10.18653/v1/W19-2204</doi>
      <bibkey>sanchez-2019-sentence</bibkey>
    </paper>
    <paper id="5">
      <title>Legal Linking: Citation Resolution and Suggestion in Constitutional Law</title>
      <author><first>Robert</first><last>Shaffer</last></author>
      <author><first>Stephen</first><last>Mayhew</last></author>
      <pages>39–44</pages>
      <abstract>This paper describes a dataset and baseline systems for linking paragraphs from court cases to clauses or amendments in the US Constitution. We implement a rule-based system, a linear model, and a neural architecture for matching pairs of paragraphs, taking training data from online databases in a distantly-supervised fashion. In experiments on a manually-annotated evaluation set, we find that our proposed neural system outperforms a rules-driven baseline. Qualitatively, this performance gap seems largest for abstract or indirect links between documents, which suggests that our system might be useful for answering political science and legal research questions or discovering novel links. We release the dataset along with the manually-annotated evaluation set to foster future work.</abstract>
      <url hash="9c40bdff">W19-2205</url>
      <doi>10.18653/v1/W19-2205</doi>
      <bibkey>shaffer-mayhew-2019-legal</bibkey>
      <pwccode url="https://github.com/mayhewsw/legal-linking" additional="false">mayhewsw/legal-linking</pwccode>
    </paper>
    <paper id="6">
      <title>Litigation Analytics: Case Outcomes Extracted from <fixed-case>US</fixed-case> Federal Court Dockets</title>
      <author><first>Thomas</first><last>Vacek</last></author>
      <author><first>Ronald</first><last>Teo</last></author>
      <author><first>Dezhao</first><last>Song</last></author>
      <author><first>Timothy</first><last>Nugent</last></author>
      <author><first>Conner</first><last>Cowling</last></author>
      <author><first>Frank</first><last>Schilder</last></author>
      <pages>45–54</pages>
      <abstract>Dockets contain a wealth of information for planning a litigation strategy, but the information is locked up in semi-structured text. Manually deriving the outcomes for each party (e.g., settlement, verdict) would be very labor intensive. Having such information available for every past court case, however, would be very useful for developing a strategy because it potentially reveals tendencies and trends of judges and courts and the opposing counsel. We used Natural Language Processing (NLP) techniques and deep learning methods allowing us to scale the automatic analysis of millions of US federal court dockets. The automatically extracted information is fed into a Litigation Analytics tool that is used by lawyers to plan how they approach concrete litigations.</abstract>
      <url hash="53670659">W19-2206</url>
      <doi>10.18653/v1/W19-2206</doi>
      <bibkey>vacek-etal-2019-litigation-analytics</bibkey>
    </paper>
    <paper id="7">
      <title>Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services</title>
      <author><first>Georg</first><last>Rehm</last></author>
      <author><first>Julián</first><last>Moreno-Schneider</last></author>
      <author><first>Jorge</first><last>Gracia</last></author>
      <author><first>Artem</first><last>Revenko</last></author>
      <author><first>Victor</first><last>Mireles</last></author>
      <author><first>Maria</first><last>Khvalchik</last></author>
      <author><first>Ilan</first><last>Kernerman</last></author>
      <author><first>Andis</first><last>Lagzdins</last></author>
      <author><first>Marcis</first><last>Pinnis</last></author>
      <author><first>Artus</first><last>Vasilevskis</last></author>
      <author><first>Elena</first><last>Leitner</last></author>
      <author><first>Jan</first><last>Milde</last></author>
      <author><first>Pia</first><last>Weißenhorn</last></author>
      <pages>55–66</pages>
      <abstract>We present a portfolio of natural legal language processing and document curation services currently under development in a collaborative European project. First, we give an overview of the project and the different use cases, while, in the main part of the article, we focus upon the 13 different processing services that are being deployed in different prototype applications using a flexible and scalable microservices architecture. Their orchestration is operationalised using a content and document curation workflow manager.</abstract>
      <url hash="170e433a">W19-2207</url>
      <doi>10.18653/v1/W19-2207</doi>
      <bibkey>rehm-etal-2019-developing</bibkey>
    </paper>
    <paper id="8">
      <title>Legal Area Classification: A Comparative Study of Text Classifiers on <fixed-case>S</fixed-case>ingapore <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Judgments</title>
      <author><first>Jerrold</first><last>Soh</last></author>
      <author><first>How Khang</first><last>Lim</last></author>
      <author><first>Ian Ernst</first><last>Chai</last></author>
      <pages>67–77</pages>
      <abstract>This paper conducts a comparative study on the performance of various machine learning approaches for classifying judgments into legal areas. Using a novel dataset of 6,227 Singapore Supreme Court judgments, we investigate how state-of-the-art NLP methods compare against traditional statistical models when applied to a legal corpus that comprised few but lengthy documents. All approaches tested, including topic model, word embedding, and language model-based classifiers, performed well with as little as a few hundred judgments. However, more work needs to be done to optimize state-of-the-art methods for the legal domain.</abstract>
      <url hash="ae4e458a">W19-2208</url>
      <doi>10.18653/v1/W19-2208</doi>
      <bibkey>soh-etal-2019-legal</bibkey>
    </paper>
    <paper id="9">
      <title>Extreme Multi-Label Legal Text Classification: A Case Study in <fixed-case>EU</fixed-case> Legislation</title>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Emmanouil</first><last>Fergadiotis</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <pages>78–87</pages>
      <abstract>We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EURLEX, the European Union’s public document database, annotated with concepts from EUROVOC, a multidisciplinary thesaurus. The dataset is substantially larger than previous EURLEX datasets and suitable for XMTC, few-shot and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with self-attention outperform the current multi-label state-of-the-art methods, which employ label-wise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance.</abstract>
      <url hash="c6740bc5">W19-2209</url>
      <doi>10.18653/v1/W19-2209</doi>
      <bibkey>chalkidis-etal-2019-extreme</bibkey>
    </paper>
  </volume>
  <volume id="23">
    <meta>
      <booktitle>Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</booktitle>
      <url hash="54dfa10f">W19-23</url>
      <editor><first>Antoine</first><last>Bosselut</last></editor>
      <editor><first>Asli</first><last>Celikyilmaz</last></editor>
      <editor><first>Marjan</first><last>Ghazvininejad</last></editor>
      <editor><first>Srinivasan</first><last>Iyer</last></editor>
      <editor><first>Urvashi</first><last>Khandelwal</last></editor>
      <editor><first>Hannah</first><last>Rashkin</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="1742f3f5">W19-2300</url>
      <bibkey>ws-2019-methods</bibkey>
    </frontmatter>
    <paper id="1">
      <title>An Adversarial Learning Framework For A Persona-Based Multi-Turn Dialogue Model</title>
      <author><first>Oluwatobi</first><last>Olabiyi</last></author>
      <author><first>Anish</first><last>Khazane</last></author>
      <author><first>Alan</first><last>Salimov</last></author>
      <author><first>Erik</first><last>Mueller</last></author>
      <pages>1–10</pages>
      <abstract>In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGANa, a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGANd, a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona Seq2Seq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).</abstract>
      <url hash="37633422">W19-2301</url>
      <doi>10.18653/v1/W19-2301</doi>
      <bibkey>olabiyi-etal-2019-adversarial</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>DAL</fixed-case>: Dual Adversarial Learning for Dialogue Generation</title>
      <author><first>Shaobo</first><last>Cui</last></author>
      <author><first>Rongzhong</first><last>Lian</last></author>
      <author><first>Di</first><last>Jiang</last></author>
      <author><first>Yuanfeng</first><last>Song</last></author>
      <author><first>Siqi</first><last>Bao</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <pages>11–20</pages>
      <abstract>In open-domain dialogue systems, generative approaches have attracted much attention for response generation. However, existing methods are heavily plagued by generating safe responses and unnatural responses. To alleviate these two problems, we propose a novel framework named Dual Adversarial Learning(DAL) for high-quality response generation. DAL innovatively utilizes the duality between query generation and response generation to avoid safe responses and increase the diversity of the generated responses. Additionally, DAL uses adversarial learning to mimic human judges and guides the system to generate natural responses. Experimental results demonstrate that DAL effectively improves both diversity and overall quality of the generated responses. DAL outperforms state-of-the-art methods regarding automatic metrics and human evaluations.</abstract>
      <url hash="4ab3829d">W19-2302</url>
      <doi>10.18653/v1/W19-2302</doi>
      <bibkey>cui-etal-2019-dal</bibkey>
    </paper>
    <paper id="3">
      <title>How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <pages>21–29</pages>
      <abstract>We show that plain ROUGE F1 scores are not ideal for comparing current neural systems which on average produce different lengths. This is due to a non-linear pattern between ROUGE F1 and summary length. To alleviate the effect of length during evaluation, we have proposed a new method which normalizes the ROUGE F1 scores of a system by that of a random system with same average output length. A pilot human evaluation has shown that humans prefer short summaries in terms of the verbosity of a summary but overall consider longer summaries to be of higher quality. While human evaluations are more expensive in time and resources, it is clear that normalization, such as the one we proposed for automatic evaluation, will make human evaluations more meaningful.</abstract>
      <url hash="b75feae4">W19-2303</url>
      <doi>10.18653/v1/W19-2303</doi>
      <bibkey>sun-etal-2019-compare</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>BERT</fixed-case> has a Mouth, and It Must Speak: <fixed-case>BERT</fixed-case> as a <fixed-case>M</fixed-case>arkov Random Field Language Model</title>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>30–36</pages>
      <abstract>We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.</abstract>
      <url hash="bbc801a2">W19-2304</url>
      <doi>10.18653/v1/W19-2304</doi>
      <bibkey>wang-cho-2019-bert</bibkey>
      <pwccode url="https://github.com/nyu-dl/bert-gen" additional="true">nyu-dl/bert-gen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="5">
      <title>Neural Text Simplification in Low-Resource Conditions Using Weak Supervision</title>
      <author><first>Alessio</first><last>Palmero Aprosio</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Mattia A.</first><last>Di Gangi</last></author>
      <pages>37–44</pages>
      <abstract>Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-to-sequence learning. Most recent efforts with such a data-demanding paradigm have dealt with the English language, for which sizeable training datasets are currently available to deploy competitive models. Similar improvements on less resource-rich languages are conditioned either to intensive manual work to create training data, or to the design of effective automatic generation techniques to bypass the data acquisition bottleneck. Inspired by the machine translation field, in which synthetic parallel pairs generated from monolingual data yield significant improvements to neural models, in this paper we exploit large amounts of heterogeneous data to automatically select simple sentences, which are then used to create synthetic simplification pairs. We also evaluate other solutions, such as oversampling and the use of external word embeddings to be fed to the neural simplification system. Our approach is evaluated on Italian and Spanish, for which few thousand gold sentence pairs are available. The results show that these techniques yield performance improvements over a baseline sequence-to-sequence configuration.</abstract>
      <url hash="22f5a6d1">W19-2305</url>
      <doi>10.18653/v1/W19-2305</doi>
      <bibkey>palmero-aprosio-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="6">
      <title>Paraphrase Generation for Semi-Supervised Learning in <fixed-case>NLU</fixed-case></title>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>He</first><last>Xie</last></author>
      <author><first>William M.</first><last>Campbell</last></author>
      <pages>45–54</pages>
      <abstract>Semi-supervised learning is an efficient way to improve performance for natural language processing systems. In this work, we propose Para-SSL, a scheme to generate candidate utterances using paraphrasing and methods from semi-supervised learning. In order to perform paraphrase generation in the context of a dialog system, we automatically extract paraphrase pairs to create a paraphrase corpus. Using this data, we build a paraphrase generation system and perform one-to-many generation, followed by a validation step to select only the utterances with good quality. The paraphrase-based semi-supervised learning is applied to five functionalities in a natural language understanding system. Our proposed method for semi-supervised learning using paraphrase generation does not require user utterances and can be applied prior to releasing a new functionality to a system. Experiments show that we can achieve up to 19% of relative slot error reduction without an access to user utterances, and up to 35% when leveraging live traffic utterances.</abstract>
      <url hash="fedd13d8">W19-2306</url>
      <doi>10.18653/v1/W19-2306</doi>
      <bibkey>cho-etal-2019-paraphrase</bibkey>
    </paper>
    <paper id="7">
      <title>Bilingual-<fixed-case>GAN</fixed-case>: A Step Towards Parallel Text Generation</title>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Alan</first><last>Do-Omri</last></author>
      <author><first>Md. Akmal</first><last>Haidar</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>55–64</pages>
      <abstract>Latent space based GAN methods and attention based sequence to sequence models have achieved impressive results in text generation and unsupervised machine translation respectively. Leveraging the two domains, we propose an adversarial latent space based model capable of generating parallel sentences in two languages concurrently and translating bidirectionally. The bilingual generation goal is achieved by sampling from the latent space that is shared between both languages. First two denoising autoencoders are trained, with shared encoders and back-translation to enforce a shared latent state between the two languages. The decoder is shared for the two translation directions. Next, a GAN is trained to generate synthetic ‘code’ mimicking the languages’ shared latent space. This code is then fed into the decoder to generate text in either language. We perform our experiments on Europarl and Multi30k datasets, on the English-French language pair, and document our performance using both supervised and unsupervised machine translation.</abstract>
      <url hash="0c93980b">W19-2307</url>
      <doi>10.18653/v1/W19-2307</doi>
      <bibkey>rashid-etal-2019-bilingual</bibkey>
    </paper>
    <paper id="8">
      <title>Designing a Symbolic Intermediate Representation for Neural Surface Realization</title>
      <author><first>Henry</first><last>Elder</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <author><first>James</first><last>Barry</last></author>
      <author><first>Alexander</first><last>O’Connor</last></author>
      <pages>65–73</pages>
      <abstract>Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by breaking out the surface realization step from typically end-to-end neural systems, we also provide a framework for non-neural based content selection and planning systems to potentially take advantage of semi-supervised pretraining of neural surface realization models.</abstract>
      <url hash="214cfc07">W19-2308</url>
      <doi>10.18653/v1/W19-2308</doi>
      <bibkey>elder-etal-2019-designing</bibkey>
    </paper>
    <paper id="9">
      <title>Neural Text Style Transfer via Denoising and Reranking</title>
      <author><first>Joseph</first><last>Lee</last></author>
      <author><first>Ziang</first><last>Xie</last></author>
      <author><first>Cindy</first><last>Wang</last></author>
      <author><first>Max</first><last>Drach</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <author><first>Andrew</first><last>Ng</last></author>
      <pages>74–81</pages>
      <abstract>We introduce a simple method for text style transfer that frames style transfer as denoising: we synthesize a noisy corpus and treat the source style as a noisy version of the target style. To control for aspects such as preserving meaning while modifying style, we propose a reranking approach in the data synthesis phase. We evaluate our method on three novel style transfer tasks: transferring between British and American varieties, text genres (formal vs. casual), and lyrics from different musical genres. By measuring style transfer quality, meaning preservation, and the fluency of generated outputs, we demonstrate that our method is able both to produce high-quality output while maintaining the flexibility to suggest syntactically rich stylistic edits.</abstract>
      <url hash="43482cf2">W19-2309</url>
      <doi>10.18653/v1/W19-2309</doi>
      <bibkey>lee-etal-2019-neural</bibkey>
    </paper>
    <paper id="10">
      <title>Better Automatic Evaluation of Open-Domain Dialogue Systems with Contextualized Embeddings</title>
      <author><first>Sarik</first><last>Ghazarian</last></author>
      <author><first>Johnny</first><last>Wei</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>82–89</pages>
      <abstract>Despite advances in open-domain dialogue systems, automatic evaluation of such systems is still a challenging problem. Traditional reference-based metrics such as BLEU are ineffective because there could be many valid responses for a given context that share no common words with reference responses. A recent work proposed Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) to combine a learning-based metric, which predicts relatedness between a generated response and a given query, with reference-based metric; it showed high correlation with human judgments. In this paper, we explore using contextualized word embeddings to compute more accurate relatedness scores, thus better evaluation metrics. Experiments show that our evaluation metrics outperform RUBER, which is trained on static embeddings.</abstract>
      <url hash="9d52cab8">W19-2310</url>
      <doi>10.18653/v1/W19-2310</doi>
      <bibkey>ghazarian-etal-2019-better</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="11">
      <title>Jointly Measuring Diversity and Quality in Text Generation Models</title>
      <author><first>Danial</first><last>Alihosseini</last></author>
      <author><first>Ehsan</first><last>Montahaei</last></author>
      <author><first>Mahdieh</first><last>Soleymani Baghshah</last></author>
      <pages>90–98</pages>
      <abstract>Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglecting their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generatorʼs density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.</abstract>
      <url hash="2950979b">W19-2311</url>
      <doi>10.18653/v1/W19-2311</doi>
      <bibkey>alihosseini-etal-2019-jointly</bibkey>
      <pwccode url="https://github.com/IAmS4n/TextGenerationEvaluationMetrics" additional="true">IAmS4n/TextGenerationEvaluationMetrics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
  </volume>
  <volume id="24">
    <meta>
      <booktitle>Proceedings of the First Workshop on Narrative Understanding</booktitle>
      <url hash="6b0e93a0">W19-24</url>
      <editor><first>David</first><last>Bamman</last></editor>
      <editor><first>Snigdha</first><last>Chaturvedi</last></editor>
      <editor><first>Elizabeth</first><last>Clark</last></editor>
      <editor><first>Madalina</first><last>Fiterau</last></editor>
      <editor><first>Mohit</first><last>Iyyer</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="186a8edd">W19-2400</url>
      <bibkey>ws-2019-narrative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards Coherent and Cohesive Long-form Text Generation</title>
      <author><first>Woon Sang</first><last>Cho</last></author>
      <author><first>Pengchuan</first><last>Zhang</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Xiujun</first><last>Li</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Mengdi</first><last>Wang</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>1–11</pages>
      <abstract>Generating coherent and cohesive long-form texts is a challenging task. Previous works relied on large amounts of human-generated texts to train neural language models. However, few attempted to explicitly improve neural language models from the perspectives of coherence and cohesion. In this work, we propose a new neural language model that is equipped with two neural discriminators which provide feedback signals at the levels of sentence (cohesion) and paragraph (coherence). Our model is trained using a simple yet efficient variant of policy gradient, called ‘negative-critical sequence training’, which is proposed to eliminate the need of training a separate critic for estimating ‘baseline’. Results demonstrate the effectiveness of our approach, showing improvements over the strong baseline – recurrent attention-based bidirectional MLE-trained neural language model.</abstract>
      <url hash="a84d649e">W19-2401</url>
      <attachment type="supplementary" hash="c2643ccb">W19-2401.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-2401</doi>
      <bibkey>cho-etal-2019-towards</bibkey>
    </paper>
    <paper id="2">
      <title>Character Identification Refined: A Proposal</title>
      <author><first>Labiba</first><last>Jahan</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>12–18</pages>
      <abstract>Characters are a key element of narrative and so character identification plays an important role in automatic narrative understanding. Unfortunately, most prior work that incorporates character identification is not built upon a clear, theoretically grounded concept of character. They either take character identification for granted (e.g., using simple heuristics on referring expressions), or rely on simplified definitions that do not capture important distinctions between characters and other referents in the story. Prior approaches have also been rather complicated, relying, for example, on predefined case bases or ontologies. In this paper we propose a narratologically grounded definition of character for discussion at the workshop, and also demonstrate a preliminary yet straightforward supervised machine learning model with a small set of features that performs well on two corpora. The most important of the two corpora is a set of 46 Russian folktales, on which the model achieves an F1 of 0.81. Error analysis suggests that features relevant to the plot will be necessary for further improvements in performance.</abstract>
      <url hash="79390108">W19-2402</url>
      <doi>10.18653/v1/W19-2402</doi>
      <bibkey>jahan-finlayson-2019-character</bibkey>
    </paper>
    <paper id="3">
      <title>Deep Natural Language Understanding of News Text</title>
      <author><first>Jaya</first><last>Shree</last></author>
      <author><first>Emily</first><last>Liu</last></author>
      <author><first>Andrew</first><last>Gordon</last></author>
      <author><first>Jerry</first><last>Hobbs</last></author>
      <pages>19–27</pages>
      <abstract>Early proposals for the deep understanding of natural language text advocated an approach of “interpretation as abduction,” where the meaning of a text was derived as an explanation that logically entailed the input words, given a knowledge base of lexical and commonsense axioms. While most subsequent NLP research has instead pursued statistical and data-driven methods, the approach of interpretation as abduction has seen steady advancements in both theory and software implementations. In this paper, we summarize advances in deriving the logical form of the text, encoding commonsense knowledge, and technologies for scalable abductive reasoning. We then explore the application of these advancements to the deep understanding of a paragraph of news text, where the subtle meaning of words and phrases are resolved by backward chaining on a knowledge base of 80 hand-authored axioms.</abstract>
      <url hash="22849ca7">W19-2403</url>
      <doi>10.18653/v1/W19-2403</doi>
      <bibkey>shree-etal-2019-deep</bibkey>
    </paper>
    <paper id="4">
      <title>Extraction of Message Sequence Charts from Narrative History Text</title>
      <author><first>Girish</first><last>Palshikar</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Sangameshwar</first><last>Patil</last></author>
      <author><first>Swapnil</first><last>Hingmire</last></author>
      <author><first>Nitin</first><last>Ramrakhiyani</last></author>
      <author><first>Harsimran</first><last>Bedi</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>28–36</pages>
      <abstract>In this paper, we advocate the use of Message Sequence Chart (MSC) as a knowledge representation to capture and visualize multi-actor interactions and their temporal ordering. We propose algorithms to automatically extract an MSC from a history narrative. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extraction, we employ a state-of-the art algorithm to temporally re-order these interactions. Our evaluation on multiple publicly available narratives shows improvements over four baselines.</abstract>
      <url hash="42cbaffc">W19-2404</url>
      <doi>10.18653/v1/W19-2404</doi>
      <bibkey>palshikar-etal-2019-extraction-message</bibkey>
    </paper>
    <paper id="5">
      <title>Unsupervised Hierarchical Story Infilling</title>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Douglas</first><last>Eck</last></author>
      <pages>37–43</pages>
      <abstract>Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.</abstract>
      <url hash="a75c9f3f">W19-2405</url>
      <doi>10.18653/v1/W19-2405</doi>
      <bibkey>ippolito-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="6">
      <title>Identifying Sensible Lexical Relations in Generated Stories</title>
      <author><first>Melissa</first><last>Roemmele</last></author>
      <pages>44–52</pages>
      <abstract>As with many text generation tasks, the focus of recent progress on story generation has been in producing texts that are perceived to “make sense” as a whole. There are few automated metrics that address this dimension of story quality even on a shallow lexical level. To initiate investigation into such metrics, we apply a simple approach to identifying word relations that contribute to the ‘narrative sense’ of a story. We use this approach to comparatively analyze the output of a few notable story generation systems in terms of these relations. We characterize differences in the distributions of relations according to their strength within each story.</abstract>
      <url hash="798b0715">W19-2406</url>
      <doi>10.18653/v1/W19-2406</doi>
      <bibkey>roemmele-2019-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
  </volume>
  <volume id="25">
    <meta>
      <booktitle>Proceedings of the 3rd Joint <fixed-case>SIGHUM</fixed-case> Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
      <url hash="cf6597f7">W19-25</url>
      <editor><first>Beatrice</first><last>Alex</last></editor>
      <editor><first>Stefania</first><last>Degaetano-Ortlieb</last></editor>
      <editor><first>Anna</first><last>Kazantseva</last></editor>
      <editor><first>Nils</first><last>Reiter</last></editor>
      <editor><first>Stan</first><last>Szpakowicz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, USA</address>
      <month>June</month>
      <year>2019</year>
      <venue>latech</venue>
    </meta>
    <frontmatter>
      <url hash="ea7b9f68">W19-2500</url>
      <bibkey>ws-2019-joint</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Modeling Word Emotion in Historical Language: Quantity Beats Supposed Stability in Seed Word Selection</title>
      <author><first>Johannes</first><last>Hellrich</last></author>
      <author><first>Sven</first><last>Buechel</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>1–11</pages>
      <abstract>To understand historical texts, we must be aware that language—including the emotional connotation attached to words—changes over time. In this paper, we aim at estimating the emotion which is associated with a given word in former language stages of English and German. Emotion is represented following the popular Valence-Arousal-Dominance (VAD) annotation scheme. While being more expressive than polarity alone, existing word emotion induction methods are typically not suited for addressing it. To overcome this limitation, we present adaptations of two popular algorithms to VAD. To measure their effectiveness in diachronic settings, we present the first gold standard for historical word emotions, which was created by scholars with proficiency in the respective language stages and covers both English and German. In contrast to claims in previous work, our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful.</abstract>
      <url hash="3606ab63">W19-2501</url>
      <doi>10.18653/v1/W19-2501</doi>
      <bibkey>hellrich-etal-2019-modeling</bibkey>
    </paper>
    <paper id="2">
      <title>Clustering-Based Article Identification in Historical Newspapers</title>
      <author><first>Martin</first><last>Riedl</last></author>
      <author><first>Daniela</first><last>Betz</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>12–17</pages>
      <abstract>This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore, the automatic segmentation based on the text results in low scores, due to the low quality of some OCRed documents.</abstract>
      <url hash="42270e42">W19-2502</url>
      <doi>10.18653/v1/W19-2502</doi>
      <bibkey>riedl-etal-2019-clustering</bibkey>
      <pwccode url="https://github.com/riedlma/cluster_identification" additional="false">riedlma/cluster_identification</pwccode>
    </paper>
    <paper id="3">
      <title>The Scientization of Literary Study</title>
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last></author>
      <author><first>Andrew</first><last>Piper</last></author>
      <pages>18–28</pages>
      <abstract>Scholarly practices within the humanities have historically been perceived as distinct from the natural sciences. We look at literary studies, a discipline strongly anchored in the humanities, and hypothesize that over the past half-century literary studies has instead undergone a process of “scientization”, adopting linguistic behavior similar to the sciences. We test this using methods based on information theory, comparing a corpus of literary studies articles (around 63,400) with a corpus of standard English and scientific English respectively. We show evidence for “scientization” effects in literary studies, though at a more muted level than scientific English, suggesting that literary studies occupies a middle ground with respect to standard English in the larger space of academic disciplines. More generally, our methodology can be applied to investigate the social positioning and development of language use across different domains (e.g. scientific disciplines, language varieties, registers).</abstract>
      <url hash="34678d70">W19-2503</url>
      <doi>10.18653/v1/W19-2503</doi>
      <bibkey>degaetano-ortlieb-piper-2019-scientization</bibkey>
    </paper>
    <paper id="4">
      <title>Are Fictional Voices Distinguishable? Classifying Character Voices in Modern Drama</title>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author><first>Adam</first><last>Hammond</last></author>
      <author><first>Graeme</first><last>Hirst</last></author>
      <pages>29–34</pages>
      <abstract>According to the literary theory of Mikhail Bakhtin, a dialogic novel is one in which characters speak in their own distinct voices, rather than serving as mouthpieces for their authors. We use text classification to determine which authors best achieve dialogism, looking at a corpus of plays from the late nineteenth and early twentieth centuries. We find that the SAGE model of text generation, which highlights deviations from a background lexical distribution, is an effective method of weighting the words of characters’ utterances. Our results show that it is indeed possible to distinguish characters by their speech in the plays of canonical writers such as George Bernard Shaw, whereas characters are clustered more closely in the works of lesser-known playwrights.</abstract>
      <url hash="15905214">W19-2504</url>
      <doi>10.18653/v1/W19-2504</doi>
      <bibkey>vishnubhotla-etal-2019-fictional</bibkey>
    </paper>
    <paper id="5">
      <title>Automatic Alignment and Annotation Projection for Literary Texts</title>
      <author><first>Uli</first><last>Steinbach</last></author>
      <author><first>Ines</first><last>Rehbein</last></author>
      <pages>35–45</pages>
      <abstract>This paper presents a modular NLP pipeline for the creation of a parallel literature corpus, followed by annotation transfer from the source to the target language. The test case we use to evaluate our pipeline is the automatic transfer of quote and speaker mention annotations from English to German. We evaluate the different components of the pipeline and discuss challenges specific to literary texts. Our experiments show that after applying a reasonable amount of semi-automatic postprocessing we can obtain high-quality aligned and annotated resources for a new language.</abstract>
      <url hash="acc00e0f">W19-2505</url>
      <doi>10.18653/v1/W19-2505</doi>
      <bibkey>steinbach-rehbein-2019-automatic</bibkey>
    </paper>
    <paper id="6">
      <title>Inferring missing metadata from environmental policy texts</title>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Egoitz</first><last>Laparra</last></author>
      <author><first>Sophia</first><last>Wang</last></author>
      <author><first>Yiyun</first><last>Zhao</last></author>
      <author><first>Ragheb</first><last>Al-Ghezi</last></author>
      <author><first>Aaron</first><last>Lien</last></author>
      <author><first>Laura</first><last>López-Hoffman</last></author>
      <pages>46–51</pages>
      <abstract>The National Environmental Policy Act (NEPA) provides a trove of data on how environmental policy decisions have been made in the United States over the last 50 years. Unfortunately, there is no central database for this information and it is too voluminous to assess manually. We describe our efforts to enable systematic research over US environmental policy by extracting and organizing metadata from the text of NEPA documents. Our contributions include collecting more than 40,000 NEPA-related documents, and evaluating rule-based baselines that establish the difficulty of three important tasks: identifying lead agencies, aligning document versions, and detecting reused text.</abstract>
      <url hash="bc5a7d16">W19-2506</url>
      <doi>10.18653/v1/W19-2506</doi>
      <bibkey>bethard-etal-2019-inferring</bibkey>
    </paper>
    <paper id="7">
      <title>Stylometric Classification of <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek Literary Texts by Genre</title>
      <author><first>Efthimios</first><last>Gianitsos</last></author>
      <author><first>Thomas</first><last>Bolt</last></author>
      <author><first>Pramit</first><last>Chaudhuri</last></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <pages>52–60</pages>
      <abstract>Classification of texts by genre is an important application of natural language processing to literary corpora but remains understudied for premodern and non-English traditions. We develop a stylometric feature set for ancient Greek that enables identification of texts as prose or verse. The set contains over 20 primarily syntactic features, which are calculated according to custom, language-specific heuristics. Using these features, we classify almost all surviving classical Greek literature as prose or verse with &gt;97% accuracy and F1 score, and further classify a selection of the verse texts into the traditional genres of epic and drama.</abstract>
      <url hash="d52de53c">W19-2507</url>
      <attachment type="software" hash="bcbe133a">W19-2507.Software.zip</attachment>
      <doi>10.18653/v1/W19-2507</doi>
      <bibkey>gianitsos-etal-2019-stylometric</bibkey>
    </paper>
    <paper id="8">
      <title>A framework for streamlined statistical prediction using topic models</title>
      <author><first>Vanessa</first><last>Glenny</last></author>
      <author><first>Jonathan</first><last>Tuke</last></author>
      <author><first>Nigel</first><last>Bean</last></author>
      <author><first>Lewis</first><last>Mitchell</last></author>
      <pages>61–70</pages>
      <abstract>In the Humanities and Social Sciences, there is increasing interest in approaches to information extraction, prediction, intelligent linkage, and dimension reduction applicable to large text corpora. With approaches in these fields being grounded in traditional statistical techniques, the need arises for frameworks whereby advanced NLP techniques such as topic modelling may be incorporated within classical methodologies. This paper provides a classical, supervised, statistical learning framework for prediction from text, using topic models as a data reduction method and the topics themselves as predictors, alongside typical statistical tools for predictive modelling. We apply this framework in a Social Sciences context (applied animal behaviour) as well as a Humanities context (narrative analysis) as examples of this framework. The results show that topic regression models perform comparably to their much less efficient equivalents that use individual words as predictors.</abstract>
      <url hash="2330e5c0">W19-2508</url>
      <doi>10.18653/v1/W19-2508</doi>
      <bibkey>glenny-etal-2019-framework</bibkey>
    </paper>
    <paper id="9">
      <title>Revisiting <fixed-case>NMT</fixed-case> for Normalization of Early <fixed-case>E</fixed-case>nglish Letters</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Tanja</first><last>Säily</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Eetu</first><last>Mäkelä</last></author>
      <pages>71–75</pages>
      <abstract>This paper studies the use of NMT (neural machine translation) as a normalization method for an early English letter corpus. The corpus has previously been normalized so that only less frequent deviant forms are left out without normalization. This paper discusses different methods for improving the normalization of these deviant forms by using different approaches. Adding features to the training data is found to be unhelpful, but using a lexicographical resource to filter the top candidates produced by the NMT model together with lemmatization improves results.</abstract>
      <url hash="fa07539d">W19-2509</url>
      <doi>10.18653/v1/W19-2509</doi>
      <bibkey>hamalainen-etal-2019-revisiting</bibkey>
    </paper>
    <paper id="10">
      <title>Graph convolutional networks for exploring authorship hypotheses</title>
      <author><first>Tom</first><last>Lippincott</last></author>
      <pages>76–81</pages>
      <abstract>This work considers a task from traditional literary criticism: annotating a structured, composite document with information about its sources. We take the Documentary Hypothesis, a prominent theory regarding the composition of the first five books of the Hebrew bible, extract stylistic features designed to avoid bias or overfitting, and train several classification models. Our main result is that the recently-introduced graph convolutional network architecture outperforms structurally-uninformed models. We also find that including information about the granularity of text spans is a crucial ingredient when employing hidden layers, in contrast to simple logistic regression. We perform error analysis at several levels, noting how some characteristic limitations of the models and simple features lead to misclassifications, and conclude with an overview of future work.</abstract>
      <url hash="93a7f451">W19-2510</url>
      <doi>10.18653/v1/W19-2510</doi>
      <bibkey>lippincott-2019-graph</bibkey>
    </paper>
    <paper id="11">
      <title>Semantics and Homothetic Clustering of Hafez Poetry</title>
      <author><first>Arya</first><last>Rahgozar</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <pages>82–90</pages>
      <abstract>We have created two sets of labels for Hafez (1315-1390) poems, using unsupervised learning. Our labels are the only semantic clustering alternative to the previously existing, hand-labeled, gold-standard classification of Hafez poems, to be used for literary research. We have cross-referenced, measured and analyzed the agreements of our clustering labels with Houman’s chronological classes. Our features are based on topic modeling and word embeddings. We also introduced a similarity of similarities’ features, we called homothetic clustering approach that proved effective, in case of Hafez’s small corpus of ghazals2. Although all our experiments showed different clusters when compared with Houman’s classes, we think they were valid in their own right to have provided further insights, and have proved useful as a contrasting alternative to Houman’s classes. Our homothetic clusterer and its feature design and engineering framework can be used for further semantic analysis of Hafez’s poetry and other similar literary research.</abstract>
      <url hash="53a51d23">W19-2511</url>
      <doi>10.18653/v1/W19-2511</doi>
      <bibkey>rahgozar-inkpen-2019-semantics</bibkey>
    </paper>
    <paper id="12">
      <title>Computational Linguistics Applications for Multimedia Services</title>
      <author><first>Kyeongmin</first><last>Rim</last></author>
      <author><first>Kelley</first><last>Lynch</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>91–97</pages>
      <abstract>We present Computational Linguistics Applications for Multimedia Services (CLAMS), a platform that provides access to computational content analysis tools for archival multimedia material that appear in different media, such as text, audio, image, and video. The primary goal of CLAMS is: (1) to develop an interchange format between multimodal metadata generation tools to ensure interoperability between tools; (2) to provide users with a portable, user-friendly workflow engine to chain selected tools to extract meaningful analyses; and (3) to create a public software development kit (SDK) for developers that eases deployment of analysis tools within the CLAMS platform. CLAMS is designed to help archives and libraries enrich the metadata associated with their mass-digitized multimedia collections, that would otherwise be largely unsearchable.</abstract>
      <url hash="56af227c">W19-2512</url>
      <doi>10.18653/v1/W19-2512</doi>
      <bibkey>rim-etal-2019-computational</bibkey>
    </paper>
    <paper id="13">
      <title>Correcting Whitespace Errors in Digitized Historical Texts</title>
      <author><first>Sandeep</first><last>Soni</last></author>
      <author><first>Lauren</first><last>Klein</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>98–103</pages>
      <abstract>Whitespace errors are common to digitized archives. This paper describes a lightweight unsupervised technique for recovering the original whitespace. Our approach is based on count statistics from Google n-grams, which are converted into a likelihood ratio test computed from interpolated trigram and bigram probabilities. To evaluate this approach, we annotate a small corpus of whitespace errors in a digitized corpus of newspapers from the 19th century United States. Our technique identifies and corrects most whitespace errors while introducing a minimal amount of oversegmentation: it achieves 77% recall at a false positive rate of less than 1%, and 91% recall at a false positive rate of less than 3%.</abstract>
      <url hash="a98bd8e7">W19-2513</url>
      <doi>10.18653/v1/W19-2513</doi>
      <bibkey>soni-etal-2019-correcting</bibkey>
      <pwccode url="https://github.com/sandeepsoni/whitespace-normalizer" additional="false">sandeepsoni/whitespace-normalizer</pwccode>
    </paper>
    <paper id="14">
      <title>On the Feasibility of Automated Detection of Allusive Text Reuse</title>
      <author><first>Enrique</first><last>Manjavacas</last></author>
      <author><first>Brian</first><last>Long</last></author>
      <author><first>Mike</first><last>Kestemont</last></author>
      <pages>104–114</pages>
      <abstract>The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely — commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.</abstract>
      <url hash="70c343c0">W19-2514</url>
      <doi>10.18653/v1/W19-2514</doi>
      <bibkey>manjavacas-etal-2019-feasibility</bibkey>
    </paper>
    <paper id="15">
      <title>The limits of <fixed-case>S</fixed-case>panglish?</title>
      <author><first>Barbara</first><last>Bullock</last></author>
      <author><first>Wally</first><last>Guzmán</last></author>
      <author><first>Almeida Jacqueline</first><last>Toribio</last></author>
      <pages>115–121</pages>
      <abstract>Linguistic code-switching (C-S) is common in oral bilingual vernacular speech. When used in literature, C-S becomes an artistic choice that can mirror the patterns of bilingual interactions. But it can also potentially exceed them. What are the limits of C-S? We model features of C-S in corpora of contemporary U.S. Spanish-English literary and conversational data to analyze why some critics view the ‘Spanglish’ texts of Ilan Stavans as deviating from a C-S norm.</abstract>
      <url hash="6ae86bf3">W19-2515</url>
      <doi>10.18653/v1/W19-2515</doi>
      <bibkey>bullock-etal-2019-limits</bibkey>
    </paper>
    <paper id="16">
      <title>Sign Clustering and Topic Extraction in <fixed-case>P</fixed-case>roto-<fixed-case>E</fixed-case>lamite</title>
      <author><first>Logan</first><last>Born</last></author>
      <author><first>Kate</first><last>Kelley</last></author>
      <author><first>Nishant</first><last>Kambhatla</last></author>
      <author><first>Carolyn</first><last>Chen</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>122–132</pages>
      <abstract>We describe a first attempt at using techniques from computational linguistics to analyze the undeciphered proto-Elamite script. Using hierarchical clustering, n-gram frequencies, and LDA topic models, we both replicate results obtained by manual decipherment and reveal previously-unobserved relationships between signs. This demonstrates the utility of these techniques as an aid to manual decipherment.</abstract>
      <url hash="8987e1ea">W19-2516</url>
      <doi>10.18653/v1/W19-2516</doi>
      <bibkey>born-etal-2019-sign</bibkey>
      <pwccode url="https://github.com/sfu-natlang/pe-decipher-toolkit" additional="false">sfu-natlang/pe-decipher-toolkit</pwccode>
    </paper>
  </volume>
  <volume id="26">
    <meta>
      <booktitle>Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications</booktitle>
      <url hash="1df6b85a">W19-26</url>
      <editor><first>Vivi</first><last>Nastase</last></editor>
      <editor><first>Benjamin</first><last>Roth</last></editor>
      <editor><first>Laura</first><last>Dietz</last></editor>
      <editor><first>Andrew</first><last>McCallum</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="66fa0581">W19-2600</url>
      <bibkey>ws-2019-extracting</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention</title>
      <author><first>Qin</first><last>Dai</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Paul</first><last>Reisert</last></author>
      <author><first>Ryo</first><last>Takahashi</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>1–10</pages>
      <abstract>The increased demand for structured scientific knowledge has attracted considerable attention in extracting scientific relation from the ever growing scientific publications. Distant supervision is widely applied approach to automatically generate large amounts of labelled data with low manual annotation cost. However, distant supervision inevitably accompanies the wrong labelling problem, which will negatively affect the performance of Relation Extraction (RE). To address this issue, (Han et al., 2018) proposes a novel framework for jointly training both RE model and Knowledge Graph Completion (KGC) model to extract structured knowledge from non-scientific dataset. In this work, we firstly investigate the feasibility of this framework on scientific dataset, specifically on biomedical dataset. Secondly, to achieve better performance on the biomedical dataset, we extend the framework with other competitive KGC models. Moreover, we proposed a new end-to-end KGC model to extend the framework. Experimental results not only show the feasibility of the framework on the biomedical dataset, but also indicate the effectiveness of our extensions, because our extended model achieves significant and consistent improvements on distant supervised RE as compared with baselines.</abstract>
      <url hash="d1750df5">W19-2601</url>
      <doi>10.18653/v1/W19-2601</doi>
      <bibkey>dai-etal-2019-distantly</bibkey>
    </paper>
    <paper id="2">
      <title>Scalable, Semi-Supervised Extraction of Structured Information from Scientific Literature</title>
      <author><first>Kritika</first><last>Agrawal</last></author>
      <author><first>Aakash</first><last>Mittal</last></author>
      <author><first>Vikram</first><last>Pudi</last></author>
      <pages>11–20</pages>
      <abstract>As scientific communities grow and evolve, there is a high demand for improved methods for finding relevant papers, comparing papers on similar topics and studying trends in the research community. All these tasks involve the common problem of extracting structured information from scientific articles. In this paper, we propose a novel, scalable, semi-supervised method for extracting relevant structured information from the vast available raw scientific literature. We extract the fundamental concepts of “aim”, ”method” and “result” from scientific articles and use them to construct a knowledge graph. Our algorithm makes use of domain-based word embedding and the bootstrap framework. Our experiments show that our system achieves precision and recall comparable to the state of the art. We also show the domain independence of our algorithm by analyzing the research trends of two distinct communities - computational linguistics and computer vision.</abstract>
      <url hash="2a55063e">W19-2602</url>
      <doi>10.18653/v1/W19-2602</doi>
      <bibkey>agrawal-etal-2019-scalable</bibkey>
    </paper>
    <paper id="3">
      <title>Understanding the Polarity of Events in the Biomedical Literature: Deep Learning vs. Linguistically-informed Methods</title>
      <author><first>Enrique</first><last>Noriega-Atala</last></author>
      <author><first>Zhengzhong</first><last>Liang</last></author>
      <author><first>John</first><last>Bachman</last></author>
      <author><first>Clayton</first><last>Morrison</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>21–30</pages>
      <abstract>An important task in the machine reading of biochemical events expressed in biomedical texts is correctly reading the polarity, i.e., attributing whether the biochemical event is a promotion or an inhibition. Here we present a novel dataset for studying polarity attribution accuracy. We use this dataset to train and evaluate several deep learning models for polarity identification, and compare these to a linguistically-informed model. The best performing deep learning architecture achieves 0.968 average F1 performance in a five-fold cross-validation study, a considerable improvement over the linguistically informed model average F1 of 0.862.</abstract>
      <url hash="f6cccf48">W19-2603</url>
      <doi>10.18653/v1/W19-2603</doi>
      <bibkey>noriega-atala-etal-2019-understanding</bibkey>
    </paper>
    <paper id="4">
      <title>Dataset Mention Extraction and Classification</title>
      <author><first>Animesh</first><last>Prasad</last></author>
      <author><first>Chenglei</first><last>Si</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <pages>31–36</pages>
      <abstract>Datasets are integral artifacts of empirical scientific research. However, due to natural language variation, their recognition can be difficult and even when identified, can often be inconsistently referred across and within publications. We report our approach to the Coleridge Initiative’s Rich Context Competition, which tasks participants with identifying dataset surface forms (dataset mention extraction) and associating the extracted mention to its referred dataset (dataset classification). In this work, we propose various neural baselines and evaluate these model on one-plus and zero-shot classification scenarios. We further explore various joint learning approaches - exploring the synergy between the tasks - and report the issues with such techniques.</abstract>
      <url hash="bb3de647">W19-2604</url>
      <doi>10.18653/v1/W19-2604</doi>
      <bibkey>prasad-etal-2019-dataset</bibkey>
    </paper>
    <paper id="5">
      <title>Annotating with Pros and Cons of Technologies in Computer Science Papers</title>
      <author><first>Hono</first><last>Shirai</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>37–42</pages>
      <abstract>This paper explores a task for extracting a technological expression and its pros/cons from computer science papers. We report ongoing efforts on an annotated corpus of pros/cons and an analysis of the nature of the automatic extraction task. Specifically, we show how to adapt the targeted sentiment analysis task for pros/cons extraction in computer science papers and conduct an annotation study. In order to identify the challenges of the automatic extraction task, we construct a strong baseline model and conduct an error analysis. The experiments show that pros/cons can be consistently annotated by several annotators, and that the task is challenging due to domain-specific knowledge. The annotated dataset is made publicly available for research purposes.</abstract>
      <url hash="66bed00c">W19-2605</url>
      <doi>10.18653/v1/W19-2605</doi>
      <bibkey>shirai-etal-2019-annotating</bibkey>
      <pwccode url="https://github.com/cl-tohoku/scientific-paper-pros-cons" additional="false">cl-tohoku/scientific-paper-pros-cons</pwccode>
    </paper>
    <paper id="6">
      <title>Browsing Health: Information Extraction to Support New Interfaces for Accessing Medical Evidence</title>
      <author><first>Soham</first><last>Parikh</last></author>
      <author><first>Elizabeth</first><last>Conrad</last></author>
      <author><first>Oshin</first><last>Agarwal</last></author>
      <author><first>Iain</first><last>Marshall</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <pages>43–47</pages>
      <abstract>Standard paradigms for search do not work well in the medical context. Typical information needs, such as retrieving a full list of medical interventions for a given condition, or finding the reported efficacy of a particular treatment with respect to a specific outcome of interest cannot be straightforwardly posed in typical text-box search. Instead, we propose faceted-search in which a user specifies a condition and then can browse treatments and outcomes that have been evaluated. Choosing from these, they can access randomized control trials (RCTs) describing individual studies. Realizing such a view of the medical evidence requires information extraction techniques to identify the population, interventions, and outcome measures in an RCT. Patients, health practitioners, and biomedical librarians all stand to benefit from such innovation in search of medical evidence. We present an initial prototype of such an interface applied to pre-registered clinical studies. We also discuss pilot studies into the applicability of information extraction methods to allow for similar access to all published trial results.</abstract>
      <url hash="5b9d4932">W19-2606</url>
      <doi>10.18653/v1/W19-2606</doi>
      <bibkey>parikh-etal-2019-browsing</bibkey>
    </paper>
    <paper id="7">
      <title>An Analysis of Deep Contextual Word Embeddings and Neural Architectures for Toponym Mention Detection in Scientific Publications</title>
      <author><first>Matthew</first><last>Magnusson</last></author>
      <author><first>Laura</first><last>Dietz</last></author>
      <pages>48–56</pages>
      <abstract>Toponym detection in scientific papers is an open task and a key first step in place entity enrichment of documents. We examine three common neural architectures in NLP: 1) convolutional neural network, 2) multi-layer perceptron (both applied in a sliding window context) and 3) bidirectional LSTM and apply contextual and non-contextual word embedding layers to these models. We find that deep contextual word embeddings improve the performance of the bi-LSTM with CRF neural architecture achieving the best performance when multiple layers of deep contextual embeddings are concatenated. Our best performing model achieves an average F1 of 0.910 when evaluated on overlap macro exceeding previous state-of-the-art models in the toponym detection task.</abstract>
      <url hash="7795bd54">W19-2607</url>
      <doi>10.18653/v1/W19-2607</doi>
      <bibkey>magnusson-dietz-2019-analysis</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>STAC</fixed-case>: Science Toolkit Based on <fixed-case>C</fixed-case>hinese Idiom Knowledge Graph</title>
      <author><first>Meiling</first><last>Wang</last></author>
      <author><first>Min</first><last>Xiao</last></author>
      <author><first>Changliang</first><last>Li</last></author>
      <author><first>Yu</first><last>Guo</last></author>
      <author><first>Zhixin</first><last>Zhao</last></author>
      <author><first>Xiaonan</first><last>Liu</last></author>
      <pages>57–61</pages>
      <abstract>Chinese idioms (Cheng Yu) have seen five thousand years’ history and culture of China, meanwhile they contain large number of scientific achievement of ancient China. However, existing Chinese online idiom dictionaries have limited function for scientific exploration. In this paper, we first construct a Chinese idiom knowledge graph by extracting domains and dynasties and associating them with idioms, and based on the idiom knowledge graph, we propose a Science Toolkit for Ancient China (STAC) aiming to support scientific exploration. In the STAC toolkit, idiom navigator helps users explore overall scientific progress from idiom perspective with visualization tools, and idiom card and idiom QA shorten action path and avoid thinking being interrupted while users are reading and writing. The current STAC toolkit is deployed at http://120.92.208.22:7476/demo/#/stac.</abstract>
      <url hash="03f58bb4">W19-2608</url>
      <doi>10.18653/v1/W19-2608</doi>
      <bibkey>wang-etal-2019-stac</bibkey>
    </paper>
    <paper id="9">
      <title>Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text</title>
      <author><first>Ronen</first><last>Tamari</last></author>
      <author><first>Hiroyuki</first><last>Shindo</last></author>
      <author><first>Dafna</first><last>Shahaf</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>62–71</pages>
      <abstract>Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.</abstract>
      <url hash="bba14f2d">W19-2609</url>
      <doi>10.18653/v1/W19-2609</doi>
      <bibkey>tamari-etal-2019-playing</bibkey>
      <pwccode url="https://github.com/ronentk/TextLabs" additional="false">ronentk/TextLabs</pwccode>
    </paper>
    <paper id="10">
      <title>Textual and Visual Characteristics of Mathematical Expressions in Scholar Documents</title>
      <author><first>Vidas</first><last>Daudaravicius</last></author>
      <pages>72–81</pages>
      <abstract>Mathematical expressions (ME) are widely used in scholar documents. In this paper we analyze characteristics of textual and visual MEs characteristics for the image-to-LaTeX translation task. While there are open data-sets of LaTeX files with MEs included it is very complicated to extract these MEs from a document and to compile the list of MEs. Therefore we release a corpus of open-access scholar documents with PDF and JATS-XML parallel files. The MEs in these documents are LaTeX encoded and are document independent. The data contains more than 1.2 million distinct annotated formulae and more than 80 million raw tokens of LaTeX MEs in more than 8 thousand documents. While the variety of textual lengths and visual sizes of MEs are not well defined we found that the task of analyzing MEs in scholar documents can be reduced to the subtask of a particular text length, image width and height bounds, and display MEs can be processed as arrays of partial MEs.</abstract>
      <url hash="d45c9cc5">W19-2610</url>
      <doi>10.18653/v1/W19-2610</doi>
      <bibkey>daudaravicius-2019-textual</bibkey>
    </paper>
  </volume>
  <volume id="27">
    <meta>
      <booktitle>Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019</booktitle>
      <url hash="22b5fa2f">W19-27</url>
      <editor><first>Amir</first><last>Zeldes</last></editor>
      <editor><first>Debopam</first><last>Das</last></editor>
      <editor><first>Erick Maziero</first><last>Galani</last></editor>
      <editor><first>Juliano Desiderato</first><last>Antonio</last></editor>
      <editor><first>Mikel</first><last>Iruskieta</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, MN</address>
      <month>June</month>
      <year>2019</year>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="50eb51db">W19-2700</url>
      <bibkey>ws-2019-discourse</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Introduction to Discourse Relation Parsing and Treebanking (<fixed-case>DISRPT</fixed-case>): 7th Workshop on <fixed-case>R</fixed-case>hetorical <fixed-case>S</fixed-case>tructure <fixed-case>T</fixed-case>heory and Related Formalisms</title>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Debopam</first><last>Das</last></author>
      <author><first>Erick Galani</first><last>Maziero</last></author>
      <author><first>Juliano</first><last>Antonio</last></author>
      <author><first>Mikel</first><last>Iruskieta</last></author>
      <pages>1–6</pages>
      <abstract>This overview summarizes the main contributions of the accepted papers at the 2019 workshop on Discourse Relation Parsing and Treebanking (DISRPT 2019). Co-located with NAACL 2019 in Minneapolis, the workshop’s aim was to bring together researchers working on corpus-based and computational approaches to discourse relations. In addition to an invited talk, eighteen papers outlined below were presented, four of which were submitted as part of a shared task on elementary discourse unit segmentation and connective detection.</abstract>
      <url hash="bddea823">W19-2701</url>
      <doi>10.18653/v1/W19-2701</doi>
      <bibkey>zeldes-etal-2019-introduction</bibkey>
    </paper>
    <paper id="2">
      <title>Toward Cross-theory Discourse Relation Annotation</title>
      <author><first>Peter</first><last>Bourgonje</last></author>
      <author><first>Olha</first><last>Zolotarenko</last></author>
      <pages>7–11</pages>
      <abstract>In this exploratory study, we attempt to automatically induce PDTB-style relations from RST trees. We work with a German corpus of news commentary articles, annotated for RST trees and explicit PDTB-style relations and we focus on inducing the implicit relations in an automated way. Preliminary results look promising as a high-precision (but low-recall) way of finding implicit relations where there is no shallow structure annotated at all, but mapping proves more difficult in cases where EDUs and relation arguments overlap, yet do not seem to signal the same relation.</abstract>
      <url hash="4c33b042">W19-2702</url>
      <doi>10.18653/v1/W19-2702</doi>
      <bibkey>bourgonje-zolotarenko-2019-toward</bibkey>
    </paper>
    <paper id="3">
      <title>Acquiring Annotated Data with Cross-lingual Explicitation for Implicit Discourse Relation Classification</title>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>12–21</pages>
      <abstract>Implicit discourse relation classification is one of the most challenging and important tasks in discourse parsing, due to the lack of connectives as strong linguistic cues. A principle bottleneck to further improvement is the shortage of training data (ca. 18k instances in the Penn Discourse Treebank (PDTB)). Shi et al. (2017) proposed to acquire additional data by exploiting connectives in translation: human translators mark discourse relations which are implicit in the source language explicitly in the translation. Using back-translations of such explicitated connectives improves discourse relation parsing performance. This paper addresses the open question of whether the choice of the translation language matters, and whether multiple translations into different languages can be effectively used to improve the quality of the additional data.</abstract>
      <url hash="f3350e9a">W19-2703</url>
      <doi>10.18653/v1/W19-2703</doi>
      <bibkey>shi-etal-2019-acquiring</bibkey>
    </paper>
    <paper id="4">
      <title>From News to Medical: Cross-domain Discourse Segmentation</title>
      <author><first>Elisa</first><last>Ferracane</last></author>
      <author><first>Titan</first><last>Page</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <pages>22–29</pages>
      <abstract>The first step in discourse analysis involves dividing a text into segments. We annotate the first high-quality small-scale medical corpus in English with discourse segments and analyze how well news-trained segmenters perform on this domain. While we expectedly find a drop in performance, the nature of the segmentation errors suggests some problems can be addressed earlier in the pipeline, while others would require expanding the corpus to a trainable size to learn the nuances of the medical domain.</abstract>
      <url hash="612afa34">W19-2704</url>
      <attachment type="presentation" hash="9741f83e">W19-2704.Presentation.pdf</attachment>
      <doi>10.18653/v1/W19-2704</doi>
      <bibkey>ferracane-etal-2019-news</bibkey>
      <pwccode url="https://github.com/elisaF/news-med-segmentation" additional="false">elisaF/news-med-segmentation</pwccode>
    </paper>
    <paper id="5">
      <title>Nuclearity in <fixed-case>RST</fixed-case> and signals of coherence relations</title>
      <author><first>Debopam</first><last>Das</last></author>
      <pages>30–37</pages>
      <abstract>We investigate the relationship between the notion of nuclearity as proposed in Rhetorical Structure Theory (RST) and the signalling of coherence relations. RST relations are categorized as either mononuclear (comprising a nucleus and a satellite span) or multinuclear (comprising two or more nuclei spans). We examine how mononuclear relations (e.g., Antithesis, Condition) and multinuclear relations (e.g., Contrast, List) are indicated by relational signals, more particularly by discourse markers (e.g., because, however, if, therefore). We conduct a corpus study, examining the distribution of either type of relations in the RST Discourse Treebank (Carlson et al., 2002) and the distribution of discourse markers for those relations in the RST Signalling Corpus (Das et al., 2015). Our results show that discourse markers are used more often to signal multinuclear relations than mononuclear relations. The findings also suggest a complex relationship between the relation types and syntactic categories of discourse markers (subordinating and coordinating conjunctions).</abstract>
      <url hash="ab875541">W19-2705</url>
      <doi>10.18653/v1/W19-2705</doi>
      <bibkey>das-2019-nuclearity</bibkey>
    </paper>
    <paper id="6">
      <title>The Rhetorical Structure of Attribution</title>
      <author><first>Andrew</first><last>Potter</last></author>
      <pages>38–49</pages>
      <abstract>The relational status of Attribution in Rhetorical Structure Theory has been a matter of ongoing debate. Although several researchers have weighed in on the topic, and although numerous studies have relied upon attributional structures for their analyses, nothing approaching consensus has emerged. This paper identifies three basic issues that must be resolved to determine the relational status of attributions. These are identified as the Discourse Units Issue, the Nuclearity Issue, and the Relation Identification Issue. These three issues are analyzed from the perspective of classical RST. A finding of this analysis is that the nuclearity and the relational identification of attribution structures are shown to depend on the writer’s intended effect, such that attributional relations cannot be considered as a single relation, but rather as attributional instances of other RST relations.</abstract>
      <url hash="e8ab6d5a">W19-2706</url>
      <attachment type="presentation" hash="00b661fa">W19-2706.Presentation.pptx</attachment>
      <doi>10.18653/v1/W19-2706</doi>
      <bibkey>potter-2019-rhetorical</bibkey>
    </paper>
    <paper id="7">
      <title>Annotating Shallow Discourse Relations in <fixed-case>T</fixed-case>witter Conversations</title>
      <author><first>Tatjana</first><last>Scheffler</last></author>
      <author><first>Berfin</first><last>Aktaş</last></author>
      <author><first>Debopam</first><last>Das</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>50–55</pages>
      <abstract>We introduce our pilot study applying PDTB-style annotation to Twitter conversations. Lexically grounded coherence annotation for Twitter threads will enable detailed investigations of the discourse structure of conversations on social media. Here, we present our corpus of 185 threads and annotation, including an inter-annotator agreement study. We discuss our observations as to how Twitter discourses differ from written news text wrt. discourse connectives and relations. We confirm our hypothesis that discourse relations in written social media conversations are expressed differently than in (news) text. We find that in Twitter, connective arguments frequently are not full syntactic clauses, and that a few general connectives expressing EXPANSION and CONTINGENCY make up the majority of the explicit relations in our data.</abstract>
      <url hash="f2c992c0">W19-2707</url>
      <doi>10.18653/v1/W19-2707</doi>
      <bibkey>scheffler-etal-2019-annotating</bibkey>
    </paper>
    <paper id="8">
      <title>A Discourse Signal Annotation System for <fixed-case>RST</fixed-case> Trees</title>
      <author><first>Luke</first><last>Gessler</last></author>
      <author id="yang-janet-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <pages>56–61</pages>
      <abstract>This paper presents a new system for open-ended discourse relation signal annotation in the framework of Rhetorical Structure Theory (RST), implemented on top of an online tool for RST annotation. We discuss existing projects annotating textual signals of discourse relations, which have so far not allowed simultaneously structuring and annotating words signaling hierarchical discourse trees, and demonstrate the design and applications of our interface by extending existing RST annotations in the freely available GUM corpus.</abstract>
      <url hash="417861cf">W19-2708</url>
      <doi>10.18653/v1/W19-2708</doi>
      <bibkey>gessler-etal-2019-discourse</bibkey>
      <pwccode url="https://github.com/amir-zeldes/rstweb" additional="false">amir-zeldes/rstweb</pwccode>
    </paper>
    <paper id="9">
      <title><fixed-case>E</fixed-case>us<fixed-case>D</fixed-case>is<fixed-case>P</fixed-case>arser: improving an under-resourced discourse parser with cross-lingual data</title>
      <author><first>Mikel</first><last>Iruskieta</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <pages>62–71</pages>
      <abstract>Development of discourse parsers to annotate the relational discourse structure of a text is crucial for many downstream tasks. However, most of the existing work focuses on English, assuming a quite large dataset. Discourse data have been annotated for Basque, but training a system on these data is challenging since the corpus is very small. In this paper, we create the first demonstrator based on RST for Basque, and we investigate the use of data in another language to improve the performance of a Basque discourse parser. More precisely, we build a monolingual system using the small set of data available and investigate the use of multilingual word embeddings to train a system for Basque using data annotated for another language. We found that our approach to building a system limited to the small set of data available for Basque allowed us to get an improvement over previous approaches making use of many data annotated in other languages. At best, we get 34.78 in F1 for the full discourse structure. More data annotation is necessary in order to improve the results obtained with these techniques. We also describe which relations match with the gold standard, in order to understand these results.</abstract>
      <url hash="87de303a">W19-2709</url>
      <doi>10.18653/v1/W19-2709</doi>
      <bibkey>iruskieta-braud-2019-eusdisparser</bibkey>
    </paper>
    <paper id="10">
      <title>Beyond The <fixed-case>W</fixed-case>all <fixed-case>S</fixed-case>treet <fixed-case>J</fixed-case>ournal: Anchoring and Comparing Discourse Signals across Genres</title>
      <author id="yang-janet-liu"><first>Yang</first><last>Liu</last></author>
      <pages>72–81</pages>
      <abstract>Recent research on discourse relations has found that they are cued not only by discourse markers (DMs) but also by other textual signals and that signaling information is indicative of genres. While several corpora exist with discourse relation signaling information such as the Penn Discourse Treebank (PDTB, Prasad et al. 2008) and the Rhetorical Structure Theory Signalling Corpus (RST-SC, Das and Taboada 2018), they both annotate the Wall Street Journal (WSJ) section of the Penn Treebank (PTB, Marcus et al. 1993), which is limited to the news domain. Thus, this paper adapts the signal identification and anchoring scheme (Liu and Zeldes, 2019) to three more genres, examines the distribution of signaling devices across relations and genres, and provides a taxonomy of indicative signals found in this dataset.</abstract>
      <url hash="bff4a7ac">W19-2710</url>
      <doi>10.18653/v1/W19-2710</doi>
      <bibkey>liu-2019-beyond</bibkey>
    </paper>
    <paper id="11">
      <title>Towards the Data-driven System for Rhetorical Parsing of <fixed-case>R</fixed-case>ussian Texts</title>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Dina</first><last>Pisarevskaya</last></author>
      <author><first>Elena</first><last>Chistova</last></author>
      <author><first>Svetlana</first><last>Toldova</last></author>
      <author><first>Maria</first><last>Kobozeva</last></author>
      <author><first>Ivan</first><last>Smirnov</last></author>
      <pages>82–87</pages>
      <abstract>Results of the first experimental evaluation of machine learning models trained on Ru-RSTreebank – first Russian corpus annotated within RST framework – are presented. Various lexical, quantitative, morphological, and semantic features were used. In rhetorical relation classification, ensemble of CatBoost model with selected features and a linear SVM model provides the best score (macro F1 = 54.67 ± 0.38). We discover that most of the important features for rhetorical relation classification are related to discourse connectives derived from the connectives lexicon for Russian and from other sources.</abstract>
      <url hash="e2c12b0d">W19-2711</url>
      <attachment type="poster" hash="260f1e2a">W19-2711.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-2711</doi>
      <bibkey>shelmanov-etal-2019-towards</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>RST</fixed-case>-Tace A tool for automatic comparison and evaluation of <fixed-case>RST</fixed-case> trees</title>
      <author><first>Shujun</first><last>Wan</last></author>
      <author><first>Tino</first><last>Kutschbach</last></author>
      <author><first>Anke</first><last>Lüdeling</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>88–96</pages>
      <abstract>This paper presents RST-Tace, a tool for automatic comparison and evaluation of RST trees. RST-Tace serves as an implementation of Iruskieta’s comparison method, which allows trees to be compared and evaluated without the influence of decisions at lower levels in a tree in terms of four factors: constituent, attachment point, nuclearity as well as relation. RST-Tace can be used regardless of the language or the size of rhetorical trees. This tool aims to measure the agreement between two annotators. The result is reflected by F-measure and inter-annotator agreement. Both the comparison table and the result of the evaluation can be obtained automatically.</abstract>
      <url hash="9156a815">W19-2712</url>
      <doi>10.18653/v1/W19-2712</doi>
      <bibkey>wan-etal-2019-rst</bibkey>
      <pwccode url="https://github.com/tkutschbach/RST-Tace" additional="false">tkutschbach/RST-Tace</pwccode>
    </paper>
    <paper id="13">
      <title>The <fixed-case>DISRPT</fixed-case> 2019 Shared Task on Elementary Discourse Unit Segmentation and Connective Detection</title>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Debopam</first><last>Das</last></author>
      <author><first>Erick Galani</first><last>Maziero</last></author>
      <author><first>Juliano</first><last>Antonio</last></author>
      <author><first>Mikel</first><last>Iruskieta</last></author>
      <pages>97–104</pages>
      <abstract>In 2019, we organized the first iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms: the DISRPT Shared Task on Elementary Discourse Unit Segmentation and Connective Detection. In this paper we review the data included in the task, which cover 2.6 million manually annotated tokens from 15 datasets in 10 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data.</abstract>
      <url hash="856d1cdc">W19-2713</url>
      <doi>10.18653/v1/W19-2713</doi>
      <bibkey>zeldes-etal-2019-disrpt</bibkey>
    </paper>
    <paper id="14">
      <title>Multi-lingual and Cross-genre Discourse Unit Segmentation</title>
      <author><first>Peter</first><last>Bourgonje</last></author>
      <author><first>Robin</first><last>Schäfer</last></author>
      <pages>105–114</pages>
      <abstract>We describe a series of experiments applied to data sets from different languages and genres annotated for coherence relations according to different theoretical frameworks. Specifically, we investigate the feasibility of a unified (theory-neutral) approach toward discourse segmentation; a process which divides a text into minimal discourse units that are involved in s coherence relation. We apply a RandomForest and an LSTM based approach for all data sets, and we improve over a simple baseline assuming simple sentence or clause-like segmentation. Performance however varies a lot depending on language, and more importantly genre, with f-scores ranging from 73.00 to 94.47.</abstract>
      <url hash="de0b1802">W19-2714</url>
      <doi>10.18653/v1/W19-2714</doi>
      <bibkey>bourgonje-schafer-2019-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/disrpt2019">DISRPT2019</pwcdataset>
    </paper>
    <paper id="15">
      <title><fixed-case>T</fixed-case>o<fixed-case>N</fixed-case>y: Contextual embeddings for accurate multilingual discourse segmentation of full documents</title>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Mathieu</first><last>Morey</last></author>
      <pages>115–124</pages>
      <abstract>Segmentation is the first step in building practical discourse parsers, and is often neglected in discourse parsing studies. The goal is to identify the minimal spans of text to be linked by discourse relations, or to isolate explicit marking of discourse relations. Existing systems on English report F1 scores as high as 95%, but they generally assume gold sentence boundaries and are restricted to English newswire texts annotated within the RST framework. This article presents a generic approach and a system, ToNy, a discourse segmenter developed for the DisRPT shared task where multiple discourse representation schemes, languages and domains are represented. In our experiments, we found that a straightforward sequence prediction architecture with pretrained contextual embeddings is sufficient to reach performance levels comparable to existing systems, when separately trained on each corpus. We report performance between 81% and 96% in F1 score. We also observed that discourse segmentation models only display a moderate generalization capability, even within the same language and discourse representation scheme.</abstract>
      <url hash="6fb2b053">W19-2715</url>
      <attachment type="poster" hash="cd20752c">W19-2715.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-2715</doi>
      <bibkey>muller-etal-2019-tony</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/disrpt2019">DISRPT2019</pwcdataset>
    </paper>
    <paper id="16">
      <title>Multilingual segmentation based on neural networks and pre-trained word embeddings</title>
      <author><first>Mikel</first><last>Iruskieta</last></author>
      <author><first>Kepa</first><last>Bengoetxea</last></author>
      <author><first>Aitziber</first><last>Atutxa Salazar</last></author>
      <author><first>Arantza</first><last>Diaz de Ilarraza</last></author>
      <pages>125–132</pages>
      <abstract>The DISPRT 2019 workshop has organized a shared task aiming to identify cross-formalism and multilingual discourse segments. Elementary Discourse Units (EDUs) are quite similar across different theories. Segmentation is the very first stage on the way of rhetorical annotation. Still, each annotation project adopted several decisions with consequences not only on the annotation of the relational discourse structure but also at the segmentation stage. In this shared task, we have employed pre-trained word embeddings, neural networks (BiLSTM+CRF) to perform the segmentation. We report F1 results for 6 languages: Basque (0.853), English (0.919), French (0.907), German (0.913), Portuguese (0.926) and Spanish (0.868 and 0.769). Finally, we also pursued an error analysis based on clause typology for Basque and Spanish, in order to understand the performance of the segmenter.</abstract>
      <url hash="f08a0202">W19-2716</url>
      <attachment type="software" hash="f0845a4b">W19-2716.Software.pdf</attachment>
      <doi>10.18653/v1/W19-2716</doi>
      <bibkey>iruskieta-etal-2019-multilingual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/disrpt2019">DISRPT2019</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>G</fixed-case>um<fixed-case>D</fixed-case>rop at the <fixed-case>DISRPT</fixed-case>2019 Shared Task: A Model Stacking Approach to Discourse Unit Segmentation and Connective Detection</title>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Yilun</first><last>Zhu</last></author>
      <author id="yang-janet-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Yan</first><last>Liu</last></author>
      <author><first>Siyao</first><last>Peng</last></author>
      <author><first>Mackenzie</first><last>Gong</last></author>
      <author><first>Amir</first><last>Zeldes</last></author>
      <pages>133–143</pages>
      <abstract>In this paper we present GumDrop, Georgetown University’s entry at the DISRPT 2019 Shared Task on automatic discourse unit segmentation and connective detection. Our approach relies on model stacking, creating a heterogeneous ensemble of classifiers, which feed into a metalearner for each final task. The system encompasses three trainable component stacks: one for sentence splitting, one for discourse unit segmentation and one for connective detection. The flexibility of each ensemble allows the system to generalize well to datasets of different sizes and with varying levels of homogeneity.</abstract>
      <url hash="d43f3fca">W19-2717</url>
      <doi>10.18653/v1/W19-2717</doi>
      <bibkey>yu-etal-2019-gumdrop</bibkey>
      <pwccode url="https://github.com/gucorpling/GumDrop" additional="false">gucorpling/GumDrop</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/disrpt2019">DISRPT2019</pwcdataset>
    </paper>
    <paper id="18">
      <title>Towards discourse annotation and sentiment analysis of the <fixed-case>B</fixed-case>asque Opinion Corpus</title>
      <author><first>Jon</first><last>Alkorta</last></author>
      <author><first>Koldo</first><last>Gojenola</last></author>
      <author><first>Mikel</first><last>Iruskieta</last></author>
      <pages>144–152</pages>
      <abstract>Discourse information is crucial for a better understanding of the text structure and it is also necessary to describe which part of an opinionated text is more relevant or to decide how a text span can change the polarity (strengthen or weaken) of other span by means of coherence relations. This work presents the first results on the annotation of the Basque Opinion Corpus using Rhetorical Structure Theory (RST). Our evaluation results and analysis show us the main avenues to improve on a future annotation process. We have also extracted the subjectivity of several rhetorical relations and the results show the effect of sentiment words in relations and the influence of each relation in the semantic orientation value.</abstract>
      <url hash="638d9e86">W19-2718</url>
      <attachment type="presentation" hash="83001e83">W19-2718.Presentation.pdf</attachment>
      <doi>10.18653/v1/W19-2718</doi>
      <bibkey>alkorta-etal-2019-towards</bibkey>
    </paper>
    <paper id="19">
      <title>Using <fixed-case>R</fixed-case>hetorical <fixed-case>S</fixed-case>tructure <fixed-case>T</fixed-case>heory to Assess Discourse Coherence for Non-native Spontaneous Speech</title>
      <author><first>Xinhao</first><last>Wang</last></author>
      <author><first>Binod</first><last>Gyawali</last></author>
      <author><first>James V.</first><last>Bruno</last></author>
      <author><first>Hillary R.</first><last>Molloy</last></author>
      <author><first>Keelan</first><last>Evanini</last></author>
      <author><first>Klaus</first><last>Zechner</last></author>
      <pages>153–162</pages>
      <abstract>This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for non-native speakers. Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts; however, limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech. Due to the fact that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of spoken language, we conducted research to obtain RST annotations on non-native spoken responses from a standardized assessment of academic English proficiency. Subsequently, automatic parsers were trained on these annotations to process non-native spontaneous speech. Finally, a set of features were extracted from automatically generated RST trees to evaluate the discourse structure of non-native spontaneous speech, which were then employed to further improve the validity of an automated speech scoring system.</abstract>
      <url hash="10f1ff3a">W19-2719</url>
      <attachment type="presentation" hash="ba7b678d">W19-2719.Presentation.pptx</attachment>
      <doi>10.18653/v1/W19-2719</doi>
      <bibkey>wang-etal-2019-using</bibkey>
    </paper>
    <paper id="20">
      <title>Applying <fixed-case>R</fixed-case>hetorical <fixed-case>S</fixed-case>tructure <fixed-case>T</fixed-case>heory to Student Essays for Providing Automated Writing Feedback</title>
      <author><first>Shiyan</first><last>Jiang</last></author>
      <author><first>Kexin</first><last>Yang</last></author>
      <author><first>Chandrakumari</first><last>Suvarna</last></author>
      <author><first>Pooja</first><last>Casula</last></author>
      <author><first>Mingtong</first><last>Zhang</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>163–168</pages>
      <abstract>We present a package of annotation resources, including annotation guideline, flowchart, and an Intelligent Tutoring System for training human annotators. These resources can be used to apply Rhetorical Structure Theory (RST) to essays written by students in K-12 schools. Furthermore, we highlight the great potential of using RST to provide automated feedback for improving writing quality across genres.</abstract>
      <url hash="ce6139bc">W19-2720</url>
      <attachment type="presentation" hash="d47ba1e8">W19-2720.Presentation.pdf</attachment>
      <doi>10.18653/v1/W19-2720</doi>
      <bibkey>jiang-etal-2019-applying</bibkey>
    </paper>
  </volume>
  <volume id="28">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Computational Models of Reference, Anaphora and Coreference</booktitle>
      <url hash="df1dc68c">W19-28</url>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Sameer</first><last>Pradhan</last></editor>
      <editor><first>Yulia</first><last>Grishina</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, USA</address>
      <month>June</month>
      <year>2019</year>
      <venue>crac</venue>
    </meta>
    <frontmatter>
      <url hash="78be2ffe">W19-2800</url>
      <bibkey>ws-2019-models</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluation of named entity coreference</title>
      <author><first>Oshin</first><last>Agarwal</last></author>
      <author><first>Sanjay</first><last>Subramanian</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1–7</pages>
      <abstract>In many NLP applications like search and information extraction for named entities, it is necessary to find all the mentions of a named entity, some of which appear as pronouns (she, his, etc.) or nominals (the professor, the German chancellor, etc.). It is therefore important that coreference resolution systems are able to link these different types of mentions to the correct entity name. We evaluate state-of-the-art coreference resolution systems for the task of resolving all mentions to named entities. Our analysis reveals that standard coreference metrics do not reflect adequately the requirements in this task: they do not penalize systems for not identifying any mentions by name to an entity and they reward systems even if systems find correctly mentions to the same entity but fail to link these to a proper name (she–the student–no name). We introduce new metrics for evaluating named entity coreference that address these discrepancies and show that for the comparisons of competitive systems, standard coreference evaluations could give misleading results for this task. We are, however, able to confirm that the state-of-the art system according to traditional evaluations also performs vastly better than other systems on the named entity coreference task.</abstract>
      <url hash="ea76f2f9">W19-2801</url>
      <attachment type="supplementary" hash="af165db0">W19-2801.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-2801</doi>
      <bibkey>agarwal-etal-2019-evaluation</bibkey>
    </paper>
    <paper id="2">
      <title>Neural Coreference Resolution with Limited Lexical Context and Explicit Mention Detection for Oral <fixed-case>F</fixed-case>rench</title>
      <author><first>Loïc</first><last>Grobol</last></author>
      <pages>8–14</pages>
      <abstract>We propose an end-to-end coreference resolution system obtained by adapting neural models that have recently improved the state-of-the-art on the OntoNotes benchmark to make them applicable to other paradigms for this task. We report the performances of our system on ANCOR, a corpus of transcribed oral French, for which it constitutes a new baseline with proper evaluation.</abstract>
      <url hash="db4fb477">W19-2802</url>
      <attachment type="supplementary" hash="bc93ffaa">W19-2802.Supplementary.tgz</attachment>
      <doi>10.18653/v1/W19-2802</doi>
      <bibkey>grobol-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="3">
      <title>Entity Decisions in Neural Language Modelling: Approaches and Problems</title>
      <author><first>Jenny</first><last>Kunz</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>15–19</pages>
      <abstract>We explore different approaches to explicit entity modelling in language models (LM). We independently replicate two existing models in a controlled setup, introduce a simplified variant of one of the models and analyze their performance in direct comparison. Our results suggest that today’s models are limited as several stochastic variables make learning difficult. We show that the most challenging point in the systems is the decision if the next token is an entity token. The low precision and recall for this variable will lead to severe cascading errors. Our own simplified approach dispenses with the need for latent variables and improves the performance in the entity yes/no decision. A standard well-tuned baseline RNN-LM with a larger number of hidden units outperforms all entity-enabled LMs in terms of perplexity.</abstract>
      <url hash="8efccffc">W19-2803</url>
      <doi>10.18653/v1/W19-2803</doi>
      <bibkey>kunz-hardmeier-2019-entity</bibkey>
    </paper>
    <paper id="4">
      <title>Cross-lingual <fixed-case>NIL</fixed-case> Entity Clustering for Low-resource Languages</title>
      <author><first>Kevin</first><last>Blissett</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>20–25</pages>
      <abstract>Clustering unlinkable entity mentions across documents in multiple languages (cross-lingual NIL Clustering) is an important task as part of Entity Discovery and Linking (EDL). This task has been largely neglected by the EDL community because it is challenging to outperform simple edit distance or other heuristics based baselines. We propose a novel approach based on encoding the orthographic similarity of the mentions using a Recurrent Neural Network (RNN) architecture. Our model adapts a training procedure from the one-shot facial recognition literature in order to achieve this. We also perform several exploratory probing tasks on our name encodings in order to determine what specific types of information are likely to be encoded by our model. Experiments show our approach provides up to a 6.6% absolute CEAFm F-Score improvement over state-of-the-art methods and successfully captures phonological relations across languages.</abstract>
      <url hash="c712ad59">W19-2804</url>
      <doi>10.18653/v1/W19-2804</doi>
      <bibkey>blissett-ji-2019-cross</bibkey>
    </paper>
    <paper id="5">
      <title>Cross-lingual Incongruences in the Annotation of Coreference</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Pauline</first><last>Krielke</last></author>
      <pages>26–34</pages>
      <abstract>In the present paper, we deal with incongruences in English-German multilingual coreference annotation and present automated methods to discover them. More specifically, we automatically detect full coreference chains in parallel texts and analyse discrepancies in their annotations. In doing so, we wish to find out whether the discrepancies rather derive from language typological constraints, from the translation or the actual annotation process. The results of our study contribute to the referential analysis of similarities and differences across languages and support evaluation of cross-lingual coreference annotation. They are also useful for cross-lingual coreference resolution systems and contrastive linguistic studies.</abstract>
      <url hash="1e352bb8">W19-2805</url>
      <doi>10.18653/v1/W19-2805</doi>
      <bibkey>lapshinova-koltunski-etal-2019-cross</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/parcorfull">ParCorFull</pwcdataset>
    </paper>
    <paper id="6">
      <title>Deep Cross-Lingual Coreference Resolution for Less-Resourced Languages: The Case of <fixed-case>B</fixed-case>asque</title>
      <author><first>Gorka</first><last>Urbizu</last></author>
      <author><first>Ander</first><last>Soraluze</last></author>
      <author><first>Olatz</first><last>Arregi</last></author>
      <pages>35–41</pages>
      <abstract>In this paper, we present a cross-lingual neural coreference resolution system for a less-resourced language such as Basque. To begin with, we build the first neural coreference resolution system for Basque, training it with the relatively small EPEC-KORREF corpus (45,000 words). Next, a cross-lingual coreference resolution system is designed. With this approach, the system learns from a bigger English corpus, using cross-lingual embeddings, to perform the coreference resolution for Basque. The cross-lingual system obtains slightly better results (40.93 F1 CoNLL) than the monolingual system (39.12 F1 CoNLL), without using any Basque language corpus to train it.</abstract>
      <url hash="c1b909e5">W19-2806</url>
      <doi>10.18653/v1/W19-2806</doi>
      <bibkey>urbizu-etal-2019-deep</bibkey>
    </paper>
  </volume>
  <volume id="29">
    <meta>
      <booktitle>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</booktitle>
      <url hash="856754db">W19-29</url>
      <editor><first>Emmanuele</first><last>Chersoni</last></editor>
      <editor><first>Cassandra</first><last>Jacobs</last></editor>
      <editor><first>Alessandro</first><last>Lenci</last></editor>
      <editor><first>Tal</first><last>Linzen</last></editor>
      <editor><first>Laurent</first><last>Prévot</last></editor>
      <editor><first>Enrico</first><last>Santus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>cmcl</venue>
    </meta>
    <frontmatter>
      <url hash="db02b81f">W19-2900</url>
      <bibkey>ws-2019-cognitive</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The Active-Filler Strategy in a Move-Eager Left-Corner <fixed-case>M</fixed-case>inimalist <fixed-case>G</fixed-case>rammar Parser</title>
      <author><first>Tim</first><last>Hunter</last></author>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <author><first>Edward</first><last>Stabler</last></author>
      <pages>1–10</pages>
      <abstract>Recent psycholinguistic evidence suggests that human parsing of moved elements is ‘active’, and perhaps even ‘hyper-active’: it seems that a leftward-moved object is related to a verbal position rapidly, perhaps even before the transitivity information associated with the verb is available to the listener. This paper presents a formal, sound and complete parser for Minimalist Grammars whose search space contains branching points that we can identify as the locus of the decision to perform this kind of active gap-finding. This brings formal models of parsing into closer contact with recent psycholinguistic theorizing than was previously possible.</abstract>
      <url hash="f7b758e6">W19-2901</url>
      <doi>10.18653/v1/W19-2901</doi>
      <bibkey>hunter-etal-2019-active</bibkey>
    </paper>
    <paper id="2">
      <title>Priming vs. Inhibition of Optional Infinitival “to”</title>
      <author><first>Robin</first><last>Melnick</last></author>
      <author><first>Thomas</first><last>Wasow</last></author>
      <pages>11–19</pages>
      <abstract>The word “to” that precedes verbs in English infinitives is optional in at least two environments: in what Wasow et al. (2015) previously called the “do-be” construction, and in the complement of “help”, which we explore in the present work. In the “do-be” construction, Wasow et al. found that a preceding infinitival “to” increases the use of following optional “to”, but the use of “to” in the complement of help is reduced following “to help”. We examine two hypotheses regarding why the same function word is primed by prior use in one construction and inhibited in another. We then test predictions made by the two hypotheses, finding support for one of them.</abstract>
      <url hash="be353973">W19-2902</url>
      <doi>10.18653/v1/W19-2902</doi>
      <bibkey>melnick-wasow-2019-priming</bibkey>
    </paper>
    <paper id="3">
      <title>Simulating <fixed-case>S</fixed-case>panish-<fixed-case>E</fixed-case>nglish Code-Switching: El Modelo Está Generating Code-Switches</title>
      <author><first>Chara</first><last>Tsoukala</last></author>
      <author><first>Stefan L.</first><last>Frank</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <author><first>Jorge</first><last>Valdés Kroff</last></author>
      <author><first>Mirjam</first><last>Broersma</last></author>
      <pages>20–29</pages>
      <abstract>Multilingual speakers are able to switch from one language to the other (“code-switch”) between or within sentences. Because the underlying cognitive mechanisms are not well understood, in this study we use computational cognitive modeling to shed light on the process of code-switching. We employed the Bilingual Dual-path model, a Recurrent Neural Network of bilingual sentence production (Tsoukala et al., 2017), and simulated sentence production in simultaneous Spanish-English bilinguals. Our first goal was to investigate whether the model would code-switch without being exposed to code-switched training input. The model indeed produced code-switches even without any exposure to such input and the patterns of code-switches are in line with earlier linguistic work (Poplack,1980). The second goal of this study was to investigate an auxiliary phrase asymmetry that exists in Spanish-English code-switched production. Using this cognitive model, we examined a possible cause for this asymmetry. To our knowledge, this is the first computational cognitive model that aims to simulate code-switched sentence production.</abstract>
      <url hash="dcadf6aa">W19-2903</url>
      <doi>10.18653/v1/W19-2903</doi>
      <bibkey>tsoukala-etal-2019-simulating</bibkey>
    </paper>
    <paper id="4">
      <title>Surprisal and Interference Effects of Case Markers in <fixed-case>H</fixed-case>indi Word Order</title>
      <author><first>Sidharth</first><last>Ranjan</last></author>
      <author><first>Sumeet</first><last>Agarwal</last></author>
      <author><first>Rajakrishnan</first><last>Rajkumar</last></author>
      <pages>30–42</pages>
      <abstract>Based on the Production-Distribution-Comprehension (PDC) account of language processing, we formulate two distinct hypotheses about case marking, word order choices and processing in Hindi. Our first hypothesis is that Hindi tends to optimize for processing efficiency at both lexical and syntactic levels. We quantify the role of case markers in this process. For the task of predicting the reference sentence occurring in a corpus (amidst meaning-equivalent grammatical variants) using a machine learning model, surprisal estimates from an artificial version of the language (i.e., Hindi without any case markers) result in lower prediction accuracy compared to natural Hindi. Our second hypothesis is that Hindi tends to minimize interference due to case markers while ordering preverbal constituents. We show that Hindi tends to avoid placing next to each other constituents whose heads are marked by identical case inflections. Our findings adhere to PDC assumptions and we discuss their implications for language production, learning and universals.</abstract>
      <url hash="dcdd12a8">W19-2904</url>
      <doi>10.18653/v1/W19-2904</doi>
      <bibkey>ranjan-etal-2019-surprisal</bibkey>
    </paper>
    <paper id="5">
      <title>Modeling Hierarchical Syntactic Structures in Morphological Processing</title>
      <author><first>Yohei</first><last>Oseki</last></author>
      <author><first>Charles</first><last>Yang</last></author>
      <author><first>Alec</first><last>Marantz</last></author>
      <pages>43–52</pages>
      <abstract>Sentences are represented as hierarchical syntactic structures, which have been successfully modeled in sentence processing. In contrast, despite the theoretical agreement on hierarchical syntactic structures within words, words have been argued to be computationally less complex than sentences and implemented by finite-state models as linear strings of morphemes, and even the psychological reality of morphemes has been denied. In this paper, extending the computational models employed in sentence processing to morphological processing, we performed a computational simulation experiment where, given incremental surprisal as a linking hypothesis, five computational models with different representational assumptions were evaluated against human reaction times in visual lexical decision experiments available from the English Lexicon Project (ELP), a “shared task” in the morphological processing literature. The simulation experiment demonstrated that (i) “amorphous” models without morpheme units underperformed relative to “morphous” models, (ii) a computational model with hierarchical syntactic structures, Probabilistic Context-Free Grammar (PCFG), most accurately explained human reaction times, and (iii) this performance was achieved on top of surface frequency effects. These results strongly suggest that morphological processing tracks morphemes incrementally from left to right and parses them into hierarchical syntactic structures, contrary to “amorphous” and finite-state models of morphological processing.</abstract>
      <url hash="3e972ad1">W19-2905</url>
      <doi>10.18653/v1/W19-2905</doi>
      <bibkey>oseki-etal-2019-modeling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="6">
      <title>A Modeling Study of the Effects of Surprisal and Entropy in Perceptual Decision Making of an Adaptive Agent</title>
      <author><first>Pyeong Whan</first><last>Cho</last></author>
      <author><first>Richard</first><last>Lewis</last></author>
      <pages>53–61</pages>
      <abstract>Processing difficulty in online language comprehension has been explained in terms of surprisal and entropy reduction. Although both hypotheses have been supported by experimental data, we do not fully understand their relative contributions on processing difficulty. To develop a better understanding, we propose a mechanistic model of perceptual decision making that interacts with a simulated task environment with temporal dynamics. The proposed model collects noisy bottom-up evidence over multiple timesteps, integrates it with its top-down expectation, and makes perceptual decisions, producing processing time data directly without relying on any linking hypothesis. Temporal dynamics in the task environment was determined by a simple finite-state grammar, which was designed to create the situations where the surprisal and entropy reduction hypotheses predict different patterns. After the model was trained to maximize rewards, the model developed an adaptive policy and both surprisal and entropy effects were observed especially in a measure reflecting earlier processing.</abstract>
      <url hash="5d7695df">W19-2906</url>
      <doi>10.18653/v1/W19-2906</doi>
      <bibkey>cho-lewis-2019-modeling</bibkey>
    </paper>
    <paper id="7">
      <title>Modeling Long-Distance Cue Integration in Spoken Word Recognition</title>
      <author><first>Wednesday</first><last>Bushong</last></author>
      <author><first>T. Florian</first><last>Jaeger</last></author>
      <pages>62–70</pages>
      <abstract>Cues to linguistic categories are distributed across the speech signal. Optimal categorization thus requires that listeners maintain gradient representations of incoming input in order to integrate that information with later cues. There is now evidence that listeners can and do integrate cues that occur far apart in time. Computational models of this integration have however been lacking. We take a first step at addressing this gap by mathematically formalizing four models of how listeners may maintain and use cue information during spoken language understanding and test them on two perception experiments. In one experiment, we find support for rational integration of cues at long distances. In a second, more memory and attention-taxing experiment, we find evidence in favor of a switching model that avoids maintaining detailed representations of cues in memory. These results are a first step in understanding what kinds of mechanisms listeners use for cue integration under different memory and attentional constraints.</abstract>
      <url hash="0cf210ca">W19-2907</url>
      <doi>10.18653/v1/W19-2907</doi>
      <bibkey>bushong-jaeger-2019-modeling</bibkey>
    </paper>
    <paper id="8">
      <title>Toward a Computational Multidimensional Lexical Similarity Measure for Modeling Word Association Tasks in Psycholinguistics</title>
      <author><first>Bruno</first><last>Gaume</last></author>
      <author><first>Lydia</first><last>Mai Ho-Dac</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <author><first>Cécile</first><last>Fabre</last></author>
      <author><first>Bénédicte</first><last>Pierrejean</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Jérôme</first><last>Farinas</last></author>
      <author><first>Julien</first><last>Pinquier</last></author>
      <author><first>Lola</first><last>Danet</last></author>
      <author><first>Patrice</first><last>Péran</last></author>
      <author><first>Xavier</first><last>De Boissezon</last></author>
      <author><first>Mélanie</first><last>Jucla</last></author>
      <pages>71–76</pages>
      <abstract>This paper presents the first results of a multidisciplinary project, the “Evolex” project, gathering researchers in Psycholinguistics, Neuropsychology, Computer Science, Natural Language Processing and Linguistics. The Evolex project aims at proposing a new data-based inductive method for automatically characterising the relation between pairs of french words collected in psycholinguistics experiments on lexical access. This method takes advantage of several complementary computational measures of semantic similarity. We show that some measures are more correlated than others with the frequency of lexical associations, and that they also differ in the way they capture different semantic relations. This allows us to consider building a multidimensional lexical similarity to automate the classification of lexical associations.</abstract>
      <url hash="b12565be">W19-2908</url>
      <doi>10.18653/v1/W19-2908</doi>
      <bibkey>gaume-etal-2019-toward</bibkey>
    </paper>
    <paper id="9">
      <title>Dependency Parsing with your Eyes: Dependency Structure Predicts Eye Regressions During Reading</title>
      <author><first>Alessandro</first><last>Lopopolo</last></author>
      <author><first>Stefan L.</first><last>Frank</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <author><first>Roel</first><last>Willems</last></author>
      <pages>77–85</pages>
      <abstract>Backward saccades during reading have been hypothesized to be involved in structural reanalysis, or to be related to the level of text difficulty. We test the hypothesis that backward saccades are involved in online syntactic analysis. If this is the case we expect that saccades will coincide, at least partially, with the edges of the relations computed by a dependency parser. In order to test this, we analyzed a large eye-tracking dataset collected while 102 participants read three short narrative texts. Our results show a relation between backward saccades and the syntactic structure of sentences.</abstract>
      <url hash="4c441515">W19-2909</url>
      <doi>10.18653/v1/W19-2909</doi>
      <bibkey>lopopolo-etal-2019-dependency</bibkey>
    </paper>
    <paper id="10">
      <title>A Framework for Decoding Event-Related Potentials from Text</title>
      <author><first>Shaorong</first><last>Yan</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <pages>86–92</pages>
      <abstract>We propose a novel framework for modeling event-related potentials (ERPs) collected during reading that couples pre-trained convolutional decoders with a language model. Using this framework, we compare the abilities of a variety of existing and novel sentence processing models to reconstruct ERPs. We find that modern contextual word embeddings underperform surprisal-based models but that, combined, the two outperform either on its own.</abstract>
      <url hash="ca2f30ce">W19-2910</url>
      <doi>10.18653/v1/W19-2910</doi>
      <bibkey>yan-white-2019-framework</bibkey>
    </paper>
    <paper id="11">
      <title>Testing a <fixed-case>M</fixed-case>inimalist <fixed-case>G</fixed-case>rammar Parser on <fixed-case>I</fixed-case>talian Relative Clause Asymmetries</title>
      <author><first>Aniello</first><last>De Santo</last></author>
      <pages>93–104</pages>
      <abstract>Stabler’s (2013) top-down parser for Minimalist grammars has been used to account for off-line processing preferences across a variety of seemingly unrelated phenomena cross-linguistically, via complexity metrics measuring “memory burden”. This paper extends the empirical coverage of the model by looking at the processing asymmetries of Italian relative clauses, as I discuss the relevance of these constructions in evaluating plausible structure-driven models of processing difficulty.</abstract>
      <url hash="b8159757">W19-2911</url>
      <doi>10.18653/v1/W19-2911</doi>
      <bibkey>de-santo-2019-testing</bibkey>
    </paper>
    <paper id="12">
      <title>Quantifiers in a Multimodal World: Hallucinating Vision with Language and Sound</title>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>105–116</pages>
      <abstract>Inspired by the literature on multisensory integration, we develop a computational model to ground quantifiers in perception. The model learns to pick, out of nine quantifiers (‘few’, ‘many’, ‘all’, etc.), the one that is more likely to describe the percent of animals in a visual-auditory input containing both animals and artifacts. We show that relying on concurrent sensory inputs increases model performance on the quantification task. Moreover, we evaluate the model in a situation in which only the auditory modality is given, while the visual one is ‘hallucinanted’ either from the auditory input itself or from a linguistic caption describing the quantity of entities in the auditory input. This way, the model exploits prior associations between modalities. We show that the model profits from the prior knowledge and outperforms the auditory-only setting.</abstract>
      <url hash="49d4e7e8">W19-2912</url>
      <doi>10.18653/v1/W19-2912</doi>
      <bibkey>testoni-etal-2019-quantifiers</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/audioset">AudioSet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="13">
      <title>Frequency vs. Association for Constraint Selection in Usage-Based Construction Grammar</title>
      <author><first>Jonathan</first><last>Dunn</last></author>
      <pages>117–128</pages>
      <abstract>A usage-based Construction Grammar (CxG) posits that slot-constraints generalize from common exemplar constructions. But what is the best model of constraint generalization? This paper evaluates competing frequency-based and association-based models across eight languages using a metric derived from the Minimum Description Length paradigm. The experiments show that association-based models produce better generalizations across all languages by a significant margin.</abstract>
      <url hash="afe1acea">W19-2913</url>
      <attachment type="software" hash="4e8f73f4">W19-2913.Software.zip</attachment>
      <doi>10.18653/v1/W19-2913</doi>
      <bibkey>dunn-2019-frequency</bibkey>
    </paper>
    <paper id="14">
      <title>The Development of Abstract Concepts in Children’s Early Lexical Networks</title>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <author><first>Isaac</first><last>Scheinfeld</last></author>
      <author><first>Michael</first><last>Frank</last></author>
      <pages>129–133</pages>
      <abstract>How do children learn abstract concepts such as animal vs. artifact? Previous research has suggested that such concepts can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a model where we represent the children’ developing lexicon as an evolving network. The nodes of this network are based on vocabulary knowledge as reported by parents, and the edges between pairs of nodes are based on the probability of their co-occurrence in a corpus of child-directed speech. We found that several abstract categories can be identified as the dense regions in such networks. In addition, our simulations suggest that these categories develop simultaneously, rather than sequentially, thanks to the children’s word learning trajectory which favors the exploration of the global conceptual space.</abstract>
      <url hash="c0e74897">W19-2914</url>
      <doi>10.18653/v1/W19-2914</doi>
      <bibkey>fourtassi-etal-2019-development</bibkey>
    </paper>
    <paper id="15">
      <title>Verb-Second Effect on Quantifier Scope Interpretation</title>
      <author><first>Asad</first><last>Sayeed</last></author>
      <author><first>Matthias</first><last>Lindemann</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>134–139</pages>
      <abstract>Sentences like “Every child climbed a tree” have at least two interpretations depending on the precedence order of the universal quantifier and the indefinite. Previous experimental work explores the role that different mechanisms such as semantic reanalysis and world knowledge may have in enabling each interpretation. This paper discusses a web-based task that uses the verb-second characteristic of German main clauses to estimate the influence of word order variation over world knowledge.</abstract>
      <url hash="f6e49434">W19-2915</url>
      <doi>10.18653/v1/W19-2915</doi>
      <bibkey>sayeed-etal-2019-verb</bibkey>
    </paper>
    <paper id="16">
      <title>Neural Models of the Psychosemantics of ‘Most’</title>
      <author><first>Lewis</first><last>O’Sullivan</last></author>
      <author><first>Shane</first><last>Steinert-Threlkeld</last></author>
      <pages>140–151</pages>
      <abstract>How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network – a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) – on the “most” verification task from Pietroski2009, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks.</abstract>
      <url hash="f08341e0">W19-2916</url>
      <doi>10.18653/v1/W19-2916</doi>
      <bibkey>osullivan-steinert-threlkeld-2019-neural</bibkey>
      <pwccode url="https://github.com/shanest/neural-vision-most" additional="false">shanest/neural-vision-most</pwccode>
    </paper>
    <paper id="17">
      <title>The Role of Utterance Boundaries and Word Frequencies for Part-of-speech Learning in <fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese Through Distributional Analysis</title>
      <author><first>Pablo Picasso</first><last>Feliciano de Faria</last></author>
      <pages>152–159</pages>
      <abstract>In this study, we address the problem of part-of-speech (or syntactic category) learning during language acquisition through distributional analysis of utterances. A model based on Redington et al.’s (1998) distributional learner is used to investigate the informativeness of distributional information in Brazilian Portuguese (BP). The data provided to the learner comes from two publicly available corpora of child directed speech. We present preliminary results from two experiments. The first one investigates the effects of different assumptions about utterance boundaries when presenting the input data to the learner. The second experiment compares the learner’s performance when counting contextual words’ frequencies versus just acknowledging their co-occurrence with a given target word. In general, our results indicate that explicit boundaries are more informative, frequencies are important, and that distributional information is useful to the child as a source of categorial information. These results are in accordance with Redington et al.’s findings for English.</abstract>
      <url hash="a9a56edc">W19-2917</url>
      <doi>10.18653/v1/W19-2917</doi>
      <bibkey>feliciano-de-faria-2019-role</bibkey>
    </paper>
    <paper id="18">
      <title>Using Grounded Word Representations to Study Theories of Lexical Concepts</title>
      <author><first>Dylan</first><last>Ebert</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>160–169</pages>
      <abstract>The fields of cognitive science and philosophy have proposed many different theories for how humans represent “concepts”. Multiple such theories are compatible with state-of-the-art NLP methods, and could in principle be operationalized using neural networks. We focus on two particularly prominent theories–Classical Theory and Prototype Theory–in the context of visually-grounded lexical representations. We compare when and how the behavior of models based on these theories differs in terms of categorization and entailment tasks. Our preliminary results suggest that Classical-based representations perform better for entailment and Prototype-based representations perform better for categorization. We discuss plans for additional experiments needed to confirm these initial observations.</abstract>
      <url hash="29e504b3">W19-2918</url>
      <doi>10.18653/v1/W19-2918</doi>
      <bibkey>ebert-pavlick-2019-using</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hyperlex">HyperLex</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
  </volume>
  <volume id="30">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</booktitle>
      <url hash="7a0a4fc4">W19-30</url>
      <editor><first>Kate</first><last>Niederhoffer</last></editor>
      <editor><first>Kristy</first><last>Hollingshead</last></editor>
      <editor><first>Philip</first><last>Resnik</last></editor>
      <editor><first>Rebecca</first><last>Resnik</last></editor>
      <editor><first>Kate</first><last>Loveys</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Minneapolis, Minnesota</address>
      <month>June</month>
      <year>2019</year>
      <venue>clpsych</venue>
    </meta>
    <frontmatter>
      <url hash="7ffe6229">W19-3000</url>
      <bibkey>ws-2019-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards augmenting crisis counselor training by improving message retrieval</title>
      <author><first>Orianna</first><last>Demasi</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <author><first>Benjamin</first><last>Recht</last></author>
      <pages>1–11</pages>
      <abstract>A fundamental challenge when training counselors is presenting novices with the opportunity to practice counseling distressed individuals without exacerbating a situation. Rather than replacing human empathy with an automated counselor, we propose simulating an individual in crisis so that human counselors in training can practice crisis counseling in a low-risk environment. Towards this end, we collect a dataset of suicide prevention counselor role-play transcripts and make initial steps towards constructing a CRISISbot for humans to counsel while in training. In this data-constrained setting, we evaluate the potential for message retrieval to construct a coherent chat agent in light of recent advances with text embedding methods. Our results show that embeddings can considerably improve retrieval approaches to make them competitive with generative models. By coherently retrieving messages, we can help counselors practice chatting in a low-risk environment.</abstract>
      <url hash="65c43839">W19-3001</url>
      <doi>10.18653/v1/W19-3001</doi>
      <bibkey>demasi-etal-2019-towards</bibkey>
    </paper>
    <paper id="2">
      <title>Identifying therapist conversational actions across diverse psychotherapeutic approaches</title>
      <author><first>Fei-Tzin</first><last>Lee</last></author>
      <author><first>Derrick</first><last>Hull</last></author>
      <author><first>Jacob</first><last>Levine</last></author>
      <author><first>Bonnie</first><last>Ray</last></author>
      <author><first>Kathy</first><last>McKeown</last></author>
      <pages>12–23</pages>
      <abstract>While conversation in therapy sessions can vary widely in both topic and style, an understanding of the underlying techniques used by therapists can provide valuable insights into how therapists best help clients of different types. Dialogue act classification aims to identify the conversational “action” each speaker takes at each utterance, such as sympathizing, problem-solving or assumption checking. We propose to apply dialogue act classification to therapy transcripts, using a therapy-specific labeling scheme, in order to gain a high-level understanding of the flow of conversation in therapy sessions. We present a novel annotation scheme that spans multiple psychotherapeutic approaches, apply it to a large and diverse corpus of psychotherapy transcripts, and present and discuss classification results obtained using both SVM and neural network-based models. The results indicate that identifying the structure and flow of therapeutic actions is an obtainable goal, opening up the opportunity in the future to provide therapeutic recommendations tailored to specific client situations.</abstract>
      <url hash="84a2f0c1">W19-3002</url>
      <doi>10.18653/v1/W19-3002</doi>
      <bibkey>lee-etal-2019-identifying</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>CLP</fixed-case>sych 2019 Shared Task: Predicting the Degree of Suicide Risk in <fixed-case>R</fixed-case>eddit Posts</title>
      <author><first>Ayah</first><last>Zirikly</last></author>
      <author><first>Philip</first><last>Resnik</last></author>
      <author><first>Özlem</first><last>Uzuner</last></author>
      <author><first>Kristy</first><last>Hollingshead</last></author>
      <pages>24–33</pages>
      <abstract>The shared task for the 2019 Workshop on Computational Linguistics and Clinical Psychology (CLPsych’19) introduced an assessment of suicide risk based on social media postings, using data from Reddit to identify users at no, low, moderate, or severe risk. Two variations of the task focused on users whose posts to the r/SuicideWatch subreddit indicated they might be at risk; a third task looked at screening users based only on their more everyday (non-SuicideWatch) posts. We received submissions from 15 different teams, and the results provide progress and insight into the value of language signal in helping to predict risk level.</abstract>
      <url hash="0cb0c937">W19-3003</url>
      <doi>10.18653/v1/W19-3003</doi>
      <bibkey>zirikly-etal-2019-clpsych</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>CL</fixed-case>a<fixed-case>C</fixed-case> at <fixed-case>CLP</fixed-case>sych 2019: Fusion of Neural Features and Predicted Class Probabilities for Suicide Risk Assessment Based on Online Posts</title>
      <author><first>Elham</first><last>Mohammadi</last></author>
      <author><first>Hessam</first><last>Amini</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>34–38</pages>
      <abstract>This paper summarizes our participation to the CLPsych 2019 shared task, under the name CLaC. The goal of the shared task was to detect and assess suicide risk based on a collection of online posts. For our participation, we used an ensemble method which utilizes 8 neural sub-models to extract neural features and predict class probabilities, which are then used by an SVM classifier. Our team ranked first in 2 out of the 3 tasks (tasks A and C).</abstract>
      <url hash="3424a531">W19-3004</url>
      <doi>10.18653/v1/W19-3004</doi>
      <bibkey>mohammadi-etal-2019-clac-clpsych</bibkey>
    </paper>
    <paper id="5">
      <title>Suicide Risk Assessment with Multi-level Dual-Context Language and <fixed-case>BERT</fixed-case></title>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>Akash</first><last>Idnani</last></author>
      <author><first>Youngseo</first><last>Son</last></author>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <author><first>Huy</first><last>Vu</last></author>
      <author><first>Mohammad</first><last>Zamani</last></author>
      <author><first>Parth</first><last>Limbachiya</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>39–44</pages>
      <abstract>Mental health predictive systems typically model language as if from a single context (e.g. Twitter posts, status updates, or forum posts) and often limited to a single level of analysis (e.g. either the message-level or user-level). Here, we bring these pieces together to explore the use of open-vocabulary (BERT embeddings, topics) and theoretical features (emotional expression lexica, personality) for the task of suicide risk assessment on support forums (the CLPsych-2019 Shared Task). We used dual context based approaches (modeling content from suicide forums separate from other content), built over both traditional ML models as well as a novel dual RNN architecture with user-factor adaptation. We find that while affect from the suicide context distinguishes with no-risk from those with “any-risk”, personality factors from the non-suicide contexts provide distinction of the levels of risk: low, medium, and high risk. Within the shared task, our dual-context approach (listed as SBU-HLAB in the official results) achieved state-of-the-art performance predicting suicide risk using a combination of suicide-context and non-suicide posts (Task B), achieving an F1 score of 0.50 over hidden test set labels.</abstract>
      <url hash="706ce849">W19-3005</url>
      <doi>10.18653/v1/W19-3005</doi>
      <bibkey>matero-etal-2019-suicide</bibkey>
    </paper>
    <paper id="6">
      <title>Using natural conversations to classify autism with limited data: Age matters</title>
      <author><first>Michael</first><last>Hauser</last></author>
      <author><first>Evangelos</first><last>Sariyanidi</last></author>
      <author><first>Birkan</first><last>Tunc</last></author>
      <author><first>Casey</first><last>Zampella</last></author>
      <author><first>Edward</first><last>Brodkin</last></author>
      <author><first>Robert</first><last>Schultz</last></author>
      <author><first>Julia</first><last>Parish-Morris</last></author>
      <pages>45–54</pages>
      <abstract>Spoken language ability is highly heterogeneous in Autism Spectrum Disorder (ASD), which complicates efforts to identify linguistic markers for use in diagnostic classification, clinical characterization, and for research and clinical outcome measurement. Machine learning techniques that harness the power of multivariate statistics and non-linear data analysis hold promise for modeling this heterogeneity, but many models require enormous datasets, which are unavailable for most psychiatric conditions (including ASD). In lieu of such datasets, good models can still be built by leveraging domain knowledge. In this study, we compare two machine learning approaches: the first approach incorporates prior knowledge about language variation across middle childhood, adolescence, and adulthood to classify 6-minute naturalistic conversation samples from 140 age- and IQ-matched participants (81 with ASD), while the other approach treats all ages the same. We found that individual age-informed models were significantly more accurate than a single model tasked with building a common algorithm across age groups. Furthermore, predictive linguistic features differed significantly by age group, confirming the importance of considering age-related changes in language use when classifying ASD. Our results suggest that limitations imposed by heterogeneity inherent to ASD and from developmental change with age can be (at least partially) overcome using domain knowledge, such as understanding spoken language development from childhood through adulthood.</abstract>
      <url hash="daadf4ab">W19-3006</url>
      <doi>10.18653/v1/W19-3006</doi>
      <bibkey>hauser-etal-2019-using</bibkey>
    </paper>
    <paper id="7">
      <title>The importance of sharing patient-generated clinical speech and language data</title>
      <author><first>Kathleen C.</first><last>Fraser</last></author>
      <author><first>Nicklas</first><last>Linz</last></author>
      <author><first>Hali</first><last>Lindsay</last></author>
      <author><first>Alexandra</first><last>König</last></author>
      <pages>55–61</pages>
      <abstract>Increased access to large datasets has driven progress in NLP. However, most computational studies of clinically-validated, patient-generated speech and language involve very few datapoints, as such data are difficult (and expensive) to collect. In this position paper, we argue that we must find ways to promote data sharing across research groups, in order to build datasets of a more appropriate size for NLP and machine learning analysis. We review the benefits and challenges of sharing clinical language data, and suggest several concrete actions by both clinical and NLP researchers to encourage multi-site and multi-disciplinary data sharing. We also propose the creation of a collaborative data sharing platform, to allow NLP researchers to take a more active responsibility for data transcription, annotation, and curation.</abstract>
      <url hash="2f11a2c3">W19-3007</url>
      <doi>10.18653/v1/W19-3007</doi>
      <bibkey>fraser-etal-2019-importance</bibkey>
    </paper>
    <paper id="8">
      <title>Depressed Individuals Use Negative Self-Focused Language When Recalling Recent Interactions with Close Romantic Partners but Not Family or <fixed-case>F</fixed-case>riends</title>
      <author><first>Taleen</first><last>Nalabandian</last></author>
      <author><first>Molly</first><last>Ireland</last></author>
      <pages>62–73</pages>
      <abstract>Depression is characterized by a self-focused negative attentional bias, which is often reflected in everyday language use. In a prospective writing study, we explored whether the association between depressive symptoms and negative, self-focused language varies across social contexts. College students (N = 243) wrote about a recent interaction with a person they care deeply about. Depression symptoms positively correlated with negative emotion words and first-person singular pronouns (or negative self-focus) when writing about a recent interaction with romantic partners or, to a lesser extent, friends, but not family members. The pattern of results was more pronounced when participants perceived greater self-other overlap (i.e., interpersonal closeness) with their romantic partner. Findings regarding how the linguistic profile of depression differs by type of relationship may inform more effective methods of clinical diagnosis and treatment.</abstract>
      <url hash="c13848e0">W19-3008</url>
      <doi>10.18653/v1/W19-3008</doi>
      <bibkey>nalabandian-ireland-2019-depressed</bibkey>
    </paper>
    <paper id="9">
      <title>Linguistic Analysis of Schizophrenia in <fixed-case>R</fixed-case>eddit Posts</title>
      <author><first>Jonathan</first><last>Zomick</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <author><first>Mark</first><last>Serper</last></author>
      <pages>74–83</pages>
      <abstract>We explore linguistic indicators of schizophrenia in Reddit discussion forums. Schizophrenia (SZ) is a chronic mental disorder that affects a person’s thoughts and behaviors. Identifying and detecting signs of SZ is difficult given that SZ is relatively uncommon, affecting approximately 1% of the US population, and people suffering with SZ often believe that they do not have the disorder. Linguistic abnormalities are a hallmark of SZ and many of the illness’s symptoms are manifested through language. In this paper we leverage the vast amount of data available from social media and use statistical and machine learning approaches to study linguistic characteristics of SZ. We collected and analyzed a large corpus of Reddit posts from users claiming to have received a formal diagnosis of SZ and identified several linguistic features that differentiated these users from a control (CTL) group. We compared these results to other findings on social media linguistic analysis and SZ. We also developed a machine learning classifier to automatically identify self-identified users with SZ on Reddit.</abstract>
      <url hash="428674cb">W19-3009</url>
      <doi>10.18653/v1/W19-3009</doi>
      <bibkey>zomick-etal-2019-linguistic</bibkey>
    </paper>
    <paper id="10">
      <title>Semantic Characteristics of Schizophrenic Speech</title>
      <author><first>Kfir</first><last>Bar</last></author>
      <author><first>Vered</first><last>Zilberstein</last></author>
      <author><first>Ido</first><last>Ziv</last></author>
      <author><first>Heli</first><last>Baram</last></author>
      <author><first>Nachum</first><last>Dershowitz</last></author>
      <author><first>Samuel</first><last>Itzikowitz</last></author>
      <author><first>Eiran</first><last>Vadim Harel</last></author>
      <pages>84–93</pages>
      <abstract>Natural language processing tools are used to automatically detect disturbances in transcribed speech of schizophrenia inpatients who speak Hebrew. We measure topic mutation over time and show that controls maintain more cohesive speech than inpatients. We also examine differences in how inpatients and controls use adjectives and adverbs to describe content words and show that the ones used by controls are more common than the those of inpatients. We provide experimental results and show their potential for automatically detecting schizophrenia in patients by means only of their speech patterns.</abstract>
      <url hash="abe6a915">W19-3010</url>
      <doi>10.18653/v1/W19-3010</doi>
      <bibkey>bar-etal-2019-semantic</bibkey>
    </paper>
    <paper id="11">
      <title>Computational Linguistics for Enhancing Scientific Reproducibility and Reducing Healthcare Inequities</title>
      <author><first>Julia</first><last>Parish-Morris</last></author>
      <pages>94–102</pages>
      <abstract>Computational linguistics holds promise for improving scientific integrity in clinical psychology, and for reducing longstanding inequities in healthcare access and quality. This paper describes how computational linguistics approaches could address the “reproducibility crisis” facing social science, particularly with regards to reliable diagnosis of neurodevelopmental and psychiatric conditions including autism spectrum disorder (ASD). It is argued that these improvements in scientific integrity are poised to naturally reduce persistent healthcare inequities in neglected subpopulations, such as verbally fluent girls and women with ASD, but that concerted attention to this issue is necessary to avoid reproducing biases built into training data. Finally, it is suggested that computational linguistics is just one component of an emergent digital phenotyping toolkit that could ultimately be used for clinical decision support, to improve clinical care via precision medicine (i.e., personalized intervention planning), granular treatment response monitoring (including remotely), and for gene-brain-behavior studies aiming to pinpoint the underlying biological etiology of otherwise behaviorally-defined conditions like ASD.</abstract>
      <url hash="d8daccdd">W19-3011</url>
      <doi>10.18653/v1/W19-3011</doi>
      <bibkey>parish-morris-2019-computational</bibkey>
    </paper>
    <paper id="12">
      <title>Temporal Analysis of the Semantic Verbal Fluency Task in Persons with Subjective and Mild Cognitive Impairment</title>
      <author><first>Nicklas</first><last>Linz</last></author>
      <author><first>Kristina</first><last>Lundholm Fors</last></author>
      <author><first>Hali</first><last>Lindsay</last></author>
      <author><first>Marie</first><last>Eckerström</last></author>
      <author><first>Jan</first><last>Alexandersson</last></author>
      <author><first>Dimitrios</first><last>Kokkinakis</last></author>
      <pages>103–113</pages>
      <abstract>The Semantic Verbal Fluency (SVF) task is a classical neuropsychological assessment where persons are asked to produce words belonging to a semantic category (e.g., animals) in a given time. This paper introduces a novel method of temporal analysis for SVF tasks utilizing time intervals and applies it to a corpus of elderly Swedish subjects (mild cognitive impairment, subjective cognitive impairment and healthy controls). A general decline in word count and lexical frequency over the course of the task is revealed, as well as an increase in word transition times. Persons with subjective cognitive impairment had a higher word count during the last intervals, but produced words of the same lexical frequencies. Persons with MCI had a steeper decline in both word count and lexical frequencies during the third interval. Additional correlations with neuropsychological scores suggest these findings are linked to a person’s overall vocabulary size and processing speed, respectively. Classification results improved when adding the novel features (AUC=0.72), supporting their diagnostic value.</abstract>
      <url hash="6f4c6da8">W19-3012</url>
      <doi>10.18653/v1/W19-3012</doi>
      <bibkey>linz-etal-2019-temporal</bibkey>
    </paper>
    <paper id="13">
      <title>Mental Health Surveillance over Social Media with Digital Cohorts</title>
      <author><first>Silvio</first><last>Amir</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <author><first>John W.</first><last>Ayers</last></author>
      <pages>114–120</pages>
      <abstract>The ability to track mental health conditions via social media opened the doors for large-scale, automated, mental health surveillance. However, inferring accurate population-level trends requires representative samples of the underlying population, which can be challenging given the biases inherent in social media data. While previous work has adjusted samples based on demographic estimates, the populations were selected based on specific outcomes, e.g. specific mental health conditions. We depart from these methods, by conducting analyses over demographically representative digital cohorts of social media users. To validated this approach, we constructed a cohort of US based Twitter users to measure the prevalence of depression and PTSD, and investigate how these illnesses manifest across demographic subpopulations. The analysis demonstrates that cohort-based studies can help control for sampling biases, contextualize outcomes, and provide deeper insights into the data.</abstract>
      <url hash="6f2438fe">W19-3013</url>
      <doi>10.18653/v1/W19-3013</doi>
      <bibkey>amir-etal-2019-mental</bibkey>
    </paper>
    <paper id="14">
      <title>Reviving a psychometric measure: Classification and prediction of the Operant Motive Test</title>
      <author><first>Dirk</first><last>Johannßen</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <author><first>David</first><last>Scheffer</last></author>
      <pages>121–125</pages>
      <abstract>Implicit motives allow for the characterization of behavior, subsequent success and long-term development. While this has been operationalized in the operant motive test, research on motives has declined mainly due to labor-intensive and costly human annotation. In this study, we analyze over 200,000 labeled data items from 40,000 participants and utilize them for engineering features for training a logistic model tree machine learning model. It captures manually assigned motives well with an F-score of 80%, coming close to the pairwise annotator intraclass correlation coefficient of r = .85. In addition, we found a significant correlation of r = .2 between subsequent academic success and data automatically labeled with our model in an extrinsic evaluation.</abstract>
      <url hash="cd37fab9">W19-3014</url>
      <doi>10.18653/v1/W19-3014</doi>
      <bibkey>johannssen-etal-2019-reviving</bibkey>
    </paper>
    <paper id="15">
      <title>Coherence models in schizophrenia</title>
      <author><first>Sandra</first><last>Just</last></author>
      <author><first>Erik</first><last>Haegert</last></author>
      <author><first>Nora</first><last>Kořánová</last></author>
      <author><first>Anna-Lena</first><last>Bröcker</last></author>
      <author><first>Ivan</first><last>Nenchev</last></author>
      <author><first>Jakob</first><last>Funcke</last></author>
      <author><first>Christiane</first><last>Montag</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>126–136</pages>
      <abstract>Incoherent discourse in schizophrenia has long been recognized as a dominant symptom of the mental disorder (Bleuler, 1911/1950). Recent studies have used modern sentence and word embeddings to compute coherence metrics for spontaneous speech in schizophrenia. While clinical ratings always have a subjective element, computational linguistic methodology allows quantification of speech abnormalities. Clinical and empirical knowledge from psychiatry provide the theoretical and conceptual basis for modelling. Our study is an interdisciplinary attempt at improving coherence models in schizophrenia. Speech samples were obtained from healthy controls and patients with a diagnosis of schizophrenia or schizoaffective disorder and different severity of positive formal thought disorder. Interviews were transcribed and coherence metrics derived from different embeddings. One model found higher coherence metrics for controls than patients. All other models remained non-significant. More detailed analysis of the data motivates different approaches to improving coherence models in schizophrenia, e.g. by assessing referential abnormalities.</abstract>
      <url hash="8ca49752">W19-3015</url>
      <doi>10.18653/v1/W19-3015</doi>
      <bibkey>just-etal-2019-coherence</bibkey>
    </paper>
    <paper id="16">
      <title>Overcoming the bottleneck in traditional assessments of verbal memory: Modeling human ratings and classifying clinical group membership</title>
      <author><first>Chelsea</first><last>Chandler</last></author>
      <author><first>Peter W.</first><last>Foltz</last></author>
      <author><first>Jian</first><last>Cheng</last></author>
      <author><first>Jared C.</first><last>Bernstein</last></author>
      <author><first>Elizabeth P.</first><last>Rosenfeld</last></author>
      <author><first>Alex S.</first><last>Cohen</last></author>
      <author><first>Terje B.</first><last>Holmlund</last></author>
      <author><first>Brita</first><last>Elvevåg</last></author>
      <pages>137–147</pages>
      <abstract>Verbal memory is affected by numerous clinical conditions and most neuropsychological and clinical examinations evaluate it. However, a bottleneck exists in such endeavors because traditional methods require expert human review, and usually only a couple of test versions exist, thus limiting the frequency of administration and clinical applications. The present study overcomes this bottleneck by automating the administration, transcription, analysis and scoring of story recall. A large group of healthy participants (n = 120) and patients with mental illness (n = 105) interacted with a mobile application that administered a wide range of assessments, including verbal memory. The resulting speech generated by participants when retelling stories from the memory task was transcribed using automatic speech recognition tools, which was compared with human transcriptions (overall word error rate = 21%). An assortment of surface-level and semantic language-based features were extracted from the verbal recalls. A final set of three features were used to both predict expert human ratings with a ridge regression model (r = 0.88) and to differentiate patients from healthy individuals with an ensemble of logistic regression classifiers (accuracy = 76%). This is the first ‘outside of the laboratory’ study to showcase the viability of the complete pipeline of automated assessment of verbal memory in naturalistic settings.</abstract>
      <url hash="3b619c63">W19-3016</url>
      <doi>10.18653/v1/W19-3016</doi>
      <bibkey>chandler-etal-2019-overcoming</bibkey>
    </paper>
    <paper id="17">
      <title>Analyzing the use of existing systems for the <fixed-case>CLP</fixed-case>sych 2019 Shared Task</title>
      <author><first>Alejandro</first><last>González Hevia</last></author>
      <author><first>Rebeca</first><last>Cerezo Menéndez</last></author>
      <author><first>Daniel</first><last>Gayo-Avello</last></author>
      <pages>148–151</pages>
      <abstract>In this paper we describe the UniOvi-WESO classification systems proposed for the 2019 Computational Linguistics and Clinical Psychology (CLPsych) Shared Task. We explore the use of two systems trained with ReachOut data from the 2016 CLPsych task, and compare them to a baseline system trained with the data provided for this task. All the classifiers were trained with features extracted just from the text of each post, without using any other metadata. We found out that the baseline system performs slightly better than the pretrained systems, mainly due to the differences in labeling between the two tasks. However, they still work reasonably well and can detect if a user is at risk of suicide or not.</abstract>
      <url hash="d8b9d931">W19-3017</url>
      <doi>10.18653/v1/W19-3017</doi>
      <bibkey>gonzalez-hevia-etal-2019-analyzing</bibkey>
    </paper>
    <paper id="18">
      <title>Similar Minds Post Alike: Assessment of Suicide Risk Using a Hybrid Model</title>
      <author><first>Lushi</first><last>Chen</last></author>
      <author><first>Abeer</first><last>Aldayel</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Tao</first><last>Gong</last></author>
      <pages>152–157</pages>
      <abstract>This paper describes our system submission for the CLPsych 2019 shared task B on suicide risk assessment. We approached the problem with three separate models: a behaviour model; a language model and a hybrid model. For the behavioral model approach, we model each user’s behaviour and thoughts with four groups of features: posting behaviour, sentiment, motivation, and content of the user’s posting. We use these features as an input in a support vector machine (SVM). For the language model approach, we trained a language model for each risk level using all the posts from the users as the training corpora. Then, we computed the perplexity of each user’s posts to determine how likely his/her posts were to belong to each risk level. Finally, we built a hybrid model that combines both the language model and the behavioral model, which demonstrates the best performance in detecting the suicide risk level.</abstract>
      <url hash="1b4d23c5">W19-3018</url>
      <doi>10.18653/v1/W19-3018</doi>
      <bibkey>chen-etal-2019-similar</bibkey>
    </paper>
    <paper id="19">
      <title>Predicting Suicide Risk from Online Postings in <fixed-case>R</fixed-case>eddit The <fixed-case>UG</fixed-case>ent-<fixed-case>IDL</fixed-case>ab submission to the <fixed-case>CLP</fixed-case>ysch 2019 Shared Task A</title>
      <author><first>Semere Kiros</first><last>Bitew</last></author>
      <author><first>Giannis</first><last>Bekoulis</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Lucas</first><last>Sterckx</last></author>
      <author><first>Klim</first><last>Zaporojets</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <pages>158–161</pages>
      <abstract>This paper describes IDLab’s text classification systems submitted to Task A as part of the CLPsych 2019 shared task. The aim of this shared task was to develop automated systems that predict the degree of suicide risk of people based on their posts on Reddit. Bag-of-words features, emotion features and post level predictions are used to derive user-level predictions. Linear models and ensembles of these models are used to predict final scores. We find that predicting fine-grained risk levels is much more difficult than flagging potentially at-risk users. Furthermore, we do not find clear added value from building richer ensembles compared to simple baselines, given the available training data and the nature of the prediction task.</abstract>
      <url hash="8d19ef91">W19-3019</url>
      <doi>10.18653/v1/W19-3019</doi>
      <bibkey>bitew-etal-2019-predicting</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>CLP</fixed-case>sych2019 Shared Task: Predicting Suicide Risk Level from <fixed-case>R</fixed-case>eddit Posts on Multiple Forums</title>
      <author><first>Victor</first><last>Ruiz</last></author>
      <author><first>Lingyun</first><last>Shi</last></author>
      <author><first>Wei</first><last>Quan</last></author>
      <author><first>Neal</first><last>Ryan</last></author>
      <author><first>Candice</first><last>Biernesser</last></author>
      <author><first>David</first><last>Brent</last></author>
      <author><first>Rich</first><last>Tsui</last></author>
      <pages>162–166</pages>
      <abstract>We aimed to predict an individual suicide risk level from longitudinal posts on Reddit discussion forums. Through participating in a shared task competition hosted by CLPsych2019, we received two annotated datasets: a training dataset with 496 users (31,553 posts) and a test dataset with 125 users (9610 posts). We submitted results from our three best-performing machine-learning models: SVM, Naïve Bayes, and an ensemble model. Each model provided a user’s suicide risk level in four categories, i.e., no risk, low risk, moderate risk, and severe risk. Among the three models, the ensemble model had the best macro-averaged F1 score 0.379 when tested on the holdout test dataset. The NB model had the best performance in two additional binary-classification tasks, i.e., no risk vs. flagged risk (any risk level other than no risk) with F1 score 0.836 and no or low risk vs. urgent risk (moderate or severe risk) with F1 score 0.736. We conclude that the NB model may serve as a tool for identifying users with flagged or urgent suicide risk based on longitudinal posts on Reddit discussion forums.</abstract>
      <url hash="fcf7a666">W19-3020</url>
      <doi>10.18653/v1/W19-3020</doi>
      <bibkey>ruiz-etal-2019-clpsych2019</bibkey>
    </paper>
    <paper id="21">
      <title>Suicide Risk Assessment on Social Media: <fixed-case>USI</fixed-case>-<fixed-case>UPF</fixed-case> at the <fixed-case>CLP</fixed-case>sych 2019 Shared Task</title>
      <author><first>Esteban</first><last>Ríssola</last></author>
      <author><first>Diana</first><last>Ramírez-Cifuentes</last></author>
      <author><first>Ana</first><last>Freire</last></author>
      <author><first>Fabio</first><last>Crestani</last></author>
      <pages>167–171</pages>
      <abstract>This paper describes the participation of the USI-UPF team at the shared task of the 2019 Computational Linguistics and Clinical Psychology Workshop (CLPsych2019). The goal is to assess the degree of suicide risk of social media users given a labelled dataset with their posts. An appropriate suicide risk assessment, with the usage of automated methods, can assist experts on the detection of people at risk and eventually contribute to prevent suicide. We propose a set of machine learning models with features based on lexicons, word embeddings, word level n-grams, and statistics extracted from users’ posts. The results show that the most effective models for the tasks are obtained integrating lexicon-based features, a selected set of n-grams, and statistical measures.</abstract>
      <url hash="b8cc01c9">W19-3021</url>
      <doi>10.18653/v1/W19-3021</doi>
      <bibkey>rissola-etal-2019-suicide</bibkey>
    </paper>
    <paper id="22">
      <title>Using Contextual Representations for Suicide Risk Assessment from <fixed-case>I</fixed-case>nternet Forums</title>
      <author><first>Ashwin Karthik</first><last>Ambalavanan</last></author>
      <author><first>Pranjali Dileep</first><last>Jagtap</last></author>
      <author><first>Soumya</first><last>Adhya</last></author>
      <author><first>Murthy</first><last>Devarakonda</last></author>
      <pages>172–176</pages>
      <abstract>Social media posts may yield clues to the subject’s (usually, the writer’s) suicide risk and intent, which can be used for timely intervention. This research, motivated by the CLPsych 2019 shared task, developed neural network-based methods for analyzing posts in one or more Reddit forums to assess the subject’s suicide risk. One of the technical challenges this task poses is the large amount of text from multiple posts of a single user. Our neural network models use the advanced multi-headed Attention-based autoencoder architecture, called Bidirectional Encoder Representations from Transformers (BERT). Our system achieved the 2nd best performance of 0.477 macro averaged F measure on Task A of the challenge. Among the three different alternatives we developed for the challenge, the single BERT model that processed all of a user’s posts performed the best on all three Tasks.</abstract>
      <url hash="7ce9458a">W19-3022</url>
      <doi>10.18653/v1/W19-3022</doi>
      <bibkey>ambalavanan-etal-2019-using</bibkey>
    </paper>
    <paper id="23">
      <title>An Investigation of Deep Learning Systems for Suicide Risk Assessment</title>
      <author><first>Michelle</first><last>Morales</last></author>
      <author><first>Prajjalita</first><last>Dey</last></author>
      <author><first>Thomas</first><last>Theisen</last></author>
      <author><first>Danny</first><last>Belitz</last></author>
      <author><first>Natalia</first><last>Chernova</last></author>
      <pages>177–181</pages>
      <abstract>This work presents the systems explored as part of the CLPsych 2019 Shared Task. More specifically, this work explores the promise of deep learning systems for suicide risk assessment.</abstract>
      <url hash="9facd832">W19-3023</url>
      <doi>10.18653/v1/W19-3023</doi>
      <bibkey>morales-etal-2019-investigation</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>C</fixed-case>onv<fixed-case>S</fixed-case>ent at <fixed-case>CLP</fixed-case>sych 2019 Task A: Using Post-level Sentiment Features for Suicide Risk Prediction on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Kristen</first><last>Allen</last></author>
      <author><first>Shrey</first><last>Bagroy</last></author>
      <author><first>Alex</first><last>Davis</last></author>
      <author><first>Tamar</first><last>Krishnamurti</last></author>
      <pages>182–187</pages>
      <abstract>This work aims to infer mental health status from public text for early detection of suicide risk. It contributes to Shared Task A in the 2019 CLPsych workshop by predicting users’ suicide risk given posts in the Reddit subforum r/SuicideWatch. We use a convolutional neural network to incorporate LIWC information at the Reddit post level about topics discussed, first-person focus, emotional experience, grammatical choices, and thematic style. In sorting users into one of four risk categories, our best system’s macro-averaged F1 score was 0.50 on the withheld test set. The work demonstrates the predictive power of the Linguistic Inquiry and Word Count dictionary, in conjunction with a convolutional network and holistic consideration of each post and user.</abstract>
      <url hash="0e1f8339">W19-3024</url>
      <doi>10.18653/v1/W19-3024</doi>
      <bibkey>allen-etal-2019-convsent</bibkey>
    </paper>
    <paper id="25">
      <title>Dictionaries and Decision Trees for the 2019 <fixed-case>CLP</fixed-case>sych Shared Task</title>
      <author><first>Micah</first><last>Iserman</last></author>
      <author><first>Taleen</first><last>Nalabandian</last></author>
      <author><first>Molly</first><last>Ireland</last></author>
      <pages>188–194</pages>
      <abstract>In this summary, we discuss our approach to the CLPsych Shared Task and its initial results. For our predictions in each task, we used a recursive partitioning algorithm (decision trees) to select from our set of features, which were primarily dictionary scores and counts of individual words. We focused primarily on Task A, which aimed to predict suicide risk, as rated by a team of expert clinicians (Shing et al., 2018), based on language used in SuicideWatch posts on Reddit. Category-level findings highlight the potential importance of social and moral language categories. Word-level correlates of risk levels underline the value of fine-grained data-driven approaches, revealing both theory-consistent and potentially novel correlates of suicide risk that may motivate future research.</abstract>
      <url hash="b6084b0f">W19-3025</url>
      <doi>10.18653/v1/W19-3025</doi>
      <bibkey>iserman-etal-2019-dictionaries</bibkey>
    </paper>
  </volume>
  <volume id="31" ingest-date="2019-09-22">
    <meta>
      <booktitle>Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing</booktitle>
      <url hash="50377ea7">W19-31</url>
      <editor><first>Heiko</first><last>Vogler</last></editor>
      <editor><first>Andreas</first><last>Maletti</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dresden, Germany</address>
      <month>September</month>
      <year>2019</year>
      <venue>fsmnlp</venue>
    </meta>
    <frontmatter>
      <url hash="8b1788ec">W19-3100</url>
      <bibkey>ws-2019-international-finite</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Grammatical Framework: an Interlingual Grammar Formalism</title>
      <author><first>Aarne</first><last>Ranta</last></author>
      <pages>1–2</pages>
      <url hash="9ac33d8b">W19-3101</url>
      <doi>10.18653/v1/W19-3101</doi>
      <bibkey>ranta-2019-grammatical</bibkey>
    </paper>
    <paper id="2">
      <title>A Survey of Recent Advances in Efficient Parsing for Graph Grammars</title>
      <author><first>Frank</first><last>Drewes</last></author>
      <pages>3</pages>
      <url hash="25a2c3fb">W19-3102</url>
      <doi>10.18653/v1/W19-3102</doi>
      <attachment type="presentation" hash="40c8eb42">W19-3102.Presentation.pdf</attachment>
      <bibkey>drewes-2019-survey</bibkey>
    </paper>
    <paper id="3">
      <title>Latent Variable Grammars for Discontinuous Parsing</title>
      <author><first>Kilian</first><last>Gebhardt</last></author>
      <pages>4–6</pages>
      <url hash="1b59ee43">W19-3103</url>
      <doi>10.18653/v1/W19-3103</doi>
      <bibkey>gebhardt-2019-latent</bibkey>
    </paper>
    <paper id="4">
      <title>Bottom-Up Unranked Tree-to-Graph Transducers for Translation into Semantic Graphs</title>
      <author><first>Johanna</first><last>Björklund</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <pages>7–17</pages>
      <url hash="49f8f431">W19-3104</url>
      <abstract>We propose a formal model for translating unranked syntactic trees, such as dependency trees, into semantic graphs. These tree-to-graph transducers can serve as a formal basis of transition systems for semantic parsing which recently have been shown to perform very well, yet hitherto lack formalization. Our model features “extended” rules and an arc-factored normal form, comes with an efficient translation algorithm, and can be equipped with weights in a straightforward manner.</abstract>
      <doi>10.18653/v1/W19-3104</doi>
      <bibkey>bjorklund-etal-2019-bottom</bibkey>
    </paper>
    <paper id="5">
      <title>On the Compression of Lexicon Transducers</title>
      <author><first>Marco</first><last>Cognetta</last></author>
      <author><first>Cyril</first><last>Allauzen</last></author>
      <author><first>Michael</first><last>Riley</last></author>
      <pages>18–26</pages>
      <url hash="65a06360">W19-3105</url>
      <abstract>In finite-state language processing pipelines, a lexicon is often a key component. It needs to be comprehensive to ensure accuracy, reducing out-of-vocabulary misses. However, in memory-constrained environments (e.g., mobile phones), the size of the component automata must be kept small. Indeed, a delicate balance between comprehensiveness, speed, and memory must be struck to conform to device requirements while providing a good user experience.

In this paper, we describe a compression scheme for lexicons when represented as finite-state transducers. We efficiently encode the graph of the transducer while storing transition labels separately. The graph encoding scheme is based on the LOUDS (Level Order Unary Degree Sequence) tree representation, which has constant time tree traversal for queries while being information-theoretically optimal in space. We find that our encoding is near the theoretical lower bound for such graphs and substantially outperforms more traditional representations in space while remaining competitive in latency benchmarks.</abstract>
      <doi>10.18653/v1/W19-3105</doi>
      <bibkey>cognetta-etal-2019-compression</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>MSO</fixed-case> with tests and reducts</title>
      <author><first>Tim</first><last>Fernando</last></author>
      <author><first>David</first><last>Woods</last></author>
      <author><first>Carl</first><last>Vogel</last></author>
      <pages>27–36</pages>
      <url hash="aed102c1">W19-3106</url>
      <abstract>Tests added to Kleene algebra (by Kozen and others) are considered within Monadic Second Order logic over strings, where they are likened to statives in natural language. Reducts are formed over tests and non-tests alike, specifying what is observable. Notions of temporal granularity are based on observable change, under the assumption that a finite set bounds what is observable (with the possibility of stretching such bounds by moving to a larger finite set). String projections at different granularities are conjoined by superpositions that provide another variant of concatenation for Booleans.</abstract>
      <doi>10.18653/v1/W19-3106</doi>
      <bibkey>fernando-etal-2019-mso</bibkey>
    </paper>
    <paper id="7">
      <title>Finite State Transducer Calculus for Whole Word Morphology</title>
      <author><first>Maciej</first><last>Janicki</last></author>
      <pages>37–45</pages>
      <url hash="62526c9b">W19-3107</url>
      <abstract>The research on machine learning of morphology often involves formulating morphological descriptions directly on surface forms of words. As the established two-level morphology paradigm requires the knowledge of the underlying structure, it is not widely used in such settings. In this paper, we propose a formalism describing structural relationships between words based on theories of morphology that reject the notions of internal word structure and morpheme. The formalism covers a wide variety of morphological phenomena (including non-concatenative ones like stem vowel alternation) without the need of workarounds and extensions. Furthermore, we show that morphological rules formulated in such way can be easily translated to FSTs, which enables us to derive performant approaches to morphological analysis, generation and automatic rule discovery.</abstract>
      <doi>10.18653/v1/W19-3107</doi>
      <bibkey>janicki-2019-finite</bibkey>
    </paper>
    <paper id="8">
      <title>Weighted parsing for grammar-based language models</title>
      <author><first>Richard</first><last>Mörbitz</last></author>
      <author><first>Heiko</first><last>Vogler</last></author>
      <pages>46–55</pages>
      <url hash="64ddfbff">W19-3108</url>
      <abstract>We develop a general framework for weighted parsing which is built on top of grammar-based language models and employs flexible weight algebras. It generalizes previous work in that area (semiring parsing, weighted deductive parsing) and also covers applications outside the classical scope of parsing, e.g., algebraic dynamic programming. We show an algorithm which terminates and is correct for a large class of weighted grammar-based language models.</abstract>
      <doi>10.18653/v1/W19-3108</doi>
      <attachment type="presentation" hash="a3acb8f6">W19-3108.Presentation.pdf</attachment>
      <bibkey>morbitz-vogler-2019-weighted</bibkey>
    </paper>
    <paper id="9">
      <title>Regular transductions with <fixed-case>MCFG</fixed-case> input syntax</title>
      <author><first>Mark-Jan</first><last>Nederhof</last></author>
      <author><first>Heiko</first><last>Vogler</last></author>
      <pages>56–64</pages>
      <url hash="9cf331cf">W19-3109</url>
      <abstract>We show that regular transductions for which the input part is generated by some multiple context-free grammar can be simulated by synchronous multiple context-free grammars. We prove that synchronous multiple context-free grammars are strictly more powerful than this combination of regular transductions and multiple context-free grammars.</abstract>
      <doi>10.18653/v1/W19-3109</doi>
      <bibkey>nederhof-vogler-2019-regular</bibkey>
    </paper>
    <paper id="10">
      <title>A Syntactically Expressive Morphological Analyzer for <fixed-case>T</fixed-case>urkish</title>
      <author><first>Adnan</first><last>Ozturel</last></author>
      <author><first>Tolga</first><last>Kayadelen</last></author>
      <author><first>Isin</first><last>Demirsahin</last></author>
      <pages>65–75</pages>
      <url hash="6d0e2e09">W19-3110</url>
      <abstract>We present a broad coverage model of Turkish morphology and an open-source morphological analyzer that implements it. The model captures intricacies of Turkish morphology-syntax interface, thus could be used as a baseline that guides language model development. It introduces a novel fine part-of-speech tagset, a fine-grained affix inventory and represents morphotactics without zero-derivations. The morphological analyzer is freely available. It consists of modular reusable components of human-annotated gold standard lexicons, implements Turkish morphotactics as finite-state transducers using OpenFst and morphophonemic processes as Thrax grammars.</abstract>
      <doi>10.18653/v1/W19-3110</doi>
      <bibkey>ozturel-etal-2019-syntactically</bibkey>
    </paper>
    <paper id="11">
      <title>Using Meta-Morph Rules to develop Morphological Analysers: A case study concerning <fixed-case>T</fixed-case>amil</title>
      <author><first>Kengatharaiyer</first><last>Sarveswaran</last></author>
      <author><first>Gihan</first><last>Dias</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <pages>76–86</pages>
      <url hash="384cb78b">W19-3111</url>
      <abstract>This paper describes a new and larger coverage Finite-State Morphological Analyser (FSM) and Generator for the Dravidian language Tamil. The FSM has been developed in the context of computational grammar engineering, adhering to the standards of the ParGram effort. Tamil is a morphologically rich language and the interaction between linguistic analysis and formal implementation is complex, resulting in a challenging task. In order to allow the development of the FSM to focus more on the linguistic analysis and less on the formal details, we have developed a system of meta-morph(ology) rules along with a script which translates these rules into FSM processable representations. The introduction of meta-morph rules makes it possible for computationally naive linguists to interact with the system and to expand it in future work. We found that the meta-morph rules help to express linguistic generalisations and reduce the manual effort of writing lexical classes for morphological analysis. Our Tamil FSM currently handles mainly the inflectional morphology of 3,300 verb roots and their 260 forms. Further, it also has a lexicon of approximately 100,000 nouns along with a guesser to handle out-of-vocabulary items. Although the Tamil FSM was primarily developed to be part of a computational grammar, it can also be used as a web or stand-alone application for other NLP tasks, as per general ParGram practice.</abstract>
      <doi>10.18653/v1/W19-3111</doi>
      <bibkey>sarveswaran-etal-2019-using</bibkey>
    </paper>
    <paper id="12">
      <title>Distilling weighted finite automata from arbitrary probabilistic models</title>
      <author><first>Ananda Theertha</first><last>Suresh</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Michael</first><last>Riley</last></author>
      <author><first>Vlad</first><last>Schogol</last></author>
      <pages>87–97</pages>
      <url hash="dc721e54">W19-3112</url>
      <abstract>Weighted finite automata (WFA) are often used to represent probabilistic models, such as n-gram language models, since they are efficient for recognition tasks in time and space. The probabilistic source to be represented as a WFA, however, may come in many forms. Given a generic probabilistic model over sequences, we propose an algorithm to approximate it as a weighted finite automaton such that the Kullback-Leibler divergence between the source model and the WFA target model is minimized. The proposed algorithm involves a counting step and a difference of convex optimization, both of which can be performed efficiently. We demonstrate the usefulness of our approach on some tasks including distilling n-gram models from neural models.</abstract>
      <doi>10.18653/v1/W19-3112</doi>
      <bibkey>suresh-etal-2019-distilling</bibkey>
    </paper>
    <paper id="13">
      <title>Silent <fixed-case>HMM</fixed-case>s: Generalized Representation of Hidden Semi-<fixed-case>M</fixed-case>arkov Models and Hierarchical <fixed-case>HMM</fixed-case>s</title>
      <author><first>Kei</first><last>Wakabayashi</last></author>
      <pages>98–107</pages>
      <url hash="f8a7abd7">W19-3113</url>
      <abstract>Modeling sequence data using probabilistic finite state machines (PFSMs) is a technique that analyzes the underlying dynamics in sequences of symbols. Hidden semi-Markov models (HSMMs) and hierarchical hidden Markov models (HHMMs) are PFSMs that have been successfully applied to a wide variety of applications by extending HMMs to make the extracted patterns easier to interpret. However, these models are independently developed with their own training algorithm, so that we cannot combine multiple kinds of structures to build a PFSM for a specific application. In this paper, we prove that silent hidden Markov models (silent HMMs) are flexible models that have more expressive power than HSMMs and HHMMs. Silent HMMs are HMMs that contain silent states, which do not emit any observations. We show that we can obtain silent HMM equivalent to given HSMMs and HHMMs. We believe that these results form a firm foundation to use silent HMMs as a unified representation for PFSM modeling.</abstract>
      <doi>10.18653/v1/W19-3113</doi>
      <bibkey>wakabayashi-2019-silent</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>L</fixed-case>atin script keyboards for <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian languages with finite-state normalization</title>
      <author><first>Lawrence</first><last>Wolf-Sonkin</last></author>
      <author><first>Vlad</first><last>Schogol</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Michael</first><last>Riley</last></author>
      <pages>108–117</pages>
      <url hash="fa598695">W19-3114</url>
      <abstract>The use of the Latin script for text entry of South Asian languages is common, even though there is no standard orthography for these languages in the script. We explore several compact finite-state architectures that permit variable spellings of words during mobile text entry. We find that approaches making use of transliteration transducers provide large accuracy improvements over baselines, but that simpler approaches involving a compact representation of many attested alternatives yields much of the accuracy gain. This is particularly important when operating under constraints on model size (e.g., on inexpensive mobile devices with limited storage and memory for keyboard models), and on speed of inference, since people typing on mobile keyboards expect no perceptual delay in keyboard responsiveness.</abstract>
      <doi>10.18653/v1/W19-3114</doi>
      <bibkey>wolf-sonkin-etal-2019-latin</bibkey>
    </paper>
    <paper id="15">
      <title>Transition-Based Coding and Formal Language Theory for Ordered Digraphs</title>
      <author><first>Anssi</first><last>Yli-Jyrä</last></author>
      <pages>118–131</pages>
      <url hash="24f7fa00">W19-3115</url>
      <abstract>Transition-based parsing of natural language uses transition systems to build directed annotation graphs (digraphs) for sentences. In this paper, we define, for an arbitrary ordered digraph, a unique decomposition and a corresponding linear encoding that are associated bijectively with each other via a new transition system. These results give us an efficient and succinct representation for digraphs and sets of digraphs. Based on the system and our analysis of its syntactic properties, we give structural bounds under which the set of encoded digraphs is restricted and becomes a context-free or a regular string language. The context-free restriction is essentially a superset of the encodings used previously to characterize properties of noncrossing digraphs and to solve maximal subgraphs problems. The regular restriction with a tight bound is shown to capture the Universal Dependencies v2.4 treebanks in linguistics.</abstract>
      <doi>10.18653/v1/W19-3115</doi>
      <bibkey>yli-jyra-2019-transition</bibkey>
    </paper>
  </volume>
  <volume id="32">
    <meta>
      <booktitle>Proceedings of the Fourth Social Media Mining for Health Applications (#SMM4H) Workshop &amp; Shared Task</booktitle>
      <url hash="56755d93">W19-32</url>
      <editor><first>Davy</first><last>Weissenbacher</last></editor>
      <editor><first>Graciela</first><last>Gonzalez-Hernandez</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="ea592485">W19-3200</url>
      <bibkey>ws-2019-social</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Extracting Kinship from Obituary to Enhance Electronic Health Records for Genetic Research</title>
      <author><first>Kai</first><last>He</last></author>
      <author><first>Jialun</first><last>Wu</last></author>
      <author><first>Xiaoyong</first><last>Ma</last></author>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Ming</first><last>Huang</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Lixia</first><last>Yao</last></author>
      <pages>1–10</pages>
      <abstract>Claims database and electronic health records database do not usually capture kinship or family relationship information, which is imperative for genetic research. We identify online obituaries as a new data source and propose a special named entity recognition and relation extraction solution to extract names and kinships from online obituaries. Built on 1,809 annotated obituaries and a novel tagging scheme, our joint neural model achieved macro-averaged precision, recall and F measure of 72.69%, 78.54% and 74.93%, and micro-averaged precision, recall and F measure of 95.74%, 98.25% and 96.98% using 57 kinships with 10 or more examples in a 10-fold cross-validation experiment. The model performance improved dramatically when trained with 34 kinships with 50 or more examples. Leveraging additional information such as age, death date, birth date and residence mentioned by obituaries, we foresee a promising future of supplementing EHR databases with comprehensive and accurate kinship information for genetic research.</abstract>
      <url hash="b91480e4">W19-3201</url>
      <doi>10.18653/v1/W19-3201</doi>
      <bibkey>he-etal-2019-extracting</bibkey>
    </paper>
    <paper id="2">
      <title>Lexical Normalization of User-Generated Medical Text</title>
      <author><first>Anne</first><last>Dirkson</last></author>
      <author><first>Suzan</first><last>Verberne</last></author>
      <author><first>Wessel</first><last>Kraaij</last></author>
      <pages>11–20</pages>
      <abstract>In the medical domain, user-generated social media text is increasingly used as a valuable complementary knowledge source to scientific medical literature. The extraction of this knowledge is complicated by colloquial language use and misspellings. Yet, lexical normalization of such data has not been addressed properly. This paper presents an unsupervised, data-driven spelling correction module for medical social media. Our method outperforms state-of-the-art spelling correction and can detect mistakes with an F0.5 of 0.888. Additionally, we present a novel corpus for spelling mistake detection and correction on a medical patient forum.</abstract>
      <url hash="844cb51b">W19-3202</url>
      <doi>10.18653/v1/W19-3202</doi>
      <bibkey>dirkson-etal-2019-lexical</bibkey>
    </paper>
    <paper id="3">
      <title>Overview of the Fourth Social Media Mining for Health (<fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case>) Shared Tasks at <fixed-case>ACL</fixed-case> 2019</title>
      <author><first>Davy</first><last>Weissenbacher</last></author>
      <author><first>Abeed</first><last>Sarker</last></author>
      <author><first>Arjun</first><last>Magge</last></author>
      <author><first>Ashlynn</first><last>Daughton</last></author>
      <author><first>Karen</first><last>O’Connor</last></author>
      <author><first>Michael J.</first><last>Paul</last></author>
      <author><first>Graciela</first><last>Gonzalez-Hernandez</last></author>
      <pages>21–30</pages>
      <abstract>The number of users of social media continues to grow, with nearly half of adults worldwide and two-thirds of all American adults using social networking. Advances in automated data processing, machine learning and NLP present the possibility of utilizing this massive data source for biomedical and public health applications, if researchers address the methodological challenges unique to this media. We present the Social Media Mining for Health Shared Tasks collocated with the ACL at Florence in 2019, which address these challenges for health monitoring and surveillance, utilizing state of the art techniques for processing noisy, real-world, and substantially creative language expressions from social media users. For the fourth execution of this challenge, we proposed four different tasks. Task 1 asked participants to distinguish tweets reporting an adverse drug reaction (ADR) from those that do not. Task 2, a follow-up to Task 1, asked participants to identify the span of text in tweets reporting ADRs. Task 3 is an end-to-end task where the goal was to first detect tweets mentioning an ADR and then map the extracted colloquial mentions of ADRs in the tweets to their corresponding standard concept IDs in the MedDRA vocabulary. Finally, Task 4 asked participants to classify whether a tweet contains a personal mention of one’s health, a more general discussion of the health issue, or is an unrelated mention. A total of 34 teams from around the world registered and 19 teams from 12 countries submitted a system run. We summarize here the corpora for this challenge which are freely available at https://competitions.codalab.org/competitions/22521, and present an overview of the methods and the results of the competing systems.</abstract>
      <url hash="f84f4204">W19-3203</url>
      <doi>10.18653/v1/W19-3203</doi>
      <bibkey>weissenbacher-etal-2019-overview</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="4">
      <title><fixed-case>M</fixed-case>ed<fixed-case>N</fixed-case>orm: A Corpus and Embeddings for Cross-terminology Medical Concept Normalisation</title>
      <author><first>Maksim</first><last>Belousov</last></author>
      <author><first>William G.</first><last>Dixon</last></author>
      <author><first>Goran</first><last>Nenadic</last></author>
      <pages>31–39</pages>
      <abstract>The medical concept normalisation task aims to map textual descriptions to standard terminologies such as SNOMED-CT or MedDRA. Existing publicly available datasets annotated using different terminologies cannot be simply merged and utilised, and therefore become less valuable when developing machine learning-based concept normalisation systems. To address that, we designed a data harmonisation pipeline and engineered a corpus of 27,979 textual descriptions simultaneously mapped to both MedDRA and SNOMED-CT, sourced from five publicly available datasets across biomedical and social media domains. The pipeline can be used in the future to integrate new datasets into the corpus and also could be applied in relevant data curation tasks. We also described a method to merge different terminologies into a single concept graph preserving their relations and demonstrated that representation learning approach based on random walks on a graph can efficiently encode both hierarchical and equivalent relations and capture semantic similarities not only between concepts inside a given terminology but also between concepts from different terminologies. We believe that making a corpus and embeddings for cross-terminology medical concept normalisation available to the research community would contribute to a better understanding of the task.</abstract>
      <url hash="3c8998b2">W19-3204</url>
      <doi>10.18653/v1/W19-3204</doi>
      <bibkey>belousov-etal-2019-mednorm</bibkey>
      <pwccode url="https://github.com/mbelousov/MedNorm-corpus" additional="false">mbelousov/MedNorm-corpus</pwccode>
    </paper>
    <paper id="5">
      <title>Passive Diagnosis Incorporating the <fixed-case>PHQ</fixed-case>-4 for Depression and Anxiety</title>
      <author><first>Fionn</first><last>Delahunty</last></author>
      <author><first>Robert</first><last>Johansson</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <pages>40–46</pages>
      <abstract>Depression and anxiety are the two most prevalent mental health disorders worldwide, impacting the lives of millions of people each year. In this work, we develop and evaluate a multilabel, multidimensional deep neural network designed to predict PHQ-4 scores based on individuals written text. Our system outperforms random baseline metrics and provides a novel approach to how we can predict psychometric scores from written text. Additionally, we explore how this architecture can be applied to analyse social media data.</abstract>
      <url hash="58115491">W19-3205</url>
      <doi>10.18653/v1/W19-3205</doi>
      <bibkey>delahunty-etal-2019-passive</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>HITSZ</fixed-case>-<fixed-case>ICRC</fixed-case>: A Report for <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets</title>
      <author><first>Shuai</first><last>Chen</last></author>
      <author><first>Yuanhang</first><last>Huang</last></author>
      <author><first>Xiaowei</first><last>Huang</last></author>
      <author><first>Haoming</first><last>Qin</last></author>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Buzhou</first><last>Tang</last></author>
      <pages>47–51</pages>
      <abstract>This is the system description of the Harbin Institute of Technology Shenzhen (HITSZ) team for the first and second subtasks of the fourth Social Media Mining for Health Applications (SMM4H) shared task in 2019. The two subtasks are automatic classification and extraction of adverse effect mentions in tweets. The systems for the two subtasks are based on bidirectional encoder representations from transformers (BERT), and achieves promising results. Among the systems we developed for subtask1, the best F1-score was 0.6457, for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively. Our system ranks first among all systems on subtask1.</abstract>
      <url hash="7f4cfd70">W19-3206</url>
      <doi>10.18653/v1/W19-3206</doi>
      <bibkey>chen-etal-2019-hitsz</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>KFU</fixed-case> <fixed-case>NLP</fixed-case> Team at <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> 2019 Tasks: Want to Extract Adverse Drugs Reactions from Tweets? <fixed-case>BERT</fixed-case> to The Rescue</title>
      <author><first>Zulfat</first><last>Miftahutdinov</last></author>
      <author><first>Ilseyar</first><last>Alimova</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>52–57</pages>
      <abstract>This paper describes a system developed for the Social Media Mining for Health (SMM4H) 2019 shared tasks. Specifically, we participated in three tasks. The goals of the first two tasks are to classify whether a tweet contains mentions of adverse drug reactions (ADR) and extract these mentions, respectively. The objective of the third task is to build an end-to-end solution: first, detect ADR mentions and then map these entities to concepts in a controlled vocabulary. We investigate the use of a language representation model BERT trained to obtain semantic representations of social media texts. Our experiments on a dataset of user reviews showed that BERT is superior to state-of-the-art models based on recurrent neural networks. The BERT-based system for Task 1 obtained an F1 of 57.38%, with improvements up to +7.19% F1 over a score averaged across all 43 submissions. The ensemble of neural networks with a voting scheme for named entity recognition ranked first among 9 teams at the SMM4H 2019 Task 2 and obtained a relaxed F1 of 65.8%. The end-to-end model based on BERT for ADR normalization ranked first at the SMM4H 2019 Task 3 and obtained a relaxed F1 of 43.2%.</abstract>
      <url hash="fdfb428b">W19-3207</url>
      <doi>10.18653/v1/W19-3207</doi>
      <bibkey>miftahutdinov-etal-2019-kfu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="8">
      <title>Approaching <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> with Merged Models and Multi-task Learning</title>
      <author><first>Tilia</first><last>Ellendorff</last></author>
      <author><first>Lenz</first><last>Furrer</last></author>
      <author><first>Nicola</first><last>Colic</last></author>
      <author><first>Noëmi</first><last>Aepli</last></author>
      <author><first>Fabio</first><last>Rinaldi</last></author>
      <pages>58–61</pages>
      <abstract>We describe our submissions to the 4th edition of the Social Media Mining for Health Applications (SMM4H) shared task. Our team (UZH) participated in two sub-tasks: Automatic classifications of adverse effects mentions in tweets (Task 1) and Generalizable identification of personal health experience mentions (Task 4). For our submissions, we exploited ensembles based on a pre-trained language representation with a neural transformer architecture (BERT) (Tasks 1 and 4) and a CNN-BiLSTM(-CRF) network within a multi-task learning scenario (Task 1). These systems are placed on top of a carefully crafted pipeline of domain-specific preprocessing steps.</abstract>
      <url hash="a748e382">W19-3208</url>
      <doi>10.18653/v1/W19-3208</doi>
      <bibkey>ellendorff-etal-2019-approaching</bibkey>
    </paper>
    <paper id="9">
      <title>Identifying Adverse Drug Events Mentions in Tweets Using Attentive, Collocated, and Aggregated Medical Representation</title>
      <author><first>Xinyan</first><last>Zhao</last></author>
      <author><first>Deahan</first><last>Yu</last></author>
      <author><first>V.G.Vinod</first><last>Vydiswaran</last></author>
      <pages>62–70</pages>
      <abstract>Identifying mentions of medical concepts in social media is challenging because of high variability in free text. In this paper, we propose a novel neural network architecture, the Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), that integrates a bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information from training data to improve the representation of medical concepts. The collocation and aggregation layers improve the model performance on the task of identifying mentions of adverse drug events (ADE) in tweets. Using the dataset made available as part of the workshop shared task, we show that careful selection of neighborhood contexts can help uncover useful local information and improve the overall medical concept representation.</abstract>
      <url hash="5b5eae37">W19-3209</url>
      <doi>10.18653/v1/W19-3209</doi>
      <bibkey>zhao-etal-2019-identifying</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="10">
      <title>Correlating <fixed-case>T</fixed-case>witter Language with Community-Level Health Outcomes</title>
      <author><first>Arno</first><last>Schneuwly</last></author>
      <author><first>Ralf</first><last>Grubenmann</last></author>
      <author><first>Séverine</first><last>Rion Logean</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <author><first>Martin</first><last>Jaggi</last></author>
      <pages>71–78</pages>
      <abstract>We study how language on social media is linked to mortal diseases such as atherosclerotic heart disease (AHD), diabetes and various types of cancer. Our proposed model leverages state-of-the-art sentence embeddings, followed by a regression model and clustering, without the need of additional labelled data. It allows to predict community-level medical outcomes from language, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of medical outcomes with life-style aspects and other socioeconomic risk factors.</abstract>
      <url hash="7d005440">W19-3210</url>
      <doi>10.18653/v1/W19-3210</doi>
      <bibkey>schneuwly-etal-2019-correlating</bibkey>
      <pwccode url="https://github.com/epfml/correlating-tweets" additional="false">epfml/correlating-tweets</pwccode>
    </paper>
    <paper id="11">
      <title>Affective Behaviour Analysis of On-line User Interactions: Are On-line Support Groups More Therapeutic than <fixed-case>T</fixed-case>witter?</title>
      <author><first>Giuliano</first><last>Tortoreto</last></author>
      <author><first>Evgeny</first><last>Stepanov</last></author>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>Mateusz</first><last>Dubiel</last></author>
      <author><first>Giuseppe</first><last>Riccardi</last></author>
      <pages>79–88</pages>
      <abstract>The increase in the prevalence of mental health problems has coincided with a growing popularity of health related social networking sites. Regardless of their therapeutic potential, on-line support groups (OSGs) can also have negative effects on patients. In this work we propose a novel methodology to automatically verify the presence of therapeutic factors in social networking websites by using Natural Language Processing (NLP) techniques. The methodology is evaluated on on-line asynchronous multi-party conversations collected from an OSG and Twitter. The results of the analysis indicate that therapeutic factors occur more frequently in OSG conversations than in Twitter conversations. Moreover, the analysis of OSG conversations reveals that the users of that platform are supportive, and interactions are likely to lead to the improvement of their emotional state. We believe that our method provides a stepping stone towards automatic analysis of emotional states of users of online platforms. Possible applications of the method include provision of guidelines that highlight potential implications of using such platforms on users’ mental health, and/or support in the analysis of their impact on specific individuals.</abstract>
      <url hash="6975661c">W19-3211</url>
      <doi>10.18653/v1/W19-3211</doi>
      <bibkey>tortoreto-etal-2019-affective</bibkey>
    </paper>
    <paper id="12">
      <title>Transfer Learning for Health-related <fixed-case>T</fixed-case>witter Data</title>
      <author><first>Anne</first><last>Dirkson</last></author>
      <author><first>Suzan</first><last>Verberne</last></author>
      <pages>89–92</pages>
      <abstract>Transfer learning is promising for many NLP applications, especially in tasks with limited labeled data. This paper describes the methods developed by team TMRLeiden for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task. Our methods use state-of-the-art transfer learning methods to classify, extract and normalise adverse drug effects (ADRs) and to classify personal health mentions from health-related tweets. The code and fine-tuned models are publicly available.</abstract>
      <url hash="599305ac">W19-3212</url>
      <doi>10.18653/v1/W19-3212</doi>
      <bibkey>dirkson-verberne-2019-transfer</bibkey>
      <pwccode url="https://github.com/AnneDirkson/SharedTaskSMM4H2019" additional="false">AnneDirkson/SharedTaskSMM4H2019</pwccode>
    </paper>
    <paper id="13">
      <title><fixed-case>NLP</fixed-case>@<fixed-case>UNED</fixed-case> at <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> 2019: Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets</title>
      <author><first>Javier</first><last>Cortes-Tejada</last></author>
      <author><first>Juan</first><last>Martinez-Romo</last></author>
      <author><first>Lourdes</first><last>Araujo</last></author>
      <pages>93–95</pages>
      <abstract>This paper describes a system for automatically classifying adverse effects mentions in tweets developed for the task 1 at Social Media Mining for Health Applications (SMM4H) Shared Task 2019. We have developed a system based on LSTM neural networks inspired by the excellent results obtained by deep learning classifiers in the last edition of this task. The network is trained along with Twitter GloVe pre-trained word embeddings.</abstract>
      <url hash="150283a1">W19-3213</url>
      <doi>10.18653/v1/W19-3213</doi>
      <bibkey>cortes-tejada-etal-2019-nlp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="14">
      <title>Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets with Multi-Head Self Attention</title>
      <author><first>Suyu</first><last>Ge</last></author>
      <author><first>Tao</first><last>Qi</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>96–98</pages>
      <abstract>This paper describes our system for the first and second shared tasks of the fourth Social Media Mining for Health Applications (SMM4H) workshop. We enhance tweet representation with a language model and distinguish the importance of different words with Multi-Head Self-Attention. In addition, transfer learning is exploited to make up for the data shortage. Our system achieved competitive results on both tasks with an F1-score of 0.5718 for task 1 and 0.653 (overlap) / 0.357 (strict) for task 2.</abstract>
      <url hash="a405a8a1">W19-3214</url>
      <doi>10.18653/v1/W19-3214</doi>
      <bibkey>ge-etal-2019-detecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="15">
      <title>Deep Learning for Identification of Adverse Effect Mentions In <fixed-case>T</fixed-case>witter Data</title>
      <author><first>Paul</first><last>Barry</last></author>
      <author><first>Ozlem</first><last>Uzuner</last></author>
      <pages>99–101</pages>
      <abstract>Social Media Mining for Health Applications (SMM4H) Adverse Effect Mentions Shared Task challenges participants to accurately identify spans of text within a tweet that correspond to Adverse Effects (AEs) resulting from medication usage (Weissenbacher et al., 2019). This task features a training data set of 2,367 tweets, in addition to a 1,000 tweet evaluation data set. The solution presented here features a bidirectional Long Short-term Memory Network (bi-LSTM) for the generation of character-level embeddings. It uses a second bi-LSTM trained on both character and token level embeddings to feed a Conditional Random Field (CRF) which provides the final classification. This paper further discusses the deep learning algorithms used in our solution.</abstract>
      <url hash="06e3c99f">W19-3215</url>
      <doi>10.18653/v1/W19-3215</doi>
      <bibkey>barry-uzuner-2019-deep</bibkey>
    </paper>
    <paper id="16">
      <title>Using Machine Learning and Deep Learning Methods to Find Mentions of Adverse Drug Reactions in Social Media</title>
      <author><first>Pilar</first><last>López Úbeda</last></author>
      <author><first>Manuel Carlos</first><last>Díaz Galiano</last></author>
      <author><first>Maite</first><last>Martin</last></author>
      <author><first>L. Alfonso</first><last>Urena Lopez</last></author>
      <pages>102–106</pages>
      <abstract>Over time the use of social networks is becoming very popular platforms for sharing health related information. Social Media Mining for Health Applications (SMM4H) provides tasks such as those described in this document to help manage information in the health domain. This document shows the first participation of the SINAI group. We study approaches based on machine learning and deep learning to extract adverse drug reaction mentions from Twitter. The results obtained in the tasks are encouraging, we are close to the average of all participants and even above in some cases.</abstract>
      <url hash="1802740a">W19-3216</url>
      <doi>10.18653/v1/W19-3216</doi>
      <bibkey>lopez-ubeda-etal-2019-using-machine</bibkey>
    </paper>
    <paper id="17">
      <title>Towards Text Processing Pipelines to Identify Adverse Drug Events-related Tweets: <fixed-case>U</fixed-case>niversity of <fixed-case>M</fixed-case>ichigan @ <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> 2019 Task 1</title>
      <author><first>V.G.Vinod</first><last>Vydiswaran</last></author>
      <author><first>Grace</first><last>Ganzel</last></author>
      <author><first>Bryan</first><last>Romas</last></author>
      <author><first>Deahan</first><last>Yu</last></author>
      <author><first>Amy</first><last>Austin</last></author>
      <author><first>Neha</first><last>Bhomia</last></author>
      <author><first>Socheatha</first><last>Chan</last></author>
      <author><first>Stephanie</first><last>Hall</last></author>
      <author><first>Van</first><last>Le</last></author>
      <author><first>Aaron</first><last>Miller</last></author>
      <author><first>Olawunmi</first><last>Oduyebo</last></author>
      <author><first>Aulia</first><last>Song</last></author>
      <author><first>Radhika</first><last>Sondhi</last></author>
      <author><first>Danny</first><last>Teng</last></author>
      <author><first>Hao</first><last>Tseng</last></author>
      <author><first>Kim</first><last>Vuong</last></author>
      <author><first>Stephanie</first><last>Zimmerman</last></author>
      <pages>107–109</pages>
      <abstract>We participated in Task 1 of the Social Media Mining for Health Applications (SMM4H) 2019 Shared Tasks on detecting mentions of adverse drug events (ADEs) in tweets. Our approach relied on a text processing pipeline for tweets, and training traditional machine learning and deep learning models. Our submitted runs performed above average for the task.</abstract>
      <url hash="9cf1a120">W19-3217</url>
      <doi>10.18653/v1/W19-3217</doi>
      <bibkey>vydiswaran-etal-2019-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="18">
      <title>Neural Network to Identify Personal Health Experience Mention in Tweets Using <fixed-case>B</fixed-case>io<fixed-case>BERT</fixed-case> Embeddings</title>
      <author><first>Shubham</first><last>Gondane</last></author>
      <pages>110–113</pages>
      <abstract>This paper describes the system developed by team ASU-NLP for the Social Media Mining for Health Applications(SMM4H) shared task 4. We extract feature embeddings from the BioBERT (Lee et al., 2019) model which has been fine-tuned on the training dataset and use that as inputs to a dense fully connected neural network. We achieve above average scores among the participant systems with the overall F1-score, accuracy, precision, recall as 0.8036, 0.8456, 0.9783, 0.6818 respectively.</abstract>
      <url hash="d3deaa4b">W19-3218</url>
      <doi>10.18653/v1/W19-3218</doi>
      <bibkey>gondane-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="19">
      <title>Give It a Shot: Few-shot Learning to Normalize <fixed-case>ADR</fixed-case> Mentions in Social Media Posts</title>
      <author><first>Emmanouil</first><last>Manousogiannis</last></author>
      <author><first>Sepideh</first><last>Mesbah</last></author>
      <author><first>Alessandro</first><last>Bozzon</last></author>
      <author><first>Selene</first><last>Baez</last></author>
      <author><first>Robert Jan</first><last>Sips</last></author>
      <pages>114–116</pages>
      <abstract>This paper describes the system that team MYTOMORROWS-TU DELFT developed for the 2019 Social Media Mining for Health Applications (SMM4H) Shared Task 3, for the end-to-end normalization of ADR tweet mentions to their corresponding MEDDRA codes. For the first two steps, we reuse a state-of-the art approach, focusing our contribution on the final entity-linking step. For that we propose a simple Few-Shot learning approach, based on pre-trained word embeddings and data from the UMLS, combined with the provided training data. Our system (relaxed F1: 0.337-0.345) outperforms the average (relaxed F1 0.2972) of the participants in this task, demonstrating the potential feasibility of few-shot learning in the context of medical text normalization.</abstract>
      <url hash="50f17f47">W19-3219</url>
      <doi>10.18653/v1/W19-3219</doi>
      <bibkey>manousogiannis-etal-2019-give</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="20">
      <title><fixed-case>BIGODM</fixed-case> System in the Social Media Mining for Health Applications Shared Task 2019</title>
      <author><first>Chen-Kai</first><last>Wang</last></author>
      <author><first>Hong-Jie</first><last>Dai</last></author>
      <author><first>Bo-Hung</first><last>Wang</last></author>
      <pages>117–119</pages>
      <abstract>In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction (ADR). Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based under-sampling ensemble approach along with linear support vector machine (SVM) to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications (SMM4H) shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.</abstract>
      <url hash="5422cc89">W19-3220</url>
      <doi>10.18653/v1/W19-3220</doi>
      <bibkey>wang-etal-2019-bigodm</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="21">
      <title>Detection of Adverse Drug Reaction Mentions in Tweets Using <fixed-case>ELM</fixed-case>o</title>
      <author><first>Sarah</first><last>Sarabadani</last></author>
      <pages>120–122</pages>
      <abstract>This paper describes the models used by our team in SMM4H 2019 shared task. We submitted results for subtasks 1 and 2. For task 1 which aims to detect tweets with Adverse Drug Reaction (ADR) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics. For task 2, which focuses on extraction of ADR mentions, first the same architecture as task 1 was used to identify whether or not a tweet contains ADR. Then, for tweets positively classified as mentioning ADR, the relevant text span was identified by similarity matching with 3 different lexicon sets.</abstract>
      <url hash="25f7abcd">W19-3221</url>
      <doi>10.18653/v1/W19-3221</doi>
      <bibkey>sarabadani-2019-detection</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="22">
      <title>Adverse Drug Effect and Personalized Health Mentions, <fixed-case>CL</fixed-case>a<fixed-case>C</fixed-case> at <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> 2019, Tasks 1 and 4</title>
      <author><first>Parsa</first><last>Bagherzadeh</last></author>
      <author><first>Nadia</first><last>Sheikh</last></author>
      <author><first>Sabine</first><last>Bergler</last></author>
      <pages>123–126</pages>
      <abstract>CLaC labs participated in Task 1 and 4 of SMM4H 2019. We pursed two main objectives in our submission. First we tried to use some textual features in a deep net framework, and second, the potential use of more than one word embedding was tested. The results seem positively affected by the proposed architectures.</abstract>
      <url hash="b2d4c84f">W19-3222</url>
      <doi>10.18653/v1/W19-3222</doi>
      <bibkey>bagherzadeh-etal-2019-adverse</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="23">
      <title><fixed-case>MIDAS</fixed-case>@<fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case>-2019: Identifying Adverse Drug Reactions and Personal Health Experience Mentions from <fixed-case>T</fixed-case>witter</title>
      <author><first>Debanjan</first><last>Mahata</last></author>
      <author><first>Sarthak</first><last>Anand</last></author>
      <author><first>Haimin</first><last>Zhang</last></author>
      <author><first>Simra</first><last>Shahid</last></author>
      <author><first>Laiba</first><last>Mehnaz</last></author>
      <author><first>Yaman</first><last>Kumar</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>127–132</pages>
      <abstract>In this paper, we present our approach and the system description for the Social Media Mining for Health Applications (SMM4H) Shared Task 1,2 and 4 (2019). Our main contribution is to show the effectiveness of Transfer Learning approaches like BERT and ULMFiT, and how they generalize for the classification tasks like identification of adverse drug reaction mentions and reporting of personal health problems in tweets. We show the use of stacked embeddings combined with BLSTM+CRF tagger for identifying spans mentioning adverse drug reactions in tweets. We also show that these approaches perform well even with imbalanced dataset in comparison to undersampling and oversampling.</abstract>
      <url hash="16a210ff">W19-3223</url>
      <doi>10.18653/v1/W19-3223</doi>
      <bibkey>mahata-etal-2019-midas-smm4h</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="24">
      <title>Detection of Adverse Drug Reaction in Tweets Using a Combination of Heterogeneous Word Embeddings</title>
      <author><first>Segun Taofeek</first><last>Aroyehun</last></author>
      <author><first>Alexander</first><last>Gelbukh</last></author>
      <pages>133–135</pages>
      <abstract>This paper details our approach to the task of detecting reportage of adverse drug reaction in tweets as part of the 2019 social media mining for healthcare applications shared task. We employed a combination of three types of word representations as input to a LSTM model. With this approach, we achieved an F1 score of 0.5209.</abstract>
      <url hash="da778f65">W19-3224</url>
      <doi>10.18653/v1/W19-3224</doi>
      <bibkey>aroyehun-gelbukh-2019-detection</bibkey>
    </paper>
    <paper id="25">
      <title>Identification of Adverse Drug Reaction Mentions in Tweets – <fixed-case>SMM</fixed-case>4<fixed-case>H</fixed-case> Shared Task 2019</title>
      <author><first>Samarth</first><last>Rawal</last></author>
      <author><first>Siddharth</first><last>Rawal</last></author>
      <author><first>Saadat</first><last>Anwar</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>136–137</pages>
      <abstract>Analyzing social media posts can offer insights into a wide range of topics that are commonly discussed online, providing valuable information for studying various health-related phenomena reported online. The outcome of this work can offer insights into pharmacovigilance research to monitor the adverse effects of medications. This research specifically looks into mentions of adverse drug reactions (ADRs) in Twitter data through the Social Media Mining for Health Applications (SMM4H) Shared Task 2019. Adverse drug reactions are undesired harmful effects which can arise from medication or other methods of treatment. The goal of this research is to build accurate models using natural language processing techniques to detect reports of adverse drug reactions in Twitter data and extract these words or phrases.</abstract>
      <url hash="a22ddb63">W19-3225</url>
      <doi>10.18653/v1/W19-3225</doi>
      <bibkey>rawal-etal-2019-identification</bibkey>
    </paper>
  </volume>
  <volume id="33">
    <meta>
      <booktitle>Proceedings of the First International Workshop on Designing Meaning Representations</booktitle>
      <url hash="4ec4432d">W19-33</url>
      <editor><first>Nianwen</first><last>Xue</last></editor>
      <editor><first>William</first><last>Croft</last></editor>
      <editor><first>Jan</first><last>Hajic</last></editor>
      <editor><first>Chu-Ren</first><last>Huang</last></editor>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Martha</first><last>Palmer</last></editor>
      <editor><first>James</first><last>Pustejovksy</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>dmr</venue>
    </meta>
    <frontmatter>
      <url hash="52cd4e5a">W19-3300</url>
      <bibkey>ws-2019-international-designing</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Cross-Linguistic Semantic Annotation: Reconciling the Language-Specific and the Universal</title>
      <author><first>Jens E. L.</first><last>Van Gysel</last></author>
      <author><first>Meagan</first><last>Vigus</last></author>
      <author><first>Pavlina</first><last>Kalm</last></author>
      <author><first>Sook-kyung</first><last>Lee</last></author>
      <author><first>Michael</first><last>Regan</last></author>
      <author><first>William</first><last>Croft</last></author>
      <pages>1–14</pages>
      <abstract>Developers of cross-linguistic semantic annotation schemes face a number of issues not encountered in monolingual annotation. This paper discusses four such issues, related to the establishment of annotation labels, and the treatment of languages with more fine-grained, more coarse-grained, and cross-cutting categories. We propose that a lattice-like architecture of the annotation categories can adequately handle all four issues, and at the same time remain both intuitive for annotators and faithful to typological insights. This position is supported by a brief annotation experiment.</abstract>
      <url hash="0c63d57b">W19-3301</url>
      <doi>10.18653/v1/W19-3301</doi>
      <bibkey>van-gysel-etal-2019-cross</bibkey>
    </paper>
    <paper id="2">
      <title>Thirty Musts for Meaning Banking</title>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>15–27</pages>
      <abstract>Meaning banking—creating a semantically annotated corpus for the purpose of semantic parsing or generation—is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper’s format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4–11), and design of meaning representations (Section 12–30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future</abstract>
      <url hash="2c32d9bf">W19-3302</url>
      <doi>10.18653/v1/W19-3302</doi>
      <bibkey>abzianidze-bos-2019-thirty</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/amr-bank">AMR Bank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/groningen-meaning-bank">Groningen Meaning Bank</pwcdataset>
    </paper>
    <paper id="3">
      <title>Modeling Quantification and Scope in <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentations</title>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Ken</first><last>Lai</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>28–33</pages>
      <abstract>In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity. Specifically, we address three phenomena not previously part of the AMR specification: quantification, negation (generally), and modality. The resulting representation, which we call “Uniform Meaning Representation” (UMR), adopts the predicative core of AMR and embeds it under a “scope” graph when appropriate. UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible.‘</abstract>
      <url hash="05893139">W19-3303</url>
      <doi>10.18653/v1/W19-3303</doi>
      <bibkey>pustejovsky-etal-2019-modeling</bibkey>
    </paper>
    <paper id="4">
      <title>Parsing Meaning Representations: Is Easier Always Better?</title>
      <author><first>Zi</first><last>Lin</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>34–43</pages>
      <abstract>The parsing accuracy varies a great deal for different meaning representations. In this paper, we compare the parsing performances between Abstract Meaning Representation (AMR) and Minimal Recursion Semantics (MRS), and provide an in-depth analysis of what factors contributed to the discrepancy in their parsing accuracy. By crystalizing the trade-off between representation expressiveness and ease of automatic parsing, we hope our results can help inform the design of the next-generation meaning representations.</abstract>
      <url hash="0a522755">W19-3304</url>
      <doi>10.18653/v1/W19-3304</doi>
      <bibkey>lin-xue-2019-parsing</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>GKR</fixed-case>: Bridging the Gap between Symbolic/structural and Distributional Meaning Representations</title>
      <author><first>Aikaterini-Lida</first><last>Kalouli</last></author>
      <author><first>Richard</first><last>Crouch</last></author>
      <author><first>Valeria</first><last>de Paiva</last></author>
      <pages>44–55</pages>
      <abstract>Three broad approaches have been attempted to combine distributional and structural/symbolic aspects to construct meaning representations: a) injecting linguistic features into distributional representations, b) injecting distributional features into symbolic representations or c) combining structural and distributional features in the final representation. This work focuses on an example of the third and less studied approach: it extends the Graphical Knowledge Representation (GKR) to include distributional features and proposes a division of semantic labour between the distributional and structural/symbolic features. We propose two extensions of GKR that clearly show this division and empirically test one of the proposals on an NLI dataset with hard compositional pairs.</abstract>
      <url hash="867010d1">W19-3305</url>
      <doi>10.18653/v1/W19-3305</doi>
      <bibkey>kalouli-etal-2019-gkr</bibkey>
      <pwccode url="https://github.com/kkalouli/GKR_semantic_parser" additional="false">kkalouli/GKR_semantic_parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="6">
      <title>Generating Discourse Inferences from Unscoped Episodic Logical Formulas</title>
      <author><first>Gene</first><last>Kim</last></author>
      <author><first>Benjamin</first><last>Kane</last></author>
      <author><first>Viet</first><last>Duong</last></author>
      <author><first>Muskaan</first><last>Mendiratta</last></author>
      <author><first>Graeme</first><last>McGuire</last></author>
      <author><first>Sophie</first><last>Sackstein</last></author>
      <author><first>Georgiy</first><last>Platonov</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>56–65</pages>
      <abstract>Abstract Unscoped episodic logical form (ULF) is a semantic representation capturing the predicate-argument structure of English within the episodic logic formalism in relation to the syntactic structure, while leaving scope, word sense, and anaphora unresolved. We describe how ULF can be used to generate natural language inferences that are grounded in the semantic and syntactic structure through a small set of rules defined over interpretable predicates and transformations on ULFs. The semantic restrictions placed by ULF semantic types enables us to ensure that the inferred structures are semantically coherent while the nearness to syntax enables accurate mapping to English. We demonstrate these inferences on four classes of conversationally-oriented inferences in a mixed genre dataset with 68.5% precision from human judgments.</abstract>
      <url hash="0bcb16e1">W19-3306</url>
      <doi>10.18653/v1/W19-3306</doi>
      <bibkey>kim-etal-2019-generating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
    </paper>
    <paper id="7">
      <title>A Plea for Information Structure as a Part of Meaning Representation</title>
      <author><first>Eva</first><last>Hajicova</last></author>
      <pages>66–72</pages>
      <abstract>The view that the representation of information structure (IS) should be a part of (any type of) representation of meaning is based on the fact that IS is a semantically relevant phenomenon. In the contribution, three arguments supporting this view are briefly summarized, namely, the relation of IS to the interpretation of negation and presupposition, the relevance of IS to the understanding of discourse connectivity and for the establishment and interpretation of coreference relations. Afterwards, possible integration of the description of the main ingredient of IS into a meaning representation is illustrated.</abstract>
      <url hash="2f1d36db">W19-3307</url>
      <doi>10.18653/v1/W19-3307</doi>
      <bibkey>hajicova-2019-plea</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>TCL</fixed-case> - a Lexicon of <fixed-case>T</fixed-case>urkish Discourse Connectives</title>
      <author><first>Deniz</first><last>Zeyrek</last></author>
      <author><first>Kezban</first><last>Başıbüyük</last></author>
      <pages>73–81</pages>
      <abstract>It is known that discourse connectives are the most salient indicators of discourse relations. State-of-the-art parsers being developed to predict explicit discourse connectives exploit annotated discourse corpora but a lexicon of discourse connectives is also needed to enable further research in discourse structure and support the development of language technologies that use these structures for text understanding. This paper presents a lexicon of Turkish discourse connectives built by automatic means. The lexicon has the format of the German connective lexicon, DiMLex, where for each discourse connective, information about the connective‘s orthographic variants, syntactic category and senses are provided along with sample relations. In this paper, we describe the data sources we used and the development steps of the lexicon.</abstract>
      <url hash="6a786da7">W19-3308</url>
      <doi>10.18653/v1/W19-3308</doi>
      <bibkey>zeyrek-basibuyuk-2019-tcl</bibkey>
    </paper>
    <paper id="9">
      <title>Meta-Semantic Representation for Early Detection of <fixed-case>A</fixed-case>lzheimer’s Disease</title>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <author><first>Mengmei</first><last>Li</last></author>
      <author><first>Felicia</first><last>Goldstein</last></author>
      <author><first>Ihab</first><last>Hajjar</last></author>
      <pages>82–91</pages>
      <abstract>This paper presents a new task-oriented meaning representation called meta-semantics, that is designed to detect patients with early symptoms of Alzheimer’s disease by analyzing their language beyond a syntactic or semantic level. Meta-semantic representation consists of three parts, entities, predicate argument structures, and discourse attributes, that derive rich knowledge graphs. For this study, 50 controls and 50 patients with mild cognitive impairment (MCI) are selected, and meta-semantic representation is annotated on their speeches transcribed in text. Inter-annotator agreement scores of 88%, 82%, and 89% are achieved for the three types of annotation, respectively. Five analyses are made using this annotation, depicting clear distinctions between the control and MCI groups. Finally, a neural model is trained on features extracted from those analyses to classify MCI patients from normal controls, showing a high accuracy of 82% that is very promising.</abstract>
      <url hash="2ce491c3">W19-3309</url>
      <doi>10.18653/v1/W19-3309</doi>
      <bibkey>choi-etal-2019-meta</bibkey>
    </paper>
    <paper id="10">
      <title>Ellipsis in <fixed-case>C</fixed-case>hinese <fixed-case>AMR</fixed-case> Corpus</title>
      <author><first>Yihuan</first><last>Liu</last></author>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Peiyi</first><last>Yan</last></author>
      <author><first>Li</first><last>Song</last></author>
      <author><first>Weiguang</first><last>Qu</last></author>
      <pages>92–99</pages>
      <abstract>Ellipsis is very common in language. It’s necessary for natural language processing to restore the elided elements in a sentence. However, there’s only a few corpora annotating the ellipsis, which draws back the automatic detection and recovery of the ellipsis. This paper introduces the annotation of ellipsis in Chinese sentences, using a novel graph-based representation Abstract Meaning Representation (AMR), which has a good mechanism to restore the elided elements manually. We annotate 5,000 sentences selected from Chinese TreeBank (CTB). We find that 54.98% of sentences have ellipses. 92% of the ellipses are restored by copying the antecedents’ concepts. and 12.9% of them are the new added concepts. In addition, we find that the elided element is a word or phrase in most cases, but sometimes only the head of a phrase or parts of a phrase, which is rather hard for the automatic recovery of ellipsis.</abstract>
      <url hash="278be415">W19-3310</url>
      <doi>10.18653/v1/W19-3310</doi>
      <bibkey>liu-etal-2019-ellipsis</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="11">
      <title>Event Structure Representation: Between Verbs and Argument Structure Constructions</title>
      <author><first>Pavlina</first><last>Kalm</last></author>
      <author><first>Michael</first><last>Regan</last></author>
      <author><first>William</first><last>Croft</last></author>
      <pages>100–109</pages>
      <abstract>This paper proposes a novel representation of event structure by separating verbal semantics and the meaning of argument structure constructions that verbs occur in. Our model demonstrates how the two meaning representations interact. Our model thus effectively deals with various verb construals in different argument structure constructions, unlike purely verb-based approaches. However, unlike many constructionally-based approaches, we also provide a richer representation of the event structure evoked by the verb meaning.</abstract>
      <url hash="2c9df8a5">W19-3311</url>
      <doi>10.18653/v1/W19-3311</doi>
      <bibkey>kalm-etal-2019-event</bibkey>
    </paper>
    <paper id="12">
      <title>Distributional Semantics Meets Construction Grammar. towards a Unified Usage-Based Model of Grammar and Meaning</title>
      <author><first>Giulia</first><last>Rambelli</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <pages>110–120</pages>
      <abstract>In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Construction Grammar, where grammar and meaning are systematically modeled from language use, and finally, we discuss the kind of contributions that distributional models can provide to CxG representation from a linguistic and cognitive perspective.</abstract>
      <url hash="4dc88a27">W19-3312</url>
      <doi>10.18653/v1/W19-3312</doi>
      <bibkey>rambelli-etal-2019-distributional</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="13">
      <title>Meaning Representation of Null Instantiated Semantic Roles in <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <author><first>Miriam R L</first><last>Petruck</last></author>
      <pages>121–127</pages>
      <abstract>Humans have the unique ability to infer information about participants in a scene, even if they are not mentioned in a text about that scene. Computer systems cannot do so without explicit information about those participants. This paper addresses the linguistic phenomenon of null-instantiated frame elements, i.e., implicit semantic roles, and their representation in FrameNet (FN). It motivates FN’s annotation practice, and illustrates three types of null-instantiated arguments that FrameNet tracks, noting that other lexical resources do not record such semantic-pragmatic information, despite its need in natural language understanding (NLU), and the elaborate efforts to create new datasets. It challenges the community to appeal to FN data to develop more sophisticated techniques for recognizing implicit semantic roles, and creating needed datasets. Although the annotation of null-instantiated roles was lexicographically motivated, FN provides useful information for text processing, and therefore must be considered in the design of any meaning representation for natural language understanding.</abstract>
      <url hash="b5184c10">W19-3313</url>
      <doi>10.18653/v1/W19-3313</doi>
      <bibkey>petruck-2019-meaning</bibkey>
    </paper>
    <paper id="14">
      <title>Copula and Case-Stacking Annotations for <fixed-case>K</fixed-case>orean <fixed-case>AMR</fixed-case></title>
      <author><first>Hyonsu</first><last>Choe</last></author>
      <author><first>Jiyoon</first><last>Han</last></author>
      <author><first>Hyejin</first><last>Park</last></author>
      <author><first>Hansaem</first><last>Kim</last></author>
      <pages>128–135</pages>
      <abstract>This paper concerns the application of Abstract Meaning Representation (AMR) to Korean. In this regard, it focuses on the copula construction and its negation and the case-stacking phenomenon thereof. To illustrate this clearly, we reviewed the :domain annotation scheme from various perspectives. In this process, the existing annotation guidelines were improved to devise annotation schemes for each issue under the principle of pursuing consistency and efficiency of annotation without distorting the characteristics of Korean.</abstract>
      <url hash="dd292606">W19-3314</url>
      <doi>10.18653/v1/W19-3314</doi>
      <bibkey>choe-etal-2019-copula</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>C</fixed-case>lear<fixed-case>TAC</fixed-case>: Verb Tense, Aspect, and Form Classification Using Neural Nets</title>
      <author><first>Skatje</first><last>Myers</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <pages>136–140</pages>
      <abstract>This paper proposes using a Bidirectional LSTM-CRF model in order to identify the tense and aspect of verbs. The information that this classifier outputs can be useful for ordering events and can provide a pre-processing step to improve efficiency of annotating this type of information. This neural network architecture has been successfully employed for other sequential labeling tasks, and we show that it significantly outperforms the rule-based tool TMV-annotator on the Propbank I dataset.</abstract>
      <url hash="eb3d34d8">W19-3315</url>
      <doi>10.18653/v1/W19-3315</doi>
      <bibkey>myers-palmer-2019-cleartac</bibkey>
    </paper>
    <paper id="16">
      <title>Preparing <fixed-case>SNACS</fixed-case> for Subjects and Objects</title>
      <author><first>Adi</first><last>Shalev</last></author>
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Ari</first><last>Rappoport</last></author>
      <pages>141–147</pages>
      <abstract>Research on adpositions and possessives in multiple languages has led to a small inventory of general-purpose meaning classes that disambiguate tokens. Importantly, that work has argued for a principled separation of the semantic role in a scene from the function coded by morphosyntax. Here, we ask whether this approach can be generalized beyond adpositions and possessives to cover all scene participants—including subjects and objects—directly, without reference to a frame lexicon. We present new guidelines for English and the results of an interannotator agreement study.</abstract>
      <url hash="ffed9ccf">W19-3316</url>
      <attachment type="poster" hash="a8969aa3">W19-3316.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-3316</doi>
      <bibkey>shalev-etal-2019-preparing</bibkey>
      <pwccode url="https://github.com/adishalev/SNACS_DMR_IAA" additional="false">adishalev/SNACS_DMR_IAA</pwccode>
    </paper>
    <paper id="17">
      <title>A Case Study on Meaning Representation for <fixed-case>V</fixed-case>ietnamese</title>
      <author><first>Ha</first><last>Linh</last></author>
      <author><first>Huyen</first><last>Nguyen</last></author>
      <pages>148–153</pages>
      <abstract>This paper presents a case study on meaning representation for Vietnamese. Having introduced several existing semantic representation schemes for different languages, we select as basis for our work on Vietnamese AMR (Abstract Meaning Representation). From it, we define a meaning representation label set by adapting the English schema and taking into account the specific characteristics of Vietnamese.</abstract>
      <url hash="f48876f8">W19-3317</url>
      <doi>10.18653/v1/W19-3317</doi>
      <bibkey>linh-nguyen-2019-case</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et Representations: Subevent Semantics for Transfer Verbs</title>
      <author><first>Susan Windisch</first><last>Brown</last></author>
      <author><first>Julia</first><last>Bonn</last></author>
      <author><first>James</first><last>Gung</last></author>
      <author><first>Annie</first><last>Zaenen</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <pages>154–163</pages>
      <abstract>This paper announces the release of a new version of the English lexical resource VerbNet with substantially revised semantic representations designed to facilitate computer planning and reasoning based on human language. We use the transfer of possession and transfer of information event representations to illustrate both the general framework of the representations and the types of nuances the new representations can capture. These representations use a Generative Lexicon-inspired subevent structure to track attributes of event participants across time, highlighting oppositions and temporal and causal relations among the subevents.</abstract>
      <url hash="47375e13">W19-3318</url>
      <doi>10.18653/v1/W19-3318</doi>
      <bibkey>brown-etal-2019-verbnet</bibkey>
    </paper>
    <paper id="19">
      <title>Semantically Constrained Multilayer Annotation: The Case of Coreference</title>
      <author><first>Jakob</first><last>Prange</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>164–176</pages>
      <abstract>We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions. We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes.</abstract>
      <url hash="12a91e9a">W19-3319</url>
      <doi>10.18653/v1/W19-3319</doi>
      <bibkey>prange-etal-2019-semantically</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gum">GUM</pwcdataset>
    </paper>
    <paper id="20">
      <title>Towards Universal Semantic Representation</title>
      <author><first>Huaiyu</first><last>Zhu</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Laura</first><last>Chiticariu</last></author>
      <pages>177–181</pages>
      <abstract>Natural language understanding at the semantic level and independent of language variations is of great practical value. Existing approaches such as semantic role labeling (SRL) and abstract meaning representation (AMR) still have features related to the peculiarities of the particular language. In this work we describe various challenges and possible solutions in designing a semantic representation that is universal across a variety of languages.</abstract>
      <url hash="51f01a80">W19-3320</url>
      <doi>10.18653/v1/W19-3320</doi>
      <bibkey>zhu-etal-2019-towards</bibkey>
    </paper>
    <paper id="21">
      <title>A Dependency Structure Annotation for Modality</title>
      <author><first>Meagan</first><last>Vigus</last></author>
      <author><first>Jens E. L.</first><last>Van Gysel</last></author>
      <author><first>William</first><last>Croft</last></author>
      <pages>182–198</pages>
      <abstract>This paper presents an annotation scheme for modality that employs a dependency structure. Events and sources (here, conceivers) are represented as nodes and epistemic strength relations characterize the edges. The epistemic strength values are largely based on Saurí and Pustejovsky’s (2009) FactBank, while the dependency structure mirrors Zhang and Xue’s (2018b) approach to temporal relations. Six documents containing 377 events have been annotated by two expert annotators with high levels of agreement.</abstract>
      <url hash="3042b80a">W19-3321</url>
      <doi>10.18653/v1/W19-3321</doi>
      <bibkey>vigus-etal-2019-dependency</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="22">
      <title>Augmenting <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation for Human-Robot Dialogue</title>
      <author><first>Claire</first><last>Bonial</last></author>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Stephanie M.</first><last>Lukin</last></author>
      <author><first>Stephen</first><last>Tratz</last></author>
      <author><first>Ron</first><last>Artstein</last></author>
      <author><first>David</first><last>Traum</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>199–210</pages>
      <abstract>We detail refinements made to Abstract Meaning Representation (AMR) that make the representation more suitable for supporting a situated dialogue system, where a human remotely controls a robot for purposes of search and rescue and reconnaissance. We propose 36 augmented AMRs that capture speech acts, tense and aspect, and spatial information. This linguistic information is vital for representing important distinctions, for example whether the robot has moved, is moving, or will move. We evaluate two existing AMR parsers for their performance on dialogue data. We also outline a model for graph-to-graph conversion, in which output from AMR parsers is converted into our refined AMRs. The design scheme presented here, though task-specific, is extendable for broad coverage of speech acts using AMR in future task-independent work.</abstract>
      <url hash="5e764134">W19-3322</url>
      <doi>10.18653/v1/W19-3322</doi>
      <bibkey>bonial-etal-2019-augmenting</bibkey>
    </paper>
  </volume>
  <volume id="34">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Storytelling</booktitle>
      <url hash="11250637">W19-34</url>
      <editor><first>Francis</first><last>Ferraro</last></editor>
      <editor><first>Ting-Hao ‘Kenneth’</first><last>Huang</last></editor>
      <editor><first>Stephanie M.</first><last>Lukin</last></editor>
      <editor><first>Margaret</first><last>Mitchell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>storynlp</venue>
    </meta>
    <frontmatter>
      <url hash="0fc83782">W19-3400</url>
      <bibkey>ws-2019-storytelling</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Composing a Picture Book by Automatic Story Understanding and Visualization</title>
      <author><first>Xiaoyu</first><last>Qi</last></author>
      <author><first>Ruihua</first><last>Song</last></author>
      <author><first>Chunting</first><last>Wang</last></author>
      <author><first>Jin</first><last>Zhou</last></author>
      <author><first>Tetsuya</first><last>Sakai</last></author>
      <pages>1–10</pages>
      <abstract>Pictures can enrich storytelling experiences. We propose a framework that can automatically compose a picture book by understanding story text and visualizing it with painting elements, i.e., characters and backgrounds. For story understanding, we extract key information from a story on both sentence level and paragraph level, including characters, scenes and actions. These concepts are organized and visualized in a way that depicts the development of a story. We collect a set of Chinese stories for children and apply our approach to compose pictures for stories. Extensive experiments are conducted towards story event extraction for visualization to demonstrate the effectiveness of our method.</abstract>
      <url hash="e3c6669a">W19-3401</url>
      <doi>10.18653/v1/W19-3401</doi>
      <bibkey>qi-etal-2019-composing</bibkey>
    </paper>
    <paper id="2">
      <title>“My Way of Telling a Story”: Persona based Grounded Story Generation</title>
      <author><first>Khyathi</first><last>Chandu</last></author>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>11–21</pages>
      <abstract>Visual storytelling is the task of generating stories based on a sequence of images. Inspired by the recent works in neural generation focusing on controlling the form of text, this paper explores the idea of generating these stories in different personas. However, one of the main challenges of performing this task is the lack of a dataset of visual stories in different personas. Having said that, there are independent datasets for both visual storytelling and annotated sentences for various persona. In this paper we describe an approach to overcome this by getting labelled persona data from a different task and leveraging those annotations to perform persona based story generation. We inspect various ways of incorporating personality in both the encoder and the decoder representations to steer the generation in the target direction. To this end, we propose five models which are incremental extensions to the baseline model to perform the task at hand. In our experiments we use five different personas to guide the generation process. We find that the models based on our hypotheses perform better at capturing words while generating stories in the target persona.</abstract>
      <url hash="4a9f63be">W19-3402</url>
      <doi>10.18653/v1/W19-3402</doi>
      <bibkey>chandu-etal-2019-way</bibkey>
    </paper>
    <paper id="3">
      <title>Using Functional Schemas to Understand Social Media Narratives</title>
      <author><first>Xinru</first><last>Yan</last></author>
      <author><first>Aakanksha</first><last>Naik</last></author>
      <author><first>Yohan</first><last>Jo</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <pages>22–33</pages>
      <abstract>We propose a novel take on understanding narratives in social media, focusing on learning ”functional story schemas”, which consist of sets of stereotypical functional structures. We develop an unsupervised pipeline to extract schemas and apply our method to Reddit posts to detect schematic structures that are characteristic of different subreddits. We validate our schemas through human interpretation and evaluate their utility via a text classification task. Our experiments show that extracted schemas capture distinctive structural patterns in different subreddits, improving classification performance of several models by 2.4% on average. We also observe that these schemas serve as lenses that reveal community norms.</abstract>
      <url hash="ca78fd99">W19-3403</url>
      <doi>10.18653/v1/W19-3403</doi>
      <bibkey>yan-etal-2019-using</bibkey>
      <pwccode url="https://github.com/xinru1414/Reddit" additional="false">xinru1414/Reddit</pwccode>
    </paper>
    <paper id="4">
      <title>A Hybrid Model for Globally Coherent Story Generation</title>
      <author><first>Fangzhou</first><last>Zhai</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Pavel</first><last>Shkadzko</last></author>
      <author><first>Wei</first><last>Shi</last></author>
      <author><first>Asad</first><last>Sayeed</last></author>
      <pages>34–45</pages>
      <abstract>Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating recipes and dialogues, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our model outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.</abstract>
      <url hash="c4325e39">W19-3404</url>
      <doi>10.18653/v1/W19-3404</doi>
      <bibkey>zhai-etal-2019-hybrid</bibkey>
    </paper>
    <paper id="5">
      <title>Guided Neural Language Generation for Automated Storytelling</title>
      <author><first>Prithviraj</first><last>Ammanabrolu</last></author>
      <author><first>Ethan</first><last>Tien</last></author>
      <author><first>Wesley</first><last>Cheung</last></author>
      <author><first>Zhaochen</first><last>Luo</last></author>
      <author><first>William</first><last>Ma</last></author>
      <author><first>Lara</first><last>Martin</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>46–55</pages>
      <abstract>Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into: (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates natural language guided by events. Our method outperforms the baseline sequence-to-sequence model. Additionally, we provide results for a full end-to-end automated story generation system, demonstrating how our model works with existing systems designed for the event-to-event problem.</abstract>
      <url hash="0ef71100">W19-3405</url>
      <doi>10.18653/v1/W19-3405</doi>
      <bibkey>ammanabrolu-etal-2019-guided</bibkey>
    </paper>
    <paper id="6">
      <title>An Analysis of Emotion Communication Channels in Fan-Fiction: Towards Emotional Storytelling</title>
      <author><first>Evgeny</first><last>Kim</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>56–64</pages>
      <abstract>Centrality of emotion for the stories told by humans is underpinned by numerous studies in literature and psychology. The research in automatic storytelling has recently turned towards emotional storytelling, in which characters’ emotions play an important role in the plot development (Theune et al., 2004; y Perez, 2007; Mendez et al., 2016). However, these studies mainly use emotion to generate propositional statements in the form “A feels affection towards B” or “A confronts B”. At the same time, emotional behavior does not boil down to such propositional descriptions, as humans display complex and highly variable patterns in communicating their emotions, both verbally and non-verbally. In this paper, we analyze how emotions are expressed non-verbally in a corpus of fan fiction short stories. Our analysis shows that stories written by humans convey character emotions along various non-verbal channels. We find that some non-verbal channels, such as facial expressions and voice characteristics of the characters, are more strongly associated with joy, while gestures and body postures are more likely to occur with trust. Based on our analysis, we argue that automatic storytelling systems should take variability of emotion into account when generating descriptions of characters’ emotions.</abstract>
      <url hash="59bcb00f">W19-3406</url>
      <doi>10.18653/v1/W19-3406</doi>
      <bibkey>kim-klinger-2019-analysis</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>N</fixed-case>arrative <fixed-case>G</fixed-case>eneration in the <fixed-case>W</fixed-case>ild: <fixed-case>M</fixed-case>ethods from <fixed-case>N</fixed-case>a<fixed-case>N</fixed-case>o<fixed-case>G</fixed-case>en<fixed-case>M</fixed-case>o</title>
      <author><first>Judith</first><last>van Stegeren</last></author>
      <author><first>Mariët</first><last>Theune</last></author>
      <pages>65–74</pages>
      <abstract>In text generation, generating long stories is still a challenge. Coherence tends to decrease rapidly as the output length increases. Especially for generated stories, coherence of the narrative is an important quality aspect of the output text. In this paper we examine how narrative coherence is attained in the submissions of NaNoGenMo 2018, an online text generation event where participants are challenged to generate a 50,000 word novel. We list the main approaches that were used to generate coherent narratives and link them to scientific literature. Finally, we give recommendations on when to use which approach.</abstract>
      <url hash="4c16502c">W19-3407</url>
      <doi>10.18653/v1/W19-3407</doi>
      <bibkey>van-stegeren-theune-2019-narrative</bibkey>
      <pwccode url="https://github.com/jd7h/narrative-gen-nanogenmo18" additional="false">jd7h/narrative-gen-nanogenmo18</pwccode>
    </paper>
    <paper id="8">
      <title>Lexical concreteness in narrative</title>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Swapna</first><last>Somasundaran</last></author>
      <pages>75–80</pages>
      <abstract>This study explores the relation between lexical concreteness and narrative text quality. We present a methodology to quantitatively measure lexical concreteness of a text. We apply it to a corpus of student stories, scored according to writing evaluation rubrics. Lexical concreteness is weakly-to-moderately related to story quality, depending on story-type. The relation is mostly borne by adjectives and nouns, but also found for adverbs and verbs.</abstract>
      <url hash="86076795">W19-3408</url>
      <doi>10.18653/v1/W19-3408</doi>
      <bibkey>flor-somasundaran-2019-lexical</bibkey>
    </paper>
    <paper id="9">
      <title>A Simple Approach to Classify Fictional and Non-Fictional Genres</title>
      <author><first>Mohammed Rameez</first><last>Qureshi</last></author>
      <author><first>Sidharth</first><last>Ranjan</last></author>
      <author><first>Rajakrishnan</first><last>Rajkumar</last></author>
      <author><first>Kushal</first><last>Shah</last></author>
      <pages>81–89</pages>
      <abstract>In this work, we deploy a logistic regression classifier to ascertain whether a given document belongs to the fiction or non-fiction genre. For genre identification, previous work had proposed three classes of features, viz., low-level (character-level and token counts), high-level (lexical and syntactic information) and derived features (type-token ratio, average word length or average sentence length). Using the Recursive feature elimination with cross-validation (RFECV) algorithm, we perform feature selection experiments on an exhaustive set of nineteen features (belonging to all the classes mentioned above) extracted from Brown corpus text. As a result, two simple features viz., the ratio of the number of adverbs to adjectives and the number of adjectives to pronouns turn out to be the most significant. Subsequently, our classification experiments aimed towards genre identification of documents from the Brown and Baby BNC corpora demonstrate that the performance of a classifier containing just the two aforementioned features is at par with that of a classifier containing the exhaustive feature set.</abstract>
      <url hash="b1a07b7d">W19-3409</url>
      <doi>10.18653/v1/W19-3409</doi>
      <bibkey>qureshi-etal-2019-simple</bibkey>
    </paper>
    <paper id="10">
      <title>Detecting Everyday Scenarios in Narrative Texts</title>
      <author><first>Lilian Diana Awuor</first><last>Wanzare</last></author>
      <author><first>Michael</first><last>Roth</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <pages>90–106</pages>
      <abstract>Script knowledge consists of detailed information on everyday activities. Such information is often taken for granted in text and needs to be inferred by readers. Therefore, script knowledge is a central component to language comprehension. Previous work on representing scripts is mostly based on extensive manual work or limited to scenarios that can be found with sufficient redundancy in large corpora. We introduce the task of scenario detection, in which we identify references to scripts. In this task, we address a wide range of different scripts (200 scenarios) and we attempt to identify all references to them in a collection of narrative texts. We present a first benchmark data set and a baseline model that tackles scenario detection using techniques from topic segmentation and text classification.</abstract>
      <url hash="bbcebc0c">W19-3410</url>
      <doi>10.18653/v1/W19-3410</doi>
      <bibkey>wanzare-etal-2019-detecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mcscript">MCScript</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/omics">OMICS</pwcdataset>
    </paper>
    <paper id="11">
      <title>Personality Traits Recognition in Literary Texts</title>
      <author><first>Daniele</first><last>Pizzolli</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>107–111</pages>
      <abstract>Interesting stories often are built around interesting characters. Finding and detailing what makes an interesting character is a real challenge, but certainly a significant cue is the character personality traits. Our exploratory work tests the adaptability of the current personality traits theories to literal characters, focusing on the analysis of utterances in theatre scripts. And, at the opposite, we try to find significant traits for interesting characters. The preliminary results demonstrate that our approach is reasonable. Using machine learning for gaining insight into the personality traits of fictional characters can make sense.</abstract>
      <url hash="dc2228d9">W19-3411</url>
      <doi>10.18653/v1/W19-3411</doi>
      <bibkey>pizzolli-strapparava-2019-personality</bibkey>
    </paper>
    <paper id="12">
      <title>Winter is here: Summarizing <fixed-case>T</fixed-case>witter Streams related to Pre-Scheduled Events</title>
      <author><first>Anietie</first><last>Andy</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>112–116</pages>
      <abstract>Pre-scheduled events, such as TV shows and sports games, usually garner considerable attention from the public. Twitter captures large volumes of discussions and messages related to these events, in real-time. Twitter streams related to pre-scheduled events are characterized by the following: (1) spikes in the volume of published tweets reflect the highlights of the event and (2) some of the published tweets make reference to the characters involved in the event, in the context in which they are currently portrayed in a subevent. In this paper, we take advantage of these characteristics to identify the highlights of pre-scheduled events from tweet streams and we demonstrate a method to summarize these highlights. We evaluate our algorithm on tweets collected around 2 episodes of a popular TV show, Game of Thrones, Season 7.</abstract>
      <url hash="dbdb4d63">W19-3412</url>
      <doi>10.18653/v1/W19-3412</doi>
      <revision id="1" href="W19-3412v1" hash="04ffdb66"/>
      <revision id="2" href="W19-3412v2" hash="dbdb4d63">We added a co-author who made substantial contributions to the project.</revision>
      <bibkey>andy-etal-2019-winter</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>W</fixed-case>riter<fixed-case>F</fixed-case>orcing: Generating more interesting story endings</title>
      <author><first>Prakhar</first><last>Gupta</last></author>
      <author><first>Vinayshekhar</first><last>Bannihatti Kumar</last></author>
      <author><first>Mukul</first><last>Bhutani</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>117–126</pages>
      <abstract>We study the problem of generating interesting endings for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same story context. In this paper, we propose models which generate more diverse and interesting outputs by 1) training models to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.</abstract>
      <url hash="48c2a419">W19-3413</url>
      <doi>10.18653/v1/W19-3413</doi>
      <bibkey>gupta-etal-2019-writerforcing</bibkey>
      <pwccode url="https://github.com/witerforcing/WriterForcing" additional="true">witerforcing/WriterForcing</pwccode>
    </paper>
    <paper id="14">
      <title>Prediction of a Movie’s Success From Plot Summaries Using Deep Learning Models</title>
      <author><first>You Jin</first><last>Kim</last></author>
      <author><first>Yun Gyung</first><last>Cheong</last></author>
      <author><first>Jung Hoon</first><last>Lee</last></author>
      <pages>127–135</pages>
      <abstract>As the size of investment for movie production grows bigger, the need for predicting a movie’s success in early stages has increased. To address this need, various approaches have been proposed, mostly relying on movie reviews, trailer movie clips, and SNS postings. However, all of these are available only after a movie is produced and released. To enable a more earlier prediction of a movie’s performance, we propose a deep-learning based approach to predict the success of a movie using only its plot summary text. This paper reports the results evaluating the efficacy of the proposed method and concludes with discussions and future work.</abstract>
      <url hash="eccaab6d">W19-3414</url>
      <doi>10.18653/v1/W19-3414</doi>
      <bibkey>kim-etal-2019-prediction</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-movie-summary-corpus">CMU Movie Summary Corpus</pwcdataset>
    </paper>
  </volume>
  <volume id="35">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Abusive Language Online</booktitle>
      <url hash="b3b427c2">W19-35</url>
      <editor><first>Sarah T.</first><last>Roberts</last></editor>
      <editor><first>Joel</first><last>Tetreault</last></editor>
      <editor><first>Vinodkumar</first><last>Prabhakaran</last></editor>
      <editor><first>Zeerak</first><last>Waseem</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>alw</venue>
    </meta>
    <frontmatter>
      <url hash="4ec94a1b">W19-3500</url>
      <bibkey>ws-2019-abusive</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Subversive Toxicity Detection using Sentiment Information</title>
      <author><first>Eloi</first><last>Brassard-Gourdeau</last></author>
      <author><first>Richard</first><last>Khoury</last></author>
      <pages>1–10</pages>
      <abstract>The presence of toxic content has become a major problem for many online communities. Moderators try to limit this problem by implementing more and more refined comment filters, but toxic users are constantly finding new ways to circumvent them. Our hypothesis is that while modifying toxic content and keywords to fool filters can be easy, hiding sentiment is harder. In this paper, we explore various aspects of sentiment detection and their correlation to toxicity, and use our results to implement a toxicity detection tool. We then test how adding the sentiment information helps detect toxicity in three different real-world datasets, and incorporate subversion to these datasets to simulate a user trying to circumvent the system. Our results show sentiment information has a positive impact on toxicity detection.</abstract>
      <url hash="fb7df7ad">W19-3501</url>
      <doi>10.18653/v1/W19-3501</doi>
      <bibkey>brassard-gourdeau-khoury-2019-subversive</bibkey>
    </paper>
    <paper id="2">
      <title>Exploring Deep Multimodal Fusion of Text and Photo for Hate Speech Classification</title>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Xiaochang</first><last>Peng</last></author>
      <author><first>Gargi</first><last>Ghosh</last></author>
      <author><first>Reshef</first><last>Shilon</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <author><first>Eider</first><last>Moore</last></author>
      <author><first>Goran</first><last>Predovic</last></author>
      <pages>11–18</pages>
      <abstract>Interactions among users on social network platforms are usually positive, constructive and insightful. However, sometimes people also get exposed to objectionable content such as hate speech, bullying, and verbal abuse etc. Most social platforms have explicit policy against hate speech because it creates an environment of intimidation and exclusion, and in some cases may promote real-world violence. As users’ interactions on today’s social networks involve multiple modalities, such as texts, images and videos, in this paper we explore the challenge of automatically identifying hate speech with deep multimodal technologies, extending previous research which mostly focuses on the text signal alone. We present a number of fusion approaches to integrate text and photo signals. We show that augmenting text with image embedding information immediately leads to a boost in performance, while applying additional attention fusion methods brings further improvement.</abstract>
      <url hash="b4fa05f3">W19-3502</url>
      <doi>10.18653/v1/W19-3502</doi>
      <bibkey>yang-etal-2019-exploring-deep</bibkey>
    </paper>
    <paper id="3">
      <title>Detecting harassment in real-time as conversations develop</title>
      <author><first>Wessel</first><last>Stoop</last></author>
      <author><first>Florian</first><last>Kunneman</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <author><first>Ben</first><last>Miller</last></author>
      <pages>19–24</pages>
      <abstract>We developed a machine-learning-based method to detect video game players that harass teammates or opponents in chat earlier in the conversation. This real-time technology would allow gaming companies to intervene during games, such as issue warnings or muting or banning a player. In a proof-of-concept experiment on League of Legends data we compute and visualize evaluation metrics for a machine learning classifier as conversations unfold, and observe that the optimal precision and recall of detecting toxic players at each moment in the conversation depends on the confidence threshold of the classifier: the threshold should start low, and increase as the conversation unfolds. How fast this sliding threshold should increase depends on the training set size.</abstract>
      <url hash="77104a69">W19-3503</url>
      <doi>10.18653/v1/W19-3503</doi>
      <bibkey>stoop-etal-2019-detecting</bibkey>
    </paper>
    <paper id="4">
      <title>Racial Bias in Hate Speech and Abusive Language Detection Datasets</title>
      <author><first>Thomas</first><last>Davidson</last></author>
      <author><first>Debasmita</first><last>Bhattacharya</last></author>
      <author><first>Ingmar</first><last>Weber</last></author>
      <pages>25–35</pages>
      <abstract>Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.</abstract>
      <url hash="62f8c061">W19-3504</url>
      <doi>10.18653/v1/W19-3504</doi>
      <bibkey>davidson-etal-2019-racial</bibkey>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech-and-offensive-language">Hate Speech and Offensive Language</pwcdataset>
    </paper>
    <paper id="5">
      <title>Automated Identification of Verbally Abusive Behaviors in Online Discussions</title>
      <author><first>Srecko</first><last>Joksimovic</last></author>
      <author><first>Ryan S.</first><last>Baker</last></author>
      <author><first>Jaclyn</first><last>Ocumpaugh</last></author>
      <author><first>Juan Miguel L.</first><last>Andres</last></author>
      <author><first>Ivan</first><last>Tot</last></author>
      <author><first>Elle Yuan</first><last>Wang</last></author>
      <author><first>Shane</first><last>Dawson</last></author>
      <pages>36–45</pages>
      <abstract>Discussion forum participation represents one of the crucial factors for learning and often the only way of supporting social interactions in online settings. However, as much as sharing new ideas or asking thoughtful questions contributes learning, verbally abusive behaviors, such as expressing negative emotions in online discussions, could have disproportionate detrimental effects. To provide means for mitigating the potential negative effects on course participation and learning, we developed an automated classifier for identifying communication that show linguistic patterns associated with hostility in online forums. In so doing, we employ several well-established automated text analysis tools and build on the common practices for handling highly imbalanced datasets and reducing the sensitivity to overfitting. Although still in its infancy, our approach shows promising results (ROC AUC .73) towards establishing a robust detector of abusive behaviors. We further provide an overview of the classification (linguistic and contextual) features most indicative of online aggression.</abstract>
      <url hash="336e5e08">W19-3505</url>
      <doi>10.18653/v1/W19-3505</doi>
      <bibkey>joksimovic-etal-2019-automated</bibkey>
    </paper>
    <paper id="6">
      <title>Multi-label Hate Speech and Abusive Language Detection in <fixed-case>I</fixed-case>ndonesian <fixed-case>T</fixed-case>witter</title>
      <author><first>Muhammad Okky</first><last>Ibrohim</last></author>
      <author><first>Indra</first><last>Budi</last></author>
      <pages>46–57</pages>
      <abstract>Hate speech and abusive language spreading on social media need to be detected automatically to avoid conflict between citizen. Moreover, hate speech has a target, category, and level that also needs to be detected to help the authority in prioritizing which hate speech must be addressed immediately. This research discusses multi-label text classification for abusive language and hate speech detection including detecting the target, category, and level of hate speech in Indonesian Twitter using machine learning approach with Support Vector Machine (SVM), Naive Bayes (NB), and Random Forest Decision Tree (RFDT) classifier and Binary Relevance (BR), Label Power-set (LP), and Classifier Chains (CC) as the data transformation method. We used several kinds of feature extractions which are term frequency, orthography, and lexicon features. Our experiment results show that in general RFDT classifier using LP as the transformation method gives the best accuracy with fast computational time.</abstract>
      <url hash="c6d0271d">W19-3506</url>
      <doi>10.18653/v1/W19-3506</doi>
      <bibkey>ibrohim-budi-2019-multi</bibkey>
      <pwccode url="https://github.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection" additional="false">okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection</pwccode>
    </paper>
    <paper id="7">
      <title>The Discourse of Online Content Moderation: Investigating Polarized User Responses to Changes in <fixed-case>R</fixed-case>eddit’s Quarantine Policy</title>
      <author><first>Qinlan</first><last>Shen</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <pages>58–69</pages>
      <abstract>Recent concerns over abusive behavior on their platforms have pressured social media companies to strengthen their content moderation policies. However, user opinions on these policies have been relatively understudied. In this paper, we present an analysis of user responses to a September 27, 2018 announcement about the quarantine policy on Reddit as a case study of to what extent the discourse on content moderation is polarized by users’ ideological viewpoint. We introduce a novel partitioning approach for characterizing user polarization based on their distribution of participation across interest subreddits. We then use automated techniques for capturing framing to examine how users with different viewpoints discuss moderation issues, finding that right-leaning users invoked censorship while left-leaning users highlighted inconsistencies on how content policies are applied. Overall, we argue for a more nuanced approach to moderation by highlighting the intersection of behavior and ideology in considering how abusive language is defined and regulated.</abstract>
      <url hash="0fa46458">W19-3507</url>
      <doi>10.18653/v1/W19-3507</doi>
      <bibkey>shen-rose-2019-discourse</bibkey>
      <pwccode url="https://github.com/qinlans/alw3_data" additional="false">qinlans/alw3_data</pwccode>
    </paper>
    <paper id="8">
      <title>Pay “Attention” to your Context when Classifying Abusive Language</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Kilol</first><last>Gupta</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>70–79</pages>
      <abstract>The goal of any social media platform is to facilitate healthy and meaningful interactions among its users. But more often than not, it has been found that it becomes an avenue for wanton attacks. We propose an experimental study that has three aims: 1) to provide us with a deeper understanding of current data sets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language, and personal attacks); 2) to investigate what type of attention mechanism (contextual vs. self-attention) is better for abusive language detection using deep learning architectures; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task.</abstract>
      <url hash="c52f1ab1">W19-3508</url>
      <doi>10.18653/v1/W19-3508</doi>
      <bibkey>chakrabarty-etal-2019-pay</bibkey>
    </paper>
    <paper id="9">
      <title>Challenges and frontiers in abusive content detection</title>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Alex</first><last>Harris</last></author>
      <author><first>Dong</first><last>Nguyen</last></author>
      <author><first>Rebekah</first><last>Tromble</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <author><first>Helen</first><last>Margetts</last></author>
      <pages>80–93</pages>
      <abstract>Online abusive content detection is an inherently difficult task. It has received considerable attention from academia, particularly within the computational linguistics community, and performance appears to have improved as the field has matured. However, considerable challenges and unaddressed frontiers remain, spanning technical, social and ethical dimensions. These issues constrain the performance, efficiency and generalizability of abusive content detection systems. In this article we delineate and clarify the main challenges and frontiers in the field, critically evaluate their implications and discuss potential solutions. We also highlight ways in which social scientific insights can advance research. We discuss the lack of support given to researchers working with abusive content and provide guidelines for ethical research.</abstract>
      <url hash="b473f1fc">W19-3509</url>
      <doi>10.18653/v1/W19-3509</doi>
      <bibkey>vidgen-etal-2019-challenges</bibkey>
      <pwccode url="https://github.com/bvidgen/Challenges-and-frontiers-in-abusive-content-detection" additional="false">bvidgen/Challenges-and-frontiers-in-abusive-content-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/on-the-origins-of-memes-by-means-of-fringe">On the Origins of Memes by Means of Fringe Web Communities</pwcdataset>
    </paper>
    <paper id="10">
      <title>A Hierarchically-Labeled <fixed-case>P</fixed-case>ortuguese Hate Speech Dataset</title>
      <author><first>Paula</first><last>Fortuna</last></author>
      <author><first>João</first><last>Rocha da Silva</last></author>
      <author><first>Juan</first><last>Soler-Company</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <author><first>Sérgio</first><last>Nunes</last></author>
      <pages>94–104</pages>
      <abstract>Over the past years, the amount of online offensive speech has been growing steadily. To successfully cope with it, machine learning are applied. However, ML-based techniques require sufficiently large annotated datasets. In the last years, different datasets were published, mainly for English. In this paper, we present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, we defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels (‘hate’ vs. ‘no-hate’). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome.</abstract>
      <url hash="908ef15b">W19-3510</url>
      <doi>10.18653/v1/W19-3510</doi>
      <bibkey>fortuna-etal-2019-hierarchically</bibkey>
      <pwccode url="https://github.com/paulafortuna/Portuguese-Hate-Speech-Dataset" additional="false">paulafortuna/Portuguese-Hate-Speech-Dataset</pwccode>
    </paper>
    <paper id="11">
      <title>A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis</title>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Giovanni</first><last>Moretti</last></author>
      <author><first>Michele</first><last>Corazza</last></author>
      <author><first>Elena</first><last>Cabrio</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Serena</first><last>Villata</last></author>
      <pages>105–110</pages>
      <abstract>Social media platforms like Twitter and Instagram face a surge in cyberbullying phenomena against young users and need to develop scalable computational methods to limit the negative consequences of this kind of abuse. Despite the number of approaches recently proposed in the Natural Language Processing (NLP) research area for detecting different forms of abusive language, the issue of identifying cyberbullying phenomena at scale is still an unsolved problem. This is because of the need to couple abusive language detection on textual message with network analysis, so that repeated attacks against the same person can be identified. In this paper, we present a system to monitor cyberbullying phenomena by combining message classification and social network analysis. We evaluate the classification module on a data set built on Instagram messages, and we describe the cyberbullying monitoring user interface.</abstract>
      <url hash="ccfcced6">W19-3511</url>
      <doi>10.18653/v1/W19-3511</doi>
      <bibkey>menini-etal-2019-system</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>L</fixed-case>-<fixed-case>HSAB</fixed-case>: A <fixed-case>L</fixed-case>evantine <fixed-case>T</fixed-case>witter Dataset for Hate Speech and Abusive Language</title>
      <author><first>Hala</first><last>Mulki</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Chedi</first><last>Bechikh Ali</last></author>
      <author><first>Halima</first><last>Alshabani</last></author>
      <pages>111–118</pages>
      <abstract>Hate speech and abusive language have become a common phenomenon on Arabic social media. Automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents. The complexity, informality and ambiguity of the Arabic dialects hindered the provision of the needed resources for Arabic abusive/hate speech detection research. In this paper, we introduce the first publicly-available Levantine Hate Speech and Abusive (L-HSAB) Twitter dataset with the objective to be a benchmark dataset for automatic detection of online Levantine toxic contents. We, further, provide a detailed review of the data collection steps and how we design the annotation guidelines such that a reliable dataset annotation is guaranteed. This has been later emphasized through the comprehensive evaluation of the annotations as the annotation agreement metrics of Cohen’s Kappa (k) and Krippendorff’s alpha (α) indicated the consistency of the annotations.</abstract>
      <url hash="8692eed0">W19-3512</url>
      <doi>10.18653/v1/W19-3512</doi>
      <bibkey>mulki-etal-2019-l</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="13">
      <title>At the Lower End of <fixed-case>L</fixed-case>anguage—<fixed-case>E</fixed-case>xploring the Vulgar and Obscene Side of <fixed-case>G</fixed-case>erman</title>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <author><first>Ulrike</first><last>Krieg-Holz</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>119–128</pages>
      <abstract>In this paper, we describe a workflow for the data-driven acquisition and semantic scaling of a lexicon that covers lexical items from the lower end of the German language register—terms typically considered as rough, vulgar or obscene. Since the fine semantic representation of grades of obscenity can only inadequately be captured at the categorical level (e.g., obscene vs. non-obscene, or rough vs. vulgar), our main contribution lies in applying best-worst scaling, a rating methodology that has already been shown to be useful for emotional language, to capture the relative strength of obscenity of lexical items. We describe the empirical foundations for bootstrapping such a low-end lexicon for German by starting from manually supplied lexicographic categorizations of a small seed set of rough and vulgar lexical items and automatically enlarging this set by means of distributional semantics. We then determine the degrees of obscenity for the full set of all acquired lexical items by letting crowdworkers comparatively assess their pejorative grade using best-worst scaling. This semi-automatically enriched lexicon already comprises 3,300 lexical items and incorporates 33,000 vulgarity ratings. Using it as a seed lexicon for fully automatic lexical acquisition, we were able to raise its coverage up to slightly more than 11,000 entries.</abstract>
      <url hash="6086edde">W19-3513</url>
      <doi>10.18653/v1/W19-3513</doi>
      <bibkey>eder-etal-2019-lower</bibkey>
    </paper>
    <paper id="14">
      <title>Preemptive Toxic Language Detection in <fixed-case>W</fixed-case>ikipedia Comments Using Thread-Level Context</title>
      <author><first>Mladen</first><last>Karan</last></author>
      <author><first>Jan</first><last>Šnajder</last></author>
      <pages>129–134</pages>
      <abstract>We address the task of automatically detecting toxic content in user generated texts. We fo cus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment. Moreover, we perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments. Using an existing dataset of conversations among Wikipedia contributors as a starting point, we compile a new large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.</abstract>
      <url hash="44af45da">W19-3514</url>
      <doi>10.18653/v1/W19-3514</doi>
      <bibkey>karan-snajder-2019-preemptive</bibkey>
    </paper>
    <paper id="15">
      <title>Neural Word Decomposition Models for Abusive Language Detection</title>
      <author><first>Sravan</first><last>Bodapati</last></author>
      <author><first>Spandana</first><last>Gella</last></author>
      <author><first>Kasturi</first><last>Bhattacharjee</last></author>
      <author><first>Yaser</first><last>Al-Onaizan</last></author>
      <pages>135–145</pages>
      <abstract>The text we see in social media suffers from lots of undesired characterstics like hatespeech, abusive language, insults etc. The nature of this text is also very different compared to the traditional text we see in news with lots of obfuscated words, intended typos. This poses several robustness challenges to many natural language processing (NLP) techniques developed for traditional text. Many techniques proposed in the recent times such as charecter encoding models, subword models, byte pair encoding to extract subwords can aid in dealing with few of these nuances. In our work, we analyze the effectiveness of each of the above techniques, compare and contrast various word decomposition techniques when used in combination with others. We experiment with recent advances of finetuning pretrained language models, and demonstrate their robustness to domain shift. We also show our approaches achieve state of the art performance on Wikipedia attack, toxicity datasets, and Twitter hatespeech dataset.</abstract>
      <url hash="2942f8ff">W19-3515</url>
      <doi>10.18653/v1/W19-3515</doi>
      <revision id="1" href="W19-3515v1" hash="b7aa5cc5"/>
      <revision id="2" href="W19-3515v2" hash="2942f8ff">Added a third author and removed her name from the acknowledgments.</revision>
      <bibkey>bodapati-etal-2019-neural</bibkey>
    </paper>
    <paper id="16">
      <title>A Platform Agnostic Dual-Strand Hate Speech Detector</title>
      <author><first>Johannes Skjeggestad</first><last>Meyer</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <pages>146–156</pages>
      <abstract>Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text’s author may differ between services, and so using such data would reduce a system’s general applicability. The paper thus focuses on using exclusively text-based input in the detection, in an optimised architecture combining Convolutional Neural Networks and Long Short-Term Memory-networks. The hate speech detector merges two strands with character n-grams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches.</abstract>
      <url hash="9d08ba6f">W19-3516</url>
      <doi>10.18653/v1/W19-3516</doi>
      <bibkey>meyer-gamback-2019-platform</bibkey>
    </paper>
    <paper id="17">
      <title>Detecting Aggression and Toxicity using a Multi Dimension Capsule Network</title>
      <author><first>Saurabh</first><last>Srivastava</last></author>
      <author><first>Prerna</first><last>Khurana</last></author>
      <pages>157–162</pages>
      <abstract>In the era of social media, hate speech, trolling and verbal abuse have become a common issue. We present an approach to automatically classify such statements, using a new deep learning architecture. Our model comprises of a Multi Dimension Capsule Network that generates the representation of sentences which we use for classification. We further provide an analysis of our model’s interpretation of such statements. We compare the results of our model with state-of-art classification algorithms and demonstrate our model’s ability. It also has the capability to handle comments that are written in both Hindi and English, which are provided in the TRAC dataset. We also compare results on Kaggle’s Toxic comment classification dataset.</abstract>
      <url hash="d6262dee">W19-3517</url>
      <doi>10.18653/v1/W19-3517</doi>
      <bibkey>srivastava-khurana-2019-detecting</bibkey>
    </paper>
    <paper id="18">
      <title>An Impossible Dialogue! Nominal Utterances and Populist Rhetoric in an <fixed-case>I</fixed-case>talian <fixed-case>T</fixed-case>witter Corpus of Hate Speech against Immigrants</title>
      <author><first>Gloria</first><last>Comandini</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <pages>163–171</pages>
      <abstract>The paper proposes an investigation on the role of populist themes and rhetoric in an Italian Twitter corpus of hate speech against immigrants. The corpus had been annotated with four new layers of analysis: Nominal Utterances, that can be seen as consistent with populist rhetoric; In-out-group rhetoric, a very common populist strategy to polarize public opinion; Slogan-like nominal utterances, that may convey the call for severe illiberal policies against immigrants; News, to recognize the role of newspapers (headlines or reference to articles) in the Twitter political discourse on immigration featured by hate speech.</abstract>
      <url hash="8ca50ff6">W19-3518</url>
      <doi>10.18653/v1/W19-3518</doi>
      <bibkey>comandini-patti-2019-impossible</bibkey>
    </paper>
    <paper id="19">
      <title>“Condescending, Rude, Assholes”: Framing gender and hostility on <fixed-case>S</fixed-case>tack <fixed-case>O</fixed-case>verflow</title>
      <author><first>Sian</first><last>Brooke</last></author>
      <pages>172–180</pages>
      <abstract>The disciplines of Gender Studies and Data Science are incompatible. This is conventional wisdom, supported by how many computational studies simplify gender into an immutable binary categorization that appears crude to the critical social researcher. I argue that the characterization of gender norms is context specific and may prove valuable in constructing useful models. I show how gender can be framed in computational studies as a stylized repetition of acts mediated by a social structure, and not a possessed biological category. By conducting a review of existing work, I show how gender should be explored in multiplicity in computational research through clustering techniques, and layout how this is being achieved in a study in progress on gender hostility on Stack Overflow.</abstract>
      <url hash="a07018c6">W19-3519</url>
      <doi>10.18653/v1/W19-3519</doi>
      <bibkey>brooke-2019-condescending</bibkey>
    </paper>
    <paper id="20">
      <title>Online aggression from a sociological perspective: An integrative view on determinants and possible countermeasures</title>
      <author><first>Sebastian</first><last>Weingartner</last></author>
      <author><first>Lea</first><last>Stahel</last></author>
      <pages>181–187</pages>
      <abstract>The present paper introduces a theoretical model for explaining aggressive online comments from a sociological perspective. It is innovative as it combines individual, situational, and social-structural determinants of online aggression and tries to theoretically derive their interplay. Moreover, the paper suggests an empirical strategy for testing the model. The main contribution will be to match online commenting data with survey data containing rich background data of non- /aggressive online commentators.</abstract>
      <url hash="8be78725">W19-3520</url>
      <doi>10.18653/v1/W19-3520</doi>
      <bibkey>weingartner-stahel-2019-online</bibkey>
    </paper>
  </volume>
  <volume id="36">
    <meta>
      <booktitle>Proceedings of the 2019 Workshop on Widening NLP</booktitle>
      <editor><first>Amittai</first><last>Axelrod</last></editor>
      <editor><first>Diyi</first><last>Yang</last></editor>
      <editor><first>Rossana</first><last>Cunha</last></editor>
      <editor><first>Samira</first><last>Shaikh</last></editor>
      <editor><first>Zeerak</first><last>Waseem</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>winlp</venue>
    </meta>
    <frontmatter>
      <url hash="632e1261">W19-3600</url>
      <bibkey>ws-2019-2019</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Development of a General Purpose Sentiment Lexicon for <fixed-case>I</fixed-case>gbo Language</title>
      <author><first>Emeka</first><last>Ogbuju</last></author>
      <author><first>Moses</first><last>Onyesolu</last></author>
      <pages>1</pages>
      <abstract>There are publicly available general purpose sentiment lexicons in some high resource languages but very few exist in the low resource languages. This makes it difficult to directly perform sentiment analysis tasks in such languages. The objective of this work is to create a general purpose sentiment lexicon for Igbo language that can determine the sentiment of documents written in Igbo language without having to translate it to English language. The material used was an automatically translated Liu’s lexicon and manual addition of Igbo native words. The result of this work is a general purpose lexicon – IgboSentilex. The performance was tested on the BBC Igbo news channel. It returned an average polarity agreement of 95% with other general purpose sentiment lexicons.</abstract>
      <bibkey>ogbuju-onyesolu-2019-development</bibkey>
    </paper>
    <paper id="2">
      <title>Towards a Resource Grammar for <fixed-case>R</fixed-case>unyankore and Rukiga</title>
      <author><first>David</first><last>Bamutura</last></author>
      <author><first>Peter</first><last>Ljunglöf</last></author>
      <pages>2–6</pages>
      <abstract>Currently, there is a lack of computational grammar resources for many under-resourced languages which limits the ability to develop Natural Language Processing (NLP) tools and applications such as Multilingual Document Authoring, Computer-Assisted Language Learning (CALL) and Low-Coverage Machine Translation (MT) for these languages. In this paper, we present our attempt to formalise the grammar of two such languages: Runyankore and Rukiga. For this formalisation we use the Grammatical Framework (GF) and its Resource Grammar Library (GF-RGL).</abstract>
      <bibkey>bamutura-ljunglof-2019-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Speech Recognition for <fixed-case>T</fixed-case>igrinya language Using Deep Neural Network Approach</title>
      <author><first>Hafte</first><last>Abera</last></author>
      <author><first>Sebsibe</first><last>H/mariam</last></author>
      <pages>7–9</pages>
      <abstract>This work presents a speech recognition model for Tigrinya language .The Deep Neural Network is used to make the recognition model. The Long Short-Term Memory Network (LSTM), which is a special kind of Recurrent Neural Network composed of Long Short-Term Memory blocks, is the primary layer of our neural network model. The 40-dimensional features are MFCC-LDA-MLLT-fMLLR with CMN were used. The acoustic models are trained on features that are obtained by projecting down to 40 dimensions using linear discriminant analysis (LDA). Moreover, speaker adaptive training (SAT) is done using a single feature-space maximum likelihood linear regression (FMLLR) transform estimated per speaker. We train and compare LSTM and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. Finally, the accuracy of the model is evaluated based on the recognition rate.</abstract>
      <bibkey>abera-h-mariam-2019-speech</bibkey>
    </paper>
    <paper id="4">
      <title>Knowledge-Based Word Sense Disambiguation with Distributional Semantic Expansion</title>
      <author><first>Hossein</first><last>Rouhizadeh</last></author>
      <author><first>Mehrnoush</first><last>Shamsfard</last></author>
      <author><first>Masoud</first><last>Rouhizadeh</last></author>
      <pages>10</pages>
      <abstract>In this paper, we presented a WSD system that uses LDA topics for semantic expansion of document words. Our system also uses sense frequency information from SemCor to give higher priority to the senses which are more probable to happen.</abstract>
      <bibkey>rouhizadeh-etal-2019-knowledge</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>A</fixed-case>spe<fixed-case>R</fixed-case>a: Aspect-Based Rating Prediction Based on User Reviews</title>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <author><first>Sergey</first><last>Nikolenko</last></author>
      <author><first>Anton</first><last>Alekseev</last></author>
      <author><first>Ilya</first><last>Shenbin</last></author>
      <pages>11–13</pages>
      <abstract>We propose a novel Aspect-based Rating Prediction model (AspeRa) that estimates user rating based on review texts for the items. It is based on aspect extraction with neural networks and combines the advantages of deep learning and topic modeling. It is mainly designed for recommendations, but an important secondary goal of AspeRa is to discover coherent aspects of reviews that can be used to explain predictions or for user profiling. We conduct a comprehensive empirical study of AspeRa, showing that it outperforms state-of-the-art models in terms of recommendation quality and produces interpretable aspects. This paper is an abridged version of our work (Nikolenko et al., 2019)</abstract>
      <bibkey>tutubalina-etal-2019-aspera</bibkey>
    </paper>
    <paper id="6">
      <title>Recognizing Arrow Of Time In The Short Stories</title>
      <author><first>Fahimeh</first><last>Hosseini</last></author>
      <author><first>Hosein</first><last>Fooladi</last></author>
      <author><first>Mohammad Reza</first><last>Samsami</last></author>
      <pages>14–16</pages>
      <abstract>Recognizing the arrow of time in the context of paragraphs in short stories is a challenging task. i.e., given only two paragraphs (excerpted from a random position in a short story), determining which comes first and which comes next is a difficult task even for humans. In this paper, we have collected and curated a novel dataset for tackling this challenging task. We have shown that a pre-trained BERT architecture achieves reasonable accuracy on the task, and outperforms RNN-based architectures.</abstract>
      <bibkey>hosseini-etal-2019-recognizing</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>A</fixed-case>mharic Word Sequence Prediction</title>
      <author><first>Nuniyat</first><last>Kifle</last></author>
      <pages>17</pages>
      <abstract>The significance of computers and handheld devices are not deniable in the modern world of today. Texts are entered to these devices using word processing programs as well as other techniques and word prediction is one of the techniques. Word Prediction is the action of guessing or forecasting what word comes after, based on some current information, and it is the main focus of this study. Even though Amharic is used by a large number of populations, no significant work is done on the topic of word sequence prediction. In this study, Amharic word sequence prediction model is developed with statistical methods using Hidden Markov Model by incorporating detailed Part of speech tag. Evaluation of the model is performed using developed prototype and keystroke savings (KSS) as a metrics. According to our experiment, prediction result using a bi-gram with detailed Part of Speech tag model has higher KSS and it is better compared to tri-gram model and better than those without Part of Speech tag. Therefore, statistical approach with Detailed POS has quite good potential on word sequence prediction for Amharic language. This research deals with designing word sequence prediction model in Amharic language. It is a language that is spoken in eastern Africa. One of the needs for Amharic word sequence prediction for mobile use and other digital devices is in order to facilitate data entry and communication in our language. Word sequence prediction is a challenging task for inflected languages. (Arora, 2007) These kinds of languages are morphologically rich and have enormous word forms. i.e. one word can have different forms. As Amharic language is highly inflected language and morphologically rich it shares this problem. (prediction, 2008) This problem makes word prediction system much more difficult and results poor performance. Due to this reason storing all forms in dictionary won’t solve the problem as in English and other less inflected languages. But considering other techniques that could help the predictor to suggest the next word like a POS based prediction should be used. Previous researches used dictionary approach with no consideration of context information. Hence storing all forms of words in dictionary for inflected languages such as Amharic language has been less effective. The main goal of this thesis is to implement Amharic word prediction model that works with better prediction speed and with narrowed search space as much as possible. We introduced two models; tags and words and linear interpolation that use part of speech tag information in addition to word n-grams in order to maximize the likelihood of syntactic appropriateness of the suggestions. We believe the results found reflect this. Amharic word sequence prediction using bi-gram model with higher POS weight and detailed Part of speech tag gave better keystroke savings in all scenarios of our experiment. The study followed Design Science Research Methodology (DSRM). Since DSRM includes approaches, techniques, tools, algorithms and evaluation mechanisms in the process, we followed statistical approach with statistical language modeling and built Amharic prediction model based on information from Part of Speech tagger. The statistics included in the systems varies from single word frequencies to part-of-speech tag n-grams. That means it included the statistics of Word frequencies, Word sequence frequencies, Part-of-speech sequence frequencies and other important information. Later on the system was evaluated using Keystroke Savings. (Lindh, 011). Linux mint was used as the main Operation System during the frame work design. We used corpus of 680,000 tagged words that has 31 tag sets, python programming language and its libraries for both the part of speech tagger and the predictor module. Other Tool that was used is the SRILIM (The SRI language modeling toolkit) in order to generate unigram bigram and trigram count as an input for the language model. SRILIM is toolkit that uses to build and apply statistical language modeling. This thesis presented Amharic word sequence prediction model using the statistical approach. We described a combined statistical and lexical word prediction system for handling inflected languages by making use of POS tags to build the language model. We developed Amharic language models of bigram and trigram for the training purpose. We obtained 29% of KSS using bigram model with detailed part ofspeech tag. Hence, Based on the experiments carried out for this study and the results obtained, the following conclusions were made. We concluded that employing syntactic information in the form of Part-of-Speech (POS) n-grams promises more effective predictions. We also can conclude data quantity, performance of POS tagger and data quality highly affects the keystroke savings. Here in our study the tests were done on a small collection of 100 phrases. According to our evaluation better Keystroke saving (KSS) is achieved when using bi-gram model than the tri-gram models. We believe the results obtained using the experiment of detailed Part of speech tags were effective Since speed and search space are the basic issues in word sequence prediction</abstract>
      <bibkey>kifle-2019-amharic</bibkey>
    </paper>
    <paper id="8">
      <title>A Framework for Relation Extraction Across Multiple Datasets in Multiple Domains</title>
      <author><first>Geeticka</first><last>Chauhan</last></author>
      <author><first>Matthew</first><last>McDermott</last></author>
      <author><first>Peter</first><last>Szolovits</last></author>
      <pages>18–20</pages>
      <abstract>In this work, we aim to build a unifying framework for relation extraction (RE), applying this on 3 highly used datasets with the ability to be extendable to new datasets. At the moment, the domain suffers from lack of reproducibility as well as a lack of consensus on generalizable techniques. Our framework will be open-sourced and will aid in performing systematic exploration on the effect of different modeling techniques, pre-processing, training methodologies and evaluation metrics on the 3 datasets to help establish a consensus.</abstract>
      <bibkey>chauhan-etal-2019-framework</bibkey>
    </paper>
    <paper id="9">
      <title>Learning and Understanding Different Categories of Sexism Using Convolutional Neural Network’s Filters</title>
      <author><first>Sima</first><last>Sharifirad</last></author>
      <author><first>Alon</first><last>Jacovi</last></author>
      <pages>21–23</pages>
      <abstract>Sexism is very common in social media and makes the boundaries of free speech tighter for female users. Automatically flagging and removing sexist content requires niche identification and description of the categories. In this study, inspired by social science work, we propose three categories of sexism toward women as follows: “Indirect sexism”, “Sexual sexism” and “Physical sexism”. We build classifiers such as Convolutional Neural Network (CNN) to automatically detect different types of sexism and address problems of annotation. Even though inherent non-interpretability of CNN is a challenge for users who detect sexism, as the reason classifying a given speech instance with regard to sexism is difficult to glance from a CNN. However, recent research developed interpretable CNN filters for text data. In a CNN, filters followed by different activation patterns along with global max-pooling can help us tease apart the most important ngrams from the rest. In this paper, we interpret a CNN model trained to classify sexism in order to understand different categories of sexism by detecting semantic categories of ngrams and clustering them. Then, these ngrams in each category are used to improve the performance of the classification task. It is a preliminary work using machine learning and natural language techniques to learn the concept of sexism and distinguishes itself by looking at more precise categories of sexism in social media along with an in-depth investigation of CNN’s filters.</abstract>
      <bibkey>sharifirad-jacovi-2019-learning</bibkey>
    </paper>
    <paper id="10">
      <title>Modeling Five Sentence Quality Representations by Finding Latent Spaces Produced with Deep Long Short-Memory Models</title>
      <author><first>Pablo</first><last>Rivas</last></author>
      <pages>24–26</pages>
      <abstract>We present a study in which we train neural models that approximate rules that assess the quality of English sentences. We modeled five rules using deep LSTMs trained over a dataset of sentences whose quality is evaluated under such rules. Preliminary results suggest the neural architecture can model such rules to high accuracy.</abstract>
      <bibkey>rivas-2019-modeling</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>E</fixed-case>thiopian Languages Statistical Machine Translation</title>
      <author><first>Solomon Teferra</first><last>Abate</last></author>
      <author><first>Michael</first><last>Melese</last></author>
      <author><first>Martha Yifiru</first><last>Tachbelie</last></author>
      <author><first>Million</first><last>Meshesha</last></author>
      <author><first>Solomon</first><last>Atinafu</last></author>
      <author><first>Wondwossen</first><last>Mulugeta</last></author>
      <author><first>Yaregal</first><last>Assabie</last></author>
      <author><first>Hafte</first><last>Abera</last></author>
      <author><first>Biniyam</first><last>Ephrem</last></author>
      <author><first>Tewodros</first><last>Gebreselassie</last></author>
      <author><first>Wondimagegnhue Tsegaye</first><last>Tufa</last></author>
      <author><first>Amanuel</first><last>Lemma</last></author>
      <author><first>Tsegaye</first><last>Andargie</last></author>
      <author><first>Seifedin</first><last>Shifaw</last></author>
      <pages>27–30</pages>
      <abstract>In this paper, we describe an attempt towards the development of parallel corpora for English and Ethiopian Languages, such as Amharic, Tigrigna, Afan-Oromo, Wolaytta and Ge’ez. The corpora are used for conducting bi-directional SMT experiments. The BLEU scores of the bi-directional SMT systems show a promising result. The morphological richness of the Ethiopian languages has a great impact on the performance of SMT especially when the targets are Ethiopian languages.</abstract>
      <bibkey>abate-etal-2019-english</bibkey>
    </paper>
    <paper id="12">
      <title>An automatic discourse relation alignment experiment on <fixed-case>TED</fixed-case>-<fixed-case>MDB</fixed-case></title>
      <author><first>Sibel</first><last>Ozer</last></author>
      <author><first>Deniz</first><last>Zeyrek</last></author>
      <pages>31–34</pages>
      <abstract>This paper describes an automatic discourse relation alignment experiment as an empirical justification of the planned annotation projection approach to enlarge the 3600-word multilingual corpus of TED Multilingual Discourse Bank (TED-MDB). The experiment is carried out on a single language pair (English-Turkish) included in TED-MDB. The paper first describes the creation of a large corpus of English-Turkish bi-sentences, then it presents a sense-based experiment that automatically aligns the relations in the English sentences of TED-MDB with the Turkish sentences. The results are very close to the results obtained from an earlier semi-automatic post-annotation alignment experiment validated by human annotators and are encouraging for future annotation projection tasks.</abstract>
      <bibkey>ozer-zeyrek-2019-automatic</bibkey>
    </paper>
    <paper id="13">
      <title>The Design and Construction of the Corpus of <fixed-case>C</fixed-case>hina <fixed-case>E</fixed-case>nglish</title>
      <author><first>Lixin</first><last>Xia</last></author>
      <author><first>Yun</first><last>Xia</last></author>
      <pages>35–37</pages>
      <abstract>The paper describes the development a corpus of an English variety, i.e. China English, in or-der to provide a linguistic resource for researchers in the field of China English. The Corpus of China English (CCE) was built with due consideration given to its representativeness and authenticity. It was composed of more than 13,962,102 tokens in 15,333 texts evenly divided between the following four genres: newspapers, magazines, fiction and academic writings. The texts cover a wide range of domains, such as news, financial, politics, environment, social, culture, technology, sports, education, philosophy, literary, etc. It is a helpful resource for research on China English, computational linguistics, natural language processing, corpus linguistics and English language education.</abstract>
      <bibkey>xia-xia-2019-design</bibkey>
    </paper>
    <paper id="14">
      <title>Learning Trilingual Dictionaries for <fixed-case>U</fixed-case>rdu – <fixed-case>R</fixed-case>oman <fixed-case>U</fixed-case>rdu – <fixed-case>E</fixed-case>nglish</title>
      <author><first>Moiz</first><last>Rauf</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>38–42</pages>
      <abstract>In this paper, we present an effort to generate a joint Urdu, Roman Urdu and English trilingual lexicon using automated methods. We make a case for using statistical machine translation approaches and parallel corpora for dictionary creation. To this purpose, we use word alignment tools on the corpus and evaluate translations using human evaluators. Despite different writing script and considerable noise in the corpus our results show promise with over 85% accuracy of Roman Urdu–Urdu and 45% English–Urdu pairs.</abstract>
      <bibkey>rauf-pado-2019-learning</bibkey>
    </paper>
    <paper id="15">
      <title>Joint Inference on Bilingual Parse Trees for <fixed-case>PP</fixed-case>-attachment Disambiguation</title>
      <author><first>Geetanjali</first><last>Rakshit</last></author>
      <pages>43–45</pages>
      <abstract>Prepositional Phrase (PP) attachment is a classical problem in NLP for languages like English, which suffer from structural ambiguity. In this work, we solve this problem with the help of another language free from such ambiguities, using the parse tree of the parallel sentence in the other language, and word alignments. We formulate an optimization framework that encourages agreement between the parse trees for two languages, and solve it using a novel Dual Decomposition (DD) based algorithm. Experiments on the English-Hindi language pair show promising improvements over the baseline.</abstract>
      <bibkey>rakshit-2019-joint</bibkey>
    </paper>
    <paper id="16">
      <title>Using Attention-based Bidirectional <fixed-case>LSTM</fixed-case> to Identify Different Categories of Offensive Language Directed Toward Female Celebrities</title>
      <author><first>Sima</first><last>Sharifirad</last></author>
      <author><first>Stan</first><last>Matwin</last></author>
      <pages>46–48</pages>
      <abstract>Social media posts reflect the emotions, intentions and mental state of the users. Twitter users who harass famous female figures may do so with different intentions and intensities. Recent studies have published datasets focusing on different types of online harassment, vulgar language, and emotional intensities. We trained, validate and test our proposed model, attention-based bidirectional neural network, on the three datasets:”online harassment”, “vulgar language” and “valance” and achieved state of the art performance in two of the datasets. We report F1 score for each dataset separately along with the final precision, recall and macro-averaged F1 score. In addition, we identify ten female figures from different professions and racial backgrounds who have experienced harassment on Twitter. We tested the trained models on ten collected corpuses each related to one famous female figure to predict the type of harassing language, the type of vulgar language and the degree of intensity of language occurring on their social platforms. Interestingly, the achieved results show different patterns of linguistic use targeting different racial background and occupations. The contribution of this study is two-fold. From the technical perspective, our proposed methodology is shown to be effective with a good margin in comparison to the previous state-of-the-art results on one of the two available datasets. From the social perspective, we introduce a methodology which can unlock facts about the nature of offensive language targeting women on online social platforms. The collected dataset will be shared publicly for further investigation.</abstract>
      <bibkey>sharifirad-matwin-2019-using</bibkey>
    </paper>
    <paper id="17">
      <title>Sentiment Analysis Model for Opinionated <fixed-case>A</fixed-case>wngi Text: Case of Music Reviews</title>
      <author><first>Melese</first><last>Mihret</last></author>
      <author><first>Muluneh</first><last>Atinaf</last></author>
      <pages>49–51</pages>
      <abstract>Abstract The analysis of sentiments is imperative to make a decision for individuals, organizations, and governments. Due to the rapid growth of Awngi (Agew) text on the web, there is no available corpus annotated for sentiment analysis. In this paper, we present a SA model for the Awngi language spoken in Ethiopia, by using a supervised machine learning approach. We developed our corpus by collecting around 1500 posts from online sources. This research is begun to build and evaluate the model for opinionated Awngi music reviews. Thus, pre-processing techniques have been employed to clean the data, to convert transliterations to the native Ethiopic script for accessibility and convenience to typing and to change the words to their base form by removing the inflectional morphemes. After pre-processing, the corpus is manually annotated by three the language professional for giving polarity, and rate, their level of confidence in their selection and sentiment intensity scale values. To improve the calculation method of feature selection and weighting and proposed a more suitable SA algorithm for feature extraction named CHI and weight calculation named TF IDF, increasing the proportion and weight of sentiment words in the feature words. We employed Support Vector Machines (SVM), Naïve Bayes (NB) and Maximum Entropy (MxEn) machine learning algorithms. Generally, the results are encouraging, despite the morphological challenge in Awngi, the data cleanness and small size of data. We are believed that the results could improve further with a larger corpus.</abstract>
      <bibkey>mihret-atinaf-2019-sentiment</bibkey>
    </paper>
    <paper id="18">
      <title>A compositional view of questions</title>
      <author><first>Maria</first><last>Boritchev</last></author>
      <author><first>Maxime</first><last>Amblard</last></author>
      <pages>52</pages>
      <abstract>We present a research on compositional treatment of questions in neo-davidsonian event semantics style. Our work is based on (Champollion, 2011) where only declarative sentences were considered. Our research is based on complex formal examples, paving the way towards further research in this domain and further testing on real-life corpora.</abstract>
      <bibkey>boritchev-amblard-2019-compositional</bibkey>
    </paper>
    <paper id="19">
      <title>Controlling the Specificity of Clarification Question Generation</title>
      <author><first>Yang Trista</first><last>Cao</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <pages>53–56</pages>
      <abstract>Unlike comprehension-style questions, clarification questions look for some missing information in a given context. However, without guidance, neural models for question generation, similar to dialog generation models, lead to generic and bland questions that cannot elicit useful information. We argue that controlling the level of specificity of the generated questions can have useful applications and propose a neural clarification question generation model for the same. We first train a classifier that annotates a clarification question with its level of specificity (generic or specific) to the given context. Our results on the Amazon questions dataset demonstrate that training a clarification question generation model on specificity annotated data can generate questions with varied levels of specificity to the given context.</abstract>
      <bibkey>cao-etal-2019-controlling</bibkey>
    </paper>
    <paper id="20">
      <title>Non-Monotonic Sequential Text Generation</title>
      <author><first>Kiante</first><last>Brantley</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Hal</first><last>Daumé</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <pages>57–59</pages>
      <abstract>Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order while achieving competitive performance with conventional left-to-right generation.</abstract>
      <bibkey>brantley-etal-2019-non</bibkey>
    </paper>
    <paper id="21">
      <title>Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</title>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>60–63</pages>
      <abstract>Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.</abstract>
      <bibkey>gonen-goldberg-2019-lipstick-pig</bibkey>
    </paper>
    <paper id="22">
      <title>How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?</title>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>64–67</pages>
      <abstract>Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun’s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While “embedding debiasing” methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words’ context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.</abstract>
      <bibkey>gonen-etal-2019-grammatical-gender</bibkey>
    </paper>
    <paper id="23">
      <title>Automatic Product Categorization for Official Statistics</title>
      <author><first>Andrea</first><last>Roberson</last></author>
      <pages>68–72</pages>
      <abstract>The North American Product Classification System (NAPCS) is a comprehensive, hierarchical classification system for products (goods and services) that is consistent across the three North American countries. Beginning in 2017, the Economic Census will use NAPCS to produce economy-wide product tabulations. Respondents are asked to report data from a long, pre-specified list of potential products in a given industry, with some lists containing more than 50 potential products. Businesses have expressed the desire to alternatively supply Universal Product Codes (UPC) to the U. S. Census Bureau. Much work has been done around the categorization of products using product descriptions. No study has applied these efforts for the calculation of official statistics (statistics published by government agencies) using only the text of UPC product descriptions. The question we address in this paper is: Given UPC codes and their associated product descriptions, can we accurately predict NAPCS? We tested the feasibility of businesses submitting a spreadsheet with Universal Product Codes and their associated text descriptions. This novel strategy classified text with very high accuracy rates, all of our algorithms surpassed over 90 percent.</abstract>
      <bibkey>roberson-2019-automatic</bibkey>
    </paper>
    <paper id="24">
      <title>An Online Topic Modeling Framework with Topics Automatically Labeled</title>
      <author><first>Jin</first><last>Fenglei</last></author>
      <author><first>Gao</first><last>Cuiyun</last></author>
      <author><first>Lyu</first><last>Michael R.</last></author>
      <pages>73–76</pages>
      <abstract>In this paper, we propose a novel online topic tracking framework, named IEDL, for tracking the topic changes related to deep learning techniques on Stack Exchange and automatically interpreting each identified topic. The proposed framework combines the prior topic distributions in a time window during inferring the topics in current time slice, and introduces a new ranking scheme to select most representative phrases and sentences for the inferred topics. Experiments on 7,076 Stack Exchange posts show the effectiveness of IEDL in tracking topic changes.</abstract>
      <bibkey>fenglei-etal-2019-online</bibkey>
    </paper>
    <paper id="25">
      <title>Construction and Alignment of Multilingual Entailment Graphs for Semantic Inference</title>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>77–79</pages>
      <abstract>This paper presents ongoing work on the construction and alignment of predicate entailment graphs in English and German. We extract predicate-argument pairs from large corpora of monolingual English and German news text and construct monolingual paraphrase clusters and entailment graphs. We use an aligned subset of entities to derive the bilingual alignment of entities and relations, and achieve better than baseline results on a translated subset of a predicate entailment data set (Levy and Dagan, 2016) and the German portion of XNLI (Conneau et al., 2018).</abstract>
      <bibkey>weber-steedman-2019-construction</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>KB</fixed-case>-<fixed-case>NLG</fixed-case>: From Knowledge Base to Natural Language Generation</title>
      <author><first>Wen</first><last>Cui</last></author>
      <author><first>Minghui</first><last>Zhou</last></author>
      <author><first>Rongwen</first><last>Zhao</last></author>
      <author><first>Narges</first><last>Norouzi</last></author>
      <pages>80–82</pages>
      <abstract>We perform the natural language generation (NLG) task by mapping sets of Resource Description Framework (RDF) triples into text. First we investigate the impact of increasing the number of entity types in delexicalisaiton on the generation quality. Second we conduct different experiments to evaluate two widely applied language generation systems, encoder-decoder with attention and the Transformer model on a large benchmark dataset. We evaluate different models on automatic metrics, as well as the training time. To our knowledge, we are the first to apply Transformer model to this task.</abstract>
      <bibkey>cui-etal-2019-kb</bibkey>
    </paper>
    <paper id="27">
      <title>Acoustic Characterization of Singaporean Children’s <fixed-case>E</fixed-case>nglish: Comparisons to <fixed-case>A</fixed-case>merican and <fixed-case>B</fixed-case>ritish Counterparts</title>
      <author><first>Yuling</first><last>Gu</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>83–87</pages>
      <abstract>We investigate English pronunciation patterns in Singaporean children in relation to their American and British counterparts by conducting archetypal analysis on selected vowel pairs. Given that Singapore adopts British English as the institutional standard, one might expect Singaporean children to follow British pronunciation patterns, but we observe that Singaporean children also present similar patterns to Americans for TRAP-BATH spilt vowels: (1) British and Singaporean children both produce these vowels with a relatively lowered tongue height. (2) These vowels are more fronted for American and Singaporean children (p &lt; 0.001). In addition, when comparing /æ/ and /ε/ productions, British speakers show the clearest distinction between the two vowels; Singaporean and American speakers exhibit a higher and more fronted tongue position for /æ/ (p &lt; 0.001), causing /æ/ to be acoustically more similar to /ε/.</abstract>
      <bibkey>gu-chen-2019-acoustic</bibkey>
    </paper>
    <paper id="28">
      <title>Rethinking Phonotactic Complexity</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>88–90</pages>
      <abstract>In this work, we propose the use of phone-level language models to estimate phonotactic complexity—measured in bits per phoneme—which makes cross-linguistic comparison straightforward. We compare the entropy across languages using this simple measure, gaining insight on how complex different language’s phonotactics are. Finally, we show a very strong negative correlation between phonotactic complexity and the average length of words—Spearman rho=-0.744—when analysing a collection of 106 languages with 1016 basic concepts each.</abstract>
      <bibkey>pimentel-etal-2019-rethinking</bibkey>
    </paper>
    <paper id="29">
      <title>Implementing a Multi-lingual Chatbot for Positive Reinforcement in Young Learners</title>
      <author><first>Francisca</first><last>Oladipo</last></author>
      <author><first>Abdulmalik</first><last>Rufai</last></author>
      <pages>91</pages>
      <abstract>This is a humanitarian work –a counter-terrorism effort. The presentation describes the experiences of developing a multi-lingua, interactive chatbot trained on the corpus of two Nigerian Languages (Hausa and Fulfude), with simultaneous translation to a third (Kanuri), to stimulate conversations, deliver tailored contents to the users thereby aiding in the detection of the probability and degree of radicalization in young learners through data analysis of the games moves and vocabularies. As chatbots have the ability to simulate a human conversation based on rhetorical behavior, the system is able to learn the need of individual user through constant interaction and deliver tailored contents that promote good behavior in Hausa, Fulfulde and Kanuri languages.</abstract>
      <bibkey>oladipo-rufai-2019-implementing</bibkey>
    </paper>
    <paper id="30">
      <title>A Deep Learning Approach to Language-independent Gender Prediction on <fixed-case>T</fixed-case>witter</title>
      <author><first>Reyhaneh</first><last>Hashempour</last></author>
      <pages>92–94</pages>
      <abstract>This work presents a set of experiments conducted to predict the gender of Twitter users based on language-independent features extracted from the text of the users’ tweets. The experiments were performed on a version of TwiSty dataset including tweets written by the users of six different languages: Portuguese, French, Dutch, English, German, and Italian. Logistic regression (LR), and feed-forward neural networks (FFNN) with back-propagation were used to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual (CL). In the IL setting, the training and testing were performed on the same language whereas in the CL, Italian and German datasets were set aside and only used as test sets and the rest were combined to compose training and development sets. In the IL, the highest accuracy score belongs to LR whereas, in the CL, FFNN with three hidden layers yields the highest score. The results show that neural network based models underperform traditional models when the size of the training set is small; however, they beat traditional models by a non-trivial margin, when they are fed with large enough data. Finally, the feature analysis confirms that men and women have different writing styles independent of their language.</abstract>
      <bibkey>hashempour-2019-deep</bibkey>
    </paper>
    <paper id="31">
      <title>Isolating the Effects of Modeling Recursive Structures: A Case Study in Pronunciation Prediction of <fixed-case>C</fixed-case>hinese Characters</title>
      <author><first>Minh</first><last>Nguyen</last></author>
      <author><first>Gia H</first><last>Ngo</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>95–97</pages>
      <abstract>Finding that explicitly modeling structures leads to better generalization, we consider the task of predicting Cantonese pronunciations of logographs (Chinese characters) using logographs’ recursive structures. This task is a suitable case study for two reasons. First, logographs’ pronunciations depend on structures (i.e. the hierarchies of sub-units in logographs) Second, the quality of logographic structures is consistent since the structures are constructed automatically using a set of rules. Thus, this task is less affected by confounds such as varying quality between annotators. Empirical results show that modeling structures explicitly using treeLSTM outperforms LSTM baseline, reducing prediction error by 6.0% relative.</abstract>
      <bibkey>nguyen-etal-2019-isolating</bibkey>
    </paper>
    <paper id="32">
      <title>Benchmarking Neural Machine Translation for <fixed-case>S</fixed-case>outhern <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Jade</first><last>Abbott</last></author>
      <author><first>Laura</first><last>Martinus</last></author>
      <pages>98–101</pages>
      <abstract>Unlike major Western languages, most African languages are very low-resourced. Furthermore, the resources that do exist are often scattered and difficult to obtain and discover. As a result, the data and code for existing research has rarely been shared, meaning researchers struggle to reproduce reported results, and almost no publicly available benchmarks or leaderboards for African machine translation models exist. To start to address these problems, we trained neural machine translation models for a subset of Southern African languages on publicly-available datasets. We provide the code for training the models and evaluate the models on a newly released evaluation set, with the aim of starting a leaderboard for Southern African languages and spur future research in the field.</abstract>
      <bibkey>abbott-martinus-2019-benchmarking</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>OCR</fixed-case> Quality and <fixed-case>NLP</fixed-case> Preprocessing</title>
      <author><first>Margot</first><last>Mieskes</last></author>
      <author><first>Stefan</first><last>Schmunk</last></author>
      <pages>102–105</pages>
      <abstract>We present initial experiments to evaluate the performance of tasks such as Part of Speech Tagging on data corrupted by Optical Character Recognition (OCR). Our results, based on English and German data, using artificial experiments as well as initial real OCRed data indicate that already a small drop in OCR quality considerably increases the error rates, which would have a significant impact on subsequent processing steps.</abstract>
      <bibkey>mieskes-schmunk-2019-ocr</bibkey>
    </paper>
    <paper id="34">
      <title>Developing a Fine-grained Corpus for a Less-resourced Language: the case of <fixed-case>K</fixed-case>urdish</title>
      <author><first>Roshna</first><last>Abdulrahman</last></author>
      <author><first>Hossein</first><last>Hassani</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <pages>106–109</pages>
      <abstract>Kurdish is a less-resourced language consisting of different dialects written in various scripts. Approximately 30 million people in different countries speak the language. The lack of corpora is one of the main obstacles in Kurdish language processing. In this paper, we present KTC-the Kurdish Textbooks Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is normalized and categorized into 12 educational subjects containing 693,800 tokens (110,297 types). Our resource is publicly available for non-commercial use under the CC BY-NC-SA 4.0 license.</abstract>
      <bibkey>abdulrahman-etal-2019-developing</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>A</fixed-case>mharic Question Answering for Biography, Definition, and Description Questions</title>
      <author><first>Tilahun Abedissa</first><last>Taffa</last></author>
      <author><first>Mulugeta</first><last>Libsie</last></author>
      <pages>110–113</pages>
      <abstract>A broad range of information needs can often be stated as a question. Question Answering (QA) systems attempt to provide users concise answer(s) to natural language questions. The existing Amharic QA systems handle fact-based questions that usually take named entities as an answer. To deal with more complex information needs we developed an Amharic non-factoid QA for biography, definition, and description questions. A hybrid approach has been used for the question classification. For document filtering and answer extraction we have used lexical patterns. On the other hand to answer biography questions we have used a summarizer and the generated summary is validated using a text classifier. Our QA system is evaluated and has shown a promising result.</abstract>
      <bibkey>taffa-libsie-2019-amharic</bibkey>
    </paper>
    <paper id="36">
      <title>Polysemous Language in Child Directed Speech</title>
      <author><first>Sammy</first><last>Floyd</last></author>
      <author><first>Libby</first><last>Barak</last></author>
      <author><first>Adele</first><last>Goldberg</last></author>
      <author><first>Casey</first><last>Lew-Williams</last></author>
      <pages>114–117</pages>
      <abstract>Polysemous Language in Child Directed Speech Learning the meaning of words is one of the fundamental building blocks of verbal communication. Models of child language acquisition have generally made the simplifying assumption that each word appears in child-directed speech with a single meaning. To understand naturalistic word learning during childhood, it is essential to know whether children hear input that is in fact constrained to single meaning per word, or whether the environment naturally contains multiple senses.In this study, we use a topic modeling approach to automatically induce word senses from child-directed speech. Our results confirm the plausibility of our automated analysis approach and reveal an increasing rate of using multiple senses in child-directed speech, starting with corpora from children as early as the first year of life.</abstract>
      <bibkey>floyd-etal-2019-polysemous</bibkey>
    </paper>
    <paper id="37">
      <title>Principled Frameworks for Evaluating Ethics in <fixed-case>NLP</fixed-case> Systems</title>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Elijah</first><last>Mayfield</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>118–121</pages>
      <abstract>We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.</abstract>
      <bibkey>prabhumoye-etal-2019-principled</bibkey>
    </paper>
    <paper id="38">
      <title>Understanding the Shades of Sexism in Popular <fixed-case>TV</fixed-case> Series</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>122–125</pages>
      <abstract>[Multiple-submission] In the midst of a generation widely exposed to and influenced by media entertainment, the NLP research community has shown relatively little attention on the sexist comments in popular TV series. To understand sexism in TV series, we propose a way of collecting distant supervision dataset using Character Persona information with the psychological theories on sexism. We assume that sexist characters from TV shows are more prone to making sexist comments when talking about women, and show that this hypothesis is valid through experiment. Finally, we conduct an interesting analysis on popular TV show characters and successfully identify different shades of sexism that is often overlooked.</abstract>
      <bibkey>lee-etal-2019-understanding</bibkey>
    </paper>
    <paper id="39">
      <title>Evaluating Ways of Adapting Word Similarity</title>
      <author><first>Libby</first><last>Barak</last></author>
      <author><first>Adele</first><last>Goldberg</last></author>
      <pages>126–128</pages>
      <abstract>People judge pairwise similarity by deciding which aspects of the words’ meanings are relevant for the comparison of the given pair. However, computational representations of meaning rely on dimensions of the vector representation for similarity comparisons, without considering the specific pairing at hand. Prior work has adapted computational similarity judgments by using the softmax function in order to address this limitation by capturing asymmetry in human judgments. We extend this analysis by showing that a simple modification of cosine similarity offers a better correlation with human judgments over a comprehensive dataset. The modification performs best when the similarity between two words is calculated with reference to other words that are most similar and dissimilar to the pair.</abstract>
      <bibkey>barak-goldberg-2019-evaluating</bibkey>
    </paper>
    <paper id="40">
      <title>Exploring the Use of Lexicons to aid Deep Learning towards the Detection of Abusive Language</title>
      <author><first>Anna</first><last>Koufakou</last></author>
      <author><first>Jason</first><last>Scott</last></author>
      <pages>129–131</pages>
      <abstract>Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focused on detecting personal attacks in online conversations. State-of-the-art research on this task has largely used deep learning with word embeddings. We explored the use of sentiment lexicons as well as semantic lexicons towards improving the accuracy of the baseline Convolutional Neural Network (CNN) using regular word embeddings. This is a work in progress, limited by time constraints and appropriate infrastructure. Our preliminary results showed promise for utilizing lexicons, especially semantic lexicons, for the task of detecting abusive language.</abstract>
      <bibkey>koufakou-scott-2019-exploring</bibkey>
    </paper>
    <paper id="41">
      <title>Entity-level Classification of Adverse Drug Reactions: a Comparison of Neural Network Models</title>
      <author><first>Ilseyar</first><last>Alimova</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <pages>132–134</pages>
      <abstract>This paper presents our experimental work on exploring the potential of neural network models developed for aspect-based sentiment analysis for entity-level adverse drug reaction (ADR) classification. Our goal is to explore how to represent local context around ADR mentions and learn an entity representation, interacting with its context. We conducted extensive experiments on various sources of text-based information, including social media, electronic health records, and abstracts of scientific articles from PubMed. The results show that Interactive Attention Neural Network (IAN) outperformed other models on four corpora in terms of macro F-measure. This work is an abridged version of our recent paper accepted to Programming and Computer Software journal in 2019.</abstract>
      <bibkey>alimova-tutubalina-2019-entity</bibkey>
    </paper>
    <paper id="42">
      <title>Context Effects on Human Judgments of Similarity</title>
      <author><first>Libby</first><last>Barak</last></author>
      <author><first>Noe</first><last>Kong-Johnson</last></author>
      <author><first>Adele</first><last>Goldberg</last></author>
      <pages>135–137</pages>
      <abstract>The semantic similarity of words forms the basis of many natural language processing methods. These computational similarity measures are often based on a mathematical comparison of vector representations of word meanings, while human judgments of similarity differ in lacking geometrical properties, e.g., symmetric similarity and triangular similarity. In this study, we propose a novel task design to further explore human behavior by asking whether a pair of words is deemed more similar depending on an immediately preceding judgment. Results from a crowdsourcing experiment show that people consistently judge words as more similar when primed by a judgment that evokes a relevant relationship. Our analysis further shows that word2vec similarity correlated significantly better with the out-of-context judgments, thus confirming the methodological differences in human-computer judgments, and offering a new testbed for probing the differences.</abstract>
      <bibkey>barak-etal-2019-context</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>NLP</fixed-case> Automation to Read Radiological Reports to Detect the Stage of Cancer Among Lung Cancer Patients</title>
      <author><first>Khushbu</first><last>Gupta</last></author>
      <author><first>Ratchainant</first><last>Thammasudjarit</last></author>
      <author><first>Ammarin</first><last>Thakkinstian</last></author>
      <pages>138–141</pages>
      <abstract>A common challenge in the healthcare industry today is physicians have access to massive amounts of healthcare data but have little time and no appropriate tools. For instance, the risk prediction model generated by logistic regression could predict the probability of diseases occurrence and thus prioritizing patients’ waiting list for further investigations. However, many medical reports available in current clinical practice system are not yet ready for analysis using either statistics or machine learning as they are in unstructured text format. The complexity of medical information makes the annotation or validation of data very challenging and thus acts as a bottleneck to apply machine learning techniques in medical data. This study is therefore conducted to create such annotations automatically where the computer can read radiological reports for oncologists and mark the staging of lung cancer. This staging information is obtained using the rule-based method implemented using the standards of Tumor Node Metastasis (TNM) staging along with deep learning technology called Long Short Term Memory (LSTM) to extract clinical information from the Computed Tomography (CT) text report. The empirical experiment shows promising results being the accuracy of up to 85%.</abstract>
      <bibkey>gupta-etal-2019-nlp</bibkey>
    </paper>
    <paper id="44">
      <title>Augmenting Named Entity Recognition with Commonsense Knowledge</title>
      <author><first>Gaith</first><last>Dekhili</last></author>
      <author><first>Tan Ngoc</first><last>Le</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>142</pages>
      <abstract>Commonsense can be vital in some applications like Natural Language Understanding (NLU), where it is often required to resolve ambiguity arising from implicit knowledge and underspecification. In spite of the remarkable success of neural network approaches on a variety of Natural Language Processing tasks, many of them struggle to react effectively in cases that require commonsense knowledge. In the present research, we take advantage of the availability of the open multilingual knowledge graph ConceptNet, by using it as an additional external resource in Named Entity Recognition (NER). Our proposed architecture involves BiLSTM layers combined with a CRF layer that was augmented with some features such as pre-trained word embedding layers and dropout layers. Moreover, apart from using word representations, we used also character-based representation to capture the morphological and the orthographic information. Our experiments and evaluations showed an improvement in the overall performance with +2.86 in the F1-measure. Commonsense reasonnig has been employed in other studies and NLP tasks but to the best of our knowledge, there is no study relating the integration of a commonsense knowledge base in NER.</abstract>
      <bibkey>dekhili-etal-2019-augmenting</bibkey>
    </paper>
    <paper id="45">
      <title>Pardon the Interruption: Automatic Analysis of Gender and Competitive Turn-Taking in <fixed-case>U</fixed-case>nited <fixed-case>S</fixed-case>tates <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Hearings</title>
      <author><first>Haley</first><last>Lepp</last></author>
      <pages>143–145</pages>
      <abstract>The United States Supreme Court plays a key role in defining the legal basis for gender discrimination throughout the country, yet there are few checks on gender bias within the court itself. In conversational turn-taking, interruptions have been documented as a marker of bias between speakers of different genders. The goal of this study is to automatically differentiate between respectful and disrespectful conversational turns taken during official hearings, which could help in detecting bias and finding remediation techniques for discourse in the courtroom. In this paper, I present a corpus of turns annotated by legal professionals, and describe the design of a semi-supervised classifier that will use acoustic and lexical features to analyze turn-taking at scale. On completion of annotations, this classifier will be trained to extract the likelihood that turns are respectful or disrespectful for use in studies of speech trends.</abstract>
      <bibkey>lepp-2019-pardon</bibkey>
    </paper>
    <paper id="46">
      <title>Evaluating Coherence in Dialogue Systems using Entailment</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Ehsan</first><last>Kamalloo</last></author>
      <author><first>Kory</first><last>Mathewson</last></author>
      <author><first>Osmar</first><last>Zaiane</last></author>
      <pages>146–148</pages>
      <abstract>Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses. This paper has been accepted in NAACL 2019.</abstract>
      <bibkey>dziri-etal-2019-evaluating-coherence</bibkey>
    </paper>
    <paper id="47">
      <title>Exploiting machine algorithms in vocalic quantification of <fixed-case>A</fixed-case>frican <fixed-case>E</fixed-case>nglish corpora</title>
      <author><first>Lasisi Adeiza</first><last>Isiaka</last></author>
      <pages>149–151</pages>
      <abstract>Towards procedural fidelity in the processing of African English speech corpora, this work demonstrates how the adaptation of machine-assisted segmentation of phonemes and automatic extraction of acoustic values can significantly speed up the processing of naturalistic data and make the vocalic analysis of the varieties less impressionistic. Research in African English phonology has, till date, been least data-driven – much less the use of comparative corpora for cross-varietal assessments. Using over 30 hours of naturalistic data (from 28 speakers in 5 Nigerian cities), the procedures for segmenting audio files into phonemic units via the Munich Automatic Segmentation System (MAUS), and the extraction of their spectral values in Praat are explained. Evidence from the speech corpora supports a more complex vocalic inventory than attested in previous auditory/manual-based accounts – thus reinforcing the resourcefulness of the algorithms for the current data and cognate varieties. Keywords: machine algorithms; naturalistic data; African English phonology; vowel segmentation</abstract>
      <bibkey>isiaka-2019-exploiting</bibkey>
    </paper>
    <paper id="48">
      <title>Assessing the Ability of Neural Machine Translation Models to Perform Syntactic Rewriting</title>
      <author><first>Jahkel</first><last>Robin</last></author>
      <author><first>Alvin</first><last>Grissom II</last></author>
      <author><first>Matthew</first><last>Roselli</last></author>
      <pages>152</pages>
      <abstract>We describe work in progress for evaluating performance of sequence-to-sequence neural networks on the task of syntax-based reordering for rules applicable to simultaneous machine translation. We train models that attempt to rewrite English sentences using rules that are commonly used by human interpreters. We examine the performance of these models to determine which forms of rewriting are more difficult for them to learn and which architectures are the best at learning them.</abstract>
      <bibkey>robin-etal-2019-assessing</bibkey>
    </paper>
    <paper id="49">
      <title>Authorship Recognition with Short-Text using Graph-based Techniques</title>
      <author><first>Laura</first><last>Cruz</last></author>
      <pages>153–156</pages>
      <abstract>In recent years, studies of authorship recognition has aroused great interest in graph-based analysis. Modeling the writing style of each author using a network of co-occurrence words. However, short texts can generate some changes in the topology of network that cause impact on techniques of feature extraction based on graph topology. In this work, we evaluate the robustness of global-strategy and local-strategy based on complex network measurements comparing with graph2vec a graph embedding technique based on skip-gram model. The experiment consists of evaluating how each modification in the length of text affects the accuracy of authorship recognition on both techniques using cross-validation and machine learning techniques.</abstract>
      <bibkey>cruz-2019-authorship</bibkey>
    </paper>
    <paper id="50">
      <title>A Parallel Corpus <fixed-case>M</fixed-case>ixtec-<fixed-case>S</fixed-case>panish</title>
      <author><first>Cynthia</first><last>Montaño</last></author>
      <author><first>Gerardo</first><last>Sierra Martínez</last></author>
      <author><first>Gemma</first><last>Bel-Enguix</last></author>
      <author><first>Helena</first><last>Gomez</last></author>
      <pages>157–159</pages>
      <abstract>This work is about the compilation process of parallel documents Spanish-Mixtec. There are not many Spanish-Mixec parallel texts and most of the sources are non-digital books. Due to this, we need to face the errors when digitizing the sources and difficulties in sentence alignment, as well as the fact that does not exist a standard orthography. Our parallel corpus consists of sixty texts coming from books and digital repositories. These documents belong to different domains: history, traditional stories, didactic material, recipes, ethnographical descriptions of each town and instruction manuals for disease prevention. We have classified this material in five major categories: didactic (6 texts), educative (6 texts), interpretative (7 texts), narrative (39 texts), and poetic (2 texts). The final total of tokens is 49,814 Spanish words and 47,774 Mixtec words. The texts belong to the states of Oaxaca (48 texts), Guerrero (9 texts) and Puebla (3 texts). According to this data, we see that the corpus is unbalanced in what refers to the representation of the different territories. While 55% of speakers are in Oaxaca, 80% of texts come from this region. Guerrero has the 30% of speakers and the 15% of texts and Puebla, with the 15% of the speakers has a representation of the 5% in the corpus.</abstract>
      <bibkey>montano-etal-2019-parallel</bibkey>
    </paper>
    <paper id="51">
      <title>Emoji Usage Across Platforms: A Case Study for the Charlottesville Event</title>
      <author><first>Khyati</first><last>Mahajan</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>160–162</pages>
      <abstract>We study emoji usage patterns across two social media platforms, one of them considered a fringe community called Gab, and the other Twitter. We find that Gab tends to comparatively use more emotionally charged emoji, but also seems more apathetic towards the violence during the event, while Twitter takes a more empathetic approach to the event.</abstract>
      <bibkey>mahajan-shaikh-2019-emoji</bibkey>
    </paper>
    <paper id="52">
      <title>Reading <fixed-case>KITTY</fixed-case>: Pitch Range as an Indicator of Reading Skill</title>
      <author><first>Alfredo</first><last>Gomez</last></author>
      <author><first>Alicia</first><last>Ngo</last></author>
      <author><first>Alessandra</first><last>Otondo</last></author>
      <author><first>Julie</first><last>Medero</last></author>
      <pages>163–165</pages>
      <abstract>While affective outcomes are generally positive for the use of eBooks and computer-based reading tutors in teaching children to read, learning outcomes are often poorer (Korat and Shamir, 2004). We describe the first iteration of Reading Kitty, an iOS application that uses NLP and speech processing to focus children’s time on close reading and prosody in oral reading, while maintaining an emphasis on creativity and artifact creation. We also share preliminary results demonstrating that pitch range can be used to automatically predict readers’ skill level.</abstract>
      <bibkey>gomez-etal-2019-reading</bibkey>
    </paper>
    <paper id="53">
      <title>Adversarial Attack on Sentiment Classification</title>
      <author><first>Yi-Ting (Alicia)</first><last>Tsai</last></author>
      <author><first>Min-Chu</first><last>Yang</last></author>
      <author><first>Han-Yu</first><last>Chen</last></author>
      <pages>166–173</pages>
      <abstract>In this paper, we propose a white-box attack algorithm called “Global Search” method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called “Greedy Search”. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed “Global Search” method generates more powerful adversarial examples with less distortion or less modification to the source text.</abstract>
      <bibkey>tsai-etal-2019-adversarial</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>CSI</fixed-case> <fixed-case>P</fixed-case>eru News: finding the culprit, victim and location in news articles</title>
      <author><first>Gina</first><last>Bustamante</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <pages>174–176</pages>
      <abstract>We introduce a shift on the DS method over the domain of crime-related news from Peru, attempting to find the culprit, victim and location of a crime description from a RE perspective. Obtained results are highly promising and show that proposed modifications are effective in non-traditional domains.</abstract>
      <bibkey>bustamante-oncevay-2019-csi</bibkey>
    </paper>
    <paper id="55">
      <title>Exploring Social Bias in Chatbots using Stereotype Knowledge</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>177–180</pages>
      <abstract>Exploring social bias in chatbot is an important, yet relatively unexplored problem. In this paper, we propose an approach to understand social bias in chatbots by leveraging stereotype knowledge. It allows interesting comparison of bias between chatbots and humans, and provides intuitive analysis of existing chatbots by borrowing the finer-grain concepts of sexism and racism.</abstract>
      <bibkey>lee-etal-2019-exploring</bibkey>
    </paper>
    <paper id="56">
      <title>Cross-Sentence Transformations in Text Simplification</title>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>181–184</pages>
      <abstract>Current approaches to Text Simplification focus on simplifying sentences individually. However, certain simplification transformations span beyond single sentences (e.g. joining and re-ordering sentences). In this paper, we motivate the need for modelling the simplification task at the document level, and assess the performance of sequence-to-sequence neural models in this setup. We analyse parallel original-simplified documents created by professional editors and show that there are frequent rewriting transformations that are not restricted to sentence boundaries. We also propose strategies to automatically evaluate the performance of a simplification model on these cross-sentence transformations. Our experiments show the inability of standard sequence-to-sequence neural models to learn these transformations, and suggest directions towards document-level simplification.</abstract>
      <bibkey>alva-manchego-etal-2019-cross</bibkey>
    </paper>
  </volume>
  <volume id="37">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing</booktitle>
      <url hash="53ea2def">W19-37</url>
      <editor><first>Tomaž</first><last>Erjavec</last></editor>
      <editor><first>Michał</first><last>Marcińczuk</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Jakub</first><last>Piskorski</last></editor>
      <editor><first>Lidia</first><last>Pivovarova</last></editor>
      <editor><first>Jan</first><last>Šnajder</last></editor>
      <editor><first>Josef</first><last>Steinberger</last></editor>
      <editor><first>Roman</first><last>Yangarber</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>bsnlp</venue>
    </meta>
    <frontmatter>
      <url hash="e2c00cd5">W19-3700</url>
      <bibkey>ws-2019-balto</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Unsupervised Induction of <fixed-case>U</fixed-case>krainian Morphological Paradigms for the New Lexicon: Extending Coverage for Named Entities and Neologisms using Inflection Tables and Unannotated Corpora</title>
      <author><first>Bogdan</first><last>Babych</last></author>
      <pages>1–11</pages>
      <abstract>The paper presents an unsupervised method for quickly extending a Ukrainian lexicon by generating paradigms and morphological feature structures for new Named Entities and neologisms, which are not covered by existing static morphological resources. This approach addresses a practical problem of modelling paradigms for entities created by the dynamic processes in the lexicon: this problem is especially serious for highly-inflected languages in domains with specialised or quickly changing lexicon. The method uses an unannotated Ukrainian corpus and a small fixed set of inflection tables, which can be found in traditional grammar textbooks. The advantage of the proposed approach is that updating the morphological lexicon does not require training or linguistic annotation, allowing fast knowledge-light extension of an existing static lexicon to improve morphological coverage on a specific corpus. The method is implemented in an open-source package on a GitHub repository. It can be applied to other low-resourced inflectional languages which have internet corpora and linguistic descriptions of their inflection system, following the example of inflection tables for Ukrainian. Evaluation results shows consistent improvements in coverage for Ukrainian corpora of different corpus types.</abstract>
      <url hash="6629085b">W19-3701</url>
      <doi>10.18653/v1/W19-3701</doi>
      <bibkey>babych-2019-unsupervised</bibkey>
    </paper>
    <paper id="2">
      <title>Multiple Admissibility: Judging Grammaticality using Unlabeled Data in Language Learning</title>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Sardana</first><last>Ivanova</last></author>
      <pages>12–22</pages>
      <abstract>We present our work on the problem of Multiple Admissibility (MA) in language learning. Multiple Admissibility occurs in many languages when more than one grammatical form of a word fits syntactically and semantically in a given context. In second language (L2) education - in particular, in intelligent tutoring systems/computer-aided language learning (ITS/CALL) systems, which generate exercises automatically - this implies that multiple alternative answers are possible. We treat the problem as a grammaticality judgement task. We train a neural network with an objective to label sentences as grammatical or ungrammatical, using a “simulated learner corpus”: a dataset with correct text, and with artificial errors generated automatically. While MA occurs commonly in many languages, this paper focuses on learning Russian. We present a detailed classification of the types of constructions in Russian, in which MA is possible, and evaluate the model using a test set built from answers provided by the users of a running language learning system.</abstract>
      <url hash="bc6b1bbd">W19-3702</url>
      <doi>10.18653/v1/W19-3702</doi>
      <bibkey>katinskaia-ivanova-2019-multiple</bibkey>
    </paper>
    <paper id="3">
      <title>Numbers Normalisation in the Inflected Languages: a Case Study of <fixed-case>P</fixed-case>olish</title>
      <author><first>Rafał</first><last>Poświata</last></author>
      <author><first>Michał</first><last>Perełkiewicz</last></author>
      <pages>23–28</pages>
      <abstract>Text normalisation in Text-to-Speech systems is a process of converting written expressions to their spoken forms. This task is complicated because in many cases the normalised form depends on the context. Furthermore, when we analysed languages like Croatian, Lithuanian, Polish, Russian or Slovak there is additional difficulty related to their inflected nature. In this paper we want to show how to deal with this problem for one of these languages: Polish, without having a large dedicated data set and using solutions prepared for other NLP tasks. We limited our study to only numbers expressions, which are the most common non-standard words to normalise. The proposed solution is a combination of morphological tagger and transducer supported by a dictionary of numbers in their spoken forms. The data set used for evaluation is based on the part of 1-million word subset of the National Corpus of Polish. The accuracy of the described approach is presented with a comparison to a simple baseline and two commercial systems: Google Cloud Text-to-Speech and Amazon Polly.</abstract>
      <url hash="febb43b0">W19-3703</url>
      <doi>10.18653/v1/W19-3703</doi>
      <bibkey>poswiata-perelkiewicz-2019-numbers</bibkey>
      <pwccode url="https://github.com/rafalposwiata/text-normalization" additional="false">rafalposwiata/text-normalization</pwccode>
    </paper>
    <paper id="4">
      <title>What does Neural Bring? Analysing Improvements in Morphosyntactic Annotation and Lemmatisation of <fixed-case>S</fixed-case>lovenian, <fixed-case>C</fixed-case>roatian and <fixed-case>S</fixed-case>erbian</title>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <pages>29–34</pages>
      <abstract>We present experiments on Slovenian, Croatian and Serbian morphosyntactic annotation and lemmatisation between the former state-of-the-art for these three languages and one of the best performing systems at the CoNLL 2018 shared task, the Stanford NLP neural pipeline. Our experiments show significant improvements in morphosyntactic annotation, especially on categories where either semantic knowledge is needed, available through word embeddings, or where long-range dependencies have to be modelled. On the other hand, on the task of lemmatisation no improvements are obtained with the neural solution, mostly due to the heavy dependence of the task on the lookup in an external lexicon, but also due to obvious room for improvements in the Stanford NLP pipeline’s lemmatisation.</abstract>
      <url hash="15d238bf">W19-3704</url>
      <doi>10.18653/v1/W19-3704</doi>
      <bibkey>ljubesic-dobrovoljc-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>AGRR</fixed-case> 2019: Corpus for Gapping Resolution in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Maria</first><last>Ponomareva</last></author>
      <author><first>Kira</first><last>Droganova</last></author>
      <author><first>Ivan</first><last>Smurov</last></author>
      <author><first>Tatiana</first><last>Shavrina</last></author>
      <pages>35–43</pages>
      <abstract>This paper provides a comprehensive overview of the gapping dataset for Russian that consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. In this paper, we pay special attention to the gapping resolution methods that were introduced within the shared task as well as an alternative test set that illustrates that our corpus is a diverse and representative subset of Russian language gapping sufficient for effective utilization of machine learning techniques.</abstract>
      <url hash="c1537f5d">W19-3705</url>
      <doi>10.18653/v1/W19-3705</doi>
      <bibkey>ponomareva-etal-2019-agrr</bibkey>
    </paper>
    <paper id="6">
      <title>Creating a Corpus for <fixed-case>R</fixed-case>ussian Data-to-Text Generation Using Neural Machine Translation and Post-Editing</title>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Elena</first><last>Khasanova</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>44–49</pages>
      <abstract>In this paper, we propose an approach for semi-automatically creating a data-to-text (D2T) corpus for Russian that can be used to learn a D2T natural language generation model. An error analysis of the output of an English-to-Russian neural machine translation system shows that 80% of the automatically translated sentences contain an error and that 53% of all translation errors bear on named entities (NE). We therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated NEs.</abstract>
      <url hash="1a188943">W19-3706</url>
      <doi>10.18653/v1/W19-3706</doi>
      <bibkey>shimorina-etal-2019-creating</bibkey>
      <pwccode url="https://gitlab.com/shimorina/bsnlp-2019" additional="false">shimorina/bsnlp-2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="7">
      <title>Data Set for Stance and Sentiment Analysis from User Comments on <fixed-case>C</fixed-case>roatian News</title>
      <author><first>Mihaela</first><last>Bošnjak</last></author>
      <author><first>Mladen</first><last>Karan</last></author>
      <pages>50–55</pages>
      <abstract>Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online. In this paper, we describe the creation of a data set that contains news articles and corresponding comments from Croatian news outlet 24 sata. Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable. Through this data, we hope to get a better understanding of the publics viewpoint on various events. In addition, we also explore the potential of applying supervised machine learning models toautomate annotation of more data.</abstract>
      <url hash="2a36865f">W19-3707</url>
      <doi>10.18653/v1/W19-3707</doi>
      <bibkey>bosnjak-karan-2019-data</bibkey>
    </paper>
    <paper id="8">
      <title>A Dataset for Noun Compositionality Detection for a <fixed-case>S</fixed-case>lavic Language</title>
      <author><first>Dmitry</first><last>Puzyrev</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>56–62</pages>
      <abstract>This paper presents the first gold-standard resource for Russian annotated with compositionality information of noun compounds. The compound phrases are collected from the Universal Dependency treebanks according to part of speech patterns, such as ADJ+NOUN or NOUN+NOUN, using the gold-standard annotations. Each compound phrase is annotated by two experts and a moderator according to the following schema: the phrase can be either compositional, non-compositional, or ambiguous (i.e., depending on the context it can be interpreted both as compositional or non-compositional). We conduct an experimental evaluation of models and methods for predicting compositionality of noun compounds in unsupervised and supervised setups. We show that methods from previous work evaluated on the proposed Russian-language resource achieve the performance comparable with results on English corpora.</abstract>
      <url hash="4b347e85">W19-3708</url>
      <doi>10.18653/v1/W19-3708</doi>
      <bibkey>puzyrev-etal-2019-dataset</bibkey>
      <pwccode url="https://github.com/slangtech/ru-comps" additional="false">slangtech/ru-comps</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="9">
      <title>The Second Cross-Lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Laska</first><last>Laskova</last></author>
      <author><first>Michał</first><last>Marcińczuk</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Josef</first><last>Steinberger</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>63–74</pages>
      <abstract>We describe the Second Multilingual Named Entity Challenge in Slavic languages. The task is recognizing mentions of named entities in Web documents, their normalization, and cross-lingual linking. The Challenge was organized as part of the 7th Balto-Slavic Natural Language Processing Workshop, co-located with the ACL-2019 conference. Eight teams participated in the competition, which covered four languages and five entity types. Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge. Seven teams covered all four languages, and five teams participated in the cross-lingual entity linking task. Detailed evaluation information is available on the shared task web page.</abstract>
      <url hash="f5dc4a7d">W19-3709</url>
      <doi>10.18653/v1/W19-3709</doi>
      <bibkey>piskorski-etal-2019-second</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>BSNLP</fixed-case>2019 Shared Task Submission: Multisource Neural <fixed-case>NER</fixed-case> Transfer</title>
      <author><first>Tatiana</first><last>Tsygankova</last></author>
      <author><first>Stephen</first><last>Mayhew</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>75–82</pages>
      <abstract>This paper describes the Cognitive Computation (CogComp) Group’s submissions to the multilingual named entity recognition shared task at the Balto-Slavic Natural Language Processing (BSNLP) Workshop. The final model submitted is a multi-source neural NER system with multilingual BERT embeddings, trained on the concatenation of training data in various Slavic languages (as well as English). The performance of our system on the official testing data suggests that multi-source approaches consistently outperform single-source approaches for this task, even with the noise of mismatching tagsets.</abstract>
      <url hash="26b8deb0">W19-3710</url>
      <doi>10.18653/v1/W19-3710</doi>
      <bibkey>tsygankova-etal-2019-bsnlp2019</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>TLR</fixed-case> at <fixed-case>BSNLP</fixed-case>2019: A Multilingual Named Entity Recognition System</title>
      <author><first>Jose G.</first><last>Moreno</last></author>
      <author><first>Elvys</first><last>Linhares Pontes</last></author>
      <author><first>Mickael</first><last>Coustaty</last></author>
      <author><first>Antoine</first><last>Doucet</last></author>
      <pages>83–88</pages>
      <abstract>This paper presents our participation at the shared task on multilingual named entity recognition at BSNLP2019. Our strategy is based on a standard neural architecture for sequence labeling. In particular, we use a mixed model which combines multilingualcontextual and language-specific embeddings. Our only submitted run is based on a voting schema using multiple models, one for each of the four languages of the task (Bulgarian, Czech, Polish, and Russian) and another for English. Results for named entity recognition are encouraging for all languages, varying from 60% to 83% in terms of Strict and Relaxed metrics, respectively.</abstract>
      <url hash="d514907d">W19-3711</url>
      <doi>10.18653/v1/W19-3711</doi>
      <bibkey>moreno-etal-2019-tlr</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="12">
      <title>Tuning Multilingual Transformers for Language-Specific Named Entity Recognition</title>
      <author><first>Mikhail</first><last>Arkhipov</last></author>
      <author><first>Maria</first><last>Trofimova</last></author>
      <author><first>Yuri</first><last>Kuratov</last></author>
      <author><first>Alexey</first><last>Sorokin</last></author>
      <pages>89–93</pages>
      <abstract>Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and demonstrated top performance in multilingual setting for two competition metrics. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.</abstract>
      <url hash="36e49551">W19-3712</url>
      <doi>10.18653/v1/W19-3712</doi>
      <bibkey>arkhipov-etal-2019-tuning</bibkey>
      <pwccode url="https://github.com/deepmipt/Slavic-BERT-NER" additional="false">deepmipt/Slavic-BERT-NER</pwccode>
    </paper>
    <paper id="13">
      <title>Multilingual Named Entity Recognition Using Pretrained Embeddings, Attention Mechanism and <fixed-case>NCRF</fixed-case></title>
      <author><first>Anton</first><last>Emelyanov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>94–99</pages>
      <abstract>In this paper we tackle multilingual named entity recognition task. We use the BERT Language Model as embeddings with bidirectional recurrent network, attention, and NCRF on the top. We apply multilingual BERT only as embedder without any fine-tuning. We test out model on the dataset of the BSNLP shared task, which consists of texts in Bulgarian, Czech, Polish and Russian languages.</abstract>
      <url hash="94278a5b">W19-3713</url>
      <doi>10.18653/v1/W19-3713</doi>
      <bibkey>emelyanov-artemova-2019-multilingual</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="14">
      <title><fixed-case>JRC</fixed-case> <fixed-case>TMA</fixed-case>-<fixed-case>CC</fixed-case>: <fixed-case>S</fixed-case>lavic Named Entity Recognition and Linking. Participation in the <fixed-case>BSNLP</fixed-case>-2019 shared task</title>
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Hristo</first><last>Tanev</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <pages>100–104</pages>
      <abstract>We report on the participation of the JRC Text Mining and Analysis Competence Centre (TMA-CC) in the BSNLP-2019 Shared Task, which focuses on named-entity recognition, lemmatisation and cross-lingual linking. We propose a hybrid system combining a rule-based approach and light ML techniques. We use multilingual lexical resources such as JRC-NAMES and BABELNET together with a named entity guesser to recognise names. In a second step, we combine known names with wild cards to increase recognition recall by also capturing inflection variants. In a third step, we increase precision by filtering these name candidates with automatically learnt inflection patterns derived from name occurrences in large news article collections. Our major requirement is to achieve high precision. We achieved an average of 65% F-measure with 93% precision on the four languages.</abstract>
      <url hash="0e6a8ce3">W19-3714</url>
      <doi>10.18653/v1/W19-3714</doi>
      <bibkey>jacquet-etal-2019-jrc</bibkey>
    </paper>
    <paper id="15">
      <title>Building <fixed-case>E</fixed-case>nglish-to-<fixed-case>S</fixed-case>erbian Machine Translation System for <fixed-case>IMD</fixed-case>b Movie Reviews</title>
      <author><first>Pintu</first><last>Lohar</last></author>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>105–113</pages>
      <abstract>This paper reports the results of the first experiment dealing with the challenges of building a machine translation system for user-generated content involving a complex South Slavic language. We focus on translation of English IMDb user movie reviews into Serbian, in a low-resource scenario. We explore potentials and limits of (i) phrase-based and neural machine translation systems trained on out-of-domain clean parallel data from news articles (ii) creating additional synthetic in-domain parallel corpus by machine-translating the English IMDb corpus into Serbian. Our main findings are that morphology and syntax are better handled by the neural approach than by the phrase-based approach even in this low-resource mismatched domain scenario, however the situation is different for the lexical aspect, especially for person names. This finding also indicates that in general, machine translation of person names into Slavic languages (especially those which require/allow transcription) should be investigated more systematically.</abstract>
      <url hash="4ca83945">W19-3715</url>
      <doi>10.18653/v1/W19-3715</doi>
      <bibkey>lohar-etal-2019-building</bibkey>
      <pwccode url="https://github.com/m-popovic/imdb-corpus-for-MT" additional="false">m-popovic/imdb-corpus-for-MT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="16">
      <title>Improving Sentiment Classification in <fixed-case>S</fixed-case>lovak Language</title>
      <author><first>Samuel</first><last>Pecar</last></author>
      <author><first>Marian</first><last>Simko</last></author>
      <author><first>Maria</first><last>Bielikova</last></author>
      <pages>114–119</pages>
      <abstract>Using different neural network architectures is widely spread for many different NLP tasks. Unfortunately, most of the research is performed and evaluated only in English language and minor languages are often omitted. We believe using similar architectures for other languages can show interesting results. In this paper, we present our study on methods for improving sentiment classification in Slovak language. We performed several experiments for two different datasets, one containing customer reviews, the other one general Twitter posts. We show comparison of performance of different neural network architectures and also different word representations. We show that another improvement can be achieved by using a model ensemble. We performed experiments utilizing different methods of model ensemble. Our proposed models achieved better results than previous models for both datasets. Our experiments showed also other potential research areas.</abstract>
      <url hash="d1a78b0c">W19-3716</url>
      <doi>10.18653/v1/W19-3716</doi>
      <bibkey>pecar-etal-2019-improving</bibkey>
      <pwccode url="https://github.com/SamuelPecar/Slovak-sentiment-analysis" additional="false">SamuelPecar/Slovak-sentiment-analysis</pwccode>
    </paper>
    <paper id="17">
      <title>Sentiment Analysis for Multilingual Corpora</title>
      <author><first>Svitlana</first><last>Galeshchuk</last></author>
      <author><first>Ju</first><last>Qiu</last></author>
      <author><first>Julien</first><last>Jourdan</last></author>
      <pages>120–125</pages>
      <abstract>The paper presents a generic approach to the supervised sentiment analysis of social media content in Slavic languages. The method proposes translating the documents from the original language to English with Google’s Neural Translation Model. The resulted texts are then converted to vectors by averaging the vectorial representation of words derived from a pre-trained Word2Vec English model. Testing the approach with several machine learning methods on Polish, Slovenian and Croatian Twitter datasets returns up to 86% of classification accuracy on the out-of-sample data.</abstract>
      <url hash="cf59cbb0">W19-3717</url>
      <doi>10.18653/v1/W19-3717</doi>
      <bibkey>galeshchuk-etal-2019-sentiment</bibkey>
      <pwccode url="https://github.com/GSukr/Sentiment_Analysis_Multilingual_Corpora" additional="false">GSukr/Sentiment_Analysis_Multilingual_Corpora</pwccode>
    </paper>
  </volume>
  <volume id="38">
    <meta>
      <booktitle>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</booktitle>
      <url hash="180e4811">W19-38</url>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Will</first><last>Radford</last></editor>
      <editor><first>Kellie</first><last>Webster</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>gebnlp</venue>
    </meta>
    <frontmatter>
      <url hash="d5a09a13">W19-3800</url>
      <bibkey>ws-2019-gender</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Gendered Ambiguous Pronoun (<fixed-case>GAP</fixed-case>) Shared Task at the Gender Bias in <fixed-case>NLP</fixed-case> Workshop 2019</title>
      <author><first>Kellie</first><last>Webster</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <author><first>Will</first><last>Radford</last></author>
      <pages>1–7</pages>
      <abstract>The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2018), both via fine-tuning and for feature extraction, as well as ensembling.</abstract>
      <url hash="7d6e0796">W19-3801</url>
      <doi>10.18653/v1/W19-3801</doi>
      <bibkey>webster-etal-2019-gendered</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="2">
      <title>Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype</title>
      <author><first>Yasmeen</first><last>Hitti</last></author>
      <author><first>Eunbee</first><last>Jang</last></author>
      <author><first>Ines</first><last>Moreno</last></author>
      <author><first>Carolyne</first><last>Pelletier</last></author>
      <pages>8–17</pages>
      <abstract>The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75%. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14%.</abstract>
      <url hash="35db227d">W19-3802</url>
      <doi>10.18653/v1/W19-3802</doi>
      <bibkey>hitti-etal-2019-proposed</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="3">
      <title>Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis</title>
      <author><first>Scott</first><last>Friedman</last></author>
      <author><first>Sonja</first><last>Schmer-Galunder</last></author>
      <author><first>Anthony</first><last>Chen</last></author>
      <author><first>Jeffrey</first><last>Rye</last></author>
      <pages>18–24</pages>
      <abstract>Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.</abstract>
      <url hash="d7e9d132">W19-3803</url>
      <doi>10.18653/v1/W19-3803</doi>
      <bibkey>friedman-etal-2019-relating</bibkey>
    </paper>
    <paper id="4">
      <title>Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories</title>
      <author><first>Kaytlin</first><last>Chaloner</last></author>
      <author><first>Alfredo</first><last>Maldonado</last></author>
      <pages>25–32</pages>
      <abstract>Prior work has shown that word embeddings capture human stereotypes, including gender bias. However, there is a lack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking, biomedical and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings. We detect some gender bias in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT’s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.</abstract>
      <url hash="4f023b3b">W19-3804</url>
      <doi>10.18653/v1/W19-3804</doi>
      <bibkey>chaloner-maldonado-2019-measuring</bibkey>
      <pwccode url="https://github.com/alfredomg/GeBNLP2019" additional="false">alfredomg/GeBNLP2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="5">
      <title>Evaluating the Underlying Gender Bias in Contextualized Word Embeddings</title>
      <author><first>Christine</first><last>Basta</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Noe</first><last>Casas</last></author>
      <pages>33–39</pages>
      <abstract>Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.</abstract>
      <url hash="0068fc60">W19-3805</url>
      <doi>10.18653/v1/W19-3805</doi>
      <bibkey>basta-etal-2019-evaluating</bibkey>
    </paper>
    <paper id="6">
      <title>Conceptor Debiasing of Word Representations Evaluated on <fixed-case>WEAT</fixed-case></title>
      <author><first>Saket</first><last>Karve</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>40–48</pages>
      <abstract>Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for post-processing both traditional and contextualized word embeddings. Our method can simultaneously remove racial and gender biases from word representations. Unlike standard debiasing methods, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al. (2017).</abstract>
      <url hash="593e9bd0">W19-3806</url>
      <doi>10.18653/v1/W19-3806</doi>
      <bibkey>karve-etal-2019-conceptor</bibkey>
    </paper>
    <paper id="7">
      <title>Filling Gender &amp; Number Gaps in Neural Machine Translation with Black-box Context Injection</title>
      <author><first>Amit</first><last>Moryossef</last></author>
      <author><first>Roee</first><last>Aharoni</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>49–54</pages>
      <abstract>When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must “guess” this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.</abstract>
      <url hash="ce9522a3">W19-3807</url>
      <doi>10.18653/v1/W19-3807</doi>
      <bibkey>moryossef-etal-2019-filling</bibkey>
    </paper>
    <paper id="8">
      <title>The Role of Protected Class Word Lists in Bias Identification of Contextualized Word Representations</title>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>55–61</pages>
      <abstract>Systemic bias in word embeddings has been widely reported and studied, and efforts made to debias them; however, new contextualized embeddings such as ELMo and BERT are only now being similarly studied. Standard debiasing methods require heterogeneous lists of target words to identify the “bias subspace”. We show show that using new contextualized word embeddings in conceptor debiasing allows us to more accurately debias word embeddings by breaking target word lists into more homogeneous subsets and then combining (”Or’ing”) the debiasing conceptors of the different subsets.</abstract>
      <url hash="7b7ae484">W19-3808</url>
      <doi>10.18653/v1/W19-3808</doi>
      <bibkey>sedoc-ungar-2019-role</bibkey>
    </paper>
    <paper id="9">
      <title>Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in Sentiment Analysis</title>
      <author><first>Jayadev</first><last>Bhaskaran</last></author>
      <author><first>Isha</first><last>Bhallamudi</last></author>
      <pages>62–68</pages>
      <abstract>In this work, we investigate the presence of occupational gender stereotypes in sentiment analysis models. Such a task has implications in reducing implicit biases in these models, which are being applied to an increasingly wide variety of downstream tasks. We release a new gender-balanced dataset of 800 sentences pertaining to specific professions and propose a methodology for using it as a test bench to evaluate sentiment analysis models. We evaluate the presence of occupational gender stereotypes in 3 different models using our approach, and explore their relationship with societal perceptions of occupations.</abstract>
      <url hash="8a7fa059">W19-3809</url>
      <doi>10.18653/v1/W19-3809</doi>
      <bibkey>bhaskaran-bhallamudi-2019-good</bibkey>
      <pwccode url="https://github.com/jayadevbhaskaran/gendered-sentiment" additional="false">jayadevbhaskaran/gendered-sentiment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="10">
      <title>Debiasing Embeddings for Reduced Gender Bias in Text Classification</title>
      <author><first>Flavien</first><last>Prost</last></author>
      <author><first>Nithum</first><last>Thain</last></author>
      <author><first>Tolga</first><last>Bolukbasi</last></author>
      <pages>69–75</pages>
      <abstract>(Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.</abstract>
      <url hash="a992df79">W19-3810</url>
      <doi>10.18653/v1/W19-3810</doi>
      <bibkey>prost-etal-2019-debiasing</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>BERT</fixed-case> Masked Language Modeling for Co-reference Resolution</title>
      <author><first>Felipe</first><last>Alfaro</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <pages>76–81</pages>
      <abstract>This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for Natural Language Processing. We have implemented two models for mask language modeling using pre-trained BERT adjusted to work for a classification problem. The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names.</abstract>
      <url hash="373c64cf">W19-3811</url>
      <doi>10.18653/v1/W19-3811</doi>
      <bibkey>alfaro-etal-2019-bert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="12">
      <title>Transfer Learning from Pre-trained <fixed-case>BERT</fixed-case> for Pronoun Resolution</title>
      <author><first>Xingce</first><last>Bao</last></author>
      <author><first>Qianqian</first><last>Qiao</last></author>
      <pages>82–88</pages>
      <abstract>The paper describes the submission of the team “We used bert!” to the shared task Gendered Pronoun Resolution (Pair pronouns to their correct entities). Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208. In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated.</abstract>
      <url hash="57bd1ee2">W19-3812</url>
      <doi>10.18653/v1/W19-3812</doi>
      <bibkey>bao-qiao-2019-transfer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>MS</fixed-case>net: A <fixed-case>BERT</fixed-case>-based Network for Gendered Pronoun Resolution</title>
      <author><first>Zili</first><last>Wang</last></author>
      <pages>89–95</pages>
      <abstract>The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at: https://github.com/ziliwang/MSnet-for-Gendered-Pronoun-Resolution</abstract>
      <url hash="57b2fa14">W19-3813</url>
      <doi>10.18653/v1/W19-3813</doi>
      <bibkey>wang-2019-msnet</bibkey>
      <pwccode url="https://github.com/ziliwang/MSnet-for-Gendered-Pronoun-Resolution" additional="false">ziliwang/MSnet-for-Gendered-Pronoun-Resolution</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="14">
      <title>Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</title>
      <author><first>Yinchuan</first><last>Xu</last></author>
      <author><first>Junlin</first><last>Yang</last></author>
      <pages>96–101</pages>
      <abstract>Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9% F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific task is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN’s embeddings outperform the original BERT embeddings on the coreference task. Our work significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9% to 80.3%. We participated in the Gender Bias for Natural Language Processing 2019 shared task, and our codes are available online.</abstract>
      <url hash="a533b60a">W19-3814</url>
      <doi>10.18653/v1/W19-3814</doi>
      <bibkey>xu-yang-2019-look</bibkey>
      <pwccode url="https://github.com/ianycxu/RGCN-with-BERT" additional="false">ianycxu/RGCN-with-BERT</pwccode>
    </paper>
    <paper id="15">
      <title>Fill the <fixed-case>GAP</fixed-case>: Exploiting <fixed-case>BERT</fixed-case> for Pronoun Resolution</title>
      <author><first>Kai-Chou</first><last>Yang</last></author>
      <author><first>Timothy</first><last>Niven</last></author>
      <author><first>Tzu Hsuan</first><last>Chou</last></author>
      <author><first>Hung-Yu</first><last>Kao</last></author>
      <pages>102–106</pages>
      <abstract>In this paper, we describe our entry in the gendered pronoun resolution competition which achieved fourth place without data augmentation. Our method is an ensemble system of BERTs which resolves co-reference in an interaction space. We report four insights from our work: BERT’s representations involve significant redundancy; modeling interaction effects similar to natural language inference models is useful for this task; there is an optimal BERT layer to extract representations for pronoun resolution; and the difference between the attention weights from the pronoun to the candidate entities was highly correlated with the correct label, with interesting implications for future work.</abstract>
      <url hash="a3ddad2a">W19-3815</url>
      <doi>10.18653/v1/W19-3815</doi>
      <bibkey>yang-etal-2019-fill</bibkey>
      <pwccode url="https://github.com/zake7749/Fill-the-GAP" additional="false">zake7749/Fill-the-GAP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="16">
      <title>On <fixed-case>GAP</fixed-case> Coreference Resolution Shared Task: Insights from the 3rd Place Solution</title>
      <author><first>Artem</first><last>Abzaliev</last></author>
      <pages>107–112</pages>
      <abstract>This paper presents the 3rd-place-winning solution to the GAP coreference resolution shared task. The approach adopted consists of two key components: fine-tuning the BERT language representation model (Devlin et al., 2018) and the usage of external datasets during the training process. The model uses hidden states from the intermediate BERT layers instead of the last layer. The resulting system almost eliminates the difference in log loss per gender during the cross-validation, while providing high performance.</abstract>
      <url hash="025792e7">W19-3816</url>
      <doi>10.18653/v1/W19-3816</doi>
      <bibkey>abzaliev-2019-gap</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="17">
      <title>Resolving Gendered Ambiguous Pronouns with <fixed-case>BERT</fixed-case></title>
      <author><first>Matei</first><last>Ionita</last></author>
      <author><first>Yury</first><last>Kashnitsky</last></author>
      <author><first>Ken</first><last>Krige</last></author>
      <author><first>Vladimir</first><last>Larin</last></author>
      <author><first>Atanas</first><last>Atanasov</last></author>
      <author><first>Dennis</first><last>Logvinenko</last></author>
      <pages>113–119</pages>
      <abstract>Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.</abstract>
      <url hash="c2b507fd">W19-3817</url>
      <doi>10.18653/v1/W19-3817</doi>
      <bibkey>ionita-etal-2019-resolving</bibkey>
      <pwccode url="https://github.com/Yorko/gender-unbiased_BERT-based_pronoun_resolution" additional="false">Yorko/gender-unbiased_BERT-based_pronoun_resolution</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="18">
      <title>Anonymized <fixed-case>BERT</fixed-case>: An Augmentation Approach to the Gendered Pronoun Resolution Challenge</title>
      <author><first>Bo</first><last>Liu</last></author>
      <pages>120–125</pages>
      <abstract>We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing training data size, this approach diversifies idiosyncratic information embedded in names. Using same set of common first names can also help the model recognize names better, shorten token length, and remove gender and regional biases associated with names. The system scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the system scores 0.1799 which would be third place.</abstract>
      <url hash="dd0ef589">W19-3818</url>
      <doi>10.18653/v1/W19-3818</doi>
      <bibkey>liu-2019-anonymized</bibkey>
      <pwccode url="https://github.com/boliu61/gendered-pronoun-resolution" additional="false">boliu61/gendered-pronoun-resolution</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="19">
      <title>Gendered Pronoun Resolution using <fixed-case>BERT</fixed-case> and an Extractive Question Answering Formulation</title>
      <author><first>Rakesh</first><last>Chada</last></author>
      <pages>126–133</pages>
      <abstract>The resolution of ambiguous pronouns is a longstanding challenge in Natural Language Understanding. Recent studies have suggested gender bias among state-of-the-art coreference resolution systems. As an example, Google AI Language team recently released a gender-balanced dataset and showed that performance of these coreference resolvers is significantly limited on the dataset. In this paper, we propose an extractive question answering (QA) formulation of pronoun resolution task that overcomes this limitation and shows much lower gender bias (0.99) on their dataset. This system uses fine-tuned representations from the pre-trained BERT model and outperforms the existing baseline by a significant margin (22.2% absolute improvement in F1 score) without using any hand-engineered features. This QA framework is equally performant even without the knowledge of the candidate antecedents of the pronoun. An ensemble of QA and BERT-based multiple choice and sequence classification models further improves the F1 (23.3% absolute improvement upon the baseline). This ensemble model was submitted to the shared task for the 1st ACL workshop on Gender Bias for Natural Language Processing. It ranked 9th on the final official leaderboard.</abstract>
      <url hash="c28cdd68">W19-3819</url>
      <doi>10.18653/v1/W19-3819</doi>
      <bibkey>chada-2019-gendered</bibkey>
      <pwccode url="https://github.com/rakeshchada/corefqa" additional="false">rakeshchada/corefqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap">GAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="20">
      <title>Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling</title>
      <author><first>Sandeep</first><last>Attree</last></author>
      <pages>134–146</pages>
      <abstract>This paper presents a strong set of results for resolving gendered ambiguous pronouns on the Gendered Ambiguous Pronouns shared task. The model presented here draws upon the strengths of state-of-the-art language and coreference resolution models, and introduces a novel evidence-based deep learning architecture. Injecting evidence from the coreference models compliments the base architecture, and analysis shows that the model is not hindered by their weaknesses, specifically gender bias. The modularity and simplicity of the architecture make it very easy to extend for further improvement and applicable to other NLP problems. Evaluation on GAP test data results in a state-of-the-art performance at 92.5% F1 (gender bias of 0.97), edging closer to the human performance of 96.6%. The end-to-end solution presented here placed 1st in the Kaggle competition, winning by a significant lead.</abstract>
      <url hash="313b33f9">W19-3820</url>
      <doi>10.18653/v1/W19-3820</doi>
      <bibkey>attree-2019-gendered</bibkey>
      <pwccode url="https://github.com/sattree/gap" additional="false">sattree/gap</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap">GAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="21">
      <title>Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques</title>
      <author><first>Joel</first><last>Escudé Font</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>147–154</pages>
      <abstract>Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.</abstract>
      <url hash="6e47b9f0">W19-3821</url>
      <doi>10.18653/v1/W19-3821</doi>
      <bibkey>escude-font-costa-jussa-2019-equalizing</bibkey>
    </paper>
    <paper id="22">
      <title>Automatic Gender Identification and Reinflection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Christine</first><last>Chung</last></author>
      <pages>155–165</pages>
      <abstract>The impressive progress in many Natural Language Processing (NLP) applications has increased the awareness of some of the biases these NLP systems have with regards to gender identities. In this paper, we propose an approach to extend biased single-output gender-blind NLP systems with gender-specific alternative reinflections. We focus on Arabic, a gender-marking morphologically rich language, in the context of machine translation (MT) from English, and for first-person-singular constructions only. Our contributions are the development of a system-independent gender-awareness wrapper, and the building of a corpus for training and evaluating first-person-singular gender identification and reinflection in Arabic. Our results successfully demonstrate the viability of this approach with 8% relative increase in Bleu score for first-person-singular feminine, and 5.3% comparable increase for first-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set.</abstract>
      <url hash="436955b7">W19-3822</url>
      <doi>10.18653/v1/W19-3822</doi>
      <revision id="1" href="W19-3822v1" hash="955fe9b8"/>
      <revision id="2" href="W19-3822v2" hash="436955b7" date="2020-07-08">Correction to the size of a synthetic dataset</revision>
      <bibkey>habash-etal-2019-automatic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="23">
      <title>Measuring Bias in Contextualized Word Representations</title>
      <author><first>Keita</first><last>Kurita</last></author>
      <author><first>Nidhi</first><last>Vyas</last></author>
      <author><first>Ayush</first><last>Pareek</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>166–172</pages>
      <abstract>Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.</abstract>
      <url hash="0ac4fdb0">W19-3823</url>
      <doi>10.18653/v1/W19-3823</doi>
      <bibkey>kurita-etal-2019-measuring</bibkey>
    </paper>
    <paper id="24">
      <title>On Measuring Gender Bias in Translation of Gender-neutral Pronouns</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Ji Won</first><last>Kim</last></author>
      <author><first>Seok Min</first><last>Kim</last></author>
      <author><first>Nam Soo</first><last>Kim</last></author>
      <pages>173–181</pages>
      <abstract>Ethics regarding social bias has recently thrown striking issues in natural language processing. Especially for gender-related topics, the need for a system that reduces the model bias has grown in areas such as image captioning, content recommendation, and automated employment. However, detection and evaluation of gender bias in the machine translation systems are not yet thoroughly investigated, for the task being cross-lingual and challenging to define. In this paper, we propose a scheme for making up a test set that evaluates the gender bias in a machine translation system, with Korean, a language with gender-neutral pronouns. Three word/phrase sets are primarily constructed, each incorporating positive/negative expressions or occupations; all the terms are gender-independent or at least not biased to one side severely. Then, additional sentence lists are constructed concerning formality of the pronouns and politeness of the sentences. With the generated sentence set of size 4,236 in total, we evaluate gender bias in conventional machine translation systems utilizing the proposed measure, which is termed here as translation gender bias index (TGBI). The corpus and the code for evaluation is available on-line.</abstract>
      <url hash="6e237c14">W19-3824</url>
      <doi>10.18653/v1/W19-3824</doi>
      <bibkey>cho-etal-2019-measuring</bibkey>
      <pwccode url="https://github.com/nolongerprejudice/tgbi" additional="false">nolongerprejudice/tgbi</pwccode>
    </paper>
  </volume>
  <volume id="39">
    <meta>
      <booktitle>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</booktitle>
      <url hash="b4185bd7">W19-39</url>
      <editor><first>Jason</first><last>Eisner</last></editor>
      <editor><first>Matthias</first><last>Gallé</last></editor>
      <editor><first>Jeffrey</first><last>Heinz</last></editor>
      <editor><first>Ariadna</first><last>Quattoni</last></editor>
      <editor><first>Guillaume</first><last>Rabusseau</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence</address>
      <month>August</month>
      <year>2019</year>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="159de9bd">W19-3900</url>
      <bibkey>ws-2019-deep</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Sequential Neural Networks as Automata</title>
      <author><first>William</first><last>Merrill</last></author>
      <pages>1–13</pages>
      <abstract>This work attempts to explain the types of computation that neural networks can perform by relating them to automata. We first define what it means for a real-time network with bounded precision to accept a language. A measure of network memory follows from this definition. We then characterize the classes of languages acceptable by various recurrent networks, attention, and convolutional networks. We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy. Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory. These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar.</abstract>
      <url hash="3d1ee95c">W19-3901</url>
      <doi>10.18653/v1/W19-3901</doi>
      <bibkey>merrill-2019-sequential</bibkey>
    </paper>
    <paper id="2">
      <title>Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing</title>
      <author><first>Chunyang</first><last>Xiao</last></author>
      <author><first>Christoph</first><last>Teichmann</last></author>
      <author><first>Konstantine</first><last>Arkoudas</last></author>
      <pages>14–23</pages>
      <abstract>While sequence-to-sequence (seq2seq) models achieve state-of-the-art performance in many natural language processing tasks, they can be too slow for real-time applications. One performance bottleneck is predicting the most likely next token over a large vocabulary; methods to circumvent this bottleneck are a current research topic. We focus specifically on using seq2seq models for semantic parsing, where we observe that grammars often exist which specify valid formal representations of utterance semantics. By developing a generic approach for restricting the predictions of a seq2seq model to grammatically permissible continuations, we arrive at a widely applicable technique for speeding up semantic parsing. The technique leads to a 74% speed-up on an in-house dataset with a large vocabulary, compared to the same neural model without grammatical restrictions</abstract>
      <url hash="b2271fa5">W19-3902</url>
      <doi>10.18653/v1/W19-3902</doi>
      <bibkey>xiao-etal-2019-grammatical</bibkey>
    </paper>
    <paper id="3">
      <title>Relating <fixed-case>RNN</fixed-case> Layers with the Spectral <fixed-case>WFA</fixed-case> Ranks in Sequence Modelling</title>
      <author><first>Farhana Ferdousi</first><last>Liza</last></author>
      <author><first>Marek</first><last>Grzes</last></author>
      <pages>24–33</pages>
      <abstract>We analyse Recurrent Neural Networks (RNNs) to understand the significance of multiple LSTM layers. We argue that the Weighted Finite-state Automata (WFA) trained using a spectral learning algorithm are helpful to analyse RNNs. Our results suggest that multiple LSTM layers in RNNs help learning distributed hidden states, but have a smaller impact on the ability to learn long-term dependencies. The analysis is based on the empirical results, however relevant theory (whenever possible) was discussed to justify and support our conclusions.</abstract>
      <url hash="ed01d225">W19-3903</url>
      <doi>10.18653/v1/W19-3903</doi>
      <bibkey>liza-grzes-2019-relating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="4">
      <title>Multi-Element Long Distance Dependencies: Using <fixed-case>SP</fixed-case>k Languages to Explore the Characteristics of Long-Distance Dependencies</title>
      <author><first>Abhijit</first><last>Mahalunkar</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>34–43</pages>
      <abstract>In order to successfully model Long Distance Dependencies (LDDs) it is necessary to under-stand the full-range of the characteristics of the LDDs exhibited in a target dataset. In this paper, we use Strictly k-Piecewise languages to generate datasets with various properties. We then compute the characteristics of the LDDs in these datasets using mutual information and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden strings, and (v) dataset size. This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs. This leads us to the challenge of modelling multi-element long-distance dependencies. Our results suggest that attention mechanisms in neural networks may aide in modeling datasets with multi-element long-distance dependencies. However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue.</abstract>
      <url hash="49309885">W19-3904</url>
      <doi>10.18653/v1/W19-3904</doi>
      <bibkey>mahalunkar-kelleher-2019-multi</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>LSTM</fixed-case> Networks Can Perform Dynamic Counting</title>
      <author><first>Mirac</first><last>Suzgun</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Stuart</first><last>Shieber</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <pages>44–54</pages>
      <abstract>In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations. All the neural models in our experiments are designed to be small-sized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour at test time. Our results demonstrate that the Long Short-Term Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple real-time k-counter machines. To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks. We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language. However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition.</abstract>
      <url hash="673bb2b8">W19-3905</url>
      <doi>10.18653/v1/W19-3905</doi>
      <bibkey>suzgun-etal-2019-lstm</bibkey>
    </paper>
  </volume>
  <volume id="40">
    <meta>
      <booktitle>Proceedings of the 13th Linguistic Annotation Workshop</booktitle>
      <url hash="44cfe02f">W19-40</url>
      <editor><first>Annemarie</first><last>Friedrich</last></editor>
      <editor><first>Deniz</first><last>Zeyrek</last></editor>
      <editor><first>Jet</first><last>Hoek</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>law</venue>
    </meta>
    <frontmatter>
      <url hash="5449ee0b">W19-4000</url>
      <bibkey>ws-2019-linguistic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Crowdsourced Hedge Term Disambiguation</title>
      <author><first>Morgan</first><last>Ulinski</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>1–5</pages>
      <abstract>We address the issue of acquiring quality annotations of hedging words and phrases, linguistic phenomenona in which words, sounds, or other constructions are used to express ambiguity or uncertainty. Due to the limited availability of existing corpora annotated for hedging, linguists and other language scientists have been constrained as to the extent they can study this phenomenon. In this paper, we introduce a new method of acquiring hedging annotations via crowdsourcing, based on reformulating the task of labeling hedges as a simple word sense disambiguation task. We also introduce a new hedging corpus we have constructed by applying this method, a collection of forum posts annotated using Amazon Mechanical Turk. We found that the crowdsourced judgments we obtained had an inter-annotator agreement of 92.89% (Fleiss’ Kappa=0.751) and, when comparing a subset of these annotations to an expert-annotated gold standard, an accuracy of 96.65%.</abstract>
      <url hash="d72543bf">W19-4001</url>
      <doi>10.18653/v1/W19-4001</doi>
      <bibkey>ulinski-hirschberg-2019-crowdsourced</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>W</fixed-case>i<fixed-case>R</fixed-case>e57 : A Fine-Grained Benchmark for Open Information Extraction</title>
      <author><first>William</first><last>Lechelle</last></author>
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>6–15</pages>
      <abstract>We build a reference for the task of Open Information Extraction, on five documents. We tentatively resolve a number of issues that arise, including coreference and granularity, and we take steps toward addressing inference, a significant problem. We seek to better pinpoint the requirements for the task. We produce our annotation guidelines specifying what is correct to extract and what is not. In turn, we use this reference to score existing Open IE systems. We address the non-trivial problem of evaluating the extractions produced by systems against the reference tuples, and share our evaluation script. Among seven compared extractors, we find the MinIE system to perform best.</abstract>
      <url hash="37c07586">W19-4002</url>
      <doi>10.18653/v1/W19-4002</doi>
      <bibkey>lechelle-etal-2019-wire57</bibkey>
      <pwccode url="https://github.com/rali-udem/WiRe57" additional="false">rali-udem/WiRe57</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wire57">WiRe57</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
    </paper>
    <paper id="3">
      <title>Crowdsourcing Discourse Relation Annotations by a Two-Step Connective Insertion Task</title>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <pages>16–25</pages>
      <abstract>The perspective of being able to crowd-source coherence relations bears the promise of acquiring annotations for new texts quickly, which could then increase the size and variety of discourse-annotated corpora. It would also open the avenue to answering new research questions: Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation. However, annotating coherence relations with untrained workers is not trivial. We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a). In our approach, coherence relation labels are inferred from connectives that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and annotations from PDTB and RSTDT on the same texts.</abstract>
      <url hash="eabf50b8">W19-4003</url>
      <doi>10.18653/v1/W19-4003</doi>
      <bibkey>yung-etal-2019-crowdsourcing</bibkey>
    </paper>
    <paper id="4">
      <title>Annotating and analyzing the interactions between meaning relations</title>
      <author><first>Darina</first><last>Gold</last></author>
      <author><first>Venelin</first><last>Kovatchev</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>26–36</pages>
      <abstract>Pairs of sentences, phrases, or other text pieces can hold semantic relations such as paraphrasing, textual entailment, contradiction, specificity, and semantic similarity. These relations are usually studied in isolation and no dataset exists where they can be compared empirically. Here we present a corpus annotated with these relations and the analysis of these results. The corpus contains 520 sentence pairs, annotated with these relations. We measure the annotation reliability of each individual relation and we examine their interactions and correlations. Among the unexpected results revealed by our analysis is that the traditionally considered direct relationship between paraphrasing and bi-directional entailment does not hold in our data.</abstract>
      <url hash="79f8145e">W19-4004</url>
      <doi>10.18653/v1/W19-4004</doi>
      <bibkey>gold-etal-2019-annotating</bibkey>
      <pwccode url="https://github.com/MeDarina/meaning_relations_interaction" additional="false">MeDarina/meaning_relations_interaction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>CCG</fixed-case>web: a New Annotation Tool and a First Quadrilingual <fixed-case>CCG</fixed-case> Treebank</title>
      <author><first>Kilian</first><last>Evang</last></author>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>37–42</pages>
      <abstract>We present the first open-source graphical annotation tool for combinatory categorial grammar (CCG), and the first set of detailed guidelines for syntactic annotation with CCG, for four languages: English, German, Italian, and Dutch. We also release a parallel pilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K single-annotator fully corrected sentences, and 82K single-annotator partially corrected sentences.</abstract>
      <url hash="3bb307ba">W19-4005</url>
      <doi>10.18653/v1/W19-4005</doi>
      <revision id="1" href="W19-4005v1" hash="22251411"/>
      <revision id="2" href="W19-4005v2" hash="9241d013">The last four rows of Table 1 had incorrect sentence counts and average lengths (too little for gold, far too much for silver). This concerns supplemental data and does not affect the main substance of the paper.

In the last paragraph of Section 3, the word "additional" was changed to "supplemental" to be more consistent with the online release.</revision>
      <revision id="3" href="W19-4005v3" date="2020-04-13" hash="3bb307ba">The published version contains an incorrect number of sentences in the supplementary data.</revision>
      <bibkey>evang-etal-2019-ccgweb</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="6">
      <title>The making of the Litkey Corpus, a richly annotated longitudinal corpus of <fixed-case>G</fixed-case>erman texts written by primary school children</title>
      <author><first>Ronja</first><last>Laarmann-Quante</last></author>
      <author><first>Stefanie</first><last>Dipper</last></author>
      <author><first>Eva</first><last>Belke</last></author>
      <pages>43–55</pages>
      <abstract>To date, corpus and computational linguistic work on written language acquisition has mostly dealt with second language learners who have usually already mastered orthography acquisition in their first language. In this paper, we present the Litkey Corpus, a richly-annotated longitudinal corpus of written texts produced by primary school children in Germany from grades 2 to 4. The paper focuses on the (semi-)automatic annotation procedure at various linguistic levels, which include POS tags, features of the word-internal structure (phonemes, syllables, morphemes) and key orthographic features of the target words as well as a categorization of spelling errors. Comprehensive evaluations show that high accuracy was achieved on all levels, making the Litkey Corpus a useful resource for corpus-based research on literacy acquisition of German primary school children and for developing NLP tools for educational purposes. The corpus is freely available under https://www.linguistics.rub.de/litkeycorpus/.</abstract>
      <url hash="06329aa4">W19-4006</url>
      <doi>10.18653/v1/W19-4006</doi>
      <bibkey>laarmann-quante-etal-2019-making</bibkey>
    </paper>
    <paper id="7">
      <title>The Materials Science Procedural Text Corpus: Annotating Materials Synthesis Procedures with Shallow Semantic Structures</title>
      <author><first>Sheshera</first><last>Mysore</last></author>
      <author><first>Zachary</first><last>Jensen</last></author>
      <author><first>Edward</first><last>Kim</last></author>
      <author><first>Kevin</first><last>Huang</last></author>
      <author><first>Haw-Shiuan</first><last>Chang</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <author><first>Elsa</first><last>Olivetti</last></author>
      <pages>56–64</pages>
      <abstract>Materials science literature contains millions of materials synthesis procedures described in unstructured natural language text. Large-scale analysis of these synthesis procedures would facilitate deeper scientific understanding of materials synthesis and enable automated synthesis planning. Such analysis requires extracting structured representations of synthesis procedures from the raw text as a first step. To facilitate the training and evaluation of synthesis extraction models, we introduce a dataset of 230 synthesis procedures annotated by domain experts with labeled graphs that express the semantics of the synthesis sentences. The nodes in this graph are synthesis operations and their typed arguments, and labeled edges specify relations between the nodes. We describe this new resource in detail and highlight some specific challenges to annotating scientific text with shallow semantic structure. We make the corpus available to the community to promote further research and development of scientific information extraction systems.</abstract>
      <url hash="92350404">W19-4007</url>
      <doi>10.18653/v1/W19-4007</doi>
      <bibkey>mysore-etal-2019-materials</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/materials-project">Materials Project</pwcdataset>
    </paper>
    <paper id="8">
      <title>Tagging modality in Oceanic languages of Melanesia</title>
      <author><first>Annika</first><last>Tjuka</last></author>
      <author><first>Lena</first><last>Weißmann</last></author>
      <author><first>Kilu</first><last>von Prince</last></author>
      <pages>65–70</pages>
      <abstract>Primary data from small, low-resource languages of Oceania have only recently become available through language documentation. In our study, we explore corpus data of five Oceanic languages of Melanesia which are known to be mood-prominent (in the sense of Bhat, 1999). In order to find out more about tense, aspect, modality, and polarity, we tagged these categories in a subset of our corpora. For the category of modality, we developed a novel tag set (MelaTAMP, 2017), which categorizes clauses into factual, possible, and counterfactual. Based on an analysis of the inter-annotator consistency, we argue that our tag set for the modal domain is efficient for our subject languages and might be useful for other languages and purposes.</abstract>
      <url hash="803a97a9">W19-4008</url>
      <doi>10.18653/v1/W19-4008</doi>
      <bibkey>tjuka-etal-2019-tagging</bibkey>
    </paper>
    <paper id="9">
      <title>Harmonizing Different Lemmatization Strategies for Building a Knowledge Base of Linguistic Resources for <fixed-case>L</fixed-case>atin</title>
      <author><first>Francesco</first><last>Mambrini</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <pages>71–80</pages>
      <abstract>The interoperability between lemmatized corpora of Latin and other resources that use the lemma as indexing key is hampered by the multiple lemmatization strategies that different projects adopt. In this paper we discuss how we tackle the challenges raised by harmonizing different lemmatization criteria in the context of a project that aims to connect linguistic resources for Latin using the Linked Data paradigm. The paper introduces the architecture supporting an open-ended, lemma-based Knowledge Base, built to make textual and lexical resources for Latin interoperable. Particularly, the paper describes the inclusion into the Knowledge Base of its lexical basis, of a word formation lexicon and of a lemmatized and syntactically annotated corpus.</abstract>
      <url hash="c4b7c080">W19-4009</url>
      <doi>10.18653/v1/W19-4009</doi>
      <bibkey>mambrini-passarotti-2019-harmonizing</bibkey>
    </paper>
    <paper id="10">
      <title>Assessing Back-Translation as a Corpus Generation Strategy for non-<fixed-case>E</fixed-case>nglish Tasks: A Study in Reading Comprehension and Word Sense Disambiguation</title>
      <author><first>Fabricio</first><last>Monsalve</last></author>
      <author><first>Kervy</first><last>Rivas Rojas</last></author>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <pages>81–89</pages>
      <abstract>Corpora curated by experts have sustained Natural Language Processing mainly in English, but the expensiveness of corpora creation is a barrier for the development in further languages. Thus, we propose a corpus generation strategy that only requires a machine translation system between English and the target language in both directions, where we filter the best translations by computing automatic translation metrics and the task performance score. By studying Reading Comprehension in Spanish and Word Sense Disambiguation in Portuguese, we identified that a more quality-oriented metric has high potential in the corpora selection without degrading the task performance. We conclude that it is possible to systematise the building of quality corpora using machine translation and automatic metrics, besides some prior effort to clean and process the data.</abstract>
      <url hash="fb980812">W19-4010</url>
      <doi>10.18653/v1/W19-4010</doi>
      <bibkey>monsalve-etal-2019-assessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="11">
      <title>A Framework for Annotating ‘Related Works’ to Support Feedback to Novice Writers</title>
      <author><first>Arlene</first><last>Casey</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Dorota</first><last>Glowacka</last></author>
      <pages>90–99</pages>
      <abstract>Understanding what is expected of academic writing can be difficult for novice writers to assimilate, and recent years have seen several automated tools become available to support academic writing. Our work presents a framework for annotating features of the Related Work section of academic writing, that supports writer feedback.</abstract>
      <url hash="a270f93e">W19-4011</url>
      <doi>10.18653/v1/W19-4011</doi>
      <bibkey>casey-etal-2019-framework</bibkey>
    </paper>
    <paper id="12">
      <title>An Online Annotation Assistant for Argument Schemes</title>
      <author><first>John</first><last>Lawrence</last></author>
      <author><first>Jacky</first><last>Visser</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <pages>100–107</pages>
      <abstract>Understanding the inferential principles underpinning an argument is essential to the proper interpretation and evaluation of persuasive discourse. Argument schemes capture the conventional patterns of reasoning appealed to in persuasion. The empirical study of these patterns relies on the availability of data about the actual use of argumentation in communicative practice. Annotated corpora of argument schemes, however, are scarce, small, and unrepresentative. Aiming to address this issue, we present one step in the development of improved datasets by integrating the Argument Scheme Key – a novel annotation method based on one of the most popular typologies of argument schemes – into the widely used OVA software for argument analysis.</abstract>
      <url hash="e3f56bad">W19-4012</url>
      <doi>10.18653/v1/W19-4012</doi>
      <bibkey>lawrence-etal-2019-online</bibkey>
    </paper>
    <paper id="13">
      <title>Annotating formulaic sequences in spoken <fixed-case>S</fixed-case>lovenian: structure, function and relevance</title>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <pages>108–112</pages>
      <abstract>This paper presents the identification of formulaic sequences in the reference corpus of spoken Slovenian and their annotation in terms of syntactic structure, pragmatic function and lexicographic relevance. The annotation campaign, specific in terms of setting, subjectivity and the multifunctionality of items under investigation, resulted in a preliminary lexicon of formulaic sequences in spoken Slovenian with immediate potential for future explorations in formulaic language research. This is especially relevant for the notable number of identified multi-word expressions with discourse-structuring and stance-marking functions, which have often been overlooked by traditional phraseology research.</abstract>
      <url hash="40643cb3">W19-4013</url>
      <doi>10.18653/v1/W19-4013</doi>
      <bibkey>dobrovoljc-2019-annotating</bibkey>
    </paper>
    <paper id="14">
      <title>Annotating Information Structure in <fixed-case>I</fixed-case>talian: Characteristics and Cross-Linguistic Applicability of a <fixed-case>QUD</fixed-case>-Based Approach</title>
      <author><first>Kordula</first><last>De Kuthy</last></author>
      <author><first>Lisa</first><last>Brunetti</last></author>
      <author><first>Marta</first><last>Berardi</last></author>
      <pages>113–123</pages>
      <abstract>We present a discourse annotation study, in which an annotation method based on Questions under Discussion (QuD) is applied to Italian data. The results of our inter-annotator agreement analysis show that the QUD-based approach, originally spelled out for English and German, can successfully be transferred cross-linguistically, supporting good agreement for the annotation of central information structure notions such as focus and non-at-issueness. Our annotation and interannotator agreement study on Italian authentic data confirms the cross-linguistic applicability of the QuD-based approach.</abstract>
      <url hash="f9a19f1c">W19-4014</url>
      <doi>10.18653/v1/W19-4014</doi>
      <bibkey>de-kuthy-etal-2019-annotating</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>DEFT</fixed-case>: A corpus for definition extraction in free- and semi-structured text</title>
      <author><first>Sasha</first><last>Spala</last></author>
      <author><first>Nicholas A.</first><last>Miller</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Carl</first><last>Dockhorn</last></author>
      <pages>124–131</pages>
      <abstract>Definition extraction has been a popular topic in NLP research for well more than a decade, but has been historically limited to well-defined, structured, and narrow conditions. In reality, natural language is messy, and messy data requires both complex solutions and data that reflects that reality. In this paper, we present a robust English corpus and annotation schema that allows us to explore the less straightforward examples of term-definition structures in free and semi-structured text.</abstract>
      <url hash="1caaf019">W19-4015</url>
      <doi>10.18653/v1/W19-4015</doi>
      <bibkey>spala-etal-2019-deft</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/deft-corpus">DEFT Corpus</pwcdataset>
    </paper>
    <paper id="16">
      <title>Explaining Simple Natural Language Inference</title>
      <author><first>Aikaterini-Lida</first><last>Kalouli</last></author>
      <author><first>Annebeth</first><last>Buis</last></author>
      <author><first>Livy</first><last>Real</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Valeria</first><last>de Paiva</last></author>
      <pages>132–143</pages>
      <abstract>The vast amount of research introducing new corpora and techniques for semi-automatically annotating corpora shows the important role that datasets play in today’s research, especially in the machine learning community. This rapid development raises concerns about the quality of the datasets created and consequently of the models trained, as recently discussed with respect to the Natural Language Inference (NLI) task. In this work we conduct an annotation experiment based on a small subset of the SICK corpus. The experiment reveals several problems in the annotation guidelines, and various challenges of the NLI task itself. Our quantitative evaluation of the experiment allows us to assign our empirical observations to specific linguistic phenomena and leads us to recommendations for future annotation tasks, for NLI and possibly for other tasks.</abstract>
      <url hash="049eff3a">W19-4016</url>
      <doi>10.18653/v1/W19-4016</doi>
      <bibkey>kalouli-etal-2019-explaining</bibkey>
      <pwccode url="https://github.com/kkalouli/SICK-processing" additional="false">kkalouli/SICK-processing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="17">
      <title>On the role of discourse relations in persuasive texts</title>
      <author><first>Ines</first><last>Rehbein</last></author>
      <pages>144–154</pages>
      <abstract>This paper investigates the use of explicitly signalled discourse relations in persuasive texts. We present a corpus study where we control for speaker and topic and show that the distribution of different discourse connectives varies considerably across different discourse settings. While this variation can be explained by genre differences, we also observe variation regarding the distribution of discourse relations across different settings. This variation, however, cannot be easily explained by genre differences. We argue that the differences regarding the use of discourse relations reflects different strategies of persuasion and that these might be due to audience design.</abstract>
      <url hash="78ac9135">W19-4017</url>
      <doi>10.18653/v1/W19-4017</doi>
      <bibkey>rehbein-2019-role</bibkey>
    </paper>
    <paper id="18">
      <title>One format to rule them all – The emtsv pipeline for <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Balázs</first><last>Indig</last></author>
      <author><first>Bálint</first><last>Sass</last></author>
      <author><first>Eszter</first><last>Simon</last></author>
      <author><first>Iván</first><last>Mittelholcz</last></author>
      <author><first>Noémi</first><last>Vadász</last></author>
      <author><first>Márton</first><last>Makrai</last></author>
      <pages>155–165</pages>
      <abstract>We present a more efficient version of the e-magyar NLP pipeline for Hungarian called emtsv. It integrates Hungarian NLP tools in a framework whose individual modules can be developed or replaced independently and allows new ones to be added. The design also allows convenient investigation and manual correction of the data flow from one module to another. The improvements we publish include effective communication between the modules and support of the use of individual modules both in the chain and standing alone. Our goals are accomplished using extended tsv (tab separated values) files, a simple, uniform, generic and self-documenting input/output format. Our vision is maintaining the system for a long time and making it easier for external developers to fit their own modules into the system, thus sharing existing competencies in the field of processing Hungarian, a mid-resourced language. The source code is available under LGPL 3.0 license at https://github.com/dlt-rilmta/emtsv .</abstract>
      <url hash="b3304373">W19-4018</url>
      <doi>10.18653/v1/W19-4018</doi>
      <bibkey>indig-etal-2019-one</bibkey>
      <pwccode url="https://github.com/dlt-rilmta/emtsv" additional="false">dlt-rilmta/emtsv</pwccode>
    </paper>
    <paper id="19">
      <title><fixed-case>T</fixed-case>urkish Treebanking: Unifying and Constructing Efforts</title>
      <author><first>Utku</first><last>Türk</last></author>
      <author><first>Furkan</first><last>Atmaca</last></author>
      <author><first>Şaziye Betül</first><last>Özateş</last></author>
      <author><first>Abdullatif</first><last>Köksal</last></author>
      <author><first>Balkiz</first><last>Ozturk Basaran</last></author>
      <author><first>Tunga</first><last>Gungor</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>166–177</pages>
      <abstract>In this paper, we present the current version of two different treebanks, the re-annotation of the Turkish PUD Treebank and the first annotation of the Turkish National Corpus Universal Dependency (henceforth TNC-UD). The annotation of both treebanks, the Turkish PUD Treebank and TNC-UD, was carried out based on the decisions concerning linguistic adequacy of re-annotation of the Turkish IMST-UD Treebank (Türk et. al., forthcoming). Both of the treebanks were annotated with the same annotation process and morphological and syntactic analyses. The TNC-UD is planned to have 10,000 sentences. In this paper, we will present the first 500 sentences along with the annotation PUD Treebank. Moreover, this paper also offers the parsing results of a graph-based neural parser on the previous and re-annotated PUD, as well as the TNC-UD. In light of the comparisons, even though we observe a slight decrease in the attachment scores of the Turkish PUD treebank, we demonstrate that the annotation of the TNC-UD improves the parsing accuracy of Turkish. In addition to the treebanks, we have also constructed a custom annotation software with advanced filtering and morphological editing options. Both the treebanks, including a full edit-history and the annotation guidelines, and the custom software are publicly available under an open license online.</abstract>
      <url hash="62eef070">W19-4019</url>
      <doi>10.18653/v1/W19-4019</doi>
      <bibkey>turk-etal-2019-turkish</bibkey>
    </paper>
    <paper id="20">
      <title>A Dataset for Semantic Role Labelling of <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Code-Mixed Tweets</title>
      <author><first>Riya</first><last>Pal</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>178–188</pages>
      <abstract>We present a data set of 1460 Hindi-English code-mixed tweets consisting of 20,949 tokens labelled with Proposition Bank labels marking their semantic roles. We created verb frames for complex predicates present in the corpus and formulated mappings from Paninian dependency labels to Proposition Bank labels. With the help of these mappings and the dependency tree, we propose a baseline rule based system for Semantic Role Labelling of Hindi-English code-mixed data. We obtain an accuracy of 96.74% for Argument Identification and are able to further classify 73.93% of the labels correctly. While there is relevant ongoing research on Semantic Role Labelling and on building tools for code-mixed social media data, this is the first attempt at labelling semantic roles in code-mixed data, to the best of our knowledge.</abstract>
      <url hash="d2e27358">W19-4020</url>
      <doi>10.18653/v1/W19-4020</doi>
      <bibkey>pal-sharma-2019-dataset</bibkey>
    </paper>
    <paper id="21">
      <title>A Multi-Platform Annotation Ecosystem for Domain Adaptation</title>
      <author><first>Richard</first><last>Eckart de Castilho</last></author>
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>Jin-Dong</first><last>Kim</last></author>
      <author><first>Jan-Christoph</first><last>Klie</last></author>
      <author><first>Keith</first><last>Suderman</last></author>
      <pages>189–194</pages>
      <abstract>This paper describes an ecosystem consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The platforms and the approach are in general domain-independent and can be readily applied to other areas of science.</abstract>
      <url hash="3a2edb0a">W19-4021</url>
      <doi>10.18653/v1/W19-4021</doi>
      <bibkey>eckart-de-castilho-etal-2019-multi</bibkey>
    </paper>
    <paper id="22">
      <title>A New Annotation Scheme for the <fixed-case>S</fixed-case>ejong Part-of-speech Tagged Corpus</title>
      <author><first>Jungyeul</first><last>Park</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>195–202</pages>
      <abstract>In this paper we present a new annotation scheme for the Sejong part-of-speech tagged corpus based on Universal Dependencies style annotation. By using a new annotation scheme, we can produce Sejong-style morphological analysis and part-of-speech tagging results which have been the <i>de facto</i> standard for Korean language processing. We also explore the possibility of doing named-entity recognition and semantic-role labelling for Korean using the new annotation scheme.</abstract>
      <url hash="22b57a4e">W19-4022</url>
      <doi>10.18653/v1/W19-4022</doi>
      <bibkey>park-tyers-2019-new</bibkey>
      <pwccode url="https://github.com/jungyeul/sjmorph" additional="false">jungyeul/sjmorph</pwccode>
    </paper>
    <paper id="23">
      <title>A <fixed-case>T</fixed-case>urkish Dataset for Gender Identification of <fixed-case>T</fixed-case>witter Users</title>
      <author><first>Erhan</first><last>Sezerer</last></author>
      <author><first>Ozan</first><last>Polatbilek</last></author>
      <author><first>Selma</first><last>Tekir</last></author>
      <pages>203–207</pages>
      <abstract>Author profiling is the identification of an author’s gender, age, and language from his/her texts. With the increasing trend of using Twitter as a means to express thought, profiling the gender of an author from his/her tweets has become a challenge. Although several datasets in different languages have been released on this problem, there is still a need for multilingualism. In this work, we propose a dataset of tweets of Turkish Twitter users which are labeled with their gender information. The dataset has 3368 users in training set and 1924 users in test set where each user has 100 tweets. The dataset is publicly available.</abstract>
      <url hash="052987cf">W19-4023</url>
      <doi>10.18653/v1/W19-4023</doi>
      <revision id="1" href="W19-4023v1" hash="57b07ed9"/>
      <revision id="2" href="W19-4023v2" hash="052987cf">Table references in the paper was wrong (shown as "in Table ??"). These references are corrected to show properly as "in Table 1".

One of the references was wrong therefore it is corrected to cite the appropriate paper. (the one with the title "Gender prediction from tweets with convolutional neural networks")</revision>
      <bibkey>sezerer-etal-2019-turkish</bibkey>
    </paper>
    <paper id="24">
      <title>Comparative judgments are more consistent than binary classification for labelling word complexity</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last></author>
      <author><first>Advait</first><last>Sarkar</last></author>
      <author><first>Alan</first><last>Blackwell</last></author>
      <pages>208–214</pages>
      <abstract>Lexical simplification systems replace complex words with simple ones based on a model of which words are complex in context. We explore how users can help train complex word identification models through labelling more efficiently and reliably. We show that using an interface where annotators make comparative rather than binary judgments leads to more reliable and consistent labels, and explore whether comparative judgments may provide a faster way for collecting labels.</abstract>
      <url hash="b50749a8">W19-4024</url>
      <doi>10.18653/v1/W19-4024</doi>
      <bibkey>gooding-etal-2019-comparative</bibkey>
    </paper>
    <paper id="25">
      <title>Continuous Quality Control and Advanced Text Segment Annotation with <fixed-case>WAT</fixed-case>-<fixed-case>SL</fixed-case> 2.0</title>
      <author><first>Christina</first><last>Lohr</last></author>
      <author><first>Johannes</first><last>Kiesel</last></author>
      <author><first>Stephanie</first><last>Luther</last></author>
      <author><first>Johannes</first><last>Hellrich</last></author>
      <author><first>Tobias</first><last>Kolditz</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>215–219</pages>
      <abstract>Today’s widely used annotation tools were designed for annotating typically short textual mentions of entities or relations, making their interface cumbersome to use for long(er) stretches of text, e.g, sentences running over several lines in a document. They also lack systematic support for hierarchically structured labels, i.e., one label being conceptually more general than another (e.g., anamnesis in relation to family anamnesis). Moreover, as a more fundamental shortcoming of today’s tools, they provide no continuous quality con trol mechanisms for the annotation process, an essential feature to intrinsically support iterative cycles in the development of annotation guidelines. We alleviated these problems by developing WAT-SL 2.0, an open-source web-based annotation tool for long-segment labeling, hierarchically structured label sets and built-ins for quality control.</abstract>
      <url hash="d84074ae">W19-4025</url>
      <doi>10.18653/v1/W19-4025</doi>
      <bibkey>lohr-etal-2019-continuous</bibkey>
      <pwccode url="https://github.com/webis-de/wat" additional="false">webis-de/wat</pwccode>
    </paper>
    <paper id="26">
      <title>Creation of a corpus with semantic role labels for <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Attila</first><last>Novák</last></author>
      <author><first>László</first><last>Laki</last></author>
      <author><first>Borbála</first><last>Novák</last></author>
      <author><first>Andrea</first><last>Dömötör</last></author>
      <author><first>Noémi</first><last>Ligeti-Nagy</last></author>
      <author><first>Ágnes</first><last>Kalivoda</last></author>
      <pages>220–229</pages>
      <abstract>In this article, an ongoing research is presented, the immediate goal of which is to create a corpus annotated with semantic role labels for Hungarian that can be used to train a parser-based system capable of formulating relevant questions about the text it processes. We briefly describe the objectives of our research, our efforts at eliminating errors in the Hungarian Universal Dependencies corpus, which we use as the base of our annotation effort, at creating a Hungarian verbal argument database annotated with thematic roles, at classifying adjuncts, and at matching verbal argument frames to specific occurrences of verbs and participles in the corpus.</abstract>
      <url hash="3f075312">W19-4026</url>
      <doi>10.18653/v1/W19-4026</doi>
      <bibkey>novak-etal-2019-creation</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="27">
      <title>Toward Dialogue Modeling: A Semantic Annotation Scheme for Questions and Answers</title>
      <author><first>María Andrea</first><last>Cruz Blandón</last></author>
      <author><first>Gosse</first><last>Minnema</last></author>
      <author><first>Aria</first><last>Nourbakhsh</last></author>
      <author><first>Maria</first><last>Boritchev</last></author>
      <author><first>Maxime</first><last>Amblard</last></author>
      <pages>230–235</pages>
      <abstract>The present study proposes an annotation scheme for classifying the content and discourse contribution of question-answer pairs. We propose detailed guidelines for using the scheme and apply them to dialogues in English, Spanish, and Dutch. Finally, we report on initial machine learning experiments for automatic annotation.</abstract>
      <url hash="0072772c">W19-4027</url>
      <doi>10.18653/v1/W19-4027</doi>
      <bibkey>cruz-blandon-etal-2019-toward</bibkey>
      <pwccode url="https://github.com/andrea08/question_answer_annotation" additional="false">andrea08/question_answer_annotation</pwccode>
    </paper>
    <paper id="28">
      <title>Towards a General <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation Corpus for <fixed-case>B</fixed-case>razilian <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
      <author><first>Thiago</first><last>Pardo</last></author>
      <pages>236–244</pages>
      <abstract>Abstract Meaning Representation (AMR) is a recent and prominent semantic representation with good acceptance and several applications in the Natural Language Processing area. For English, there is a large annotated corpus (with approximately 39K sentences) that supports the research with the representation. However, to the best of our knowledge, there is only one restricted corpus for Portuguese, which contains 1,527 sentences. In this context, this paper presents an effort to build a general purpose AMR-annotated corpus for Brazilian Portuguese by translating and adapting AMR English guidelines. Our results show that such approach is feasible, but there are some challenging phenomena to solve. More than this, efforts are necessary to increase the coverage of the corresponding lexical resource that supports the annotation.</abstract>
      <url hash="ed31d53e">W19-4028</url>
      <doi>10.18653/v1/W19-4028</doi>
      <bibkey>sobrevilla-cabezudo-pardo-2019-towards</bibkey>
    </paper>
  </volume>
  <volume id="41">
    <meta>
      <booktitle>Proceedings of the First Workshop on NLP for Conversational AI</booktitle>
      <url hash="d156f322">W19-41</url>
      <editor><first>Yun-Nung</first><last>Chen</last></editor>
      <editor><first>Tania</first><last>Bedrax-Weiss</last></editor>
      <editor><first>Dilek</first><last>Hakkani-Tur</last></editor>
      <editor><first>Anuj</first><last>Kumar</last></editor>
      <editor><first>Mike</first><last>Lewis</last></editor>
      <editor><first>Thang-Minh</first><last>Luong</last></editor>
      <editor><first>Pei-Hao</first><last>Su</last></editor>
      <editor><first>Tsung-Hsien</first><last>Wen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="35f915e3">W19-4100</url>
      <bibkey>ws-2019-nlp-conversational</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Repository of Conversational Datasets</title>
      <author><first>Matthew</first><last>Henderson</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Sam</first><last>Coope</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Girish</first><last>Kumar</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Georgios</first><last>Spithourakis</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <pages>1–10</pages>
      <abstract>Progress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.</abstract>
      <url hash="d86245e7">W19-4101</url>
      <doi>10.18653/v1/W19-4101</doi>
      <bibkey>henderson-etal-2019-repository</bibkey>
      <pwccode url="https://github.com/PolyAI-LDN/conversational-datasets" additional="true">PolyAI-LDN/conversational-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-corpus">Reddit Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="2">
      <title>A Simple but Effective Method to Incorporate Multi-turn Context with <fixed-case>BERT</fixed-case> for Conversational Machine Comprehension</title>
      <author><first>Yasuhito</first><last>Ohsugi</last></author>
      <author><first>Itsumi</first><last>Saito</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Hisako</first><last>Asano</last></author>
      <author><first>Junji</first><last>Tomita</last></author>
      <pages>11–17</pages>
      <abstract>Conversational machine comprehension (CMC) requires understanding the context of multi-turn dialogue. Using BERT, a pretraining language model, has been successful for single-turn machine comprehension, while modeling multiple turns of question answering with BERT has not been established because BERT has a limit on the number and the length of input sequences. In this paper, we propose a simple but effective method with BERT for CMC. Our method uses BERT to encode a paragraph independently conditioned with each question and each answer in a multi-turn context. Then, the method predicts an answer on the basis of the paragraph representations encoded with BERT. The experiments with representative CMC datasets, QuAC and CoQA, show that our method outperformed recently published methods (+0.8 F1 on QuAC and +2.1 F1 on CoQA). In addition, we conducted a detailed analysis of the effects of the number and types of dialogue history on the accuracy of CMC, and we found that the gold answer history, which may not be given in an actual conversation, contributed to the model performance most on both datasets.</abstract>
      <url hash="d1bd32fc">W19-4102</url>
      <doi>10.18653/v1/W19-4102</doi>
      <bibkey>ohsugi-etal-2019-simple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="3">
      <title>Augmenting Neural Response Generation with Context-Aware Topical Attention</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Ehsan</first><last>Kamalloo</last></author>
      <author><first>Kory</first><last>Mathewson</last></author>
      <author><first>Osmar</first><last>Zaiane</last></author>
      <pages>18–31</pages>
      <abstract>Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.</abstract>
      <url hash="dbd612e5">W19-4103</url>
      <doi>10.18653/v1/W19-4103</doi>
      <bibkey>dziri-etal-2019-augmenting</bibkey>
      <pwccode url="https://github.com/nouhadziri/THRED" additional="false">nouhadziri/THRED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-conversation-corpus">Reddit Conversation Corpus</pwcdataset>
    </paper>
    <paper id="4">
      <title>Building a Production Model for Retrieval-Based Chatbots</title>
      <author><first>Kyle</first><last>Swanson</last></author>
      <author><first>Lili</first><last>Yu</last></author>
      <author><first>Christopher</first><last>Fox</last></author>
      <author><first>Jeremy</first><last>Wohlwend</last></author>
      <author><first>Tao</first><last>Lei</last></author>
      <pages>32–41</pages>
      <abstract>Response suggestion is an important task for building human-computer conversation systems. Recent approaches to conversation modeling have introduced new model architectures with impressive results, but relatively little attention has been paid to whether these models would be practical in a production setting. In this paper, we describe the unique challenges of building a production retrieval-based conversation system, which selects outputs from a whitelist of candidate responses. To address these challenges, we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist. We also introduce and compare two methods for generating whitelists, and we carry out a comprehensive analysis of the model and whitelists. Experimental results on a large, proprietary help desk chat dataset, including both offline metrics and a human evaluation, indicate production-quality performance and illustrate key lessons about conversation modeling in practice.</abstract>
      <url hash="5343b0c4">W19-4104</url>
      <doi>10.18653/v1/W19-4104</doi>
      <revision id="1" href="W19-4104v1" hash="6851b93c"/>
      <revision id="2" href="W19-4104v2" hash="5343b0c4">Correctsone of the equations appearing in both the text and in one of the figures.</revision>
      <bibkey>swanson-etal-2019-building</bibkey>
    </paper>
    <paper id="5">
      <title>Co-Operation as an Asymmetric Form of Human-Computer Creativity. Case: Peace Machine</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Timo</first><last>Honkela</last></author>
      <pages>42–50</pages>
      <abstract>This theoretical paper identifies a need for a definition of asymmetric co-creativity where creativity is expected from the computational agent but not from the human user. Our co-operative creativity framework takes into account that the computational agent has a message to convey in a co-operative fashion, which introduces a trade-off on how creative the computer can be. The requirements of co-operation are identified from an interdisciplinary point of view. We divide co-operative creativity in message creativity, contextual creativity and communicative creativity. Finally these notions are applied in the context of the Peace Machine system concept.</abstract>
      <url hash="98a308d1">W19-4105</url>
      <doi>10.18653/v1/W19-4105</doi>
      <bibkey>hamalainen-honkela-2019-co</bibkey>
    </paper>
    <paper id="6">
      <title>Conversational Response Re-ranking Based on Event Causality and Role Factored Tensor Event Embedding</title>
      <author><first>Shohei</first><last>Tanaka</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>51–59</pages>
      <abstract>We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causality relations between events in a dialogue history and response candidates (e.g., “be stressed out” precedes “relieve stress”). We use distributed event representation based on the Role Factored Tensor Model for a robust matching of event causality relations due to limited event causality knowledge of the system. Experimental results showed that the proposed method improved coherency and dialogue continuity of system responses.</abstract>
      <url hash="19234efb">W19-4106</url>
      <doi>10.18653/v1/W19-4106</doi>
      <bibkey>tanaka-etal-2019-conversational</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="7">
      <title><fixed-case>DSTC</fixed-case>7 Task 1: Noetic End-to-End Response Selection</title>
      <author><first>Chulaka</first><last>Gunasekara</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <author><first>Lazaros</first><last>Polymenakos</last></author>
      <author><first>Walter</first><last>Lasecki</last></author>
      <pages>60–67</pages>
      <abstract>Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.</abstract>
      <url hash="bebc2a9b">W19-4107</url>
      <doi>10.18653/v1/W19-4107</doi>
      <bibkey>gunasekara-etal-2019-dstc7</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/advising-corpus">Advising Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dstc7-task-1">DSTC7 Task 1</pwcdataset>
    </paper>
    <paper id="8">
      <title>End-to-End Neural Context Reconstruction in <fixed-case>C</fixed-case>hinese Dialogue</title>
      <author><first>Wei</first><last>Yang</last></author>
      <author><first>Rui</first><last>Qiao</last></author>
      <author><first>Haocheng</first><last>Qin</last></author>
      <author><first>Amy</first><last>Sun</last></author>
      <author><first>Luchen</first><last>Tan</last></author>
      <author><first>Kun</first><last>Xiong</last></author>
      <author><first>Ming</first><last>Li</last></author>
      <pages>68–76</pages>
      <abstract>We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace pronouns, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context. Following a standard decomposition of the context reconstruction task into referring expression detection and coreference resolution, we propose a novel end-to-end architecture for separately and jointly accomplishing this task. Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism. One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data. The combination of more data and better models yields accuracy higher than the state-of-the-art method in coreference resolution and end-to-end context reconstruction.</abstract>
      <url hash="f4311fad">W19-4108</url>
      <doi>10.18653/v1/W19-4108</doi>
      <bibkey>yang-etal-2019-end-end-neural</bibkey>
    </paper>
    <paper id="9">
      <title>Energy-Based Modelling for Dialogue State Tracking</title>
      <author><first>Anh Duong</first><last>Trinh</last></author>
      <author><first>Robert</first><last>Ross</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <pages>77–86</pages>
      <abstract>The uncertainties of language and the complexity of dialogue contexts make accurate dialogue state tracking one of the more challenging aspects of dialogue processing. To improve state tracking quality, we argue that relationships between different aspects of dialogue state must be taken into account as they can often guide a more accurate interpretation process. To this end, we present an energy-based approach to dialogue state tracking as a structured classification task. The novelty of our approach lies in the use of an energy network on top of a deep learning architecture to explore more signal correlations between network variables including input features and output labels. We demonstrate that the energy-based approach improves the performance of a deep learning dialogue state tracker towards state-of-the-art results without the need for many of the other steps required by current state-of-the-art methods.</abstract>
      <url hash="680aab86">W19-4109</url>
      <doi>10.18653/v1/W19-4109</doi>
      <bibkey>trinh-etal-2019-energy</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogue-state-tracking-challenge">Dialogue State Tracking Challenge</pwcdataset>
    </paper>
    <paper id="10">
      <title>Evaluation and Improvement of Chatbot Text Classification Data Quality Using Plausible Negative Examples</title>
      <author><first>Kit</first><last>Kuksenok</last></author>
      <author><first>Andriy</first><last>Martyniv</last></author>
      <pages>87–95</pages>
      <abstract>We describe and validate a metric for estimating multi-class classifier performance based on cross-validation and adapted for improvement of small, unbalanced natural-language datasets used in chatbot design. Our experiences draw upon building recruitment chatbots that mediate communication between job-seekers and recruiters by exposing the ML/NLP dataset to the recruiting team. Evaluation approaches must be understandable to various stakeholders, and useful for improving chatbot performance. The metric, nex-cv, uses negative examples in the evaluation of text classification, and fulfils three requirements. First, it is actionable: it can be used by non-developer staff. Second, it is not overly optimistic compared to human ratings, making it a fast method for comparing classifiers. Third, it allows model-agnostic comparison, making it useful for comparing systems despite implementation differences. We validate the metric based on seven recruitment-domain datasets in English and German over the course of one year.</abstract>
      <url hash="39330d89">W19-4110</url>
      <doi>10.18653/v1/W19-4110</doi>
      <bibkey>kuksenok-martyniv-2019-evaluation</bibkey>
      <pwccode url="https://github.com/jobpal/nex-cv" additional="false">jobpal/nex-cv</pwccode>
    </paper>
    <paper id="11">
      <title>Improving Long Distance Slot Carryover in Spoken Dialogue Systems</title>
      <author><first>Tongfei</first><last>Chen</last></author>
      <author><first>Chetan</first><last>Naik</last></author>
      <author><first>Hua</first><last>He</last></author>
      <author><first>Pushpendre</first><last>Rastogi</last></author>
      <author><first>Lambert</first><last>Mathias</last></author>
      <pages>96–105</pages>
      <abstract>Tracking the state of the conversation is a central component in task-oriented spoken dialogue systems. One such approach for tracking the dialogue state is slot carryover, where a model makes a binary decision if a slot from the context is relevant to the current turn. Previous work on the slot carryover task used models that made independent decisions for each slot. A close analysis of the results show that this approach results in poor performance over longer context dialogues. In this paper, we propose to jointly model the slots. We propose two neural network architectures, one based on pointer networks that incorporate slot ordering information, and the other based on transformer networks that uses self attention mechanism to model the slot interdependencies. Our experiments on an internal dialogue benchmark dataset and on the public DSTC2 dataset demonstrate that our proposed models are able to resolve longer distance slot references and are able to achieve competitive performance.</abstract>
      <url hash="2cb98fe7">W19-4111</url>
      <doi>10.18653/v1/W19-4111</doi>
      <bibkey>chen-etal-2019-improving-long</bibkey>
    </paper>
    <paper id="12">
      <title>Insights from Building an Open-Ended Conversational Agent</title>
      <author><first>Khyatti</first><last>Gupta</last></author>
      <author><first>Meghana</first><last>Joshi</last></author>
      <author><first>Ankush</first><last>Chatterjee</last></author>
      <author><first>Sonam</first><last>Damani</last></author>
      <author><first>Kedhar Nath</first><last>Narahari</last></author>
      <author><first>Puneet</first><last>Agrawal</last></author>
      <pages>106–112</pages>
      <abstract>Dialogue systems and conversational agents are becoming increasingly popular in modern society. We conceptualized one such conversational agent, Microsoft’s “Ruuh” with the promise to be able to talk to its users on any subject they choose. Building an open-ended conversational agent like Ruuh at onset seems like a daunting task, since the agent needs to think beyond the utilitarian notion of merely generating “relevant” responses and meet a wider range of user social needs, like expressing happiness when user’s favourite sports team wins, sharing a cute comment on showing the pictures of the user’s pet and so on. The agent also needs to detect and respond to abusive language, sensitive topics and trolling behaviour of the users. Many of these problems pose significant research challenges as well as product design limitations as one needs to circumnavigate the technical limitations to create an acceptable user experience. However, as the product reaches the real users the true test begins, and one realizes the challenges and opportunities that lie in the vast domain of conversations. With over 2.5 million real-world users till date who have generated over 300 million user conversations with Ruuh, there is a plethora of learning, insights and opportunities that we will talk about in this paper.</abstract>
      <url hash="7f28748a">W19-4112</url>
      <doi>10.18653/v1/W19-4112</doi>
      <bibkey>gupta-etal-2019-insights</bibkey>
    </paper>
    <paper id="13">
      <title>Learning to Explain: Answering Why-Questions via Rephrasing</title>
      <author><first>Allen</first><last>Nie</last></author>
      <author><first>Erin</first><last>Bennett</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <pages>113–120</pages>
      <abstract>Providing plausible responses to why questions is a challenging but critical goal for language based human-machine interaction. Explanations are challenging in that they require many different forms of abstract knowledge and reasoning. Previous work has either relied on human-curated structured knowledge bases or detailed domain representation to generate satisfactory explanations. They are also often limited to ranking pre-existing explanation choices. In our work, we contribute to the under-explored area of generating natural language explanations for general phenomena. We automatically collect large datasets of explanation-phenomenon pairs which allow us to train sequence-to-sequence models to generate natural language explanations. We compare different training strategies and evaluate their performance using both automatic scores and human ratings. We demonstrate that our strategy is sufficient to generate highly plausible explanations for general open-domain phenomena compared to other models trained on different datasets.</abstract>
      <url hash="01f122c9">W19-4113</url>
      <doi>10.18653/v1/W19-4113</doi>
      <bibkey>nie-etal-2019-learning</bibkey>
      <pwccode url="https://github.com/windweller/L2EWeb" additional="false">windweller/L2EWeb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="14">
      <title>Multi-turn Dialogue Response Generation in an Adversarial Learning Framework</title>
      <author><first>Oluwatobi</first><last>Olabiyi</last></author>
      <author><first>Alan O</first><last>Salimov</last></author>
      <author><first>Anish</first><last>Khazane</last></author>
      <author><first>Erik</first><last>Mueller</last></author>
      <pages>121–132</pages>
      <abstract>We propose an adversarial learning approach for generating multi-turn dialogue responses. Our proposed framework, hredGAN, is based on conditional generative adversarial networks (GANs). The GAN’s generator is a modified hierarchical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bidirectional RNN that shares context and word embeddings with the generator. During inference, noise samples conditioned on the dialogue history are used to perturb the generator’s latent space to generate several possible responses. The final response is the one ranked best by the discriminator. The hredGAN shows improved performance over existing methods: (1) it generalizes better than networks trained using only the log-likelihood criterion, and (2) it generates longer, more informative and more diverse responses with high utterance and topic relevance even with limited training data. This performance improvement is demonstrated on the Movie triples and Ubuntu dialogue datasets with both the automatic and human evaluations.</abstract>
      <url hash="c9580e7c">W19-4114</url>
      <doi>10.18653/v1/W19-4114</doi>
      <bibkey>olabiyi-etal-2019-multi</bibkey>
    </paper>
    <paper id="15">
      <title>Relevant and Informative Response Generation using Pointwise Mutual Information</title>
      <author><first>Junya</first><last>Takayama</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>133–138</pages>
      <abstract>A sequence-to-sequence model tends to generate generic responses with little information for input utterances. To solve this problem, we propose a neural model that generates relevant and informative responses. Our model has simple architecture to enable easy application to existing neural dialogue models. Specifically, using positive pointwise mutual information, it first identifies keywords that frequently co-occur in responses given an utterance. Then, the model encourages the decoder to use the keywords for response generation. Experiment results demonstrate that our model successfully diversifies responses relative to previous models.</abstract>
      <url hash="d90d7944">W19-4115</url>
      <doi>10.18653/v1/W19-4115</doi>
      <bibkey>takayama-arase-2019-relevant</bibkey>
    </paper>
    <paper id="16">
      <title>Responsive and Self-Expressive Dialogue Generation</title>
      <author><first>Kozo</first><last>Chikai</last></author>
      <author><first>Junya</first><last>Takayama</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>139–149</pages>
      <abstract>A neural conversation model is a promising approach to develop dialogue systems with the ability of chit-chat. It allows training a model in an end-to-end manner without complex rule design nor feature engineering. However, as a side effect, the neural model tends to generate safe but uninformative and insensitive responses like “OK” and “I don’t know.” Such replies are called generic responses and regarded as a critical problem for user-engagement of dialogue systems. For a more engaging chit-chat experience, we propose a neural conversation model that generates responsive and self-expressive replies. Specifically, our model generates domain-aware and sentiment-rich responses. Experiments empirically confirmed that our model outperformed the sequence-to-sequence model; 68.1% of our responses were domain-aware with sentiment polarities, which was only 2.7% for responses generated by the sequence-to-sequence model.</abstract>
      <url hash="78254e11">W19-4116</url>
      <doi>10.18653/v1/W19-4116</doi>
      <bibkey>chikai-etal-2019-responsive</bibkey>
      <pwccode url="https://github.com/KChikai/Responsive-Dialogue-Generation" additional="false">KChikai/Responsive-Dialogue-Generation</pwccode>
    </paper>
  </volume>
  <volume id="42">
    <meta>
      <booktitle>Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
      <url hash="cb8e012d">W19-42</url>
      <editor><first>Garrett</first><last>Nicolai</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>acl</venue>
    </meta>
    <frontmatter>
      <url hash="1a1c6377">W19-4200</url>
      <bibkey>ws-2019-research</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>AX</fixed-case> Semantics’ Submission to the <fixed-case>SIGMORPHON</fixed-case> 2019 Shared Task</title>
      <author><first>Andreas</first><last>Madsack</last></author>
      <author><first>Robert</first><last>Weißgraeber</last></author>
      <pages>1–5</pages>
      <abstract>This paper describes the AX Semantics’ submission to the SIGMORPHON 2019 shared task on morphological reinflection. We implemented two systems, both tackling the task for all languages in one codebase, without any underlying language specific features. The first one is an encoder-decoder model using AllenNLP; the second system uses the same model modified by a custom trainer that trains only with the target language resources after a specific threshold. We especially focused on building an implementation using AllenNLP with out-of-the-box methods to facilitate easy operation and reuse.</abstract>
      <url hash="a40589d0">W19-4201</url>
      <doi>10.18653/v1/W19-4201</doi>
      <bibkey>madsack-weissgraeber-2019-ax</bibkey>
    </paper>
    <paper id="2">
      <title>Cognate Projection for Low-Resource Inflection Generation</title>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Amir Ahmad</first><last>Habibi</last></author>
      <author><first>Yixing</first><last>Luan</last></author>
      <author><first>Rashed Rubby</first><last>Riyadh</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>6–11</pages>
      <abstract>We propose cognate projection as a method of crosslingual transfer for inflection generation in the context of the SIGMORPHON 2019 Shared Task. The results on four language pairs show the method is effective when no low-resource training data is available.</abstract>
      <url hash="8b714f93">W19-4202</url>
      <doi>10.18653/v1/W19-4202</doi>
      <bibkey>hauer-etal-2019-cognate</bibkey>
    </paper>
    <paper id="3">
      <title>Cross-Lingual Lemmatization and Morphology Tagging with Two-Stage Multilingual <fixed-case>BERT</fixed-case> Fine-Tuning</title>
      <author><first>Dan</first><last>Kondratyuk</last></author>
      <pages>12–18</pages>
      <abstract>We present our CHARLES-SAARLAND system for the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology, in task 2, Morphological Analysis and Lemmatization in Context. We leverage the multilingual BERT model and apply several fine-tuning strategies introduced by UDify demonstrating exceptional evaluation performance on morpho-syntactic tasks. Our results show that fine-tuning multilingual BERT on the concatenation of all available treebanks allows the model to learn cross-lingual information that is able to boost lemmatization and morphology tagging accuracy over fine-tuning it purely monolingually. Unlike UDify, however, we show that when paired with additional character-level and word-level LSTM layers, a second stage of fine-tuning on each treebank individually can improve evaluation even further. Out of all submissions for this shared task, our system achieves the highest average accuracy and f1 score in morphology tagging and places second in average lemmatization accuracy.</abstract>
      <url hash="4a150149">W19-4203</url>
      <doi>10.18653/v1/W19-4203</doi>
      <bibkey>kondratyuk-2019-cross</bibkey>
      <pwccode url="https://github.com/hyperparticle/udify" additional="false">hyperparticle/udify</pwccode>
    </paper>
    <paper id="4">
      <title><fixed-case>CBNU</fixed-case> System for <fixed-case>SIGMORPHON</fixed-case> 2019 Shared Task 2: a Pipeline Model</title>
      <author><first>Uygun</first><last>Shadikhodjaev</last></author>
      <author><first>Jae Sung</first><last>Lee</last></author>
      <pages>19–24</pages>
      <abstract>In this paper we describe our system for morphological analysis and lemmatization in context, using a transformer-based sequence to sequence model and a biaffine attention based BiLSTM model. First, a lemma is produced for a given word, and then both the lemma and the given word are used for morphological analysis. We also make use of character level word encodings and trainable encodings to improve accuracy. Overall, our system ranked fifth in lemmatization and sixth in morphological accuracy among twelve systems, and demonstrated considerable improvements over the baseline in morphological analysis.</abstract>
      <url hash="889110d2">W19-4204</url>
      <doi>10.18653/v1/W19-4204</doi>
      <bibkey>shadikhodjaev-lee-2019-cbnu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>M</fixed-case>orpheus: A Neural Network for Jointly Learning Contextual Lemmatization and Morphological Tagging</title>
      <author><first>Eray</first><last>Yildiz</last></author>
      <author><first>A. Cüneyd</first><last>Tantuğ</last></author>
      <pages>25–34</pages>
      <abstract>In this study, we present Morpheus, a joint contextual lemmatizer and morphological tagger. Morpheus is based on a neural sequential architecture where inputs are the characters of the surface words in a sentence and the outputs are the minimum edit operations between surface words and their lemmata as well as the morphological tags assigned to the words. The experiments on the datasets in nearly 100 languages provided by SigMorphon 2019 Shared Task 2 organizers show that the performance of Morpheus is comparable to the state-of-the-art system in terms of lemmatization. In morphological tagging, on the other hand, Morpheus significantly outperforms the SigMorphon baseline. In our experiments, we also show that the neural encoder-decoder architecture trained to predict the minimum edit operations can produce considerably better results than the architecture trained to predict the characters in lemmata directly as in previous studies. According to the SigMorphon 2019 Shared Task 2 results, Morpheus has placed 3rd in lemmatization and reached the 9th place in morphological tagging among all participant teams.</abstract>
      <url hash="535fcf09">W19-4205</url>
      <doi>10.18653/v1/W19-4205</doi>
      <bibkey>yildiz-tantug-2019-morpheus</bibkey>
      <pwccode url="https://github.com/erayyildiz/Morpheus" additional="false">erayyildiz/Morpheus</pwccode>
    </paper>
    <paper id="6">
      <title>Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>35–49</pages>
      <abstract>This paper describes our submission to SIGMORPHON 2019 Task 2: Morphological analysis and lemmatization in context. Our model is a multi-task sequence to sequence neural network, which jointly learns morphological tagging and lemmatization. On the encoding side, we exploit character-level as well as contextual information. We introduce a multi-attention decoder to selectively focus on different parts of character and word sequences. To further improve the model, we train on multiple datasets simultaneously and use external embeddings for initialization. Our final model reaches an average morphological tagging F1 score of 94.54 and a lemma accuracy of 93.91 on the test data, ranking respectively 3rd and 6th out of 13 teams in the SIGMORPHON 2019 shared task.</abstract>
      <url hash="696fe9f9">W19-4206</url>
      <doi>10.18653/v1/W19-4206</doi>
      <bibkey>ustun-etal-2019-multi</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>IT</fixed-case>–<fixed-case>IST</fixed-case> at the <fixed-case>SIGMORPHON</fixed-case> 2019 Shared Task: Sparse Two-headed Models for Inflection</title>
      <author><first>Ben</first><last>Peters</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>50–56</pages>
      <abstract>This paper presents the Instituto de Telecomunicações–Instituto Superior Técnico submission to Task 1 of the SIGMORPHON 2019 Shared Task. Our models combine sparse sequence-to-sequence models with a two-headed attention mechanism that learns separate attention distributions for the lemma and inflectional tags. Among submissions to Task 1, our models rank second and third. Despite the low data setting of the task (only 100 in-language training examples), they learn plausible inflection patterns and often concentrate all probability mass into a small set of hypotheses, making beam search exact.</abstract>
      <url hash="b6b76c15">W19-4207</url>
      <doi>10.18653/v1/W19-4207</doi>
      <bibkey>peters-martins-2019-ist</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>CMU</fixed-case>-01 at the <fixed-case>SIGMORPHON</fixed-case> 2019 Shared Task on Crosslinguality and Context in Morphology</title>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Gayatri</first><last>Bhat</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Jaime</first><last>Carbonell</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>57–70</pages>
      <abstract>This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and Lemmatization in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 treebanks. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, Case, etc.) independently. However, most treebanks are under-resourced, thus making it challenging to train deep neural models for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology.</abstract>
      <url hash="ea83cb85">W19-4208</url>
      <doi>10.18653/v1/W19-4208</doi>
      <bibkey>chaudhary-etal-2019-cmu</bibkey>
      <pwccode url="https://github.com/Aditi138/MorphologicalAnalysis" additional="false">Aditi138/MorphologicalAnalysis</pwccode>
    </paper>
    <paper id="9">
      <title>Cross-lingual morphological inflection with explicit alignment</title>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>71–79</pages>
      <abstract>This paper describes two related systems for cross-lingual morphological inflection for SIGMORPHON 2019 Shared Task participation. Both sets of results submitted to the shared task for evaluation are obtained using a simple approach of predicting transducer actions based on initial alignments on the training set, where cross-lingual transfer is limited to only using the high-resource language data as additional training set. The performance of the system does not reach the performance of the top two systems in the competition. However, we show that results can be improved with further tuning. We also present further analyses demonstrating that the cross-lingual gain is rather modest.</abstract>
      <url hash="64a6eeda">W19-4209</url>
      <doi>10.18653/v1/W19-4209</doi>
      <bibkey>coltekin-2019-cross</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>THOMAS</fixed-case>: The Hegemonic <fixed-case>OSU</fixed-case> Morphological Analyzer using Seq2seq</title>
      <author><first>Byung-Doh</first><last>Oh</last></author>
      <author><first>Pranav</first><last>Maneriker</last></author>
      <author><first>Nanjiang</first><last>Jiang</last></author>
      <pages>80–86</pages>
      <abstract>This paper describes the OSU submission to the SIGMORPHON 2019 shared task, Crosslinguality and Context in Morphology. Our system addresses the <i>contextual morphological analysis</i> subtask of Task 2, which is to produce the morphosyntactic description (MSD) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of MSD tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer MSD tag sequences. In addition, our system seems to capture the structured correlation between MSD tags, such as that between the “verb” tag and TAM-related tags.</abstract>
      <url hash="fca606f2">W19-4210</url>
      <doi>10.18653/v1/W19-4210</doi>
      <bibkey>oh-etal-2019-thomas</bibkey>
    </paper>
    <paper id="11">
      <title>Sigmorphon 2019 Task 2 system description paper: Morphological analysis in context for many languages, with supervision from only a few</title>
      <author><first>Brad</first><last>Aiken</last></author>
      <author><first>Jared</first><last>Kelly</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <author><first>Suleyman Olcay</first><last>Polat</last></author>
      <author><first>Taraka</first><last>Rama</last></author>
      <author><first>Rodney</first><last>Nielsen</last></author>
      <pages>87–94</pages>
      <abstract>This paper presents the UNT HiLT+Ling system for the Sigmorphon 2019 shared Task 2: Morphological Analysis and Lemmatization in Context. Our core approach focuses on the morphological tagging task; part-of-speech tagging and lemmatization are treated as secondary tasks. Given the highly multilingual nature of the task, we propose an approach which makes minimal use of the supplied training data, in order to be extensible to languages without labeled training data for the morphological inflection task. Specifically, we use a parallel Bible corpus to align contextual embeddings at the verse level. The aligned verses are used to build cross-language translation matrices, which in turn are used to map between embedding spaces for the various languages. Finally, we use sets of inflected forms, primarily from a high-resource language, to induce vector representations for individual UniMorph tags. Morphological analysis is performed by matching vector representations to embeddings for individual tokens. While our system results are dramatically below the average system submitted for the shared task evaluation campaign, our method is (we suspect) unique in its minimal reliance on labeled training data.</abstract>
      <url hash="8cb396ed">W19-4211</url>
      <doi>10.18653/v1/W19-4211</doi>
      <bibkey>aiken-etal-2019-sigmorphon</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>UDP</fixed-case>ipe at <fixed-case>SIGMORPHON</fixed-case> 2019: Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging</title>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <author><first>Jan</first><last>Hajic</last></author>
      <pages>95–103</pages>
      <abstract>We present our contribution to the SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization. We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation. As our first improvement, we use the pretrained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as regularization; and finally, we merge the selected corpora of the same language. In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46). In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system’s 93.23.</abstract>
      <url hash="30278e18">W19-4212</url>
      <doi>10.18653/v1/W19-4212</doi>
      <bibkey>straka-etal-2019-udpipe</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>CUNI</fixed-case>–<fixed-case>M</fixed-case>alta system at <fixed-case>SIGMORPHON</fixed-case> 2019 Shared Task on Morphological Analysis and Lemmatization in context: Operation-based word formation</title>
      <author><first>Ronald</first><last>Cardenas</last></author>
      <author><first>Claudia</first><last>Borg</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>104–112</pages>
      <abstract>This paper presents the submission by the Charles University-University of Malta team to the SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context. We present a lemmatization model based on previous work on neural transducers (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2016). The key difference is that our model transforms the whole word form in every step, instead of consuming it character by character. We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations. The resulting operations not only encode the actions to be performed but the relative position in the word token and how characters need to be transformed. Our morphological tagger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hierarchical manner. Even though relative performance according to metrics is below the baseline, experiments show that our models capture important associations between interpretable operation labels and fine-grained morpho-syntax labels.</abstract>
      <url hash="8461346c">W19-4213</url>
      <doi>10.18653/v1/W19-4213</doi>
      <bibkey>cardenas-etal-2019-cuni</bibkey>
    </paper>
    <paper id="14">
      <title>A Little Linguistics Goes a Long Way: Unsupervised Segmentation with Limited Language Specific Guidance</title>
      <author><first>Alexander</first><last>Erdmann</last></author>
      <author><first>Salam</first><last>Khalifa</last></author>
      <author><first>Mai</first><last>Oudah</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <pages>113–124</pages>
      <abstract>We present de-lexical segmentation, a linguistically motivated alternative to greedy or other unsupervised methods, requiring only minimal language specific input. Our technique involves creating a small grammar of closed-class affixes which can be written in a few hours. The grammar over generates analyses for word forms attested in a raw corpus which are disambiguated based on features of the linguistic base proposed for each form. Extending the grammar to cover orthographic, morpho-syntactic or lexical variation is simple, making it an ideal solution for challenging corpora with noisy, dialect-inconsistent, or otherwise non-standard content. In two evaluations, we consistently outperform competitive unsupervised baselines and approach the performance of state-of-the-art supervised models trained on large amounts of data, providing evidence for the value of linguistic input during preprocessing.</abstract>
      <url hash="dc5ab69e">W19-4214</url>
      <doi>10.18653/v1/W19-4214</doi>
      <bibkey>erdmann-etal-2019-little</bibkey>
    </paper>
    <paper id="15">
      <title>Equiprobable mappings in weighted constraint grammars</title>
      <author><first>Arto</first><last>Anttila</last></author>
      <author><first>Scott</first><last>Borgeson</last></author>
      <author><first>Giorgio</first><last>Magri</last></author>
      <pages>125–134</pages>
      <abstract>We show that MaxEnt is so rich that it can distinguish between any two different mappings: there always exists a nonnegative weight vector which assigns them different MaxEnt probabilities. Stochastic HG instead does admit equiprobable mappings and we give a complete formal characterization of them.</abstract>
      <url hash="1f890be0">W19-4215</url>
      <doi>10.18653/v1/W19-4215</doi>
      <bibkey>anttila-etal-2019-equiprobable</bibkey>
    </paper>
    <paper id="16">
      <title>Unbounded Stress in Subregular Phonology</title>
      <author><first>Yiding</first><last>Hao</last></author>
      <author><first>Samuel</first><last>Andersson</last></author>
      <pages>135–143</pages>
      <abstract>This paper situates culminative unbounded stress systems within the subregular hierarchy for functions. While Baek (2018) has argued that such systems can be uniformly understood as input tier-based strictly local constraints, we show here that default-to-opposite-side and default-to-same-side stress systems belong to distinct subregular classes when they are viewed as functions that assign primary stress to underlying forms. While the former system can be captured by input tier-based input strictly local functions, a subsequential function class that we define here, the latter system is not subsequential, though it is weakly deterministic according to McCollum et al.’s (2018) non-interaction criterion. Our results motivate the extension of recently proposed subregular language classes to subregular functions and argue in favor of McCollum et al’s definition of weak determinism over that of Heinz and Lai (2013).</abstract>
      <url hash="d2ed2103">W19-4216</url>
      <doi>10.18653/v1/W19-4216</doi>
      <bibkey>hao-andersson-2019-unbounded</bibkey>
    </paper>
    <paper id="17">
      <title>Data mining <fixed-case>M</fixed-case>andarin tone contour shapes</title>
      <author><first>Shuo</first><last>Zhang</last></author>
      <pages>144–153</pages>
      <abstract>In spontaneous speech, Mandarin tones that belong to the same tone category may exhibit many different contour shapes. We explore the use of time-series data mining techniques for understanding the variability of tones in a large corpus of Mandarin newscast speech. First, we adapt a graph-based approach to characterize the clusters (fuzzy types) of tone contour shapes observed in each tone n-gram category. Second, we show correlations between these realized contour shape clusters and a bag of automatically extracted linguistic features. We discuss the implications of the current study within the context of phonological and information theory.</abstract>
      <url hash="6918a667">W19-4217</url>
      <doi>10.18653/v1/W19-4217</doi>
      <bibkey>zhang-2019-data</bibkey>
    </paper>
    <paper id="18">
      <title>Convolutional neural networks for low-resource morpheme segmentation: baseline or state-of-the-art?</title>
      <author><first>Alexey</first><last>Sorokin</last></author>
      <pages>154–159</pages>
      <abstract>We apply convolutional neural networks to the task of shallow morpheme segmentation using low-resource datasets for 5 different languages. We show that both in fully supervised and semi-supervised settings our model beats previous state-of-the-art approaches. We argue that convolutional neural networks reflect local nature of morpheme segmentation better than other semi-supervised approaches.</abstract>
      <url hash="bcfb53a1">W19-4218</url>
      <doi>10.18653/v1/W19-4218</doi>
      <bibkey>sorokin-2019-convolutional</bibkey>
    </paper>
    <paper id="19">
      <title>What do phone embeddings learn about Phonology?</title>
      <author><first>Sudheer</first><last>Kolachina</last></author>
      <author><first>Lilla</first><last>Magyar</last></author>
      <pages>160–169</pages>
      <abstract>Recent work has looked at evaluation of phone embeddings using sound analogies and correlations between distinctive feature space and embedding space. It has not been clear what aspects of natural language phonology are learnt by neural network inspired distributed representational models such as word2vec. To study the kinds of phonological relationships learnt by phone embeddings, we present artificial phonology experiments that show that phone embeddings learn paradigmatic relationships such as phonemic and allophonic distribution quite well. They are also able to capture co-occurrence restrictions among vowels such as those observed in languages with vowel harmony. However, they are unable to learn co-occurrence restrictions among the class of consonants.</abstract>
      <url hash="77bf9fbb">W19-4219</url>
      <doi>10.18653/v1/W19-4219</doi>
      <bibkey>kolachina-magyar-2019-phone</bibkey>
    </paper>
    <paper id="20">
      <title>Inverting and Modeling Morphological Inflection</title>
      <author><first>Yohei</first><last>Oseki</last></author>
      <author><first>Yasutada</first><last>Sudo</last></author>
      <author><first>Hiromu</first><last>Sakai</last></author>
      <author><first>Alec</first><last>Marantz</last></author>
      <pages>170–177</pages>
      <abstract>Previous “wug” tests (Berko, 1958) on Japanese verbal inflection have demonstrated that Japanese speakers, both adults and children, cannot inflect novel present tense forms to “correct” past tense forms predicted by rules of existent verbs (de Chene, 1982; Vance, 1987, 1991; Klafehn, 2003, 2013), indicating that Japanese verbs are merely stored in the mental lexicon. However, the implicit assumption that present tense forms are bases for verbal inflection should not be blindly extended to morphologically rich languages like Japanese in which both present and past tense forms are morphologically complex without inherent direction (Albright, 2002). Interestingly, there are also independent observations in the acquisition literature to suggest that past tense forms may be bases for verbal inflection in Japanese (Klafehn, 2003; Murasugi et al., 2010; Hirose, 2017; Tatsumi et al., 2018). In this paper, we computationally simulate two directions of verbal inflection in Japanese, Present → Past and Past → Present, with the rule-based computational model called Minimal Generalization Learner (MGL; Albright and Hayes, 2003) and experimentally evaluate the model with the bidirectional “wug” test where humans inflect novel verbs in two opposite directions. We conclude that Japanese verbs can be computed online via some generalizations and those generalizations do depend on the direction of morphological inflection.</abstract>
      <url hash="6620a449">W19-4220</url>
      <doi>10.18653/v1/W19-4220</doi>
      <bibkey>oseki-etal-2019-inverting</bibkey>
    </paper>
    <paper id="21">
      <title>Augmenting a <fixed-case>G</fixed-case>erman Morphological Database by Data-Intense Methods</title>
      <author><first>Petra</first><last>Steiner</last></author>
      <pages>178–188</pages>
      <abstract>This paper deals with the automatic enhancement of a new German morphological database. While there are some databases for flat word segmentation, this is the first available resource which can be directly used for deep parsing of German words. We combine the entries of this morphological database with the morphological tools SMOR and Moremorph and a context-based evaluation method which builds on a large Wikipedia corpus. We describe the state of the art and the essential characteristics of the database and the context method. The approach is tested on an inflight magazine of Lufthansa. We derive over 5,000 new instances of complex words. The coverage for the lemma types reaches up to over 99 percent. The precision of new found complex splits and monomorphemes is between 0.93 and 0.99.</abstract>
      <url hash="38ed24ac">W19-4221</url>
      <doi>10.18653/v1/W19-4221</doi>
      <bibkey>steiner-2019-augmenting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="22">
      <title>Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages</title>
      <author><first>Ramy</first><last>Eskander</last></author>
      <author><first>Judith</first><last>Klavans</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>189–195</pages>
      <abstract>Polysynthetic languages pose a challenge for morphological analysis due to the root-morpheme complexity and to the word class “squish”. In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages.</abstract>
      <url hash="0e5aec34">W19-4222</url>
      <doi>10.18653/v1/W19-4222</doi>
      <bibkey>eskander-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="23">
      <title>Weakly deterministic transformations are subregular</title>
      <author><first>Andrew</first><last>Lamont</last></author>
      <author><first>Charlie</first><last>O’Hara</last></author>
      <author><first>Caitlin</first><last>Smith</last></author>
      <pages>196–205</pages>
      <abstract>Whether phonological transformations in general are subregular is an open question. This is the case for most transformations, which have been shown to be subsequential, but it is not known whether weakly deterministic mappings form a proper subset of the regular functions. This paper demonstrates that there are regular functions that are not weakly deterministic, and, because all attested processes are weakly deterministic, supports the subregular hypothesis.</abstract>
      <url hash="9fd3433d">W19-4223</url>
      <doi>10.18653/v1/W19-4223</doi>
      <bibkey>lamont-etal-2019-weakly</bibkey>
    </paper>
    <paper id="24">
      <title>Encoder-decoder models for latent phonological representations of words</title>
      <author><first>Cassandra L.</first><last>Jacobs</last></author>
      <author><first>Fred</first><last>Mailhot</last></author>
      <pages>206–217</pages>
      <abstract>We use sequence-to-sequence networks trained on sequential phonetic encoding tasks to construct compositional phonological representations of words. We show that the output of an encoder network can predict the phonetic durations of American English words better than a number of alternative forms. We also show that the model’s learned representations map onto existing measures of words’ phonological structure (phonological neighborhood density and phonotactic probability).</abstract>
      <url hash="7ef2c2e6">W19-4224</url>
      <doi>10.18653/v1/W19-4224</doi>
      <bibkey>jacobs-mailhot-2019-encoder</bibkey>
    </paper>
    <paper id="25">
      <title>Action-Sensitive Phonological Dependencies</title>
      <author><first>Yiding</first><last>Hao</last></author>
      <author><first>Dustin</first><last>Bowers</last></author>
      <pages>218–228</pages>
      <abstract>This paper defines a subregular class of functions called the tier-based synchronized strictly local (TSSL) functions. These functions are similar to the the tier-based input-output strictly local (TIOSL) functions, except that the locality condition is enforced not on the input and output streams, but on the computation history of the minimal subsequential finite-state transducer. We show that TSSL functions naturally describe rhythmic syncope while TIOSL functions cannot, and we argue that TSSL functions provide a more restricted characterization of rhythmic syncope than existing treatments within Optimality Theory.</abstract>
      <url hash="e100874b">W19-4225</url>
      <doi>10.18653/v1/W19-4225</doi>
      <bibkey>hao-bowers-2019-action</bibkey>
    </paper>
    <paper id="26">
      <title>The <fixed-case>SIGMORPHON</fixed-case> 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection</title>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Chaitanya</first><last>Malaviya</last></author>
      <author><first>Lawrence</first><last>Wolf-Sonkin</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Sabrina J.</first><last>Mielke</last></author>
      <author><first>Jeffrey</first><last>Heinz</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>229–244</pages>
      <abstract>The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years’ inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year’s strong baselines or highly ranked systems from previous years’ shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.</abstract>
      <url hash="8ee99498">W19-4226</url>
      <doi>10.18653/v1/W19-4226</doi>
      <revision id="1" href="W19-4226v1" hash="6a2c2480"/>
      <revision id="2" href="W19-4226v2" hash="0c6e9010">Added an omitted author.</revision>
      <revision id="3" href="W19-4226v3" hash="8ee99498" date="2020-09-01">Name change for one of the authors.</revision>
      <bibkey>mccarthy-etal-2019-sigmorphon</bibkey>
    </paper>
  </volume>
  <volume id="43">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</booktitle>
      <url hash="9896dde8">W19-43</url>
      <editor><first>Isabelle</first><last>Augenstein</last></editor>
      <editor><first>Spandana</first><last>Gella</last></editor>
      <editor><first>Sebastian</first><last>Ruder</last></editor>
      <editor><first>Katharina</first><last>Kann</last></editor>
      <editor><first>Burcu</first><last>Can</last></editor>
      <editor><first>Johannes</first><last>Welbl</last></editor>
      <editor><first>Alexis</first><last>Conneau</last></editor>
      <editor><first>Xiang</first><last>Ren</last></editor>
      <editor><first>Marek</first><last>Rei</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>repl4nlp</venue>
    </meta>
    <frontmatter>
      <url hash="21a0a232">W19-4300</url>
      <bibkey>ws-2019-representation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Deep Generalized Canonical Correlation Analysis</title>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Huda</first><last>Khayrallah</last></author>
      <author><first>Biman</first><last>Gujral</last></author>
      <author><first>Dee Ann</first><last>Reisinger</last></author>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Raman</first><last>Arora</last></author>
      <pages>1–6</pages>
      <abstract>We present Deep Generalized Canonical Correlation Analysis (DGCCA) – a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn and evaluate DGCCA representations for three downstream tasks: phonetic transcription from acoustic &amp; articulatory measurements, recommending hashtags and recommending friends on a dataset of Twitter users.</abstract>
      <url hash="06d2dcd9">W19-4301</url>
      <doi>10.18653/v1/W19-4301</doi>
      <bibkey>benton-etal-2019-deep</bibkey>
      <pwccode url="https://bitbucket.org/adrianbenton/dgcca-py3" additional="true">adrianbenton/dgcca-py3</pwccode>
    </paper>
    <paper id="2">
      <title>To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</title>
      <author><first>Matthew E.</first><last>Peters</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>7–14</pages>
      <abstract>While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.</abstract>
      <url hash="fcff1713">W19-4302</url>
      <doi>10.18653/v1/W19-4302</doi>
      <bibkey>peters-etal-2019-tune</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="3">
      <title>Generative Adversarial Networks for Text Using Word2vec Intermediaries</title>
      <author><first>Akshay</first><last>Budhkar</last></author>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author><first>Safwan</first><last>Hossain</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>15–26</pages>
      <abstract>Generative adversarial networks (GANs) have shown considerable success, especially in the realistic generation of images. In this work, we apply similar techniques for the generation of text. We propose a novel approach to handle the discrete nature of text, during training, using word embeddings. Our method is agnostic to vocabulary size and achieves competitive results relative to methods with various discrete gradient estimators.</abstract>
      <url hash="500d1c3c">W19-4303</url>
      <doi>10.18653/v1/W19-4303</doi>
      <bibkey>budhkar-etal-2019-generative</bibkey>
    </paper>
    <paper id="4">
      <title>An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation</title>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>27–32</pages>
      <abstract>In this paper, we explore a multilingual translation model with a cross-lingually shared layer that can be used as fixed-size sentence representation in different downstream tasks. We systematically study the impact of the size of the shared layer and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that the performance in translation does correlate with trainable downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas non-trainable benchmarks can be confused by other types of information also encoded in the representation of a sentence.</abstract>
      <url hash="87066e12">W19-4304</url>
      <doi>10.18653/v1/W19-4304</doi>
      <bibkey>raganato-etal-2019-evaluation</bibkey>
    </paper>
    <paper id="5">
      <title>Multilingual <fixed-case>NMT</fixed-case> with a Language-Independent Attention Bridge</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <pages>33–39</pages>
      <abstract>In this paper, we propose an architecture for machine translation (MT) capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages. We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side. The attention bridge exploits the semantics from each language for translation and develops into a language-agnostic meaning representation that can efficiently be used for transfer learning. We present a new framework for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training. We have tested the approach in a systematic way with a multi-parallel data set. The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning.</abstract>
      <url hash="c2fbeac1">W19-4305</url>
      <doi>10.18653/v1/W19-4305</doi>
      <bibkey>vazquez-etal-2019-multilingual</bibkey>
    </paper>
    <paper id="6">
      <title>Efficient Language Modeling with Automatic Relevance Determination in Recurrent Neural Networks</title>
      <author><first>Maxim</first><last>Kodryan</last></author>
      <author><first>Artem</first><last>Grachev</last></author>
      <author><first>Dmitry</first><last>Ignatov</last></author>
      <author><first>Dmitry</first><last>Vetrov</last></author>
      <pages>40–48</pages>
      <abstract>Reduction of the number of parameters is one of the most important goals in Deep Learning. In this article we propose an adaptation of Doubly Stochastic Variational Inference for Automatic Relevance Determination (DSVI-ARD) for neural networks compression. We find this method to be especially useful in language modeling tasks, where large number of parameters in the input and output layers is often excessive. We also show that DSVI-ARD can be applied together with encoder-decoder weight tying allowing to achieve even better sparsity and performance. Our experiments demonstrate that more than 90% of the weights in both encoder and decoder layers can be removed with a minimal quality loss.</abstract>
      <url hash="e0da606d">W19-4306</url>
      <doi>10.18653/v1/W19-4306</doi>
      <bibkey>kodryan-etal-2019-efficient</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>M</fixed-case>o<fixed-case>RT</fixed-case>y: Unsupervised Learning of Task-specialized Word Embeddings by Autoencoding</title>
      <author><first>Nils</first><last>Rethmeier</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>49–54</pages>
      <abstract>Word embeddings have undoubtedly revolutionized NLP. However, pretrained embeddings do not always work for a specific task (or set of tasks), particularly in limited resource setups. We introduce a simple yet effective, self-supervised post-processing method that constructs task-specialized word representations by picking from a menu of reconstructing transformations to yield improved end-task performance (MORTY). The method is complementary to recent state-of-the-art approaches to inductive transfer via fine-tuning, and forgoes costly model architectures and annotation. We evaluate MORTY on a broad range of setups, including different word embedding methods, corpus sizes and end-task semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks.</abstract>
      <url hash="7afafc75">W19-4307</url>
      <doi>10.18653/v1/W19-4307</doi>
      <bibkey>rethmeier-plank-2019-morty</bibkey>
      <pwccode url="https://github.com/NilsRethmeier/MoRTy" additional="false">NilsRethmeier/MoRTy</pwccode>
    </paper>
    <paper id="8">
      <title>Pitfalls in the Evaluation of Sentence Embeddings</title>
      <author><first>Steffen</first><last>Eger</last></author>
      <author><first>Andreas</first><last>Rücklé</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>55–60</pages>
      <abstract>Deep learning models continuously break new records across different NLP tasks. At the same time, their success exposes weaknesses of model evaluation. Here, we compile several key pitfalls of evaluation of sentence embeddings, a currently very popular NLP paradigm. These pitfalls include the comparison of embeddings of different sizes, normalization of embeddings, and the low (and diverging) correlations between transfer and probing tasks. Our motivation is to challenge the current evaluation of sentence embeddings and to provide an easy-to-access reference for future research. Based on our insights, we also recommend better practices for better future evaluations of sentence embeddings.</abstract>
      <url hash="d051a7a0">W19-4308</url>
      <doi>10.18653/v1/W19-4308</doi>
      <bibkey>eger-etal-2019-pitfalls</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="9">
      <title>Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron</title>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Hendrik</first><last>Rosendahl</last></author>
      <author><first>Nick</first><last>Rossenbach</last></author>
      <author><first>Jan</first><last>Rosendahl</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>61–71</pages>
      <abstract>We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data. Our method connects autoencoding and neural machine translation to force the source and target sentence embeddings to share the same space without the help of a pivot language or an additional transformation. We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data. Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.</abstract>
      <url hash="cf9dfeca">W19-4309</url>
      <doi>10.18653/v1/W19-4309</doi>
      <bibkey>kim-etal-2019-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="10">
      <title>Specializing Distributional Vectors of All Words for Lexical Entailment</title>
      <author><first>Aishwarya</first><last>Kamath</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>72–83</pages>
      <abstract>Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g. WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first post-processing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymy-hypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feed-forward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymy-hypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.</abstract>
      <url hash="1296f0aa">W19-4310</url>
      <doi>10.18653/v1/W19-4310</doi>
      <bibkey>kamath-etal-2019-specializing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hyperlex">HyperLex</pwcdataset>
    </paper>
    <paper id="11">
      <title>Composing Noun Phrase Vector Representations</title>
      <author><first>Aikaterini-Lida</first><last>Kalouli</last></author>
      <author><first>Valeria</first><last>de Paiva</last></author>
      <author><first>Richard</first><last>Crouch</last></author>
      <pages>84–95</pages>
      <abstract>Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks. While there seems to be a consensus about the usefulness of word embeddings and how to learn them, it is still unclear which representations can capture the meaning of phrases or even whole sentences. Recent work has shown that simple operations outperform more complex deep architectures. In this work, we propose two novel constraints for computing noun phrase vector representations. First, we propose that the semantic and not the syntactic contribution of each component of a noun phrase should be considered, so that the resulting composed vectors express more of the phrase meaning. Second, the composition process of the two phrase vectors should apply suitable dimensions’ selection in a way that specific semantic features captured by the phrase’s meaning become more salient. Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets. Our results show that these constraints lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.</abstract>
      <url hash="c2260bf1">W19-4311</url>
      <doi>10.18653/v1/W19-4311</doi>
      <bibkey>kalouli-etal-2019-composing</bibkey>
    </paper>
    <paper id="12">
      <title>Towards Robust Named Entity Recognition for Historic <fixed-case>G</fixed-case>erman</title>
      <author><first>Stefan</first><last>Schweter</last></author>
      <author><first>Johannes</first><last>Baiter</last></author>
      <pages>96–103</pages>
      <abstract>In this paper we study the influence of using language model pre-training for named entity recognition for Historic German. We achieve new state-of-the-art results using carefully chosen training data for language models. For a low-resource domain like named entity recognition for Historic German, language model pre-training can be a strong competitor to CRF-only methods. We show that language model pre-training can be more effective than using transfer-learning with labeled datasets. Furthermore, we introduce a new language model pre-training objective, synthetic masked language model pre-training (SMLM), that allows a transfer from one domain (contemporary texts) to another domain (historical texts) by using only the same (character) vocabulary. Results show that using SMLM can achieve comparable results for Historic named entity recognition, even when they are only trained on contemporary texts. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6%.</abstract>
      <url hash="973cfd4a">W19-4312</url>
      <doi>10.18653/v1/W19-4312</doi>
      <bibkey>schweter-baiter-2019-towards</bibkey>
      <pwccode url="https://github.com/stefan-it/historic-ner" additional="true">stefan-it/historic-ner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/europeana-newspapers">Europeana Newspapers</pwcdataset>
    </paper>
    <paper id="13">
      <title>On Evaluating Embedding Models for Knowledge Base Completion</title>
      <author><first>Yanjie</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Ruffinelli</last></author>
      <author><first>Rainer</first><last>Gemulla</last></author>
      <author><first>Samuel</first><last>Broscheit</last></author>
      <author><first>Christian</first><last>Meilicke</last></author>
      <pages>104–112</pages>
      <abstract>Knowledge graph embedding models have recently received significant attention in the literature. These models learn latent semantic representations for the entities and relations in a given knowledge base; the representations can be used to infer missing knowledge. In this paper, we study the question of how well recent embedding models perform for the task of knowledge base completion, i.e., the task of inferring new facts from an incomplete knowledge base. We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated. We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task. We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy. Moreover, we found that a simple rule-based model often provided superior performance. Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.</abstract>
      <url hash="0a33c8bf">W19-4313</url>
      <doi>10.18653/v1/W19-4313</doi>
      <bibkey>wang-etal-2019-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
    </paper>
    <paper id="14">
      <title>Constructive Type-Logical Supertagging With Self-Attention Networks</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Michael</first><last>Moortgat</last></author>
      <author><first>Tejaswini</first><last>Deoskar</last></author>
      <pages>113–123</pages>
      <abstract>We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar’s type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.</abstract>
      <url hash="dae22be6">W19-4314</url>
      <doi>10.18653/v1/W19-4314</doi>
      <bibkey>kogkalidis-etal-2019-constructive</bibkey>
      <pwccode url="https://github.com/konstantinosKokos/Lassy-TLG-Supertagging" additional="false">konstantinosKokos/Lassy-TLG-Supertagging</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aethel">aethel</pwcdataset>
    </paper>
    <paper id="15">
      <title>Auto-Encoding Variational Neural Machine Translation</title>
      <author><first>Bryan</first><last>Eikema</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <pages>124–141</pages>
      <abstract>We present a deep generative model of bilingual sentence pairs for machine translation. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. We perform efficient training using amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three machine translation scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling (i.e. standard neural machine translation) in all such scenarios.</abstract>
      <url hash="0bf100fd">W19-4315</url>
      <doi>10.18653/v1/W19-4315</doi>
      <bibkey>eikema-aziz-2019-auto</bibkey>
      <pwccode url="https://github.com/Roxot/AEVNMT" additional="false">Roxot/AEVNMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="16">
      <title>Learning Bilingual Word Embeddings Using Lexical Definitions</title>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Yingtao</first><last>Tian</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>142–147</pages>
      <abstract>Bilingual word embeddings, which represent lexicons of different languages in a shared embedding space, are essential for supporting semantic and knowledge transfers in a variety of cross-lingual NLP tasks. Existing approaches to training bilingual word embeddings require either large collections of pre-defined seed lexicons that are expensive to obtain, or parallel sentences that comprise coarse and noisy alignment. In contrast, we propose BiLex that leverages publicly available lexical definitions for bilingual word embedding learning. Without the need of predefined seed lexicons, BiLex comprises a novel word pairing strategy to automatically identify and propagate the precise fine-grain word alignment from lexical definitions. We evaluate BiLex in word-level and sentence-level translation tasks, which seek to find the cross-lingual counterparts of words and sentences respectively. BiLex significantly outperforms previous embedding methods on both tasks.</abstract>
      <url hash="e53c4e5d">W19-4316</url>
      <doi>10.18653/v1/W19-4316</doi>
      <bibkey>shi-etal-2019-learning</bibkey>
    </paper>
    <paper id="17">
      <title>An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection</title>
      <author><first>Andres</first><last>Garcia-Silva</last></author>
      <author><first>Cristian</first><last>Berrio</last></author>
      <author><first>José Manuel</first><last>Gómez-Pérez</last></author>
      <pages>148–155</pages>
      <abstract>Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of NLP downstream tasks. Usually, such language models are learned from large and well-formed text corpora from e.g. encyclopedic resources, books or news. However, a significant amount of the text to be analyzed nowadays is Web data, often from social media. In this paper we consider the research question: How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media? To answer this question, we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings. Our results also show strong performance variations among the different language model approaches, which suggest further research.</abstract>
      <url hash="d9e8c6cc">W19-4317</url>
      <doi>10.18653/v1/W19-4317</doi>
      <bibkey>garcia-silva-etal-2019-empirical</bibkey>
    </paper>
    <paper id="18">
      <title>Probing Multilingual Sentence Representations With <fixed-case>X</fixed-case>-Probe</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>156–168</pages>
      <abstract>This paper extends the task of probing sentence representations for linguistic insight in a multilingual domain. In doing so, we make two contributions: first, we provide datasets for multilingual probing, derived from Wikipedia, in five languages, viz. English, French, German, Spanish and Russian. Second, we evaluate six sentence encoders for each language, each trained by mapping sentence representations to English sentence representations, using sentences in a parallel corpus. We discover that cross-lingually mapped representations are often better at retaining certain linguistic information than representations derived from English encoders trained on natural language inference (NLI) as a downstream task.</abstract>
      <url hash="022ffe10">W19-4318</url>
      <doi>10.18653/v1/W19-4318</doi>
      <bibkey>ravishankar-etal-2019-probing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="19">
      <title>Fine-Grained Entity Typing in Hyperbolic Space</title>
      <author><first>Federico</first><last>López</last></author>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>169–180</pages>
      <abstract>How can we represent hierarchical information present in large type inventories for entity typing? We study the suitability of hyperbolic embeddings to capture hierarchical relations between mentions in context and their target types in a shared vector space. We evaluate on two datasets and propose two different techniques to extract hierarchical information from the type inventory: from an expert-generated ontology and by automatically mining the dataset. The hyperbolic model shows improvements in some but not all cases over its Euclidean counterpart. Our analysis suggests that the adequacy of this geometry depends on the granularity of the type inventory and the representation of its distribution.</abstract>
      <url hash="fc1a356c">W19-4319</url>
      <doi>10.18653/v1/W19-4319</doi>
      <bibkey>lopez-etal-2019-fine</bibkey>
      <pwccode url="https://github.com/nlpAThits/figet-hyperbolic-space" additional="false">nlpAThits/figet-hyperbolic-space</pwccode>
    </paper>
    <paper id="20">
      <title>Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition</title>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>181–186</pages>
      <abstract>In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by leveraging monolingual pre-trained embeddings. MME learns to utilize information from these embeddings via a self-attention mechanism without explicit language identification. We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting. The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task.</abstract>
      <url hash="c9a311fd">W19-4320</url>
      <doi>10.18653/v1/W19-4320</doi>
      <bibkey>winata-etal-2019-learning</bibkey>
    </paper>
    <paper id="21">
      <title>Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Bálint</first><last>Döbrössy</last></author>
      <author><first>Márton</first><last>Makrai</last></author>
      <author><first>Balázs</first><last>Tarján</last></author>
      <author><first>György</first><last>Szaszák</last></author>
      <pages>187–193</pages>
      <abstract>For morphologically rich languages, word embeddings provide less consistent semantic representations due to higher variance in word forms. Moreover, these languages often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of word embeddings measured on word analogy tasks drops by 50-75% compared to English. We observed that embeddings learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies – character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) – to boost semantic consistency in Hungarian word vectors. The effect of changing embedding dimension and context window size have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings’ semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard.</abstract>
      <url hash="1f1a3671">W19-4321</url>
      <doi>10.18653/v1/W19-4321</doi>
      <bibkey>dobrossy-etal-2019-investigating</bibkey>
    </paper>
    <paper id="22">
      <title>A Self-Training Approach for Short Text Clustering</title>
      <author><first>Amir</first><last>Hadifar</last></author>
      <author><first>Lucas</first><last>Sterckx</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <pages>194–199</pages>
      <abstract>Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or embeddings can counter that sparseness problem: their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in computer vision, relatively little work has focused on NLP. The method we propose, learns discriminative features from both an autoencoder and a sentence embedding, then uses assignments from a clustering algorithm as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our method.</abstract>
      <url hash="98b270fc">W19-4322</url>
      <doi>10.18653/v1/W19-4322</doi>
      <bibkey>hadifar-etal-2019-self</bibkey>
    </paper>
    <paper id="23">
      <title>Improving Word Embeddings Using Kernel <fixed-case>PCA</fixed-case></title>
      <author><first>Vishwani</first><last>Gupta</last></author>
      <author><first>Sven</first><last>Giesselbach</last></author>
      <author><first>Stefan</first><last>Rüping</last></author>
      <author><first>Christian</first><last>Bauckhage</last></author>
      <pages>200–208</pages>
      <abstract>Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections; however, they fail to do so with small datasets. Extensions such as fastText reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce training time and enhance their performance. We use word embeddings generated using both word2vec and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in English and German, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results.</abstract>
      <url hash="da5a4ee4">W19-4323</url>
      <doi>10.18653/v1/W19-4323</doi>
      <bibkey>gupta-etal-2019-improving</bibkey>
    </paper>
    <paper id="24">
      <title>Assessing Incrementality in Sequence-to-Sequence Models</title>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <pages>209–217</pages>
      <abstract>Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics. The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable. In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to build up incrementally more informative representations of incoming sentences. This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered. In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences.</abstract>
      <url hash="dd7073c7">W19-4324</url>
      <doi>10.18653/v1/W19-4324</doi>
      <bibkey>ulmer-etal-2019-assessing</bibkey>
      <pwccode url="https://github.com/i-machine-think/incremental_encoding" additional="false">i-machine-think/incremental_encoding</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="25">
      <title>On Committee Representations of Adversarial Learning Models for Question-Answer Ranking</title>
      <author><first>Sparsh</first><last>Gupta</last></author>
      <author><first>Vitor</first><last>Carvalho</last></author>
      <pages>218–223</pages>
      <abstract>Adversarial training is a process in Machine Learning that explicitly trains models on adversarial inputs (inputs designed to deceive or trick the learning process) in order to make it more robust or accurate. In this paper we investigate how representing adversarial training models as committees can be used to effectively improve the performance of Question-Answer (QA) Ranking. We start by empirically probing the effects of adversarial training over multiple QA ranking algorithms, including the state-of-the-art Multihop Attention Network model. We evaluate these algorithms on several benchmark datasets and observe that, while adversarial training is beneficial to most baseline algorithms, there are cases where it may lead to overfitting and performance degradation. We investigate the causes of such degradation, and then propose a new representation procedure for this adversarial learning problem, based on committee learning, that not only is capable of consistently improving all baseline algorithms, but also outperforms the previous state-of-the-art algorithm by as much as 6% in NDCG.</abstract>
      <url hash="2cd1bc2d">W19-4325</url>
      <doi>10.18653/v1/W19-4325</doi>
      <bibkey>gupta-carvalho-2019-committee</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/insuranceqa">InsuranceQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="26">
      <title>Meta-Learning Improves Lifelong Relation Extraction</title>
      <author><first>Abiola</first><last>Obamuyide</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>224–229</pages>
      <abstract>Most existing relation extraction models assume a fixed set of relations and are unable to adapt to exploit newly available supervision data to extract new relations. In order to alleviate such problems, there is the need to develop approaches that make relation extraction models capable of continuous adaptation and learning. We investigate and present results for such an approach, based on a combination of ideas from lifelong learning and optimization-based meta-learning. We evaluate the proposed approach on two recent lifelong relation extraction benchmarks, and demonstrate that it markedly outperforms current state-of-the-art approaches.</abstract>
      <url hash="f52557f3">W19-4326</url>
      <doi>10.18653/v1/W19-4326</doi>
      <bibkey>obamuyide-vlachos-2019-meta</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="27">
      <title>Best Practices for Learning Domain-Specific Cross-Lingual Embeddings</title>
      <author><first>Lena</first><last>Shakurova</last></author>
      <author><first>Beata</first><last>Nyari</last></author>
      <author><first>Chao</first><last>Li</last></author>
      <author><first>Mihai</first><last>Rotaru</last></author>
      <pages>230–234</pages>
      <abstract>Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a linear projection from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.</abstract>
      <url hash="179ab58c">W19-4327</url>
      <doi>10.18653/v1/W19-4327</doi>
      <bibkey>shakurova-etal-2019-best</bibkey>
    </paper>
    <paper id="28">
      <title>Effective Dimensionality Reduction for Word Embeddings</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <pages>235–243</pages>
      <abstract>Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on improving the pretrained word vectors through post-processing algorithms. One improvement area is reducing the dimensionality of word embeddings. Reducing the size of word embeddings can improve their utility in memory constrained devices, benefiting several real world applications. In this work, we present a novel technique that efficiently combines PCA based dimensionality reduction with a recently proposed post-processing algorithm (Mu and Viswanath, 2018), to construct effective word embeddings of lower dimensions. Empirical evaluations on several benchmarks show that our algorithm efficiently reduces the embedding size while achieving similar or (more often) better performance than original embeddings. We have released the source code along with this paper.</abstract>
      <url hash="aa1300be">W19-4328</url>
      <doi>10.18653/v1/W19-4328</doi>
      <bibkey>raunak-etal-2019-effective</bibkey>
      <pwccode url="https://github.com/vyraun/Half-Size" additional="false">vyraun/Half-Size</pwccode>
    </paper>
    <paper id="29">
      <title>Learning Word Embeddings without Context Vectors</title>
      <author><first>Alexey</first><last>Zobnin</last></author>
      <author><first>Evgenia</first><last>Elistratova</last></author>
      <pages>244–249</pages>
      <abstract>Most word embedding algorithms such as word2vec or fastText construct two sort of vectors: for words and for contexts. Naive use of vectors of only one sort leads to poor results. We suggest using indefinite inner product in skip-gram negative sampling algorithm. This allows us to use only one sort of vectors without loss of quality. Our “context-free” cf algorithm performs on par with SGNS on word similarity datasets</abstract>
      <url hash="1ff1baed">W19-4329</url>
      <doi>10.18653/v1/W19-4329</doi>
      <bibkey>zobnin-elistratova-2019-learning</bibkey>
    </paper>
    <paper id="30">
      <title>Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model</title>
      <author><first>Muthu</first><last>Chidambaram</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Steve</first><last>Yuan</last></author>
      <author><first>Yunhsuan</first><last>Sung</last></author>
      <author><first>Brian</first><last>Strope</last></author>
      <author><first>Ray</first><last>Kurzweil</last></author>
      <pages>250–259</pages>
      <abstract>The scarcity of labeled training data across many languages is a significant roadblock for multilingual neural language processing. We approach the lack of in-language training data using sentence embeddings that map text written in different languages, but with similar meanings, to nearby embedding space representations. The representations are produced using a dual-encoder based model trained to maximize the representational similarity between sentence pairs drawn from parallel data. The representations are enhanced using multitask training and unsupervised monolingual corpora. The effectiveness of our multilingual sentence embeddings are assessed on a comprehensive collection of monolingual, cross-lingual, and zero-shot/few-shot learning tasks.</abstract>
      <url hash="689c9722">W19-4330</url>
      <doi>10.18653/v1/W19-4330</doi>
      <bibkey>chidambaram-etal-2019-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="31">
      <title>Modality-based Factorization for Multimodal Fusion</title>
      <author><first>Elham J.</first><last>Barezi</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>260–269</pages>
      <abstract>We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (<i>M</i>+1)-way tensor to consider the high-order relationships between <i>M</i> modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.</abstract>
      <url hash="7e7e55ac">W19-4331</url>
      <doi>10.18653/v1/W19-4331</doi>
      <bibkey>barezi-fung-2019-modality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="32">
      <title>Leveraging Pre-Trained Embeddings for <fixed-case>W</fixed-case>elsh Taggers</title>
      <author><first>Ignatius</first><last>Ezeani</last></author>
      <author><first>Scott</first><last>Piao</last></author>
      <author><first>Steven</first><last>Neale</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Dawn</first><last>Knight</last></author>
      <pages>270–280</pages>
      <abstract>While the application of word embedding models to downstream Natural Language Processing (NLP) tasks has been shown to be successful, the benefits for low-resource languages is somewhat limited due to lack of adequate data for training the models. However, NLP research efforts for low-resource languages have focused on constantly seeking ways to harness pre-trained models to improve the performance of NLP systems built to process these languages without the need to re-invent the wheel. One such language is Welsh and therefore, in this paper, we present the results of our experiments on learning a simple multi-task neural network model for part-of-speech and semantic tagging for Welsh using a pre-trained embedding model from FastText. Our model’s performance was compared with those of the existing rule-based stand-alone taggers for part-of-speech and semantic taggers. Despite its simplicity and capacity to perform both tasks simultaneously, our tagger compared very well with the existing taggers.</abstract>
      <url hash="776cff98">W19-4332</url>
      <doi>10.18653/v1/W19-4332</doi>
      <bibkey>ezeani-etal-2019-leveraging</bibkey>
    </paper>
  </volume>
  <volume id="44">
    <meta>
      <booktitle>Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</booktitle>
      <url hash="82fb4e04">W19-44</url>
      <editor><first>Helen</first><last>Yannakoudakis</last></editor>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Claudia</first><last>Leacock</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Ildikó</first><last>Pilán</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>bea</venue>
    </meta>
    <frontmatter>
      <url hash="44235c02">W19-4400</url>
      <bibkey>ws-2019-innovative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The many dimensions of algorithmic fairness in educational applications</title>
      <author><first>Anastassia</first><last>Loukina</last></author>
      <author><first>Nitin</first><last>Madnani</last></author>
      <author><first>Klaus</first><last>Zechner</last></author>
      <pages>1–10</pages>
      <abstract>The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people’s lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers’ native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.</abstract>
      <url hash="a1683fa8">W19-4401</url>
      <doi>10.18653/v1/W19-4401</doi>
      <bibkey>loukina-etal-2019-many</bibkey>
    </paper>
    <paper id="2">
      <title>Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam</title>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Peter</first><last>Baldwin</last></author>
      <author><first>Janet</first><last>Mee</last></author>
      <pages>11–20</pages>
      <abstract>Predicting the construct-relevant difficulty of Multiple-Choice Questions (MCQs) has the potential to reduce cost while maintaining the quality of high-stakes exams. In this paper, we propose a method for estimating the difficulty of MCQs from a high-stakes medical exam, where all questions were deliberately written to a common reading level. To accomplish this, we extract a large number of linguistic features and embedding types, as well as features quantifying the difficulty of the items for an automatic question-answering system. The results show that the proposed approach outperforms various baselines with a statistically significant difference. Best results were achieved when using the full feature set, where embeddings had the highest predictive power, followed by linguistic features. An ablation study of the various types of linguistic features suggested that information from all levels of linguistic processing contributes to predicting item difficulty, with features related to semantic ambiguity and the psycholinguistic properties of words having a slightly higher importance. Owing to its generic nature, the presented approach has the potential to generalize over other exams containing MCQs.</abstract>
      <url hash="2de540e3">W19-4402</url>
      <doi>10.18653/v1/W19-4402</doi>
      <bibkey>ha-etal-2019-predicting</bibkey>
    </paper>
    <paper id="3">
      <title>An Intelligent Testing Strategy for Vocabulary Assessment of <fixed-case>C</fixed-case>hinese Second Language Learners</title>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Renfen</first><last>Hu</last></author>
      <author><first>Feipeng</first><last>Sun</last></author>
      <author><first>Ronghuai</first><last>Huang</last></author>
      <pages>21–29</pages>
      <abstract>Vocabulary is one of the most important parts of language competence. Testing of vocabulary knowledge is central to research on reading and language. However, it usually costs a large amount of time and human labor to build an item bank and to test large number of students. In this paper, we propose a novel testing strategy by combining automatic item generation (AIG) and computerized adaptive testing (CAT) in vocabulary assessment for Chinese L2 learners. Firstly, we generate three types of vocabulary questions by modeling both the vocabulary knowledge and learners’ writing error data. After evaluation and calibration, we construct a balanced item pool with automatically generated items, and implement a three-parameter computerized adaptive test. We conduct manual item evaluation and online student tests in the experiments. The results show that the combination of AIG and CAT can construct test items efficiently and reduce test cost significantly. Also, the test result of CAT can provide valuable feedback to AIG algorithms.</abstract>
      <url hash="5e9f4e4b">W19-4403</url>
      <doi>10.18653/v1/W19-4403</doi>
      <bibkey>zhou-etal-2019-intelligent</bibkey>
    </paper>
    <paper id="4">
      <title>Computationally Modeling the Impact of Task-Appropriate Language Complexity and Accuracy on Human Grading of <fixed-case>G</fixed-case>erman Essays</title>
      <author><first>Zarah</first><last>Weiss</last></author>
      <author><first>Anja</first><last>Riemenschneider</last></author>
      <author><first>Pauline</first><last>Schröter</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>30–45</pages>
      <abstract>Computational linguistic research on the language complexity of student writing typically involves human ratings as a gold standard. However, educational science shows that teachers find it difficult to identify and cleanly separate accuracy, different aspects of complexity, contents, and structure. In this paper, we therefore explore the use of computational linguistic methods to investigate how task-appropriate complexity and accuracy relate to the grading of overall performance, content performance, and language performance as assigned by teachers. Based on texts written by students for the official school-leaving state examination (Abitur), we show that teachers successfully assign higher language performance grades to essays with higher task-appropriate language complexity and properly separate this from content scores. Yet, accuracy impacts teacher assessment for all grading rubrics, also the content score, overemphasizing the role of accuracy. Our analysis is based on broad computational linguistic modeling of German language complexity and an innovative theory- and data-driven feature aggregation method inferring task-appropriate language complexity.</abstract>
      <url hash="da63fbde">W19-4404</url>
      <doi>10.18653/v1/W19-4404</doi>
      <bibkey>weiss-etal-2019-computationally</bibkey>
    </paper>
    <paper id="5">
      <title>Analysing Rhetorical Structure as a Key Feature of Summary Coherence</title>
      <author><first>Jan</first><last>Šnajder</last></author>
      <author><first>Tamara</first><last>Sladoljev-Agejev</last></author>
      <author><first>Svjetlana</first><last>Kolić Vehovec</last></author>
      <pages>46–51</pages>
      <abstract>We present a model for automatic scoring of coherence based on comparing the rhetorical structure (RS) of college student summaries in L2 (English) against expert summaries. Coherence is conceptualised as a construct consisting of the rhetorical relation and its arguments. Comparison with expert-assigned scores shows that RS scores correlate with both cohesion and coherence. Furthermore, RS scores improve the accuracy of a regression model for cohesion score prediction.</abstract>
      <url hash="f220a129">W19-4405</url>
      <doi>10.18653/v1/W19-4405</doi>
      <bibkey>snajder-etal-2019-analysing</bibkey>
    </paper>
    <paper id="6">
      <title>The <fixed-case>BEA</fixed-case>-2019 Shared Task on Grammatical Error Correction</title>
      <author><first>Christopher</first><last>Bryant</last></author>
      <author><first>Mariano</first><last>Felice</last></author>
      <author><first>Øistein E.</first><last>Andersen</last></author>
      <author><first>Ted</first><last>Briscoe</last></author>
      <pages>52–75</pages>
      <abstract>This paper reports on the BEA-2019 Shared Task on Grammatical Error Correction (GEC). As with the CoNLL-2014 shared task, participants are required to correct all types of errors in test data. One of the main contributions of the BEA-2019 shared task is the introduction of a new dataset, the Write&amp;Improve+LOCNESS corpus, which represents a wider range of native and learner English levels and abilities. Another contribution is the introduction of tracks, which control the amount of annotated data available to participants. Systems are evaluated in terms of ERRANT F_0.5, which allows us to report a much wider range of performance statistics. The competition was hosted on Codalab and remains open for further submissions on the blind test set.</abstract>
      <url hash="5c88f72d">W19-4406</url>
      <doi>10.18653/v1/W19-4406</doi>
      <bibkey>bryant-etal-2019-bea</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="7">
      <title>A Benchmark Corpus of <fixed-case>E</fixed-case>nglish Misspellings and a Minimally-supervised Model for Spelling Correction</title>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Michael</first><last>Fried</last></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>76–86</pages>
      <abstract>Spelling correction has attracted a lot of attention in the NLP community. However, models have been usually evaluated on artificiallycreated or proprietary corpora. A publiclyavailable corpus of authentic misspellings, annotated in context, is still lacking. To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners. We also develop a minimallysupervised context-aware approach to spelling correction. It achieves strong results on our data: 88.12% accuracy. This approach can also train with a minimal amount of annotated data (performance reduced by less than 1%). Furthermore, this approach allows easy portability to new domains. We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data.</abstract>
      <url hash="1b4302e5">W19-4407</url>
      <doi>10.18653/v1/W19-4407</doi>
      <bibkey>flor-etal-2019-benchmark</bibkey>
      <pwccode url="https://github.com/EducationalTestingService/toefl-spell" additional="false">EducationalTestingService/toefl-spell</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="8">
      <title>Artificial Error Generation with Fluency Filtering</title>
      <author><first>Mengyang</first><last>Qiu</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>87–91</pages>
      <abstract>The quantity and quality of training data plays a crucial role in grammatical error correction (GEC). However, due to the fact that obtaining human-annotated GEC data is both time-consuming and expensive, several studies have focused on generating artificial error sentences to boost training data for grammatical error correction, and shown significantly better performance. The present study explores how fluency filtering can affect the quality of artificial errors. By comparing artificial data filtered by different levels of fluency, we find that artificial error sentences with low fluency can greatly facilitate error correction, while high fluency errors introduce more noise.</abstract>
      <url hash="3973956d">W19-4408</url>
      <doi>10.18653/v1/W19-4408</doi>
      <bibkey>qiu-park-2019-artificial</bibkey>
    </paper>
    <paper id="9">
      <title>Regression or classification? Automated Essay Scoring for <fixed-case>N</fixed-case>orwegian</title>
      <author><first>Stig</first><last>Johan Berggren</last></author>
      <author><first>Taraka</first><last>Rama</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>92–102</pages>
      <abstract>In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification. We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.</abstract>
      <url hash="3aa92729">W19-4409</url>
      <doi>10.18653/v1/W19-4409</doi>
      <bibkey>johan-berggren-etal-2019-regression</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="10">
      <title>Context is Key: Grammatical Error Detection with Contextual Word Representations</title>
      <author><first>Samuel</first><last>Bell</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <pages>103–115</pages>
      <abstract>Grammatical error detection (GED) in non-native writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors.</abstract>
      <url hash="b90ea521">W19-4410</url>
      <doi>10.18653/v1/W19-4410</doi>
      <bibkey>bell-etal-2019-context</bibkey>
      <pwccode url="https://github.com/samueljamesbell/sequence-labeler" additional="false">samueljamesbell/sequence-labeler</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="11">
      <title>How to account for mispellings: Quantifying the benefit of character representations in neural content scoring models</title>
      <author><first>Brian</first><last>Riordan</last></author>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Robert</first><last>Pugh</last></author>
      <pages>116–126</pages>
      <abstract>Character-based representations in neural models have been claimed to be a tool to overcome spelling variation in in word token-based input. We examine this claim in neural models for content scoring. We formulate precise hypotheses about the possible effects of adding character representations to word-based models and test these hypotheses on large-scale real world content scoring datasets. We find that, while character representations may provide small performance gains in general, their effectiveness in accounting for spelling variation may be limited. We show that spelling correction can provide larger gains than character representations, and that spelling correction improves the performance of models with character representations. With these insights, we report a new state of the art on the ASAP-SAS content scoring dataset.</abstract>
      <url hash="a184e9a7">W19-4411</url>
      <doi>10.18653/v1/W19-4411</doi>
      <bibkey>riordan-etal-2019-account</bibkey>
    </paper>
    <paper id="12">
      <title>The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction</title>
      <author><first>Dimitris</first><last>Alikaniotis</last></author>
      <author><first>Vipul</first><last>Raheja</last></author>
      <pages>127–133</pages>
      <abstract>Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.</abstract>
      <url hash="d08916aa">W19-4412</url>
      <doi>10.18653/v1/W19-4412</doi>
      <bibkey>alikaniotis-raheja-2019-unreasonable</bibkey>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="13">
      <title>(Almost) Unsupervised Grammatical Error Correction using Synthetic Comparable Corpus</title>
      <author><first>Satoru</first><last>Katsumata</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>134–138</pages>
      <abstract>We introduce unsupervised techniques based on phrase-based statistical machine translation for grammatical error correction (GEC) trained on a pseudo learner corpus created by Google Translation. We verified our GEC system through experiments on a low resource track of the shared task at BEA2019. As a result, we achieved an F0.5 score of 28.31 points with the test data.</abstract>
      <url hash="6e18a5b7">W19-4413</url>
      <doi>10.18653/v1/W19-4413</doi>
      <bibkey>katsumata-komachi-2019-almost</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="14">
      <title>Learning to combine Grammatical Error Corrections</title>
      <author><first>Yoav</first><last>Kantor</last></author>
      <author><first>Yoav</first><last>Katz</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Edo</first><last>Cohen-Karlik</last></author>
      <author><first>Naftali</first><last>Liberman</last></author>
      <author><first>Assaf</first><last>Toledo</last></author>
      <author><first>Amir</first><last>Menczel</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>139–148</pages>
      <abstract>The field of Grammatical Error Correction (GEC) has produced various systems to deal with focused phenomena or general text editing. We propose an automatic way to combine black-box systems. Our method automatically detects the strength of a system or the combination of several systems per error type, improving precision and recall while optimizing F-score directly. We show consistent improvement over the best standalone system in all the configurations tested. This approach also outperforms average ensembling of different RNN models with random initializations. In addition, we analyze the use of BERT for GEC - reporting promising results on this end. We also present a spellchecker created for this task which outperforms standard spellcheckers tested on the task of spellchecking. This paper describes a system submission to Building Educational Applications 2019 Shared Task: Grammatical Error Correction. Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F-0.5 score by 3.7 points over the best result reported.</abstract>
      <url hash="9858f1ea">W19-4414</url>
      <doi>10.18653/v1/W19-4414</doi>
      <bibkey>kantor-etal-2019-learning</bibkey>
      <pwccode url="https://github.com/IBM/learning-to-combine-grammatical-error-corrections" additional="false">IBM/learning-to-combine-grammatical-error-corrections</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="15">
      <title>Erroneous data generation for Grammatical Error Correction</title>
      <author><first>Shuyao</first><last>Xu</last></author>
      <author><first>Jiehao</first><last>Zhang</last></author>
      <author><first>Jin</first><last>Chen</last></author>
      <author><first>Long</first><last>Qin</last></author>
      <pages>149–158</pages>
      <abstract>It has been demonstrated that the utilization of a monolingual corpus in neural Grammatical Error Correction (GEC) systems can significantly improve the system performance. The previous state-of-the-art neural GEC system is an ensemble of four Transformer models pretrained on a large amount of Wikipedia Edits. The Singsound GEC system follows a similar approach but is equipped with a sophisticated erroneous data generating component. Our system achieved an F0:5 of 66.61 in the BEA 2019 Shared Task: Grammatical Error Correction. With our novel erroneous data generating component, the Singsound neural GEC system yielded an M2 of 63.2 on the CoNLL-2014 benchmark (8.4% relative improvement over the previous state-of-the-art system).</abstract>
      <url hash="5c4b6201">W19-4415</url>
      <doi>10.18653/v1/W19-4415</doi>
      <bibkey>xu-etal-2019-erroneous</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="16">
      <title>The <fixed-case>LAIX</fixed-case> Systems in the <fixed-case>BEA</fixed-case>-2019 <fixed-case>GEC</fixed-case> Shared Task</title>
      <author><first>Ruobing</first><last>Li</last></author>
      <author><first>Chuan</first><last>Wang</last></author>
      <author><first>Yefei</first><last>Zha</last></author>
      <author><first>Yonghong</first><last>Yu</last></author>
      <author><first>Shiman</first><last>Guo</last></author>
      <author><first>Qiang</first><last>Wang</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Hui</first><last>Lin</last></author>
      <pages>159–167</pages>
      <abstract>In this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.</abstract>
      <url hash="9fe333f2">W19-4416</url>
      <doi>10.18653/v1/W19-4416</doi>
      <bibkey>li-etal-2019-laix</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="17">
      <title>The <fixed-case>CUED</fixed-case>’s Grammatical Error Correction Systems for <fixed-case>BEA</fixed-case>-2019</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>168–175</pages>
      <abstract>We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction. Our submission to the low-resource track is based on prior work on using finite state transducers together with strong neural language models. Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with back-translation and a combination of checkpoint averaging and fine-tuning – without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab.</abstract>
      <url hash="9700d046">W19-4417</url>
      <doi>10.18653/v1/W19-4417</doi>
      <bibkey>stahlberg-byrne-2019-cueds</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
    </paper>
    <paper id="18">
      <title>The <fixed-case>AIP</fixed-case>-Tohoku System at the <fixed-case>BEA</fixed-case>-2019 Shared Task</title>
      <author><first>Hiroki</first><last>Asano</last></author>
      <author><first>Masato</first><last>Mita</last></author>
      <author><first>Tomoya</first><last>Mizumoto</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <pages>176–182</pages>
      <abstract>We introduce the AIP-Tohoku grammatical error correction (GEC) system for the BEA-2019 shared task in Track 1 (Restricted Track) and Track 2 (Unrestricted Track) using the same system architecture. Our system comprises two key components: error generation and sentence-level error detection. In particular, GEC with sentence-level grammatical error detection is a novel and versatile approach, and we experimentally demonstrate that it significantly improves the precision of the base model. Our system is ranked 9th in Track 1 and 2nd in Track 2.</abstract>
      <url hash="627c53bb">W19-4418</url>
      <doi>10.18653/v1/W19-4418</doi>
      <bibkey>asano-etal-2019-aip</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>CUNI</fixed-case> System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction</title>
      <author><first>Jakub</first><last>Náplava</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <pages>183–190</pages>
      <abstract>Our submitted models are NMT systems based on the Transformer model, which we improve by incorporating several enhancements: applying dropout to whole source and target words, weighting target subwords, averaging model checkpoints, and using the trained model iteratively for correcting the intermediate translations. The system in the Restricted Track is trained on the provided corpora with oversampled “cleaner” sentences and reaches 59.39 F0.5 score on the test set. The system in the Low-Resource Track is trained from Wikipedia revision histories and reaches 44.13 F0.5 score. Finally, we finetune the system from the Low-Resource Track on restricted data and achieve 64.55 F0.5 score.</abstract>
      <url hash="0dc6d1f4">W19-4419</url>
      <doi>10.18653/v1/W19-4419</doi>
      <bibkey>naplava-straka-2019-cuni</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="20">
      <title>Noisy Channel for Low Resource Grammatical Error Correction</title>
      <author><first>Simon</first><last>Flachs</last></author>
      <author><first>Ophélie</first><last>Lacroix</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>191–196</pages>
      <abstract>This paper describes our contribution to the low-resource track of the BEA 2019 shared task on Grammatical Error Correction (GEC). Our approach to GEC builds on the theory of the noisy channel by combining a channel model and language model. We generate confusion sets from the Wikipedia edit history and use the frequencies of edits to estimate the channel model. Additionally, we use two pre-trained language models: 1) Google’s BERT model, which we fine-tune for specific error types and 2) OpenAI’s GPT-2 model, utilizing that it can operate with previous sentences as context. Furthermore, we search for the optimal combinations of corrections using beam search.</abstract>
      <url hash="931eccba">W19-4420</url>
      <doi>10.18653/v1/W19-4420</doi>
      <bibkey>flachs-etal-2019-noisy</bibkey>
    </paper>
    <paper id="21">
      <title>The <fixed-case>BLCU</fixed-case> System in the <fixed-case>BEA</fixed-case> 2019 Shared Task</title>
      <author><first>Liner</first><last>Yang</last></author>
      <author><first>Chencheng</first><last>Wang</last></author>
      <pages>197–206</pages>
      <abstract>This paper describes the BLCU Group submissions to the Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC). The task is to detect and correct grammatical errors that occurred in essays. We participate in 2 tracks including the Restricted Track and the Unrestricted Track. Our system is based on a Transformer model architecture. We integrate many effective methods proposed in recent years. Such as, Byte Pair Encoding, model ensemble, checkpoints average and spell checker. We also corrupt the public monolingual data to further improve the performance of the model. On the test data of the BEA 2019 Shared Task, our system yields F0.5 = 58.62 and 59.50, ranking twelfth and fourth respectively.</abstract>
      <url hash="7337aa67">W19-4421</url>
      <doi>10.18653/v1/W19-4421</doi>
      <bibkey>yang-wang-2019-blcu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="22">
      <title><fixed-case>TMU</fixed-case> Transformer System Using <fixed-case>BERT</fixed-case> for Re-ranking at <fixed-case>BEA</fixed-case> 2019 Grammatical Error Correction on Restricted Track</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Kengo</first><last>Hotate</last></author>
      <author><first>Satoru</first><last>Katsumata</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>207–212</pages>
      <abstract>We introduce our system that is submitted to the restricted track of the BEA 2019 shared task on grammatical error correction1 (GEC). It is essential to select an appropriate hypothesis sentence from the candidates list generated by the GEC model. A re-ranker can evaluate the naturalness of a corrected sentence using language models trained on large corpora. On the other hand, these language models and language representations do not explicitly take into account the grammatical errors written by learners. Thus, it is not straightforward to utilize language representations trained from a large corpus, such as Bidirectional Encoder Representations from Transformers (BERT), in a form suitable for the learner’s grammatical errors. Therefore, we propose to fine-tune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W&amp;I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance.</abstract>
      <url hash="60a366b6">W19-4422</url>
      <doi>10.18653/v1/W19-4422</doi>
      <bibkey>kaneko-etal-2019-tmu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="23">
      <title>A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning</title>
      <author><first>Yo Joong</first><last>Choe</last></author>
      <author><first>Jiyeon</first><last>Ham</last></author>
      <author><first>Kyubyong</first><last>Park</last></author>
      <author><first>Yeoil</first><last>Yoon</last></author>
      <pages>213–227</pages>
      <abstract>Grammatical error correction can be viewed as a low-resource sequence-to-sequence task, because publicly available parallel corpora are limited.To tackle this challenge, we first generate erroneous versions of large unannotated corpora using a realistic noising function. The resulting parallel corpora are sub-sequently used to pre-train Transformer models. Then, by sequentially applying transfer learning, we adapt these models to the domain and style of the test set. Combined with a context-aware neural spellchecker, our system achieves competitive results in both restricted and low resource tracks in ACL 2019 BEAShared Task. We release all of our code and materials for reproducibility.</abstract>
      <url hash="c9dd0c00">W19-4423</url>
      <doi>10.18653/v1/W19-4423</doi>
      <bibkey>choe-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/kakaobrain/helo_word" additional="false">kakaobrain/helo_word</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="24">
      <title>Neural and <fixed-case>FST</fixed-case>-based approaches to grammatical error correction</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last></author>
      <pages>228–239</pages>
      <abstract>In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly available data, along with artificial examples generated through back-translation. The n-best lists of these two machine translation systems are then combined and scored using a finite state transducer (FST). Finally, an unsupervised re-ranking system is applied to the n-best output of the FST. The re-ranker uses a number of error detection features to re-rank the FST n-best list and identify the final 1-best correction hypothesis. Our system achieves 66.75% F 0.5 on error correction (ranking 4th), and 82.52% F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.</abstract>
      <url hash="1949599b">W19-4424</url>
      <doi>10.18653/v1/W19-4424</doi>
      <bibkey>yuan-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="25">
      <title>Improving Precision of Grammatical Error Correction with a Cheat Sheet</title>
      <author><first>Mengyang</first><last>Qiu</last></author>
      <author><first>Xuejiao</first><last>Chen</last></author>
      <author><first>Maggie</first><last>Liu</last></author>
      <author><first>Krishna</first><last>Parvathala</last></author>
      <author><first>Apurva</first><last>Patil</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>240–245</pages>
      <abstract>In this paper, we explore two approaches of generating error-focused phrases and examine whether these phrases can lead to better performance in grammatical error correction for the restricted track of BEA 2019 Shared Task on GEC. Our results show that phrases directly extracted from GEC corpora outperform phrases from statistical machine translation phrase table by a large margin. Appending error+context phrases to the original GEC corpora yields comparably high precision. We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track. The additional training data greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance.</abstract>
      <url hash="530412b6">W19-4425</url>
      <doi>10.18653/v1/W19-4425</doi>
      <bibkey>qiu-etal-2019-improving</bibkey>
    </paper>
    <paper id="26">
      <title>Multi-headed Architecture Based on <fixed-case>BERT</fixed-case> for Grammatical Errors Correction</title>
      <author><first>Bohdan</first><last>Didenko</last></author>
      <author><first>Julia</first><last>Shaptala</last></author>
      <pages>246–251</pages>
      <abstract>In this paper, we describe our approach to GEC using the BERT model for creation of encoded representation and some of our enhancements, namely, “Heads” are fully-connected networks which are used for finding the errors and later receive recommendation from the networks on dealing with a highlighted part of the sentence only. Among the main advantages of our solution is increasing the system productivity and lowering the time of processing while keeping the high accuracy of GEC results.</abstract>
      <url hash="c25f1a84">W19-4426</url>
      <doi>10.18653/v1/W19-4426</doi>
      <bibkey>didenko-shaptala-2019-multi</bibkey>
    </paper>
    <paper id="27">
      <title>Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data</title>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>252–263</pages>
      <abstract>Considerable effort has been made to address the data sparsity problem in neural grammatical error correction. In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data. Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F<tex-math>_{0.5}</tex-math> in the restricted and low-resource tracks respectively, both on the W&amp;I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M² for the submitted system, and 61.30 M² for the constrained system trained on the NUCLE and Lang-8 data.</abstract>
      <url hash="65d134f2">W19-4427</url>
      <doi>10.18653/v1/W19-4427</doi>
      <bibkey>grundkiewicz-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/grammatical/pretraining-bea2019" additional="false">grammatical/pretraining-bea2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="28">
      <title>Evaluation of automatic collocation extraction methods for language learning</title>
      <author><first>Vishal</first><last>Bhalla</last></author>
      <author><first>Klara</first><last>Klimcikova</last></author>
      <pages>264–274</pages>
      <abstract>A number of methods have been proposed to automatically extract collocations, i.e., conventionalized lexical combinations, from text corpora. However, the attempts to evaluate and compare them with a specific application in mind lag behind. This paper compares three end-to-end resources for collocation learning, all of which used the same corpus but different methods. Adopting a gold-standard evaluation method, the results show that the method of dependency parsing outperforms regex-over-pos in collocation identification. The lexical association measures (AMs) used for collocation ranking perform about the same overall but differently for individual collocation types. Further analysis has also revealed that there are considerable differences between other commonly used AMs.</abstract>
      <url hash="3606a59d">W19-4428</url>
      <doi>10.18653/v1/W19-4428</doi>
      <bibkey>bhalla-klimcikova-2019-evaluation</bibkey>
      <pwccode url="https://github.com/vishalbhalla/autocoleval" additional="false">vishalbhalla/autocoleval</pwccode>
    </paper>
    <paper id="29">
      <title>Anglicized Words and Misspelled Cognates in Native Language Identification</title>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Vivi</first><last>Nastase</last></author>
      <author><first>Carlo</first><last>Strapparava</last></author>
      <pages>275–284</pages>
      <abstract>In this paper, we present experiments that estimate the impact of specific lexical choices of people writing in a second language (L2). In particular, we look at misspelled words that indicate lexical uncertainty on the part of the author, and separate them into three categories: misspelled cognates, “L2-ed” (in our case, anglicized) words, and all other spelling errors. We test the assumption that such errors contain clues about the native language of an essay’s author through the task of native language identification. The results of the experiments show that the information brought by each of these categories is complementary. We also note that while the distribution of such features changes with the proficiency level of the writer, their contribution towards native language identification remains significant at all levels.</abstract>
      <url hash="51c9e15f">W19-4429</url>
      <doi>10.18653/v1/W19-4429</doi>
      <bibkey>markov-etal-2019-anglicized</bibkey>
    </paper>
    <paper id="30">
      <title>Linguistically-Driven Strategy for Concept Prerequisites Learning on <fixed-case>I</fixed-case>talian</title>
      <author><first>Alessio</first><last>Miaschi</last></author>
      <author><first>Chiara</first><last>Alzetta</last></author>
      <author><first>Franco Alberto</first><last>Cardillo</last></author>
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <pages>285–295</pages>
      <abstract>We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for Italian and English. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the classifier performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it.</abstract>
      <url hash="ec132b0f">W19-4430</url>
      <doi>10.18653/v1/W19-4430</doi>
      <bibkey>miaschi-etal-2019-linguistically</bibkey>
    </paper>
    <paper id="31">
      <title>Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of <fixed-case>J</fixed-case>apanese as a Second Language</title>
      <author><first>Mio</first><last>Arai</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>296–305</pages>
      <abstract>Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner’s writing.</abstract>
      <url hash="d4215ca2">W19-4431</url>
      <doi>10.18653/v1/W19-4431</doi>
      <bibkey>arai-etal-2019-grammatical</bibkey>
    </paper>
    <paper id="32">
      <title>Toward Automated Content Feedback Generation for Non-native Spontaneous Speech</title>
      <author><first>Su-Youn</first><last>Yoon</last></author>
      <author><first>Ching-Ni</first><last>Hsieh</last></author>
      <author><first>Klaus</first><last>Zechner</last></author>
      <author><first>Matthew</first><last>Mulholland</last></author>
      <author><first>Yuan</first><last>Wang</last></author>
      <author><first>Nitin</first><last>Madnani</last></author>
      <pages>306–315</pages>
      <abstract>In this study, we developed an automated algorithm to provide feedback about the specific content of non-native English speakers’ spoken responses. The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and/or read passages and integrated the core content in their spoken responses. Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models: (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features. Both models achieved a substantially improved performance over the majority baseline, and the combination of the two models achieved a significant further improvement. In particular, the models were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions. The accuracy and F-score of the best model for the questions included in the train set were 0.80 and 0.68, respectively. Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner’s response, based on automatically detected missing key points.</abstract>
      <url hash="fe76a83c">W19-4432</url>
      <doi>10.18653/v1/W19-4432</doi>
      <bibkey>yoon-etal-2019-toward</bibkey>
    </paper>
    <paper id="33">
      <title>Analytic Score Prediction and Justification Identification in Automated Short Answer Scoring</title>
      <author><first>Tomoya</first><last>Mizumoto</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Yoriko</first><last>Isobe</last></author>
      <author><first>Paul</first><last>Reisert</last></author>
      <author><first>Ryo</first><last>Nagata</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>316–325</pages>
      <abstract>This paper provides an analytical assessment of student short answer responses with a view to potential benefits in pedagogical contexts. We first propose and formalize two novel analytical assessment tasks: analytic score prediction and justification identification, and then provide the first dataset created for analytic short answer scoring research. Subsequently, we present a neural baseline model and report our extensive empirical results to demonstrate how our dataset can be used to explore new and intriguing technical challenges in short answer scoring. The dataset is publicly available for research purposes.</abstract>
      <url hash="845fdcb4">W19-4433</url>
      <doi>10.18653/v1/W19-4433</doi>
      <bibkey>mizumoto-etal-2019-analytic</bibkey>
    </paper>
    <paper id="34">
      <title>Content Customization for Micro Learning using Human Augmented <fixed-case>AI</fixed-case> Techniques</title>
      <author><first>Ayush</first><last>Shah</last></author>
      <author><first>Tamer</first><last>Abuelsaad</last></author>
      <author><first>Jae-Wook</first><last>Ahn</last></author>
      <author><first>Prasenjit</first><last>Dey</last></author>
      <author><first>Ravi</first><last>Kokku</last></author>
      <author><first>Ruhi</first><last>Sharma Mittal</last></author>
      <author><first>Aditya</first><last>Vempaty</last></author>
      <author><first>Mourvi</first><last>Sharma</last></author>
      <pages>326–335</pages>
      <abstract>Visual content has been proven to be effective for micro-learning compared to other media. In this paper, we discuss leveraging this observation in our efforts to build audio-visual content for young learners’ vocabulary learning. We attempt to tackle two major issues in the process of traditional visual curation tasks. Generic learning videos do not necessarily satisfy the unique context of a learner and/or an educator, and hence may not result in maximal learning outcomes. Also, manual video curation by educators is a highly labor-intensive process. To this end, we present a customizable micro-learning audio-visual content curation tool that is designed to reduce the human (educator) effort in creating just-in-time learning videos from a textual description (learning script). This provides educators with control of the content while preparing the learning scripts, and in turn can also be customized to capture the desired learning objectives and outcomes. As a use case, we automatically generate learning videos with British National Corpus’ (BNC) frequently spoken vocabulary words and evaluate them with experts. They positively recommended the generated learning videos with an average rating of 4.25 on a Likert scale of 5 points. The inter-annotator agreement between the experts for the video quality was substantial (Fleiss Kappa=0.62) with an overall agreement of 81%.</abstract>
      <url hash="cceea1a1">W19-4434</url>
      <doi>10.18653/v1/W19-4434</doi>
      <bibkey>shah-etal-2019-content</bibkey>
    </paper>
    <paper id="35">
      <title>Curio <fixed-case>S</fixed-case>mart<fixed-case>C</fixed-case>hat : A system for Natural Language Question Answering for Self-Paced K-12 Learning</title>
      <author><first>Srikrishna</first><last>Raamadhurai</last></author>
      <author><first>Ryan</first><last>Baker</last></author>
      <author><first>Vikraman</first><last>Poduval</last></author>
      <pages>336–342</pages>
      <abstract>During learning, students often have questions which they would benefit from responses to in real time. In class, a student can ask a question to a teacher. During homework, or even in class if the student is shy, it can be more difficult to receive a rapid response. In this work, we introduce Curio SmartChat, an automated question answering system for middle school Science topics. Our system has now been used by around 20,000 students who have so far asked over 100,000 questions. We present data on the challenge created by students’ grammatical errors and spelling mistakes, and discuss our system’s approach and degree of effectiveness at disambiguating questions that the system is initially unsure about. We also discuss the prevalence of student “small talk” not related to science topics, the pluses and minuses of this behavior, and how a system should respond to these conversational acts. We conclude with discussions and point to directions for potential future work.</abstract>
      <url hash="8765f1f4">W19-4435</url>
      <doi>10.18653/v1/W19-4435</doi>
      <bibkey>raamadhurai-etal-2019-curio</bibkey>
    </paper>
    <paper id="36">
      <title>Supporting content evaluation of student summaries by Idea Unit embedding</title>
      <author><first>Marcello</first><last>Gecchele</last></author>
      <author><first>Hiroaki</first><last>Yamada</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <author><first>Yasuyo</first><last>Sawaki</last></author>
      <pages>343–348</pages>
      <abstract>This paper discusses the computer-assisted content evaluation of summaries. We propose a method to make a correspondence between the segments of the source text and its summary. As a unit of the segment, we adopt “Idea Unit (IU)” which is proposed in Applied Linguistics. Introducing IUs enables us to make a correspondence even for the sentences that contain multiple ideas. The IU correspondence is made based on the similarity between vector representations of IU. An evaluation experiment with two source texts and 20 summaries showed that the proposed method is more robust against rephrased expressions than the conventional ROUGE-based baselines. Also, the proposed method outperformed the baselines in recall. We im-plemented the proposed method in a GUI tool“Segment Matcher” that aids teachers to estab-lish a link between corresponding IUs acrossthe summary and source text.</abstract>
      <url hash="d3c3cb05">W19-4436</url>
      <doi>10.18653/v1/W19-4436</doi>
      <bibkey>gecchele-etal-2019-supporting</bibkey>
    </paper>
    <paper id="37">
      <title>On Understanding the Relation between Expert Annotations of Text Readability and Target Reader Comprehension</title>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <author><first>Ivana</first><last>Lucic</last></author>
      <pages>349–359</pages>
      <abstract>Automatic readability assessment aims to ensure that readers read texts that they can comprehend. However, computational models are typically trained on texts created from the perspective of the text writer, not the target reader. There is little experimental research on the relationship between expert annotations of readability, reader’s language proficiency, and different levels of reading comprehension. To address this gap, we conducted a user study in which over a 100 participants read texts of different reading levels and answered questions created to test three forms of comprehension. Our results indicate that more than readability annotation or reader proficiency, it is the type of comprehension question asked that shows differences between reader responses - inferential questions were difficult for users of all levels of proficiency across reading levels. The data collected from this study will be released with this paper, which will, for the first time, provide a collection of 45 reader bench marked texts to evaluate readability assessment systems developed for adult learners of English. It can also potentially be useful for the development of question generation approaches in intelligent tutoring systems research.</abstract>
      <url hash="7836442e">W19-4437</url>
      <doi>10.18653/v1/W19-4437</doi>
      <bibkey>vajjala-lucic-2019-understanding</bibkey>
      <pwccode url="https://github.com/nishkalavallabhi/BEA19UserstudyData" additional="false">nishkalavallabhi/BEA19UserstudyData</pwccode>
    </paper>
    <paper id="38">
      <title>Measuring Text Complexity for <fixed-case>I</fixed-case>talian as a Second Language Learning Purposes</title>
      <author><first>Luciana</first><last>Forti</last></author>
      <author><first>Alfredo</first><last>Milani</last></author>
      <author><first>Luisa</first><last>Piersanti</last></author>
      <author><first>Filippo</first><last>Santarelli</last></author>
      <author><first>Valentino</first><last>Santucci</last></author>
      <author><first>Stefania</first><last>Spina</last></author>
      <pages>360–368</pages>
      <abstract>The selection of texts for second language learning purposes typically relies on teachers’ and test developers’ individual judgment of the observable qualitative properties of a text. Little or no consideration is generally given to the quantitative dimension within an evidence-based framework of reproducibility. This study aims to fill the gap by evaluating the effectiveness of an automatic tool trained to assess text complexity in the context of Italian as a second language learning. A dataset of texts labeled by expert test developers was used to evaluate the performance of three classifier models (decision tree, random forest, and support vector machine), which were trained using linguistic features measured quantitatively and extracted from the texts. The experimental analysis provided satisfactory results, also in relation to which kind of linguistic trait contributed the most to the final outcome.</abstract>
      <url hash="ff21bf2d">W19-4438</url>
      <doi>10.18653/v1/W19-4438</doi>
      <bibkey>forti-etal-2019-measuring</bibkey>
    </paper>
    <paper id="39">
      <title>Simple Construction of Mixed-Language Texts for Vocabulary Learning</title>
      <author><first>Adithya</first><last>Renduchintala</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>369–379</pages>
      <abstract>We present a machine foreign-language teacher that takes documents written in a student’s native language and detects situations where it can replace words with their foreign glosses such that new foreign vocabulary can be learned simply through reading the resulting mixed-language text. We show that it is possible to design such a machine teacher without any supervised data from (human) students. We accomplish this by modifying a cloze language model to incrementally learn new vocabulary items, and use this language model as a proxy for the word guessing and learning ability of real students. Our machine foreign-language teacher decides which subset of words to replace by consulting this language model. We evaluate three variants of our student proxy language models through a study on Amazon Mechanical Turk (MTurk). We find that MTurk “students” were able to guess the meanings of foreign words introduced by the machine teacher with high accuracy for both function words as well as content words in two out of the three models. In addition, we show that students are able to retain their knowledge about the foreign words after they finish reading the document.</abstract>
      <url hash="0b277ede">W19-4439</url>
      <doi>10.18653/v1/W19-4439</doi>
      <bibkey>renduchintala-etal-2019-simple</bibkey>
    </paper>
    <paper id="40">
      <title>Analyzing Linguistic Complexity and Accuracy in Academic Language Development of <fixed-case>G</fixed-case>erman across Elementary and Secondary School</title>
      <author><first>Zarah</first><last>Weiss</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>380–393</pages>
      <abstract>We track the development of writing complexity and accuracy in German students’ early academic language development from first to eighth grade. Combining an empirically broad approach to linguistic complexity with the high-quality error annotation included in the Karlsruhe Children’s Text corpus (Lavalley et al. 2015) used, we construct models of German academic language development that successfully identify the student’s grade level. We show that classifiers for the early years rely more on accuracy development, whereas development in secondary school is better characterized by increasingly complex language in all domains: linguistic system, language use, and human sentence processing characteristics. We demonstrate the generalizability and robustness of models using such a broad complexity feature set across writing topics.</abstract>
      <url hash="1f2e2ad6">W19-4440</url>
      <doi>10.18653/v1/W19-4440</doi>
      <bibkey>weiss-meurers-2019-analyzing</bibkey>
    </paper>
    <paper id="41">
      <title>Content Modeling for Automated Oral Proficiency Scoring System</title>
      <author><first>Su-Youn</first><last>Yoon</last></author>
      <author><first>Chong Min</first><last>Lee</last></author>
      <pages>394–401</pages>
      <abstract>We developed an automated oral proficiency scoring system for non-native English speakers’ spontaneous speech. Automated systems that score holistic proficiency are expected to assess a wide range of performance categories, and the content is one of the core performance categories. In order to assess the quality of the content, we trained a Siamese convolutional neural network (CNN) to model the semantic relationship between key points generated by experts and a test response. The correlation between human scores and Siamese CNN scores was comparable to human-human agreement (r=0.63), and it was higher than the baseline content features. The inclusion of Siamese CNN-based feature to the existing state-of-the-art automated scoring model achieved a small but statistically significant improvement. However, the new model suffered from score inflation for long atypical responses with serious content issues. We investigated the reasons of this score inflation by analyzing the associations with linguistic features and identifying areas strongly associated with the score errors.</abstract>
      <url hash="44a350f6">W19-4441</url>
      <doi>10.18653/v1/W19-4441</doi>
      <bibkey>yoon-lee-2019-content</bibkey>
    </paper>
    <paper id="42">
      <title>Learning Outcomes and Their Relatedness in a Medical Curriculum</title>
      <author><first>Sneha</first><last>Mondal</last></author>
      <author><first>Tejas</first><last>Dhamecha</last></author>
      <author><first>Shantanu</first><last>Godbole</last></author>
      <author><first>Smriti</first><last>Pathak</last></author>
      <author><first>Red</first><last>Mendoza</last></author>
      <author><first>K Gayathri</first><last>Wijayarathna</last></author>
      <author><first>Nabil</first><last>Zary</last></author>
      <author><first>Swarnadeep</first><last>Saha</last></author>
      <author><first>Malolan</first><last>Chetlur</last></author>
      <pages>402–411</pages>
      <abstract>A typical medical curriculum is organized in a hierarchy of instructional objectives called Learning Outcomes (LOs); a few thousand LOs span five years of study. Gaining a thorough understanding of the curriculum requires learners to recognize and apply related LOs across years, and across different parts of the curriculum. However, given the large scope of the curriculum, manually labeling related LOs is tedious, and almost impossible to scale. In this paper, we build a system that learns relationships between LOs, and we achieve up to human-level performance in the LO relationship extraction task. We then present an application where the proposed system is employed to build a map of related LOs and Learning Resources (LRs) pertaining to a virtual patient case. We believe that our system can help medical students grasp the curriculum better, within classroom as well as in Intelligent Tutoring Systems (ITS) settings.</abstract>
      <url hash="da703e12">W19-4442</url>
      <doi>10.18653/v1/W19-4442</doi>
      <bibkey>mondal-etal-2019-learning</bibkey>
    </paper>
    <paper id="43">
      <title>Measuring text readability with machine comprehension: a pilot study</title>
      <author><first>Marc</first><last>Benzahra</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>412–422</pages>
      <abstract>This article studies the relationship between text readability indice and automatic machine understanding systems. Our hypothesis is that the simpler a text is, the better it should be understood by a machine. We thus expect to a strong correlation between readability levels on the one hand, and performance of automatic reading systems on the other hand. We test this hypothesis with several understanding systems based on language models of varying strengths, measuring this correlation on two corpora of journalistic texts. Our results suggest that this correlation is rather small that existing comprehension systems are far to reproduce the gradual improvement of their performance on texts of decreasing complexity.</abstract>
      <url hash="6f17bb3f">W19-4443</url>
      <doi>10.18653/v1/W19-4443</doi>
      <bibkey>benzahra-yvon-2019-measuring</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="44">
      <title>Metaphors in Text Simplification: To change or not to change, that is the question</title>
      <author><first>Yulia</first><last>Clausen</last></author>
      <author><first>Vivi</first><last>Nastase</last></author>
      <pages>423–434</pages>
      <abstract>We present an analysis of metaphors in news text simplification. Using features that capture general and metaphor specific characteristics, we test whether we can automatically identify which metaphors will be changed or preserved, and whether there are features that have different predictive power for metaphors or literal words. The experiments show that the Age of Acquisition is the most distinctive feature for both metaphors and literal words. Features that capture Imageability and Concreteness are useful when used alone, but within the full set of features they lose their impact. Frequency of use seems to be the best feature to differentiate metaphors that should be changed and those to be preserved.</abstract>
      <url hash="35318d47">W19-4444</url>
      <doi>10.18653/v1/W19-4444</doi>
      <bibkey>clausen-nastase-2019-metaphors</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="45">
      <title>Application of an Automatic Plagiarism Detection System in a Large-scale Assessment of <fixed-case>E</fixed-case>nglish Speaking Proficiency</title>
      <author><first>Xinhao</first><last>Wang</last></author>
      <author><first>Keelan</first><last>Evanini</last></author>
      <author><first>Matthew</first><last>Mulholland</last></author>
      <author><first>Yao</first><last>Qian</last></author>
      <author><first>James V.</first><last>Bruno</last></author>
      <pages>435–443</pages>
      <abstract>This study aims to build an automatic system for the detection of plagiarized spoken responses in the context of an assessment of English speaking proficiency for non-native speakers. Classification models were trained to distinguish between plagiarized and non-plagiarized responses with two different types of features: text-to-text content similarity measures, which are commonly used in the task of plagiarism detection for written documents, and speaking proficiency measures, which were specifically designed for spontaneous speech and extracted using an automated speech scoring system. The experiments were first conducted on a large data set drawn from an operational English proficiency assessment across multiple years, and the best classifier on this heavily imbalanced data set resulted in an F1-score of 0.761 on the plagiarized class. This system was then validated on operational responses collected from a single administration of the assessment and achieved a recall of 0.897. The results indicate that the proposed system can potentially be used to improve the validity of both human and automated assessment of non-native spoken English.</abstract>
      <url hash="70a67f01">W19-4445</url>
      <doi>10.18653/v1/W19-4445</doi>
      <bibkey>wang-etal-2019-application</bibkey>
    </paper>
    <paper id="46">
      <title>Equity Beyond Bias in Language Technologies for Education</title>
      <author><first>Elijah</first><last>Mayfield</last></author>
      <author><first>Michael</first><last>Madaio</last></author>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>David</first><last>Gerritsen</last></author>
      <author><first>Brittany</first><last>McLaughlin</last></author>
      <author><first>Ezekiel</first><last>Dixon-Román</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>444–460</pages>
      <abstract>There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.</abstract>
      <url hash="b9d5e4d4">W19-4446</url>
      <doi>10.18653/v1/W19-4446</doi>
      <bibkey>mayfield-etal-2019-equity</bibkey>
    </paper>
    <paper id="47">
      <title>From Receptive to Productive: Learning to Use Confusing Words through Automatically Selected Example Sentences</title>
      <author><first>Chieh-Yang</first><last>Huang</last></author>
      <author><first>Yi-Ting</first><last>Huang</last></author>
      <author><first>MeiHua</first><last>Chen</last></author>
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <pages>461–471</pages>
      <abstract>Knowing how to use words appropriately has been a key to improving language proficiency. Previous studies typically discuss how students learn receptively to select the correct candidate from a set of confusing words in the fill-in-the-blank task where specific context is given. In this paper, we go one step further, assisting students to learn to use confusing words appropriately in a productive task: sentence translation. We leverage the GiveMe-Example system, which suggests example sentences for each confusing word, to achieve this goal. In this study, students learn to differentiate the confusing words by reading the example sentences, and then choose the appropriate word(s) to complete the sentence translation task. Results show students made substantial progress in terms of sentence structure. In addition, highly proficient students better managed to learn confusing words. In view of the influence of the first language on learners, we further propose an effective approach to improve the quality of the suggested sentences.</abstract>
      <url hash="7bda3b06">W19-4447</url>
      <doi>10.18653/v1/W19-4447</doi>
      <bibkey>huang-etal-2019-receptive</bibkey>
    </paper>
    <paper id="48">
      <title>Equipping Educational Applications with Domain Knowledge</title>
      <author><first>Tarek</first><last>Sakakini</last></author>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Jong Yoon</first><last>Lee</last></author>
      <author><first>Robert</first><last>Schloss</last></author>
      <author><first>JinJun</first><last>Xiong</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <pages>472–477</pages>
      <abstract>One of the challenges of building natural language processing (NLP) applications for education is finding a large domain-specific corpus for the subject of interest (e.g., history or science). To address this challenge, we propose a tool, Dexter, that extracts a subject-specific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations. We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in better performances than while using a general domain corpus, a heuristically constructed domain-specific corpus, and a corpus generated by a popular system: BootCaT.</abstract>
      <url hash="dbc16416">W19-4448</url>
      <doi>10.18653/v1/W19-4448</doi>
      <bibkey>sakakini-etal-2019-equipping</bibkey>
    </paper>
    <paper id="49">
      <title>The Unbearable Weight of Generating Artificial Errors for Grammatical Error Correction</title>
      <author><first>Phu Mon</first><last>Htut</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <pages>478–483</pages>
      <abstract>In this paper, we investigate the impact of using 4 recent neural models for generating artificial errors to help train the neural grammatical error correction models. We conduct a battery of experiments on the effect of data size, models, and comparison with a rule-based approach.</abstract>
      <url hash="b9295325">W19-4449</url>
      <doi>10.18653/v1/W19-4449</doi>
      <bibkey>htut-tetreault-2019-unbearable</bibkey>
    </paper>
    <paper id="50">
      <title>Automated Essay Scoring with Discourse-Aware Neural Models</title>
      <author><first>Farah</first><last>Nadeem</last></author>
      <author id="huy-nguyen-lls"><first>Huy</first><last>Nguyen</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>484–493</pages>
      <abstract>Automated essay scoring systems typically rely on hand-crafted features to predict essay quality, but such systems are limited by the cost of feature engineering. Neural networks offer an alternative to feature engineering, but they typically require more annotated data. This paper explores network structures, contextualized embeddings and pre-training strategies aimed at capturing discourse characteristics of essays. Experiments on three essay scoring tasks show benefits from all three strategies in different combinations, with simpler architectures being more effective when less training data is available.</abstract>
      <url hash="919b24d8">W19-4450</url>
      <doi>10.18653/v1/W19-4450</doi>
      <bibkey>nadeem-etal-2019-automated</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="51">
      <title>Modeling language learning using specialized Elo rating</title>
      <author><first>Jue</first><last>Hou</last></author>
      <author><first>Koppatz</first><last>Maximilian</last></author>
      <author><first>José María</first><last>Hoya Quecedo</last></author>
      <author><first>Nataliya</first><last>Stoyanova</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>494–506</pages>
      <abstract>Automatic assessment of the proficiency levels of the learner is a critical part of Intelligent Tutoring Systems. We present methods for assessment in the context of language learning. We use a specialized Elo formula used in conjunction with educational data mining. We simultaneously obtain ratings for the proficiency of the learners and for the difficulty of the linguistic concepts that the learners are trying to master. From the same data we also learn a graph structure representing a domain model capturing the relations among the concepts. This application of Elo provides ratings for learners and concepts which correlate well with subjective proficiency levels of the learners and difficulty levels of the concepts.</abstract>
      <url hash="c15954d2">W19-4451</url>
      <doi>10.18653/v1/W19-4451</doi>
      <bibkey>hou-etal-2019-modeling</bibkey>
    </paper>
    <paper id="52">
      <title>Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays</title>
      <author><first>Yanjun</first><last>Gao</last></author>
      <author><first>Alex</first><last>Driban</last></author>
      <author><first>Brennan</first><last>Xavier McManus</last></author>
      <author><first>Elena</first><last>Musi</last></author>
      <author><first>Patricia</first><last>Davies</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <pages>507–518</pages>
      <abstract>We present a unique dataset of student source-based argument essays to facilitate research on the relations between content, argumentation skills, and assessment. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the rubric, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays.</abstract>
      <url hash="7fd29ab4">W19-4452</url>
      <doi>10.18653/v1/W19-4452</doi>
      <bibkey>gao-etal-2019-rubric</bibkey>
      <pwccode url="https://github.com/psunlpgroup/SEAView" additional="false">psunlpgroup/SEAView</pwccode>
    </paper>
  </volume>
  <volume id="45">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Argument Mining</booktitle>
      <url hash="045a28fa">W19-45</url>
      <editor><first>Benno</first><last>Stein</last></editor>
      <editor><first>Henning</first><last>Wachsmuth</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>argmining</venue>
    </meta>
    <frontmatter>
      <url hash="6e130b75">W19-4500</url>
      <bibkey>ws-2019-argument</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Segmentation of Argumentative Texts with Contextualised Word Representations</title>
      <author><first>Georgios</first><last>Petasis</last></author>
      <pages>1–10</pages>
      <abstract>The segmentation of argumentative units is an important subtask of argument mining, which is frequently addressed at a coarse granularity, usually assuming argumentative units to be no smaller than sentences. Approaches focusing at the clause-level granularity, typically address the task as sequence labeling at the token level, aiming to classify whether a token begins, is inside, or is outside of an argumentative unit. Most approaches exploit highly engineered, manually constructed features, and algorithms typically used in sequential tagging – such as Conditional Random Fields, while more recent approaches try to exploit manually constructed features in the context of deep neural networks. In this context, we examined to what extend recent advances in sequential labelling allow to reduce the need for highly sophisticated, manually constructed features, and whether limiting features to embeddings, pre-trained on large corpora is a promising approach. Evaluation results suggest the examined models and approaches can exhibit comparable performance, minimising the need for feature engineering.</abstract>
      <url hash="c0d9fda4">W19-4501</url>
      <doi>10.18653/v1/W19-4501</doi>
      <bibkey>petasis-2019-segmentation</bibkey>
    </paper>
    <paper id="2">
      <title>A Cascade Model for Proposition Extraction in Argumentation</title>
      <author><first>Yohan</first><last>Jo</last></author>
      <author><first>Jacky</first><last>Visser</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>11–24</pages>
      <abstract>We present a model to tackle a fundamental but understudied problem in computational argumentation: proposition extraction. Propositions are the basic units of an argument and the primary building blocks of most argument mining systems. However, they are usually substituted by argumentative discourse units obtained via surface-level text segmentation, which may yield text segments that lack semantic information necessary for subsequent argument mining processes. In contrast, our cascade model aims to extract complete propositions by handling anaphora resolution, text segmentation, reported speech, questions, imperatives, missing subjects, and revision. We formulate each task as a computational problem and test various models using a corpus of the 2016 U.S. presidential debates. We show promising performance for some tasks and discuss main challenges in proposition extraction.</abstract>
      <url hash="5828dd8c">W19-4502</url>
      <doi>10.18653/v1/W19-4502</doi>
      <bibkey>jo-etal-2019-cascade</bibkey>
    </paper>
    <paper id="3">
      <title>Dissecting Content and Context in Argumentative Relation Analysis</title>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <pages>25–34</pages>
      <abstract>When assessing relations between argumentative units (e.g., support or attack), computational systems often exploit disclosing indicators or markers that are not part of elementary argumentative units (EAUs) themselves, but are gained from their context (position in paragraph, preceding tokens, etc.). We show that this dependency is much stronger than previously assumed. In fact, we show that by completely masking the EAU text spans and only feeding information from their context, a competitive system may function even better. We argue that an argument analysis system that relies more on discourse context than the argument’s content is unsafe, since it can easily be tricked. To alleviate this issue, we separate argumentative units from their context such that the system is forced to model and rely on an EAU’s content. We show that the resulting classification system is more robust, and argue that such models are better suited for predicting argumentative relations across documents.</abstract>
      <url hash="063edf6e">W19-4503</url>
      <doi>10.18653/v1/W19-4503</doi>
      <bibkey>opitz-frank-2019-dissecting</bibkey>
    </paper>
    <paper id="4">
      <title>Aligning Discourse and Argumentation Structures using Subtrees and Redescription Mining</title>
      <author><first>Laurine</first><last>Huber</last></author>
      <author><first>Yannick</first><last>Toussaint</last></author>
      <author><first>Charlotte</first><last>Roze</last></author>
      <author><first>Mathilde</first><last>Dargnat</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <pages>35–40</pages>
      <abstract>In this paper, we investigate similarities between discourse and argumentation structures by aligning subtrees in a corpus containing both annotations. Contrary to previous works, we focus on comparing sub-structures and not only relations matches. Using data mining techniques, we show that discourse and argumentation most often align well, and the double annotation allows to derive a mapping between structures. Moreover, this approach enables the study of similarities between discourse structures and differences in their expressive power.</abstract>
      <url hash="27a9b332">W19-4504</url>
      <doi>10.18653/v1/W19-4504</doi>
      <bibkey>huber-etal-2019-aligning</bibkey>
    </paper>
    <paper id="5">
      <title>Transferring Knowledge from Discourse to Arguments: A Case Study with Scientific Abstracts</title>
      <author><first>Pablo</first><last>Accuosto</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>41–51</pages>
      <abstract>In this work we propose to leverage resources available with discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. In particular, we implement and evaluate a transfer learning approach in which contextualized representations learned from discourse parsing tasks are used as input of argument mining models. As a pilot application, we explore the feasibility of using automatically identified argumentative components and relations to predict the acceptance of papers in computer science venues. In order to conduct our experiments, we propose an annotation scheme for argumentative units and relations and use it to enrich an existing corpus with an argumentation layer.</abstract>
      <url hash="1403df0d">W19-4505</url>
      <doi>10.18653/v1/W19-4505</doi>
      <bibkey>accuosto-saggion-2019-transferring</bibkey>
    </paper>
    <paper id="6">
      <title>The <fixed-case>S</fixed-case>wedish <fixed-case>P</fixed-case>oli<fixed-case>G</fixed-case>raph: A Semantic Graph for Argument Mining of <fixed-case>S</fixed-case>wedish Parliamentary Data</title>
      <author><first>Stian Rødven</first><last>Eide</last></author>
      <pages>52–57</pages>
      <abstract>As part of a larger project on argument mining of Swedish parliamentary data, we have created a semantic graph that, together with named entity recognition and resolution (NER), should make it easier to establish connections between arguments in a given debate. The graph is essentially a semantic database that keeps track of Members of Parliament (MPs), in particular their presence in the parliament and activity in debates, but also party affiliation and participation in commissions. The hope is that the Swedish PoliGraph will enable us to perform named entity resolution on debates in the Swedish parliament with a high accuracy, with the aim of determining to whom an argument is directed.</abstract>
      <url hash="19904ee3">W19-4506</url>
      <doi>10.18653/v1/W19-4506</doi>
      <bibkey>eide-2019-swedish</bibkey>
    </paper>
    <paper id="7">
      <title>Towards Effective Rebuttal: Listening Comprehension Using Corpus-Wide Claim Mining</title>
      <author><first>Tamar</first><last>Lavee</last></author>
      <author><first>Matan</first><last>Orbach</last></author>
      <author><first>Lili</first><last>Kotlerman</last></author>
      <author><first>Yoav</first><last>Kantor</last></author>
      <author><first>Shai</first><last>Gretz</last></author>
      <author><first>Lena</first><last>Dankin</last></author>
      <author><first>Michal</first><last>Jacovi</last></author>
      <author><first>Yonatan</first><last>Bilu</last></author>
      <author><first>Ranit</first><last>Aharonov</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>58–66</pages>
      <abstract>Engaging in a live debate requires, among other things, the ability to effectively rebut arguments claimed by your opponent. In particular, this requires identifying these arguments. Here, we suggest doing so by automatically mining claims from a corpus of news articles containing billions of sentences, and searching for them in a given speech. This raises the question of whether such claims indeed correspond to those made in spoken speeches. To this end, we collected a large dataset of 400 speeches in English discussing 200 controversial topics, mined claims for each topic, and asked annotators to identify the mined claims mentioned in each speech. Results show that in the vast majority of speeches debaters indeed make use of such claims. In addition, we present several baselines for the automatic detection of mined claims in speeches, forming the basis for future work. All collected data is freely available for research.</abstract>
      <url hash="9eab8510">W19-4507</url>
      <doi>10.18653/v1/W19-4507</doi>
      <bibkey>lavee-etal-2019-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Lexicon Guided Attentive Neural Network Model for Argument Mining</title>
      <author><first>Jian-Fu</first><last>Lin</last></author>
      <author><first>Kuo Yu</first><last>Huang</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>67–73</pages>
      <abstract>Identification of argumentative components is an important stage of argument mining. Lexicon information is reported as one of the most frequently used features in the argument mining research. In this paper, we propose a methodology to integrate lexicon information into a neural network model by attention mechanism. We conduct experiments on the UKP dataset, which is collected from heterogeneous sources and contains several text types, e.g., microblog, Wikipedia, and news. We explore lexicons from various application scenarios such as sentiment analysis and emotion detection. We also compare the experimental results of leveraging different lexicons.</abstract>
      <url hash="5a8204a1">W19-4508</url>
      <doi>10.18653/v1/W19-4508</doi>
      <bibkey>lin-etal-2019-lexicon</bibkey>
    </paper>
    <paper id="9">
      <title>Is It Worth the Attention? A Comparative Evaluation of Attention Layers for Argument Unit Segmentation</title>
      <author><first>Maximilian</first><last>Spliethöver</last></author>
      <author><first>Jonas</first><last>Klaff</last></author>
      <author><first>Hendrik</first><last>Heuer</last></author>
      <pages>74–82</pages>
      <abstract>Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.</abstract>
      <url hash="24adbbca">W19-4509</url>
      <doi>10.18653/v1/W19-4509</doi>
      <bibkey>spliethover-etal-2019-worth</bibkey>
      <pwccode url="https://gitlab.informatik.uni-bremen.de/covis1819/worth-the-attention" additional="false">covis1819/worth-the-attention</pwccode>
    </paper>
    <paper id="10">
      <title>Argument Component Classification by Relation Identification by Neural Network and <fixed-case>T</fixed-case>ext<fixed-case>R</fixed-case>ank</title>
      <author><first>Mamoru</first><last>Deguchi</last></author>
      <author><first>Kazunori</first><last>Yamaguchi</last></author>
      <pages>83–91</pages>
      <abstract>In recent years, argumentation mining, which automatically extracts the structure of argumentation from unstructured documents such as essays and debates, is gaining attention. For argumentation mining applications, argument-component classification is an important subtask. The existing methods can be classified into supervised methods and unsupervised methods. Supervised document classification performs classification using a single sentence without relying on the whole document. On the other hand, unsupervised document classification has the advantage of being able to use the whole document, but accuracy of these methods is not so high. In this paper, we propose a method for argument-component classification that combines relation identification by neural networks and TextRank to integrate relation informations (i.e. the strength of the relation). This method can use argumentation-specific knowledge by employing a supervised learning on a corpus while maintaining the advantage of using the whole document. Experiments on two corpora, one consisting of student essay and the other of Wikipedia articles, show the effectiveness of this method.</abstract>
      <url hash="1f7443a6">W19-4510</url>
      <doi>10.18653/v1/W19-4510</doi>
      <bibkey>deguchi-yamaguchi-2019-argument</bibkey>
    </paper>
    <paper id="11">
      <title>Argumentative Evidences Classification and Argument Scheme Detection Using Tree Kernels</title>
      <author><first>Davide</first><last>Liga</last></author>
      <pages>92–97</pages>
      <abstract>The purpose of this study is to deploy a novel methodology for classifying different argumentative support (supporting evidences) in arguments, without considering the context. The proposed methodology is based on the idea that the use of Tree Kernel algorithms can be a good way to discriminate between different types of argumentative stances without the need of highly engineered features. This can be useful in different Argumentation Mining sub-tasks. This work provides an example of classifier built using a Tree Kernel method, which can discriminate between different kinds of argumentative support with a high accuracy. The ability to distinguish different kinds of support is, in fact, a key step toward Argument Scheme classification.</abstract>
      <url hash="f729d6f4">W19-4511</url>
      <doi>10.18653/v1/W19-4511</doi>
      <bibkey>liga-2019-argumentative</bibkey>
    </paper>
    <paper id="12">
      <title>The Utility of Discourse Parsing Features for Predicting Argumentation Structure</title>
      <author><first>Freya</first><last>Hewett</last></author>
      <author><first>Roshan</first><last>Prakash Rane</last></author>
      <author><first>Nina</first><last>Harlacher</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>98–103</pages>
      <abstract>Research on argumentation mining from text has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the ‘argumentative microtexts’ (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the microtexts, we find that PDTB features can indeed improve its performance.</abstract>
      <url hash="8bd8d3ef">W19-4512</url>
      <doi>10.18653/v1/W19-4512</doi>
      <bibkey>hewett-etal-2019-utility</bibkey>
    </paper>
    <paper id="13">
      <title>Detecting Argumentative Discourse Acts with Linguistic Alignment</title>
      <author><first>Timothy</first><last>Niven</last></author>
      <author><first>Hung-Yu</first><last>Kao</last></author>
      <pages>104–112</pages>
      <abstract>We report the results of preliminary investigations into the relationship between linguistic alignment and dialogical argumentation at the level of discourse acts. We annotated a proof of concept dataset with illocutions and transitions at the comment level based on Inference Anchoring Theory. We estimated linguistic alignment across discourse acts and found significant variation. Alignment features calculated at the dyad level are found to be useful for detecting a range of argumentative discourse acts.</abstract>
      <url hash="b9dac675">W19-4513</url>
      <doi>10.18653/v1/W19-4513</doi>
      <bibkey>niven-kao-2019-detecting</bibkey>
      <pwccode url="https://github.com/IKMLab/argalign1" additional="false">IKMLab/argalign1</pwccode>
    </paper>
    <paper id="14">
      <title>Annotation of Rhetorical Moves in Biochemistry Articles</title>
      <author><first>Mohammed</first><last>Alliheedi</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <author><first>Robin</first><last>Cohen</last></author>
      <pages>113–123</pages>
      <abstract>This paper focuses on the real world application of scientific writing and on determining rhetorical moves, an important step in establishing the argument structure of biomedical articles. Using the observation that the structure of scholarly writing in laboratory-based experimental sciences closely follows laboratory procedures, we examine most closely the Methods section of the texts and adopt an approach of identifying rhetorical moves that are procedure-oriented. We also propose a verb-centric frame semantics with an effective set of semantic roles in order to support the analysis. These components are designed to support a computational model that extends a promising proposal of appropriate rhetorical moves for this domain, but one which is merely descriptive. Our work also contributes to the understanding of argument-related annotation schemes. In particular, we conduct a detailed study with human annotators to confirm that our selection of semantic roles is effective in determining the underlying rhetorical structure of existing biomedical articles in an extensive dataset. The annotated dataset that we produce provides the important knowledge needed for our ultimate goal of analyzing biochemistry articles.</abstract>
      <url hash="635db10d">W19-4514</url>
      <doi>10.18653/v1/W19-4514</doi>
      <bibkey>alliheedi-etal-2019-annotation</bibkey>
    </paper>
    <paper id="15">
      <title>Evaluation of Scientific Elements for Text Similarity in Biomedical Publications</title>
      <author><first>Mariana</first><last>Neves</last></author>
      <author><first>Daniel</first><last>Butzke</last></author>
      <author><first>Barbara</first><last>Grune</last></author>
      <pages>124–135</pages>
      <abstract>Rhetorical elements from scientific publications provide a more structured view of the document and allow algorithms to focus on particular parts of the text. We surveyed the literature for previously proposed schemes for rhetorical elements and present an overview of its current state of the art. We also searched for available tools using these schemes and applied four tools for our particular task of ranking biomedical abstracts based on text similarity. Comparison of the tools with two strong baselines shows that the predictions provided by the ArguminSci tool can support our use case of mining alternative methods for animal experiments.</abstract>
      <url hash="70cff855">W19-4515</url>
      <doi>10.18653/v1/W19-4515</doi>
      <bibkey>neves-etal-2019-evaluation</bibkey>
      <pwccode url="https://github.com/mariananeves/scientific-elements-text-similarity" additional="false">mariananeves/scientific-elements-text-similarity</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed-rct">PubMed RCT</pwcdataset>
    </paper>
    <paper id="16">
      <title>Categorizing Comparative Sentences</title>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <author><first>Alexander</first><last>Bondarenko</last></author>
      <author><first>Mirco</first><last>Franzek</last></author>
      <author><first>Matthias</first><last>Hagen</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>136–145</pages>
      <abstract>We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., “Python has better NLP libraries than MATLAB” → Python, better, MATLAB). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27% of the sentences contain an oriented comparison in the sense of “better” or “worse”). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85% in our experimental evaluation. The model can be used to extract comparative sentences for pro/con argumentation in comparative / argument search engines or debating technologies.</abstract>
      <url hash="9e3955c9">W19-4516</url>
      <doi>10.18653/v1/W19-4516</doi>
      <bibkey>panchenko-etal-2019-categorizing</bibkey>
      <pwccode url="https://github.com/uhh-lt/comparative" additional="true">uhh-lt/comparative</pwccode>
    </paper>
    <paper id="17">
      <title>Ranking Passages for Argument Convincingness</title>
      <author><first>Peter</first><last>Potash</last></author>
      <author><first>Adam</first><last>Ferguson</last></author>
      <author><first>Timothy J.</first><last>Hazen</last></author>
      <pages>146–155</pages>
      <abstract>In data ranking applications, pairwise annotation is often more consistent than cardinal annotation for learning ranking models. We examine this in a case study on ranking text passages for argument convincingness. Our task is to choose text passages that provide the highest-quality, most-convincing arguments for opposing sides of a topic. Using data from a deployed system within the Bing search engine, we construct a pairwise-labeled dataset for argument convincingness that is substantially more comprehensive in topical coverage compared to existing public resources. We detail the process of extracting topical passages for queries submitted to a search engine, creating annotated sets of passages aligned to different stances on a topic, and assessing argument convincingness of passages using pairwise annotation. Using a state-of-the-art convincingness model, we evaluate several methods for using pairwise-annotated data examples to train models for ranking passages. Our results show pairwise training outperforms training that regresses to a target score for each passage. Our results also show a simple ‘win-rate’ score is a better regression target than the previously proposed page-rank target. Lastly, addressing the need to filter noisy crowd-sourced annotations when constructing a dataset, we show that filtering for transitivity within pairwise annotations is more effective than filtering based on annotation confidence measures for individual examples.</abstract>
      <url hash="8a261bbc">W19-4517</url>
      <doi>10.18653/v1/W19-4517</doi>
      <bibkey>potash-etal-2019-ranking</bibkey>
    </paper>
    <paper id="18">
      <title>Gradual Argumentation Evaluation for Stance Aggregation in Automated Fake News Detection</title>
      <author><first>Neema</first><last>Kotonya</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <pages>156–166</pages>
      <abstract>Stance detection plays a pivot role in fake news detection. The task involves determining the point of view or stance – for or against – a text takes towards a claim. One very important stage in employing stance detection for fake news detection is the aggregation of multiple stance labels from different text sources in order to compute a prediction for the veracity of a claim. Typically, aggregation is treated as a credibility-weighted average of stance predictions. In this work, we take the novel approach of applying, for aggregation, a gradual argumentation semantics to bipolar argumentation frameworks mined using stance detection. Our empirical evaluation shows that our method results in more accurate veracity predictions.</abstract>
      <url hash="0e2722b8">W19-4518</url>
      <doi>10.18653/v1/W19-4518</doi>
      <bibkey>kotonya-toni-2019-gradual</bibkey>
    </paper>
    <paper id="19">
      <title>Persuasion of the Undecided: Language vs. the Listener</title>
      <author><first>Liane</first><last>Longpre</last></author>
      <author><first>Esin</first><last>Durmus</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>167–176</pages>
      <abstract>This paper examines the factors that govern persuasion for a priori UNDECIDED versus DECIDED audience members in the context of on-line debates. We separately study two types of influences: linguistic factors — features of the language of the debate itself; and audience factors — features of an audience member encoding demographic information, prior beliefs, and debate platform behavior. In a study of users of a popular debate platform, we find first that different combinations of linguistic features are critical for predicting persuasion outcomes for UNDECIDED versus DECIDED members of the audience. We additionally find that audience factors have more influence on predicting the side (PRO/CON) that persuaded UNDECIDED users than for DECIDED users that flip their stance to the opposing side. Our results emphasize the importance of considering the undecided and decided audiences separately when studying linguistic factors of persuasion.</abstract>
      <url hash="e0cc0062">W19-4519</url>
      <doi>10.18653/v1/W19-4519</doi>
      <bibkey>longpre-etal-2019-persuasion</bibkey>
    </paper>
    <paper id="20">
      <title>Towards Assessing Argumentation Annotation - A First Step</title>
      <author><first>Anna</first><last>Lindahl</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Jacobo</first><last>Rouces</last></author>
      <pages>177–186</pages>
      <abstract>This paper presents a first attempt at using Walton’s argumentation schemes for annotating arguments in Swedish political text and assessing the feasibility of using this particular set of schemes with two linguistically trained annotators. The texts are not pre-annotated with argumentation structure beforehand. The results show that the annotators differ both in number of annotated arguments and selection of the conclusion and premises which make up the arguments. They also differ in their labeling of the schemes, but grouping the schemes increases their agreement. The outcome from this will be used to develop guidelines for future annotations.</abstract>
      <url hash="3a912cd5">W19-4520</url>
      <doi>10.18653/v1/W19-4520</doi>
      <bibkey>lindahl-etal-2019-towards</bibkey>
    </paper>
  </volume>
  <volume id="46">
    <meta>
      <booktitle>Proceedings of the Fourth Arabic Natural Language Processing Workshop</booktitle>
      <url hash="a50d4d69">W19-46</url>
      <editor><first>Wassim</first><last>El-Hajj</last></editor>
      <editor><first>Lamia Hadrich</first><last>Belguith</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Imed</first><last>Zitouni</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Mahmoud</first><last>El-Haj</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>wanlp</venue>
    </meta>
    <frontmatter>
      <url hash="0fe4d36d">W19-4600</url>
      <bibkey>ws-2019-arabic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings</title>
      <author><first>Marimuthu</first><last>Kalimuthu</last></author>
      <author><first>Michael</first><last>Barz</last></author>
      <author><first>Daniel</first><last>Sonntag</last></author>
      <pages>1–10</pages>
      <abstract>We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training. In this paper, we propose a novel query strategy for selecting “unlabeled” samples from a new domain based on sentence embeddings for Arabic. We accelerate the fine-tuning process of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to active learning, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50% of the annotation costs without loss in quality, demonstrating the effectiveness of our approach.</abstract>
      <url hash="c2030301">W19-4601</url>
      <doi>10.18653/v1/W19-4601</doi>
      <bibkey>kalimuthu-etal-2019-incremental</bibkey>
      <pwccode url="https://github.com/DFKI-Interactive-Machine-Learning/AraSIF" additional="false">DFKI-Interactive-Machine-Learning/AraSIF</pwccode>
    </paper>
    <paper id="2">
      <title>Morphology-aware Word-Segmentation in Dialectal <fixed-case>A</fixed-case>rabic Adaptation of Neural Machine Translation</title>
      <author><first>Ahmed</first><last>Tawfik</last></author>
      <author><first>Mahitab</first><last>Emam</last></author>
      <author><first>Khaled</first><last>Essam</last></author>
      <author><first>Robert</first><last>Nabil</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <pages>11–17</pages>
      <abstract>Parallel corpora available for building machine translation (MT) models for dialectal Arabic (DA) are rather limited. The scarcity of resources has prompted the use of Modern Standard Arabic (MSA) abundant resources to complement the limited dialectal resource. However, dialectal clitics often differ between MSA and DA. This paper compares morphology-aware DA word segmentation to other word segmentation approaches like Byte Pair Encoding (BPE) and Sub-word Regularization (SR). A set of experiments conducted on Egyptian Arabic (EA), Levantine Arabic (LA), and Gulf Arabic (GA) show that a sufficiently accurate morphology-aware segmentation used in conjunction with BPE outperforms the other word segmentation approaches.</abstract>
      <url hash="60db9239">W19-4602</url>
      <doi>10.18653/v1/W19-4602</doi>
      <bibkey>tawfik-etal-2019-morphology</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>POS</fixed-case> Tagging for Improving Code-Switching Identification in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Mohammed</first><last>Attia</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Ali</first><last>Elkahky</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <pages>18–29</pages>
      <abstract>When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language. This paper explores the possibility of utilizing this pattern in improving code-switching identification between Modern Standard Arabic (MSA) and Egyptian Arabic (EA). We try to answer the question of how strong is the POS signal in word-level code-switching identification. We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9% on the development set and 1.0% on the test set. We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the dialectal language while the majority of content words come from the standard language.</abstract>
      <url hash="e22f0c51">W19-4603</url>
      <doi>10.18653/v1/W19-4603</doi>
      <bibkey>attia-etal-2019-pos</bibkey>
    </paper>
    <paper id="4">
      <title>Syntax-Ignorant N-gram Embeddings for Sentiment Analysis of <fixed-case>A</fixed-case>rabic Dialects</title>
      <author><first>Hala</first><last>Mulki</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Mourad</first><last>Gridach</last></author>
      <author><first>Ismail</first><last>Babaoğlu</last></author>
      <pages>30–39</pages>
      <abstract>Arabic sentiment analysis models have employed compositional embedding features to represent the Arabic dialectal content. These embeddings are usually composed via ordered, syntax-aware composition functions and learned within deep neural frameworks. With the free word order and the varying syntax nature across the different Arabic dialects, a sentiment analysis system developed for one dialect might not be efficient for the others. Here we present syntax-ignorant n-gram embeddings to be used in sentiment analysis of several Arabic dialects. The proposed embeddings were composed and learned using an unordered composition function and a shallow neural model. Five datasets of different dialects were used to evaluate the produced embeddings in the sentiment analysis task. The obtained results revealed that, our syntax-ignorant embeddings could outperform word2vec model and doc2vec both variant models in addition to hand-crafted system baselines, while a competent performance was noticed towards baseline systems that adopted more complicated neural architectures.</abstract>
      <url hash="6fda96f6">W19-4604</url>
      <doi>10.18653/v1/W19-4604</doi>
      <bibkey>mulki-etal-2019-syntax</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tsac">TSAC</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>A</fixed-case>rb<fixed-case>E</fixed-case>ng<fixed-case>V</fixed-case>ec : <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>nglish Cross-Lingual Word Embedding Model</title>
      <author><first>Raki</first><last>Lachraf</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Youcef</first><last>Ayachi</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>40–48</pages>
      <abstract>Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas. In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models. To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences. In addition, we perform both extrinsic and intrinsic evaluations for the different word embedding model variants. The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task.</abstract>
      <url hash="f29e5440">W19-4605</url>
      <doi>10.18653/v1/W19-4605</doi>
      <bibkey>lachraf-etal-2019-arbengvec</bibkey>
    </paper>
    <paper id="6">
      <title>Homograph Disambiguation through Selective Diacritic Restoration</title>
      <author><first>Sawsan</first><last>Alqahtani</last></author>
      <author><first>Hanan</first><last>Aldarmaki</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>49–59</pages>
      <abstract>Lexical ambiguity, a challenging phenomenon in all natural languages, is particularly prevalent for languages with diacritics that tend to be omitted in writing, such as Arabic. Omitting diacritics leads to an increase in the number of homographs: different words with the same spelling. Diacritic restoration could theoretically help disambiguate these words, but in practice, the increase in overall sparsity leads to performance degradation in NLP applications. In this paper, we propose approaches for automatically marking a subset of words for diacritic restoration, which leads to selective homograph disambiguation. Compared to full or no diacritic restoration, these approaches yield selectively-diacritized datasets that balance sparsity and lexical disambiguation. We evaluate the various selection strategies extrinsically on several downstream applications: neural machine translation, part-of-speech tagging, and semantic textual similarity. Our experiments on Arabic show promising results, where our devised strategies on selective diacritization lead to a more balanced and consistent performance in downstream applications.</abstract>
      <url hash="8389a259">W19-4606</url>
      <doi>10.18653/v1/W19-4606</doi>
      <bibkey>alqahtani-etal-2019-homograph</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>A</fixed-case>rabic Named Entity Recognition: What Works and What’s Next</title>
      <author><first>Liyuan</first><last>Liu</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>60–67</pages>
      <abstract>This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com. The proposed model integrates various tailored techniques together, including representation learning, feature engineering, sequence labeling, and ensemble learning. The final model achieves a test F_1 score of 75.82% on the AQMAR dataset and outperforms baselines by a large margin. Detailed analyses are conducted to reveal both its strengths and limitations. Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data. All implementations and pre-trained models are made public.</abstract>
      <url hash="b1b70c0f">W19-4607</url>
      <doi>10.18653/v1/W19-4607</doi>
      <bibkey>liu-etal-2019-arabic</bibkey>
    </paper>
    <paper id="8">
      <title>h<fixed-case>ULM</fixed-case>on<fixed-case>A</fixed-case>: The Universal Language Model in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Obeida</first><last>ElJundi</last></author>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Nour</first><last>El Droubi</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <author><first>Wassim</first><last>El-Hajj</last></author>
      <author><first>Khaled</first><last>Shaban</last></author>
      <pages>68–77</pages>
      <abstract>Arabic is a complex language with limited resources which makes it challenging to produce accurate text classification tasks such as sentiment analysis. The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English. TL models are pre-trained on large corpora, and then fine-tuned on task-specific datasets. In particular, universal language models (ULMs), such as recently developed BERT, have achieved state-of-the-art results in various NLP tasks in English. In this paper, we hypothesize that similar success can be achieved for Arabic. The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hULMonA - حلمنا meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multi-lingual BERT can also be used for Arabic. We then conduct a benchmark study to evaluate both ULM successes with Arabic sentiment analysis. Experiment results show that the developed hULMonA and multi-lingual ULM are able to generalize well to multiple Arabic data sets and achieve new state of the art results in Arabic Sentiment Analysis for some of the tested sets.</abstract>
      <url hash="453e9ef5">W19-4608</url>
      <doi>10.18653/v1/W19-4608</doi>
      <bibkey>eljundi-etal-2019-hulmona</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
    </paper>
    <paper id="9">
      <title>Neural Models for Detecting Binary Semantic Textual Similarity for <fixed-case>A</fixed-case>lgerian and <fixed-case>MSA</fixed-case></title>
      <author><first>Wafia</first><last>Adouane</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>78–87</pages>
      <abstract>We explore the extent to which neural networks can learn to identify semantically equivalent sentences from a small variable dataset using an end-to-end training. We collect a new noisy non-standardised user-generated Algerian (ALG) dataset and also translate it to Modern Standard Arabic (MSA) which serves as its regularised counterpart. We compare the performance of various models on both datasets and report the best performing configurations. The results show that relatively simple models composed of 2 LSTM layers outperform by far other more sophisticated attention-based architectures, for both ALG and MSA datasets.</abstract>
      <url hash="47a45a76">W19-4609</url>
      <doi>10.18653/v1/W19-4609</doi>
      <bibkey>adouane-etal-2019-neural</bibkey>
    </paper>
    <paper id="10">
      <title>Constrained Sequence-to-sequence <fixed-case>S</fixed-case>emitic Root Extraction for Enriching Word Embeddings</title>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Xingyu</first><last>Fu</last></author>
      <author><first>Aseel</first><last>Addawood</last></author>
      <author><first>Nahil</first><last>Sobh</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>88–96</pages>
      <abstract>In this paper, we tackle the problem of “root extraction” from words in the Semitic language family. A challenge in applying natural language processing techniques to these languages is the data sparsity problem that arises from their rich internal morphology, where the substructure is inherently non-concatenative and morphemes are interdigitated in word formation. While previous automated methods have relied on human-curated rules or multiclass classification, they have not fully leveraged the various combinations of regular, sequential concatenative morphology within the words and the internal interleaving within templatic stems of roots and patterns. To address this, we propose a constrained sequence-to-sequence root extraction method. Experimental results show our constrained model outperforms a variety of methods at root extraction. Furthermore, by enriching word embeddings with resulting decompositions, we show improved results on word analogy, word similarity, and language modeling tasks.</abstract>
      <url hash="f37948a8">W19-4610</url>
      <doi>10.18653/v1/W19-4610</doi>
      <bibkey>el-kishky-etal-2019-constrained</bibkey>
    </paper>
    <paper id="11">
      <title>En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects</title>
      <author><first>Taghreed</first><last>Alqaisi</last></author>
      <author><first>Simon</first><last>O’Keefe</last></author>
      <pages>97–107</pages>
      <abstract>This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA). We investigate the effect of sentence length and embedding size on the learning process. Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the ATB and D0 schemes in all different training settings.</abstract>
      <url hash="231f7024">W19-4611</url>
      <doi>10.18653/v1/W19-4611</doi>
      <bibkey>alqaisi-okeefe-2019-en</bibkey>
    </paper>
    <paper id="12">
      <title>Neural <fixed-case>A</fixed-case>rabic Question Answering</title>
      <author><first>Hussein</first><last>Mozannar</last></author>
      <author><first>Elie</first><last>Maamary</last></author>
      <author><first>Karl</first><last>El Hajal</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>108–118</pages>
      <abstract>This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.</abstract>
      <url hash="a1b82b44">W19-4612</url>
      <doi>10.18653/v1/W19-4612</doi>
      <bibkey>mozannar-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/husseinmozannar/SOQAL" additional="false">husseinmozannar/SOQAL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="13">
      <title>Segmentation for Domain Adaptation in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Mohammed</first><last>Attia</last></author>
      <author><first>Ali</first><last>Elkahky</last></author>
      <pages>119–129</pages>
      <abstract>Segmentation serves as an integral part in many NLP applications including Machine Translation, Parsing, and Information Retrieval. When a model trained on the standard language is applied to dialects, the accuracy drops dramatically. However, there are more lexical items shared by the standard language and dialects than can be found by mere surface word matching. This shared lexicon is obscured by a lot of cliticization, gemination, and character repetition. In this paper, we prove that segmentation and base normalization of dialects can help in domain adaptation by reducing data sparseness. Segmentation will improve a system performance by reducing the number of OOVs, help isolate the differences and allow better utilization of the commonalities. We show that adding a small amount of dialectal segmentation training data reduced OOVs by 5% and remarkably improves POS tagging for dialects by 7.37% f-score, even though no dialect-specific POS training data is included.</abstract>
      <url hash="3176093d">W19-4613</url>
      <doi>10.18653/v1/W19-4613</doi>
      <bibkey>attia-elkahky-2019-segmentation</bibkey>
    </paper>
    <paper id="14">
      <title>Assessing <fixed-case>A</fixed-case>rabic Weblog Credibility via Deep Co-learning</title>
      <author><first>Chadi</first><last>Helwe</last></author>
      <author><first>Shady</first><last>Elbassuoni</last></author>
      <author><first>Ayman</first><last>Al Zaatari</last></author>
      <author><first>Wassim</first><last>El-Hajj</last></author>
      <pages>130–136</pages>
      <abstract>Assessing the credibility of online content has garnered a lot of attention lately. We focus on one such type of online content, namely weblogs or blogs for short. Some recent work attempted the task of automatically assessing the credibility of blogs, typically via machine learning. However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust machine learning models for this difficult task. To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs. In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data. Each one of these classifiers is then used to classify unlabeled data, and its prediction is used to train the other classifiers in a semi-supervised fashion. We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models.</abstract>
      <url hash="eed2b5d2">W19-4614</url>
      <doi>10.18653/v1/W19-4614</doi>
      <bibkey>helwe-etal-2019-assessing</bibkey>
    </paper>
    <paper id="15">
      <title>Morphologically Annotated Corpora for Seven <fixed-case>A</fixed-case>rabic Dialects: <fixed-case>T</fixed-case>aizi, <fixed-case>S</fixed-case>anaani, <fixed-case>N</fixed-case>ajdi, <fixed-case>J</fixed-case>ordanian, <fixed-case>S</fixed-case>yrian, <fixed-case>I</fixed-case>raqi and <fixed-case>M</fixed-case>oroccan</title>
      <author><first>Faisal</first><last>Alshargi</last></author>
      <author><first>Shahd</first><last>Dibas</last></author>
      <author><first>Sakhar</first><last>Alkhereyf</last></author>
      <author><first>Reem</first><last>Faraj</last></author>
      <author><first>Basmah</first><last>Abdulkareem</last></author>
      <author><first>Sane</first><last>Yagi</last></author>
      <author><first>Ouafaa</first><last>Kacha</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <pages>137–147</pages>
      <abstract>We present a collection of morphologically annotated corpora for seven Arabic dialects: Taizi Yemeni, Sanaani Yemeni, Najdi, Jordanian, Syrian, Iraqi and Moroccan Arabic. The corpora collectively cover over 200,000 words, and are all manually annotated in a common set of standards for orthography, diacritized lemmas, tokenization, morphological units and English glosses. These corpora will be publicly available to serve as benchmarks for training and evaluating systems for Arabic dialect morphological analysis and disambiguation.</abstract>
      <url hash="8992fdcc">W19-4615</url>
      <doi>10.18653/v1/W19-4615</doi>
      <bibkey>alshargi-etal-2019-morphologically</bibkey>
    </paper>
    <paper id="16">
      <title>Construction and Annotation of the <fixed-case>J</fixed-case>ordan Comprehensive Contemporary <fixed-case>A</fixed-case>rabic Corpus (<fixed-case>JCCA</fixed-case>)</title>
      <author><first>Majdi</first><last>Sawalha</last></author>
      <author><first>Faisal</first><last>Alshargi</last></author>
      <author><first>Abdallah</first><last>AlShdaifat</last></author>
      <author><first>Sane</first><last>Yagi</last></author>
      <author><first>Mohammad A.</first><last>Qudah</last></author>
      <pages>148–157</pages>
      <abstract>To compile a modern dictionary that catalogues the words in currency, and to study linguistic patterns in the contemporary language, it is necessary to have a corpus of authentic texts that reflect current usage of the language. Although there are numerous Arabic corpora, none claims to be representative of the language in terms of the combination of geographical region, genre, subject matter, mode, and medium. This paper describes a 100-million-word corpus that takes the British National Corpus (BNC) as a model. The aim of the corpus is to be balanced, annotated, comprehensive, and representative of contemporary Arabic as written and spoken in Arab countries today. It will be different from most others in not being heavily-dominated by the news or in mixing the classical with the modern. In this paper is an outline of the methodology adopted for the design, construction, and annotation of this corpus. DIWAN (Alshargi and Rambow, 2015) was used to annotate a one-million-word snapshot of the corpus. DIWAN is a dialectal word annotation tool, but we upgraded it by adding a new tag-set that is based on traditional Arabic grammar and by adding the roots and morphological patterns of nouns and verbs. Moreover, the corpus we constructed covers the major spoken varieties of Arabic.</abstract>
      <url hash="de505678">W19-4616</url>
      <doi>10.18653/v1/W19-4616</doi>
      <bibkey>sawalha-etal-2019-construction</bibkey>
    </paper>
    <paper id="17">
      <title>Translating Between Morphologically Rich Languages: An <fixed-case>A</fixed-case>rabic-to-<fixed-case>T</fixed-case>urkish Machine Translation System</title>
      <author><first>İlknur</first><last>Durgar El-Kahlout</last></author>
      <author><first>Emre</first><last>Bektaş</last></author>
      <author><first>Naime Şeyma</first><last>Erdem</last></author>
      <author><first>Hamza</first><last>Kaya</last></author>
      <pages>158–166</pages>
      <abstract>This paper introduces the work on building a machine translation system for Arabic-to-Turkish in the news domain. Our work includes collecting parallel datasets in several ways for a new and low-resourced language pair, building baseline systems with state-of-the-art architectures and developing language specific algorithms for better translation. Parallel datasets are mainly collected three different ways; i) translating Arabic texts into Turkish by professional translators, ii) exploiting the web for open-source Arabic-Turkish parallel texts, iii) using back-translation. We per-formed preliminary experiments for Arabic-to-Turkish machine translation with neural(Marian) machine translation tools with a novel morphologically motivated vocabulary reduction method.</abstract>
      <url hash="8e60dc5b">W19-4617</url>
      <doi>10.18653/v1/W19-4617</doi>
      <bibkey>durgar-el-kahlout-etal-2019-translating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="18">
      <title>Improved Generalization of <fixed-case>A</fixed-case>rabic Text Classifiers</title>
      <author><first>Alaa</first><last>Khaddaj</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <author><first>Wassim</first><last>El-Hajj</last></author>
      <pages>167–174</pages>
      <abstract>While transfer learning for text has been very active in the English language, progress in Arabic has been slow, including the use of Domain Adaptation (DA). Domain Adaptation is used to generalize the performance of any classifier by trying to balance the classifier’s accuracy for a particular task among different text domains. In this paper, we propose and evaluate two variants of a domain adaptation technique: the first is a base model called Domain Adversarial Neural Network (DANN), while the second is a variation that incorporates representational learning. Similar to previous approaches, we propose the use of proxy A-distance as a metric to assess the success of generalization. We make use of ArSentDLEV, a multi-topic dataset collected from the Levantine countries, to test the performance of the models. We show the superiority of the proposed method in accuracy and robustness when dealing with the Arabic language.</abstract>
      <url hash="1e80d38c">W19-4618</url>
      <doi>10.18653/v1/W19-4618</doi>
      <bibkey>khaddaj-etal-2019-improved</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
    </paper>
    <paper id="19">
      <title><fixed-case>OSIAN</fixed-case>: Open Source International <fixed-case>A</fixed-case>rabic News Corpus - Preparation and Integration into the <fixed-case>CLARIN</fixed-case>-infrastructure</title>
      <author><first>Imad</first><last>Zeroual</last></author>
      <author><first>Dirk</first><last>Goldhahn</last></author>
      <author><first>Thomas</first><last>Eckart</last></author>
      <author><first>Abdelhak</first><last>Lakhouaja</last></author>
      <pages>175–182</pages>
      <abstract>The World Wide Web has become a fundamental resource for building large text corpora. Broadcasting platforms such as news websites are rich sources of data regarding diverse topics and form a valuable foundation for research. The Arabic language is extensively utilized on the Web. Still, Arabic is relatively an under-resourced language in terms of availability of freely annotated corpora. This paper presents the first version of the Open Source International Arabic News (OSIAN) corpus. The corpus data was collected from international Arabic news websites, all being freely available on the Web. The corpus consists of about 3.5 million articles comprising more than 37 million sentences and roughly 1 billion tokens. It is encoded in XML; each article is annotated with metadata information. Moreover, each word is annotated with lemma and part-of-speech. the described corpus is processed, archived and published into the CLARIN infrastructure. This publication includes descriptive metadata via OAI-PMH, direct access to the plain text material (available under Creative Commons Attribution-Non-Commercial 4.0 International License - CC BY-NC 4.0), and integration into the WebLicht annotation platform and CLARIN’s Federated Content Search FCS.</abstract>
      <url hash="6217a78a">W19-4619</url>
      <doi>10.18653/v1/W19-4619</doi>
      <bibkey>zeroual-etal-2019-osian</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>A</fixed-case>rabic Tweet-Act: Speech Act Recognition for <fixed-case>A</fixed-case>rabic Asynchronous Conversations</title>
      <author><first>Bushra</first><last>Algotiml</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>183–191</pages>
      <abstract>Speech acts are the actions that a speaker intends when performing an utterance within conversations. In this paper, we proposed speech act classification for asynchronous conversations on Twitter using multiple machine learning methods including SVM and deep neural networks. We applied the proposed methods on the ArSAS tweets dataset. The obtained results show that superiority of deep learning methods compared to SVMs, where Bi-LSTM managed to achieve an accuracy of 87.5% and a macro-averaged F1 score 61.5%. We believe that our results are the first to be reported on the task of speech-act recognition for asynchronous conversations on Arabic Twitter.</abstract>
      <url hash="6d146a6b">W19-4620</url>
      <doi>10.18653/v1/W19-4620</doi>
      <bibkey>algotiml-etal-2019-arabic</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>M</fixed-case>azajak: An Online <fixed-case>A</fixed-case>rabic Sentiment Analyser</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>192–198</pages>
      <abstract>Sentiment analysis (SA) is one of the most useful natural language processing applications. Literature is flooding with many papers and systems addressing this task, but most of the work is focused on English. In this paper, we present “Mazajak”, an online system for Arabic SA. The system is based on a deep learning model, which achieves state-of-the-art results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such system should assist various applications and research that rely on sentiment analysis as a tool.</abstract>
      <url hash="6808439a">W19-4621</url>
      <doi>10.18653/v1/W19-4621</doi>
      <bibkey>abu-farha-magdy-2019-mazajak</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
    </paper>
    <paper id="22">
      <title>The <fixed-case>MADAR</fixed-case> Shared Task on <fixed-case>A</fixed-case>rabic Fine-Grained Dialect Identification</title>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>199–207</pages>
      <abstract>In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. This shared task was organized as part of The Fourth Arabic Natural Language Processing Workshop, collocated with ACL 2019. The shared task includes two subtasks: the MADAR Travel Domain Dialect Identification subtask (Subtask 1) and the MADAR Twitter User Dialect Identification subtask (Subtask 2). This shared task is the first to target a large set of dialect labels at the city and country levels. The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project. A total of 21 teams from 15 countries participated in the shared task.</abstract>
      <url hash="86bdb1fd">W19-4622</url>
      <doi>10.18653/v1/W19-4622</doi>
      <bibkey>bouamor-etal-2019-madar</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>ZCU</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>MADAR</fixed-case> 2019: Recognizing <fixed-case>A</fixed-case>rabic Dialects</title>
      <author><first>Pavel</first><last>Přibáň</last></author>
      <author><first>Stephen</first><last>Taylor</last></author>
      <pages>208–213</pages>
      <abstract>In this paper, we present our systems for the MADAR Shared Task: Arabic Fine-Grained Dialect Identification. The shared task consists of two subtasks. The goal of Subtask– 1 (S-1) is to detect an Arabic city dialect in a given text and the goal of Subtask–2 (S-2) is to predict the country of origin of a Twitter user by using tweets posted by the user. In S-1, our proposed systems are based on language modelling. We use language models to extract features that are later used as an input for other machine learning algorithms. We also experiment with recurrent neural networks (RNN), but these experiments showed that simpler machine learning algorithms are more successful. Our system achieves 0.658 macro F1-score and our rank is 6th out of 19 teams in S-1 and 7th in S-2 with 0.475 macro F1-score.</abstract>
      <url hash="06635e70">W19-4623</url>
      <doi>10.18653/v1/W19-4623</doi>
      <bibkey>priban-taylor-2019-zcu</bibkey>
    </paper>
    <paper id="24">
      <title>Simple But Not Naïve: Fine-Grained <fixed-case>A</fixed-case>rabic Dialect Identification Using Only N-Grams</title>
      <author><first>Sohaila</first><last>Eltanbouly</last></author>
      <author><first>May</first><last>Bashendy</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>214–218</pages>
      <abstract>This paper presents the participation of Qatar University team in MADAR shared task, which addresses the problem of sentence-level fine-grained Arabic Dialect Identification over 25 different Arabic dialects in addition to the Modern Standard Arabic. Arabic Dialect Identification is not a trivial task since different dialects share some features, e.g., utilizing the same character set and some vocabularies. We opted to adopt a very simple approach in terms of extracted features and classification models; we only utilize word and character n-grams as features, and Na ̈ıve Bayes models as classifiers. Surprisingly, the simple approach achieved non-na ̈ıve performance. The official results, reported on a held-out testing set, show that the dialect of a given sentence can be identified at an accuracy of 64.58% by our best submitted run.</abstract>
      <url hash="5945f989">W19-4624</url>
      <doi>10.18653/v1/W19-4624</doi>
      <bibkey>eltanbouly-etal-2019-simple</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>LIUM</fixed-case>-<fixed-case>MIRACL</fixed-case> Participation in the <fixed-case>MADAR</fixed-case> <fixed-case>A</fixed-case>rabic Dialect Identification Shared Task</title>
      <author><first>Saméh</first><last>Kchaou</last></author>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Lamia</first><last>Hadrich-Belguith</last></author>
      <pages>219–223</pages>
      <abstract>This paper describes the joint participation of the LIUM and MIRACL Laboratories at the Arabic dialect identification challenge of the MADAR Shared Task (Bouamor et al., 2019) conducted during the Fourth Arabic Natural Language Processing Workshop (WANLP 2019). We participated to the Travel Domain Dialect Identification subtask. We built several systems and explored different techniques including conventional machine learning methods and deep learning algorithms. Deep learning approaches did not perform well on this task. We experimented several classification systems and we were able to identify the dialect of an input sentence with an F1-score of 65.41% on the official test set using only the training data supplied by the shared task organizers.</abstract>
      <url hash="1771a796">W19-4625</url>
      <doi>10.18653/v1/W19-4625</doi>
      <bibkey>kchaou-etal-2019-lium</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>A</fixed-case>rabic Dialect Identification with Deep Learning and Hybrid Frequency Based Features</title>
      <author><first>Youssef</first><last>Fares</last></author>
      <author><first>Zeyad</first><last>El-Zanaty</last></author>
      <author><first>Kareem</first><last>Abdel-Salam</last></author>
      <author><first>Muhammed</first><last>Ezzeldin</last></author>
      <author><first>Aliaa</first><last>Mohamed</last></author>
      <author><first>Karim</first><last>El-Awaad</last></author>
      <author><first>Marwan</first><last>Torki</last></author>
      <pages>224–228</pages>
      <abstract>Studies on Dialectical Arabic are growing more important by the day as it becomes the primary written and spoken form of Arabic online in informal settings. Among the important problems that should be explored is that of dialect identification. This paper reports different techniques that can be applied towards such goal and reports their performance on the Multi Arabic Dialect Applications and Resources (MADAR) Arabic Dialect Corpora. Our results show that improving on traditional systems using frequency based features and non deep learning classifiers is a challenging task. We propose different models based on different word and document representations. Our top model is able to achieve an F1 macro averaged score of 65.66 on MADAR’s small-scale parallel corpus of 25 dialects and Modern Standard Arabic (MSA).</abstract>
      <url hash="4b501033">W19-4626</url>
      <doi>10.18653/v1/W19-4626</doi>
      <bibkey>fares-etal-2019-arabic</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>MICHAEL</fixed-case>: Mining Character-level Patterns for <fixed-case>A</fixed-case>rabic Dialect Identification (<fixed-case>MADAR</fixed-case> Challenge)</title>
      <author><first>Dhaou</first><last>Ghoul</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>229–233</pages>
      <abstract>We present MICHAEL, a simple lightweight method for automatic Arabic Dialect Identification on the MADAR travel domain Dialect Identification (DID). MICHAEL uses simple character-level features in order to perform a pre-processing free classification. More precisely, Character N-grams extracted from the original sentences are used to train a Multinomial Naive Bayes classifier. This system achieved an official score (accuracy) of 53.25% with 1&lt;=N&lt;=3 but showed a much better result with character 4-grams (62.17% accuracy).</abstract>
      <url hash="1d609ecc">W19-4627</url>
      <doi>10.18653/v1/W19-4627</doi>
      <bibkey>ghoul-lejeune-2019-michael</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>A</fixed-case>rabic Dialect Identification for Travel and <fixed-case>T</fixed-case>witter Text</title>
      <author><first>Pruthwik</first><last>Mishra</last></author>
      <author><first>Vandan</first><last>Mujadia</last></author>
      <pages>234–238</pages>
      <abstract>This paper presents the results of the experiments done as a part of MADAR Shared Task in WANLP 2019 on Arabic Fine-Grained Dialect Identification. Dialect Identification is one of the prominent tasks in the field of Natural language processing where the subsequent language modules can be improved based on it. We explored the use of different features like char, word n-gram, language model probabilities, etc on different classifiers. Results show that these features help to improve dialect classification accuracy. Results also show that traditional machine learning classifier tends to perform better when compared to neural network models on this task in a low resource setting.</abstract>
      <url hash="fc1910eb">W19-4628</url>
      <doi>10.18653/v1/W19-4628</doi>
      <bibkey>mishra-mujadia-2019-arabic</bibkey>
    </paper>
    <paper id="29">
      <title>Mawdoo3 <fixed-case>AI</fixed-case> at <fixed-case>MADAR</fixed-case> Shared Task: <fixed-case>A</fixed-case>rabic Tweet Dialect Identification</title>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Wael</first><last>Farhan</last></author>
      <author><first>Ahmed</first><last>Altakrouri</last></author>
      <author><first>Hussein</first><last>Al-Natsheh</last></author>
      <pages>239–243</pages>
      <abstract>Arabic dialect identification is an inherently complex problem, as Arabic dialect taxonomy is convoluted and aims to dissect a continuous space rather than a discrete one. In this work, we present machine and deep learning approaches to predict 21 fine-grained dialects form a set of given tweets per user. We adopted numerous feature extraction methods most of which showed improvement in the final model, such as word embedding, Tf-idf, and other tweet features. Our results show that a simple LinearSVC can outperform any complex deep learning model given a set of curated features. With a relatively complex user voting mechanism, we were able to achieve a Macro-Averaged F1-score of 71.84% on MADAR shared subtask-2. Our best submitted model ranked second out of all participating teams.</abstract>
      <url hash="a21e55ab">W19-4629</url>
      <doi>10.18653/v1/W19-4629</doi>
      <bibkey>talafha-etal-2019-mawdoo3</bibkey>
    </paper>
    <paper id="30">
      <title>Mawdoo3 <fixed-case>AI</fixed-case> at <fixed-case>MADAR</fixed-case> Shared Task: <fixed-case>A</fixed-case>rabic Fine-Grained Dialect Identification with Ensemble Learning</title>
      <author><first>Ahmad</first><last>Ragab</last></author>
      <author><first>Haitham</first><last>Seelawi</last></author>
      <author><first>Mostafa</first><last>Samir</last></author>
      <author><first>Abdelrahman</first><last>Mattar</last></author>
      <author><first>Hesham</first><last>Al-Bataineh</last></author>
      <author><first>Mohammad</first><last>Zaghloul</last></author>
      <author><first>Ahmad</first><last>Mustafa</last></author>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Abed Alhakim</first><last>Freihat</last></author>
      <author><first>Hussein</first><last>Al-Natsheh</last></author>
      <pages>244–248</pages>
      <abstract>In this paper we discuss several models we used to classify 25 city-level Arabic dialects in addition to Modern Standard Arabic (MSA) as part of MADAR shared task (sub-task 1). We propose an ensemble model of a group of experimentally designed best performing classifiers on a various set of features. Our system achieves an accuracy of 69.3% macro F1-score with an improvement of 1.4% accuracy from the baseline model on the DEV dataset. Our best run submitted model ranked as third out of 19 participating teams on the TEST dataset with only 0.12% macro F1-score behind the top ranked system.</abstract>
      <url hash="fe76732e">W19-4630</url>
      <doi>10.18653/v1/W19-4630</doi>
      <bibkey>ragab-etal-2019-mawdoo3</bibkey>
    </paper>
    <paper id="31">
      <title>Hierarchical Deep Learning for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Gael</first><last>de Francony</last></author>
      <author><first>Victor</first><last>Guichard</last></author>
      <author><first>Praveen</first><last>Joshi</last></author>
      <author><first>Haithem</first><last>Afli</last></author>
      <author><first>Abdessalam</first><last>Bouchekif</last></author>
      <pages>249–253</pages>
      <abstract>In this paper, we present two approaches for Arabic Fine-Grained Dialect Identification. The first approach is based on Recurrent Neural Networks (BLSTM, BGRU) using hierarchical classification. The main idea is to separate the classification process for a sentence from a given text in two stages. We start with a higher level of classification (8 classes) and then the finer-grained classification (26 classes). The second approach is given by a voting system based on Naive Bayes and Random Forest. Our system achieves an F1 score of 63.02 % on the subtask evaluation dataset.</abstract>
      <url hash="4681ae2a">W19-4631</url>
      <doi>10.18653/v1/W19-4631</doi>
      <bibkey>de-francony-etal-2019-hierarchical</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>A</fixed-case>rb<fixed-case>D</fixed-case>ialect<fixed-case>ID</fixed-case> at <fixed-case>MADAR</fixed-case> Shared Task 1: Language Modelling and Ensemble Learning for Fine Grained <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Kathrein</first><last>Abu Kwaik</last></author>
      <author><first>Motaz</first><last>Saad</last></author>
      <pages>254–258</pages>
      <abstract>In this paper, we present a Dialect Identification system (ArbDialectID) that competed at Task 1 of the MADAR shared task, MADARTravel Domain Dialect Identification. We build a course and a fine-grained identification model to predict the label (corresponding to a dialect of Arabic) of a given text. We build two language models by extracting features at two levels (words and characters). We firstly build a coarse identification model to classify each sentence into one out of six dialects, then use this label as a feature for the fine-grained model that classifies the sentence among 26 dialects from different Arab cities, after that we apply ensemble voting classifier on both sub-systems. Our system ranked 1st that achieving an f-score of 67.32%. Both the models and our feature engineering tools are made available to the research community.</abstract>
      <url hash="969af372">W19-4632</url>
      <doi>10.18653/v1/W19-4632</doi>
      <bibkey>abu-kwaik-saad-2019-arbdialectid</bibkey>
    </paper>
    <paper id="33">
      <title>The <fixed-case>SM</fixed-case>ar<fixed-case>T</fixed-case> Classifier for <fixed-case>A</fixed-case>rabic Fine-Grained Dialect Identification</title>
      <author><first>Karima</first><last>Meftouh</last></author>
      <author><first>Karima</first><last>Abidi</last></author>
      <author><first>Salima</first><last>Harrat</last></author>
      <author><first>Kamel</first><last>Smaili</last></author>
      <pages>259–263</pages>
      <abstract>This paper describes the approach adopted by the SMarT research group to build a dialect identification system in the framework of the Madar shared task on Arabic fine-grained dialect identification. We experimented several approaches, but we finally decided to use a Multinomial Naive Bayes classifier based on word and character ngrams in addition to the language model probabilities. We achieved a score of 67.73% in terms of Macro accuracy and a macro-averaged F1-score of 67.31%</abstract>
      <url hash="25549de6">W19-4633</url>
      <doi>10.18653/v1/W19-4633</doi>
      <bibkey>meftouh-etal-2019-smart</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>JHU</fixed-case> System Description for the <fixed-case>MADAR</fixed-case> <fixed-case>A</fixed-case>rabic Dialect Identification Shared Task</title>
      <author><first>Tom</first><last>Lippincott</last></author>
      <author><first>Pamela</first><last>Shapiro</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Paul</first><last>McNamee</last></author>
      <pages>264–268</pages>
      <abstract>Our submission to the MADAR shared task on Arabic dialect identification employed a language modeling technique called Prediction by Partial Matching, an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models. We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2. Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel. This suggests the variety of architectures are learning redundant information, and future work may focus on encouraging decorrelated learning.</abstract>
      <url hash="b2a0c21b">W19-4634</url>
      <doi>10.18653/v1/W19-4634</doi>
      <bibkey>lippincott-etal-2019-jhu</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>ST</fixed-case> <fixed-case>MADAR</fixed-case> 2019 Shared Task: <fixed-case>A</fixed-case>rabic Fine-Grained Dialect Identification</title>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Abed Alhakim</first><last>Freihat</last></author>
      <pages>269–273</pages>
      <abstract>This paper describes the solution that we propose on MADAR 2019 Arabic Fine-Grained Dialect Identification task. The proposed solution utilized a set of classifiers that we trained on character and word features. These classifiers are: Support Vector Machines (SVM), Bernoulli Naive Bayes (BNB), Multinomial Naive Bayes (MNB), Logistic Regression (LR), Stochastic Gradient Descent (SGD), Passive Aggressive(PA) and Perceptron (PC). The system achieved competitive results, with a performance of 62.87 % and 62.12 % for both development and test sets.</abstract>
      <url hash="872bab2e">W19-4635</url>
      <doi>10.18653/v1/W19-4635</doi>
      <bibkey>abbas-etal-2019-st</bibkey>
    </paper>
    <paper id="36">
      <title>A Character Level Convolutional <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Mohamed</first><last>Elaraby</last></author>
      <author><first>Ahmed</first><last>Zahran</last></author>
      <pages>274–278</pages>
      <abstract>In this paper, we describe CU-RAISA teamcontribution to the 2019Madar shared task2, which focused on Twitter User fine-grained dialect identification.Among par-ticipating teams, our system ranked the4th(with 61.54%) F1-Macro measure.Our sys-tem is trained using a character level convo-lutional bidirectional long-short-term memorynetwork trained on 2k users’ data. We showthat training on concatenated user tweets asinput is further superior to training on usertweets separately and assign user’s label on themode of user’s tweets’ predictions.</abstract>
      <url hash="1a27a3ed">W19-4636</url>
      <doi>10.18653/v1/W19-4636</doi>
      <bibkey>elaraby-zahran-2019-character</bibkey>
    </paper>
    <paper id="37">
      <title>No Army, No Navy: <fixed-case>BERT</fixed-case> Semi-Supervised Learning of <fixed-case>A</fixed-case>rabic Dialects</title>
      <author><first>Chiyu</first><last>Zhang</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>279–284</pages>
      <abstract>We present our deep leaning system submitted to MADAR shared task 2 focused on twitter user dialect identification. We develop tweet-level identification models based on GRUs and BERT in supervised and semi-supervised set-tings. We then introduce a simple, yet effective, method of porting tweet-level labels at the level of users. Our system ranks top 1 in the competition, with 71.70% macro F1 score and 77.40% accuracy.</abstract>
      <url hash="4f6fcff2">W19-4637</url>
      <doi>10.18653/v1/W19-4637</doi>
      <bibkey>zhang-abdul-mageed-2019-army</bibkey>
    </paper>
    <paper id="38">
      <title>Team <fixed-case>JUST</fixed-case> at the <fixed-case>MADAR</fixed-case> Shared Task on <fixed-case>A</fixed-case>rabic Fine-Grained Dialect Identification</title>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Ali</first><last>Fadel</last></author>
      <author><first>Mahmoud</first><last>Al-Ayyoub</last></author>
      <author><first>Yaser</first><last>Jararweh</last></author>
      <author><first>Mohammad</first><last>AL-Smadi</last></author>
      <author><first>Patrick</first><last>Juola</last></author>
      <pages>285–289</pages>
      <abstract>In this paper, we describe our team’s effort on the MADAR Shared Task on Arabic Fine-Grained Dialect Identification. The task requires building a system capable of differentiating between 25 different Arabic dialects in addition to MSA. Our approach is simple. After preprocessing the data, we use Data Augmentation (DA) to enlarge the training data six times. We then build a language model and extract n-gram word-level and character-level TF-IDF features and feed them into an MNB classifier. Despite its simplicity, the resulting model performs really well producing the 4th highest F-measure and region-level accuracy and the 5th highest precision, recall, city-level accuracy and country-level accuracy among the participating teams.</abstract>
      <url hash="9a81fbc6">W19-4638</url>
      <doi>10.18653/v1/W19-4638</doi>
      <bibkey>talafha-etal-2019-team</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>QC</fixed-case>-<fixed-case>GO</fixed-case> Submission for <fixed-case>MADAR</fixed-case> Shared Task: <fixed-case>A</fixed-case>rabic Fine-Grained Dialect Identification</title>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Mohammed</first><last>Attia</last></author>
      <author><first>Mohamed</first><last>Eldesouki</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <pages>290–294</pages>
      <abstract>This paper describes the QC-GO team submission to the MADAR Shared Task Subtask 1 (travel domain dialect identification) and Subtask 2 (Twitter user location identification). In our participation in both subtasks, we explored a number of approaches and system combinations to obtain the best performance for both tasks. These include deep neural nets and heuristics. Since individual approaches suffer from various shortcomings, the combination of different approaches was able to fill some of these gaps. Our system achieves F1-Scores of 66.1% and 67.0% on the development sets for Subtasks 1 and 2 respectively.</abstract>
      <url hash="92879007">W19-4639</url>
      <doi>10.18653/v1/W19-4639</doi>
      <bibkey>samih-etal-2019-qc</bibkey>
    </paper>
  </volume>
  <volume id="47">
    <meta>
      <booktitle>Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change</booktitle>
      <url hash="7c49a9dc">W19-47</url>
      <editor><first>Nina</first><last>Tahmasebi</last></editor>
      <editor><first>Lars</first><last>Borin</last></editor>
      <editor><first>Adam</first><last>Jatowt</last></editor>
      <editor><first>Yang</first><last>Xu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>lchange</venue>
    </meta>
    <frontmatter>
      <url hash="609a30b3">W19-4700</url>
      <bibkey>ws-2019-international-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>From Insanely Jealous to Insanely Delicious: Computational Models for the Semantic Bleaching of <fixed-case>E</fixed-case>nglish Intensifiers</title>
      <author><first>Yiwei</first><last>Luo</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <author><first>Beth</first><last>Levin</last></author>
      <pages>1–13</pages>
      <abstract>We introduce novel computational models for modeling semantic bleaching, a widespread category of change in which words become more abstract or lose elements of meaning, like the development of “arrive” from its earlier meaning ‘become at shore.’ We validate our methods on a widespread case of bleaching in English: de-adjectival adverbs that originate as manner adverbs (as in “awfully behaved”) and later become intensifying adverbs (as in “awfully nice”). Our methods formally quantify three reflexes of bleaching: decreasing similarity to the source meaning (e.g., “awful”), increasing similarity to a fully bleached prototype (e.g., “very”), and increasing productivity (e.g., the breadth of adjectives that an adverb modifies). We also test a new causal model and find evidence that bleaching is initially triggered in contexts such as “conspicuously evident” and “insanely jealous”, where an adverb premodifies a semantically similar adjective. These contexts provide a form of “bridging context” (Evans and Wilkins, 2000) that allow a manner adverb to be reinterpreted as an intensifying adverb similar to “very”.</abstract>
      <url hash="0e6fd3a8">W19-4701</url>
      <doi>10.18653/v1/W19-4701</doi>
      <bibkey>luo-etal-2019-insanely</bibkey>
    </paper>
    <paper id="2">
      <title>Computational Analysis of the Historical Changes in Poetry and Prose</title>
      <author><first>Amitha</first><last>Gopidi</last></author>
      <author><first>Aniket</first><last>Alam</last></author>
      <pages>14–22</pages>
      <abstract>The esoteric definitions of poetry are insufficient in enveloping the changes in poetry that the age of mechanical reproduction has witnessed with the widespread proliferation of the use of digital media and artificial intelligence. They are also insufficient in distinguishing between prose and poetry, as the content of both prose and poetry can be poetic. Using quotes as prose considering their poetic, context-free and celebrated nature, stylistic differences between poetry and prose are delved into. Novel features in grammar and meter are justified as distinguishing features. Datasets of popular prose and poetry spanning across 1870-1920 and 1970-2019 have been created, and multiple experiments have been conducted to prove that prose and poetry in the latter period are more alike than they were in the former. The accuracy of classification of poetry and prose of 1970-2019 is significantly lesser than that of 1870-1920, thereby proving the convergence of poetry and prose in 1970-2019.</abstract>
      <url hash="5f18d3dd">W19-4702</url>
      <doi>10.18653/v1/W19-4702</doi>
      <bibkey>gopidi-alam-2019-computational</bibkey>
    </paper>
    <paper id="3">
      <title>Studying Semantic Chain Shifts with <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec: <fixed-case>FOOD</fixed-case>&gt;<fixed-case>MEAT</fixed-case>&gt;<fixed-case>FLESH</fixed-case></title>
      <author><first>Richard</first><last>Zimmermann</last></author>
      <pages>23–28</pages>
      <abstract>Word2Vec models are used to study the semantic chain shift FOOD&gt;MEAT&gt;FLESH in the history of English, c. 1425-1925. The development stretches out over a long time, starting before 1500, and may possibly be continuing to this day. The semantic changes likely proceeded as a push chain.</abstract>
      <url hash="71526c27">W19-4703</url>
      <doi>10.18653/v1/W19-4703</doi>
      <bibkey>zimmermann-2019-studying</bibkey>
    </paper>
    <paper id="4">
      <title>Evaluation of Semantic Change of Harm-Related Concepts in Psychology</title>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <author><first>Sean</first><last>Murphy</last></author>
      <author><first>Nicholas</first><last>Haslam</last></author>
      <pages>29–34</pages>
      <abstract>The paper focuses on diachronic evaluation of semantic changes of harm-related concepts in psychology. More specifically, we investigate a hypothesis that certain concepts such as “addiction”, “bullying”, “harassment”, “prejudice”, and “trauma” became broader during the last four decades. We evaluate semantic changes using two models: an LSA-based model from Sagi et al. (2009) and a diachronic adaptation of word2vec from Hamilton et al. (2016), that are trained on a large corpus of journal abstracts covering the period of 1980– 2019. Several concepts showed evidence of broadening. “Addiction” moved from physiological dependency on a substance to include psychological dependency on gaming and the Internet. Similarly, “harassment” and “trauma” shifted towards more psychological meanings. On the other hand, “bullying” has transformed into a more victim-related concept and expanded to new areas such as workplaces.</abstract>
      <url hash="570fc605">W19-4704</url>
      <doi>10.18653/v1/W19-4704</doi>
      <bibkey>vylomova-etal-2019-evaluation</bibkey>
    </paper>
    <paper id="5">
      <title>Contextualized Diachronic Word Representations</title>
      <author><first>Ganesh</first><last>Jawahar</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>35–47</pages>
      <abstract>Diachronic word embeddings play a key role in capturing interesting patterns about how language evolves over time. Most of the existing work focuses on studying corpora spanning across several decades, which is understandably still not a possibility when working on social media-based user-generated content. In this work, we address the problem of studying semantic changes in a large Twitter corpus collected over five years, a much shorter period than what is usually the norm in diachronic studies. We devise a novel attentional model, based on Bernoulli word embeddings, that are conditioned on contextual extra-linguistic (social) features such as network, spatial and socio-economic variables, which are associated with Twitter users, as well as topic-based features. We posit that these social features provide an inductive bias that helps our model to overcome the narrow time-span regime problem. Our extensive experiments reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain contextual features are absent. Our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods.</abstract>
      <url hash="a4674618">W19-4705</url>
      <doi>10.18653/v1/W19-4705</doi>
      <bibkey>jawahar-seddah-2019-contextualized</bibkey>
      <pwccode url="https://github.com/ganeshjawahar/social_word_emb" additional="false">ganeshjawahar/social_word_emb</pwccode>
    </paper>
    <paper id="6">
      <title>Semantic Change and Semantic Stability: Variation is Key</title>
      <author><first>Claire</first><last>Bowern</last></author>
      <pages>48–55</pages>
      <abstract>I survey some recent approaches to studying change in the lexicon, particularly change in meaning across phylogenies. I briefly sketch an evolutionary approach to language change and point out some issues in recent approaches to studying semantic change that rely on temporally stratified word embeddings. I draw illustrations from lexical cognate models in Pama-Nyungan to identify meaning classes most appropriate for lexical phylogenetic inference, particularly highlighting the importance of variation in studying change over time.</abstract>
      <url hash="bf85ff0a">W19-4706</url>
      <doi>10.18653/v1/W19-4706</doi>
      <bibkey>bowern-2019-semantic</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>GASC</fixed-case>: Genre-Aware Semantic Change for <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek</title>
      <author><first>Valerio</first><last>Perrone</last></author>
      <author><first>Marco</first><last>Palma</last></author>
      <author><first>Simon</first><last>Hengchen</last></author>
      <author><first>Alessandro</first><last>Vatri</last></author>
      <author><first>Jim Q.</first><last>Smith</last></author>
      <author><first>Barbara</first><last>McGillivray</last></author>
      <pages>56–66</pages>
      <abstract>Word meaning changes over time, depending on linguistic and extra-linguistic factors. Associating a word’s correct meaning in its historical context is a central challenge in diachronic research, and is relevant to a range of NLP tasks, including information retrieval and semantic search in historical texts. Bayesian models for semantic change have emerged as a powerful tool to address this challenge, providing explicit and interpretable representations of semantic change phenomena. However, while corpora typically come with rich metadata, existing models are limited by their inability to exploit contextual information (such as text genre) beyond the document time-stamp. This is particularly critical in the case of ancient languages, where lack of data and long diachronic span make it harder to draw a clear distinction between polysemy (the fact that a word has several senses) and semantic change (the process of acquiring, losing, or changing senses), and current systems perform poorly on these languages. We develop GASC, a dynamic semantic change model that leverages categorical metadata about the texts’ genre to boost inference and uncover the evolution of meanings in Ancient Greek corpora. In a new evaluation framework, our model achieves improved predictive performance compared to the state of the art.</abstract>
      <url hash="0437127d">W19-4707</url>
      <doi>10.18653/v1/W19-4707</doi>
      <bibkey>perrone-etal-2019-gasc</bibkey>
    </paper>
    <paper id="8">
      <title>Modeling Markedness with a Split-and-Merger Model of Sound Change</title>
      <author><first>Andrea</first><last>Ceolin</last></author>
      <author><first>Ollie</first><last>Sayeed</last></author>
      <pages>67–70</pages>
      <abstract>The concept of ‘markedness’ has been influential in phonology for almost a century. Theoretical phonology has found it useful to describe some segments as more ‘marked’ than others, referring to a cluster of language-internal and -external properties (Jakobson 1968, Haspelmath 2006). We argue, using a simple mathematical model based on Evolutionary Phonology (Blevins 2004), that markedness is an epiphenomenon of phonetically grounded sound change.</abstract>
      <url hash="178847e7">W19-4708</url>
      <doi>10.18653/v1/W19-4708</doi>
      <bibkey>ceolin-sayeed-2019-modeling</bibkey>
    </paper>
    <paper id="9">
      <title>A Method to Automatically Identify Diachronic Variation in Collocations.</title>
      <author><first>Marcos</first><last>Garcia</last></author>
      <author><first>Marcos</first><last>García Salido</last></author>
      <pages>71–80</pages>
      <abstract>This paper introduces a novel method to track collocational variations in diachronic corpora that can identify several changes undergone by these phraseological combinations and to propose alternative solutions found in later periods. The strategy consists of extracting syntactically-related candidates of collocations and ranking them using statistical association measures. Then, starting from the first period of the corpus, the system tracks each combination over time, verifying different types of historical variation such as the loss of one or both lemmas, the disappearance of the collocation, or its diachronic frequency trend. Using a distributional semantics strategy, it also proposes other linguistic structures which convey similar meanings to those extinct collocations. A case study on historical corpora of Portuguese and Spanish shows that the system speeds up and facilitates the finding of some diachronic changes and phraseological shifts that are harder to identify without using automated methods.</abstract>
      <url hash="3e92ce70">W19-4709</url>
      <doi>10.18653/v1/W19-4709</doi>
      <bibkey>garcia-garcia-salido-2019-method</bibkey>
    </paper>
    <paper id="10">
      <title>Written on Leaves or in Stones?: Computational Evidence for the Era of Authorship of Old <fixed-case>T</fixed-case>hai Prose</title>
      <author><first>Attapol</first><last>Rutherford</last></author>
      <author><first>Santhawat</first><last>Thanyawong</last></author>
      <pages>81–85</pages>
      <abstract>We aim to provide computational evidence for the era of authorship of two important old Thai texts: Traiphumikatha and Pumratchatham. The era of authorship of these two books is still an ongoing debate among Thai literature scholars. Analysis of old Thai texts present a challenge for standard natural language processing techniques, due to the lack of corpora necessary for building old Thai word and syllable segmentation. We propose an accurate and interpretable model to classify each segment as one of the three eras of authorship (Sukhothai, Ayuddhya, or Rattanakosin) without sophisticated linguistic preprocessing. Contrary to previous hypotheses, our model suggests that both books were written during the Sukhothai era. Moreover, the second half of the Pumratchtham is uncharacteristic of the Sukhothai era, which may have confounded literary scholars in the past. Further, our model reveals that the most indicative linguistic changes stem from unidirectional grammaticalized words and polyfunctional words, which show up as most dominant features in the model.</abstract>
      <url hash="a6044e03">W19-4710</url>
      <doi>10.18653/v1/W19-4710</doi>
      <bibkey>rutherford-thanyawong-2019-written</bibkey>
    </paper>
    <paper id="11">
      <title>Identifying Temporal Trends Based on Perplexity and Clustering: Are We Looking at Language Change?</title>
      <author><first>Sidsel</first><last>Boldsen</last></author>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <author><first>Patrizia</first><last>Paggio</last></author>
      <pages>86–91</pages>
      <abstract>In this work we propose a data-driven methodology for identifying temporal trends in a corpus of medieval charters. We have used perplexities derived from RNNs as a distance measure between documents and then, performed clustering on those distances. We argue that perplexities calculated by such language models are representative of temporal trends. The clusters produced using the K-Means algorithm give an insight of the differences in language in different time periods at least partly due to language change. We suggest that the temporal distribution of the individual clusters might provide a more nuanced picture of temporal trends compared to discrete bins, thus providing better results when used in a classification task.</abstract>
      <url hash="c9d284d7">W19-4711</url>
      <doi>10.18653/v1/W19-4711</doi>
      <bibkey>boldsen-etal-2019-identifying</bibkey>
    </paper>
    <paper id="12">
      <title>Using Word Embeddings to Examine Gender Bias in <fixed-case>D</fixed-case>utch Newspapers, 1950-1990</title>
      <author><first>Melvin</first><last>Wevers</last></author>
      <pages>92–97</pages>
      <abstract>Contemporary debates on filter bubbles and polarization in public and social media raise the question to what extent news media of the past exhibited biases. This paper specifically examines bias related to gender in six Dutch national newspapers between 1950 and 1990. We measure bias related to gender by comparing local changes in word embedding models trained on newspapers with divergent ideological backgrounds. We demonstrate clear differences in gender bias and changes within and between newspapers over time. In relation to themes such as sexuality and leisure, we see the bias moving toward women, whereas, generally, the bias shifts in the direction of men, despite growing female employment number and feminist movements. Even though Dutch society became less stratified ideologically (depillarization), we found an increasing divergence in gender bias between religious and social-democratic on the one hand and liberal newspapers on the other. Methodologically, this paper illustrates how word embeddings can be used to examine historical language change. Future work will investigate how fine-tuning deep contextualized embedding models, such as ELMO, might be used for similar tasks with greater contextual information.</abstract>
      <url hash="a59dfd38">W19-4712</url>
      <doi>10.18653/v1/W19-4712</doi>
      <bibkey>wevers-2019-using</bibkey>
      <pwccode url="https://github.com/melvinwevers/historical_concepts" additional="false">melvinwevers/historical_concepts</pwccode>
    </paper>
    <paper id="13">
      <title>Predicting Historical Phonetic Features using Deep Neural Networks: A Case Study of the Phonetic System of <fixed-case>P</fixed-case>roto-<fixed-case>I</fixed-case>ndo-<fixed-case>E</fixed-case>uropean</title>
      <author><first>Frederik</first><last>Hartmann</last></author>
      <pages>98–108</pages>
      <abstract>Traditional historical linguistics lacks the possibility to empirically assess its assumptions regarding the phonetic systems of past languages and language stages since most current methods rely on comparative tools to gain insights into phonetic features of sounds in proto- or ancestor languages. The paper at hand presents a computational method based on deep neural networks to predict phonetic features of historical sounds where the exact quality is unknown and to test the overall coherence of reconstructed historical phonetic features. The method utilizes the principles of coarticulation, local predictability and statistical phonological constraints to predict phonetic features by the features of their immediate phonetic environment. The validity of this method will be assessed using New High German phonetic data and its specific application to diachronic linguistics will be demonstrated in a case study of the phonetic system Proto-Indo-European.</abstract>
      <url hash="bfba18c3">W19-4713</url>
      <doi>10.18653/v1/W19-4713</doi>
      <bibkey>hartmann-2019-predicting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>P</fixed-case>ar<fixed-case>H</fixed-case>ist<fixed-case>V</fixed-case>is: Visualization of Parallel Multilingual Historical Data</title>
      <author><first>Aikaterini-Lida</first><last>Kalouli</last></author>
      <author><first>Rebecca</first><last>Kehlbeck</last></author>
      <author><first>Rita</first><last>Sevastjanova</last></author>
      <author><first>Katharina</first><last>Kaiser</last></author>
      <author><first>Georg A.</first><last>Kaiser</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <pages>109–114</pages>
      <abstract>The study of language change through parallel corpora can be advantageous for the analysis of complex interactions between time, text domain and language. Often, those advantages cannot be fully exploited due to the sparse but high-dimensional nature of such historical data. To tackle this challenge, we introduce ParHistVis: a novel, free, easy-to-use, interactive visualization tool for parallel, multilingual, diachronic and synchronic linguistic data. We illustrate the suitability of the components of the tool based on a use case of word order change in Romance wh-interrogatives.</abstract>
      <url hash="73ad784e">W19-4714</url>
      <doi>10.18653/v1/W19-4714</doi>
      <bibkey>kalouli-etal-2019-parhistvis</bibkey>
    </paper>
    <paper id="15">
      <title>Tracing Antisemitic Language Through Diachronic Embedding Projections: <fixed-case>F</fixed-case>rance 1789-1914</title>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Massimo</first><last>Warglien</last></author>
      <author><first>Simon</first><last>Levis Sullam</last></author>
      <author><first>Deborah</first><last>Paci</last></author>
      <pages>115–125</pages>
      <abstract>We investigate some aspects of the history of antisemitism in France, one of the cradles of modern antisemitism, using diachronic word embeddings. We constructed a large corpus of French books and periodicals issues that contain a keyword related to Jews and performed a diachronic word embedding over the 1789-1914 period. We studied the changes over time in the semantic spaces of 4 target words and performed embedding projections over 6 streams of antisemitic discourse. This allowed us to track the evolution of antisemitic bias in the religious, economic, socio-politic, racial, ethic and conspiratorial domains. Projections show a trend of growing antisemitism, especially in the years starting in the mid-80s and culminating in the Dreyfus affair. Our analysis also allows us to highlight the peculiar adverse bias towards Judaism in the broader context of other religions.</abstract>
      <url hash="a135ce17">W19-4715</url>
      <doi>10.18653/v1/W19-4715</doi>
      <bibkey>tripodi-etal-2019-tracing</bibkey>
      <pwccode url="https://github.com/roccotrip/antisem" additional="false">roccotrip/antisem</pwccode>
    </paper>
    <paper id="16">
      <title><fixed-case>D</fixed-case>ia<fixed-case>HC</fixed-case>lust: an Iterative Hierarchical Clustering Approach for Identifying Stages in Language Change</title>
      <author><first>Christin</first><last>Schätzle</last></author>
      <author><first>Hannah</first><last>Booth</last></author>
      <pages>126–135</pages>
      <abstract>Language change is often assessed against a set of pre-determined time periods in order to be able to trace its diachronic trajectory. This is problematic, since a pre-determined periodization might obscure significant developments and lead to false assumptions about the data. Moreover, these time periods can be based on factors which are either arbitrary or non-linguistic, e.g., dividing the corpus data into equidistant stages or taking into account language-external events. Addressing this problem, in this paper we present a data-driven approach to periodization: ‘DiaHClust’. DiaHClust is based on iterative hierarchical clustering and offers a multi-layered perspective on change from text-level to broader time periods. We demonstrate the usefulness of DiaHClust via a case study investigating syntactic change in Icelandic, modelling the syntactic system of the language in terms of vectors of syntactic change.</abstract>
      <url hash="bbb6e2a9">W19-4716</url>
      <doi>10.18653/v1/W19-4716</doi>
      <bibkey>schatzle-booth-2019-diahclust</bibkey>
    </paper>
    <paper id="17">
      <title>Treat the Word As a Whole or Look Inside? Subword Embeddings Model Language Change and Typology</title>
      <author><first>Yang</first><last>Xu</last></author>
      <author><first>Jiasheng</first><last>Zhang</last></author>
      <author><first>David</first><last>Reitter</last></author>
      <pages>136–145</pages>
      <abstract>We use a variant of word embedding model that incorporates subword information to characterize the degree of compositionality in lexical semantics. Our models reveal some interesting yet contrastive patterns of long-term change in multiple languages: Indo-European languages put more weight on subword units in newer words, while conversely Chinese puts less weights on the subwords, but more weight on the word as a whole. Our method provides novel evidence and methodology that enriches existing theories in evolutionary linguistics. The resulting word vectors also has decent performance in NLP-related tasks.</abstract>
      <url hash="b8d3a07d">W19-4717</url>
      <doi>10.18653/v1/W19-4717</doi>
      <bibkey>xu-etal-2019-treat</bibkey>
      <pwccode url="https://github.com/innerfirexy/lchange2019" additional="false">innerfirexy/lchange2019</pwccode>
    </paper>
    <paper id="18">
      <title>Times Are Changing: Investigating the Pace of Language Change in Diachronic Word Embeddings</title>
      <author><first>Stephanie</first><last>Brandl</last></author>
      <author><first>David</first><last>Lassner</last></author>
      <pages>146–150</pages>
      <abstract>We propose Word Embedding Networks, a novel method that is able to learn word embeddings of individual data slices while simultaneously aligning and ordering them without feeding temporal information a priori to the model. This gives us the opportunity to analyse the dynamics in word embeddings on a large scale in a purely data-driven manner. In experiments on two different newspaper corpora, the New York Times (English) and die Zeit (German), we were able to show that time actually determines the dynamics of semantic change. However, there is by no means a uniform evolution, but instead times of faster and times of slower change.</abstract>
      <url hash="9016ddf2">W19-4718</url>
      <doi>10.18653/v1/W19-4718</doi>
      <bibkey>brandl-lassner-2019-times</bibkey>
    </paper>
    <paper id="19">
      <title>The Rationality of Semantic Change</title>
      <author><first>Omer</first><last>Korat</last></author>
      <pages>151–160</pages>
      <abstract>This study investigates the mutual effects over time of semantically related function words on each other’s distribution over syntactic environments. Words that can have the same meaning are observed to have opposite trends of change in frequency across different syntactic structures which correspond to the shared meaning. This phenomenon is demonstrated to have a rational basis: it increases communicative efficiency by prioritizing words differently in the environments on which they compete.</abstract>
      <url hash="6e2f524c">W19-4719</url>
      <doi>10.18653/v1/W19-4719</doi>
      <bibkey>korat-2019-rationality</bibkey>
    </paper>
    <paper id="20">
      <title>Studying Laws of Semantic Divergence across Languages using Cognate Sets</title>
      <author><first>Ana</first><last>Uban</last></author>
      <author><first>Alina Maria</first><last>Ciobanu</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>161–166</pages>
      <abstract>Semantic divergence in related languages is a key concern of historical linguistics. Intra-lingual semantic shift has been previously studied in computational linguistics, but this can only provide a limited picture of the evolution of word meanings, which often develop in a multilingual environment. In this paper we investigate semantic change across languages by measuring the semantic distance of cognate words in multiple languages. By comparing current meanings of cognates in different languages, we hope to uncover information about their previous meanings, and about how they diverged in their respective languages from their common original etymon. We further study the properties of their semantic divergence, by analyzing how the features of words such as frequency and polysemy are related to the divergence in their meaning, and thus make the first steps towards formulating laws of cross-lingual semantic change.</abstract>
      <url hash="8c1dde9c">W19-4720</url>
      <doi>10.18653/v1/W19-4720</doi>
      <bibkey>uban-etal-2019-studying</bibkey>
    </paper>
    <paper id="21">
      <title>Detecting Syntactic Change Using a Neural Part-of-Speech Tagger</title>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Gigi</first><last>Stark</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <pages>167–174</pages>
      <abstract>We train a diachronic long short-term memory (LSTM) part-of-speech tagger on a large corpus of American English from the 19th, 20th, and 21st centuries. We analyze the tagger’s ability to implicitly learn temporal structure between years, and the extent to which this knowledge can be transferred to date new sentences. The learned year embeddings show a strong linear correlation between their first principal component and time. We show that temporal information encoded in the model can be used to predict novel sentences’ years of composition relatively well. Comparisons to a feedforward baseline suggest that the temporal change learned by the LSTM is syntactic rather than purely lexical. Thus, our results suggest that our tagger is implicitly learning to model syntactic change in American English over the course of the 19th, 20th, and early 21st centuries.</abstract>
      <url hash="8381feb4">W19-4721</url>
      <doi>10.18653/v1/W19-4721</doi>
      <bibkey>merrill-etal-2019-detecting</bibkey>
      <pwccode url="https://github.com/viking-sudo-rm/DiachronicPOSTagger" additional="false">viking-sudo-rm/DiachronicPOSTagger</pwccode>
    </paper>
    <paper id="22">
      <title>Grammar and Meaning: Analysing the Topology of Diachronic Word Embeddings</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last></author>
      <author><first>Katrin</first><last>Menzel</last></author>
      <author><first>Pauline</first><last>Krielke</last></author>
      <author><first>Elke</first><last>Teich</last></author>
      <pages>175–185</pages>
      <abstract>The paper showcases the application of word embeddings to change in language use in the domain of science, focusing on the Late Modern English period (17-19th century). Historically, this is the period in which many registers of English developed, including the language of science. Our overarching interest is the linguistic development of scientific writing to a distinctive (group of) register(s). A register is marked not only by the choice of lexical words (discourse domain) but crucially by grammatical choices which indicate style. The focus of the paper is on the latter, tracing words with primarily grammatical functions (function words and some selected, poly-functional word forms) diachronically. To this end, we combine diachronic word embeddings with appropriate visualization and exploratory techniques such as clustering and relative entropy for meaningful aggregation of data and diachronic comparison.</abstract>
      <url hash="35e3cd54">W19-4722</url>
      <doi>10.18653/v1/W19-4722</doi>
      <bibkey>bizzoni-etal-2019-grammar</bibkey>
    </paper>
    <paper id="23">
      <title>Spatio-Temporal Prediction of Dialectal Variant Usage</title>
      <author><first>Péter</first><last>Jeszenszky</last></author>
      <author><first>Panote</first><last>Siriaraya</last></author>
      <author><first>Philipp</first><last>Stoeckle</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <pages>186–195</pages>
      <abstract>The distribution of most dialectal variants have not only spatial but also temporal patterns. Based on the ‘apparent time hypothesis’, much of dialect change is happening through younger speakers accepting innovations. Thus, synchronic diversity can be interpreted diachronically. With the assumption of the ‘contact effect’, i.e. contact possibility (contact and isolation) between speaker communities being responsible for language change, and the apparent time hypothesis, we aim to predict the usage of dialectal variants. In this paper we model the contact possibility based on two of the most important factors in sociolinguistics to be affecting language change: age and distance. The first steps of the approach involve modeling contact possibility using a logistic predictor, taking the age of respondents into account. We test the <i>global</i>, and the <i>local</i> role of age for variation where the local level means spatial subsets around each survey site, chosen based on <i>k</i> nearest neighbors. The prediction approach is tested on Swiss German syntactic survey data, featuring multiple respondents from different age cohorts at survey sites. The results show the relative success of the logistic prediction approach and the limitations of the method, therefore further proposals are made to develop the methodology.</abstract>
      <url hash="c8a412a5">W19-4723</url>
      <doi>10.18653/v1/W19-4723</doi>
      <bibkey>jeszenszky-etal-2019-spatio</bibkey>
    </paper>
    <paper id="24">
      <title>One-to-<fixed-case>X</fixed-case> Analogical Reasoning on Word Embeddings: a Case for Diachronic Armed Conflict Prediction from News Texts</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>196–201</pages>
      <abstract>We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type ‘location:armed-group’ based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.</abstract>
      <url hash="3069a75e">W19-4724</url>
      <doi>10.18653/v1/W19-4724</doi>
      <bibkey>kutuzov-etal-2019-one</bibkey>
      <pwccode url="https://github.com/ltgoslo/diachronic_armed_conflicts" additional="false">ltgoslo/diachronic_armed_conflicts</pwccode>
    </paper>
    <paper id="25">
      <title>Measuring Diachronic Evolution of Evaluative Adjectives with Word Embeddings: the Case for <fixed-case>E</fixed-case>nglish, <fixed-case>N</fixed-case>orwegian, and <fixed-case>R</fixed-case>ussian</title>
      <author><first>Julia</first><last>Rodina</last></author>
      <author><first>Daria</first><last>Bakshandaeva</last></author>
      <author><first>Vadim</first><last>Fomin</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>202–209</pages>
      <abstract>We measure the intensity of diachronic semantic shifts in adjectives in English, Norwegian and Russian across 5 decades. This is done in order to test the hypothesis that evaluative adjectives are more prone to temporal semantic change. To this end, 6 different methods of quantifying semantic change are used. Frequency-controlled experimental results show that, depending on the particular method, evaluative adjectives either do not differ from other types of adjectives in terms of semantic change or appear to actually be less prone to shifting (particularly, to ‘jitter’-type shifting). Thus, in spite of many well-known examples of semantically changing evaluative adjectives (like ‘terrific’ or ‘incredible’), it seems that such cases are not specific to this particular type of words.</abstract>
      <url hash="9f9447d8">W19-4725</url>
      <doi>10.18653/v1/W19-4725</doi>
      <bibkey>rodina-etal-2019-measuring</bibkey>
    </paper>
    <paper id="26">
      <title>Semantic Change in the Language of <fixed-case>UK</fixed-case> Parliamentary Debates</title>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last></author>
      <pages>210–215</pages>
      <abstract>We investigate changes in the meanings of words used in the UK Parliament across two different epochs. We use word embeddings to explore changes in the distribution of words of interest and uncover words that appear to have undergone semantic transformation in the intervening period, and explore different ways of obtaining target words for this purpose. We find that semantic changes are generally in line with those found in other corpora, and little evidence that parliamentary language is more static than general English. It also seems that words with senses that have been recorded in the dictionary as having fallen into disuse do not undergo semantic changes in this domain.</abstract>
      <url hash="d4bb7b4f">W19-4726</url>
      <doi>10.18653/v1/W19-4726</doi>
      <bibkey>abercrombie-batista-navarro-2019-semantic</bibkey>
    </paper>
    <paper id="27">
      <title>Semantic Change and Emerging Tropes In a Large Corpus of <fixed-case>N</fixed-case>ew <fixed-case>H</fixed-case>igh <fixed-case>G</fixed-case>erman Poetry</title>
      <author><first>Thomas</first><last>Haider</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>216–222</pages>
      <abstract>Due to its semantic succinctness and novelty of expression, poetry is a great test-bed for semantic change analysis. However, so far there is a scarcity of large diachronic corpora. Here, we provide a large corpus of German poetry which consists of about 75k poems with more than 11 million tokens, with poems ranging from the 16th to early 20th century. We then track semantic change in this corpus by investigating the rise of tropes (‘love is magic’) over time and detecting change points of meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry.</abstract>
      <url hash="9cfd156e">W19-4727</url>
      <doi>10.18653/v1/W19-4727</doi>
      <bibkey>haider-eger-2019-semantic</bibkey>
      <pwccode url="https://github.com/thomasnikolaushaider/DLK" additional="false">thomasnikolaushaider/DLK</pwccode>
    </paper>
    <paper id="28">
      <title>Conceptual Change and Distributional Semantic Models: an Exploratory Study on Pitfalls and Possibilities</title>
      <author><first>Pia</first><last>Sommerauer</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>223–233</pages>
      <abstract>Studying conceptual change using embedding models has become increasingly popular in the Digital Humanities community while critical observations about them have received less attention. This paper investigates what the impact of known pitfalls can be on the conclusions drawn in a digital humanities study through the use case of “Racism”. In addition, we suggest an approach for modeling a complex concept in terms of words and relations representative of the conceptual system. Our results show that different models created from the same data yield different results, but also indicate that using different model architectures, comparing different corpora and comparing to control words and relations can help to identify which results are solid and which may be due to artefact. We propose guidelines to conduct similar studies, but also note that more work is needed to fully understand how we can distinguish artefacts from actual conceptual changes.</abstract>
      <url hash="5af40a75">W19-4728</url>
      <doi>10.18653/v1/W19-4728</doi>
      <bibkey>sommerauer-fokkens-2019-conceptual</bibkey>
    </paper>
    <paper id="29">
      <title>Measuring the Compositionality of Noun-Noun Compounds over Time</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Janis</first><last>Pagel</last></author>
      <author><first>Lonneke</first><last>van der Plas</last></author>
      <pages>234–239</pages>
      <abstract>We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.</abstract>
      <url hash="d11101b7">W19-4729</url>
      <doi>10.18653/v1/W19-4729</doi>
      <bibkey>dhar-etal-2019-measuring</bibkey>
      <pwccode url="https://github.com/prajitdhar/Compounding" additional="false">prajitdhar/Compounding</pwccode>
    </paper>
    <paper id="30">
      <title>Towards Automatic Variant Analysis of Ancient Devotional Texts</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <author><first>Dominique</first><last>Stutzmann</last></author>
      <author><first>Jacob</first><last>Currie</last></author>
      <author><first>Christine</first><last>Jacquin</last></author>
      <pages>240–249</pages>
      <abstract>We address in this paper the issue of text reuse in liturgical manuscripts of the middle ages. More specifically, we study variant readings of the Obsecro Te prayer, part of the devotional Books of Hours often used by Christians as guidance for their daily prayers. We aim at automatically extracting and categorising pairs of words and expressions that exhibit variant relations. For this purpose, we adopt a linguistic classification that allows to better characterize the variants than edit operations. Then, we study the evolution of Obsecro Te texts from a temporal and geographical axis. Finally, we contrast several unsupervised state-of-the-art approaches for the automatic extraction of Obsecro Te variants. Based on the manual observation of 772 Obsecro Te copies which show more than 21,000 variants, we show that the proposed methodology is helpful for an automatic study of variants and may serve as basis to analyze and to depict useful information from devotional texts.</abstract>
      <url hash="eb1f72fb">W19-4730</url>
      <doi>10.18653/v1/W19-4730</doi>
      <bibkey>hazem-etal-2019-towards</bibkey>
    </paper>
    <paper id="31">
      <title>Understanding the Evolution of Circular Economy through Language Change</title>
      <author><first>Sampriti</first><last>Mahanty</last></author>
      <author><first>Frank</first><last>Boons</last></author>
      <author><first>Julia</first><last>Handl</last></author>
      <author><first>Riza</first><last>Theresa Batista-Navarro</last></author>
      <pages>250–253</pages>
      <abstract>In this study, we propose to focus on understanding the evolution of a specific scientific concept—that of Circular Economy (CE)—by analysing how the language used in academic discussions has changed semantically. It is worth noting that the meaning and central theme of this concept has remained the same; however, we hypothesise that it has undergone semantic change by way of additional layers being added to the concept. We have shown that semantic change in language is a reflection of shifts in scientific ideas, which in turn help explain the evolution of a concept. Focusing on the CE concept, our analysis demonstrated that the change over time in the language used in academic discussions of CE is indicative of the way in which the concept evolved and expanded.</abstract>
      <url hash="ee35f206">W19-4731</url>
      <doi>10.18653/v1/W19-4731</doi>
      <bibkey>mahanty-etal-2019-understanding</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>G</fixed-case>aussian Process Models of Sound Change in <fixed-case>I</fixed-case>ndo-<fixed-case>A</fixed-case>ryan Dialectology</title>
      <author><first>Chundra</first><last>Cathcart</last></author>
      <pages>254–264</pages>
      <abstract>This paper proposes a Gaussian Process model of sound change targeted toward questions in Indo-Aryan dialectology. Gaussian Processes (GPs) provide a flexible means of expressing covariance between outcomes, and can be extended to a wide variety of probability distributions. We find that GP models fare better in terms of some key posterior predictive checks than models that do not express covariance between sound changes, and outline directions for future work.</abstract>
      <url hash="e3f50b31">W19-4732</url>
      <doi>10.18653/v1/W19-4732</doi>
      <bibkey>cathcart-2019-gaussian</bibkey>
    </paper>
    <paper id="33">
      <title>Modeling a Historical Variety of a Low-Resource Language: <fixed-case>L</fixed-case>anguage Contact Effects in the Verbal Cluster of <fixed-case>E</fixed-case>arly-<fixed-case>M</fixed-case>odern <fixed-case>F</fixed-case>risian</title>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Arjen</first><last>Versloot</last></author>
      <author><first>Fred</first><last>Weerman</last></author>
      <pages>265–271</pages>
      <abstract>Certain phenomena of interest to linguists mainly occur in low-resource languages, such as contact-induced language change. We show that it is possible to study contact-induced language change computationally in a historical variety of a low-resource language, Early-Modern Frisian, by creating a model using features that were established to be relevant in a closely related language, modern Dutch. This allows us to test two hypotheses on two types of language contact that may have taken place between Frisian and Dutch during this time. Our model shows that Frisian verb cluster word orders are associated with different context features than Dutch verb orders, supporting the ‘learned borrowing’ hypothesis.</abstract>
      <url hash="22b0f853">W19-4733</url>
      <doi>10.18653/v1/W19-4733</doi>
      <bibkey>bloem-etal-2019-modeling</bibkey>
    </paper>
    <paper id="34">
      <title>Visualizing Linguistic Change as Dimension Interactions</title>
      <author><first>Christin</first><last>Schätzle</last></author>
      <author><first>Frederik L.</first><last>Dennig</last></author>
      <author><first>Michael</first><last>Blumenschein</last></author>
      <author><first>Daniel A.</first><last>Keim</last></author>
      <author><first>Miriam</first><last>Butt</last></author>
      <pages>272–278</pages>
      <abstract>Historical change typically is the result of complex interactions between several linguistic factors. Identifying the relevant factors and understanding how they interact across the temporal dimension is the core remit of historical linguistics. With respect to corpus work, this entails a separate annotation, extraction and painstaking pair-wise comparison of the relevant bits of information. This paper presents a significant extension of HistoBankVis, a multilayer visualization system which allows a fast and interactive exploration of complex linguistic data. Linguistic factors can be understood as data dimensions which show complex interrelationships. We model these relationships with the Parallel Sets technique. We demonstrate the powerful potential of this technique by applying the system to understanding the interaction of case, grammatical relations and word order in the history of Icelandic.</abstract>
      <url hash="22b7c307">W19-4734</url>
      <doi>10.18653/v1/W19-4734</doi>
      <bibkey>schatzle-etal-2019-visualizing</bibkey>
    </paper>
  </volume>
  <volume id="48">
    <meta>
      <booktitle>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
      <url hash="1c4c6922">W19-48</url>
      <editor><first>Tal</first><last>Linzen</last></editor>
      <editor><first>Grzegorz</first><last>Chrupała</last></editor>
      <editor><first>Yonatan</first><last>Belinkov</last></editor>
      <editor><first>Dieuwke</first><last>Hupkes</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>blackboxnlp</venue>
    </meta>
    <frontmatter>
      <url hash="752511b7">W19-4800</url>
      <bibkey>ws-2019-2019-acl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Transcoding Compositionally: Using Attention to Find More Generalizable Solutions</title>
      <author><first>Kris</first><last>Korrel</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <pages>1–11</pages>
      <abstract>While sequence-to-sequence models have shown remarkable generalization power across several natural language tasks, their construct of solutions are argued to be less compositional than human-like generalization. In this paper, we present seq2attn, a new architecture that is specifically designed to exploit attention to find compositional patterns in the input. In seq2attn, the two standard components of an encoder-decoder model are connected via a transcoder, that modulates the information flow between them. We show that seq2attn can successfully generalize, without requiring any additional supervision, on two tasks which are specifically constructed to challenge the compositional skills of neural networks. The solutions found by the model are highly interpretable, allowing easy analysis of both the types of solutions that are found and potential causes for mistakes. We exploit this opportunity to introduce a new paradigm to test compositionality that studies the extent to which a model overgeneralizes when confronted with exceptions. We show that seq2attn exhibits such overgeneralization to a larger degree than a standard sequence-to-sequence model.</abstract>
      <url hash="4c22dd10">W19-4801</url>
      <doi>10.18653/v1/W19-4801</doi>
      <bibkey>korrel-etal-2019-transcoding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="2">
      <title>Sentiment Analysis Is Not Solved! Assessing and Probing Sentiment Classification</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>12–23</pages>
      <abstract>Neural methods for sentiment analysis have led to quantitative improvements over previous approaches, but these advances are not always accompanied with a thorough analysis of the qualitative differences. Therefore, it is not clear what outstanding conceptual challenges for sentiment analysis remain. In this work, we attempt to discover what challenges still prove a problem for sentiment classifiers for English and to provide a challenging dataset. We collect the subset of sentences that an (oracle) ensemble of state-of-the-art sentiment classifiers misclassify and then annotate them for 18 linguistic and paralinguistic phenomena, such as negation, sarcasm, modality, etc. Finally, we provide a case study that demonstrates the usefulness of the dataset to probe the performance of a given sentiment classifier with respect to linguistic phenomena.</abstract>
      <url hash="3099259e">W19-4802</url>
      <doi>10.18653/v1/W19-4802</doi>
      <bibkey>barnes-etal-2019-sentiment</bibkey>
      <pwccode url="https://github.com/ltgoslo/assessing_and_probing_sentiment" additional="false">ltgoslo/assessing_and_probing_sentiment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="3">
      <title>Second-order Co-occurrence Sensitivity of Skip-Gram with Negative Sampling</title>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <author><first>Cennet</first><last>Oguz</last></author>
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <pages>24–30</pages>
      <abstract>We simulate first- and second-order context overlap and show that Skip-Gram with Negative Sampling is similar to Singular Value Decomposition in capturing second-order co-occurrence information, while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks.</abstract>
      <url hash="93c93da0">W19-4803</url>
      <doi>10.18653/v1/W19-4803</doi>
      <bibkey>schlechtweg-etal-2019-second</bibkey>
      <pwccode url="https://github.com/Garrafao/SecondOrder" additional="false">Garrafao/SecondOrder</pwccode>
    </paper>
    <paper id="4">
      <title>Can Neural Networks Understand Monotonicity Reasoning?</title>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <author><first>Daisuke</first><last>Bekki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <author><first>Lasha</first><last>Abzianidze</last></author>
      <author><first>Johan</first><last>Bos</last></author>
      <pages>31–40</pages>
      <abstract>Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55%, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning.</abstract>
      <url hash="222baf97">W19-4804</url>
      <doi>10.18653/v1/W19-4804</doi>
      <revision id="1" href="W19-4804v1" hash="9b1c665c"/>
      <revision id="2" href="W19-4804v2" hash="222baf97">Corrected missing acknowledgment.</revision>
      <bibkey>yanaka-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/verypluming/MED" additional="false">verypluming/MED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/med">MED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="5">
      <title>Multi-Granular Text Encoding for Self-Explaining Categorization</title>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Lin</first><last>Pan</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Yousef</first><last>El-Kurdi</last></author>
      <pages>41–45</pages>
      <abstract>Self-explaining text categorization requires a classifier to make a prediction along with supporting evidence. A popular type of evidence is sub-sequences extracted from the input text which are sufficient for the classifier to make the prediction. In this work, we define multi-granular ngrams as basic units for explanation, and organize all ngrams into a hierarchical structure, so that shorter ngrams can be reused while computing longer ngrams. We leverage the tree-structured LSTM to learn a context-independent representation for each unit via parameter sharing. Experiments on medical disease classification show that our model is more accurate, efficient and compact than the BiLSTM and CNN baselines. More importantly, our model can extract intuitive multi-granular evidence to support its predictions.</abstract>
      <url hash="00a0041e">W19-4805</url>
      <doi>10.18653/v1/W19-4805</doi>
      <bibkey>wang-etal-2019-multi-granular</bibkey>
    </paper>
    <paper id="6">
      <title>The Meaning of “Most” for Visual Question Answering Models</title>
      <author><first>Alexander</first><last>Kuhnle</last></author>
      <author><first>Ann</first><last>Copestake</last></author>
      <pages>46–55</pages>
      <abstract>The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of “most”, we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber’s law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.</abstract>
      <url hash="00c17d69">W19-4806</url>
      <doi>10.18653/v1/W19-4806</doi>
      <bibkey>kuhnle-copestake-2019-meaning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/shapeworld">ShapeWorld</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="7">
      <title>Do Human Rationales Improve Machine Explanations?</title>
      <author><first>Julia</first><last>Strout</last></author>
      <author><first>Ye</first><last>Zhang</last></author>
      <author><first>Raymond</first><last>Mooney</last></author>
      <pages>56–62</pages>
      <abstract>Work on “learning with rationales” shows that humans providing explanations to a machine learning system can improve the system’s predictive accuracy. However, this work has not been connected to work in “explainable AI” which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine’s explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using “supervised attention” are judged superior to explanations generated using normal unsupervised attention.</abstract>
      <url hash="54843077">W19-4807</url>
      <doi>10.18653/v1/W19-4807</doi>
      <bibkey>strout-etal-2019-human</bibkey>
    </paper>
    <paper id="8">
      <title>Analyzing the Structure of Attention in a Transformer Language Model</title>
      <author><first>Jesse</first><last>Vig</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>63–76</pages>
      <abstract>The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.</abstract>
      <url hash="b91dd333">W19-4808</url>
      <attachment type="poster" hash="2e730b16">W19-4808.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-4808</doi>
      <bibkey>vig-belinkov-2019-analyzing</bibkey>
    </paper>
    <paper id="9">
      <title>Detecting Political Bias in News Articles Using Headline Attention</title>
      <author><first>Rama Rohit Reddy</first><last>Gangula</last></author>
      <author><first>Suma Reddy</first><last>Duggenpudi</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>77–84</pages>
      <abstract>Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we find a subtle bias towards or against someone or something. When it comes to politics, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of biased news and conspiracy theories. Automating bias detection in newspaper articles could be a good challenge for research in NLP. We proposed a headline attention network for this bias detection. Our model has two distinctive characteristics: (i) it has a structure that mirrors a person’s way of reading a news article (ii) it has attention mechanism applied on the article based on its headline, enabling it to attend to more critical content to predict bias. As the required datasets were not available, we created a dataset comprising of 1329 news articles collected from various Telugu newspapers and marked them for bias towards a particular political party. The experiments conducted on it demonstrated that our model outperforms various baseline methods by a substantial margin.</abstract>
      <url hash="0b063209">W19-4809</url>
      <doi>10.18653/v1/W19-4809</doi>
      <bibkey>gangula-etal-2019-detecting</bibkey>
    </paper>
    <paper id="10">
      <title>Testing the Generalization Power of Neural Network Models across <fixed-case>NLI</fixed-case> Benchmarks</title>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <pages>85–94</pages>
      <abstract>Neural network models have been very successful in natural language inference, with the best models reaching 90% accuracy in some benchmarks. However, the success of these models turns out to be largely benchmark specific. We show that models trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of inference assumed in these benchmarks is the same or similar. We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task. In light of these results, we argue that most of the current neural network models are not able to generalize well in the task of natural language inference. We find that using large pre-trained language models helps with transfer learning when the datasets are similar enough. Our results also highlight that the current NLI datasets do not cover the different nuances of inference extensively enough.</abstract>
      <url hash="49e45e83">W19-4810</url>
      <doi>10.18653/v1/W19-4810</doi>
      <bibkey>talman-chatzikyriakidis-2019-testing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
    </paper>
    <paper id="11">
      <title>Character Eyes: Seeing Language through Character-Level Taggers</title>
      <author><first>Yuval</first><last>Pinter</last></author>
      <author><first>Marc</first><last>Marone</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>95–102</pages>
      <abstract>Character-level models have been used extensively in recent years in NLP tasks as both supplements and replacements for closed-vocabulary token-level word representations. In one popular architecture, character-level LSTMs are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (POS) tags. In this work, we examine the behavior of POS taggers across languages from the perspective of individual hidden units within the character LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.</abstract>
      <url hash="6abe06c0">W19-4811</url>
      <doi>10.18653/v1/W19-4811</doi>
      <bibkey>pinter-etal-2019-character</bibkey>
      <pwccode url="https://github.com/ruyimarone/character-eyes" additional="false">ruyimarone/character-eyes</pwccode>
    </paper>
    <paper id="12">
      <title>Faithful Multimodal Explanation for Visual Question Answering</title>
      <author><first>Jialin</first><last>Wu</last></author>
      <author><first>Raymond</first><last>Mooney</last></author>
      <pages>103–112</pages>
      <abstract>AI systems’ ability to explain their reasoning is critical to their utility and trustworthiness. Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA). However, most of them are opaque black boxes with limited explanatory capability. This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation.</abstract>
      <url hash="1dadfddd">W19-4812</url>
      <doi>10.18653/v1/W19-4812</doi>
      <bibkey>wu-mooney-2019-faithful</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="13">
      <title>Evaluating Recurrent Neural Network Explanations</title>
      <author><first>Leila</first><last>Arras</last></author>
      <author><first>Ahmed</first><last>Osman</last></author>
      <author><first>Klaus-Robert</first><last>Müller</last></author>
      <author><first>Wojciech</first><last>Samek</last></author>
      <pages>113–126</pages>
      <abstract>Recently, several methods have been proposed to explain the predictions of recurrent neural networks (RNNs), in particular of LSTMs. The goal of these methods is to understand the network’s decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these methods were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the negation in sentiment analysis reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples.</abstract>
      <url hash="64fd258c">W19-4813</url>
      <doi>10.18653/v1/W19-4813</doi>
      <bibkey>arras-etal-2019-evaluating</bibkey>
      <pwccode url="https://github.com/ArrasL/LRP_for_LSTM" additional="false">ArrasL/LRP_for_LSTM</pwccode>
    </paper>
    <paper id="14">
      <title>On the Realization of Compositionality in Neural Networks</title>
      <author><first>Joris</first><last>Baan</last></author>
      <author><first>Jana</first><last>Leible</last></author>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>David</first><last>Rau</last></author>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Tim</first><last>Baumgärtner</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Elia</first><last>Bruni</last></author>
      <pages>127–137</pages>
      <abstract>We present a detailed comparison of two types of sequence to sequence models trained to conduct a compositional task. The models are architecturally identical at inference time, but differ in the way that they are trained: our baseline model is trained with a task-success signal only, while the other model receives additional supervision on its attention mechanism (Attentive Guidance), which has shown to be an effective method for encouraging more compositional solutions. We first confirm that the models with attentive guidance indeed infer more compositional solutions than the baseline, by training them on the lookup table task presented by Liska et al. (2019). We then do an in-depth analysis of the structural differences between the two model types, focusing in particular on the organisation of the parameter space and the hidden layer activations and find noticeable differences in both these aspects. Guided networks focus more on the components of the input rather than the sequence as a whole and develop small functional groups of neurons with specific purposes that use their gates more selectively. Results from parameter heat maps, component swapping and graph analysis also indicate that guided networks exhibit a more modular structure with a small number of specialized, strongly connected neurons.</abstract>
      <url hash="1547ab52">W19-4814</url>
      <doi>10.18653/v1/W19-4814</doi>
      <bibkey>baan-etal-2019-realization</bibkey>
    </paper>
    <paper id="15">
      <title>Learning the <fixed-case>D</fixed-case>yck Language with Attention-based <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Models</title>
      <author><first>Xiang</first><last>Yu</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <pages>138–146</pages>
      <abstract>The generalized Dyck language has been used to analyze the ability of Recurrent Neural Networks (RNNs) to learn context-free grammars (CFGs). Recent studies draw conflicting conclusions on their performance, especially regarding the generalizability of the models with respect to the depth of recursion. In this paper, we revisit several common models and experimental settings, discuss the potential problems of the tasks and analyses. Furthermore, we explore the use of attention mechanisms within the seq2seq framework to learn the Dyck language, which could compensate for the limited encoding ability of RNNs. Our findings reveal that attention mechanisms still cannot truly generalize over the recursion depth, although they perform much better than other models on the closing bracket tagging task. Moreover, this also suggests that this commonly used task is not sufficient to test a model’s understanding of CFGs.</abstract>
      <url hash="0b253586">W19-4815</url>
      <doi>10.18653/v1/W19-4815</doi>
      <bibkey>yu-etal-2019-learning</bibkey>
    </paper>
    <paper id="16">
      <title>Modeling Paths for Explainable Knowledge Base Completion</title>
      <author><first>Josua</first><last>Stadelmaier</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>147–157</pages>
      <abstract>A common approach in knowledge base completion (KBC) is to learn representations for entities and relations in order to infer missing facts by generalizing existing ones. A shortcoming of standard models is that they do not explain their predictions to make them verifiable easily to human inspection. In this paper, we propose the Context Path Model (CPM) which generates explanations for new facts in KBC by providing sets of <i>context paths</i> as supporting evidence for these triples. For example, a new triple (Theresa May, nationality, Britain) may be explained by the path (Theresa May, born in, Eastbourne, contained in, Britain). The CPM is formulated as a wrapper that can be applied on top of various existing KBC models. We evaluate it for the well-established TransE model. We observe that its performance remains very close despite the added complexity, and that most of the paths proposed as explanations provide meaningful evidence to assess the correctness.</abstract>
      <url hash="12c1a133">W19-4816</url>
      <doi>10.18653/v1/W19-4816</doi>
      <bibkey>stadelmaier-pado-2019-modeling</bibkey>
      <pwccode url="https://github.com/JosuaStadelmaier/CPM" additional="false">JosuaStadelmaier/CPM</pwccode>
    </paper>
    <paper id="17">
      <title>Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in <fixed-case>F</fixed-case>rench and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>158–172</pages>
      <abstract>The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages. Results are at present inconclusive. In this paper, we extend previous work on long-distance dependencies in three ways. We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties under study. We extend the work to sentence embeddings and to new languages. We confirm previous negative results: word embeddings and sentence embeddings do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.</abstract>
      <url hash="08d52991">W19-4817</url>
      <doi>10.18653/v1/W19-4817</doi>
      <bibkey>merlo-2019-probing</bibkey>
    </paper>
    <paper id="18">
      <title>Derivational Morphological Relations in Word Embeddings</title>
      <author><first>Tomáš</first><last>Musil</last></author>
      <author><first>Jonáš</first><last>Vidra</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>173–180</pages>
      <abstract>Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmas into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation ‘bake–baker’ belongs to category ‘actor’, and a correct clustering puts it into the same cluster as ‘govern–governor’).</abstract>
      <url hash="38344cde">W19-4818</url>
      <doi>10.18653/v1/W19-4818</doi>
      <bibkey>musil-etal-2019-derivational</bibkey>
    </paper>
    <paper id="19">
      <title>Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations</title>
      <author><first>Ethan</first><last>Wilcox</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <pages>181–190</pages>
      <abstract>Work using artificial languages as training input has shown that LSTMs are capable of inducing the stack-like data structures required to represent context-free and certain mildly context-sensitive languages — formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena: center embedding sentences and syntactic island constraints on the filler–gap dependency. In order to properly predict words in these structures, a model must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state.</abstract>
      <url hash="0b22863c">W19-4819</url>
      <doi>10.18653/v1/W19-4819</doi>
      <bibkey>wilcox-etal-2019-hierarchical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="20">
      <title>Blackbox Meets Blackbox: Representational Similarity &amp; Stability Analysis of Neural Language Models and Brains</title>
      <author><first>Samira</first><last>Abnar</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Willem</first><last>Zuidema</last></author>
      <pages>191–203</pages>
      <abstract>In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al./ (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.</abstract>
      <url hash="05e3855e">W19-4820</url>
      <doi>10.18653/v1/W19-4820</doi>
      <bibkey>abnar-etal-2019-blackbox</bibkey>
      <pwccode url="https://github.com/samiraabnar/Bridge" additional="false">samiraabnar/Bridge</pwccode>
    </paper>
    <paper id="21">
      <title>An <fixed-case>LSTM</fixed-case> Adaptation Study of (Un)grammaticality</title>
      <author><first>Shammur Absar</first><last>Chowdhury</last></author>
      <author><first>Roberto</first><last>Zamparelli</last></author>
      <pages>204–212</pages>
      <abstract>We propose a novel approach to the study of how artificial neural network perceive the distinction between grammatical and ungrammatical sentences, a crucial task in the growing field of synthetic linguistics. The method is based on performance measures of language models trained on corpora and fine-tuned with either grammatical or ungrammatical sentences, then applied to (different types of) grammatical or ungrammatical sentences. The results show that both in the difficult and highly symmetrical task of detecting subject islands and in the more open CoLA dataset, grammatical sentences give rise to better scores than ungrammatical ones, possibly because they can be better integrated within the body of linguistic structural knowledge that the language model has accumulated.</abstract>
      <url hash="aaebd5b8">W19-4821</url>
      <doi>10.18653/v1/W19-4821</doi>
      <bibkey>chowdhury-zamparelli-2019-lstm</bibkey>
      <pwccode url="https://github.com/LiCo-TREiL/Computational-Ungrammaticality" additional="false">LiCo-TREiL/Computational-Ungrammaticality</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
    </paper>
    <paper id="22">
      <title>An Analysis of Source-Side Grammatical Errors in <fixed-case>NMT</fixed-case></title>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <pages>213–223</pages>
      <abstract>The quality of Neural Machine Translation (NMT) has been shown to significantly degrade when confronted with source-side noise. We present the first large-scale study of state-of-the-art English-to-German NMT on real grammatical noise, by evaluating on several Grammar Correction corpora. We present methods for evaluating NMT robustness without true references, and we use them for extensive analysis of the effects that different grammatical errors have on the NMT output. We also introduce a technique for visualizing the divergence distribution caused by a source-side error, which allows for additional insights.</abstract>
      <url hash="202bae8b">W19-4822</url>
      <doi>10.18653/v1/W19-4822</doi>
      <bibkey>anastasopoulos-2019-analysis</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="23">
      <title>Finding Hierarchical Structure in Neural Stacks Using Unsupervised Parsing</title>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Lenny</first><last>Khazan</last></author>
      <author><first>Noah</first><last>Amsel</last></author>
      <author><first>Yiding</first><last>Hao</last></author>
      <author><first>Simon</first><last>Mendelsohn</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <pages>224–232</pages>
      <abstract>Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities. It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differentiable stack is not always interpretable. In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsupervised learning of constituency structure. Using a technique due to Shen et al. (2018a,b), we extract syntactic trees from the pushing behavior of stack RNNs trained on language modeling and classification objectives. We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack RNNs do indeed infer linguistically relevant hierarchical structure.</abstract>
      <url hash="1af922d8">W19-4823</url>
      <doi>10.18653/v1/W19-4823</doi>
      <bibkey>merrill-etal-2019-finding</bibkey>
      <pwccode url="https://github.com/viking-sudo-rm/industrial-stacknns" additional="false">viking-sudo-rm/industrial-stacknns</pwccode>
    </paper>
    <paper id="24">
      <title>Adversarial Attack on Sentiment Classification</title>
      <author><first>Yi-Ting</first><last>Tsai</last></author>
      <author><first>Min-Chu</first><last>Yang</last></author>
      <author><first>Han-Yu</first><last>Chen</last></author>
      <pages>233–240</pages>
      <abstract>In this paper, we propose a white-box attack algorithm called “Global Search” method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called “Greedy Search”. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed “Global Search” method generates more powerful adversarial examples with less distortion or less modification to the source text.</abstract>
      <url hash="a017d6fd">W19-4824</url>
      <doi>10.18653/v1/W19-4824</doi>
      <bibkey>tsai-etal-2019-adversarial-attack</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="25">
      <title>Open Sesame: Getting inside <fixed-case>BERT</fixed-case>’s Linguistic Knowledge</title>
      <author><first>Yongjie</first><last>Lin</last></author>
      <author><first>Yi Chern</first><last>Tan</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <pages>241–253</pages>
      <abstract>How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT’s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT’s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT’s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.</abstract>
      <url hash="335da8ed">W19-4825</url>
      <doi>10.18653/v1/W19-4825</doi>
      <bibkey>lin-etal-2019-open</bibkey>
      <pwccode url="https://github.com/yongjie-lin/bert-opensesame" additional="false">yongjie-lin/bert-opensesame</pwccode>
    </paper>
    <paper id="26">
      <title><fixed-case>GE</fixed-case>val: Tool for Debugging <fixed-case>NLP</fixed-case> Datasets and Models</title>
      <author><first>Filip</first><last>Graliński</last></author>
      <author><first>Anna</first><last>Wróblewska</last></author>
      <author><first>Tomasz</first><last>Stanisławek</last></author>
      <author><first>Kamil</first><last>Grabowski</last></author>
      <author><first>Tomasz</first><last>Górecki</last></author>
      <pages>254–262</pages>
      <abstract>This paper presents a simple but general and effective method to debug the output of machine learning (ML) supervised models, including neural networks. The algorithm looks for features that lower the evaluation metric in such a way that it cannot be ascribed to chance (as measured by their p-values). Using this method – implemented as MLEval tool – you can find: (1) anomalies in test sets, (2) issues in preprocessing, (3) problems in the ML model itself. It can give you an insight into what can be improved in the datasets and/or the model. The same method can be used to compare ML models or different versions of the same model. We present the tool, the theory behind it and use cases for text-based models of various types.</abstract>
      <url hash="2c80a217">W19-4826</url>
      <doi>10.18653/v1/W19-4826</doi>
      <bibkey>gralinski-etal-2019-geval</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="27">
      <title>From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions</title>
      <author><first>David</first><last>Mareček</last></author>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <pages>263–275</pages>
      <abstract>We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.</abstract>
      <url hash="2ad7ef9e">W19-4827</url>
      <doi>10.18653/v1/W19-4827</doi>
      <bibkey>marecek-rosa-2019-balustrades</bibkey>
    </paper>
    <paper id="28">
      <title>What Does <fixed-case>BERT</fixed-case> Look at? An Analysis of <fixed-case>BERT</fixed-case>’s Attention</title>
      <author><first>Kevin</first><last>Clark</last></author>
      <author><first>Urvashi</first><last>Khandelwal</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <pages>276–286</pages>
      <abstract>Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.</abstract>
      <url hash="54f98d80">W19-4828</url>
      <doi>10.18653/v1/W19-4828</doi>
      <bibkey>clark-etal-2019-bert</bibkey>
      <pwccode url="https://github.com/clarkkev/attention-analysis" additional="true">clarkkev/attention-analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
  </volume>
  <volume id="49">
    <meta>
      <booktitle>Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP</booktitle>
      <url hash="b3f053c0">W19-49</url>
      <editor><first>Haim</first><last>Dubossarsky</last></editor>
      <editor><first>Arya D.</first><last>McCarthy</last></editor>
      <editor><first>Edoardo Maria</first><last>Ponti</last></editor>
      <editor><first>Ivan</first><last>Vulić</last></editor>
      <editor><first>Ekaterina</first><last>Vylomova</last></editor>
      <editor><first>Yevgeni</first><last>Berzak</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <editor><first>Manaal</first><last>Faruqui</last></editor>
      <editor><first>Anna</first><last>Korhonen</last></editor>
      <editor><first>Roi</first><last>Reichart</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>sigtyp</venue>
    </meta>
    <frontmatter>
      <url hash="6e726bb6">W19-4900</url>
      <bibkey>ws-2019-typ</bibkey>
    </frontmatter>
  </volume>
  <volume id="50">
    <meta>
      <booktitle>Proceedings of the 18th BioNLP Workshop and Shared Task</booktitle>
      <url hash="3e7a898e">W19-50</url>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <editor><first>Kevin Bretonnel</first><last>Cohen</last></editor>
      <editor><first>Sophia</first><last>Ananiadou</last></editor>
      <editor><first>Junichi</first><last>Tsujii</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>bionlp</venue>
    </meta>
    <frontmatter>
      <url hash="10a1583d">W19-5000</url>
      <bibkey>ws-2019-bionlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Classifying the reported ability in clinical mobility descriptions</title>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Ayah</first><last>Zirikly</last></author>
      <author><first>Guy</first><last>Divita</last></author>
      <author><first>Bart</first><last>Desmet</last></author>
      <pages>1–10</pages>
      <abstract>Assessing how individuals perform different activities is key information for modeling health states of individuals and populations. Descriptions of activity performance in clinical free text are complex, including syntactic negation and similarities to textual entailment tasks. We explore a variety of methods for the novel task of classifying four types of assertions about activity performance: Able, Unable, Unclear, and None (no information). We find that ensembling an SVM trained with lexical features and a CNN achieves 77.9% macro F1 score on our task, and yields nearly 80% recall on the rare Unclear and Unable samples. Finally, we highlight several challenges in classifying performance assertions, including capturing information about sources of assistance, incorporating syntactic structure and negation scope, and handling new modalities at test time. Our findings establish a strong baseline for this novel task, and identify intriguing areas for further research.</abstract>
      <url hash="3a902d3d">W19-5001</url>
      <doi>10.18653/v1/W19-5001</doi>
      <bibkey>newman-griffis-etal-2019-classifying</bibkey>
    </paper>
    <paper id="2">
      <title>Learning from the Experience of Doctors: Automated Diagnosis of Appendicitis Based on Clinical Notes</title>
      <author><first>Steven Kester</first><last>Yuwono</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <author><first>Kee Yuan</first><last>Ngiam</last></author>
      <pages>11–19</pages>
      <abstract>The objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ED) note and additional structured information (e.g., lab test results). Our clinical corpus consists of about 180,000 ED notes based on ten years of patient visits to the Accident and Emergency (A&amp;E) Department of the National University Hospital (NUH), Singapore. We propose a novel neural network approach that learns to diagnose acute appendicitis based on doctors’ free-text ED notes without any feature engineering. On a test set of 2,000 ED notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ED notes only consist of abdominal-related diagnosis, our model is able to achieve a promising F_0.5-score of 0.895 while ED doctors achieve F_0.5-score of 0.900. Visualization shows that our model is able to learn important features, signs, and symptoms of patients from unstructured free-text ED notes, which will help doctors to make better diagnosis.</abstract>
      <url hash="8f976c72">W19-5002</url>
      <doi>10.18653/v1/W19-5002</doi>
      <bibkey>yuwono-etal-2019-learning</bibkey>
    </paper>
    <paper id="3">
      <title>A Paraphrase Generation System for <fixed-case>EHR</fixed-case> Question Answering</title>
      <author><first>Sarvesh</first><last>Soni</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <pages>20–29</pages>
      <abstract>This paper proposes a dataset and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs). Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters. This corpus is then used with a deep learning-based question paraphrasing method utilizing variational autoencoder and LSTM encoder/decoder. The ultimate use of such a method is to improve the performance of automatic question answering methods for EHRs.</abstract>
      <url hash="f5a6e8f8">W19-5003</url>
      <doi>10.18653/v1/W19-5003</doi>
      <bibkey>soni-roberts-2019-paraphrase</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>RE</fixed-case>flex: Flexible Framework for Relation Extraction in Multiple Domains</title>
      <author><first>Geeticka</first><last>Chauhan</last></author>
      <author><first>Matthew B.A.</first><last>McDermott</last></author>
      <author><first>Peter</first><last>Szolovits</last></author>
      <pages>30–47</pages>
      <abstract>Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets. By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of preprocessing are a large contributor performance and that omission of such information can further hinder fair comparison. Other insights from our exploration allow us to provide recommendations for future research in this area.</abstract>
      <url hash="874ff45c">W19-5004</url>
      <doi>10.18653/v1/W19-5004</doi>
      <bibkey>chauhan-etal-2019-reflex</bibkey>
      <pwccode url="https://github.com/geetickachauhan/relation-extraction" additional="false">geetickachauhan/relation-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ddi">DDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="5">
      <title>Analysing Representations of Memory Impairment in a Clinical Notes Classification Model</title>
      <author><first>Mark</first><last>Ormerod</last></author>
      <author><first>Jesús</first><last>Martínez-del-Rincón</last></author>
      <author><first>Neil</first><last>Robertson</last></author>
      <author><first>Bernadette</first><last>McGuinness</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <pages>48–57</pages>
      <abstract>Despite recent advances in the application of deep neural networks to various kinds of medical data, extracting information from unstructured textual sources remains a challenging task. The challenges of training and interpreting document classification models are amplified when dealing with small and highly technical datasets, as are common in the clinical domain. Using a dataset of de-identified clinical letters gathered at a memory clinic, we construct several recurrent neural network models for letter classification, and evaluate them on their ability to build meaningful representations of the documents and predict patients’ diagnoses. Additionally, we probe sentence embedding models in order to build a human-interpretable representation of the neural network’s features, using a simple and intuitive technique based on perturbative approaches to sentence importance. In addition to showing which sentences in a document are most informative about the patient’s condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses. Furthermore, we identify clusters of sentences in the embedding space that correlate strongly with importance scores for each clinical diagnosis class.</abstract>
      <url hash="1e15a39f">W19-5005</url>
      <doi>10.18653/v1/W19-5005</doi>
      <bibkey>ormerod-etal-2019-analysing</bibkey>
    </paper>
    <paper id="6">
      <title>Transfer Learning in Biomedical Natural Language Processing: An Evaluation of <fixed-case>BERT</fixed-case> and <fixed-case>ELM</fixed-case>o on Ten Benchmarking Datasets</title>
      <author><first>Yifan</first><last>Peng</last></author>
      <author><first>Shankai</first><last>Yan</last></author>
      <author><first>Zhiyong</first><last>Lu</last></author>
      <pages>58–65</pages>
      <abstract>Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark.</abstract>
      <url hash="a11e840b">W19-5006</url>
      <doi>10.18653/v1/W19-5006</doi>
      <bibkey>peng-etal-2019-transfer</bibkey>
      <pwccode url="https://github.com/ncbi-nlp/NCBI_BERT" additional="true">ncbi-nlp/NCBI_BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/biosses">BIOSSES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chemprot">ChemProt</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ddi">DDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hoc-1">HOC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mednli">MedNLI</pwcdataset>
    </paper>
    <paper id="7">
      <title>Combining Structured and Free-text Electronic Medical Record Data for Real-time Clinical Decision Support</title>
      <author><first>Emilia</first><last>Apostolova</last></author>
      <author><first>Tony</first><last>Wang</last></author>
      <author><first>Tim</first><last>Tschampel</last></author>
      <author><first>Ioannis</first><last>Koutroulis</last></author>
      <author><first>Tom</first><last>Velez</last></author>
      <pages>66–70</pages>
      <abstract>The goal of this work is to utilize Electronic Medical Record (EMR) data for real-time Clinical Decision Support (CDS). We present a deep learning approach to combining in real time available diagnosis codes (ICD codes) and free-text notes: Patient Context Vectors. Patient Context Vectors are created by averaging ICD code embeddings, and by predicting the same from free-text notes via a Convolutional Neural Network. The Patient Context Vectors were then simply appended to available structured data (vital signs and lab results) to build prediction models for a specific condition. Experiments on predicting ARDS, a rare and complex condition, demonstrate the utility of Patient Context Vectors as a means of summarizing the patient history and overall condition, and improve significantly the prediction model results.</abstract>
      <url hash="2d685886">W19-5007</url>
      <doi>10.18653/v1/W19-5007</doi>
      <bibkey>apostolova-etal-2019-combining</bibkey>
      <pwccode url="https://github.com/ema-/patient-context-vectors" additional="false">ema-/patient-context-vectors</pwccode>
    </paper>
    <paper id="8">
      <title><fixed-case>M</fixed-case>o<fixed-case>NER</fixed-case>o: a Biomedical Gold Standard Corpus for the <fixed-case>R</fixed-case>omanian Language</title>
      <author><first>Maria</first><last>Mitrofan</last></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Grigorina</first><last>Mitrofan</last></author>
      <pages>71–79</pages>
      <abstract>In an era when large amounts of data are generated daily in various fields, the biomedical field among others, linguistic resources can be exploited for various tasks of Natural Language Processing. Moreover, increasing number of biomedical documents are available in languages other than English. To be able to extract information from natural language free text resources, methods and tools are needed for a variety of languages. This paper presents the creation of the MoNERo corpus, a gold standard biomedical corpus for Romanian, annotated with both part of speech tags and named entities. MoNERo comprises 154,825 morphologically annotated tokens and 23,188 entity annotations belonging to four entity semantic groups corresponding to UMLS Semantic Groups.</abstract>
      <url hash="b6ee4ebf">W19-5008</url>
      <doi>10.18653/v1/W19-5008</doi>
      <bibkey>mitrofan-etal-2019-monero</bibkey>
    </paper>
    <paper id="9">
      <title>Domain Adaptation of <fixed-case>SRL</fixed-case> Systems for Biological Processes</title>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Nidhi</first><last>Vyas</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Anirudha</first><last>Rayasam</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>80–87</pages>
      <abstract>Domain adaptation remains one of the most challenging aspects in the wide-spread use of Semantic Role Labeling (SRL) systems. Current state-of-the-art methods are typically trained on large-scale datasets, but their performances do not directly transfer to low-resource domain-specific settings. In this paper, we propose two approaches for domain adaptation in the biological domain that involves pre-training LSTM-CRF based on existing large-scale datasets and adapting it for a low-resource corpus of biological processes. Our first approach defines a mapping between the source labels and the target labels, and the other approach modifies the final CRF layer in sequence-labeling neural network architecture. We perform our experiments on ProcessBank dataset which contains less than 200 paragraphs on biological processes. We improve over the previous state-of-the-art system on this dataset by 21 F1 points. We also show that, by incorporating event-event relationship in ProcessBank, we are able to achieve an additional 2.6 F1 gain, giving us possible insights into how to improve SRL systems for biological process using richer annotations.</abstract>
      <url hash="fa7e335a">W19-5009</url>
      <doi>10.18653/v1/W19-5009</doi>
      <bibkey>rajagopal-etal-2019-domain</bibkey>
    </paper>
    <paper id="10">
      <title>Deep Contextualized Biomedical Abbreviation Expansion</title>
      <author><first>Qiao</first><last>Jin</last></author>
      <author><first>Jinling</first><last>Liu</last></author>
      <author><first>Xinghua</first><last>Lu</last></author>
      <pages>88–96</pages>
      <abstract>Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as information retrieval and question answering systems. In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple heuristic. Then it utilizes BioELMo to extract the contextualized features of words, and feed those features to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings.</abstract>
      <url hash="2c727e76">W19-5010</url>
      <doi>10.18653/v1/W19-5010</doi>
      <bibkey>jin-etal-2019-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="11">
      <title><fixed-case>RNN</fixed-case> Embeddings for Identifying Difficult to Understand Medical Words</title>
      <author><first>Hanna</first><last>Pylieva</last></author>
      <author><first>Artem</first><last>Chernodub</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <pages>97–104</pages>
      <abstract>Patients and their families often require a better understanding of medical information provided by doctors. We currently address this issue by improving the identification of difficult to understand medical words. We introduce novel embeddings received from RNN - FrnnMUTE (French RNN Medical Understandability Text Embeddings) which allow to reach up to 87.0 F1 score in identification of difficult words. We also note that adding pre-trained FastText word embeddings to the feature set substantially improves the performance of the model which classifies words according to their difficulty. We study the generalizability of different models through three cross-validation scenarios which allow testing classifiers in real-world conditions: understanding of medical words by new users, and classification of new unseen words by the automatic models. The RNN - FrnnMUTE embeddings and the categorization code are being made available for the research.</abstract>
      <url hash="9cc80062">W19-5011</url>
      <doi>10.18653/v1/W19-5011</doi>
      <bibkey>pylieva-etal-2019-rnn</bibkey>
      <pwccode url="https://github.com/hpylieva/FrnnMUTE" additional="false">hpylieva/FrnnMUTE</pwccode>
    </paper>
    <paper id="12">
      <title>A distantly supervised dataset for automated data extraction from diagnostic studies</title>
      <author><first>Christopher</first><last>Norman</last></author>
      <author><first>Mariska</first><last>Leeflang</last></author>
      <author><first>René</first><last>Spijker</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>105–114</pages>
      <abstract>Systematic reviews are important in evidence based medicine, but are expensive to produce. Automating or semi-automating the data extraction of index test, target condition, and reference standard from articles has the potential to decrease the cost of conducting systematic reviews of diagnostic test accuracy, but relevant training data is not available. We create a distantly supervised dataset of approximately 90,000 sentences, and let two experts manually annotate a small subset of around 1,000 sentences for evaluation. We evaluate the performance of BioBERT and logistic regression for ranking the sentences, and compare the performance for distant and direct supervision. Our results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators.</abstract>
      <url hash="cbbbc269">W19-5012</url>
      <doi>10.18653/v1/W19-5012</doi>
      <bibkey>norman-etal-2019-distantly</bibkey>
    </paper>
    <paper id="13">
      <title>Query selection methods for automated corpora construction with a use case in food-drug interactions</title>
      <author><first>Georgeta</first><last>Bordea</last></author>
      <author><first>Tsanta</first><last>Randriatsitohaina</last></author>
      <author><first>Fleur</first><last>Mougin</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <pages>115–124</pages>
      <abstract>In this paper, we address the problem of automatically constructing a relevant corpus of scientific articles about food-drug interactions. There is a growing number of scientific publications that describe food-drug interactions but currently building a high-coverage corpus that can be used for information extraction purposes is not trivial. We investigate several methods for automating the query selection process using an expert-curated corpus of food-drug interactions. Our experiments show that index term features along with a decision tree classifier are the best approach for this task and that feature selection approaches and in particular gain ratio outperform frequency-based methods for query selection.</abstract>
      <url hash="73a7e558">W19-5013</url>
      <doi>10.18653/v1/W19-5013</doi>
      <bibkey>bordea-etal-2019-query</bibkey>
    </paper>
    <paper id="14">
      <title>Enhancing biomedical word embeddings by retrofitting to verb clusters</title>
      <author><first>Billy</first><last>Chiu</last></author>
      <author><first>Simon</first><last>Baker</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>125–134</pages>
      <abstract>Verbs play a fundamental role in many biomed-ical tasks and applications such as relation and event extraction. We hypothesize that performance on many downstream tasks can be improved by aligning the input pretrained embeddings according to semantic verb classes.In this work, we show that by using semantic clusters for verbs, a large lexicon of verbclasses derived from biomedical literature, weare able to improve the performance of common pretrained embeddings in downstream tasks by retrofitting them to verb classes. We present a simple and computationally efficient approach using a widely-available “off-the-shelf” retrofitting algorithm to align pretrained embeddings according to semantic verb clusters. We achieve state-of-the-art results on text classification and relation extraction tasks.</abstract>
      <url hash="3f32885e">W19-5014</url>
      <doi>10.18653/v1/W19-5014</doi>
      <bibkey>chiu-etal-2019-enhancing</bibkey>
      <pwccode url="https://github.com/cambridgeltl/retrofitted-bio-embeddings" additional="false">cambridgeltl/retrofitted-bio-embeddings</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hoc-1">HOC</pwcdataset>
    </paper>
    <paper id="15">
      <title>A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics</title>
      <author><first>Aditya</first><last>Joshi</last></author>
      <author><first>Sarvnaz</first><last>Karimi</last></author>
      <author><first>Ross</first><last>Sparks</last></author>
      <author><first>Cecile</first><last>Paris</last></author>
      <author><first>C Raina</first><last>MacIntyre</last></author>
      <pages>135–141</pages>
      <abstract>Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4% in the accuracy when these context-based representations are used instead of word-based representations.</abstract>
      <url hash="4bcab4e3">W19-5015</url>
      <doi>10.18653/v1/W19-5015</doi>
      <bibkey>joshi-etal-2019-comparison</bibkey>
    </paper>
    <paper id="16">
      <title>Constructing large scale biomedical knowledge bases from scratch with rapid annotation of interpretable patterns</title>
      <author><first>Julien</first><last>Fauqueur</last></author>
      <author><first>Ashok</first><last>Thillaisundaram</last></author>
      <author><first>Theodosia</first><last>Togia</last></author>
      <pages>142–151</pages>
      <abstract>Knowledge base construction is crucial for summarising, understanding and inferring relationships between biomedical entities. However, for many practical applications such as drug discovery, the scarcity of relevant facts (e.g. gene X is therapeutic target for disease Y) severely limits a domain expert’s ability to create a usable knowledge base, either directly or by training a relation extraction model. In this paper, we present a simple and effective method of extracting new facts with a pre-specified binary relationship type from the biomedical literature, without requiring any training data or hand-crafted rules. Our system discovers, ranks and presents the most salient patterns to domain experts in an interpretable form. By marking patterns as compatible with the desired relationship type, experts indirectly batch-annotate candidate pairs whose relationship is expressed with such patterns in the literature. Even with a complete absence of seed data, experts are able to discover thousands of high-quality pairs with the desired relationship within minutes. When a small number of relevant pairs do exist - even when their relationship is more general (e.g. gene X is biologically associated with disease Y) than the relationship of interest - our system leverages them in order to i) learn a better ranking of the patterns to be annotated or ii) generate weakly labelled pairs in a fully automated manner. We evaluate our method both intrinsically and via a downstream knowledge base completion task, and show that it is an effective way of constructing knowledge bases when few or no relevant facts are already available.</abstract>
      <url hash="9128de0b">W19-5016</url>
      <doi>10.18653/v1/W19-5016</doi>
      <bibkey>fauqueur-etal-2019-constructing</bibkey>
    </paper>
    <paper id="17">
      <title>First Steps towards Building a Medical Lexicon for <fixed-case>S</fixed-case>panish with Linguistic and Semantic Information</title>
      <author><first>Leonardo</first><last>Campillos-Llanos</last></author>
      <pages>152–164</pages>
      <abstract>We report the work-in-progress of collecting MedLexSp, an unified medical lexicon for the Spanish language, featuring terms and inflected word forms mapped to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs), semantic types and groups. First, we leveraged a list of term lemmas and forms from a previous project, and mapped them to UMLS terms and CUIs. To enrich the lexicon, we used both domain-corpora (e.g. Summaries of Product Characteristics and MedlinePlus) and natural language processing techniques such as string distance methods or generation of syntactic variants of multi-word terms. We also added term variants by mapping their CUIs to missing items available in the Spanish versions of standard thesauri (e.g. Medical Subject Headings and World Health Organization Adverse Drug Reactions terminology). We enhanced the vocabulary coverage by gathering missing terms from resources such as the Anatomical Therapeutical Classification, the National Cancer Institute (NCI) Dictionary of Cancer Terms, OrphaData, or the Nomenclátor de Prescripción for drug names. Part-of-Speech information is being included in the lexicon, and the current version amounts up to 76 454 lemmas and 203 043 inflected forms (including conjugated verbs, number and gender variants), corresponding to 30 647 UMLS CUIs. MedLexSp is distributed freely for research purposes.</abstract>
      <url hash="ab0f8265">W19-5017</url>
      <doi>10.18653/v1/W19-5017</doi>
      <bibkey>campillos-llanos-2019-first</bibkey>
      <pwccode url="https://github.com/lcampillos/bionlp2019" additional="false">lcampillos/bionlp2019</pwccode>
    </paper>
    <paper id="18">
      <title>Incorporating Figure Captions and Descriptive Text in <fixed-case>M</fixed-case>e<fixed-case>SH</fixed-case> Term Indexing</title>
      <author><first>Xindi</first><last>Wang</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <pages>165–175</pages>
      <abstract>The goal of text classification is to automatically assign categories to documents. Deep learning automatically learns effective features from data instead of adopting human-designed features. In this paper, we focus specifically on biomedical document classification using a deep learning approach. We present a novel multichannel TextCNN model for MeSH term indexing. Beyond the normal use of the text from the abstract and title for model training, we also consider figure and table captions, as well as paragraphs associated with the figures and tables. We demonstrate that these latter text sources are important feature sources for our method. A new dataset consisting of these text segments curated from 257,590 full text articles together with the articles’ MEDLINE/PubMed MeSH terms is publicly available.</abstract>
      <url hash="ad8d2449">W19-5018</url>
      <doi>10.18653/v1/W19-5018</doi>
      <bibkey>wang-mercer-2019-incorporating</bibkey>
      <pwccode url="https://github.com/xdwang0726/Mesh" additional="false">xdwang0726/Mesh</pwccode>
    </paper>
    <paper id="19">
      <title><fixed-case>B</fixed-case>io<fixed-case>R</fixed-case>el<fixed-case>E</fixed-case>x 1.0: Biological Relation Extraction Benchmark</title>
      <author><first>Hrant</first><last>Khachatrian</last></author>
      <author><first>Lilit</first><last>Nersisyan</last></author>
      <author><first>Karen</first><last>Hambardzumyan</last></author>
      <author><first>Tigran</first><last>Galstyan</last></author>
      <author><first>Anna</first><last>Hakobyan</last></author>
      <author><first>Arsen</first><last>Arakelyan</last></author>
      <author><first>Andrey</first><last>Rzhetsky</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>176–190</pages>
      <abstract>Automatic extraction of relations and interactions between biological entities from scientific literature remains an extremely challenging problem in biomedical information extraction and natural language processing in general. One of the reasons for slow progress is the relative scarcity of standardized and publicly available benchmarks. In this paper we introduce BioRelEx, a new dataset of fully annotated sentences from biomedical literature that capture <i>binding</i> interactions between proteins and/or biomolecules. To foster reproducible research on the interaction extraction task, we define a precise and transparent evaluation process, tools for error analysis and significance tests. Finally, we conduct extensive experiments to evaluate several baselines, including SciIE, a recently introduced neural multi-task architecture that has demonstrated state-of-the-art performance on several tasks.</abstract>
      <url hash="af6c95a9">W19-5019</url>
      <doi>10.18653/v1/W19-5019</doi>
      <bibkey>khachatrian-etal-2019-biorelex</bibkey>
      <pwccode url="https://github.com/YerevaNN/BioRelEx" additional="false">YerevaNN/BioRelEx</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="20">
      <title>Extraction of Lactation Frames from Drug Labels and <fixed-case>L</fixed-case>act<fixed-case>M</fixed-case>ed</title>
      <author><first>Heath</first><last>Goodrum</last></author>
      <author><first>Meghana</first><last>Gudala</last></author>
      <author><first>Ankita</first><last>Misra</last></author>
      <author><first>Kirk</first><last>Roberts</last></author>
      <pages>191–200</pages>
      <abstract>This paper describes a natural language processing (NLP) approach to extracting lactation-specific drug information from two sources: FDA-mandated drug labels and the NLM Drugs and Lactation Database (LactMed). A frame semantic approach is utilized, and the paper describes the selected frames, their annotation on a set of 900 sections from drug labels and LactMed articles, and the NLP system to extract such frame instances automatically. The ultimate goal of the project is to use such a system to identify discrepancies in lactation-related drug information between these resources.</abstract>
      <url hash="96e58333">W19-5020</url>
      <doi>10.18653/v1/W19-5020</doi>
      <bibkey>goodrum-etal-2019-extraction</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="21">
      <title>Annotating Temporal Information in Clinical Notes for Timeline Reconstruction: Towards the Definition of Calendar Expressions</title>
      <author><first>Natalia</first><last>Viani</last></author>
      <author><first>Hegler</first><last>Tissot</last></author>
      <author><first>Ariane</first><last>Bernardino</last></author>
      <author><first>Sumithra</first><last>Velupillai</last></author>
      <pages>201–210</pages>
      <abstract>To automatically analyse complex trajectory information enclosed in clinical text (e.g. timing of symptoms, duration of treatment), it is important to understand the related temporal aspects, anchoring each event on an absolute point in time. In the clinical domain, few temporally annotated corpora are currently available. Moreover, underlying annotation schemas - which mainly rely on the TimeML standard - are not necessarily easily applicable for applications such as patient timeline reconstruction. In this work, we investigated how temporal information is documented in clinical text by annotating a corpus of medical reports with time expressions (TIMEXes), based on TimeML. The developed corpus is available to the NLP community. Starting from our annotations, we analysed the suitability of the TimeML TIMEX schema for capturing timeline information, identifying challenges and possible solutions. As a result, we propose a novel annotation schema that could be useful for timeline reconstruction: CALendar EXpression (CALEX).</abstract>
      <url hash="9321a6b5">W19-5021</url>
      <doi>10.18653/v1/W19-5021</doi>
      <bibkey>viani-etal-2019-annotating</bibkey>
      <pwccode url="https://github.com/medesto/timeline-reconstruction" additional="false">medesto/timeline-reconstruction</pwccode>
    </paper>
    <paper id="22">
      <title>Leveraging Sublanguage Features for the Semantic Categorization of Clinical Terms</title>
      <author><first>Leonie</first><last>Grön</last></author>
      <author><first>Ann</first><last>Bertels</last></author>
      <author><first>Kris</first><last>Heylen</last></author>
      <pages>211–216</pages>
      <abstract>The automatic processing of clinical documents, such as Electronic Health Records (EHRs), could benefit substantially from the enrichment of medical terminologies with terms encountered in clinical practice. To integrate such terms into existing knowledge sources, they must be linked to corresponding concepts. We present a method for the semantic categorization of clinical terms based on their surface form. We find that features based on sublanguage properties can provide valuable cues for the classification of term variants.</abstract>
      <url hash="55737987">W19-5022</url>
      <doi>10.18653/v1/W19-5022</doi>
      <bibkey>gron-etal-2019-leveraging</bibkey>
    </paper>
    <paper id="23">
      <title>Enhancing <fixed-case>PIO</fixed-case> Element Detection in Medical Text Using Contextualized Embedding</title>
      <author><first>Hichem</first><last>Mezaoui</last></author>
      <author><first>Isuru</first><last>Gunasekara</last></author>
      <author><first>Aleksandr</first><last>Gontcharov</last></author>
      <pages>217–222</pages>
      <abstract>In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.</abstract>
      <url hash="de6e2ac1">W19-5023</url>
      <doi>10.18653/v1/W19-5023</doi>
      <bibkey>mezaoui-etal-2019-enhancing</bibkey>
    </paper>
    <paper id="24">
      <title>Contributions to Clinical Named Entity Recognition in <fixed-case>P</fixed-case>ortuguese</title>
      <author><first>Fábio</first><last>Lopes</last></author>
      <author><first>César</first><last>Teixeira</last></author>
      <author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
      <pages>223–233</pages>
      <abstract>Having in mind that different languages might present different challenges, this paper presents the following contributions to the area of Information Extraction from clinical text, targeting the Portuguese language: a collection of 281 clinical texts in this language, with manually-annotated named entities; word embeddings trained in a larger collection of similar texts; results of using BiLSTM-CRF neural networks for named entity recognition on the annotated collection, including a comparison of using in-domain or out-of-domain word embeddings in this task. Although learned with much less data, performance is higher when using in-domain embeddings. When tested in 20 independent clinical texts, this model achieved better results than a model using larger out-of-domain embeddings.</abstract>
      <url hash="9a82542c">W19-5024</url>
      <doi>10.18653/v1/W19-5024</doi>
      <bibkey>lopes-etal-2019-contributions</bibkey>
      <pwccode url="https://github.com/fabioacl/PortugueseClinicalNER" additional="false">fabioacl/PortugueseClinicalNER</pwccode>
    </paper>
    <paper id="25">
      <title>Can Character Embeddings Improve Cause-of-Death Classification for Verbal Autopsy Narratives?</title>
      <author><first>Zhaodong</first><last>Yan</last></author>
      <author><first>Serena</first><last>Jeblee</last></author>
      <author><first>Graeme</first><last>Hirst</last></author>
      <pages>234–239</pages>
      <abstract>We present two models for combining word and character embeddings for cause-of-death classification of verbal autopsy reports using the text of the narratives. We find that for smaller datasets (500 to 1000 records), adding character information to the model improves classification, making character-based CNNs a promising method for automated verbal autopsy coding.</abstract>
      <url hash="dab0642a">W19-5025</url>
      <doi>10.18653/v1/W19-5025</doi>
      <bibkey>yan-etal-2019-character</bibkey>
    </paper>
    <paper id="26">
      <title>Is artificial data useful for biomedical Natural Language Processing algorithms?</title>
      <author><first>Zixu</first><last>Wang</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Sumithra</first><last>Velupillai</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>240–249</pages>
      <abstract>A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility. This problem can be addressed by generating medical data artificially. Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited. We propose a generic methodology to guide the generation of clinical text with key phrases. We use the artificial data as additional training data in two key biomedical NLP tasks: text classification and temporal relation extraction. We show that artificially generated training data used in conjunction with real training data can lead to performance boosts for data-greedy neural network algorithms. We also demonstrate the usefulness of the generated data for NLP setups where it fully replaces real training data.</abstract>
      <url hash="15c3be5e">W19-5026</url>
      <doi>10.18653/v1/W19-5026</doi>
      <bibkey>wang-etal-2019-artificial</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="27">
      <title><fixed-case>C</fixed-case>hi<fixed-case>M</fixed-case>ed: A <fixed-case>C</fixed-case>hinese Medical Corpus for Question Answering</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Weicheng</first><last>Ma</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>250–260</pages>
      <abstract>Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks.</abstract>
      <url hash="87990aa4">W19-5027</url>
      <doi>10.18653/v1/W19-5027</doi>
      <bibkey>tian-etal-2019-chimed</bibkey>
      <pwccode url="https://github.com/yuanheTian/ChiMed" additional="false">yuanheTian/ChiMed</pwccode>
    </paper>
    <paper id="28">
      <title>Clinical Concept Extraction for Document-Level Coding</title>
      <author><first>Sarah</first><last>Wiegreffe</last></author>
      <author><first>Edward</first><last>Choi</last></author>
      <author><first>Sherry</first><last>Yan</last></author>
      <author><first>Jimeng</first><last>Sun</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>261–272</pages>
      <abstract>The text of clinical notes can be a valuable source of patient information and clinical assessments. Historically, the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology. However, recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes. We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features, which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels, which are used to learn a better representation of the text. Unfortunately, the resulting concepts do not yield performance gains on the document-level clinical coding task. We explore possible explanations and future research directions.</abstract>
      <url hash="c5cf7f5f">W19-5028</url>
      <doi>10.18653/v1/W19-5028</doi>
      <bibkey>wiegreffe-etal-2019-clinical</bibkey>
    </paper>
    <paper id="29">
      <title>Clinical Case Reports for <fixed-case>NLP</fixed-case></title>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <pages>273–282</pages>
      <abstract>Textual data are useful for accessing expert information. Yet, since the texts are representative of distinct language uses, it is necessary to build specific corpora in order to be able to design suitable NLP tools. In some domains, such as medical domain, it may be complicated to access the representative textual data and their semantic annotations, while there exists a real need for providing efficient tools and methods. Our paper presents a corpus of clinical cases written in French, and their semantic annotations. Thus, we manually annotated a set of 717 files into four general categories (age, gender, outcome, and origin) for a total number of 2,835 annotations. The values of age, gender, and outcome are normalized. A subset with 70 files has been additionally manually annotated into 27 categories for a total number of 5,198 annotations.</abstract>
      <url hash="119699dd">W19-5029</url>
      <doi>10.18653/v1/W19-5029</doi>
      <bibkey>grouin-etal-2019-clinical</bibkey>
    </paper>
    <paper id="30">
      <title>Two-stage Federated Phenotyping and Patient Representation Learning</title>
      <author><first>Dianbo</first><last>Liu</last></author>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <pages>283–291</pages>
      <abstract>A large percentage of medical information is in unstructured text format in electronic medical record systems. Manual extraction of information from clinical notes is extremely time consuming. Natural language processing has been widely used in recent years for automatic information extraction from medical texts. However, algorithms trained on data from a single healthcare provider are not generalizable and error-prone due to the heterogeneity and uniqueness of medical documents. We develop a two-stage federated natural language processing method that enables utilization of clinical notes from different hospitals or clinics without moving the data, and demonstrate its performance using obesity and comorbities phenotyping as medical task. This approach not only improves the quality of a specific clinical task but also facilitates knowledge progression in the whole healthcare system, which is an essential part of learning health system. To the best of our knowledge, this is the first application of federated machine learning in clinical NLP.</abstract>
      <url hash="c6c5aa23">W19-5030</url>
      <doi>10.18653/v1/W19-5030</doi>
      <revision id="1" href="W19-5030v1" hash="f90669f7"/>
      <revision id="2" href="W19-5030v2" hash="c6c5aa23">Added an acknowledgment section crediting the grant that funded this work.</revision>
      <bibkey>liu-etal-2019-two</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="31">
      <title>Transfer Learning for Causal Sentence Detection</title>
      <author><first>Manolis</first><last>Kyriakakis</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Artur</first><last>Saudabayev</last></author>
      <author><first>Joan</first><last>Ginés i Ametllé</last></author>
      <pages>292–297</pages>
      <abstract>We consider the task of detecting sentences that express causality, as a step towards mining causal relations from texts. To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO and BERT, using a bidirectional GRU with self-attention ( BIGRUATT ) as a baseline. We experiment with both generic public relation extraction datasets and a new biomedical causal sentence detection dataset, a subset of which we make publicly available. We find that transfer learning helps only in very small datasets. With larger datasets, BIGRUATT reaches a performance plateau, then larger datasets and transfer learning do not help.</abstract>
      <url hash="42bb8494">W19-5031</url>
      <doi>10.18653/v1/W19-5031</doi>
      <bibkey>kyriakakis-etal-2019-transfer</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="32">
      <title>Embedding Biomedical Ontologies by Jointly Encoding Network Structure and Textual Node Descriptors</title>
      <author><first>Sotiris</first><last>Kotitsas</last></author>
      <author><first>Dimitris</first><last>Pappas</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Ryan</first><last>McDonald</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <pages>298–308</pages>
      <abstract>Network Embedding (NE) methods, which map network nodes to low-dimensional feature vectors, have wide applications in network analysis and bioinformatics. Many existing NE methods rely only on network structure, overlooking other information associated with the nodes, e.g., text describing the nodes. Recent attempts to combine the two sources of information only consider local network structure. We extend NODE2VEC, a well-known NE method that considers broader network structure, to also consider textual node descriptors using recurrent neural encoders. Our method is evaluated on link prediction in two networks derived from UMLS. Experimental results demonstrate the effectiveness of the proposed approach compared to previous work.</abstract>
      <url hash="16aa1d7d">W19-5032</url>
      <doi>10.18653/v1/W19-5032</doi>
      <bibkey>kotitsas-etal-2019-embedding</bibkey>
      <pwccode url="https://github.com/SotirisKot/Content-Aware-N2V" additional="false">SotirisKot/Content-Aware-N2V</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/is-a">IS-A</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/part-of">PART-OF</pwcdataset>
    </paper>
    <paper id="33">
      <title>Simplification-induced transformations: typology and some characteristics</title>
      <author><first>Anaïs</first><last>Koptient</last></author>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>309–318</pages>
      <abstract>The purpose of automatic text simplification is to transform technical or difficult to understand texts into a more friendly version. The semantics must be preserved during this transformation. Automatic text simplification can be done at different levels (lexical, syntactic, semantic, stylistic...) and relies on the corresponding knowledge and resources (lexicon, rules...). Our objective is to propose methods and material for the creation of transformation rules from a small set of parallel sentences differentiated by their technicity. We also propose a typology of transformations and quantify them. We work with French-language data related to the medical domain, although we assume that the method can be exploited on texts in any language and from any domain.</abstract>
      <url hash="a4ba6dd4">W19-5033</url>
      <doi>10.18653/v1/W19-5033</doi>
      <bibkey>koptient-etal-2019-simplification</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>S</fixed-case>cispa<fixed-case>C</fixed-case>y: Fast and Robust Models for Biomedical Natural Language Processing</title>
      <author><first>Mark</first><last>Neumann</last></author>
      <author><first>Daniel</first><last>King</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Waleed</first><last>Ammar</last></author>
      <pages>319–327</pages>
      <abstract>Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/.</abstract>
      <url hash="4615297c">W19-5034</url>
      <doi>10.18653/v1/W19-5034</doi>
      <bibkey>neumann-etal-2019-scispacy</bibkey>
      <pwccode url="https://github.com/allenai/scispacy" additional="false">allenai/scispacy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="35">
      <title>Improving Chemical Named Entity Recognition in Patents with Contextualized Word Embeddings</title>
      <author><first>Zenan</first><last>Zhai</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Saber</first><last>Akhondi</last></author>
      <author><first>Camilo</first><last>Thorne</last></author>
      <author><first>Christian</first><last>Druckenbrodt</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Michelle</first><last>Gregory</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>328–338</pages>
      <abstract>Chemical patents are an important resource for chemical information. However, few chemical Named Entity Recognition (NER) systems have been evaluated on patent documents, due in part to their structural and linguistic complexity. In this paper, we explore the NER performance of a BiLSTM-CRF model utilising pre-trained word embeddings, character-level word representations and contextualized ELMo word representations for chemical patents. We compare word embeddings pre-trained on biomedical and chemical patent corpora. The effect of tokenizers optimized for the chemical domain on NER performance in chemical patents is also explored. The results on two patent corpora show that contextualized word representations generated from ELMo substantially improve chemical NER performance w.r.t. the current state-of-the-art. We also show that domain-specific resources such as word embeddings trained on chemical patents and chemical-specific tokenizers, have a positive impact on NER performance.</abstract>
      <url hash="7ee1a8bc">W19-5035</url>
      <doi>10.18653/v1/W19-5035</doi>
      <bibkey>zhai-etal-2019-improving</bibkey>
      <pwccode url="https://github.com/zenanz/ChemPatentEmbeddings" additional="false">zenanz/ChemPatentEmbeddings</pwccode>
    </paper>
    <paper id="36">
      <title>Improving classification of Adverse Drug Reactions through Using Sentiment Analysis and Transfer Learning</title>
      <author><first>Hassan</first><last>Alhuzali</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>339–347</pages>
      <abstract>The availability of large-scale and real-time data on social media has motivated research into adverse drug reactions (ADRs). ADR classification helps to identify negative effects of drugs, which can guide health professionals and pharmaceutical companies in making medications safer and advocating patients’ safety. Based on the observation that in social media, negative sentiment is frequently expressed towards ADRs, this study presents a neural model that combines sentiment analysis with transfer learning techniques to improve ADR detection in social media postings. Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus. We then apply transfer learning to adapt the model to the task of detecting ADRs in social media postings. We show that, in combination with rich representations of words and their contexts, transfer learning is beneficial, especially given the large degree of vocabulary overlap between the current affairs posts in the SemEval17-task4A corpus and posts about ADRs. We compare our results with previous approaches, and show that our model can outperform them by up to 3% F-score.</abstract>
      <url hash="ed8face5">W19-5036</url>
      <doi>10.18653/v1/W19-5036</doi>
      <bibkey>alhuzali-ananiadou-2019-improving</bibkey>
    </paper>
    <paper id="37">
      <title>Exploring Diachronic Changes of Biomedical Knowledge using Distributed Concept Representations</title>
      <author><first>Gaurav</first><last>Vashisth</last></author>
      <author><first>Jan-Niklas</first><last>Voigt-Antons</last></author>
      <author><first>Michael</first><last>Mikhailov</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <pages>348–358</pages>
      <abstract>In research best practices can change over time as new discoveries are made and novel methods are implemented. Scientific publications reporting about the latest facts and current state-of-the-art can be possibly outdated after some years or even proved to be false. A publication usually sheds light only on the knowledge of the period it has been published. Thus, the aspect of time can play an essential role in the reliability of the presented information. In Natural Language Processing many methods focus on information extraction from text, such as detecting entities and their relationship to each other. Those methods mostly focus on the facts presented in the text itself and not on the aspects of knowledge which changes over time. This work instead examines the evolution in biomedical knowledge over time using scientific literature in terms of diachronic change. Mainly the usage of temporal and distributional concept representations are explored and evaluated by a proof-of-concept.</abstract>
      <url hash="4f2df763">W19-5037</url>
      <doi>10.18653/v1/W19-5037</doi>
      <bibkey>vashisth-etal-2019-exploring</bibkey>
    </paper>
    <paper id="38">
      <title>Extracting relations between outcomes and significance levels in Randomized Controlled Trials (<fixed-case>RCT</fixed-case>s) publications</title>
      <author><first>Anna</first><last>Koroleva</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>359–369</pages>
      <abstract>Randomized controlled trials assess the effects of an experimental intervention by comparing it to a control intervention with regard to some variables - trial outcomes. Statistical hypothesis testing is used to test if the experimental intervention is superior to the control. Statistical significance is typically reported for the measured outcomes and is an important characteristic of the results. We propose a machine learning approach to automatically extract reported outcomes, significance levels and the relation between them. We annotated a corpus of 663 sentences with 2,552 outcome - significance level relations (1,372 positive and 1,180 negative relations). We compared several classifiers, using a manually crafted feature set, and a number of deep learning models. The best performance (F-measure of 94%) was shown by the BioBERT fine-tuned model.</abstract>
      <url hash="eaca7edd">W19-5038</url>
      <doi>10.18653/v1/W19-5038</doi>
      <bibkey>koroleva-paroubek-2019-extracting</bibkey>
    </paper>
    <paper id="39">
      <title>Overview of the <fixed-case>MEDIQA</fixed-case> 2019 Shared Task on Textual Inference, Question Entailment and Question Answering</title>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Chaitanya</first><last>Shivade</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last></author>
      <pages>370–379</pages>
      <abstract>This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop. The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain, and their application to improve domain specific information retrieval and question answering systems. MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98% in the NLI task, 74.9% in the RQE task, and 78.3% in the QA task. In this paper, we describe the tasks, the datasets, and the participants’ approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.</abstract>
      <url hash="33b6bd46">W19-5039</url>
      <doi>10.18653/v1/W19-5039</doi>
      <bibkey>ben-abacha-etal-2019-overview</bibkey>
      <pwccode url="https://github.com/abachaa/MEDIQA2019" additional="false">abachaa/MEDIQA2019</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medquad">MedQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="40">
      <title><fixed-case>PANLP</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation</title>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Xiaofeng</first><last>Zhou</last></author>
      <author><first>Keqiang</first><last>Wang</last></author>
      <author><first>Xun</first><last>Luo</last></author>
      <author><first>Xiepeng</first><last>Li</last></author>
      <author><first>Yuan</first><last>Ni</last></author>
      <author><first>Guotong</first><last>Xie</last></author>
      <pages>380–388</pages>
      <abstract>This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP. We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b). We find that pre-trained language models can significantly outperform traditional deep learning models. Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large. A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models. Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances. Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.</abstract>
      <url hash="104f43be">W19-5040</url>
      <doi>10.18653/v1/W19-5040</doi>
      <bibkey>zhu-etal-2019-panlp</bibkey>
    </paper>
    <paper id="41">
      <title>Pentagon at <fixed-case>MEDIQA</fixed-case> 2019: Multi-task Learning for Filtering and Re-ranking Answers using Language Inference and Question Entailment</title>
      <author><first>Hemant</first><last>Pugaliya</last></author>
      <author><first>Karan</first><last>Saxena</last></author>
      <author><first>Shefali</first><last>Garg</last></author>
      <author><first>Sheetal</first><last>Shalini</last></author>
      <author><first>Prashant</first><last>Gupta</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <pages>389–398</pages>
      <abstract>Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have quickly become the state of the art, bypassing previous deep and shallow learning methods by a large margin. More recently, pre-trained models from large related datasets have been able to perform well on many downstream tasks by just fine-tuning on domain-specific datasets (similar to transfer learning). However, using powerful models on non-trivial tasks, such as ranking and large document classification, still remains a challenge due to input size limitations of parallel architecture and extremely small datasets (insufficient for fine-tuning). In this work, we introduce an end-to-end system, trained in a multi-task setting, to filter and re-rank answers in the medical domain. We use task-specific pre-trained models as deep feature extractors. Our model achieves the highest Spearman’s Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on the ACL-BioNLP workshop MediQA Question Answering shared-task.</abstract>
      <url hash="d361e291">W19-5041</url>
      <doi>10.18653/v1/W19-5041</doi>
      <bibkey>pugaliya-etal-2019-pentagon</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>D</fixed-case>ouble<fixed-case>T</fixed-case>ransfer at <fixed-case>MEDIQA</fixed-case> 2019: Multi-Source Transfer Learning for Natural Language Understanding in the Medical Domain</title>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Hoifung</first><last>Poon</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>399–405</pages>
      <abstract>This paper describes our competing system to enter the MEDIQA-2019 competition. We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN and SciBERT to natural language understanding tasks in the medical domain. For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task.</abstract>
      <url hash="9bb3857d">W19-5042</url>
      <doi>10.18653/v1/W19-5042</doi>
      <bibkey>xu-etal-2019-doubletransfer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medquad">MedQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="43">
      <title>Surf at <fixed-case>MEDIQA</fixed-case> 2019: Improving Performance of Natural Language Inference in the Clinical Domain by Adopting Pre-trained Language Model</title>
      <author><first>Jiin</first><last>Nam</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>406–414</pages>
      <abstract>While deep learning techniques have shown promising results in many natural language processing (NLP) tasks, it has not been widely applied to the clinical domain. The lack of large datasets and the pervasive use of domain-specific language (i.e. abbreviations and acronyms) in the clinical domain causes slower progress in NLP tasks than that of the general NLP tasks. To fill this gap, we employ word/subword-level based models that adopt large-scale data-driven methods such as pre-trained language models and transfer learning in analyzing text for the clinical domain. Empirical results demonstrate the superiority of the proposed methods by achieving 90.6% accuracy in medical domain natural language inference task. Furthermore, we inspect the independent strengths of the proposed approaches in quantitative and qualitative manners. This analysis will help researchers to select necessary components in building models for the medical domain.</abstract>
      <url hash="42d769c5">W19-5043</url>
      <doi>10.18653/v1/W19-5043</doi>
      <bibkey>nam-etal-2019-surf</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="44">
      <title><fixed-case>WTMED</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: A Hybrid Approach to Biomedical Natural Language Inference</title>
      <author><first>Zhaofeng</first><last>Wu</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Sicong</first><last>Huang</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Fei</first><last>Xia</last></author>
      <pages>415–426</pages>
      <abstract>Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1.</abstract>
      <url hash="9faf35ba">W19-5044</url>
      <doi>10.18653/v1/W19-5044</doi>
      <bibkey>wu-etal-2019-wtmed</bibkey>
      <pwccode url="https://github.com/ZhaofengWu/MEDIQA_WTMED" additional="false">ZhaofengWu/MEDIQA_WTMED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="45">
      <title><fixed-case>KU</fixed-case>_ai at <fixed-case>MEDIQA</fixed-case> 2019: Domain-specific Pre-training and Transfer Learning for Medical <fixed-case>NLI</fixed-case></title>
      <author><first>Cemil</first><last>Cengiz</last></author>
      <author><first>Ulaş</first><last>Sert</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>427–436</pages>
      <abstract>In this paper, we describe our system and results submitted for the Natural Language Inference (NLI) track of the MEDIQA 2019 Shared Task. As KU_ai team, we used BERT as our baseline model and pre-processed the MedNLI dataset to mitigate the negative impact of de-identification artifacts. Moreover, we investigated different pre-training and transfer learning approaches to improve the performance. We show that pre-training the language model on rich biomedical corpora has a significant effect in teaching the model domain-specific language. In addition, training the model on large NLI datasets such as MultiNLI and SNLI helps in learning task-specific reasoning. Finally, we ensembled our highest-performing models, and achieved 84.7% accuracy on the unseen test dataset and ranked 10th out of 17 teams in the official results.</abstract>
      <url hash="7766edde">W19-5045</url>
      <doi>10.18653/v1/W19-5045</doi>
      <bibkey>cengiz-etal-2019-ku</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="46">
      <title><fixed-case>DUT</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: An Adversarial Multi-Task Network to Jointly Model Recognizing Question Entailment and Question Answering</title>
      <author><first>Huiwei</first><last>Zhou</last></author>
      <author><first>Xuefei</first><last>Li</last></author>
      <author><first>Weihong</first><last>Yao</last></author>
      <author><first>Chengkun</first><last>Lang</last></author>
      <author><first>Shixian</first><last>Ning</last></author>
      <pages>437–445</pages>
      <abstract>In this paper, we propose a novel model called Adversarial Multi-Task Network (AMTN) for jointly modeling Recognizing Question Entailment (RQE) and medical Question Answering (QA) tasks. AMTN utilizes a pre-trained BioBERT model and an Interactive Transformer to learn the shared semantic representations across different task through parameter sharing mechanism. Meanwhile, an adversarial training strategy is introduced to separate the private features of each task from the shared representations. Experiments on BioNLP 2019 RQE and QA Shared Task datasets show that our model benefits from the shared representations of both tasks provided by multi-task learning and adversarial training, and obtains significant improvements upon the single-task models.</abstract>
      <url hash="f824d38d">W19-5046</url>
      <doi>10.18653/v1/W19-5046</doi>
      <bibkey>zhou-etal-2019-dut</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>DUT</fixed-case>-<fixed-case>BIM</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Utilizing Transformer Network and Medical Domain-Specific Contextualized Representations for Question Answering</title>
      <author><first>Huiwei</first><last>Zhou</last></author>
      <author><first>Bizun</first><last>Lei</last></author>
      <author><first>Zhe</first><last>Liu</last></author>
      <author><first>Zhuang</first><last>Liu</last></author>
      <pages>446–452</pages>
      <abstract>In medical domain, given a medical question, it is difficult to manually select the most relevant information from a large number of search results. BioNLP 2019 proposes Question Answering (QA) task, which encourages the use of text mining technology to automatically judge whether a search result is an answer to the medical question. The main challenge of QA task is how to mine the semantic relation between question and answer. We propose BioBERT Transformer model to tackle this challenge, which applies Transformers to extract semantic relation between different words in questions and answers. Furthermore, BioBERT is utilized to encode medical domain-specific contextualized word representations. Our method has reached the accuracy of 76.24% and spearman of 17.12% on the BioNLP 2019 QA task.</abstract>
      <url hash="d292e351">W19-5047</url>
      <doi>10.18653/v1/W19-5047</doi>
      <bibkey>zhou-etal-2019-dut-bim</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>D</fixed-case>r.<fixed-case>Q</fixed-case>uad at <fixed-case>MEDIQA</fixed-case> 2019: Towards Textual Inference and Question Entailment using contextualized representations</title>
      <author><first>Vinayshekhar</first><last>Bannihatti Kumar</last></author>
      <author><first>Ashwin</first><last>Srinivasan</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>James</first><last>Route</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <pages>453–461</pages>
      <abstract>This paper presents the submissions by TeamDr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed specialized domains such as medicine.</abstract>
      <url hash="62335eea">W19-5048</url>
      <doi>10.18653/v1/W19-5048</doi>
      <bibkey>bannihatti-kumar-etal-2019-dr</bibkey>
    </paper>
    <paper id="49">
      <title>Sieg at <fixed-case>MEDIQA</fixed-case> 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment</title>
      <author><first>Sai Abishek</first><last>Bhaskar</last></author>
      <author><first>Rashi</first><last>Rungta</last></author>
      <author><first>James</first><last>Route</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <pages>462–470</pages>
      <abstract>This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomedical domain. Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH. We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our model to learn better biomedical feature representations. Our final models for the NLI and RQE tasks achieve the 4th and 2nd rank on the shared-task leaderboard respectively.</abstract>
      <url hash="f8986a34">W19-5049</url>
      <doi>10.18653/v1/W19-5049</doi>
      <bibkey>bhaskar-etal-2019-sieg</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="50">
      <title><fixed-case>IIT</fixed-case>-<fixed-case>KGP</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Recognizing Question Entailment using Sci-<fixed-case>BERT</fixed-case> stacked with a Gradient Boosting Classifier</title>
      <author><first>Prakhar</first><last>Sharma</last></author>
      <author><first>Sumegh</first><last>Roychowdhury</last></author>
      <pages>471–477</pages>
      <abstract>Official System Description paper of Team IIT-KGP ranked 1st in the Development phase and 3rd in Testing Phase in MEDIQA 2019 - Recognizing Question Entailment (RQE) Shared Task of BioNLP workshop - ACL 2019. The number of people turning to the Internet to search for a diverse range of health-related subjects continues to grow and with this multitude of information available, duplicate questions are becoming more frequent and finding the most appropriate answers becomes problematic. This issue is important for question answering platforms as it complicates the retrieval of all information relevant to the same topic, particularly when questions similar in essence are expressed differently, and answering a given medical question by retrieving similar questions that are already answered by human experts seems to be a promising solution. In this paper, we present our novel approach to detect question entailment by determining the type of question asked rather than focusing on the type of the ailment given. This unique methodology makes the approach robust towards examples which have different ailment names but are synonyms of each other. Also, it enables us to check entailment at a much more fine-grained level. QSpider is a staged system consisting of state-of-the-art model Sci-BERT used as a multi-class classifier aimed at capturing both question types and semantic relations stacked with a Gradient Boosting Classifier which checks for entailment. QSpider achieves an accuracy score of 68.4% on the Test set which outperforms the baseline model (54.1%) by an accuracy score of 14.3%.</abstract>
      <url hash="ebab6036">W19-5050</url>
      <doi>10.18653/v1/W19-5050</doi>
      <bibkey>sharma-roychowdhury-2019-iit-kgp</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>ANU</fixed-case>-<fixed-case>CSIRO</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Question Answering Using Deep Contextual Knowledge</title>
      <author><first>Vincent</first><last>Nguyen</last></author>
      <author><first>Sarvnaz</first><last>Karimi</last></author>
      <author><first>Zhenchang</first><last>Xing</last></author>
      <pages>478–487</pages>
      <abstract>We report on our system for textual inference and question entailment in the medical domain for the ACL BioNLP 2019 Shared Task, MEDIQA. Textual inference is the task of finding the semantic relationships between pairs of text. Question entailment involves identifying pairs of questions which have similar semantic content. To improve upon medical natural language inference and question entailment approaches to further medical question answering, we propose a system that incorporates open-domain and biomedical domain approaches to improve semantic understanding and ambiguity resolution. Our models achieve 80% accuracy on medical natural language inference (6.5% absolute improvement over the original baseline), 48.9% accuracy on recognising medical question entailment, 0.248 Spearman’s rho for question answering ranking and 68.6% accuracy for question answering classification.</abstract>
      <url hash="a9f765cd">W19-5051</url>
      <doi>10.18653/v1/W19-5051</doi>
      <bibkey>nguyen-etal-2019-anu</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>MSIT</fixed-case>_<fixed-case>SRIB</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain.</title>
      <author><first>Sahil</first><last>Chopra</last></author>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Anupama</first><last>Kaushik</last></author>
      <pages>488–492</pages>
      <abstract>In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge. Bio-MTDNN utilizes “transfer learning” based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results.</abstract>
      <url hash="cc1b5ef6">W19-5052</url>
      <doi>10.18653/v1/W19-5052</doi>
      <bibkey>chopra-etal-2019-msit</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>UU</fixed-case>_<fixed-case>TAILS</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Learning Textual Entailment in the Medical Domain</title>
      <author><first>Noha</first><last>Tawfik</last></author>
      <author><first>Marco</first><last>Spruit</last></author>
      <pages>493–499</pages>
      <abstract>This article describes the participation of the UU_TAILS team in the 2019 MEDIQA challenge intended to improve domain-specific models in medical and clinical NLP. The challenge consists of 3 tasks: medical language inference (NLI), recognizing textual entailment (RQE) and question answering (QA). Our team participated in tasks 1 and 2 and our best runs achieved a performance accuracy of 0.852 and 0.584 respectively for the test sets. The models proposed for task 1 relied on BERT embeddings and different ensemble techniques. For the RQE task, we trained a traditional multilayer perceptron network based on embeddings generated by the universal sentence encoder.</abstract>
      <url hash="3fa6ab8c">W19-5053</url>
      <doi>10.18653/v1/W19-5053</doi>
      <bibkey>tawfik-spruit-2019-uu</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>UW</fixed-case>-<fixed-case>BHI</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: An Analysis of Representation Methods for Medical Natural Language Inference</title>
      <author><first>William</first><last>Kearns</last></author>
      <author><first>Wilson</first><last>Lau</last></author>
      <author><first>Jason</first><last>Thomas</last></author>
      <pages>500–509</pages>
      <abstract>Recent advances in distributed language modeling have led to large performance increases on a variety of natural language processing (NLP) tasks. However, it is not well understood how these methods may be augmented by knowledge-based approaches. This paper compares the performance and internal representation of an Enhanced Sequential Inference Model (ESIM) between three experimental conditions based on the representation method: Bidirectional Encoder Representations from Transformers (BERT), Embeddings of Semantic Predications (ESP), or Cui2Vec. The methods were evaluated on the Medical Natural Language Inference (MedNLI) subtask of the MEDIQA 2019 shared task. This task relied heavily on semantic understanding and thus served as a suitable evaluation set for the comparison of these representation methods.</abstract>
      <url hash="f187a2de">W19-5054</url>
      <doi>10.18653/v1/W19-5054</doi>
      <bibkey>kearns-etal-2019-uw</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="55">
      <title>Saama Research at <fixed-case>MEDIQA</fixed-case> 2019: Pre-trained <fixed-case>B</fixed-case>io<fixed-case>BERT</fixed-case> with Attention Visualisation for Medical Natural Language Inference</title>
      <author><first>Kamal raj</first><last>Kanakarajan</last></author>
      <author><first>Suriyadeepan</first><last>Ramamoorthy</last></author>
      <author><first>Vaidheeswaran</first><last>Archana</last></author>
      <author><first>Soham</first><last>Chatterjee</last></author>
      <author><first>Malaikannan</first><last>Sankarasubbu</last></author>
      <pages>510–516</pages>
      <abstract>Natural Language inference is the task of identifying relation between two sentences as entailment, contradiction or neutrality. MedNLI is a biomedical flavour of NLI for clinical domain. This paper explores the use of Bidirectional Encoder Representation from Transformer (BERT) for solving MedNLI. The proposed model, BERT pre-trained on PMC, PubMed and fine-tuned on MIMICIII v1.4, achieves state of the art results on MedNLI (83.45%) and an accuracy of 78.5% in MEDIQA challenge. The authors present an analysis of the attention patterns that emerged as a result of training BERT on MedNLI using a visualization tool, bertviz.</abstract>
      <url hash="3fa63fa5">W19-5055</url>
      <doi>10.18653/v1/W19-5055</doi>
      <bibkey>kanakarajan-etal-2019-saama</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mednli">MedNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="56">
      <title><fixed-case>IITP</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Systems Report for Natural Language Inference, Question Entailment and Question Answering</title>
      <author><first>Dibyanayan</first><last>Bandyopadhyay</last></author>
      <author><first>Baban</first><last>Gain</last></author>
      <author><first>Tanik</first><last>Saikh</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>517–522</pages>
      <abstract>This paper presents the experiments accomplished as a part of our participation in the MEDIQA challenge, an (Abacha et al., 2019) shared task. We participated in all the three tasks defined in this particular shared task. The tasks are viz. i. Natural Language Inference (NLI) ii. Recognizing Question Entailment(RQE) and their application in medical Question Answering (QA). We submitted runs using multiple deep learning based systems (runs) for each of these three tasks. We submitted five system results in each of the NLI and RQE tasks, and four system results for the QA task. The systems yield encouraging results in all the three tasks. The highest performance obtained in NLI, RQE and QA tasks are 81.8%, 53.2%, and 71.7%, respectively.</abstract>
      <url hash="dac7b5c8">W19-5056</url>
      <doi>10.18653/v1/W19-5056</doi>
      <bibkey>bandyopadhyay-etal-2019-iitp</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>L</fixed-case>asige<fixed-case>B</fixed-case>io<fixed-case>TM</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition</title>
      <author><first>Andre</first><last>Lamurias</last></author>
      <author><first>Francisco M</first><last>Couto</last></author>
      <pages>523–527</pages>
      <abstract>Biomedical Question Answering (QA) aims at providing automated answers to user questions, regarding a variety of biomedical topics. For example, these questions may ask for related to diseases, drugs, symptoms, or medical procedures. Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions. The MEDIQA challenge consisted of three tasks concerning various aspects of biomedical QA. This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA. Our approach explored a common Transformer-based architecture that could be applied to each task. This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data. Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool. Our approach obtained high levels of accuracy, in particular on the NLI task, which classified pairs of text according to their relation. For the QA task, we obtained higher Spearman’s rank correlation values using the entities recognized by MER.</abstract>
      <url hash="2d65199a">W19-5057</url>
      <doi>10.18653/v1/W19-5057</doi>
      <bibkey>lamurias-couto-2019-lasigebiotm</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>NCUEE</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019: Medical Text Inference Using Ensemble <fixed-case>BERT</fixed-case>-<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>-Attention Model</title>
      <author><first>Lung-Hao</first><last>Lee</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Po-Han</first><last>Chen</last></author>
      <author><first>Po-Lei</first><last>Lee</last></author>
      <author><first>Kuo-Kai</first><last>Shyu</last></author>
      <pages>528–532</pages>
      <abstract>This study describes the model design of the NCUEE system for the MEDIQA challenge at the ACL-BioNLP 2019 workshop. We use the BERT (Bidirectional Encoder Representations from Transformers) as the word embedding method to integrate the BiLSTM (Bidirectional Long Short-Term Memory) network with an attention mechanism for medical text inferences. A total of 42 teams participated in natural language inference task at MEDIQA 2019. Our best accuracy score of 0.84 ranked the top-third among all submissions in the leaderboard.</abstract>
      <url hash="46d2374a">W19-5058</url>
      <doi>10.18653/v1/W19-5058</doi>
      <bibkey>lee-etal-2019-ncuee</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>ARS</fixed-case>_<fixed-case>NITK</fixed-case> at <fixed-case>MEDIQA</fixed-case> 2019:Analysing Various Methods for Natural Language Inference, Recognising Question Entailment and Medical Question Answering System</title>
      <author><first>Anumeha</first><last>Agrawal</last></author>
      <author><first>Rosa</first><last>Anil George</last></author>
      <author><first>Selvan Suntiha</first><last>Ravi</last></author>
      <author><first>Sowmya</first><last>Kamath S</last></author>
      <author><first>Anand</first><last>Kumar</last></author>
      <pages>533–540</pages>
      <abstract>In this paper, we present three approaches for Natural Language Inference, Question Entailment Recognition and Question-Answering to improve domain-specific Information Retrieval. For addressing the NLI task, the UMLS Metathesaurus was used to find the synonyms of medical terms in given sentences, on which the InferSent model was trained to predict if the given sentence is an entailment, contradictory or neutral. We also introduce a new Extreme gradient boosting model built on PubMed embeddings to perform RQE. Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers for a given question is also discussed.</abstract>
      <url hash="976170b3">W19-5059</url>
      <doi>10.18653/v1/W19-5059</doi>
      <bibkey>agrawal-etal-2019-ars</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
  </volume>
  <volume id="51">
    <meta>
      <booktitle>Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)</booktitle>
      <url hash="b9c630a3">W19-51</url>
      <editor><first>Agata</first><last>Savary</last></editor>
      <editor><first>Carla Parra</first><last>Escartín</last></editor>
      <editor><first>Francis</first><last>Bond</last></editor>
      <editor><first>Jelena</first><last>Mitrović</last></editor>
      <editor><first>Verginica Barbu</first><last>Mititelu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>mwe</venue>
    </meta>
    <frontmatter>
      <url hash="5846d185">W19-5100</url>
      <bibkey>ws-2019-joint-multiword</bibkey>
    </frontmatter>
    <paper id="1">
      <title>When the whole is greater than the sum of its parts: Multiword expressions and idiomaticity</title>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <pages>1</pages>
      <abstract>Multiword expressions (MWEs) feature prominently in the mental lexicon of native speakers (Jackendoff, 1997) in all languages and domains, from informal to technical contexts (Biber et al., 1999) with about four MWEs being produced per minute of discourse (Glucksberg, 1989). MWEs come in all shapes and forms, including idioms like rock the boat (as cause problems or disturb a situation) and compound nouns like monkey business (as dishonest behaviour). Their accurate detection and understanding may often require more than knowledge about individual words and how they can be combined (Fillmore, 1979), as they may display various degrees of idiosyncrasy, including lexical, syntactic, semantic and statistical (Sag et al., 2002; Baldwin and Kim, 2010), which provide new challenges and opportunities for language processing (Constant et al., 2017). For instance, while for some combinations the meaning can be inferred from their parts like olive oil (oil made of olives) this is not always the case, as in dark horse (meaning an unknown candidate who unexpectedly succeeds), and when processing a sentence some of the challenges are to identify which words form an expression (Ramisch, 2015), and whether the expression is idiomatic (Cordeiro et al., 2019). In this talk I will give an overview of advances on the identification and treatment of multiword expressions, in particular concentrating on techniques for identifying their degree of idiomaticity.</abstract>
      <url hash="78d81759">W19-5101</url>
      <doi>10.18653/v1/W19-5101</doi>
      <bibkey>villavicencio-2019-whole</bibkey>
    </paper>
    <paper id="2">
      <title>Hear about Verbal Multiword Expressions in the <fixed-case>B</fixed-case>ulgarian and the <fixed-case>R</fixed-case>omanian Wordnets Straight from the Horse’s Mouth</title>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Ivelina</first><last>Stoyanova</last></author>
      <author><first>Svetlozara</first><last>Leseva</last></author>
      <author><first>Maria</first><last>Mitrofan</last></author>
      <author><first>Tsvetana</first><last>Dimitrova</last></author>
      <author><first>Maria</first><last>Todorova</last></author>
      <pages>2–12</pages>
      <abstract>In this paper we focus on verbal multiword expressions (VMWEs) in Bulgarian and Romanian as reflected in the wordnets of the two languages. The annotation of VMWEs relies on the classification defined within the PARSEME Cost Action. After outlining the properties of various types of VMWEs, a cross-language comparison is drawn, aimed to highlight the similarities and the differences between Bulgarian and Romanian with respect to the lexicalization and distribution of VMWEs. The contribution of this work is in outlining essential features of the description and classification of VMWEs and the cross-language comparison at the lexical level, which is essential for the understanding of the need for uniform annotation guidelines and a viable procedure for validation of the annotation.</abstract>
      <url hash="a6e82d03">W19-5102</url>
      <doi>10.18653/v1/W19-5102</doi>
      <bibkey>barbu-mititelu-etal-2019-hear</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>R</fixed-case>omanian Corpus Annotated with Verbal Multiword Expressions</title>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Mihaela</first><last>Cristescu</last></author>
      <author><first>Mihaela</first><last>Onofrei</last></author>
      <pages>13–21</pages>
      <abstract>This paper reports on the Romanian journalistic corpus annotated with verbal multiword expressions following the PARSEME guidelines. The corpus is sentence split, tokenized, part-of-speech tagged, lemmatized, syntactically annotated and verbal multiword expressions are identified and classified. It offers insights into the frequency of such Romanian word combinations and allows for their characterization. We offer data about the types of verbal multiword expressions in the corpus and some of their characteristics, such as internal structure, diversity in the corpus, average length, productivity of the verbs. This is a language resource that is important per se, as well as for the task of automatic multiword expressions identification, which can be further used in other systems. It was already used as training and test material in the shared tasks for the automatic identification of verbal multiword expressions organized by PARSEME.</abstract>
      <url hash="48f433bd">W19-5103</url>
      <doi>10.18653/v1/W19-5103</doi>
      <bibkey>barbu-mititelu-etal-2019-romanian</bibkey>
    </paper>
    <paper id="4">
      <title>Using <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-Lemon for Representing and Interlinking <fixed-case>G</fixed-case>erman Multiword Expressions in <fixed-case>O</fixed-case>de<fixed-case>N</fixed-case>et and <fixed-case>MMORPH</fixed-case></title>
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Melanie</first><last>Siegel</last></author>
      <author><first>Stefania</first><last>Racioppa</last></author>
      <pages>22–29</pages>
      <abstract>We describe work consisting in porting two large German lexical resources into the OntoLex-Lemon model in order to establish complementary interlinkings between them. One resource is OdeNet (Open German WordNet) and the other is a further development of the German version of the MMORPH morphological analyzer. We show how the Multiword Expressions (MWEs) contained in OdeNet can be morphologically specified by the use of the lexical representation and linking features of OntoLex-Lemon, which also support the formulation of restrictions in the usage of such expressions.</abstract>
      <url hash="524f585a">W19-5104</url>
      <doi>10.18653/v1/W19-5104</doi>
      <bibkey>declerck-etal-2019-using</bibkey>
    </paper>
    <paper id="5">
      <title>Learning to Predict Novel Noun-Noun Compounds</title>
      <author><first>Prajit</first><last>Dhar</last></author>
      <author><first>Lonneke</first><last>van der Plas</last></author>
      <pages>30–39</pages>
      <abstract>We introduce temporally and contextually-aware models for the novel task of predicting unseen but plausible concepts, as conveyed by noun-noun compounds in a time-stamped corpus. We train compositional models on observed compounds, more specifically the composed distributed representations of their constituents across a time-stamped corpus, while giving it corrupted instances (where head or modifier are replaced by a random constituent) as negative evidence. The model captures generalisations over this data and learns what combinations give rise to plausible compounds and which ones do not. After training, we query the model for the plausibility of automatically generated novel combinations and verify whether the classifications are accurate. For our best model, we find that in around 85% of the cases, the novel compounds generated are attested in previously unseen data. An additional estimated 5% are plausible despite not being attested in the recent corpus, based on judgments from independent human raters.</abstract>
      <url hash="b88ba0d0">W19-5105</url>
      <doi>10.18653/v1/W19-5105</doi>
      <bibkey>dhar-van-der-plas-2019-learning</bibkey>
      <pwccode url="https://github.com/prajitdhar/Compounding" additional="false">prajitdhar/Compounding</pwccode>
    </paper>
    <paper id="6">
      <title>Unsupervised Compositional Translation of Multiword Expressions</title>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <author><first>Marcos</first><last>Garcia</last></author>
      <pages>40–48</pages>
      <abstract>This article describes a dependency-based strategy that uses compositional distributional semantics and cross-lingual word embeddings to translate multiword expressions (MWEs). Our unsupervised approach performs translation as a process of word contextualization by taking into account lexico-syntactic contexts and selectional preferences. This strategy is suited to translate phraseological combinations and phrases whose constituent words are lexically restricted by each other. Several experiments in adjective-noun and verb-object compounds show that mutual contextualization (co-compositionality) clearly outperforms other compositional methods. The paper also contributes with a new freely available dataset of English-Spanish MWEs used to validate the proposed compositional strategy.</abstract>
      <url hash="670e6ee4">W19-5106</url>
      <doi>10.18653/v1/W19-5106</doi>
      <bibkey>gamallo-garcia-2019-unsupervised</bibkey>
    </paper>
    <paper id="7">
      <title>A comparison of statistical association measures for identifying dependency-based collocations in various languages.</title>
      <author><first>Marcos</first><last>Garcia</last></author>
      <author><first>Marcos</first><last>García Salido</last></author>
      <author><first>Margarita</first><last>Alonso-Ramos</last></author>
      <pages>49–59</pages>
      <abstract>This paper presents an exploration of different statistical association measures to automatically identify collocations from corpora in English, Portuguese, and Spanish. To evaluate the impact of the association metrics we manually annotated corpora with three different syntactic patterns of collocations (adjective-noun, verb-object and nominal compounds). We took advantage of the PARSEME 1.1 Shared Task corpora by selecting a subset of 155k tokens in the three referred languages, in which we annotated 1,526 collocations with the corresponding Lexical Functions according to the Meaning-Text Theory. Using the resulting gold-standard, we have carried out a comparison between frequency data and several well-known association measures, both symmetric and asymmetric. The results show that the combination of dependency triples with raw frequency information is as powerful as the best association measures in most syntactic patterns and languages. Furthermore, and despite the asymmetric behaviour of collocations, directional approaches perform worse than the symmetric ones in the extraction of these phraseological combinations.</abstract>
      <url hash="cc925eac">W19-5107</url>
      <doi>10.18653/v1/W19-5107</doi>
      <bibkey>garcia-etal-2019-comparison</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>L</fixed-case>2 Processing Advantages of Multiword Sequences: Evidence from Eye-Tracking</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Arndt</first><last>Heilmann</last></author>
      <author><first>Stella</first><last>Neumann</last></author>
      <pages>60–69</pages>
      <abstract>A substantial body of research has demonstrated that native speakers are sensitive to the frequencies of multiword sequences (MWS). Here, we ask whether and to what extent intermediate-advanced L2 speakers of English can also develop the sensitivity to the statistics of MWS. To this end, we aimed to replicate the MWS frequency effects found for adult native language speakers based on evidence from self-paced reading and sentence recall tasks in an ecologically more valid eye-tracking study. L2 speakers’ sensitivity to MWS frequency was evaluated using generalized linear mixed-effects regression with separate models fitted for each of the four dependent measures. Mixed-effects modeling revealed significantly faster processing of sentences containing MWS compared to sentences containing equivalent control items across all eyetracking measures. Taken together, these findings suggest that, in line with emergentist approaches, MWS are important building blocks of language and that similar mechanisms underlie both native and non-native language processing.</abstract>
      <url hash="1a0e80a8">W19-5108</url>
      <doi>10.18653/v1/W19-5108</doi>
      <bibkey>kerz-etal-2019-l2</bibkey>
    </paper>
    <paper id="9">
      <title>Modeling <fixed-case>MWE</fixed-case>s in <fixed-case>BTB</fixed-case>-<fixed-case>WN</fixed-case></title>
      <author><first>Laska</first><last>Laskova</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Ivajlo</first><last>Radev</last></author>
      <author><first>Zara</first><last>Kancheva</last></author>
      <pages>70–78</pages>
      <abstract>The paper presents the characteristics of the predominant types of MultiWord expressions (MWEs) in the BulTreeBank WordNet – BTB-WN. Their distribution in BTB-WN is discussed with respect to the overall hierarchical organization of the lexical resource. Also, a catena-based modeling is proposed for handling the issues of lexical semantics of MWEs.</abstract>
      <url hash="ad30d42e">W19-5109</url>
      <doi>10.18653/v1/W19-5109</doi>
      <bibkey>laskova-etal-2019-modeling</bibkey>
    </paper>
    <paper id="10">
      <title>Without lexicons, multiword expression identification will never fly: A position statement</title>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Silvio</first><last>Cordeiro</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <pages>79–91</pages>
      <abstract>Because most multiword expressions (MWEs), especially verbal ones, are semantically non-compositional, their automatic identification in running text is a prerequisite for semantically-oriented downstream applications. However, recent developments, driven notably by the PARSEME shared task on automatic identification of verbal MWEs, show that this task is harder than related tasks, despite recent contributions both in multilingual corpus annotation and in computational models. In this paper, we analyse possible reasons for this state of affairs. They lie in the nature of the MWE phenomenon, as well as in its distributional properties. We also offer a comparative analysis of the state-of-the-art systems, which exhibit particularly strong sensitivity to unseen data. On this basis, we claim that, in order to make strong headway in MWE identification, the community should bend its mind into coupling identification of MWEs with their discovery, via syntactic MWE lexicons. Such lexicons need not necessarily achieve a linguistically complete modelling of MWEs’ behavior, but they should provide minimal morphosyntactic information to cover some potential uses, so as to complement existing MWE-annotated corpora. We define requirements for such minimal NLP-oriented lexicon, and we propose a roadmap for the MWE community driven by these requirements.</abstract>
      <url hash="cb3d65cb">W19-5110</url>
      <doi>10.18653/v1/W19-5110</doi>
      <bibkey>savary-etal-2019-without</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
    </paper>
    <paper id="11">
      <title>A Systematic Comparison of <fixed-case>E</fixed-case>nglish Noun Compound Representations</title>
      <author><first>Vered</first><last>Shwartz</last></author>
      <pages>92–103</pages>
      <abstract>Building meaningful representations of noun compounds is not trivial since many of them scarcely appear in the corpus. To that end, composition functions approximate the distributional representation of a noun compound by combining its constituent distributional vectors. In the more general case, phrase embeddings have been trained by minimizing the distance between the vectors representing paraphrases. We compare various types of noun compound representations, including distributional, compositional, and paraphrase-based representations, through a series of tasks and analyses, and with an extensive number of underlying word embeddings. We find that indeed, in most cases, composition functions produce higher quality representations than distributional ones, and they improve with computational power. No single function performs best in all scenarios, suggesting that a joint training objective may produce improved representations.</abstract>
      <url hash="5966421f">W19-5111</url>
      <attachment type="poster" hash="ea8277bd">W19-5111.Poster.pdf</attachment>
      <doi>10.18653/v1/W19-5111</doi>
      <bibkey>shwartz-2019-systematic</bibkey>
      <pwccode url="https://github.com/vered1986/NC_Embeddings" additional="false">vered1986/NC_Embeddings</pwccode>
    </paper>
    <paper id="12">
      <title>Semantic Modelling of Adjective-Noun Collocations Using <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <author><first>Yana</first><last>Strakatova</last></author>
      <author><first>Erhard</first><last>Hinrichs</last></author>
      <pages>104–113</pages>
      <abstract>In this paper we argue that Frame Semantics (Fillmore, 1982) provides a good framework for semantic modelling of adjective-noun collocations. More specifically, the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a noun in terms of Frame Elements. We have substantiated these findings by considering a sample of adjective-noun collocations from German such as “enger Freund” ‘close friend’ and “starker Regen” ‘heavy rain’. The data sample is taken from different semantic fields identified in the German wordnet GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010). The study is based on the electronic dictionary DWDS (Klein and Geyken, 2010) and uses the collocation extraction tool Wortprofil (Geyken et al., 2009). The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu. Since FrameNets are available for a range of typologically different languages, it is feasible to extend the current case study to other languages.</abstract>
      <url hash="3e73957a">W19-5112</url>
      <doi>10.18653/v1/W19-5112</doi>
      <bibkey>strakatova-hinrichs-2019-semantic</bibkey>
    </paper>
    <paper id="13">
      <title>A Neural Graph-based Approach to Verbal <fixed-case>MWE</fixed-case> Identification</title>
      <author><first>Jakub</first><last>Waszczuk</last></author>
      <author><first>Rafael</first><last>Ehren</last></author>
      <author><first>Regina</first><last>Stodden</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <pages>114–124</pages>
      <abstract>We propose to tackle the problem of verbal multiword expression (VMWE) identification using a neural graph parsing-based approach. Our solution involves encoding VMWE annotations as labellings of dependency trees and, subsequently, applying a neural network to model the probabilities of different labellings. This strategy can be particularly effective when applied to discontinuous VMWEs and, thanks to dense, pre-trained word vector representations, VMWEs unseen during training. Evaluation of our approach on three PARSEME datasets (German, French, and Polish) shows that it allows to achieve performance on par with the previous state-of-the-art (Al Saied et al., 2018).</abstract>
      <url hash="18afff95">W19-5113</url>
      <doi>10.18653/v1/W19-5113</doi>
      <bibkey>waszczuk-etal-2019-neural</bibkey>
      <pwccode url="https://github.com/kawu/vine" additional="false">kawu/vine</pwccode>
    </paper>
    <paper id="14">
      <title>Confirming the Non-compositionality of Idioms for Sentiment Analysis</title>
      <author><first>Alyssa</first><last>Hwang</last></author>
      <author><first>Christopher</first><last>Hidey</last></author>
      <pages>125–129</pages>
      <abstract>An idiom is defined as a non-compositional multiword expression, one whose meaning cannot be deduced from the definitions of the component words. This definition does not explicitly define the compositionality of an idiom’s sentiment; this paper aims to determine whether the sentiment of the component words of an idiom is related to the sentiment of that idiom. We use the Dictionary of Affect in Language augmented by WordNet to give each idiom in the Sentiment Lexicon of IDiomatic Expressions (SLIDE) a component-wise sentiment score and compare it to the phrase-level sentiment label crowdsourced by the creators of SLIDE. We find that there is no discernible relation between these two measures of idiom sentiment. This supports the hypothesis that idioms are not compositional for sentiment along with semantics and motivates further work in handling idioms for sentiment analysis.</abstract>
      <url hash="58f95300">W19-5114</url>
      <doi>10.18653/v1/W19-5114</doi>
      <bibkey>hwang-hidey-2019-confirming</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>IDION</fixed-case>: A database for <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek multiword expressions</title>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <author><first>Panagiotis</first><last>Minos</last></author>
      <author><first>George</first><last>Zakis</last></author>
      <author><first>Vassiliki</first><last>Moutzouri</last></author>
      <author><first>Maria</first><last>Chantou</last></author>
      <pages>130–134</pages>
      <abstract>We report on the ongoing development of IDION, a web resource of richly documented multiword expressions (MWEs) of Modern Greek addressed to the human user and to NLP. IDION contains about 2000 verb MWEs (VMWEs) of which about 850 are fully documented as regards their syntactic flexibility, their semantics and the semantic relations with other VMWEs. Sets of synonymous MWEs are defined in a bottom-up manner revealing the conceptual organization of the MG VMWE domain.</abstract>
      <url hash="17bde444">W19-5115</url>
      <doi>10.18653/v1/W19-5115</doi>
      <bibkey>markantonatou-etal-2019-idion</bibkey>
    </paper>
    <paper id="16">
      <title>Identification of Adjective-Noun Neologisms using Pretrained Language Models</title>
      <author><first>John Philip</first><last>McCrae</last></author>
      <pages>135–141</pages>
      <abstract>Neologism detection is a key task in the constructing of lexical resources and has wider implications for NLP, however the identification of multiword neologisms has received little attention. In this paper, we show that we can effectively identify the distinction between compositional and non-compositional adjective-noun pairs by using pretrained language models and comparing this with individual word embeddings. Our results show that the use of these models significantly improves over baseline linguistic features, however the combination with linguistic features still further improves the results, suggesting the strength of a hybrid approach.</abstract>
      <url hash="ed33eb06">W19-5116</url>
      <doi>10.18653/v1/W19-5116</doi>
      <bibkey>mccrae-2019-identification</bibkey>
      <pwccode url="https://github.com/jmccrae/adj-noun-neologism-identification" additional="false">jmccrae/adj-noun-neologism-identification</pwccode>
    </paper>
    <paper id="17">
      <title>Neural Lemmatization of Multiword Expressions</title>
      <author><first>Marine</first><last>Schmitt</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <pages>142–148</pages>
      <abstract>This article focuses on the lemmatization of multiword expressions (MWEs). We propose a deep encoder-decoder architecture generating for every MWE word its corresponding part in the lemma, based on the internal context of the MWE. The encoder relies on recurrent networks based on (1) the character sequence of the individual words to capture their morphological properties, and (2) the word sequence of the MWE to capture lexical and syntactic properties. The decoder in charge of generating the corresponding part of the lemma for each word of the MWE is based on a classical character-level attention-based recurrent model. Our model is evaluated for Italian, French, Polish and Portuguese and shows good performances except for Polish.</abstract>
      <url hash="32beefa2">W19-5117</url>
      <doi>10.18653/v1/W19-5117</doi>
      <bibkey>schmitt-constant-2019-neural</bibkey>
    </paper>
    <paper id="18">
      <title>Evaluating Automatic Term Extraction Methods on Individual Documents</title>
      <author><first>Antonio</first><last>Šajatović</last></author>
      <author><first>Maja</first><last>Buljan</last></author>
      <author><first>Jan</first><last>Šnajder</last></author>
      <author><first>Bojana</first><last>Dalbelo Bašić</last></author>
      <pages>149–154</pages>
      <abstract>Automatic Term Extraction (ATE) extracts terminology from domain-specific corpora. ATE is used in many NLP tasks, including Computer Assisted Translation, where it is typically applied to individual documents rather than the entire corpus. While corpus-level ATE has been extensively evaluated, it is not obvious how the results transfer to document-level ATE. To fill this gap, we evaluate 16 state-of-the-art ATE methods on full-length documents from three different domains, on both corpus and document levels. Unlike existing studies, our evaluation is more realistic as we take into account all gold terms. We show that no single method is best in corpus-level ATE, but C-Value and KeyConceptRelatendess surpass others in document-level ATE.</abstract>
      <url hash="190ede4b">W19-5118</url>
      <doi>10.18653/v1/W19-5118</doi>
      <bibkey>sajatovic-etal-2019-evaluating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="19">
      <title>Cross-lingual Transfer Learning and Multitask Learning for Capturing Multiword Expressions</title>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Omid</first><last>Rohanian</last></author>
      <author><first>Le An</first><last>Ha</last></author>
      <pages>155–161</pages>
      <abstract>Recent developments in deep learning have prompted a surge of interest in the application of multitask and transfer learning to NLP problems. In this study, we explore for the first time, the application of transfer learning (TRL) and multitask learning (MTL) to the identification of Multiword Expressions (MWEs). For MTL, we exploit the shared syntactic information between MWE and dependency parsing models to jointly train a single model on both tasks. We specifically predict two types of labels: MWE and dependency parse. Our neural MTL architecture utilises the supervision of dependency parsing in lower layers and predicts MWE tags in upper layers. In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language. In both scenarios, the resulting models achieved higher performance compared to standard neural approaches.</abstract>
      <url hash="32213ede">W19-5119</url>
      <doi>10.18653/v1/W19-5119</doi>
      <bibkey>taslimipoor-etal-2019-cross</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>I</fixed-case>lfhocail: A Lexicon of <fixed-case>I</fixed-case>rish <fixed-case>MWE</fixed-case>s</title>
      <author><first>Abigail</first><last>Walsh</last></author>
      <author><first>Teresa</first><last>Lynn</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>162–168</pages>
      <abstract>This paper describes the categorisation of Irish MWEs, and the construction of the first version of a lexicon of Irish MWEs for NLP purposes (Ilfhocail, meaning ‘Multiwords’), collected from a number of resources. For the purposes of quality assurance, 530 entries of this lexicon were examined and manually annotated for POS information and MWE category.</abstract>
      <url hash="7e4a5453">W19-5120</url>
      <doi>10.18653/v1/W19-5120</doi>
      <bibkey>walsh-etal-2019-ilfhocail</bibkey>
    </paper>
    <paper id="21">
      <title>The Impact of Word Representations on Sequential Neural <fixed-case>MWE</fixed-case> Identification</title>
      <author><first>Nicolas</first><last>Zampieri</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Geraldine</first><last>Damnati</last></author>
      <pages>169–175</pages>
      <abstract>Recent initiatives such as the PARSEME shared task allowed the rapid development of MWE identification systems. Many of those are based on recent NLP advances, using neural sequence models that take continuous word representations as input. We study two related questions in neural MWE identification: (a) the use of lemmas and/or surface forms as input features, and (b) the use of word-based or character-based embeddings to represent them. Our experiments on Basque, French, and Polish show that character-based representations yield systematically better results than word-based ones. In some cases, character-based representations of surface forms can be used as a proxy for lemmas, depending on the morphological complexity of the language.</abstract>
      <url hash="0682f903">W19-5121</url>
      <doi>10.18653/v1/W19-5121</doi>
      <bibkey>zampieri-etal-2019-impact</bibkey>
    </paper>
  </volume>
  <volume id="52" ingest-date="2019-08-15">
    <meta>
      <booktitle>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</booktitle>
      <url hash="b88306f2">W19-52</url>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <editor><first>Aurélie</first><last>Névéol</last></editor>
      <editor><first>Mariana</first><last>Neves</last></editor>
      <editor><first>Matt</first><last>Post</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Karin</first><last>Verspoor</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="8f35e6df">W19-5200</url>
      <bibkey>ws-2019-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Saliency-driven Word Alignment Interpretation for Neural Machine Translation</title>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Hainan</first><last>Xu</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>1–12</pages>
      <abstract>Despite their original goal to jointly learn to align and translate, Neural Machine Translation (NMT) models, especially Transformer, are often perceived as not learning interpretable word alignments. In this paper, we show that NMT models do learn interpretable word alignments, which could only be revealed with proper interpretation methods. We propose a series of such methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter update or architectural change. We show that under the force decoding setup, the alignments induced by our interpretation method are of better quality than fast-align for some systems, and when performing free decoding, they agree well with the alignments induced by automatic alignment tools.</abstract>
      <url hash="d06b9a48">W19-5201</url>
      <doi>10.18653/v1/W19-5201</doi>
      <attachment type="presentation" hash="1093d960">W19-5201.Presentation.pdf</attachment>
      <bibkey>ding-etal-2019-saliency</bibkey>
      <pwccode url="https://github.com/shuoyangd/meerkat" additional="false">shuoyangd/meerkat</pwccode>
    </paper>
    <paper id="2">
      <title>Improving Zero-shot Translation with Language-Independent Constraints</title>
      <author><first>Ngoc-Quan</first><last>Pham</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>13–23</pages>
      <abstract>An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.</abstract>
      <url hash="751531b8">W19-5202</url>
      <doi>10.18653/v1/W19-5202</doi>
      <bibkey>pham-etal-2019-improving</bibkey>
    </paper>
    <paper id="3">
      <title>Incorporating Source Syntax into Transformer-Based Neural Machine Translation</title>
      <author><first>Anna</first><last>Currey</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>24–33</pages>
      <abstract>Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it. We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from English into twenty target languages, showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.</abstract>
      <url hash="58123eae">W19-5203</url>
      <doi>10.18653/v1/W19-5203</doi>
      <bibkey>currey-heafield-2019-incorporating</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>APE</fixed-case> at Scale and Its Implications on <fixed-case>MT</fixed-case> Evaluation Biases</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Isaac</first><last>Caswell</last></author>
      <author><first>Scott</first><last>Roy</last></author>
      <pages>34–44</pages>
      <abstract>In this work, we train an Automatic Post-Editing (APE) model and use it to reveal biases in standard MT evaluation procedures. The goal of our APE model is to correct typical errors introduced by the translation process, and convert the “translationese” output into natural text. Our APE model is trained entirely on monolingual data that has been round-trip translated through English, to mimic errors that are similar to the ones introduced by NMT. We apply our model to the output of existing NMT systems, and demonstrate that, while the human-judged quality improves in all cases, BLEU scores drop with forward-translated test sets. We verify these results for the WMT18 English to German, WMT15 English to French, and WMT16 English to Romanian tasks. Furthermore, we selectively apply our APE model on the output of the top submissions of the most recent WMT evaluation campaigns. We see quality improvements on all tasks of up to 2.5 BLEU points.</abstract>
      <url hash="51b2821e">W19-5204</url>
      <doi>10.18653/v1/W19-5204</doi>
      <bibkey>freitag-etal-2019-ape</bibkey>
    </paper>
    <paper id="5">
      <title>Generalizing Back-Translation in Neural Machine Translation</title>
      <author><first>Miguel</first><last>Graça</last></author>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Julian</first><last>Schamper</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>45–52</pages>
      <abstract>Back-translation — data augmentation by translating target monolingual data — is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German &lt;-&gt; English news translation task.</abstract>
      <url hash="ae3784e5">W19-5205</url>
      <doi>10.18653/v1/W19-5205</doi>
      <bibkey>graca-etal-2019-generalizing</bibkey>
    </paper>
    <paper id="6">
      <title>Tagged Back-Translation</title>
      <author><first>Isaac</first><last>Caswell</last></author>
      <author><first>Ciprian</first><last>Chelba</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <pages>53–63</pages>
      <abstract>Recent work in Neural Machine Translation (NMT) has shown significant quality gains from noised-beam decoding during back-translation, a method to generate synthetic parallel data. We show that the main role of such synthetic noise is not to diversify the source side, as previously suggested, but simply to indicate to the model that the given source is synthetic. We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token. Our results on WMT outperform noised back-translation in English-Romanian and match performance on English-German, redefining the state-of-the-art on the former.</abstract>
      <url hash="864201c0">W19-5206</url>
      <doi>10.18653/v1/W19-5206</doi>
      <bibkey>caswell-etal-2019-tagged</bibkey>
    </paper>
    <paper id="7">
      <title>Hierarchical Document Encoder for Parallel Corpus Mining</title>
      <author><first>Mandy</first><last>Guo</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Keith</first><last>Stevens</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Heming</first><last>Ge</last></author>
      <author><first>Yun-hsuan</first><last>Sung</last></author>
      <author><first>Brian</first><last>Strope</last></author>
      <author><first>Ray</first><last>Kurzweil</last></author>
      <pages>64–72</pages>
      <abstract>We explore using multilingual document embeddings for nearest neighbor mining of parallel data. Three document-level representations are investigated: (i) document embeddings generated by simply averaging multilingual sentence embeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a hierarchical multilingual document encoder (HiDE) that builds on our sentence-level model. The results show document embeddings derived from sentence-level averaging are surprisingly effective for clean datasets, but suggest models trained hierarchically at the document-level are more effective on noisy data. Analysis experiments demonstrate our hierarchical models are very robust to variations in the underlying sentence embedding quality. Using document embeddings trained with HiDE achieves the state-of-the-art on United Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1 for en-es.</abstract>
      <url hash="959650cf">W19-5207</url>
      <doi>10.18653/v1/W19-5207</doi>
      <bibkey>guo-etal-2019-hierarchical</bibkey>
    </paper>
    <paper id="8">
      <title>The Effect of Translationese in Machine Translation Test Sets</title>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>73–81</pages>
      <abstract>The effect of translationese has been studied in the field of machine translation (MT), mostly with respect to training data. We study in depth the effect of translationese on test data, using the test sets from the last three editions of WMT’s news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.</abstract>
      <url hash="a8ac5f94">W19-5208</url>
      <attachment type="supplementary" hash="cbe8a6cc">W19-5208.Supplementary.zip</attachment>
      <doi>10.18653/v1/W19-5208</doi>
      <attachment type="presentation" hash="dc0cf8d2">W19-5208.Presentation.pdf</attachment>
      <bibkey>zhang-toral-2019-effect</bibkey>
      <pwccode url="https://github.com/jjzha/translationese" additional="false">jjzha/translationese</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="9">
      <title>Customizing Neural Machine Translation for Subtitling</title>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Patrick</first><last>Wilken</last></author>
      <author><first>Yota</first><last>Georgakopoulou</last></author>
      <pages>82–93</pages>
      <abstract>In this work, we customized a neural machine translation system for translation of subtitles in the domain of entertainment. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a recurrent neural network learned from human segmentation decisions. This model is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a documentary and a sitcom). It showed a notable productivity increase of up to 37% as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.</abstract>
      <url hash="9df93a0d">W19-5209</url>
      <doi>10.18653/v1/W19-5209</doi>
      <bibkey>matusov-etal-2019-customizing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="10">
      <title>Integration of Dubbing Constraints into Machine Translation</title>
      <author><first>Ashutosh</first><last>Saboo</last></author>
      <author><first>Timo</first><last>Baumann</last></author>
      <pages>94–101</pages>
      <abstract>Translation systems aim to perform a meaning-preserving conversion of linguistic material (typically text but also speech) from a source to a target language (and, to a lesser degree, the corresponding socio-cultural contexts). Dubbing, i.e., the lip-synchronous translation and revoicing of speech adds to this constraints about the close matching of phonetic and resulting visemic synchrony characteristics of source and target material. There is an inherent conflict between a translation’s meaning preservation and ‘dubbability’ and the resulting trade-off can be controlled by weighing the synchrony constraints. We introduce our work, which to the best of our knowledge is the first of its kind, on integrating synchrony constraints into the machine translation paradigm. We present first results for the integration of synchrony constraints into encoder decoder-based neural machine translation and show that considerably more ‘dubbable’ translations can be achieved with only a small impact on BLEU score, and dubbability improves more steeply than BLEU degrades.</abstract>
      <url hash="b7d994bb">W19-5210</url>
      <doi>10.18653/v1/W19-5210</doi>
      <bibkey>saboo-baumann-2019-integration</bibkey>
    </paper>
    <paper id="11">
      <title>Widening the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>102–115</pages>
      <abstract>The transformer is a state-of-the-art neural translation model that uses attention to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the model’s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the encoder and decoder. This enables the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of lexical information passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.</abstract>
      <url hash="7f78b64c">W19-5211</url>
      <doi>10.18653/v1/W19-5211</doi>
      <bibkey>emelin-etal-2019-widening</bibkey>
      <pwccode url="https://github.com/demelin/transformer_lexical_shortcuts" additional="false">demelin/transformer_lexical_shortcuts</pwccode>
    </paper>
    <paper id="12">
      <title>A High-Quality Multilingual Dataset for Structured Documentation Translation</title>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Raffaella</first><last>Buschiazzo</last></author>
      <author><first>James</first><last>Bradbury</last></author>
      <author><first>Teresa</first><last>Marshall</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>116–127</pages>
      <abstract>This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 × 16 translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for post-editing.</abstract>
      <url hash="d169e464">W19-5212</url>
      <attachment type="supplementary" hash="08c767bb">W19-5212.Supplementary.pdf</attachment>
      <doi>10.18653/v1/W19-5212</doi>
      <bibkey>hashimoto-etal-2019-high</bibkey>
    </paper>
  </volume>
  <volume id="53" ingest-date="2019-08-15">
    <meta>
      <booktitle>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</booktitle>
      <url hash="f3ea3fb7">W19-53</url>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <editor><first>Aurélie</first><last>Névéol</last></editor>
      <editor><first>Mariana</first><last>Neves</last></editor>
      <editor><first>Matt</first><last>Post</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Karin</first><last>Verspoor</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="ab8823f5">W19-5300</url>
      <bibkey>ws-2019-machine-translation</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the 2019 Conference on Machine Translation (<fixed-case>WMT</fixed-case>19)</title>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <author><first>Mathias</first><last>Müller</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>1–61</pages>
      <abstract>This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.</abstract>
      <url hash="0409ba69">W19-5301</url>
      <doi>10.18653/v1/W19-5301</doi>
      <bibkey>barrault-etal-2019-findings</bibkey>
    </paper>
    <paper id="2">
      <title>Results of the <fixed-case>WMT</fixed-case>19 Metrics Shared Task: Segment-Level and Strong <fixed-case>MT</fixed-case> Systems Pose Big Challenges</title>
      <author><first>Qingsong</first><last>Ma</last></author>
      <author><first>Johnny</first><last>Wei</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <pages>62–90</pages>
      <abstract>This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less “metrics” and constitute submissions to the joint task with WMT19 Quality Estimation Task, “QE as a Metric”. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.</abstract>
      <url hash="108a04b1">W19-5302</url>
      <doi>10.18653/v1/W19-5302</doi>
      <attachment type="presentation" hash="23b70463">W19-5302.Presentation.pdf</attachment>
      <bibkey>ma-etal-2019-results</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt19-metrics-task">WMT19 Metrics Task</pwcdataset>
    </paper>
    <paper id="3">
      <title>Findings of the First Shared Task on Machine Translation Robustness</title>
      <author><first>Xian</first><last>Li</last></author>
      <author><first>Paul</first><last>Michel</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <pages>91–102</pages>
      <abstract>We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models’ robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson’s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.</abstract>
      <url hash="061460ae">W19-5303</url>
      <doi>10.18653/v1/W19-5303</doi>
      <bibkey>li-etal-2019-findings</bibkey>
      <pwccode url="https://github.com/neulab/compare-mt" additional="false">neulab/compare-mt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="4">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s Submissions to the <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Faheem</first><last>Kirefu</last></author>
      <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>103–115</pages>
      <abstract>The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English↔Gujarati, English↔Chinese, German→English, and English→Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English↔Gujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German→English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English→Czech, we compared different preprocessing and tokenisation regimes.</abstract>
      <url hash="8c91ee06">W19-5304</url>
      <doi>10.18653/v1/W19-5304</doi>
      <bibkey>bawden-etal-2019-university</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>GTCOM</fixed-case> Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Chao</first><last>Bei</last></author>
      <author><first>Hao</first><last>Zong</last></author>
      <author><first>Conghu</first><last>Yuan</last></author>
      <author><first>Qingming</first><last>Liu</last></author>
      <author><first>Baoyong</first><last>Fan</last></author>
      <pages>116–121</pages>
      <abstract>This paper describes the Global Tone Communication Co., Ltd.’s submission of the WMT19 shared news translation task. We participate in six directions: English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to Gujarati and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on back-translation, knowledge distillation and reranking to build a competitive model for this task. Also, we apply language model to filter monolingual data, back-translated data and parallel data. The techniques we apply for data filtering include filtering by rules, language models. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.</abstract>
      <url hash="67d47834">W19-5305</url>
      <doi>10.18653/v1/W19-5305</doi>
      <bibkey>bei-etal-2019-gtcom</bibkey>
    </paper>
    <paper id="6">
      <title>Machine Translation with parfda, <fixed-case>M</fixed-case>oses, kenlm, nplm, and <fixed-case>PRO</fixed-case></title>
      <author><first>Ergun</first><last>Biçici</last></author>
      <pages>122–128</pages>
      <abstract>We build parfda Moses statistical machine translation (SMT) models for most language pairs in the news translation task. We experiment with a hybrid approach using neural language models integrated into Moses. We obtain the constrained data statistics on the machine translation task, the coverage of the test sets, and the upper bounds on the translation results. We also contribute a new testsuite for the German-English language pair and a new automated key phrase extraction technique for the evaluation of the testsuite translations.</abstract>
      <url hash="36c43081">W19-5306</url>
      <revision id="1" href="W19-5306v1" hash="444c5306"/>
      <revision id="2" href="W19-5306v2" hash="3767b9cc">Clarifies notation in Table 7, Figure 2 caption, and Table 4.</revision>
      <doi>10.18653/v1/W19-5306</doi>
      <revision id="3" href="W19-5306v3" hash="36c43081">Clarified notation in Table 7.</revision>
      <bibkey>bicici-2019-machine</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>LIUM</fixed-case>’s Contributions to the <fixed-case>WMT</fixed-case>2019 News Translation Task: Data and Systems for <fixed-case>G</fixed-case>erman-<fixed-case>F</fixed-case>rench Language Pairs</title>
      <author><first>Fethi</first><last>Bougares</last></author>
      <author><first>Jane</first><last>Wottawa</last></author>
      <author><first>Anne</first><last>Baillot</last></author>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Adrien</first><last>Bardet</last></author>
      <pages>129–133</pages>
      <abstract>This paper describes the neural machine translation (NMT) systems of the LIUM Laboratory developed for the French↔German news translation task of the Fourth Conference onMachine Translation (WMT 2019). The chosen language pair is included for the first time in the WMT news translation task. We de-scribe how the training and the evaluation data was created. We also present our participation in the French↔German translation directions using self-attentional Transformer networks with small and big architectures.</abstract>
      <url hash="9f5c34fe">W19-5307</url>
      <doi>10.18653/v1/W19-5307</doi>
      <bibkey>bougares-etal-2019-liums</bibkey>
    </paper>
    <paper id="8">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>M</fixed-case>aryland’s <fixed-case>K</fixed-case>azakh-<fixed-case>E</fixed-case>nglish Neural Machine Translation System at <fixed-case>WMT</fixed-case>19</title>
      <author><first>Eleftheria</first><last>Briakou</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>134–140</pages>
      <abstract>This paper describes the University of Maryland’s submission to the WMT 2019 Kazakh-English news translation task. We study the impact of transfer learning from another low-resource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh-only baseline by +5.45 BLEU on newstest2019.</abstract>
      <url hash="fc22b7c9">W19-5308</url>
      <doi>10.18653/v1/W19-5308</doi>
      <bibkey>briakou-carpuat-2019-university</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>DBMS</fixed-case>-<fixed-case>KU</fixed-case> Interpolation for <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Sari Dewi</first><last>Budiwati</last></author>
      <author><first>Al Hafiz Akbar Maulana</first><last>Siagian</last></author>
      <author><first>Tirana Noor</first><last>Fatyanosa</last></author>
      <author><first>Masayoshi</first><last>Aritsugi</last></author>
      <pages>141–146</pages>
      <abstract>This paper presents the participation of DBMS-KU Interpolation system in WMT19 shared task, namely, Kazakh-English language pair. We examine the use of interpolation method using a different language model order. Our Interpolation system combines a direct translation with Russian as a pivot language. We use 3-gram and 5-gram language model orders to perform the language translation in this work. To reduce noise in the pivot translation process, we prune the phrase table of source-pivot and pivot-target. Our experimental results show that our Interpolation system outperforms the Baseline in terms of BLEU-cased score by +0.5 and +0.1 points in Kazakh-English and English-Kazakh, respectively. In particular, using the 5-gram language model order in our system could obtain better BLEU-cased score than utilizing the 3-gram one. Interestingly, we found that by employing the Interpolation system could reduce the perplexity score of English-Kazakh when using 3-gram language model order.</abstract>
      <url hash="561f2f43">W19-5309</url>
      <doi>10.18653/v1/W19-5309</doi>
      <bibkey>budiwati-etal-2019-dbms</bibkey>
    </paper>
    <paper id="10">
      <title>Lingua Custodia at <fixed-case>WMT</fixed-case>’19: Attempts to Control Terminology</title>
      <author><first>Franck</first><last>Burlot</last></author>
      <pages>147–154</pages>
      <abstract>This paper describes Lingua Custodia’s submission to the WMT’19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a machine translation system to a specific topic, aimed at providing more accurate translations of specific entities like political parties and person names, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data.</abstract>
      <url hash="4a693340">W19-5310</url>
      <doi>10.18653/v1/W19-5310</doi>
      <bibkey>burlot-2019-lingua</bibkey>
    </paper>
    <paper id="11">
      <title>The <fixed-case>TALP</fixed-case>-<fixed-case>UPC</fixed-case> Machine Translation Systems for <fixed-case>WMT</fixed-case>19 News Translation Task: Pivoting Techniques for Low Resource <fixed-case>MT</fixed-case></title>
      <author><first>Noe</first><last>Casas</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Christine</first><last>Basta</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>155–162</pages>
      <abstract>In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for Kazakh-English. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.</abstract>
      <url hash="ca8deb09">W19-5311</url>
      <doi>10.18653/v1/W19-5311</doi>
      <attachment type="poster" hash="d2ef2106">W19-5311.Poster.pdf</attachment>
      <bibkey>casas-etal-2019-talp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/united-nations-parallel-corpus">United Nations Parallel Corpus</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>K</fixed-case>yoto <fixed-case>U</fixed-case>niversity Participation to the <fixed-case>WMT</fixed-case> 2019 News Shared Task</title>
      <author><first>Fabien</first><last>Cromieres</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>163–167</pages>
      <abstract>We describe here the experiments we did for the the news translation shared task of WMT 2019. We focused on the new German-to-French language direction, and mostly used current standard approaches to develop a Neural Machine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their “checkpoint agreement”.</abstract>
      <url hash="0dabd670">W19-5312</url>
      <doi>10.18653/v1/W19-5312</doi>
      <bibkey>cromieres-kurohashi-2019-kyoto</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>NICT</fixed-case>’s Supervised Neural Machine Translation Systems for the <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Marie</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>168–174</pages>
      <abstract>In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for Kazakh↔English, Gujarati↔English, Chinese↔English, and English→Finnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: Kazakh↔English and Gujarati↔English translation. For the Chinese↔English translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of Chinese↔English. For English→Finnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year’s task.</abstract>
      <url hash="4be41821">W19-5313</url>
      <doi>10.18653/v1/W19-5313</doi>
      <bibkey>dabre-etal-2019-nicts</bibkey>
    </paper>
    <paper id="14">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>S</fixed-case>ydney’s Machine Translation System for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <pages>175–182</pages>
      <abstract>This paper describes the University of Sydney’s submission of the WMT 2019 shared news translation task. We participated in the Finnish-&gt;English direction and got the best BLEU(33.0) score among all the participants. Our system is based on the self-attentional Transformer networks, into which we integrated the most recent effective strategies from academic research (e.g., BPE, back translation, multi-features data selection, data augmentation, greedy model ensemble, reranking, ConMBR system combination, and postprocessing). Furthermore, we propose a novel augmentation method Cycle Translation and a data mixture strategy Big/Small parallel construction to entirely exploit the synthetic corpus. Extensive experiments show that adding the above techniques can make continuous improvements of the BLEU scores, and the best result outperforms the baseline (Transformer ensemble model trained with the original parallel corpus) by approximately 5.3 BLEU score, achieving the state-of-the-art performance.</abstract>
      <url hash="98a93471">W19-5314</url>
      <doi>10.18653/v1/W19-5314</doi>
      <bibkey>ding-tao-2019-university</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016-news">WMT 2016 News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="15">
      <title><fixed-case>U</fixed-case>d<fixed-case>S</fixed-case>-<fixed-case>DFKI</fixed-case> Participation at <fixed-case>WMT</fixed-case> 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems</title>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <pages>183–190</pages>
      <abstract>This paper describes the UdS-DFKI submission to the WMT2019 news translation task for Gujarati–English (low-resourced pair) and German–English (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.</abstract>
      <url hash="b8d2a9b4">W19-5315</url>
      <doi>10.18653/v1/W19-5315</doi>
      <bibkey>espana-bonet-ruiter-2019-uds</bibkey>
    </paper>
    <paper id="16">
      <title>The <fixed-case>IIIT</fixed-case>-<fixed-case>H</fixed-case> <fixed-case>G</fixed-case>ujarati-<fixed-case>E</fixed-case>nglish Machine Translation System for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Vikrant</first><last>Goyal</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <pages>191–195</pages>
      <abstract>This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the Gujarati→English news translation shared task of WMT19. Our system is basedon encoder-decoder framework with attention mechanism. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English</abstract>
      <url hash="ec436651">W19-5316</url>
      <doi>10.18653/v1/W19-5316</doi>
      <bibkey>goyal-sharma-2019-iiit</bibkey>
    </paper>
    <paper id="17">
      <title>Kingsoft’s Neural Machine Translation System for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Xinze</first><last>Guo</last></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Xiaolong</first><last>Li</last></author>
      <author><first>Yiran</first><last>Wang</last></author>
      <author><first>Guoliang</first><last>Li</last></author>
      <author><first>Feng</first><last>Wang</last></author>
      <author><first>Zhitao</first><last>Xu</last></author>
      <author><first>Liuyi</first><last>Yang</last></author>
      <author><first>Li</first><last>Ma</last></author>
      <author><first>Changliang</first><last>Li</last></author>
      <pages>196–202</pages>
      <abstract>This paper describes the Kingsoft AI Lab’s submission to the WMT2019 news translation shared task. We participated in two language directions: English-Chinese and Chinese-English. For both language directions, we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. The best translation result was obtained with ensemble and reranking techniques. According to automatic metrics (BLEU) our Chinese-English system reached the second highest score, and our English-Chinese system reached the second highest score for this subtask.</abstract>
      <url hash="93d3d390">W19-5317</url>
      <doi>10.18653/v1/W19-5317</doi>
      <bibkey>guo-etal-2019-kingsofts</bibkey>
    </paper>
    <paper id="18">
      <title>The <fixed-case>AFRL</fixed-case> <fixed-case>WMT</fixed-case>19 Systems: Old Favorites and New Tricks</title>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <author><first>Grant</first><last>Erdmann</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <pages>203–208</pages>
      <abstract>This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign. This year, we refine our approach to training popular neural machine translation toolkits, experiment with a new domain adaptation technique and again measure improvements in performance on the Russian–English language pair.</abstract>
      <url hash="89e8631c">W19-5318</url>
      <doi>10.18653/v1/W19-5318</doi>
      <bibkey>gwinnup-etal-2019-afrl</bibkey>
    </paper>
    <paper id="19">
      <title>Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models</title>
      <author><first>Chris</first><last>Hokamp</last></author>
      <author><first>John</first><last>Glover</last></author>
      <author><first>Demian</first><last>Gholipour Ghalandari</last></author>
      <pages>209–217</pages>
      <abstract>We study several methods for full or partial sharing of the decoder parameters of multi-lingual NMT models. Using only the WMT 2019 shared task parallel datasets for training, we evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions. We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available. To our knowledge, this is the largest evaluation of multi-lingual translation yet conducted in terms of the total size of the training data we use, and in terms of the number of zero-shot translation pairs we evaluate. We conduct an in-depth evaluation of the translation performance of different models, highlighting the trade-offs between methods of sharing decoder parameters. We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks.</abstract>
      <url hash="d4c6cf20">W19-5319</url>
      <doi>10.18653/v1/W19-5319</doi>
      <bibkey>hokamp-etal-2019-evaluating</bibkey>
    </paper>
    <paper id="20">
      <title>The <fixed-case>MLLP</fixed-case>-<fixed-case>UPV</fixed-case> Supervised Machine Translation Systems for <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Javier</first><last>Iranzo-Sánchez</last></author>
      <author><first>Gonçal</first><last>Garcés Díaz-Munío</last></author>
      <author><first>Jorge</first><last>Civera</last></author>
      <author><first>Alfons</first><last>Juan</last></author>
      <pages>218–224</pages>
      <abstract>This paper describes the participation of the MLLP research group of the Universitat Politècnica de València in the WMT 2019 News Translation Shared Task. In this edition, we have submitted systems for the German ↔ English and German ↔ French language pairs, participating in both directions of each pair. Our submitted systems, based on the Transformer architecture, make ample use of data filtering, synthetic data and domain adaptation through fine-tuning.</abstract>
      <url hash="f82e7e55">W19-5320</url>
      <doi>10.18653/v1/W19-5320</doi>
      <attachment type="poster" hash="78f90a5d">W19-5320.Poster.pdf</attachment>
      <bibkey>iranzo-sanchez-etal-2019-mllp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018-news">WMT 2018 News</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>M</fixed-case>icrosoft Translator at <fixed-case>WMT</fixed-case> 2019: Towards Large-Scale Document-Level Neural Machine Translation</title>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <pages>225–233</pages>
      <abstract>This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with translationese input. We explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.</abstract>
      <url hash="b1427483">W19-5321</url>
      <doi>10.18653/v1/W19-5321</doi>
      <attachment type="poster" hash="cacbd7c2">W19-5321.Poster.pdf</attachment>
      <bibkey>junczys-dowmunt-2019-microsoft</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>CUNI</fixed-case> Submission for Low-Resource Languages in <fixed-case>WMT</fixed-case> News 2019</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>234–240</pages>
      <abstract>This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages: Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines transfer learning from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission.</abstract>
      <url hash="375941f4">W19-5322</url>
      <doi>10.18653/v1/W19-5322</doi>
      <attachment type="poster" hash="0950e838">W19-5322.Poster.pdf</attachment>
      <bibkey>kocmi-bojar-2019-cuni</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>CUNI</fixed-case> Systems for the Unsupervised News Translation Task in <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Ivana</first><last>Kvapilíková</last></author>
      <author><first>Dominik</first><last>Macháček</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>241–248</pages>
      <abstract>In this paper we describe the CUNI translation system used for the unsupervised news shared task of the ACL 2019 Fourth Conference on Machine Translation (WMT19). We follow the strategy of Artetxe ae at. (2018b), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel data. The synthetic corpus was produced from a monolingual corpus by a tuned PBMT model refined through iterative back-translation. We further focus on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffers most. Our system reaches a BLEU score of 15.3 on the German-Czech WMT19 shared task.</abstract>
      <url hash="2d0223a5">W19-5323</url>
      <doi>10.18653/v1/W19-5323</doi>
      <attachment type="poster" hash="ebc6a16b">W19-5323.Poster.pdf</attachment>
      <bibkey>kvapilikova-etal-2019-cuni</bibkey>
    </paper>
    <paper id="24">
      <title>A Comparison on Fine-grained Pre-trained Embeddings for the <fixed-case>WMT</fixed-case>19<fixed-case>C</fixed-case>hinese-<fixed-case>E</fixed-case>nglish News Translation Task</title>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>249–256</pages>
      <abstract>This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task. Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information. We compare models with these different granularity levels using different evaluating metics. We find that a finer granularity embeddings can help the model according to character level evaluation and that the pre-trained embeddings can also be beneficial for model performance marginally when the training data is limited.</abstract>
      <url hash="26c54e1b">W19-5324</url>
      <doi>10.18653/v1/W19-5324</doi>
      <bibkey>li-specia-2019-comparison</bibkey>
    </paper>
    <paper id="25">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans Machine Translation Systems for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Bei</first><last>Li</last></author>
      <author><first>Yinqiao</first><last>Li</last></author>
      <author><first>Chen</first><last>Xu</last></author>
      <author><first>Ye</first><last>Lin</last></author>
      <author><first>Jiqiang</first><last>Liu</last></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Ziyang</first><last>Wang</last></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Nuo</first><last>Xu</last></author>
      <author><first>Zeyang</first><last>Wang</last></author>
      <author><first>Kai</first><last>Feng</last></author>
      <author><first>Hexuan</first><last>Chen</last></author>
      <author><first>Tengbo</first><last>Liu</last></author>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Qiang</first><last>Wang</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>257–266</pages>
      <abstract>This paper described NiuTrans neural machine translation systems for the WMT 2019 news translation tasks. We participated in 13 translation directions, including 11 supervised tasks, namely EN↔{ZH, DE, RU, KK, LT}, GU→EN and the unsupervised DE↔CS sub-track. Our systems were built on Deep Transformer and several back-translation methods. Iterative knowledge distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions.</abstract>
      <url hash="9e8d33a8">W19-5325</url>
      <doi>10.18653/v1/W19-5325</doi>
      <bibkey>li-etal-2019-niutrans</bibkey>
    </paper>
    <paper id="26">
      <title>Multi-Source Transformer for <fixed-case>K</fixed-case>azakh-<fixed-case>R</fixed-case>ussian-<fixed-case>E</fixed-case>nglish Neural Machine Translation</title>
      <author><first>Patrick</first><last>Littell</last></author>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <author><first>Darlene</first><last>Stewart</last></author>
      <pages>267–274</pages>
      <abstract>We describe the neural machine translation (NMT) system developed at the National Research Council of Canada (NRC) for the Kazakh-English news translation task of the Fourth Conference on Machine Translation (WMT19). Our submission is a multi-source NMT taking both the original Kazakh sentence and its Russian translation as input for translating into English.</abstract>
      <url hash="39a6ebb8">W19-5326</url>
      <doi>10.18653/v1/W19-5326</doi>
      <bibkey>littell-etal-2019-multi</bibkey>
    </paper>
    <paper id="27">
      <title>Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring</title>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>275–282</pages>
      <abstract>This paper describes CAiRE’s submission to the unsupervised machine translation track of the WMT’19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.</abstract>
      <url hash="7939a732">W19-5327</url>
      <doi>10.18653/v1/W19-5327</doi>
      <bibkey>liu-etal-2019-incorporating-word</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>JUMT</fixed-case> at <fixed-case>WMT</fixed-case>2019 News Translation Task: A Hybrid Approach to Machine Translation for <fixed-case>L</fixed-case>ithuanian to <fixed-case>E</fixed-case>nglish</title>
      <author><first>Sainik Kumar</first><last>Mahata</last></author>
      <author><first>Avishek</first><last>Garain</last></author>
      <author><first>Adityar</first><last>Rayala</last></author>
      <author><first>Dipankar</first><last>Das</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>283–286</pages>
      <abstract>In the current work, we present a description of the system submitted to WMT 2019 News Translation Shared task. The system was created to translate news text from Lithuanian to English. To accomplish the given task, our system used a Word Embedding based Neural Machine Translation model to post edit the outputs generated by a Statistical Machine Translation model. The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same. Our system garnered a BLEU score of 17.6.</abstract>
      <url hash="c01ddc58">W19-5328</url>
      <doi>10.18653/v1/W19-5328</doi>
      <bibkey>mahata-etal-2019-jumt</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>J</fixed-case>ohns <fixed-case>H</fixed-case>opkins <fixed-case>U</fixed-case>niversity Submission for <fixed-case>WMT</fixed-case> News Translation Task</title>
      <author><first>Kelly</first><last>Marchisio</last></author>
      <author><first>Yash Kumar</first><last>Lal</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>287–293</pages>
      <abstract>We describe the work of Johns Hopkins University for the shared task of news translation organized by the Fourth Conference on Machine Translation (2019). We submitted systems for both directions of the English-German language pair. The systems combine multiple techniques – sampling, filtering, iterative backtranslation, and continued training – previously used to improve performance of neural machine translation models. At submission time, we achieve a BLEU score of 38.1 for De-En and 42.5 for En-De translation directions on newstest2019. Post-submission, the score is 38.4 for De-En and 42.8 for En-De. Various experiments conducted in the process are also described.</abstract>
      <url hash="bc538c19">W19-5329</url>
      <doi>10.18653/v1/W19-5329</doi>
      <bibkey>marchisio-etal-2019-johns</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>NICT</fixed-case>’s Unsupervised Neural and Statistical Machine Translation Systems for the <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Benjamin</first><last>Marie</last></author>
      <author><first>Haipeng</first><last>Sun</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>294–301</pages>
      <abstract>This paper presents the NICT’s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction: German-Czech. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (“constraint’”), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.</abstract>
      <url hash="80482f43">W19-5330</url>
      <doi>10.18653/v1/W19-5330</doi>
      <bibkey>marie-etal-2019-nicts</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>PROMT</fixed-case> Systems for <fixed-case>WMT</fixed-case> 2019 Shared Translation Task</title>
      <author><first>Alexander</first><last>Molchanov</last></author>
      <pages>302–307</pages>
      <abstract>This paper describes the PROMT submissions for the WMT 2019 Shared News Translation Task. This year we participated in two language pairs and in three directions: English-Russian, English-German and German-English. All our submissions are Marian-based neural systems. We use significantly more data compared to the last year. We also present our improved data filtering pipeline.</abstract>
      <url hash="3bb55e8d">W19-5331</url>
      <doi>10.18653/v1/W19-5331</doi>
      <bibkey>molchanov-2019-promt</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>JU</fixed-case>-<fixed-case>S</fixed-case>aarland Submission to the <fixed-case>WMT</fixed-case>2019 <fixed-case>E</fixed-case>nglish–<fixed-case>G</fixed-case>ujarati Translation Shared Task</title>
      <author><first>Riktim</first><last>Mondal</last></author>
      <author><first>Shankha Raj</first><last>Nayek</last></author>
      <author><first>Aditya</first><last>Chowdhury</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>308–313</pages>
      <abstract>In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English–Gujarati language pair within the translation task sub-track. Our baseline and primary submissions are built using Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism. Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, building a high quality NMT system for this language pair is a difficult task. We produced synthetic data through back-translation from available monolingual data. We report the translation quality of our English–Gujarati and Gujarati–English NMT systems trained at word, byte-pair and character encoding levels where RNN at word level is considered as the baseline and used for comparison purpose. Our English–Gujarati system ranked in the second position in the shared task.</abstract>
      <url hash="4e02ba75">W19-5332</url>
      <doi>10.18653/v1/W19-5332</doi>
      <bibkey>mondal-etal-2019-ju</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>F</fixed-case>acebook <fixed-case>FAIR</fixed-case>’s <fixed-case>WMT</fixed-case>19 News Translation Task Submission</title>
      <author><first>Nathan</first><last>Ng</last></author>
      <author><first>Kyra</first><last>Yee</last></author>
      <author><first>Alexei</first><last>Baevski</last></author>
      <author><first>Myle</first><last>Ott</last></author>
      <author><first>Michael</first><last>Auli</last></author>
      <author><first>Sergey</first><last>Edunov</last></author>
      <pages>314–319</pages>
      <abstract>This paper describes Facebook FAIR’s submission to the WMT19 shared news translation task. We participate in four language directions, English &lt;-&gt; German and English &lt;-&gt; Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system’s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian.</abstract>
      <url hash="e4a814e0">W19-5333</url>
      <doi>10.18653/v1/W19-5333</doi>
      <bibkey>ng-etal-2019-facebook</bibkey>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="34">
      <title>e<fixed-case>T</fixed-case>ranslation’s Submissions to the <fixed-case>WMT</fixed-case> 2019 News Translation Task</title>
      <author><first>Csaba</first><last>Oravecz</last></author>
      <author><first>Katina</first><last>Bontcheva</last></author>
      <author><first>Adrien</first><last>Lardilleux</last></author>
      <author><first>László</first><last>Tihanyi</last></author>
      <author><first>Andreas</first><last>Eisele</last></author>
      <pages>320–326</pages>
      <abstract>This paper describes the submissions of the eTranslation team to the WMT 2019 news translation shared task. The systems have been developed with the aim of identifying and following rather than establishing best practices, under the constraints imposed by a low resource training and decoding environment normally used for our production systems. Thus most of the findings and results are transferable to systems used in the eTranslation service. Evaluations suggest that this approach is able to produce decent models with good performance and speed without the overhead of using prohibitively deep and complex architectures.</abstract>
      <url hash="ed6ebe3f">W19-5334</url>
      <doi>10.18653/v1/W19-5334</doi>
      <attachment type="poster" hash="ee396b0b">W19-5334.Poster.pdf</attachment>
      <bibkey>oravecz-etal-2019-etranslations</bibkey>
    </paper>
    <paper id="35">
      <title>Tilde’s Machine Translation Systems for <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Marcis</first><last>Pinnis</last></author>
      <author><first>Rihards</first><last>Krišlauks</last></author>
      <author><first>Matīss</first><last>Rikters</last></author>
      <pages>327–334</pages>
      <abstract>The paper describes the development process of Tilde’s NMT systems for the WMT 2019 shared task on news translation. We trained systems for the English-Lithuanian and Lithuanian-English translation directions in constrained and unconstrained tracks. We build upon the best methods of the previous year’s competition and combine them with recent advancements in the field. We also present a new method to ensure source domain adherence in back-translated data. Our systems achieved a shared first place in human evaluation.</abstract>
      <url hash="f39f78de">W19-5335</url>
      <doi>10.18653/v1/W19-5335</doi>
      <attachment type="poster" hash="c220d13f">W19-5335.Poster.pdf</attachment>
      <bibkey>pinnis-etal-2019-tildes</bibkey>
    </paper>
    <paper id="36">
      <title>Apertium-fin-eng–Rule-based Shallow Machine Translation for <fixed-case>WMT</fixed-case> 2019 Shared Task</title>
      <author><first>Tommi</first><last>Pirinen</last></author>
      <pages>335–341</pages>
      <abstract>In this paper we describe a rule-based, bi-directional machine translation system for the Finnish—English language pair. The baseline system was based on the existing data of FinnWordNet, omorfi and apertium-eng. We have built the disambiguation, lexical selection and translation rules by hand. The dictionaries and rules have been developed based on the shared task data. We describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort.</abstract>
      <url hash="1d2cec39">W19-5336</url>
      <doi>10.18653/v1/W19-5336</doi>
      <attachment type="poster" hash="a20f5033">W19-5336.Poster.pdf</attachment>
      <bibkey>pirinen-2019-apertium</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech Systems in <fixed-case>WMT</fixed-case>19: Document-Level Transformer</title>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Dominik</first><last>Macháček</last></author>
      <author><first>Michal</first><last>Auersperger</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>342–348</pages>
      <abstract>We describe our NMT systems submitted to the WMT19 shared task in English→Czech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation, this “document-level”-trained system achieves a +0.6 BLEU improvement (p &lt; 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence.</abstract>
      <url hash="591cfdef">W19-5337</url>
      <doi>10.18653/v1/W19-5337</doi>
      <bibkey>popel-etal-2019-english</bibkey>
    </paper>
    <paper id="38">
      <title>The <fixed-case>RWTH</fixed-case> <fixed-case>A</fixed-case>achen <fixed-case>U</fixed-case>niversity Machine Translation Systems for <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Jan</first><last>Rosendahl</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Yunsu</first><last>Kim</last></author>
      <author><first>Miguel</first><last>Graça</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Parnia</first><last>Bahar</last></author>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>349–355</pages>
      <abstract>This paper describes the neural machine translation systems developed at the RWTH Aachen University for the German-English, Chinese-English and Kazakh-English news translation tasks of the Fourth Conference on Machine Translation (WMT19). For all tasks, the final submitted system is based on the Transformer architecture. We focus on improving data filtering and fine-tuning as well as systematically evaluating interesting approaches like unigram language model segmentation and transfer learning. For the De-En task, none of the tested methods gave a significant improvement over last years winning system and we end up with the same performance, resulting in 39.6% BLEU on newstest2019. In the Zh-En task, we show 1.3% BLEU improvement over our last year’s submission, which we mostly attribute to the splitting of long sentences during translation. We further report results on the Kazakh-English task where we gain improvements of 11.1% BLEU over our baseline system. On the same task we present a recent transfer learning approach, which uses half of the free parameters of our submission system and performs on par with it.</abstract>
      <url hash="1db58e3f">W19-5338</url>
      <doi>10.18653/v1/W19-5338</doi>
      <bibkey>rosendahl-etal-2019-rwth</bibkey>
    </paper>
    <paper id="39">
      <title>The <fixed-case>U</fixed-case>niversitat d’Alacant Submissions to the <fixed-case>E</fixed-case>nglish-to-<fixed-case>K</fixed-case>azakh News Translation Task at <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
      <pages>356–363</pages>
      <abstract>This paper describes the two submissions of Universitat d’Alacant to the English-to-Kazakh news translation task at WMT 2019. Our submissions take advantage of monolingual data and parallel data from other language pairs by means of iterative backtranslation, pivot backtranslation and transfer learning. They also use linguistic information in two ways: morphological segmentation of Kazakh text, and integration of the output of a rule-based machine translation system. Our systems were ranked second in terms of chrF++ despite being built from an ensemble of only 2 independent training runs.</abstract>
      <url hash="4cba8e7b">W19-5339</url>
      <doi>10.18653/v1/W19-5339</doi>
      <bibkey>sanchez-cartagena-etal-2019-universitat</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>CUED</fixed-case>@<fixed-case>WMT</fixed-case>19:<fixed-case>EWC</fixed-case>&amp;<fixed-case>LM</fixed-case>s</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Adrià</first><last>de Gispert</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>364–373</pages>
      <abstract>Two techniques provide the fabric of the Cambridge University Engineering Department’s (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.</abstract>
      <url hash="481471d2">W19-5340</url>
      <doi>10.18653/v1/W19-5340</doi>
      <bibkey>stahlberg-etal-2019-cued</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>B</fixed-case>aidu Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Meng</first><last>Sun</last></author>
      <author><first>Bojian</first><last>Jiang</last></author>
      <author><first>Hao</first><last>Xiong</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>374–381</pages>
      <abstract>In this paper we introduce the systems Baidu submitted for the WMT19 shared task on Chinese&lt;-&gt;English news translation. Our systems are based on the Transformer architecture with some effective improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our Chinese-&gt;English system achieved the highest case-sensitive BLEU score among all constrained submissions, and our English-&gt;Chinese system ranked the second in all submissions.</abstract>
      <url hash="4e08bc54">W19-5341</url>
      <doi>10.18653/v1/W19-5341</doi>
      <bibkey>sun-etal-2019-baidu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="42">
      <title><fixed-case>U</fixed-case>niversity of <fixed-case>T</fixed-case>artu’s Multilingual Multi-domain <fixed-case>WMT</fixed-case>19 News Translation Shared Task Submission</title>
      <author><first>Andre</first><last>Tättar</last></author>
      <author><first>Elizaveta</first><last>Korotkova</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>382–385</pages>
      <abstract>This paper describes the University of Tartu’s submission to the news translation shared task of WMT19, where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results. We only used the constrained data from the shared task. We describe our approach and its results and discuss the technical issues we faced.</abstract>
      <url hash="97fb5887">W19-5342</url>
      <doi>10.18653/v1/W19-5342</doi>
      <bibkey>tattar-etal-2019-university</bibkey>
    </paper>
    <paper id="43">
      <title>Neural Machine Translation for <fixed-case>E</fixed-case>nglish–<fixed-case>K</fixed-case>azakh with Morphological Segmentation and Synthetic Data</title>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Lukas</first><last>Edman</last></author>
      <author><first>Galiya</first><last>Yeshmagambetova</last></author>
      <author><first>Jennifer</first><last>Spenader</last></author>
      <pages>386–392</pages>
      <abstract>This paper presents the systems submitted by the University of Groningen to the English– Kazakh language pair (both translation directions) for the WMT 2019 news translation task. We explore the potential benefits of (i) morphological segmentation (both unsupervised and rule-based), given the agglutinative nature of Kazakh, (ii) data from two additional languages (Turkish and Russian), given the scarcity of English–Kazakh data and (iii) synthetic data, both for the source and for the target language. Our best submissions ranked second for Kazakh→English and third for English→Kazakh in terms of the BLEU automatic evaluation metric.</abstract>
      <url hash="2febf8ab">W19-5343</url>
      <doi>10.18653/v1/W19-5343</doi>
      <bibkey>toral-etal-2019-neural</bibkey>
    </paper>
    <paper id="44">
      <title>The <fixed-case>LMU</fixed-case> <fixed-case>M</fixed-case>unich Unsupervised Machine Translation System for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>393–399</pages>
      <abstract>We describe LMU Munich’s machine translation system for German→Czech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our model using monolingual data only from both languages. The final model is an unsupervised neural model using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.</abstract>
      <url hash="3957685d">W19-5344</url>
      <doi>10.18653/v1/W19-5344</doi>
      <bibkey>stojanovski-etal-2019-lmu</bibkey>
    </paper>
    <paper id="45">
      <title>Combining Local and Document-Level Context: The <fixed-case>LMU</fixed-case> <fixed-case>M</fixed-case>unich Neural Machine Translation System at <fixed-case>WMT</fixed-case>19</title>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>400–406</pages>
      <abstract>We describe LMU Munich’s machine translation system for English→German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.</abstract>
      <url hash="a2d8fbff">W19-5345</url>
      <doi>10.18653/v1/W19-5345</doi>
      <bibkey>stojanovski-fraser-2019-combining</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>IITP</fixed-case>-<fixed-case>MT</fixed-case> System for <fixed-case>G</fixed-case>ujarati-<fixed-case>E</fixed-case>nglish News Translation Task at <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Sukanta</first><last>Sen</last></author>
      <author><first>Kamal Kumar</first><last>Gupta</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>407–411</pages>
      <abstract>We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.</abstract>
      <url hash="580edaa1">W19-5346</url>
      <doi>10.18653/v1/W19-5346</doi>
      <bibkey>sen-etal-2019-iitp</bibkey>
    </paper>
    <paper id="47">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki Submissions to the <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Umut</first><last>Sulubacak</last></author>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Arvi</first><last>Hurskainen</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>412–423</pages>
      <abstract>In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs: English-German, English-Finnish and Finnish-English. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</abstract>
      <url hash="731c482b">W19-5347</url>
      <doi>10.18653/v1/W19-5347</doi>
      <bibkey>talman-etal-2019-university</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>M</fixed-case>icrosoft <fixed-case>R</fixed-case>esearch <fixed-case>A</fixed-case>sia’s Systems for <fixed-case>WMT</fixed-case>19</title>
      <author><first>Yingce</first><last>Xia</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Fei</first><last>Tian</last></author>
      <author><first>Fei</first><last>Gao</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Weicong</first><last>Chen</last></author>
      <author><first>Yang</first><last>Fan</last></author>
      <author><first>Linyuan</first><last>Gong</last></author>
      <author><first>Yichong</first><last>Leng</last></author>
      <author><first>Renqian</first><last>Luo</last></author>
      <author><first>Yiren</first><last>Wang</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Jinhua</first><last>Zhu</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>424–433</pages>
      <abstract>We Microsoft Research Asia made submissions to 11 language directions in the WMT19 news translation tasks. We won the first place for 8 of the 11 directions and the second place for the other three. Our basic systems are built on Transformer, back translation and knowledge distillation. We integrate several of our rececent techniques to enhance the baseline systems: multi-agent dual learning (MADL), masked sequence-to-sequence pre-training (MASS), neural architecture optimization (NAO), and soft contextual data augmentation (SCA).</abstract>
      <url hash="3d15681f">W19-5348</url>
      <doi>10.18653/v1/W19-5348</doi>
      <bibkey>xia-etal-2019-microsoft</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/asirra">ASIRRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/azure-functions-trace-2019">Azure Functions Trace 2019</pwcdataset>
    </paper>
    <paper id="49">
      <title>The En-<fixed-case>R</fixed-case>u Two-way Integrated Machine Translation System Based on Transformer</title>
      <author><first>Doron</first><last>Yu</last></author>
      <pages>434–439</pages>
      <abstract>Machine translation is one of the most popular areas in natural language processing. WMT is a conference to assess the level of machine translation capabilities of organizations around the world, which is the evaluation activity we participated in. In this review we participated in a two-way translation track from Russian to English and English to Russian. We used official training data, 38 million parallel corpora, and 10 million monolingual corpora. The overall framework we use is the Transformer neural machine translation model, supplemented by data filtering, post-processing, reordering and other related processing methods. The BLEU value of our final translation result from Russian to English is 38.7, ranking 5th, while from English to Russian is 27.8, ranking 10th.</abstract>
      <url hash="00c64262">W19-5349</url>
      <doi>10.18653/v1/W19-5349</doi>
      <bibkey>yu-2019-en</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>DFKI</fixed-case>-<fixed-case>NMT</fixed-case> Submission to the <fixed-case>WMT</fixed-case>19 News Translation Task</title>
      <author><first>Jingyi</first><last>Zhang</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>440–444</pages>
      <abstract>This paper describes the DFKI-NMT submission to the WMT19 News translation task. We participated in both English-to-German and German-to-English directions. We trained Transformer models and adopted various techniques for effectively training our models, including data selection, back-translation and in-domain fine-tuning. We give a detailed analysis of the performance of our system.</abstract>
      <url hash="34d0a297">W19-5350</url>
      <doi>10.18653/v1/W19-5350</doi>
      <bibkey>zhang-van-genabith-2019-dfki</bibkey>
    </paper>
    <paper id="51">
      <title>Linguistic Evaluation of <fixed-case>G</fixed-case>erman-<fixed-case>E</fixed-case>nglish Machine Translation Using a Test Suite</title>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Ursula</first><last>Strohriegel</last></author>
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <pages>445–454</pages>
      <abstract>We present the results of the application of a grammatical test suite for German-to-English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories. The systems still translate wrong one out of four test items in average. Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency. When compared to last year, there has been a improvement of function words, non verbal agreement and punctuation. More detailed conclusions about particular systems and phenomena are also presented.</abstract>
      <url hash="760d7b24">W19-5351</url>
      <attachment type="supplementary" hash="d43c6078">W19-5351.Supplementary.tgz</attachment>
      <doi>10.18653/v1/W19-5351</doi>
      <bibkey>avramidis-etal-2019-linguistic</bibkey>
    </paper>
    <paper id="52">
      <title>A Test Suite and Manual Evaluation of Document-Level <fixed-case>NMT</fixed-case> at <fixed-case>WMT</fixed-case>19</title>
      <author><first>Kateřina</first><last>Rysová</last></author>
      <author><first>Magdaléna</first><last>Rysová</last></author>
      <author><first>Tomáš</first><last>Musil</last></author>
      <author><first>Lucie</first><last>Poláková</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>455–463</pages>
      <abstract>As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.</abstract>
      <url hash="e2dd1f22">W19-5352</url>
      <doi>10.18653/v1/W19-5352</doi>
      <attachment type="poster" hash="ad3f373a">W19-5352.Poster.pdf</attachment>
      <bibkey>rysova-etal-2019-test</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="53">
      <title>Evaluating Conjunction Disambiguation on <fixed-case>E</fixed-case>nglish-to-<fixed-case>G</fixed-case>erman and <fixed-case>F</fixed-case>rench-to-<fixed-case>G</fixed-case>erman <fixed-case>WMT</fixed-case> 2019 Translation Hypotheses</title>
      <author><first>Maja</first><last>Popović</last></author>
      <pages>464–469</pages>
      <abstract>We present a test set for evaluating an MT system’s capability to translate ambiguous conjunctions depending on the sentence structure. We concentrate on the English conjunction “but” and its French equivalent “mais” which can be translated into two different German conjunctions. We evaluate all English-to-German and French-to-German submissions to the WMT 2019 shared translation task. The evaluation is done mainly automatically, with additional fast manual inspection of unclear cases. All systems almost perfectly recognise the target conjunction “aber”, whereas accuracies for the other target conjunction “sondern” range from 78% to 97%, and the errors are mostly caused by replacing it with the alternative conjunction “aber”. The best performing system for both language pairs is a multilingual Transformer “TartuNLP” system trained on all WMT 2019 language pairs which use the Latin script, indicating that the multilingual approach is beneficial for conjunction disambiguation. As for other system features, such as using synthetic back-translated data, context-aware, hybrid, etc., no particular (dis)advantages can be observed. Qualitative manual inspection of translation hypotheses shown that highly ranked systems generally produce translations with high adequacy and fluency, meaning that these systems are not only capable of capturing the right conjunction whereas the rest of the translation hypothesis is poor. On the other hand, the low ranked systems generally exhibit lower fluency and poor adequacy.</abstract>
      <url hash="51b384d0">W19-5353</url>
      <doi>10.18653/v1/W19-5353</doi>
      <bibkey>popovic-2019-evaluating</bibkey>
    </paper>
    <paper id="54">
      <title>The <fixed-case>M</fixed-case>u<fixed-case>C</fixed-case>o<fixed-case>W</fixed-case> Test Suite at <fixed-case>WMT</fixed-case> 2019: Automatically Harvested Multilingual Contrastive Word Sense Disambiguation Test Sets for Machine Translation</title>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>470–480</pages>
      <abstract>Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs. One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types. We present MuCoW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 thousand contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet. We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using NMT pretrained models. The MuCoW test suite is available at http://github.com/Helsinki-NLP/MuCoW.</abstract>
      <url hash="b7c7d4c6">W19-5354</url>
      <doi>10.18653/v1/W19-5354</doi>
      <bibkey>raganato-etal-2019-mucow</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/MuCoW" additional="false">Helsinki-NLP/MuCoW</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="55">
      <title><fixed-case>SAO</fixed-case> <fixed-case>WMT</fixed-case>19 Test Suite: Machine Translation of Audit Reports</title>
      <author><first>Tereza</first><last>Vojtěchová</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Miloš</first><last>Klouček</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>481–493</pages>
      <abstract>This paper describes a machine translation test set of documents from the auditing domain and its use as one of the “test suites” in the WMT19 News Translation Task for translation directions involving Czech, English and German. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.</abstract>
      <url hash="cca76b73">W19-5355</url>
      <doi>10.18653/v1/W19-5355</doi>
      <attachment type="poster" hash="73e7f771">W19-5355.Poster.pdf</attachment>
      <bibkey>vojtechova-etal-2019-sao</bibkey>
      <pwccode url="https://github.com/ELITR/wmt19-elitr-testsuite" additional="false">ELITR/wmt19-elitr-testsuite</pwccode>
    </paper>
    <paper id="56">
      <title><fixed-case>WMDO</fixed-case>: Fluency-based Word Mover’s Distance for Machine Translation Evaluation</title>
      <author><first>Julian</first><last>Chow</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <pages>494–500</pages>
      <abstract>We propose WMDO, a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation’s word order have not been fully explored. Building on the Word Mover’s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics.</abstract>
      <url hash="7c8219b6">W19-5356</url>
      <doi>10.18653/v1/W19-5356</doi>
      <bibkey>chow-etal-2019-wmdo</bibkey>
    </paper>
    <paper id="57">
      <title>Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation</title>
      <author><first>Yinuo</first><last>Guo</last></author>
      <author><first>Junfeng</first><last>Hu</last></author>
      <pages>501–506</pages>
      <abstract>This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the lexical level and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed metric outperforms all previous versions of Meteor.</abstract>
      <url hash="0bc6a04e">W19-5357</url>
      <doi>10.18653/v1/W19-5357</doi>
      <bibkey>guo-hu-2019-meteor</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>Y</fixed-case>i<fixed-case>S</fixed-case>i - a Unified Semantic <fixed-case>MT</fixed-case> Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources</title>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <pages>507–513</pages>
      <abstract>We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1’s scores with human judgment is made by using contextual embeddings in multilingual BERT–Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available.</abstract>
      <url hash="7428f607">W19-5358</url>
      <doi>10.18653/v1/W19-5358</doi>
      <bibkey>lo-2019-yisi</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>EED</fixed-case>: Extended Edit Distance Measure for Machine Translation</title>
      <author><first>Peter</first><last>Stanchev</last></author>
      <author><first>Weiyue</first><last>Wang</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>514–520</pages>
      <abstract>Over the years a number of machine translation metrics have been developed in order to evaluate the accuracy and quality of machine-generated translations. Metrics such as BLEU and TER have been used for decades. However, with the rapid progress of machine translation systems, the need for better metrics is growing. This paper proposes an extension of the edit distance, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.</abstract>
      <url hash="8a4abc9a">W19-5359</url>
      <doi>10.18653/v1/W19-5359</doi>
      <bibkey>stanchev-etal-2019-eed</bibkey>
    </paper>
    <paper id="60">
      <title>Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation</title>
      <author><first>Ryoma</first><last>Yoshimura</last></author>
      <author><first>Hiroki</first><last>Shimanaka</last></author>
      <author><first>Yukio</first><last>Matsumura</last></author>
      <author><first>Hayahide</first><last>Yamagishi</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>521–525</pages>
      <abstract>In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of sentence BLEU using filtered pseudo-references. We propose a method to filter pseudo-references by paraphrasing for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by paraphrasing in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references.</abstract>
      <url hash="4241b220">W19-5360</url>
      <doi>10.18653/v1/W19-5360</doi>
      <bibkey>yoshimura-etal-2019-filtering</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
    </paper>
    <paper id="61">
      <title>Naver Labs <fixed-case>E</fixed-case>urope’s Systems for the <fixed-case>WMT</fixed-case>19 Machine Translation Robustness Task</title>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <author><first>Claude</first><last>Roux</last></author>
      <pages>526–532</pages>
      <abstract>This paper describes the systems that we submitted to the WMT19 Machine Translation robustness task. This task aims to improve MT’s robustness to noise found on social media, like informal language, spelling mistakes and other orthographic variations. The organizers provide parallel data extracted from a social media website in two language pairs: French-English and Japanese-English (one for each language direction). The goal is to obtain the best scores on unseen test sets from the same source, according to automatic metrics (BLEU) and human evaluation. We propose one single and one ensemble system for each translation direction. Our ensemble models ranked first in all language pairs, according to BLEU evaluation. We discuss the pre-processing choices that we made, and present our solutions for robustness to noise and domain adaptation.</abstract>
      <url hash="b69adddc">W19-5361</url>
      <doi>10.18653/v1/W19-5361</doi>
      <bibkey>berard-etal-2019-naver</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="62">
      <title><fixed-case>NICT</fixed-case>’s Supervised Neural Machine Translation Systems for the <fixed-case>WMT</fixed-case>19 Translation Robustness Task</title>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>533–536</pages>
      <abstract>In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.</abstract>
      <url hash="1bf963d7">W19-5362</url>
      <doi>10.18653/v1/W19-5362</doi>
      <bibkey>dabre-sumita-2019-nicts-supervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="63">
      <title>System Description: The Submission of <fixed-case>FOKUS</fixed-case> to the <fixed-case>WMT</fixed-case> 19 Robustness Task</title>
      <author><first>Cristian</first><last>Grozea</last></author>
      <pages>537–538</pages>
      <abstract>This paper describes the systems of Fraunhofer FOKUS for the WMT 2019 machine translation robustness task. We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs. The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task. These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data. The last one used the same model class and training procedure, with induced typos in the training data to increase the model robustness.</abstract>
      <url hash="d407eedb">W19-5363</url>
      <doi>10.18653/v1/W19-5363</doi>
      <bibkey>grozea-2019-system</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>CUNI</fixed-case> System for the <fixed-case>WMT</fixed-case>19 Robustness Task</title>
      <author><first>Jindřich</first><last>Helcl</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>539–543</pages>
      <abstract>We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.</abstract>
      <url hash="8a2f79e1">W19-5364</url>
      <doi>10.18653/v1/W19-5364</doi>
      <bibkey>helcl-etal-2019-cuni</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="65">
      <title><fixed-case>NTT</fixed-case>’s Machine Translation Systems for <fixed-case>WMT</fixed-case>19 Robustness Task</title>
      <author><first>Soichiro</first><last>Murakami</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>544–551</pages>
      <abstract>This paper describes NTT’s submission to the WMT19 robustness task. This task mainly focuses on translating noisy text (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as news. Our submission combined techniques including utilization of a synthetic corpus, domain adaptation, and a placeholder mechanism, which significantly improved over the previous baseline. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including emojis and emoticons with special placeholder tokens during translation, improves translation accuracy even with noisy texts.</abstract>
      <url hash="dbf7c87f">W19-5365</url>
      <doi>10.18653/v1/W19-5365</doi>
      <bibkey>murakami-etal-2019-ntts</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="66">
      <title><fixed-case>JHU</fixed-case> 2019 Robustness Task System Description</title>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>552–558</pages>
      <abstract>We describe the JHU submissions to the French–English, Japanese–English, and English–Japanese Robustness Task at WMT 2019. Our goal was to evaluate the performance of baseline systems on both the official noisy test set as well as news data, in order to ensure that performance gains in the latter did not come at the expense of general-domain performance. To this end, we built straightforward 6-layer Transformer models and experimented with a handful of variables including subword processing (FR→EN) and a handful of hyperparameters settings (JA↔EN). As expected, our systems performed reasonably.</abstract>
      <url hash="de7f472e">W19-5366</url>
      <doi>10.18653/v1/W19-5366</doi>
      <bibkey>post-duh-2019-jhu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="67">
      <title>Robust Machine Translation with Domain Sensitive Pseudo-Sources: <fixed-case>B</fixed-case>aidu-<fixed-case>OSU</fixed-case> <fixed-case>WMT</fixed-case>19 <fixed-case>MT</fixed-case> Robustness Shared Task System Report</title>
      <author><first>Renjie</first><last>Zheng</last></author>
      <author><first>Hairong</first><last>Liu</last></author>
      <author><first>Mingbo</first><last>Ma</last></author>
      <author><first>Baigong</first><last>Zheng</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <pages>559–564</pages>
      <abstract>This paper describes the machine translation system developed jointly by Baidu Research and Oregon State University for WMT 2019 Machine Translation Robustness Shared Task. Translation of social media is a very challenging problem, since its style is very different from normal parallel corpora (e.g. News) and also include various types of noises. To make it worse, the amount of social media parallel corpora is extremely limited. In this paper, we use a domain sensitive training method which leverages a large amount of parallel data from popular domains together with a little amount of parallel data from social media. Furthermore, we generate a parallel dataset with pseudo noisy source sentences which are back-translated from monolingual data using a model trained by a similar domain sensitive way. In this way, we achieve more than 10 BLEU improvement in both En-Fr and Fr-En translation compared with the baseline methods.</abstract>
      <url hash="01ab376d">W19-5367</url>
      <doi>10.18653/v1/W19-5367</doi>
      <bibkey>zheng-etal-2019-robust</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="68">
      <title>Improving Robustness of Neural Machine Translation with Multi-task Learning</title>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Xiangkai</first><last>Zeng</last></author>
      <author><first>Yingqi</first><last>Zhou</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>565–571</pages>
      <abstract>While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.</abstract>
      <url hash="a78f3918">W19-5368</url>
      <doi>10.18653/v1/W19-5368</doi>
      <bibkey>zhou-etal-2019-improving</bibkey>
      <pwccode url="https://github.com/shuyanzhou/multitask_transformer" additional="false">shuyanzhou/multitask_transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
  </volume>
  <volume id="54" ingest-date="2019-08-15">
    <meta>
      <booktitle>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</booktitle>
      <url hash="99c07870">W19-54</url>
      <editor><first>Ondřej</first><last>Bojar</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>André</first><last>Martins</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <editor><first>Matteo</first><last>Negri</last></editor>
      <editor><first>Aurélie</first><last>Névéol</last></editor>
      <editor><first>Mariana</first><last>Neves</last></editor>
      <editor><first>Matt</first><last>Post</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Karin</first><last>Verspoor</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Florence, Italy</address>
      <month>August</month>
      <year>2019</year>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="490ae4f1">W19-5400</url>
      <bibkey>ws-2019-machine-translation-3</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2019 Shared Tasks on Quality Estimation</title>
      <author><first>Erick</first><last>Fonseca</last></author>
      <author><first>Lisa</first><last>Yankovskaya</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <pages>1–10</pages>
      <abstract>We report the results of the WMT19 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems given just the source text and the hypothesis translations. The task includes estimation at three granularity levels: word, sentence and document. A novel addition is evaluating sentence-level QE against human judgments: in other words, designing MT metrics that do not need a reference translation. This year we include three language pairs, produced solely by neural machine translation systems. Participating teams from eleven institutions submitted a variety of systems to different task variants and language pairs.</abstract>
      <url hash="f2a327bc">W19-5401</url>
      <revision id="1" href="W19-5401v1" hash="7cf54363"/>
      <revision id="2" href="W19-5401v2" hash="f2a327bc">Added the missing results of one of the three tasks.</revision>
      <doi>10.18653/v1/W19-5401</doi>
      <bibkey>fonseca-etal-2019-findings</bibkey>
    </paper>
    <paper id="2">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2019 Shared Task on Automatic Post-Editing</title>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>11–28</pages>
      <abstract>We present the results from the 5th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a “black-box” machine translation system by learning from human corrections. Keeping the same general evaluation setting of the previous four rounds, this year we focused on two language pairs (English-German and English-Russian) and on domain-specific data (In-formation Technology). For both the language directions, MT outputs were produced by neural systems unknown to par-ticipants. Seven teams participated in the English-German task, with a total of 18 submitted runs. The evaluation, which was performed on the same test set used for the 2018 round, shows a slight progress in APE technology: 4 teams achieved better results than last year’s winning system, with improvements up to -0.78 TER and +1.23 BLEU points over the baseline. Two teams participated in theEnglish-Russian task submitting 2 runs each. On this new language direction, characterized by a higher quality of the original translations, the task proved to be particularly challenging. None of the submitted runs improved the very high results of the strong system used to produce the initial translations(16.16 TER, 76.20 BLEU).</abstract>
      <url hash="bcace452">W19-5402</url>
      <doi>10.18653/v1/W19-5402</doi>
      <bibkey>chatterjee-etal-2019-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="3">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2019 Biomedical Translation Shared Task: Evaluation for <fixed-case>MEDLINE</fixed-case> Abstracts and Biomedical Terminologies</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Kevin</first><last>Bretonnel Cohen</last></author>
      <author><first>Cristian</first><last>Grozea</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Madeleine</first><last>Kittner</last></author>
      <author><first>Martin</first><last>Krallinger</last></author>
      <author><first>Nancy</first><last>Mah</last></author>
      <author><first>Aurelie</first><last>Neveol</last></author>
      <author><first>Mariana</first><last>Neves</last></author>
      <author><first>Felipe</first><last>Soares</last></author>
      <author><first>Amy</first><last>Siu</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <author><first>Maika</first><last>Vicente Navarro</last></author>
      <pages>29–53</pages>
      <abstract>In the fourth edition of the WMT Biomedical Translation task, we considered a total of six languages, namely Chinese (zh), English (en), French (fr), German (de), Portuguese (pt), and Spanish (es). We performed an evaluation of automatic translations for a total of 10 language directions, namely, zh/en, en/zh, fr/en, en/fr, de/en, en/de, pt/en, en/pt, es/en, and en/es. We provided training data based on MEDLINE abstracts for eight of the 10 language pairs and test sets for all of them. In addition to that, we offered a new sub-task for the translation of terms in biomedical terminologies for the en/es language direction. Higher BLEU scores (close to 0.5) were obtained for the es/en, en/es and en/pt test sets, as well as for the terminology sub-task. After manual validation of the primary runs, some submissions were judged to be better than the reference translations, for instance, for de/en, en/es and es/en.</abstract>
      <url hash="c0f2f75a">W19-5403</url>
      <doi>10.18653/v1/W19-5403</doi>
      <bibkey>bawden-etal-2019-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/united-nations-parallel-corpus">United Nations Parallel Corpus</pwcdataset>
    </paper>
    <paper id="4">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions</title>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <pages>54–72</pages>
      <abstract>Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2% and 10% of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of Nepali-English and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.</abstract>
      <url hash="0ae3d092">W19-5404</url>
      <doi>10.18653/v1/W19-5404</doi>
      <bibkey>koehn-etal-2019-findings</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>RTM</fixed-case> Stacking Results for Machine Translation Performance Prediction</title>
      <author><first>Ergun</first><last>Biçici</last></author>
      <pages>73–77</pages>
      <abstract>We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine features extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.</abstract>
      <url hash="da2d0ec4">W19-5405</url>
      <doi>10.18653/v1/W19-5405</doi>
      <bibkey>bicici-2019-rtm</bibkey>
    </paper>
    <paper id="6">
      <title>Unbabel’s Participation in the <fixed-case>WMT</fixed-case>19 Translation Quality Estimation Shared Task</title>
      <author><first>Fabio</first><last>Kepler</last></author>
      <author><first>Jonay</first><last>Trénous</last></author>
      <author><first>Marcos</first><last>Treviso</last></author>
      <author><first>Miguel</first><last>Vera</last></author>
      <author><first>António</first><last>Góis</last></author>
      <author><first>M. Amin</first><last>Farajian</last></author>
      <author><first>António V.</first><last>Lopes</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>78–84</pages>
      <abstract>We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: We combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.</abstract>
      <url hash="66e120c9">W19-5406</url>
      <doi>10.18653/v1/W19-5406</doi>
      <bibkey>kepler-etal-2019-unbabels</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>QE</fixed-case> <fixed-case>BERT</fixed-case>: Bilingual <fixed-case>BERT</fixed-case> Using Multi-task Learning for Neural Quality Estimation</title>
      <author><first>Hyun</first><last>Kim</last></author>
      <author><first>Joon-Ho</first><last>Lim</last></author>
      <author><first>Hyun-Ki</first><last>Kim</last></author>
      <author><first>Seung-Hoon</first><last>Na</last></author>
      <pages>85–89</pages>
      <abstract>For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses multi-task learning for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.</abstract>
      <url hash="d279ad25">W19-5407</url>
      <doi>10.18653/v1/W19-5407</doi>
      <bibkey>kim-etal-2019-qe</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>MIPT</fixed-case> System for World-Level Quality Estimation</title>
      <author><first>Mikhail</first><last>Mosyagin</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <pages>90–94</pages>
      <abstract>We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a model similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as BERT, baseline features for the task and transformer encoders. We evaluate the performance of our models on the English-German dataset for the corresponding shared task.</abstract>
      <url hash="c1bbce87">W19-5408</url>
      <doi>10.18653/v1/W19-5408</doi>
      <bibkey>mosyagin-logacheva-2019-mipt</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>NJU</fixed-case> Submissions for the <fixed-case>WMT</fixed-case>19 Quality Estimation Shared Task</title>
      <author><first>Hou</first><last>Qi</last></author>
      <pages>95–100</pages>
      <abstract>In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a feature extractor and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.</abstract>
      <url hash="3eade5bd">W19-5409</url>
      <doi>10.18653/v1/W19-5409</doi>
      <bibkey>qi-2019-nju</bibkey>
    </paper>
    <paper id="10">
      <title>Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings</title>
      <author><first>Elizaveta</first><last>Yankovskaya</last></author>
      <author><first>Andre</first><last>Tättar</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>101–105</pages>
      <abstract>We propose the use of pre-trained embeddings as features of a regression model for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).</abstract>
      <url hash="67a978b5">W19-5410</url>
      <doi>10.18653/v1/W19-5410</doi>
      <bibkey>yankovskaya-etal-2019-quality</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>SOURCE</fixed-case>: <fixed-case>SOUR</fixed-case>ce-Conditional Elmo-style Model for Machine Translation Quality Estimation</title>
      <author><first>Junpei</first><last>Zhou</last></author>
      <author><first>Zhisong</first><last>Zhang</last></author>
      <author><first>Zecong</first><last>Hu</last></author>
      <pages>106–111</pages>
      <abstract>Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of post-editing, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation-like strategy. The strategy is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for QE. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.</abstract>
      <url hash="32a12337">W19-5411</url>
      <doi>10.18653/v1/W19-5411</doi>
      <bibkey>zhou-etal-2019-source</bibkey>
    </paper>
    <paper id="12">
      <title>Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of Decoder</title>
      <author><first>WonKee</first><last>Lee</last></author>
      <author><first>Jaehun</first><last>Shin</last></author>
      <author><first>Jong-Hyeok</first><last>Lee</last></author>
      <pages>112–117</pages>
      <abstract>This paper describes POSTECH’s submission to the WMT 2019 shared task on Automatic Post-Editing (APE). In this paper, we propose a new multi-source APE model by extending Transformer. The main contributions of our study are that we 1) reconstruct the encoder to generate a joint representation of translation (mt) and its src context, in addition to the conventional src encoding and 2) suggest two types of multi-source attention layers to compute attention between two outputs of the encoder and the decoder state in the decoder. Furthermore, we train our model by applying various teacher-forcing ratios to alleviate exposure bias. Finally, we adopt the ensemble technique across variations of our model. Experiments on the WMT19 English-German APE data set show improvements in terms of both TER and BLEU scores over the baseline. Our primary submission achieves -0.73 in TER and +1.49 in BLEU compare to the baseline.</abstract>
      <url hash="9c88edc6">W19-5412</url>
      <doi>10.18653/v1/W19-5412</doi>
      <bibkey>lee-etal-2019-transformer</bibkey>
    </paper>
    <paper id="13">
      <title>Unbabel’s Submission to the <fixed-case>WMT</fixed-case>2019 <fixed-case>APE</fixed-case> Shared Task: <fixed-case>BERT</fixed-case>-Based Encoder-Decoder for Automatic Post-Editing</title>
      <author><first>António V.</first><last>Lopes</last></author>
      <author><first>M. Amin</first><last>Farajian</last></author>
      <author><first>Gonçalo M.</first><last>Correia</last></author>
      <author><first>Jonay</first><last>Trénous</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>118–123</pages>
      <abstract>This paper describes Unbabel’s submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pre-trained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder (BED) model in which a single pretrained BERT encoder receives both the source src and machine translation mt strings. Furthermore, we explore a conservativeness factor to constrain the APE system to perform fewer edits. As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservativeness penalty improves significantly the translations of a strong NMT system by -0.78 and +1.23 in terms of TER and BLEU, respectively. Finally, our submission achieves a new state-of-the-art, ex-aequo, in English-German APE of NMT.</abstract>
      <url hash="90fa3e92">W19-5413</url>
      <doi>10.18653/v1/W19-5413</doi>
      <bibkey>lopes-etal-2019-unbabels</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>USAAR</fixed-case>-<fixed-case>DFKI</fixed-case> – The Transference Architecture for <fixed-case>E</fixed-case>nglish–<fixed-case>G</fixed-case>erman Automatic Post-Editing</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Nico</first><last>Herbig</last></author>
      <author><first>Antonio</first><last>Krüger</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>124–131</pages>
      <abstract>In this paper we present an English–German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019. Our transference model is based on a multi-encoder transformer architecture. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a transformer decoder block, but without masking, for self-attention on mt, which effectively acts as second encoder combining src –&gt; mt, and (iii) feeds this representation into a final decoder block generating pe. Our model improves over the raw black-box neural machine translation system by 0.9 and 1.0 absolute BLEU points on the WMT 2019 APE development and test set. Our submission ranked 3rd, however compared to the two top systems, performance differences are not statistically significant.</abstract>
      <url hash="5dd77790">W19-5414</url>
      <doi>10.18653/v1/W19-5414</doi>
      <bibkey>pal-etal-2019-usaar</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="15">
      <title><fixed-case>APE</fixed-case> through Neural and Statistical <fixed-case>MT</fixed-case> with Augmented Data. <fixed-case>ADAPT</fixed-case>/<fixed-case>DCU</fixed-case> Submission to the <fixed-case>WMT</fixed-case> 2019 <fixed-case>APE</fixed-case> Shared Task</title>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Joachim</first><last>Wagner</last></author>
      <author><first>Félix</first><last>do Carmo</last></author>
      <pages>132–138</pages>
      <abstract>Automatic post-editing (APE) can be reduced to a machine translation (MT) task, where the source is the output of a specific MT system and the target is its post-edited variant. However, this approach does not consider context information that can be found in the original source of the MT system. Thus a better approach is to employ multi-source MT, where two input sequences are considered – the one being the original source and the other being the MT output. Extra context information can be introduced in the form of extra tokens that identify certain global property of a group of segments, added as a prefix or a suffix to each segment. Successfully applied in domain adaptation of MT as well as on APE, this technique deserves further attention. In this work we investigate multi-source neural APE (or NPE) systems with training data which has been augmented with two types of extra context tokens. We experiment with authentic and synthetic data provided by WMT 2019 and submit our results to the APE shared task. We also experiment with using statistical machine translation (SMT) methods for APE. While our systems score bellow the baseline, we consider this work a step towards understanding the added value of extra context in the case of APE.</abstract>
      <url hash="861f599a">W19-5415</url>
      <doi>10.18653/v1/W19-5415</doi>
      <bibkey>shterionov-etal-2019-ape</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="16">
      <title>Effort-Aware Neural Automatic Post-Editing</title>
      <author><first>Amirhossein</first><last>Tebbifakhr</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>139–144</pages>
      <abstract>For this round of the WMT 2019 APE shared task, our submission focuses on addressing the “over-correction” problem in APE. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the system about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of post-editing. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.</abstract>
      <url hash="1414fe85">W19-5416</url>
      <doi>10.18653/v1/W19-5416</doi>
      <bibkey>tebbifakhr-etal-2019-effort</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>U</fixed-case>d<fixed-case>S</fixed-case> Submission for the <fixed-case>WMT</fixed-case> 19 Automatic Post-Editing Task</title>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Qiuhui</first><last>Liu</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>145–150</pages>
      <abstract>In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder.</abstract>
      <url hash="5c155e69">W19-5417</url>
      <doi>10.18653/v1/W19-5417</doi>
      <attachment type="poster" hash="cd5d97ef">W19-5417.Poster.pdf</attachment>
      <bibkey>xu-etal-2019-uds</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="18">
      <title>Terminology-Aware Segmentation and Domain Feature for the <fixed-case>WMT</fixed-case>19 Biomedical Translation Task</title>
      <author><first>Casimiro Pio</first><last>Carrino</last></author>
      <author><first>Bardia</first><last>Rafieian</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <pages>151–155</pages>
      <abstract>In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the terms information at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively.</abstract>
      <url hash="7e798e9a">W19-5418</url>
      <doi>10.18653/v1/W19-5418</doi>
      <bibkey>carrino-etal-2019-terminology</bibkey>
    </paper>
    <paper id="19">
      <title>Exploring Transfer Learning and Domain Data Selection for the Biomedical Translation</title>
      <author><first>Noor-e-</first><last>Hira</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Kiran</first><last>Kiani</last></author>
      <author><first>Ammara</first><last>Zafar</last></author>
      <author><first>Raheel</first><last>Nawaz</last></author>
      <pages>156–163</pages>
      <abstract>Transfer Learning and Selective data training are two of the many approaches being extensively investigated to improve the quality of Neural Machine Translation systems. This paper presents a series of experiments by applying transfer learning and selective data training for participation in the Bio-medical shared task of WMT19. We have used Information Retrieval to selectively choose related sentences from out-of-domain data and used them as additional training data using transfer learning. We also report the effect of tokenization on translation model performance.</abstract>
      <url hash="d51377b7">W19-5419</url>
      <doi>10.18653/v1/W19-5419</doi>
      <bibkey>hira-etal-2019-exploring</bibkey>
    </paper>
    <paper id="20">
      <title>Huawei’s <fixed-case>NMT</fixed-case> Systems for the <fixed-case>WMT</fixed-case> 2019 Biomedical Translation Task</title>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Jianfeng</first><last>Liu</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>164–168</pages>
      <abstract>This paper describes Huawei’s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering English–Chinese, English–French and English–German language pairs. Our submitted systems achieve the best BLEU scores on English–French and English–German language pairs according to the official evaluation results. In the English–Chinese translation task, our systems are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated models developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.</abstract>
      <url hash="92090ae6">W19-5420</url>
      <doi>10.18653/v1/W19-5420</doi>
      <bibkey>peng-etal-2019-huaweis</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>UCAM</fixed-case> Biomedical Translation at <fixed-case>WMT</fixed-case>19: Transfer Learning Multi-domain Ensembles</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>169–174</pages>
      <abstract>The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.</abstract>
      <url hash="40bf86c1">W19-5421</url>
      <doi>10.18653/v1/W19-5421</doi>
      <bibkey>saunders-etal-2019-ucam</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>BSC</fixed-case> Participation in the <fixed-case>WMT</fixed-case> Translation of Biomedical Abstracts</title>
      <author><first>Felipe</first><last>Soares</last></author>
      <author><first>Martin</first><last>Krallinger</last></author>
      <pages>175–178</pages>
      <abstract>This paper describes the machine translation systems developed by the Barcelona Supercomputing (BSC) team for the biomedical translation shared task of WMT19. Our system is based on Neural Machine Translation unsing the OpenNMT-py toolkit and Transformer architecture. We participated in four translation directions for the English/Spanish and English/Portuguese language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from UMLS.</abstract>
      <url hash="051821bc">W19-5422</url>
      <doi>10.18653/v1/W19-5422</doi>
      <bibkey>soares-krallinger-2019-bsc</bibkey>
    </paper>
    <paper id="23">
      <title>The <fixed-case>MLLP</fixed-case>-<fixed-case>UPV</fixed-case> <fixed-case>S</fixed-case>panish-<fixed-case>P</fixed-case>ortuguese and <fixed-case>P</fixed-case>ortuguese-<fixed-case>S</fixed-case>panish Machine Translation Systems for <fixed-case>WMT</fixed-case>19 Similar Language Translation Task</title>
      <author><first>Pau</first><last>Baquero-Arnal</last></author>
      <author><first>Javier</first><last>Iranzo-Sánchez</last></author>
      <author><first>Jorge</first><last>Civera</last></author>
      <author><first>Alfons</first><last>Juan</last></author>
      <pages>179–184</pages>
      <abstract>This paper describes the participation of the MLLP research group of the Universitat Politècnica de València in the WMT 2019 Similar Language Translation Shared Task. We have submitted systems for the Portuguese ↔ Spanish language pair, in both directions. We have submitted systems based on the Transformer architecture as well as an in development novel architecture which we have called 2D alternating RNN. We have carried out domain adaptation through fine-tuning.</abstract>
      <url hash="bbd6d7cf">W19-5423</url>
      <doi>10.18653/v1/W19-5423</doi>
      <bibkey>baquero-arnal-etal-2019-mllp</bibkey>
    </paper>
    <paper id="24">
      <title>The <fixed-case>TALP</fixed-case>-<fixed-case>UPC</fixed-case> System for the <fixed-case>WMT</fixed-case> Similar Language Task: Statistical vs Neural Machine Translation</title>
      <author><first>Magdalena</first><last>Biesialska</last></author>
      <author><first>Lluis</first><last>Guardia</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>185–191</pages>
      <abstract>Although the problem of similar language translation has been an area of research interest for many years, yet it is still far from being solved. In this paper, we study the performance of two popular approaches: statistical and neural. We conclude that both methods yield similar results; however, the performance varies depending on the language pair. While the statistical approach outperforms the neural one by a difference of 6 BLEU points for the Spanish-Portuguese language pair, the proposed neural model surpasses the statistical one by a difference of 2 BLEU points for Czech-Polish. In the former case, the language similarity (based on perplexity) is much higher than in the latter case. Additionally, we report negative results for the system combination with back-translation. Our TALP-UPC system submission won 1st place for Czech-&gt;Polish and 2nd place for Spanish-&gt;Portuguese in the official evaluation of the 1st WMT Similar Language Translation task.</abstract>
      <url hash="1eb6be08">W19-5424</url>
      <doi>10.18653/v1/W19-5424</doi>
      <bibkey>biesialska-etal-2019-talp</bibkey>
    </paper>
    <paper id="25">
      <title>Machine Translation from an Intercomprehension Perspective</title>
      <author><first>Yu</first><last>Chen</last></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>192–196</pages>
      <abstract>Within the first shared task on machine translation between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the mutual intelligibility of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The translation results are evaluated using BLEU. On this metric, none of our proposals could outperform the baselines on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future.</abstract>
      <url hash="0574e1df">W19-5425</url>
      <doi>10.18653/v1/W19-5425</doi>
      <bibkey>chen-avgustinova-2019-machine</bibkey>
    </paper>
    <paper id="26">
      <title>Utilizing Monolingual Data in <fixed-case>NMT</fixed-case> for Similar Languages: Submission to Similar Language Translation Task</title>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>197–201</pages>
      <abstract>This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi -&gt; Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.</abstract>
      <url hash="8dedfdd7">W19-5426</url>
      <doi>10.18653/v1/W19-5426</doi>
      <bibkey>khatri-bhattacharyya-2019-utilizing</bibkey>
    </paper>
    <paper id="27">
      <title>Neural Machine Translation: <fixed-case>H</fixed-case>indi-<fixed-case>N</fixed-case>epali</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>202–207</pages>
      <abstract>With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi ⇔ Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved BLEU score 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.</abstract>
      <url hash="d3f990f2">W19-5427</url>
      <doi>10.18653/v1/W19-5427</doi>
      <bibkey>laskar-etal-2019-neural</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>NICT</fixed-case>’s Machine Translation Systems for the <fixed-case>WMT</fixed-case>19 Similar Language Translation Task</title>
      <author><first>Benjamin</first><last>Marie</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <pages>208–212</pages>
      <abstract>This paper presents the NICT’s participation in the WMT19 shared Similar Language Translation Task. We participated in the Spanish-Portuguese task. For both translation directions, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems with the Transformer architecture were trained on the provided parallel data enlarged with a large quantity of back-translated monolingual data. Our primary submission to the task is the result of a simple combination of our SMT and NMT systems. According to BLEU, our systems were ranked second and third respectively for the Portuguese-to-Spanish and Spanish-to-Portuguese translation directions. For contrastive experiments, we also submitted outputs generated with an unsupervised SMT system.</abstract>
      <url hash="d0a48252">W19-5428</url>
      <doi>10.18653/v1/W19-5428</doi>
      <bibkey>marie-etal-2019-nicts-machine</bibkey>
    </paper>
    <paper id="29">
      <title>Panlingua-<fixed-case>KMI</fixed-case> <fixed-case>MT</fixed-case> System for Similar Language Translation Task at <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Akanksha</first><last>Bansal</last></author>
      <author><first>Priya</first><last>Rani</last></author>
      <pages>213–218</pages>
      <abstract>The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi ↔ Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for Nepali-Hindi, 1 PBSMT for Hindi-Nepali and 2 NMT systems were developed for Nepali↔Hindi. The results show that PBSMT could be an effective method for developing MT systems for closely-related languages. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.</abstract>
      <url hash="134e38fe">W19-5429</url>
      <doi>10.18653/v1/W19-5429</doi>
      <bibkey>ojha-etal-2019-panlingua</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>UDS</fixed-case>–<fixed-case>DFKI</fixed-case> Submission to the <fixed-case>WMT</fixed-case>2019 <fixed-case>C</fixed-case>zech–<fixed-case>P</fixed-case>olish Similar Language Translation Shared Task</title>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>219–223</pages>
      <abstract>In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our system in translating from Czech to Polish and comment on the impact of out-of-domain test data in the performance of our system. UDS-DFKI achieved competitive performance ranking second among ten teams in Czech to Polish translation.</abstract>
      <url hash="de5781b2">W19-5430</url>
      <doi>10.18653/v1/W19-5430</doi>
      <bibkey>pal-etal-2019-uds</bibkey>
    </paper>
    <paper id="31">
      <title>Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation</title>
      <author><first>Michael</first><last>Przystupa</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>224–235</pages>
      <abstract>We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of neural machine translation on three low-resource, similar language pairs: Spanish – Portuguese, Czech – Polish, and Hindi – Nepali. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish – Portuguese and Czech – Polish translation, whereas LSTMs with global attention worked best on Hindi – Nepali translation.</abstract>
      <url hash="30cbb075">W19-5431</url>
      <doi>10.18653/v1/W19-5431</doi>
      <bibkey>przystupa-abdul-mageed-2019-neural</bibkey>
    </paper>
    <paper id="32">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki Submissions to the <fixed-case>WMT</fixed-case>19 Similar Language Translation Task</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <pages>236–244</pages>
      <abstract>This paper describes the University of Helsinki Language Technology group’s participation in the WMT 2019 similar language translation task. We trained neural machine translation models for the language pairs Czech &lt;-&gt; Polish and Spanish &lt;-&gt; Portuguese. Our experiments focused on different subword segmentation methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmentation and unsupervised segmentation methods for which the data from different languages were simply concatenated. We did not observe major benefits from cognate-aware segmentation methods, but further research may be needed to explore larger parts of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding.</abstract>
      <url hash="7b56c6eb">W19-5432</url>
      <doi>10.18653/v1/W19-5432</doi>
      <bibkey>scherrer-etal-2019-university</bibkey>
    </paper>
    <paper id="33">
      <title>Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data</title>
      <author><first>Amittai</first><last>Axelrod</last></author>
      <author><first>Anish</first><last>Kumar</last></author>
      <author><first>Steve</first><last>Sloto</last></author>
      <pages>245–251</pages>
      <abstract>We introduce a purely monolingual approach to filtering for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.</abstract>
      <url hash="864a24be">W19-5433</url>
      <doi>10.18653/v1/W19-5433</doi>
      <attachment type="poster" hash="4486ee99">W19-5433.Poster.pdf</attachment>
      <bibkey>axelrod-etal-2019-dual</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>NRC</fixed-case> Parallel Corpus Filtering System for <fixed-case>WMT</fixed-case> 2019</title>
      <author><first>Gabriel</first><last>Bernier-Colborne</last></author>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <pages>252–260</pages>
      <abstract>We describe the National Research Council Canada team’s submissions to the parallel corpus filtering task at the Fourth Conference on Machine Translation.</abstract>
      <url hash="39376e66">W19-5434</url>
      <doi>10.18653/v1/W19-5434</doi>
      <bibkey>bernier-colborne-lo-2019-nrc</bibkey>
    </paper>
    <paper id="35">
      <title>Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings</title>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Yuqing</first><last>Tang</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>261–266</pages>
      <abstract>In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios.</abstract>
      <url hash="0054b34c">W19-5435</url>
      <doi>10.18653/v1/W19-5435</doi>
      <bibkey>chaudhary-etal-2019-low</bibkey>
    </paper>
    <paper id="36">
      <title>Quality and Coverage: The <fixed-case>AFRL</fixed-case> Submission to the <fixed-case>WMT</fixed-case>19 Parallel Corpus Filtering for Low-Resource Conditions Task</title>
      <author><first>Grant</first><last>Erdmann</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <pages>267–270</pages>
      <abstract>The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of Nepali-English and Sinhala-English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over baseline and the relative benefits of different options.</abstract>
      <url hash="dcb74cf8">W19-5436</url>
      <doi>10.18653/v1/W19-5436</doi>
      <bibkey>erdmann-gwinnup-2019-quality</bibkey>
    </paper>
    <paper id="37">
      <title>Webinterpret Submission to the <fixed-case>WMT</fixed-case>2019 Shared Task on Parallel Corpus Filtering</title>
      <author><first>Jesús</first><last>González-Rubio</last></author>
      <pages>271–276</pages>
      <abstract>This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.</abstract>
      <url hash="ac9c7227">W19-5437</url>
      <doi>10.18653/v1/W19-5437</doi>
      <bibkey>gonzalez-rubio-2019-webinterpret</bibkey>
    </paper>
    <paper id="38">
      <title>Noisy Parallel Corpus Filtering through Projected Word Embeddings</title>
      <author><first>Murathan</first><last>Kurfalı</last></author>
      <author><first>Robert</first><last>Östling</last></author>
      <pages>277–281</pages>
      <abstract>We present a very simple method for parallel text cleaning of low-resource languages, based on projection of word embeddings trained on large monolingual corpora in high-resource languages. In spite of its simplicity, we approach the strong baseline system in the downstream machine translation evaluation.</abstract>
      <url hash="54f5f301">W19-5438</url>
      <doi>10.18653/v1/W19-5438</doi>
      <bibkey>kurfali-ostling-2019-noisy</bibkey>
    </paper>
    <paper id="39">
      <title>Filtering of Noisy Parallel Corpora Based on Hypothesis Generation</title>
      <author><first>Zuzanna</first><last>Parcheta</last></author>
      <author><first>Germán</first><last>Sanchis-Trilles</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <pages>282–288</pages>
      <abstract>The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs: Nepali–English and Sinhala–English using provided parallel corpora. We select the training subset for three language pairs (Nepali, Sinhala and Hindi to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These heuristics are based on sentence length, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed filtering system is domain independent and all experiments are conducted using neural machine translation.</abstract>
      <url hash="41542ede">W19-5439</url>
      <doi>10.18653/v1/W19-5439</doi>
      <bibkey>parcheta-etal-2019-filtering</bibkey>
    </paper>
    <paper id="40">
      <title>Parallel Corpus Filtering Based on Fuzzy String Matching</title>
      <author><first>Sukanta</first><last>Sen</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>289–293</pages>
      <abstract>In this paper, we describe the IIT Patna’s submission to WMT 2019 shared task on parallel corpus filtering. This shared task asks the participants to develop methods for scoring each parallel sentence from a given noisy parallel corpus. Quality of the scoring method is judged based on the quality of SMT and NMT systems trained on smaller set of high-quality parallel sentences sub-sampled from the original noisy corpus. This task has two language pairs. We submit for both the Nepali-English and Sinhala-English language pairs. We define fuzzy string matching score between English and the translated (into English) source based on Levenshtein distance. Based on the scores, we sub-sample two sets (having 1 million and 5 millions English tokens) of parallel sentences from each parallel corpus, and train SMT systems for development purpose only. The organizers publish the official evaluation using both SMT and NMT on the final official test set. Total 10 teams participated in the shared task and according the official evaluation, our scoring method obtains 2nd position in the team ranking for 1-million NepaliEnglish NMT and 5-million Sinhala-English NMT categories.</abstract>
      <url hash="8c908ac3">W19-5440</url>
      <doi>10.18653/v1/W19-5440</doi>
      <bibkey>sen-etal-2019-parallel</bibkey>
    </paper>
    <paper id="41">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>H</fixed-case>elsinki Submission to the <fixed-case>WMT</fixed-case>19 Parallel Corpus Filtering Task</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Umut</first><last>Sulubacak</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>294–300</pages>
      <abstract>This paper describes the University of Helsinki Language Technology group’s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of filters to remove the ‘bad’ quality sentences. Then, we produced scores for each sentence by weighting these features with a classification model. This methodology allowed us to build a simple and reliable system that is easily adaptable to other language pairs.</abstract>
      <url hash="ec937e25">W19-5441</url>
      <doi>10.18653/v1/W19-5441</doi>
      <bibkey>vazquez-etal-2019-university</bibkey>
    </paper>
  </volume>
  <volume id="55">
    <meta>
      <booktitle>Proceedings of the First Workshop on Financial Technology and Natural Language Processing</booktitle>
      <url hash="98fd0a78">W19-55</url>
      <editor><first>Chung-Chi</first><last>Chen</last></editor>
      <editor><first>Hen-Hsen</first><last>Huang</last></editor>
      <editor><first>Hiroya</first><last>Takamura</last></editor>
      <editor><first>Hsin-Hsi</first><last>Chen</last></editor>
      <address>Macao, China</address>
      <month>August</month>
      <year>2019</year>
      <venue>finnlp</venue>
    </meta>
    <frontmatter>
      <url hash="fdb6847c">W19-5500</url>
      <bibkey>ws-2019-financial</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Business Taxonomy Construction Using Concept-Level Hierarchical Clustering</title>
      <author><first>Haodong</first><last>Bai</last></author>
      <author><first>Frank</first><last>Xing</last></author>
      <author><first>Erik</first><last>Cambria</last></author>
      <author><first>Win-Bin</first><last>Huang</last></author>
      <pages>1–7</pages>
      <url hash="022b1732">W19-5501</url>
      <bibkey>bai-etal-2019-business</bibkey>
      <pwccode url="https://github.com/SenticNet/neeq-annual-reports" additional="false">SenticNet/neeq-annual-reports</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/neeq-annual-reports">NEEQ Annual Reports</pwcdataset>
    </paper>
    <paper id="2">
      <title>Towards Disambiguating Contracts for their Successful Execution - A Case from Finance Domain</title>
      <author><first>Preethu Rose</first><last>Anish</last></author>
      <author><first>Abhishek</first><last>Sainani</last></author>
      <author><first>Nitin</first><last>Ramrakhiyani</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Girish K</first><last>Palshikar</last></author>
      <author><first>Smita</first><last>Ghaisas</last></author>
      <pages>8–13</pages>
      <url hash="d05179e4">W19-5502</url>
      <bibkey>anish-etal-2019-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Rationale Classification for Educational Trading Platforms</title>
      <author><first>Annie</first><last>Ying</last></author>
      <author><first>Pablo</first><last>Duboue</last></author>
      <pages>14–20</pages>
      <url hash="94bb56a4">W19-5503</url>
      <bibkey>ying-duboue-2019-rationale</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>C</fixed-case>o<fixed-case>F</fixed-case>i<fixed-case>F</fixed-case>: A Corpus of Financial Reports in <fixed-case>F</fixed-case>rench Language</title>
      <author><first>Tobias</first><last>Daudert</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <pages>21–26</pages>
      <url hash="bf6222f3">W19-5504</url>
      <bibkey>daudert-ahmadi-2019-cofif</bibkey>
      <pwccode url="https://github.com/CoFiF/Corpus" additional="false">CoFiF/Corpus</pwccode>
    </paper>
    <paper id="5">
      <title>Step-wise Refinement Classification Approach for Enterprise Legal Litigation</title>
      <author><first>Ying</first><last>Mao</last></author>
      <author><first>Xian</first><last>Wang</last></author>
      <author><first>Jianbo</first><last>Tang</last></author>
      <author><first>Changliang</first><last>Li</last></author>
      <pages>27–33</pages>
      <url hash="652411a5">W19-5505</url>
      <bibkey>mao-etal-2019-step</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>C</fixed-case>o<fixed-case>SACT</fixed-case>: A Collaborative Tool for Fine-Grained Sentiment Annotation and Consolidation of Text</title>
      <author><first>Tobias</first><last>Daudert</last></author>
      <author><first>Manel</first><last>Zarrouk</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>34–39</pages>
      <url hash="ed1a1c0a">W19-5506</url>
      <bibkey>daudert-etal-2019-cosact</bibkey>
    </paper>
    <paper id="7">
      <title>Financial Text Data Analytics Framework for Business Confidence Indices and Inter-Industry Relations</title>
      <author><first>Hiroki</first><last>Sakaji</last></author>
      <author><first>Ryota</first><last>Kuramoto</last></author>
      <author><first>Hiroyasu</first><last>Matsushima</last></author>
      <author><first>Kiyoshi</first><last>Izumi</last></author>
      <author><first>Takashi</first><last>Shimada</last></author>
      <author><first>Keita</first><last>Sunakawa</last></author>
      <pages>40–46</pages>
      <url hash="87a24853">W19-5507</url>
      <bibkey>sakaji-etal-2019-financial</bibkey>
    </paper>
    <paper id="8">
      <title>Learning to Learn Sales Prediction with Social Media Sentiment</title>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Cong</first><last>Gao</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>47–53</pages>
      <url hash="d8d53e81">W19-5508</url>
      <bibkey>lin-etal-2019-learning</bibkey>
    </paper>
    <paper id="9">
      <title>Leveraging <fixed-case>BERT</fixed-case> to Improve the <fixed-case>FEARS</fixed-case> Index for Stock Forecasting</title>
      <author><first>Linyi</first><last>Yang</last></author>
      <author><first>Ruihai</first><last>Dong</last></author>
      <author><first>Tin Lok James</first><last>Ng</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <pages>54–60</pages>
      <url hash="2276a984">W19-5509</url>
      <bibkey>yang-etal-2019-leveraging</bibkey>
    </paper>
    <paper id="10">
      <title>Economic Causal-Chain Search using Text Mining Technology</title>
      <author><first>Kiyoshi</first><last>Izumi</last></author>
      <author><first>Hiroki</first><last>Sakaji</last></author>
      <pages>61–65</pages>
      <url hash="f893676b">W19-5510</url>
      <bibkey>izumi-sakaji-2019-economic</bibkey>
    </paper>
    <paper id="11">
      <title>Transformer-Based Capsule Network For Stock Movement Prediction</title>
      <author><first>Jintao</first><last>Liu</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <author><first>Xikai</first><last>Liu</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <author><first>Yuqi</first><last>Ren</last></author>
      <author><first>Yufeng</first><last>Diao</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <pages>66–73</pages>
      <url hash="83866fe4">W19-5511</url>
      <bibkey>liu-etal-2019-transformer</bibkey>
    </paper>
    <paper id="12">
      <title>The <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case>-2019 Shared Task: Sentence Boundary Detection in <fixed-case>PDF</fixed-case> Noisy Text in the Financial Domain</title>
      <author><first>Abderrahim Ait</first><last>Azzi</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Sira</first><last>Ferradans</last></author>
      <pages>74–80</pages>
      <url hash="571da245">W19-5512</url>
      <bibkey>azzi-etal-2019-finsbd</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>AIG</fixed-case> <fixed-case>I</fixed-case>nvestments.<fixed-case>AI</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> Task: Sentence Boundary Detection through Sequence Labelling and <fixed-case>BERT</fixed-case> Fine-tuning</title>
      <author><first>Jinhua</first><last>Du</last></author>
      <author><first>Yan</first><last>Huang</last></author>
      <author><first>Karo</first><last>Moilanen</last></author>
      <pages>81–87</pages>
      <url hash="3266ea24">W19-5513</url>
      <bibkey>du-etal-2019-aig</bibkey>
    </paper>
    <paper id="14">
      <title>aiai at <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> task: Sentence Boundary Detection in Noisy Texts From Financial Documents Using Deep Attention Model</title>
      <author><first>Ke</first><last>Tian</last></author>
      <author><first>Zi Jun</first><last>Peng</last></author>
      <pages>88–92</pages>
      <url hash="e7377acb">W19-5514</url>
      <bibkey>tian-peng-2019-aiai</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>P</fixed-case>luto: A Deep Learning Based Watchdog for Anti Money Laundering</title>
      <author><first>Hao-Yuan</first><last>Chen</last></author>
      <author><first>Shang-Xuan</first><last>Zou</last></author>
      <author><first>Cheng-Lung</first><last>Sung</last></author>
      <pages>93–95</pages>
      <url hash="8f4ba7bf">W19-5515</url>
      <bibkey>chen-etal-2019-pluto</bibkey>
    </paper>
    <paper id="16">
      <title>From Creditworthiness to Trustworthiness with Alternative <fixed-case>NLP</fixed-case>/<fixed-case>NLU</fixed-case> Approaches</title>
      <author><first>Charles</first><last>Crouspeyre</last></author>
      <author><first>Eleonore</first><last>Alesi</last></author>
      <author><first>Karine</first><last>Lespinasse</last></author>
      <pages>96–98</pages>
      <url hash="ccbdcf31">W19-5516</url>
      <bibkey>crouspeyre-etal-2019-creditworthiness</bibkey>
    </paper>
    <paper id="17">
      <title>On a Chatbot Conducting a Virtual Dialogue in Financial Domain</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <pages>99–101</pages>
      <url hash="2e6fd579">W19-5517</url>
      <bibkey>galitsky-ilvovsky-2019-chatbot</bibkey>
    </paper>
    <paper id="18">
      <title>mhirano at the <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> Task: Pointwise Prediction Based on Multi-layer Perceptron for Sentence Boundary Detection</title>
      <author><first>Masanori</first><last>Hirano</last></author>
      <author><first>Hiroki</first><last>Sakaji</last></author>
      <author><first>Kiyoshi</first><last>Izumi</last></author>
      <author><first>Hiroyasu</first><last>Matsushima</last></author>
      <pages>102–107</pages>
      <url hash="ce0506eb">W19-5518</url>
      <bibkey>hirano-etal-2019-mhirano</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>NUIG</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> Task: Sentence Boundary Detection for Noisy Financial <fixed-case>PDF</fixed-case>s in <fixed-case>E</fixed-case>nglish and <fixed-case>F</fixed-case>rench</title>
      <author><first>Tobias</first><last>Daudert</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <pages>108–114</pages>
      <url hash="917bb540">W19-5519</url>
      <bibkey>daudert-ahmadi-2019-nuig</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>HITS</fixed-case>-<fixed-case>SBD</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> Task: Machine Learning vs. Rule-based Sentence Boundary Detection</title>
      <author><first>Mehwish</first><last>Fatima</last></author>
      <author><first>Mark-Christoph</first><last>Mueller</last></author>
      <pages>115–121</pages>
      <url hash="7b94237d">W19-5520</url>
      <bibkey>fatima-mueller-2019-hits</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>P</fixed-case>oly<fixed-case>U</fixed-case>_<fixed-case>CBS</fixed-case>-<fixed-case>CFA</fixed-case> at the <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> Task: Sentence Boundary Detection of Financial Data with Domain Knowledge Enhancement and Bilingual Training</title>
      <author><first>Mingyu</first><last>Wan</last></author>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Natalia</first><last>Klyueva</last></author>
      <author><first>Kathleen</first><last>Ahrens</last></author>
      <author><first>Bin</first><last>Miao</last></author>
      <author><first>David</first><last>Broadstock</last></author>
      <author><first>Jian</first><last>Kang</last></author>
      <author><first>Amos</first><last>Yung</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>122–129</pages>
      <url hash="c9e4857a">W19-5521</url>
      <bibkey>wan-etal-2019-polyu</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>AI</fixed-case>_<fixed-case>B</fixed-case>lues at <fixed-case>F</fixed-case>in<fixed-case>SBD</fixed-case> Shared Task: <fixed-case>CRF</fixed-case>-based Sentence Boundary Detection in <fixed-case>PDF</fixed-case> Noisy Text in the Financial Domain</title>
      <author><first>Ditty</first><last>Mathew</last></author>
      <author><first>Chinnappa</first><last>Guggilla</last></author>
      <pages>130–136</pages>
      <url hash="1b11df9d">W19-5522</url>
      <bibkey>mathew-guggilla-2019-ai</bibkey>
    </paper>
  </volume>
  <volume id="56">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Arabic Corpus Linguistics</booktitle>
      <url hash="17f1bbec">W19-56</url>
      <editor><first>Mahmoud</first><last>El-Haj</last></editor>
      <editor><first>Paul</first><last>Rayson</last></editor>
      <editor><first>Eric</first><last>Atwell</last></editor>
      <editor><first>Lama</first><last>Alsudias</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Cardiff, United Kingdom</address>
      <month>July</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="416931eb">W19-5600</url>
      <bibkey>ws-2019-arabic-corpus</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Computer Stylometric Comparison of Writings by Qassim Amin and Mohammed Abdu on Women’s Rights</title>
      <author><first>Ahmed</first><last>Omer</last></author>
      <author><first>Michael</first><last>Oakes</last></author>
      <pages>1–6</pages>
      <url hash="fcbd33ef">W19-5601</url>
      <bibkey>omer-oakes-2019-computer</bibkey>
    </paper>
    <paper id="2">
      <title>Compiling and Analysing a Corpus of Transcribed Spoken Gulf Pidgin <fixed-case>A</fixed-case>rabic Based on Length of Stay in the Gulf</title>
      <author><first>Najah</first><last>Albaqawi</last></author>
      <author><first>Michael</first><last>Oakes</last></author>
      <pages>7–15</pages>
      <url hash="7c4a3bf5">W19-5602</url>
      <bibkey>albaqawi-oakes-2019-compiling</bibkey>
    </paper>
    <paper id="3">
      <title>Writing Styles of Salwa and Al-Qarni</title>
      <author><first>Ahmed</first><last>Omer</last></author>
      <author><first>Michael</first><last>Oakes</last></author>
      <pages>16–21</pages>
      <url hash="7779bd91">W19-5603</url>
      <bibkey>omer-oakes-2019-writing</bibkey>
    </paper>
    <paper id="4">
      <title>Classifying Information Sources in <fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter to Support Online Monitoring of Infectious Diseases</title>
      <author><first>Lama</first><last>Alsudias</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <pages>22–30</pages>
      <url hash="da1cf978">W19-5604</url>
      <bibkey>alsudias-rayson-2019-classifying</bibkey>
    </paper>
    <paper id="5">
      <title>Text Segmentation Using N-grams to Annotate <fixed-case>H</fixed-case>adith Corpus</title>
      <author><first>Shatha</first><last>Altammami</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Ammar</first><last>Alsalka</last></author>
      <pages>31–39</pages>
      <url hash="2aee817c">W19-5605</url>
      <bibkey>altammami-etal-2019-text</bibkey>
    </paper>
    <paper id="6">
      <title>Can <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic Approaches be used for <fixed-case>A</fixed-case>rabic Dialects? Sentiment Analysis as a Case Study</title>
      <author><first>Kathrein</first><last>Abu Kwaik</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>40–50</pages>
      <url hash="28d9c78b">W19-5606</url>
      <bibkey>qwaider-etal-2019-modern</bibkey>
    </paper>
    <paper id="7">
      <title>Classifying <fixed-case>A</fixed-case>rabic dialect text in the Social Media <fixed-case>A</fixed-case>rabic Dialect Corpus (<fixed-case>SMADC</fixed-case>)</title>
      <author><first>Areej</first><last>Alshutayri</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <pages>51–59</pages>
      <url hash="8216ad7e">W19-5607</url>
      <bibkey>alshutayri-atwell-2019-classifying</bibkey>
    </paper>
    <paper id="8">
      <title>Verbs in <fixed-case>E</fixed-case>gyptian <fixed-case>A</fixed-case>rabic: a case for register variation</title>
      <author><first>Michael Grant</first><last>White</last></author>
      <author><first>Deryle W.</first><last>Lonsdale</last></author>
      <pages>60–71</pages>
      <url hash="0f0a261e">W19-5608</url>
      <bibkey>white-lonsdale-2019-verbs</bibkey>
    </paper>
    <paper id="9">
      <title>Crisis Detection from <fixed-case>A</fixed-case>rabic Tweets</title>
      <author><first>Alaa</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>72–79</pages>
      <url hash="f16b72dc">W19-5609</url>
      <bibkey>alharbi-lee-2019-crisis</bibkey>
    </paper>
    <paper id="10">
      <title>The Design of the <fixed-case>S</fixed-case>au<fixed-case>LTC</fixed-case> application for the <fixed-case>E</fixed-case>nglish-<fixed-case>A</fixed-case>rabic Learner Translation Corpus</title>
      <author><first>Maha</first><last>Al-Harthi</last></author>
      <author><first>Amal</first><last>Alsaif</last></author>
      <pages>80–88</pages>
      <url hash="521444a4">W19-5610</url>
      <bibkey>al-harthi-alsaif-2019-design</bibkey>
    </paper>
    <paper id="11">
      <title>Distance-Based Authorship Verification Across <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic Genres</title>
      <author><first>Hossam</first><last>Ahmed</last></author>
      <pages>89–96</pages>
      <url hash="cff7e62f">W19-5611</url>
      <bibkey>ahmed-2019-distance</bibkey>
    </paper>
  </volume>
  <volume id="57">
    <meta>
      <booktitle>Proceedings of the 16th Meeting on the Mathematics of Language</booktitle>
      <url hash="e994cd2a">W19-57</url>
      <editor><first>Philippe</first><last>de Groote</last></editor>
      <editor><first>Frank</first><last>Drewes</last></editor>
      <editor><first>Gerald</first><last>Penn</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2019</year>
      <venue>mol</venue>
    </meta>
    <frontmatter>
      <url hash="11f83759">W19-5700</url>
      <bibkey>ws-2019-mathematics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Parsing Weighted Order-Preserving Hyperedge Replacement Grammars</title>
      <author><first>Henrik</first><last>Björklund</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <author><first>Petter</first><last>Ericson</last></author>
      <pages>1–11</pages>
      <url hash="d76b0b80">W19-5701</url>
      <doi>10.18653/v1/W19-5701</doi>
      <bibkey>bjorklund-etal-2019-parsing</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>S</fixed-case>ensing Tree Automata as a Model of Syntactic Dependencies</title>
      <author><first>Thomas</first><last>Graf</last></author>
      <author><first>Aniello</first><last>De Santo</last></author>
      <pages>12–26</pages>
      <url hash="9a046374">W19-5702</url>
      <doi>10.18653/v1/W19-5702</doi>
      <bibkey>graf-de-santo-2019-sensing</bibkey>
    </paper>
    <paper id="3">
      <title>Presupposition Projection and Repair Strategies in Trivalent Semantics</title>
      <author><first>Yoad</first><last>Winter</last></author>
      <pages>27–39</pages>
      <url hash="e952661b">W19-5703</url>
      <doi>10.18653/v1/W19-5703</doi>
      <bibkey>winter-2019-presupposition</bibkey>
    </paper>
    <paper id="4">
      <title>Dependently-Typed <fixed-case>M</fixed-case>ontague Semantics in the Proof Assistant Agda-flat</title>
      <author><first>Colin</first><last>Zwanziger</last></author>
      <pages>40–49</pages>
      <url hash="ffd40f91">W19-5704</url>
      <doi>10.18653/v1/W19-5704</doi>
      <bibkey>zwanziger-2019-dependently</bibkey>
    </paper>
    <paper id="5">
      <title>Quantifier-free least fixed point functions for phonology</title>
      <author><first>Jane</first><last>Chandlee</last></author>
      <author><first>Adam</first><last>Jardine</last></author>
      <pages>50–62</pages>
      <url hash="34dc985e">W19-5705</url>
      <doi>10.18653/v1/W19-5705</doi>
      <bibkey>chandlee-jardine-2019-quantifier</bibkey>
    </paper>
    <paper id="6">
      <title>Some classes of sets of structures definable without quantifiers</title>
      <author><first>James</first><last>Rogers</last></author>
      <author><first>Dakotah</first><last>Lambert</last></author>
      <pages>63–77</pages>
      <url hash="3a79e2a8">W19-5706</url>
      <doi>10.18653/v1/W19-5706</doi>
      <bibkey>rogers-lambert-2019-classes</bibkey>
    </paper>
    <paper id="7">
      <title>Efficient learning of Output Tier-based Strictly 2-Local functions</title>
      <author><first>Phillip</first><last>Burness</last></author>
      <author><first>Kevin</first><last>McMullin</last></author>
      <pages>78–90</pages>
      <url hash="ce49816f">W19-5707</url>
      <doi>10.18653/v1/W19-5707</doi>
      <bibkey>burness-mcmullin-2019-efficient</bibkey>
    </paper>
    <paper id="8">
      <title>Learning with Partially Ordered Representations</title>
      <author><first>Jane</first><last>Chandlee</last></author>
      <author><first>Remi</first><last>Eyraud</last></author>
      <author><first>Jeffrey</first><last>Heinz</last></author>
      <author><first>Adam</first><last>Jardine</last></author>
      <author><first>Jonathan</first><last>Rawski</last></author>
      <pages>91–101</pages>
      <url hash="8344d550">W19-5708</url>
      <doi>10.18653/v1/W19-5708</doi>
      <bibkey>chandlee-etal-2019-learning</bibkey>
    </paper>
    <paper id="9">
      <title>Maximum Likelihood Estimation of Factored Regular Deterministic Stochastic Languages</title>
      <author><first>Chihiro</first><last>Shibata</last></author>
      <author><first>Jeffrey</first><last>Heinz</last></author>
      <pages>102–113</pages>
      <url hash="533a69db">W19-5709</url>
      <doi>10.18653/v1/W19-5709</doi>
      <bibkey>shibata-heinz-2019-maximum</bibkey>
    </paper>
    <paper id="10">
      <title>Sentence Length</title>
      <author><first>Gábor</first><last>Borbély</last></author>
      <author><first>András</first><last>Kornai</last></author>
      <pages>114–125</pages>
      <url hash="f248bdaa">W19-5710</url>
      <doi>10.18653/v1/W19-5710</doi>
      <bibkey>borbely-kornai-2019-sentence</bibkey>
    </paper>
  </volume>
  <volume id="58">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)</booktitle>
      <url hash="8db96abc">W19-58</url>
      <editor><first>Luis</first><last>Espinosa-Anke</last></editor>
      <editor><first>Thierry</first><last>Declerck</last></editor>
      <editor><first>Dagmar</first><last>Gromann</last></editor>
      <editor><first>Jose</first><last>Camacho-Collados</last></editor>
      <editor><first>Mohammad Taher</first><last>Pilehvar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Macau, China</address>
      <month>August</month>
      <year>2019</year>
      <venue>semdeep</venue>
    </meta>
    <frontmatter>
      <url hash="47711bb4">W19-5800</url>
      <bibkey>ws-2019-semantic</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>LIAAD</fixed-case> at <fixed-case>S</fixed-case>em<fixed-case>D</fixed-case>eep-5 Challenge: Word-in-Context (<fixed-case>W</fixed-case>i<fixed-case>C</fixed-case>)</title>
      <author><first>Daniel</first><last>Loureiro</last></author>
      <author><first>Alípio</first><last>Jorge</last></author>
      <pages>1–5</pages>
      <url hash="f7a7de80">W19-5801</url>
      <bibkey>loureiro-jorge-2019-liaad</bibkey>
      <pwccode url="https://github.com/danlou/LMMS" additional="false">danlou/LMMS</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>LIMSI</fixed-case>-<fixed-case>MULTISEM</fixed-case> at the <fixed-case>IJCAI</fixed-case> <fixed-case>S</fixed-case>em<fixed-case>D</fixed-case>eep-5 <fixed-case>W</fixed-case>i<fixed-case>C</fixed-case> Challenge: Context Representations for Word Usage Similarity Estimation</title>
      <author><first>Aina</first><last>Garí Soler</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>6–11</pages>
      <url hash="5bc990b1">W19-5802</url>
      <bibkey>gari-soler-etal-2019-limsi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="3">
      <title>Bridging the Gap: Improve Part-of-speech Tagging for <fixed-case>C</fixed-case>hinese Social Media Texts with Foreign Words</title>
      <author><first>Dingmin</first><last>Wang</last></author>
      <author><first>Meng</first><last>Fang</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <pages>12–20</pages>
      <url hash="aa9eb5d2">W19-5803</url>
      <bibkey>wang-etal-2019-bridging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="4">
      <title>An <fixed-case>ELM</fixed-case>o-inspired approach to <fixed-case>S</fixed-case>em<fixed-case>D</fixed-case>eep-5’s Word-in-Context task</title>
      <author><first>Alan</first><last>Ansell</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <author><first>Bernhard</first><last>Pfahringer</last></author>
      <pages>21–25</pages>
      <url hash="bf40beca">W19-5804</url>
      <bibkey>ansell-etal-2019-elmo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="5">
      <title>Using hyperbolic large-margin classifiers for biological link prediction</title>
      <author><first>Asan</first><last>Agibetov</last></author>
      <author><first>Georg</first><last>Dorffner</last></author>
      <author><first>Matthias</first><last>Samwald</last></author>
      <pages>26–30</pages>
      <url hash="dae33348">W19-5805</url>
      <bibkey>agibetov-etal-2019-using</bibkey>
      <pwccode url="https://github.com/plumdeq/hsvm" additional="false">plumdeq/hsvm</pwccode>
    </paper>
    <paper id="6">
      <title>Extending Neural Question Answering with Linguistic Input Features</title>
      <author><first>Fabian</first><last>Hommel</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>Matthias</first><last>Orlikowski</last></author>
      <author><first>Matthias</first><last>Hartung</last></author>
      <pages>31–39</pages>
      <url hash="51564d97">W19-5806</url>
      <bibkey>hommel-etal-2019-extending</bibkey>
    </paper>
    <paper id="7">
      <title>How to Use Gazetteers for Entity Recognition with Neural Models</title>
      <author><first>Simone</first><last>Magnolini</last></author>
      <author><first>Valerio</first><last>Piccioni</last></author>
      <author><first>Vevake</first><last>Balaraman</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>40–49</pages>
      <url hash="93e7690f">W19-5807</url>
      <bibkey>magnolini-etal-2019-use</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="8">
      <title>Learning Household Task Knowledge from <fixed-case>W</fixed-case>iki<fixed-case>H</fixed-case>ow Descriptions</title>
      <author><first>Yilun</first><last>Zhou</last></author>
      <author><first>Julie</first><last>Shah</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>50–56</pages>
      <url hash="6451b964">W19-5808</url>
      <bibkey>zhou-etal-2019-learning-household</bibkey>
      <pwccode url="https://github.com/YilunZhou/wikihow-embedding" additional="false">YilunZhou/wikihow-embedding</pwccode>
    </paper>
    <paper id="9">
      <title>A Sequence Modeling Approach for Structured Data Extraction from Unstructured Text</title>
      <author><first>Jayati</first><last>Deshmukh</last></author>
      <author><first>Annervaz K</first><last>M</last></author>
      <author><first>Shubhashis</first><last>Sengupta</last></author>
      <pages>57–66</pages>
      <url hash="fed815d3">W19-5809</url>
      <bibkey>deshmukh-etal-2019-sequence</bibkey>
    </paper>
  </volume>
  <volume id="59" ingest-date="2019-10-07">
    <meta>
      <booktitle>Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
      <url hash="20ed9820">W19-59</url>
      <editor><first>Satoshi</first><last>Nakamura</last></editor>
      <editor><first>Milica</first><last>Gasic</last></editor>
      <editor><first>Ingrid</first><last>Zuckerman</last></editor>
      <editor><first>Gabriel</first><last>Skantze</last></editor>
      <editor><first>Mikio</first><last>Nakano</last></editor>
      <editor><first>Alexandros</first><last>Papangelis</last></editor>
      <editor><first>Stefan</first><last>Ultes</last></editor>
      <editor><first>Koichiro</first><last>Yoshino</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Stockholm, Sweden</address>
      <month>September</month>
      <year>2019</year>
      <venue>sigdial</venue>
    </meta>
    <frontmatter>
      <url hash="79748134">W19-5900</url>
      <bibkey>ws-2019-sigdial</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Deep Reinforcement Learning For Modeling Chit-Chat Dialog With Discrete Attributes</title>
      <author><first>Chinnadhurai</first><last>Sankar</last></author>
      <author><first>Sujith</first><last>Ravi</last></author>
      <pages>1–10</pages>
      <abstract>Open domain dialog systems face the challenge of being repetitive and producing generic responses. In this paper, we demonstrate that by conditioning the response generation on interpretable discrete dialog attributes and composed attributes, it helps improve the model perplexity and results in diverse and interesting non-redundant responses. We propose to formulate the dialog attribute prediction as a reinforcement learning (RL) problem and use policy gradients methods to optimize utterance generation using long-term rewards. Unlike existing RL approaches which formulate the token prediction as a policy, our method reduces the complexity of the policy optimization by limiting the action space to dialog attributes, thereby making the policy optimization more practical and sample efficient. We demonstrate this with experimental and human evaluations.</abstract>
      <url hash="c5773e8c">W19-5901</url>
      <doi>10.18653/v1/W19-5901</doi>
      <bibkey>sankar-ravi-2019-deep</bibkey>
    </paper>
    <paper id="2">
      <title>Improving Interaction Quality Estimation with <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case>s and the Impact on Dialogue Policy Learning</title>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>11–20</pages>
      <abstract>Learning suitable and well-performing dialogue behaviour in statistical spoken dialogue systems has been in the focus of research for many years. While most work which is based on reinforcement learning employs an objective measure like task success for modelling the reward signal, we use a reward based on user satisfaction estimation. We propose a novel estimator and show that it outperforms all previous estimators while learning temporal dependencies implicitly. Furthermore, we apply this novel user satisfaction estimation model live in simulated experiments where the satisfaction estimation model is trained on one domain and applied in many other domains which cover a similar task. We show that applying this model results in higher estimated satisfaction, similar task success rates and a higher robustness to noise.</abstract>
      <url hash="1a6a33d6">W19-5902</url>
      <doi>10.18653/v1/W19-5902</doi>
      <bibkey>ultes-2019-improving</bibkey>
    </paper>
    <paper id="3">
      <title>Lifelong and Interactive Learning of Factual Knowledge in Dialogues</title>
      <author><first>Sahisnu</first><last>Mazumder</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Nianzu</first><last>Ma</last></author>
      <pages>21–31</pages>
      <abstract>Dialogue systems are increasingly using knowledge bases (KBs) storing real-world facts to help generate quality responses. However, as the KBs are inherently incomplete and remain fixed during conversation, it limits dialogue systems’ ability to answer questions and to handle questions involving entities or relations that are not in the KB. In this paper, we make an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn and infer new knowledge during conversations. With more knowledge accumulated over time, they will be able to learn better and answer more questions. Our empirical evaluation shows that CILK is promising.</abstract>
      <url hash="de7d5815">W19-5903</url>
      <doi>10.18653/v1/W19-5903</doi>
      <bibkey>mazumder-etal-2019-lifelong</bibkey>
    </paper>
    <paper id="4">
      <title>Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach</title>
      <author><first>Igor</first><last>Shalyminov</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <author><first>Arash</first><last>Eshghi</last></author>
      <author><first>Oliver</first><last>Lemon</last></author>
      <pages>32–39</pages>
      <abstract>Learning with minimal data is one of the key challenges in the development of practical, production-ready goal-oriented dialogue systems. In a real-world enterprise setting where dialogue systems are developed rapidly and are expected to work robustly for an ever-growing variety of domains, products, and scenarios, efficient learning from a limited number of examples becomes indispensable. In this paper, we introduce a technique to achieve state-of-the-art dialogue generation performance in a few-shot setup, without using any annotated data. We do this by leveraging background knowledge from a larger, more highly represented dialogue source — namely, the MetaLWOz dataset. We evaluate our model on the Stanford Multi-Domain Dialogue Dataset, consisting of human-human goal-oriented dialogues in in-car navigation, appointment scheduling, and weather information domains. We show that our few-shot approach achieves state-of-the art results on that dataset by consistently outperforming the previous best model in terms of BLEU and Entity F1 scores, while being more data-efficient than it by not requiring any data annotation.</abstract>
      <url hash="efd6be8a">W19-5904</url>
      <doi>10.18653/v1/W19-5904</doi>
      <bibkey>shalyminov-etal-2019-shot</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/metalwoz">MetaLWOz</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>SIM</fixed-case>: A Slot-Independent Neural Model for Dialogue State Tracking</title>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <author><first>Xuedong</first><last>Huang</last></author>
      <pages>40–45</pages>
      <abstract>Dialogue state tracking is an important component in task-oriented dialogue systems to identify users’ goals and requests as a dialogue proceeds. However, as most previous models are dependent on dialogue slots, the model complexity soars when the number of slots increases. In this paper, we put forward a slot-independent neural model (SIM) to track dialogue states while keeping the model complexity invariant to the number of dialogue slots. The model utilizes attention mechanisms between user utterance and system actions. SIM achieves state-of-the-art results on WoZ and DSTC2 tasks, with only 20% of the model size of previous models.</abstract>
      <url hash="14a38c72">W19-5905</url>
      <doi>10.18653/v1/W19-5905</doi>
      <bibkey>zhu-etal-2019-sim</bibkey>
    </paper>
    <paper id="6">
      <title>Simple, Fast, Accurate Intent Classification and Slot Labeling for Goal-Oriented Dialogue Systems</title>
      <author><first>Arshit</first><last>Gupta</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Katrin</first><last>Kirchhoff</last></author>
      <pages>46–55</pages>
      <abstract>With the advent of conversational assistants, like Amazon Alexa, Google Now, etc., dialogue systems are gaining a lot of traction, especially in industrial setting. These systems typically consist of Spoken Language understanding component which, in turn, consists of two tasks - Intent Classification (IC) and Slot Labeling (SL). Generally, these two tasks are modeled together jointly to achieve best performance. However, this joint modeling adds to model obfuscation. In this work, we first design framework for a modularization of joint IC-SL task to enhance architecture transparency. Then, we explore a number of self-attention, convolutional, and recurrent models, contributing a large-scale analysis of modeling paradigms for IC+SL across two datasets. Finally, using this framework, we propose a class of ‘label-recurrent’ models that otherwise non-recurrent, with a 10-dimensional representation of the label history, and show that our proposed systems are easy to interpret, highly accurate (achieving over 30% error reduction in SL over the state-of-the-art on the Snips dataset), as well as fast, at 2x the inference and 2/3 to 1/2 the training time of comparable recurrent models, thus giving an edge in critical real-world systems.</abstract>
      <url hash="2c293753">W19-5906</url>
      <doi>10.18653/v1/W19-5906</doi>
      <bibkey>gupta-etal-2019-simple</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="7">
      <title>Time Masking: Leveraging Temporal Information in Spoken Dialogue Systems</title>
      <author><first>Rylan</first><last>Conway</last></author>
      <author><first>Mathias</first><last>Lambert</last></author>
      <pages>56–61</pages>
      <abstract>In a spoken dialogue system, dialogue state tracker (DST) components track the state of the conversation by updating a distribution of values associated with each of the slots being tracked for the current user turn, using the interactions until then. Much of the previous work has relied on modeling the natural order of the conversation, using distance based offsets as an approximation of time. In this work, we hypothesize that leveraging the wall-clock temporal difference between turns is crucial for finer-grained control of dialogue scenarios. We develop a novel approach that applies a time mask, based on the wall-clock time difference, to the associated slot embeddings and empirically demonstrate that our proposed approach outperforms existing approaches that leverage distance offsets, on both an internal benchmark dataset as well as DSTC2.</abstract>
      <url hash="a727e9a8">W19-5907</url>
      <doi>10.18653/v1/W19-5907</doi>
      <bibkey>conway-lambert-2019-time</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/segtrack-v2-1">SegTrack-v2</pwcdataset>
    </paper>
    <paper id="8">
      <title>To Combine or Not To Combine? A Rainbow Deep Reinforcement Learning Agent for Dialog Policies</title>
      <author><first>Dirk</first><last>Väth</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>62–67</pages>
      <abstract>In this paper, we explore state-of-the-art deep reinforcement learning methods for dialog policy training such as prioritized experience replay, double deep Q-Networks, dueling network architectures and distributional learning. Our main findings show that each individual method improves the rewards and the task success rate but combining these methods in a Rainbow agent, which performs best across tasks and environments, is a non-trivial task. We, therefore, provide insights about the influence of each method on the combination and how to combine them to form a Rainbow agent.</abstract>
      <url hash="f8c42956">W19-5908</url>
      <doi>10.18653/v1/W19-5908</doi>
      <bibkey>vath-vu-2019-combine</bibkey>
    </paper>
    <paper id="9">
      <title>Contextualized Representations for Low-resource Utterance Tagging</title>
      <author><first>Bhargavi</first><last>Paranjape</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>68–74</pages>
      <abstract>Utterance-level analysis of the speaker’s intentions and emotions is a core task in conversational understanding. Depending on the end objective of the conversational understanding task, different categorical dialog-act or affect labels are expertly designed to cover specific aspects of the speakers’ intentions or emotions respectively. Accurately annotating with these labels requires a high level of human expertise, and thus applying this process to a large conversation corpus or new domains is prohibitively expensive. The resulting paucity of data limits the use of sophisticated neural models. In this paper, we tackle these limitations by performing unsupervised training of utterance representations from a large corpus of spontaneous dialogue data. Models initialized with these representations achieve competitive performance on utterance-level dialogue-act recognition and emotion classification, especially in low-resource settings encountered when analyzing conversations in new domains.</abstract>
      <url hash="4ab15806">W19-5909</url>
      <doi>10.18653/v1/W19-5909</doi>
      <bibkey>paranjape-neubig-2019-contextualized</bibkey>
    </paper>
    <paper id="10">
      <title>Capturing Dialogue State Variable Dependencies with an Energy-based Neural Dialogue State Tracker</title>
      <author><first>Anh Duong</first><last>Trinh</last></author>
      <author><first>Robert J.</first><last>Ross</last></author>
      <author><first>John D.</first><last>Kelleher</last></author>
      <pages>75–84</pages>
      <abstract>Dialogue state tracking requires the population and maintenance of a multi-slot frame representation of the dialogue state. Frequently, dialogue state tracking systems assume independence between slot values within a frame. In this paper we argue that treating the prediction of each slot value as an independent prediction task may ignore important associations between the slot values, and, consequently, we argue that treating dialogue state tracking as a structured prediction problem can help to improve dialogue state tracking performance. To support this argument, the research presented in this paper is structured into three stages: (i) analyzing variable dependencies in dialogue data; (ii) applying an energy-based methodology to model dialogue state tracking as a structured prediction task; and (iii) evaluating the impact of inter-slot relationships on model performance. Overall we demonstrate that modelling the associations between target slots with an energy-based formalism improves dialogue state tracking performance in a number of ways.</abstract>
      <url hash="6f90986d">W19-5910</url>
      <doi>10.18653/v1/W19-5910</doi>
      <bibkey>trinh-etal-2019-capturing</bibkey>
    </paper>
    <paper id="11">
      <title>Leveraging Non-Conversational Tasks for Low Resource Slot Filling: Does it help?</title>
      <author><first>Samuel</first><last>Louvan</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <pages>85–91</pages>
      <abstract>Slot filling is a core operation for utterance understanding in task-oriented dialogue systems. Slots are typically domain-specific, and adding new domains to a dialogue system involves data and time-intensive processes. A popular technique to address the problem is transfer learning, where it is assumed the availability of a large slot filling dataset for the source domain, to be used to help slot filling on the target domain, with fewer data. In this work, instead, we propose to leverage source tasks based on semantically related non-conversational resources (e.g., semantic sequence tagging datasets), as they are both cheaper to obtain and reusable to several slot filling domains. We show that using auxiliary non-conversational tasks in a multi-task learning setup consistently improves low resource slot filling performance.</abstract>
      <url hash="655fa148">W19-5911</url>
      <doi>10.18653/v1/W19-5911</doi>
      <bibkey>louvan-magnini-2019-leveraging</bibkey>
    </paper>
    <paper id="12">
      <title>Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning</title>
      <author><first>Alexandros</first><last>Papangelis</last></author>
      <author><first>Yi-Chia</first><last>Wang</last></author>
      <author><first>Piero</first><last>Molino</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <pages>92–102</pages>
      <abstract>Some of the major challenges in training conversational agents include the lack of large-scale data of real-world complexity, defining appropriate evaluation measures, and managing meaningful conversations across many topics over long periods of time. Moreover, most works tend to assume that the conversational agent’s environment is stationary, a somewhat strong assumption. To remove this assumption and overcome the lack of data, we take a step away from the traditional training pipeline and model the conversation as a stochastic collaborative game. Each agent (player) has a role (“assistant”, “tourist”, “eater”, etc.) and their own objectives, and can only interact via language they generate. Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own LU and LG, the other agent’s LU, Policy, and LG). In this work, we present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language and show that they outperform supervised and deep learning baselines.</abstract>
      <url hash="a46925d2">W19-5912</url>
      <doi>10.18653/v1/W19-5912</doi>
      <bibkey>papangelis-etal-2019-collaborative</bibkey>
      <pwccode url="https://github.com/uber-research/plato-research-dialogue-system" additional="true">uber-research/plato-research-dialogue-system</pwccode>
    </paper>
    <paper id="13">
      <title>Scoring Interactional Aspects of Human-Machine Dialog for Language Learning and Assessment using Text Features</title>
      <author><first>Vikram</first><last>Ramanarayanan</last></author>
      <author><first>Matthew</first><last>Mulholland</last></author>
      <author><first>Yao</first><last>Qian</last></author>
      <pages>103–109</pages>
      <abstract>While there has been much work in the language learning and assessment literature on human and automated scoring of essays and short constructed responses, there is little to no work examining text features for scoring of dialog data, particularly interactional aspects thereof, to assess conversational proficiency over and above constructed response skills. Our work bridges this gap by investigating both human and automated approaches towards scoring human–machine text dialog in the context of a real-world language learning application. We collected conversational data of human learners interacting with a cloud-based standards-compliant dialog system, triple-scored these data along multiple dimensions of conversational proficiency, and then analyzed the performance trends. We further examined two different approaches to automated scoring of such data and show that these approaches are able to perform at or above par with human agreement for a majority of dimensions of the scoring rubric.</abstract>
      <url hash="f2ae00b2">W19-5913</url>
      <doi>10.18653/v1/W19-5913</doi>
      <bibkey>ramanarayanan-etal-2019-scoring</bibkey>
    </paper>
    <paper id="14">
      <title>Spoken Conversational Search for General Knowledge</title>
      <author><first>Lina M.</first><last>Rojas Barahona</last></author>
      <author><first>Pascal</first><last>Bellec</last></author>
      <author><first>Benoit</first><last>Besset</last></author>
      <author><first>Martinho</first><last>Dossantos</last></author>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Munshi</first><last>Asadullah</last></author>
      <author><first>Olivier</first><last>Leblouch</last></author>
      <author><first>Jeanyves.</first><last>Lancien</last></author>
      <author><first>Geraldine</first><last>Damnati</last></author>
      <author><first>Emmanuel</first><last>Mory</last></author>
      <author><first>Frederic</first><last>Herledan</last></author>
      <pages>110–113</pages>
      <abstract>We present a spoken conversational question answering proof of concept that is able to answer questions about general knowledge from Wikidata. The dialogue agent does not only orchestrate various agents but also solve coreferences and ellipsis.</abstract>
      <url hash="4accbca0">W19-5914</url>
      <doi>10.18653/v1/W19-5914</doi>
      <bibkey>rojas-barahona-etal-2019-spoken</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>G</fixed-case>raph2<fixed-case>B</fixed-case>ots, Unsupervised Assistance for Designing Chatbots</title>
      <author><first>Jean-Leon</first><last>Bouraoui</last></author>
      <author><first>Sonia</first><last>Le Meitour</last></author>
      <author><first>Romain</first><last>Carbou</last></author>
      <author><first>Lina M.</first><last>Rojas Barahona</last></author>
      <author><first>Vincent</first><last>Lemaire</last></author>
      <pages>114–117</pages>
      <abstract>We present Graph2Bots, a tool for assisting conversational agent designers. It extracts a graph representation from human-human conversations by using unsupervised learning. The generated graph contains the main stages of the dialogue and their inner transitions. The graphical user interface (GUI) then allows graph editing.</abstract>
      <url hash="1cd302aa">W19-5915</url>
      <doi>10.18653/v1/W19-5915</doi>
      <bibkey>bouraoui-etal-2019-graph2bots</bibkey>
    </paper>
    <paper id="16">
      <title>On a Chatbot Conducting Dialogue-in-Dialogue</title>
      <author><first>Boris</first><last>Galitsky</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <pages>118–121</pages>
      <abstract>We demo a chatbot that delivers content in the form of virtual dialogues automatically produced from plain texts extracted and selected from documents. This virtual dialogue content is provided in the form of answers derived from the found and selected documents split into fragments, and questions are automatically generated for these answers.</abstract>
      <url hash="bf6e2b17">W19-5916</url>
      <doi>10.18653/v1/W19-5916</doi>
      <bibkey>galitsky-etal-2019-chatbot-conducting</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>D</fixed-case>eep<fixed-case>C</fixed-case>opy: Grounded Response Generation with Hierarchical Pointer Networks</title>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Guan-Lin</first><last>Chao</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>122–132</pages>
      <abstract>Recent advances in neural sequence-to-sequence models have led to promising results for several language generation-based tasks, including dialogue response generation, summarization, and machine translation. However, these models are known to have several problems, especially in the context of chit-chat based dialogue systems: they tend to generate short and dull responses that are often too generic. Furthermore, these models do not ground conversational responses on knowledge and facts, resulting in turns that are not accurate, informative and engaging for the users. In this paper, we propose and experiment with a series of response generation models that aim to serve in the general scenario where in addition to the dialogue context, relevant unstructured external knowledge in the form of text is also assumed to be available for models to harness. Our proposed approach extends pointer-generator networks (See et al., 2017) by allowing the decoder to hierarchically attend and copy from external knowledge in addition to the dialogue context. We empirically show the effectiveness of the proposed model compared to several baselines including (Ghazvininejadet al., 2018; Zhang et al., 2018) through both automatic evaluation metrics and human evaluation on ConvAI2 dataset.</abstract>
      <url hash="42f490b9">W19-5917</url>
      <doi>10.18653/v1/W19-5917</doi>
      <bibkey>yavuz-etal-2019-deepcopy</bibkey>
    </paper>
    <paper id="18">
      <title>Towards End-to-End Learning for Efficient Dialogue Agent by Modeling Looking-ahead Ability</title>
      <author><first>Zhuoxuan</first><last>Jiang</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Ziming</first><last>Huang</last></author>
      <author><first>Jie</first><last>Ma</last></author>
      <author><first>Shaochun</first><last>Li</last></author>
      <pages>133–142</pages>
      <abstract>Learning an efficient manager of dialogue agent from data with little manual intervention is important, especially for goal-oriented dialogues. However, existing methods either take too many manual efforts (e.g. reinforcement learning methods) or cannot guarantee the dialogue efficiency (e.g. sequence-to-sequence methods). In this paper, we address this problem by proposing a novel end-to-end learning model to train a dialogue agent that can look ahead for several future turns and generate an optimal response to make the dialogue efficient. Our method is data-driven and does not require too much manual work for intervention during system design. We evaluate our method on two datasets of different scenarios and the experimental results demonstrate the efficiency of our model.</abstract>
      <url hash="7afe0c3a">W19-5918</url>
      <doi>10.18653/v1/W19-5918</doi>
      <bibkey>jiang-etal-2019-towards</bibkey>
    </paper>
    <paper id="19">
      <title>Unsupervised Dialogue Spectrum Generation for Log Dialogue Ranking</title>
      <author><first>Xinnuo</first><last>Xu</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Lars</first><last>Liden</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <pages>143–154</pages>
      <abstract>Although the data-driven approaches of some recent bot building platforms make it possible for a wide range of users to easily create dialogue systems, those platforms don’t offer tools for quickly identifying which log dialogues contain problems. This is important since corrections to log dialogues provide a means to improve performance after deployment. A log dialogue ranker, which ranks problematic dialogues higher, is an essential tool due to the sheer volume of log dialogues that could be generated. However, training a ranker typically requires labelling a substantial amount of data, which is not feasible for most users. In this paper, we present a novel unsupervised approach for dialogue ranking using GANs and release a corpus of labelled dialogues for evaluation and comparison with supervised methods. The evaluation result shows that our method compares favorably to supervised methods without any labelled data.</abstract>
      <url hash="ec4a1298">W19-5919</url>
      <doi>10.18653/v1/W19-5919</doi>
      <bibkey>xu-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="20">
      <title>Tree-Structured Semantic Encoder with Knowledge Sharing for Domain Adaptation in Natural Language Generation</title>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Yen-chen</first><last>Wu</last></author>
      <author><first>Milica</first><last>Gasic</last></author>
      <pages>155–164</pages>
      <abstract>Domain adaptation in natural language generation (NLG) remains challenging because of the high complexity of input semantics across domains and limited data of a target domain. This is particularly the case for dialogue systems, where we want to be able to seamlessly include new domains into the conversation. Therefore, it is crucial for generation models to share knowledge across domains for the effective adaptation from one domain to another. In this study, we exploit a tree-structured semantic encoder to capture the internal structure of complex semantic representations required for multi-domain dialogues in order to facilitate knowledge sharing across domains. In addition, a layer-wise attention mechanism between the tree encoder and the decoder is adopted to further improve the model’s capability. The automatic evaluation results show that our model outperforms previous methods in terms of the BLEU score and the slot error rate, in particular when the adaptation data is limited. In subjective evaluation, human judges tend to prefer the sentences generated by our model, rating them more highly on informativeness and naturalness than other systems.</abstract>
      <url hash="93ad2447">W19-5920</url>
      <doi>10.18653/v1/W19-5920</doi>
      <bibkey>tseng-etal-2019-tree</bibkey>
    </paper>
    <paper id="21">
      <title>Structured Fusion Networks for Dialog</title>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <pages>165–177</pages>
      <abstract>Neural dialog models have exhibited strong performance, however their end-to-end nature lacks a representation of the explicit structure of dialog. This results in a loss of generalizability, controllability and a data-hungry nature. Conversely, more traditional dialog systems do have strong models of explicit structure. This paper introduces several approaches for explicitly incorporating structure into neural models of dialog. Structured Fusion Networks first learn neural dialog modules corresponding to the structured components of traditional dialog systems and then incorporate these modules in a higher-level generative model. Structured Fusion Networks obtain strong results on the MultiWOZ dataset, both with and without reinforcement learning. Structured Fusion Networks are shown to have several valuable properties, including better domain generalizability, improved performance in reduced data scenarios and robustness to divergence during reinforcement learning.</abstract>
      <url hash="568ea27b">W19-5921</url>
      <doi>10.18653/v1/W19-5921</doi>
      <bibkey>mehri-etal-2019-structured</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="22">
      <title>Flexibly-Structured Model for Task-Oriented Dialogues</title>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Piero</first><last>Molino</last></author>
      <author><first>Mahdi</first><last>Namazifar</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Huaixiu</first><last>Zheng</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <pages>178–187</pages>
      <abstract>This paper proposes a novel end-to-end architecture for task-oriented dialogue systems. It is based on a simple and practical yet very effective sequence-to-sequence approach, where language understanding and state tracking tasks are modeled jointly with a structured copy-augmented sequential decoder and a multi-label decoder for each slot. The policy engine and language generation tasks are modeled jointly following that. The copy-augmented sequential decoder deals with new or unknown values in the conversation, while the multi-label decoder combined with the sequential decoder ensures the explicit assignment of values to slots. On the generation part, slot binary classifiers are used to improve performance. This architecture is scalable to real-world scenarios and is shown through an empirical evaluation to achieve state-of-the-art performance on both the Cambridge Restaurant dataset and the Stanford in-car assistant dataset.</abstract>
      <url hash="cec9294c">W19-5922</url>
      <doi>10.18653/v1/W19-5922</doi>
      <bibkey>shu-etal-2019-flexibly</bibkey>
      <pwccode url="https://github.com/uber-research/FSDM" additional="false">uber-research/FSDM</pwccode>
    </paper>
    <paper id="23">
      <title><fixed-case>F</fixed-case>riends<fixed-case>QA</fixed-case>: Open-Domain Question Answering on <fixed-case>TV</fixed-case> Show Transcripts</title>
      <author><first>Zhengzhe</first><last>Yang</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>188–197</pages>
      <abstract>This paper presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations. Each dialogue, involving multiple speakers, is annotated with several types of questions regarding the dialogue contexts, and the answers are annotated with certain spans in the dialogue. A series of crowdsourcing tasks are conducted to ensure good annotation quality, resulting a high inter-annotator agreement of 81.82%. A comprehensive annotation analytics is provided for a deeper understanding in this dataset. Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset. BERT in particular depicts promising results, an accuracy of 74.2% for answer utterance selection and an F1-score of 64.2% for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.</abstract>
      <url hash="8794882f">W19-5923</url>
      <doi>10.18653/v1/W19-5923</doi>
      <bibkey>yang-choi-2019-friendsqa</bibkey>
    </paper>
    <paper id="24">
      <title>Foundations of Collaborative Task-Oriented Dialogue: What’s in a Slot?</title>
      <author><first>Philip</first><last>Cohen</last></author>
      <pages>198–209</pages>
      <abstract>In this paper, we examine the foundations of task-oriented dialogues, in which systems are requested to perform tasks for humans. We argue that the way this dialogue task has been framed has limited its applicability to processing simple requests with atomic “slot-fillers”. However, real task-oriented dialogues can contain more complex utterances that provide non-atomic constraints on slot values. For example, in response to the system’s question “What time do you want me to reserve the restaurant?”, a user should be able to say “the earliest time available,” which cannot be handled by classic “intent + slots” approaches that do not incorporate expressive logical form meaning representations. Furthermore, situations for which it would be desirable to build task-oriented dialogue systems, e.g., to engage in mixed-initiative, collaborative or multiparty dialogues, will require a more general approach. In order to overcome these limitations and to provide such an approach, we give a logical analysis of the “intent+slot” dialogue setting using a modal logic of intention and including a more expansive notion of “dialogue state”. Finally, we briefly discuss our program of research to build a next generation of plan-based dialogue systems that goes beyond “intent + slots”.</abstract>
      <url hash="980cd673">W19-5924</url>
      <doi>10.18653/v1/W19-5924</doi>
      <bibkey>cohen-2019-foundations</bibkey>
    </paper>
    <paper id="25">
      <title>Speaker-adapted neural-network-based fusion for multimodal reference resolution</title>
      <author><first>Diana</first><last>Kleingarn</last></author>
      <author><first>Nima</first><last>Nabizadeh</last></author>
      <author><first>Martin</first><last>Heckmann</last></author>
      <author><first>Dorothea</first><last>Kolossa</last></author>
      <pages>210–214</pages>
      <abstract>Humans use a variety of approaches to reference objects in the external world, including verbal descriptions, hand and head gestures, eye gaze or any combination of them. The amount of useful information from each modality, however, may vary depending on the specific person and on several other factors. For this reason, it is important to learn the correct combination of inputs for inferring the best-fitting reference. In this paper, we investigate appropriate speaker-dependent and independent fusion strategies in a multimodal reference resolution task. We show that without any change in the modality models, only through an optimized fusion technique, it is possible to reduce the error rate of the system on a reference resolution task by more than 50%.</abstract>
      <url hash="d31b9db1">W19-5925</url>
      <doi>10.18653/v1/W19-5925</doi>
      <bibkey>kleingarn-etal-2019-speaker</bibkey>
    </paper>
    <paper id="26">
      <title>Learning Question-Guided Video Representation for Multi-Turn Video Question Answering</title>
      <author><first>Guan-Lin</first><last>Chao</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Jindong</first><last>Chen</last></author>
      <author><first>Ian</first><last>Lane</last></author>
      <pages>215–225</pages>
      <abstract>Understanding and conversing about dynamic scenes is one of the key capabilities of AI agents that navigate the environment and convey useful information to humans. Video question answering is a specific scenario of such AI-human interaction where an agent generates a natural language response to a question regarding the video of a dynamic scene. Incorporating features from multiple modalities, which often provide supplementary information, is one of the challenging aspects of video question answering. Furthermore, a question often concerns only a small segment of the video, hence encoding the entire video sequence using a recurrent neural network is not computationally efficient. Our proposed question-guided video representation module efficiently generates the token-level video summary guided by each word in the question. The learned representations are then fused with the question to generate the answer. Through empirical evaluation on the Audio Visual Scene-aware Dialog (AVSD) dataset, our proposed models in single-turn and multi-turn question answering achieve state-of-the-art performance on several automatic natural language generation evaluation metrics.</abstract>
      <url hash="f23ff6cc">W19-5926</url>
      <doi>10.18653/v1/W19-5926</doi>
      <bibkey>chao-etal-2019-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kinetics">Kinetics</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
    </paper>
    <paper id="27">
      <title>Zero-shot transfer for implicit discourse relation classification</title>
      <author><first>Murathan</first><last>Kurfalı</last></author>
      <author><first>Robert</first><last>Östling</last></author>
      <pages>226–231</pages>
      <abstract>Automatically classifying the relation between sentences in a discourse is a challenging task, in particular when there is no overt expression of the relation. It becomes even more challenging by the fact that annotated training data exists only for a small number of languages, such as English and Chinese. We present a new system using zero-shot transfer learning for implicit discourse relation classification, where the only resource used for the target language is unannotated parallel text. This system is evaluated on the discourse-annotated TED-MDB parallel corpus, where it obtains good results for all seven languages using only English training data.</abstract>
      <url hash="d8dc8874">W19-5927</url>
      <doi>10.18653/v1/W19-5927</doi>
      <bibkey>kurfali-ostling-2019-zero</bibkey>
    </paper>
    <paper id="28">
      <title>A Quantitative Analysis of Patients’ Narratives of Heart Failure</title>
      <author><first>Sabita</first><last>Acharya</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <author><first>Andrew</first><last>Boyd</last></author>
      <author><first>Richard</first><last>Cameron</last></author>
      <author><first>Karen</first><last>Dunn Lopez</last></author>
      <author><first>Pamela</first><last>Martyn-Nemeth</last></author>
      <author><first>Debaleena</first><last>Chattopadhyay</last></author>
      <author><first>Pantea</first><last>Habibi</last></author>
      <author><first>Carolyn</first><last>Dickens</last></author>
      <author><first>Haleh</first><last>Vatani</last></author>
      <author><first>Amer</first><last>Ardati</last></author>
      <pages>232–238</pages>
      <abstract>Patients with chronic conditions like heart failure are the most likely to be re-hospitalized. One step towards avoiding re-hospitalization is to devise strategies for motivating patients to take care of their own health. In this paper, we perform a quantitative analysis of patients’ narratives of their experience with heart failure and explore the different topics that patients talk about. We compare two different groups of patients- those unable to take charge of their illness, and those who make efforts to improve their health. We will use the findings from our analysis to refine and personalize the summaries of hospitalizations that our system automatically generates.</abstract>
      <url hash="3da01246">W19-5928</url>
      <doi>10.18653/v1/W19-5928</doi>
      <bibkey>acharya-etal-2019-quantitative</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>TDD</fixed-case>iscourse: A Dataset for Discourse-Level Temporal Ordering of Events</title>
      <author><first>Aakanksha</first><last>Naik</last></author>
      <author><first>Luke</first><last>Breitfeller</last></author>
      <author><first>Carolyn</first><last>Rose</last></author>
      <pages>239–249</pages>
      <abstract>Prior work on temporal relation classification has focused extensively on event pairs in the same or adjacent sentences (local), paying scant attention to discourse-level (global) pairs. This restricts the ability of systems to learn temporal links between global pairs, since reliance on local syntactic features suffices to achieve reasonable performance on existing datasets. However, systems should be capable of incorporating cues from document-level structure to assign temporal relations. In this work, we take a first step towards discourse-level temporal ordering by creating TDDiscourse, the first dataset focusing specifically on temporal links between event pairs which are more than one sentence apart. We create TDDiscourse by augmenting TimeBank-Dense, a corpus of English news articles, manually annotating global pairs that cannot be inferred automatically from existing annotations. Our annotations double the number of temporal links in TimeBank-Dense, while possessing several desirable properties such as focusing on long-distance pairs and not being automatically inferable. We adapt and benchmark the performance of three state-of-the-art models on TDDiscourse and observe that existing systems indeed find discourse-level temporal ordering harder.</abstract>
      <url hash="d18f0008">W19-5929</url>
      <doi>10.18653/v1/W19-5929</doi>
      <bibkey>naik-etal-2019-tddiscourse</bibkey>
    </paper>
    <paper id="30">
      <title>Real Life Application of a Question Answering System Using <fixed-case>BERT</fixed-case> Language Model</title>
      <author><first>Francesca</first><last>Alloatti</last></author>
      <author><first>Luigi</first><last>Di Caro</last></author>
      <author><first>Gianpiero</first><last>Sportelli</last></author>
      <pages>250–253</pages>
      <abstract>It is often hard to apply the newest advances in research to real life scenarios. They usually require the resolution of some specific task applied to a restricted domain, all the while providing small amounts of data to begin with. In this study we apply one of the newest innovations in Deep Learning to a task of text classification. We created a question answering system in Italian that provides information about a specific subject, e-invoicing and digital billing. Italy recently introduced a new legislation about e-invoicing and people have some legit doubts, therefore a large share of professionals could benefit from this tool.</abstract>
      <url hash="d041b0b1">W19-5930</url>
      <doi>10.18653/v1/W19-5930</doi>
      <bibkey>alloatti-etal-2019-real</bibkey>
    </paper>
    <paper id="31">
      <title>Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational <fixed-case>AI</fixed-case>: <fixed-case>HERMIT</fixed-case> <fixed-case>NLU</fixed-case></title>
      <author><first>Andrea</first><last>Vanzo</last></author>
      <author><first>Emanuele</first><last>Bastianelli</last></author>
      <author><first>Oliver</first><last>Lemon</last></author>
      <pages>254–263</pages>
      <abstract>We present a new neural architecture for wide-coverage Natural Language Understanding in Spoken Dialogue Systems. We develop a hierarchical multi-task architecture, which delivers a multi-layer representation of sentence meaning (i.e., Dialogue Acts and Frame-like structures). The architecture is a hierarchy of self-attention mechanisms and BiLSTM encoders followed by CRF tagging layers. We describe a variety of experiments, showing that our approach obtains promising results on a dataset annotated with Dialogue Acts and Frame Semantics. Moreover, we demonstrate its applicability to a different, publicly available NLU dataset annotated with domain-specific intents and corresponding semantic roles, providing overall performance higher than state-of-the-art tools such as RASA, Dialogflow, LUIS, and Watson. For example, we show an average 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and LUIS.</abstract>
      <url hash="963b9417">W19-5931</url>
      <doi>10.18653/v1/W19-5931</doi>
      <bibkey>vanzo-etal-2019-hierarchical</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="32">
      <title>Dialog State Tracking: A Neural Reading Comprehension Approach</title>
      <author><first>Shuyang</first><last>Gao</last></author>
      <author><first>Abhishek</first><last>Sethi</last></author>
      <author><first>Sanchit</first><last>Agarwal</last></author>
      <author><first>Tagyoung</first><last>Chung</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>264–273</pages>
      <abstract>Dialog state tracking is used to estimate the current belief state of a dialog given all the preceding conversation. Machine reading comprehension, on the other hand, focuses on building systems that read passages of text and answer questions that require some understanding of passages. We formulate dialog state tracking as a reading comprehension task to answer the question what is the state of the current dialog? after reading conversational context. In contrast to traditional state tracking methods where the dialog state is often predicted as a distribution over a closed set of all the possible slot values within an ontology, our method uses a simple attention-based neural network to point to the slot values within the conversation. Experiments on MultiWOZ-2.0 cross-domain dialog dataset show that our simple system can obtain similar accuracies compared to the previous more complex methods. By exploiting recent advances in contextual word embeddings, adding a model that explicitly tracks whether a slot value should be carried over to the next turn, and combining our method with a traditional joint state tracking method that relies on closed set vocabulary, we can obtain a joint-goal accuracy of 47.33% on the standard test split, exceeding current state-of-the-art by 11.75%**.</abstract>
      <url hash="e0bfd484">W19-5932</url>
      <doi>10.18653/v1/W19-5932</doi>
      <bibkey>gao-etal-2019-dialog</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="33">
      <title>Cross-Corpus Data Augmentation for Acoustic Addressee Detection</title>
      <author><first>Oleg</first><last>Akhtiamov</last></author>
      <author><first>Ingo</first><last>Siegert</last></author>
      <author><first>Alexey</first><last>Karpov</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <pages>274–283</pages>
      <abstract>Acoustic addressee detection (AD) is a modern paralinguistic and dialogue challenge that especially arises in voice assistants. In the present study, we distinguish addressees in two settings (a conversation between several people and a spoken dialogue system, and a conversation between several adults and a child) and introduce the first competitive baseline (unweighted average recall equals 0.891) for the Voice Assistant Conversation Corpus that models the first setting. We jointly solve both classification problems, using three models: a linear support vector machine dealing with acoustic functionals and two neural networks utilising raw waveforms alongside with acoustic low-level descriptors. We investigate how different corpora influence each other, applying the mixup approach to data augmentation. We also study the influence of various acoustic context lengths on AD. Two-second speech fragments turn out to be sufficient for reliable AD. Mixup is shown to be beneficial for merging acoustic data (extracted features but not raw waveforms) from different domains that allows us to reach a higher classification performance on human-machine AD and also for training a multipurpose neural network that is capable of solving both human-machine and adult-child AD problems.</abstract>
      <url hash="1e1c659e">W19-5933</url>
      <doi>10.18653/v1/W19-5933</doi>
      <bibkey>akhtiamov-etal-2019-cross</bibkey>
    </paper>
    <paper id="34">
      <title>A Scalable Method for Quantifying the Role of Pitch in Conversational Turn-Taking</title>
      <author><first>Kornel</first><last>Laskowski</last></author>
      <author><first>Marcin</first><last>Wlodarczak</last></author>
      <author><first>Mattias</first><last>Heldner</last></author>
      <pages>284–292</pages>
      <abstract>Pitch has long been held as an important signalling channel when planning and deploying speech in conversation, and myriad studies have been undertaken to determine the extent to which it actually plays this role. Unfortunately, these studies have required considerable human investment in data preparation and analysis, and have therefore often been limited to a handful of specific conversational contexts. The current article proposes a framework which addresses these limitations, by enabling a scalable, quantitative characterization of the role of pitch throughout an entire conversation, requiring only the raw signal and speech activity references. The framework is evaluated on the Switchboard dialogue corpus. Experiments indicate that pitch trajectories of both parties are predictive of their incipient speech activity; that pitch should be expressed on a logarithmic scale and Z-normalized, as well as accompanied by a binary voicing variable; and that only the most recent 400 ms of the pitch trajectory are useful in incipient speech activity prediction.</abstract>
      <url hash="d9551c99">W19-5934</url>
      <doi>10.18653/v1/W19-5934</doi>
      <bibkey>laskowski-etal-2019-scalable</bibkey>
    </paper>
    <paper id="35">
      <title>A Large-Scale User Study of an <fixed-case>A</fixed-case>lexa <fixed-case>P</fixed-case>rize Chatbot: Effect of <fixed-case>TTS</fixed-case> Dynamism on Perceived Quality of Social Dialog</title>
      <author><first>Michelle</first><last>Cohn</last></author>
      <author><first>Chun-Yen</first><last>Chen</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>293–306</pages>
      <abstract>This study tests the effect of cognitive-emotional expression in an Alexa text-to-speech (TTS) voice on users’ experience with a social dialog system. We systematically introduced emotionally expressive interjections (e.g., “Wow!”) and filler words (e.g., “um”, “mhmm”) in an Amazon Alexa Prize socialbot, Gunrock. We tested whether these TTS manipulations improved users’ ratings of their conversation across thousands of real user interactions (n=5,527). Results showed that interjections and fillers each improved users’ holistic ratings, an improvement that further increased if the system used both manipulations. A separate perception experiment corroborated the findings from the user study, with improved social ratings for conversations including interjections; however, no positive effect was observed for fillers, suggesting that the role of the rater in the conversation—as active participant or external listener—is an important factor in assessing social dialogs.</abstract>
      <url hash="d56a2e3d">W19-5935</url>
      <doi>10.18653/v1/W19-5935</doi>
      <bibkey>cohn-etal-2019-large</bibkey>
    </paper>
    <paper id="36">
      <title>Influence of Time and Risk on Response Acceptability in a Simple Spoken Dialogue System</title>
      <author><first>Andisheh</first><last>Partovi</last></author>
      <author><first>Ingrid</first><last>Zukerman</last></author>
      <pages>307–319</pages>
      <abstract>We describe a longitudinal user study conducted in the context of a Spoken Dialogue System for a household robot, where we examined the influence of time displacement and situational risk on users’ preferred responses. To this effect, we employed a corpus of spoken requests that asked a robot to fetch or move objects in a room. In the first stage of our study, participants selected among four response types to these requests under two risk conditions: low and high. After some time, the same participants rated several responses to the previous requests — these responses were instantiated from the four response types. Our results show that participants did not rate highly their own response types; moreover, they rated their own response types similarly to different ones. This suggests that, at least in this context, people’s preferences at a particular point in time may not reflect their general attitudes, and that various reasonable response types may be equally acceptable. Our study also reveals that situational risk influences the acceptability of some response types.</abstract>
      <url hash="ac443390">W19-5936</url>
      <doi>10.18653/v1/W19-5936</doi>
      <bibkey>partovi-zukerman-2019-influence</bibkey>
    </paper>
    <paper id="37">
      <title>Characterizing the Response Space of Questions: a Corpus Study for <fixed-case>E</fixed-case>nglish and <fixed-case>P</fixed-case>olish</title>
      <author><first>Jonathan</first><last>Ginzburg</last></author>
      <author><first>Zulipiye</first><last>Yusupujiang</last></author>
      <author><first>Chuyuan</first><last>Li</last></author>
      <author><first>Kexin</first><last>Ren</last></author>
      <author><first>Paweł</first><last>Łupkowski</last></author>
      <pages>320–330</pages>
      <abstract>The main aim of this paper is to provide a characterization of the response space for questions using a taxonomy grounded in a dialogical formal semantics. As a starting point we take the typology for responses in the form of questions provided in (Lupkowski and Ginzburg, 2016). This work develops a wide coverage taxonomy for question/question sequences observable in corpora including the BNC, CHILDES, and BEE, as well as formal modelling of all the postulated classes. Our aim is to extend this work to cover all responses to questions. We present the extended typology of responses to questions based on a corpus studies of BNC, BEE and Maptask with include 506, 262, and 467 question/response pairs respectively. We compare the data for English with data from Polish using the Spokes corpus (205 question/response pairs). We discuss annotation reliability and disagreement analysis. We sketch how each class can be formalized using a dialogical semantics appropriate for dialogue management.</abstract>
      <url hash="6bce02d1">W19-5937</url>
      <doi>10.18653/v1/W19-5937</doi>
      <bibkey>ginzburg-etal-2019-characterizing</bibkey>
    </paper>
    <paper id="38">
      <title>From Explainability to Explanation: Using a Dialogue Setting to Elicit Annotations with Justifications</title>
      <author><first>Nazia</first><last>Attari</last></author>
      <author><first>Martin</first><last>Heckmann</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>331–335</pages>
      <abstract>Despite recent attempts in the field of explainable AI to go beyond black box prediction models, typically already the training data for supervised machine learning is collected in a manner that treats the annotator as a “black box”, the internal workings of which remains unobserved. We present an annotation method where a task is given to a pair of annotators who collaborate on finding the best response. With this we want to shed light on the questions if the collaboration increases the quality of the responses and if this “thinking together” provides useful information in itself, as it at least partially reveals their reasoning steps. Furthermore, we expect that this setting puts the focus on explanation as a linguistic act, vs. explainability as a property of models. In a crowd-sourcing experiment, we investigated three different annotation tasks, each in a collaborative dialogical (two annotators) and monological (one annotator) setting. Our results indicate that our experiment elicits collaboration and that this collaboration increases the response accuracy. We see large differences in the annotators’ behavior depending on the task. Similarly, we also observe that the dialog patterns emerging from the collaboration vary significantly with the task.</abstract>
      <url hash="dd58d2a4">W19-5938</url>
      <doi>10.18653/v1/W19-5938</doi>
      <bibkey>attari-etal-2019-explainability</bibkey>
    </paper>
    <paper id="39">
      <title>Prediction of User Emotion and Dialogue Success Using Audio Spectrograms and Convolutional Neural Networks</title>
      <author><first>Athanasios</first><last>Lykartsis</last></author>
      <author><first>Margarita</first><last>Kotti</last></author>
      <pages>336–344</pages>
      <abstract>In this paper we aim to predict dialogue success and user satisfaction as well as emotion on a turn level. To achieve this, we investigate the use of spectrogram representations, extracted from audio files, in combination with several types of convolutional neural networks. The experiments were performed on the Let’s Go V2 database, comprising 5065 audio files and having labels for subjective and objective dialogue turn success, as well as the emotional state of the user. Results show that by using only audio, it is possible to predict turn success with very high accuracy for all three labels (90%). The best performing input representation were 1s long mel-spectrograms in combination with a CNN with a bottleneck architecture. The resulting system has the potential to be used real-time. Our results significantly surpass the state of the art for dialogue success prediction based only on audio.</abstract>
      <url hash="dc1ba819">W19-5939</url>
      <doi>10.18653/v1/W19-5939</doi>
      <bibkey>lykartsis-kotti-2019-prediction</bibkey>
    </paper>
    <paper id="40">
      <title>Modelling Adaptive Presentations in Human-Robot Interaction using Behaviour Trees</title>
      <author><first>Nils</first><last>Axelsson</last></author>
      <author><first>Gabriel</first><last>Skantze</last></author>
      <pages>345–352</pages>
      <abstract>In dialogue, speakers continuously adapt their speech to accommodate the listener, based on the feedback they receive. In this paper, we explore the modelling of such behaviours in the context of a robot presenting a painting. A Behaviour Tree is used to organise the behaviour on different levels, and allow the robot to adapt its behaviour in real-time; the tree organises engagement, joint attention, turn-taking, feedback and incremental speech processing. An initial implementation of the model is presented, and the system is evaluated in a user study, where the adaptive robot presenter is compared to a non-adaptive version. The adaptive version is found to be more engaging by the users, although no effects are found on the retention of the presented material.</abstract>
      <url hash="a02705c5">W19-5940</url>
      <doi>10.18653/v1/W19-5940</doi>
      <bibkey>axelsson-skantze-2019-modelling</bibkey>
    </paper>
    <paper id="41">
      <title>Coached Conversational Preference Elicitation: A Case Study in Understanding Movie Preferences</title>
      <author><first>Filip</first><last>Radlinski</last></author>
      <author><first>Krisztian</first><last>Balog</last></author>
      <author id="bill-byrne-ucsd"><first>Bill</first><last>Byrne</last></author>
      <author><first>Karthik</first><last>Krishnamoorthi</last></author>
      <pages>353–360</pages>
      <abstract>Conversational recommendation has recently attracted significant attention. As systems must understand users’ preferences, training them has called for conversational corpora, typically derived from task-oriented conversations. We observe that such corpora often do not reflect how people naturally describe preferences. We present a new approach to obtaining user preferences in dialogue: Coached Conversational Preference Elicitation. It allows collection of natural yet structured conversational preferences. Studying the dialogues in one domain, we present a brief quantitative analysis of how people describe movie preferences at scale. Demonstrating the methodology, we release the CCPE-M dataset to the community with over 500 movie preference dialogues expressing over 10,000 preferences.</abstract>
      <url hash="b0635ad5">W19-5941</url>
      <doi>10.18653/v1/W19-5941</doi>
      <bibkey>radlinski-etal-2019-coached</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccpe-m">CCPE-M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coached-conversational-preference-elicitation">Coached Conversational Preference Elicitation</pwcdataset>
    </paper>
    <paper id="42">
      <title>A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents</title>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>361–366</pages>
      <abstract>How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as “polite refusal”, score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user’s perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.</abstract>
      <url hash="8a8023f3">W19-5942</url>
      <doi>10.18653/v1/W19-5942</doi>
      <bibkey>cercas-curry-rieser-2019-crowd</bibkey>
      <pwccode url="https://github.com/amandacurry/metoo_corpus" additional="false">amandacurry/metoo_corpus</pwccode>
    </paper>
    <paper id="43">
      <title>A Dynamic Strategy Coach for Effective Negotiation</title>
      <author><first>Yiheng</first><last>Zhou</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>367–378</pages>
      <abstract>Negotiation is a complex activity involving strategic reasoning, persuasion, and psychology. An average person is often far from an expert in negotiation. Our goal is to assist humans to become better negotiators through a machine-in-the-loop approach that combines machine’s advantage at data-driven decision-making and human’s language generation ability. We consider a bargaining scenario where a seller and a buyer negotiate the price of an item for sale through a text-based dialogue. Our negotiation coach monitors messages between them and recommends strategies in real time to the seller to get a better deal (e.g., “reject the proposal and propose a price”, “talk about your personal experience with the product”). The best strategy largely depends on the context (e.g., the current price, the buyer’s attitude). Therefore, we first identify a set of negotiation strategies, then learn to predict the best strategy in a given dialogue context from a set of human-human bargaining dialogues. Evaluation on human-human dialogues shows that our coach increases the profits of the seller by almost 60%.</abstract>
      <url hash="6859dde8">W19-5943</url>
      <doi>10.18653/v1/W19-5943</doi>
      <bibkey>zhou-etal-2019-dynamic</bibkey>
    </paper>
    <paper id="44">
      <title>Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References</title>
      <author><first>Prakhar</first><last>Gupta</last></author>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Tiancheng</first><last>Zhao</last></author>
      <author><first>Amy</first><last>Pavel</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <author><first>Jeffrey</first><last>Bigham</last></author>
      <pages>379–391</pages>
      <abstract>The aim of this paper is to mitigate the shortcomings of automatic evaluation of open-domain dialog systems through multi-reference evaluation. Existing metrics have been shown to correlate poorly with human judgement, particularly in open-domain dialog. One alternative is to collect human annotations for evaluation, which can be expensive and time consuming. To demonstrate the effectiveness of multi-reference evaluation, we augment the test set of DailyDialog with multiple references. A series of experiments show that the use of multiple references results in improved correlation between several automatic metrics and human judgement for both the quality and the diversity of system output.</abstract>
      <url hash="bb441efe">W19-5944</url>
      <doi>10.18653/v1/W19-5944</doi>
      <bibkey>gupta-etal-2019-investigating</bibkey>
      <pwccode url="https://github.com/prakharguptaz/multirefeval" additional="true">prakharguptaz/multirefeval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="45">
      <title>User Evaluation of a Multi-dimensional Statistical Dialogue System</title>
      <author><first>Simon</first><last>Keizer</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Xingkun</first><last>Liu</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>392–398</pages>
      <abstract>We present the first complete spoken dialogue system driven by a multiimensional statistical dialogue manager. This framework has been shown to substantially reduce data needs by leveraging domain-independent dimensions, such as social obligations or feedback, which (as we show) can be transferred between domains. In this paper, we conduct a user study and show that the performance of a multi-dimensional system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch.</abstract>
      <url hash="9b228715">W19-5945</url>
      <doi>10.18653/v1/W19-5945</doi>
      <bibkey>keizer-etal-2019-user</bibkey>
      <pwccode url="https://bitbucket.org/skeizer/madrigal" additional="false">skeizer/madrigal</pwccode>
    </paper>
    <paper id="46">
      <title>Dialogue Act Classification in Team Communication for Robot Assisted Disaster Response</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <pages>399–410</pages>
      <abstract>We present the results we obtained on the classification of dialogue acts in a corpus of human-human team communication in the domain of robot-assisted disaster response. We annotated dialogue acts according to the ISO 24617-2 standard scheme and carried out experiments using the FastText linear classifier as well as several neural architectures, including feed-forward, recurrent and convolutional neural models with different types of embeddings, context and attention mechanism. The best performance was achieved with a ”Divide &amp; Merge” architecture presented in the paper, using trainable GloVe embeddings and a structured dialogue history. This model learns from the current utterance and the preceding context separately and then combines the two generated representations. Average accuracy of 10-fold cross-validation is 79.8%, F-score 71.8%.</abstract>
      <url hash="f6d7982e">W19-5946</url>
      <doi>10.18653/v1/W19-5946</doi>
      <bibkey>anikina-kruijff-korbayova-2019-dialogue</bibkey>
    </paper>
    <paper id="47">
      <title>Multi-Task Learning of System Dialogue Act Selection for Supervised Pretraining of Goal-Oriented Dialogue Policies</title>
      <author><first>Sarah</first><last>McLeod</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <author><first>Bernd</first><last>Kiefer</last></author>
      <pages>411–417</pages>
      <abstract>This paper describes the use of Multi-Task Neural Networks (NNs) for system dialogue act selection. These models leverage the representations learned by the Natural Language Understanding (NLU) unit to enable robust initialization/bootstrapping of dialogue policies from medium sized initial data sets. We evaluate the models on two goal-oriented dialogue corpora in the travel booking domain. Results show the proposed models improve over models trained without knowledge of NLU tasks.</abstract>
      <url hash="0bd2f8d2">W19-5947</url>
      <doi>10.18653/v1/W19-5947</doi>
      <bibkey>mcleod-etal-2019-multi</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>B</fixed-case>. Rex: a dialogue agent for book recommendations</title>
      <author><first>Mitchell</first><last>Abrams</last></author>
      <author><first>Luke</first><last>Gessler</last></author>
      <author><first>Matthew</first><last>Marge</last></author>
      <pages>418–421</pages>
      <abstract>We present B. Rex, a dialogue agent for book recommendations. B. Rex aims to exploit the cognitive ease of natural dialogue and the excitement of a whimsical persona in order to engage users who might not enjoy using more common interfaces for finding new books. B. Rex succeeds in making book recommendations with good quality based on only information revealed by the user in the dialogue.</abstract>
      <url hash="68e3b696">W19-5948</url>
      <doi>10.18653/v1/W19-5948</doi>
      <bibkey>abrams-etal-2019-b</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>S</fixed-case>pace<fixed-case>R</fixed-case>ef<fixed-case>N</fixed-case>et: a neural approach to spatial reference resolution in a real city environment</title>
      <author><first>Dmytro</first><last>Kalpakchi</last></author>
      <author><first>Johan</first><last>Boye</last></author>
      <pages>422–431</pages>
      <abstract>Adding interactive capabilities to pedestrian wayfinding systems in the form of spoken dialogue will make them more natural to humans. Such an interactive wayfinding system needs to continuously understand and interpret pedestrian’s utterances referring to the spatial context. Achieving this requires the system to identify exophoric referring expressions in the utterances, and link these expressions to the geographic entities in the vicinity. This exophoric spatial reference resolution problem is difficult, as there are often several dozens of candidate referents. We present a neural network-based approach for identifying pedestrian’s references (using a network called RefNet) and resolving them to appropriate geographic objects (using a network called SpaceRefNet). Both methods show promising results beating the respective baselines and earlier reported results in the literature.</abstract>
      <url hash="9d75dbed">W19-5949</url>
      <doi>10.18653/v1/W19-5949</doi>
      <bibkey>kalpakchi-boye-2019-spacerefnet</bibkey>
    </paper>
    <paper id="50">
      <title>Which aspects of discourse relations are hard to learn? Primitive decomposition for discourse relation classification</title>
      <author><first>Charlotte</first><last>Roze</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <pages>432–441</pages>
      <abstract>Discourse relation classification has proven to be a hard task, with rather low performance on several corpora that notably differ on the relation set they use. We propose to decompose the task into smaller, mostly binary tasks corresponding to various primitive concepts encoded into the discourse relation definitions. More precisely, we translate the discourse relations into a set of values for attributes based on distinctions used in the mappings between discourse frameworks proposed by Sanders et al. (2018). This arguably allows for a more robust representation of discourse relations, and enables us to address usually ignored aspects of discourse relation prediction, namely multiple labels and underspecified annotations. We show experimentally which of the conceptual primitives are harder to learn from the Penn Discourse Treebank English corpus, and propose a correspondence to predict the original labels, with preliminary empirical comparisons with a direct model.</abstract>
      <url hash="4b9f5a34">W19-5950</url>
      <doi>10.18653/v1/W19-5950</doi>
      <bibkey>roze-etal-2019-aspects</bibkey>
    </paper>
    <paper id="51">
      <title>Discourse Relation Prediction: Revisiting Word Pairs with Convolutional Networks</title>
      <author><first>Siddharth</first><last>Varia</last></author>
      <author><first>Christopher</first><last>Hidey</last></author>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <pages>442–452</pages>
      <abstract>Word pairs across argument spans have been shown to be effective for predicting the discourse relation between them. We propose an approach to distill knowledge from word pairs for discourse relation classification with convolutional neural networks by incorporating joint learning of implicit and explicit relations. Our novel approach of representing the input as word pairs achieves state-of-the-art results on four-way classification of both implicit and explicit relations as well as one of the binary classification tasks. For explicit relation prediction, we achieve around 20% error reduction on the four-way task. At the same time, compared to a two-layered Bi-LSTM-CRF model, our model is able to achieve these results with half the number of learnable parameters and approximately half the amount of training time.</abstract>
      <url hash="932beaba">W19-5951</url>
      <doi>10.18653/v1/W19-5951</doi>
      <bibkey>varia-etal-2019-discourse</bibkey>
    </paper>
  </volume>
  <volume id="60">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers)</booktitle>
      <url hash="a4217d3f">W19-60</url>
      <editor><first>Antti</first><last>Arppe</last></editor>
      <editor><first>Jeff</first><last>Good</last></editor>
      <editor><first>Mans</first><last>Hulden</last></editor>
      <editor><first>Jordan</first><last>Lachler</last></editor>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Lane</first><last>Schwartz</last></editor>
      <editor><first>Miikka</first><last>Silfverberg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Honolulu</address>
      <month>February</month>
      <year>2019</year>
      <venue>computel</venue>
    </meta>
    <frontmatter>
      <url hash="fe3011f3">W19-6000</url>
      <bibkey>ws-2019-use</bibkey>
    </frontmatter>
    <paper id="1">
      <title>An Online Platform for Community-Based Language Description and Documentation</title>
      <author><first>Rebecca</first><last>Everson</last></author>
      <author><first>Wolf</first><last>Honore</last></author>
      <author><first>Scott</first><last>Grimm</last></author>
      <pages>1–5</pages>
      <url hash="f064aeb5">W19-6001</url>
      <bibkey>everson-etal-2019-online</bibkey>
    </paper>
    <paper id="2">
      <title>Developing without developers: choosing labor-saving tools for language documentation apps</title>
      <author><first>Luke</first><last>Gessler</last></author>
      <pages>6–13</pages>
      <url hash="4c38b7f0">W19-6002</url>
      <bibkey>gessler-2019-developing</bibkey>
    </paper>
    <paper id="3">
      <title>Future Directions in Technological Support for Language Documentation</title>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Ben</first><last>Foley</last></author>
      <author><first>Nay</first><last>San</last></author>
      <pages>14–22</pages>
      <url hash="7743db63">W19-6003</url>
      <bibkey>van-esch-etal-2019-future</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>OCR</fixed-case> evaluation tools for the 21st century</title>
      <author><first>Eddie Antonio</first><last>Santos</last></author>
      <pages>23–27</pages>
      <url hash="8f9209e3">W19-6004</url>
      <bibkey>santos-2019-ocr</bibkey>
    </paper>
    <paper id="5">
      <title>Handling cross-cutting properties in automatic inference of lexical classes: A case study of Chintang</title>
      <author><first>Olga</first><last>Zamaraeva</last></author>
      <author><first>Kristen</first><last>Howell</last></author>
      <author><first>Emily M.</first><last>Bender</last></author>
      <pages>28–38</pages>
      <url hash="36c94a67">W19-6005</url>
      <bibkey>zamaraeva-etal-2019-handling</bibkey>
    </paper>
    <paper id="6">
      <title>Finding <fixed-case>S</fixed-case>ami Cognates with a Character-Based <fixed-case>NMT</fixed-case> Approach</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <pages>39–45</pages>
      <url hash="3cc3d5ea">W19-6006</url>
      <bibkey>hamalainen-rueter-2019-finding</bibkey>
    </paper>
    <paper id="7">
      <title>Seeing more than whitespace — Tokenisation and disambiguation in a <fixed-case>N</fixed-case>orth <fixed-case>S</fixed-case>ámi grammar checker</title>
      <author><first>Linda</first><last>Wiechetek</last></author>
      <author><first>Sjur Nørstebø</first><last>Moshagen</last></author>
      <author><first>Kevin Brubeck</first><last>Unhammer</last></author>
      <pages>46–55</pages>
      <url hash="8ed0489c">W19-6007</url>
      <bibkey>wiechetek-etal-2019-seeing</bibkey>
    </paper>
    <paper id="8">
      <title>Corpus of usage examples: What is it good for?</title>
      <author><first>Timofey</first><last>Arkhangelskiy</last></author>
      <pages>56–63</pages>
      <url hash="6d054079">W19-6008</url>
      <bibkey>arkhangelskiy-2019-corpus</bibkey>
    </paper>
    <paper id="9">
      <title>A Preliminary <fixed-case>P</fixed-case>lains <fixed-case>C</fixed-case>ree Speech Synthesizer</title>
      <author><first>Atticus</first><last>Harrigan</last></author>
      <author><first>Antti</first><last>Arppe</last></author>
      <author><first>Timothy</first><last>Mills</last></author>
      <pages>64–73</pages>
      <url hash="33fd22e6">W19-6009</url>
      <bibkey>harrigan-etal-2019-preliminary</bibkey>
    </paper>
    <paper id="10">
      <title>A biscriptual morphological transducer for <fixed-case>C</fixed-case>rimean <fixed-case>T</fixed-case>atar</title>
      <author><first>Francis M.</first><last>Tyers</last></author>
      <author><first>Jonathan</first><last>Washington</last></author>
      <author><first>Darya</first><last>Kavitskaya</last></author>
      <author><first>Memduh</first><last>Gökırmak</last></author>
      <author><first>Nick</first><last>Howell</last></author>
      <author><first>Remziye</first><last>Berberova</last></author>
      <pages>74–80</pages>
      <url hash="d9ad0d1b">W19-6010</url>
      <bibkey>tyers-etal-2019-biscriptual</bibkey>
    </paper>
    <paper id="11">
      <title>Improving Low-Resource Morphological Learning with Intermediate Forms from Finite State Transducers</title>
      <author><first>Sarah</first><last>Moeller</last></author>
      <author><first>Ghazaleh</first><last>Kazeminejad</last></author>
      <author><first>Andrew</first><last>Cowell</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>81–86</pages>
      <url hash="7b6f7ae4">W19-6011</url>
      <bibkey>moeller-etal-2019-improving</bibkey>
    </paper>
    <paper id="12">
      <title>Bootstrapping a Neural Morphological Analyzer for <fixed-case>S</fixed-case>t. <fixed-case>L</fixed-case>awrence <fixed-case>I</fixed-case>sland <fixed-case>Y</fixed-case>upik from a Finite-State Transducer</title>
      <author><first>Lane</first><last>Schwartz</last></author>
      <author><first>Emily</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Hunt</last></author>
      <author><first>Sylvia L.R.</first><last>Schreiner</last></author>
      <pages>87–96</pages>
      <url hash="122f610e">W19-6012</url>
      <bibkey>schwartz-etal-2019-bootstrapping</bibkey>
    </paper>
  </volume>
  <volume id="61" ingest-date="2019-09-30">
    <meta>
      <booktitle>Proceedings of the 22nd Nordic Conference on Computational Linguistics</booktitle>
      <url hash="dfac8b1a">W19-61</url>
      <editor><first>Mareike</first><last>Hartmann</last></editor>
      <editor><first>Barbara</first><last>Plank</last></editor>
      <publisher>Linköping University Electronic Press</publisher>
      <address>Turku, Finland</address>
      <month>September–October</month>
      <year>2019</year>
      <venue>nodalida</venue>
    </meta>
    <frontmatter>
      <url hash="17f1b4f0">W19-6100</url>
      <bibkey>ws-2019-nordic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Comparison between <fixed-case>NMT</fixed-case> and <fixed-case>PBSMT</fixed-case> Performance for Translating Noisy User-Generated Content</title>
      <author><first>José Carlos</first><last>Rosales Núñez</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <pages>2–14</pages>
      <abstract>This work compares the performances achieved by Phrase-Based Statistical Machine Translation systems (PB-SMT) and attention-based Neuronal Machine Translation systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models.</abstract>
      <url hash="00c203c5">W19-6101</url>
      <bibkey>rosales-nunez-etal-2019-comparison</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="2">
      <title>Bootstrapping <fixed-case>UD</fixed-case> treebanks for Delexicalized Parsing</title>
      <author><first>Prasanth</first><last>Kolachina</last></author>
      <author><first>Aarne</first><last>Ranta</last></author>
      <pages>15–24</pages>
      <abstract>Standard approaches to treebanking traditionally employ a waterfall model (Sommerville, 2010), where annotation guidelines guide the annotation process and insights from the annotation process in turn lead to subsequent changes in the annotation guidelines. This process remains a very expensive step in creating linguistic resources for a target language, necessitates both linguistic expertise and manual effort to develop the annotations and is subject to inconsistencies in the annotation due to human errors. In this paper, we propose an alternative approach to treebanking—one that requires writing grammars. This approach is motivated specifically in the context of Universal Dependencies, an effort to develop uniform and cross-lingually consistent treebanks across multiple languages. We show here that a bootstrapping approach to treebanking via interlingual grammars is plausible and useful in a process where grammar engineering and treebanking are jointly pursued when creating resources for the target language. We demonstrate the usefulness of synthetic treebanks in the task of delexicalized parsing. Our experiments reveal that simple models for treebank generation are cheaper than human annotated treebanks, especially in the lower ends of the learning curves for delexicalized parsing, which is relevant in particular in the context of low-resource languages.</abstract>
      <url hash="70875fc3">W19-6102</url>
      <bibkey>kolachina-ranta-2019-bootstrapping</bibkey>
    </paper>
    <paper id="3">
      <title>Lexical Resources for Low-Resource <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> Tagging in Neural Times</title>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Sigrid</first><last>Klerke</last></author>
      <pages>25–34</pages>
      <abstract>More and more evidence is appearing that integrating symbolic lexical knowledge into neural models aids learning. This contrasts the widely-held belief that neural networks largely learn their own feature representations. For example, recent work has shows benefits of integrating lexicons to aid cross-lingual part-of-speech (PoS). However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources. This paper seeks to fill this gap by providing a thorough analysis on the contributions of lexical resources for cross-lingual PoS tagging in neural times.</abstract>
      <url hash="4f34c830">W19-6103</url>
      <bibkey>plank-klerke-2019-lexical</bibkey>
    </paper>
    <paper id="4">
      <title>Gender Bias in Pretrained <fixed-case>S</fixed-case>wedish Embeddings</title>
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <author><first>Fredrik</first><last>Olsson</last></author>
      <pages>35–43</pages>
      <abstract>This paper investigates the presence of gender bias in pretrained Swedish embeddings. We focus on a scenario where names are matched with occupations, and we demonstrate how a number of standard pretrained embeddings handle this task. Our experiments show some significant differences between the pretrained embeddings, with word-based methods showing the most bias and contextualized language models showing the least. We also demonstrate that the previously proposed debiasing method does not affect the performance of the various embeddings in this scenario.</abstract>
      <url hash="63d05817">W19-6104</url>
      <bibkey>sahlgren-olsson-2019-gender</bibkey>
    </paper>
    <paper id="5">
      <title>A larger-scale evaluation resource of terms and their shift direction for diachronic lexical semantics</title>
      <author><first>Astrid</first><last>van Aggelen</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Laura</first><last>Hollink</last></author>
      <author><first>Jacco</first><last>van Ossenbruggen</last></author>
      <pages>44–54</pages>
      <abstract>Determining how words have changed their meaning is an important topic in Natural Language Processing. However, evaluations of methods to characterise such change have been limited to small, handcrafted resources. We introduce an English evaluation set which is larger, more varied, and more realistic than seen to date, with terms derived from a historical thesaurus. Moreover, the dataset is unique in that it represents change as a shift from the term of interest to a WordNet synset. Using the synset lemmas, we can use this set to evaluate (standard) methods that detect change between word pairs, as well as (adapted) methods that detect the change between a term and a sense overall. We show that performance on the new data set is much lower than earlier reported findings, setting a new standard.</abstract>
      <url hash="2044ddc4">W19-6105</url>
      <bibkey>van-aggelen-etal-2019-larger</bibkey>
    </paper>
    <paper id="6">
      <title>Some steps towards the generation of diachronic <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>ets</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last></author>
      <pages>55–64</pages>
      <abstract>We apply hyperbolic embeddings to trace the dynamics of change of conceptual-semantic relationships in a large diachronic scientific corpus (200 years). Our focus is on emerging scientific fields and the increasingly specialized terminology establishing around them. Reproducing high-quality hierarchical structures such as WordNet on a diachronic scale is a very difficult task. Hyperbolic embeddings can map partial graphs into low dimensional, continuous hierarchical spaces, making more explicit the latent structure of the input. We show that starting from simple lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields.</abstract>
      <url hash="fa571567">W19-6106</url>
      <bibkey>bizzoni-etal-2019-steps</bibkey>
    </paper>
    <paper id="7">
      <title>An evaluation of <fixed-case>C</fixed-case>zech word embeddings</title>
      <author><first>Karolína</first><last>Hořeňovská</last></author>
      <pages>65–75</pages>
      <abstract>We present an evaluation of Czech low-dimensional distributed word representations, also known as word embeddings. We describe five different approaches to training the models and three different corpora used in training. We evaluate the resulting models on five different datasets, report the results and provide their further analysis.</abstract>
      <url hash="fdd3300c">W19-6107</url>
      <bibkey>horenovska-2019-evaluation</bibkey>
    </paper>
    <paper id="8">
      <title>Language Modeling with Syntactic and Semantic Representation for Sentence Acceptability Predictions</title>
      <author><first>Adam</first><last>Ek</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Shalom</first><last>Lappin</last></author>
      <pages>76–85</pages>
      <abstract>In this paper, we investigate the effect of enhancing lexical embeddings in LSTM language models (LM) with syntactic and semantic representations. We evaluate the language models using perplexity, and we evaluate the performance of the models on the task of predicting human sentence acceptability judgments. We train LSTM language models on sentences automatically annotated with universal syntactic dependency roles (Nivre, 2016), dependency depth and universal semantic tags (Abzianidze et al., 2017) to predict sentence acceptability judgments. Our experiments indicate that syntactic tags lower perplexity, while semantic tags increase it. Our experiments also show that neither syntactic nor semantic tags improve the performance of LSTM language models on the task of predicting sentence acceptability judgments.</abstract>
      <url hash="4c22d125">W19-6108</url>
      <bibkey>ek-etal-2019-language</bibkey>
      <pwccode url="https://github.com/gu-clasp/predicting-acceptability" additional="false">gu-clasp/predicting-acceptability</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="9">
      <title>Comparing linear and neural models for competitive <fixed-case>MWE</fixed-case> identification</title>
      <author><first>Hazem Al</first><last>Saied</last></author>
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <pages>86–96</pages>
      <abstract>In this paper, we compare the use of linear versus neural classifiers in a greedy transition system for MWE identification. Both our linear and neural models achieve a new state-of-the-art on the PARSEME 1.1 shared task data sets, comprising 20 languages. Surprisingly, our best model is a simple feed-forward network with one hidden layer, although more sophisticated (recurrent) architectures were tested. The feedback from this study is that tuning a SVM is rather straightforward, whereas tuning our neural system revealed more challenging. Given the number of languages and the variety of linguistic phenomena to handle for the MWE identification task, we have designed an accurate tuning procedure, and we show that hyperparameters are better selected by using a majority-vote within random search configurations rather than a simple best configuration selection. Although the performance is rather good (better than both the best shared task system and the average of the best per-language results), further work is needed to improve the generalization power, especially on unseen MWEs.</abstract>
      <url hash="727b19a0">W19-6109</url>
      <bibkey>saied-etal-2019-comparing</bibkey>
    </paper>
    <paper id="10">
      <title>Syntax-based identification of light-verb constructions</title>
      <author><first>Silvio Ricardo</first><last>Cordeiro</last></author>
      <author><first>Marie</first><last>Candito</last></author>
      <pages>97–104</pages>
      <abstract>This paper analyzes results on light-verb construction identification from the PARSEME shared-task, distinguishing between simple cases that could be directly learned from training data from more complex cases that require an extra level of semantic processing. We propose a simple baseline that beats the state of the art for the simple cases, and couple it with another simple baseline to handle the complex cases. We additionally present two other classifiers based on a richer set of features, with results surpassing the state of the art by 8 percentage points.</abstract>
      <url hash="cabdd0ac">W19-6110</url>
      <bibkey>cordeiro-candito-2019-syntax</bibkey>
    </paper>
    <paper id="11">
      <title>Comparing the Performance of Feature Representations for the Categorization of the Easy-to-Read Variety vs Standard Language</title>
      <author><first>Marina</first><last>Santini</last></author>
      <author><first>Benjamin</first><last>Danielsson</last></author>
      <author><first>Arne</first><last>Jönsson</last></author>
      <pages>105–114</pages>
      <abstract>We explore the effectiveness of four feature representations – bag-of-words, word embeddings, principal components and autoencoders – for the binary categorization of the easy-to-read variety vs standard language. Standard language refers to the ordinary language variety used by a population as a whole or by a community, while the “easy-to-read” variety is a simpler (or a simplified) version of the standard language. We test the efficiency of these feature representations on three corpora, which differ in size, class balance, unit of analysis, language and topic. We rely on supervised and unsupervised machine learning algorithms. Results show that bag-of-words is a robust and straightforward feature representation for this task and performs well in many experimental settings. Its performance is equivalent or equal to the performance achieved with principal components and autoencorders, whose preprocessing is however more time-consuming. Word embeddings are less accurate than the other feature representations for this classification task.</abstract>
      <url hash="8e412137">W19-6111</url>
      <bibkey>santini-etal-2019-comparing</bibkey>
    </paper>
    <paper id="12">
      <title>Unsupervised Inference of Object Affordance from Text Corpora</title>
      <author><first>Michele</first><last>Persiani</last></author>
      <author><first>Thomas</first><last>Hellström</last></author>
      <pages>115–120</pages>
      <abstract>Affordances denote actions that can be performed in the presence of different objects, or possibility of action in an environment. In robotic systems, affordances and actions may suffer from poor semantic generalization capabilities due to the high amount of required hand-crafted specifications. To alleviate this issue, we propose a method to mine for object-action pairs in free text corpora, successively training and evaluating different prediction models of affordance based on word embeddings.</abstract>
      <url hash="6523e0b2">W19-6112</url>
      <bibkey>persiani-hellstrom-2019-unsupervised</bibkey>
    </paper>
    <paper id="13">
      <title>Annotating evaluative sentences for sentiment analysis: a dataset for <fixed-case>N</fixed-case>orwegian</title>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>121–130</pages>
      <abstract>This paper documents the creation of a large-scale dataset of evaluative sentences – i.e. both subjective and objective sentences that are found to be sentiment-bearing – based on mixed-domain professional reviews from various news-sources. We present both the annotation scheme and first results for classification experiments. The effort represents a step toward creating a Norwegian dataset for fine-grained sentiment analysis.</abstract>
      <url hash="3855238b">W19-6113</url>
      <bibkey>maehlum-etal-2019-annotating</bibkey>
      <pwccode url="https://github.com/ltgoslo/norec_eval" additional="false">ltgoslo/norec_eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/norec">NoReC</pwcdataset>
    </paper>
    <paper id="14">
      <title>An Unsupervised Query Rewriting Approach Using N-gram Co-occurrence Statistics to Find Similar Phrases in Large Text Corpora</title>
      <author><first>Hans</first><last>Moen</last></author>
      <author><first>Laura-Maria</first><last>Peltonen</last></author>
      <author><first>Henry</first><last>Suhonen</last></author>
      <author><first>Hanna-Maria</first><last>Matinolli</last></author>
      <author><first>Riitta</first><last>Mieronkoski</last></author>
      <author><first>Kirsi</first><last>Telen</last></author>
      <author><first>Kirsi</first><last>Terho</last></author>
      <author><first>Tapio</first><last>Salakoski</last></author>
      <author><first>Sanna</first><last>Salanterä</last></author>
      <pages>131–139</pages>
      <abstract>We present our work towards developing a system that should find, in a large text corpus, contiguous phrases expressing similar meaning as a query phrase of arbitrary length. Depending on the use case, this task can be seen as a form of (phrase-level) query rewriting. The suggested approach works in a generative manner, is unsupervised and uses a combination of a semantic word n-gram model, a statistical language model and a document search engine. A central component is a distributional semantic model containing word n-grams vectors (or embeddings) which models semantic similarities between n-grams of different order. As data we use a large corpus of PubMed abstracts. The presented experiment is based on manual evaluation of extracted phrases for arbitrary queries provided by a group of evaluators. The results indicate that the proposed approach is promising and that the use of distributional semantic models trained with uni-, bi- and trigrams seems to work better than a more traditional unigram model.</abstract>
      <url hash="7c50a0a4">W19-6114</url>
      <bibkey>moen-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="15">
      <title>Compiling and Filtering <fixed-case>P</fixed-case>ar<fixed-case>I</fixed-case>ce: An <fixed-case>E</fixed-case>nglish-<fixed-case>I</fixed-case>celandic Parallel Corpus</title>
      <author><first>Starkaður</first><last>Barkarson</last></author>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <pages>140–145</pages>
      <abstract>We present ParIce, a new English-Icelandic parallel corpus. This is the first parallel corpus built for the purposes of language technology development and research for Icelandic, although some Icelandic texts can be found in various other multilingual parallel corpora. We map out which Icelandic texts are available for these purposes, collect aligned data and align other bilingual texts we acquired. We describe the alignment process and how we filter the data to weed out noise and bad alignments. In total we collected 43 million Icelandic words in 4.3 million aligned segment pairs, but after filtering, our corpus includes 38.8 million Icelandic words in 3.5 million segment pairs. We estimate that approximately 5% of the corpus data is noise or faulty alignments while more than 50% of the segments we deleted were faulty. We estimate that our filtering process reduced the number of faulty segments in the corpus by more than 60% while only reducing the number of good alignments by approximately 8%.</abstract>
      <url hash="d6950bc5">W19-6115</url>
      <bibkey>barkarson-steingrimsson-2019-compiling</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tilde-model-corpus">Tilde MODEL Corpus</pwcdataset>
    </paper>
    <paper id="16">
      <title><fixed-case>DIM</fixed-case>: The Database of <fixed-case>I</fixed-case>celandic Morphology</title>
      <author><first>Kristín</first><last>Bjarnadóttir</last></author>
      <author><first>Kristín Ingibjörg</first><last>Hlynsdóttir</last></author>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <pages>146–154</pages>
      <abstract>The topic of this paper is The Database of Icelandic Morphology (DIM), a multipurpose linguistic resource, created for use in language technology, as a reference for the general public in Iceland, and for use in research on the Icelandic language. DIM contains inflectional paradigms and analysis of word formation, with a vocabulary of approx. 285,000 lemmas. DIM is based on The Database of Modern Icelandic Inflection, which has been in use since 2004.</abstract>
      <url hash="c4ce9a32">W19-6116</url>
      <bibkey>bjarnadottir-etal-2019-dim</bibkey>
    </paper>
    <paper id="17">
      <title>Tools for supporting language learning for Sakha</title>
      <author><first>Sardana</first><last>Ivanova</last></author>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>155–163</pages>
      <abstract>This paper presents an overview of the available linguistic resources for the Sakha language, and presents new tools for supporting language learning for Sakha. The essential resources include a morphological analyzer, digital dictionaries, and corpora of Sakha texts. Based on these resources, we implement a language-learning environment for Sakha in the Revita CALL platform. We extended an earlier, preliminary version of the morphological analyzer/transducer, built on the Apertium finite-state platform. The analyzer currently has an adequate level of coverage, between 86% and 89% on two Sakha corpora. Revita is a freely available online language learning platform for learners beyond the beginner level. We describe the tools for Sakha currently integrated into the Revita platform. To the best of our knowledge, at present, this is the first large-scale project undertaken to support intermediate-advanced learners of a minority Siberian language.</abstract>
      <url hash="f92e3010">W19-6117</url>
      <bibkey>ivanova-etal-2019-tools</bibkey>
    </paper>
    <paper id="18">
      <title>Inferring morphological rules from small examples using 0/1 linear programming</title>
      <author><first>Ann</first><last>Lillieström</last></author>
      <author><first>Koen</first><last>Claessen</last></author>
      <author><first>Nicholas</first><last>Smallbone</last></author>
      <pages>164–174</pages>
      <abstract>We show how to express the problem of finding an optimal morpheme segmentation from a set of labelled words as a 0/1 linear programming problem, and how to build on this to analyse a language’s morphology. The approach works even when there is very little training data available.</abstract>
      <url hash="85782ca1">W19-6118</url>
      <bibkey>lilliestrom-etal-2019-inferring</bibkey>
    </paper>
    <paper id="19">
      <title>Lexicon information in neural sentiment analysis: a multi-task learning approach</title>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>175–186</pages>
      <abstract>This paper explores the use of multi-task learning (MTL) for incorporating external knowledge in neural models. Specifically, we show how MTL can enable a BiLSTM sentiment classifier to incorporate information from sentiment lexicons. Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian.</abstract>
      <url hash="b20eaffb">W19-6119</url>
      <bibkey>barnes-etal-2019-lexicon</bibkey>
      <pwccode url="https://github.com/ltgoslo/norsentlex" additional="true">ltgoslo/norsentlex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="20">
      <title>Aspect-Based Sentiment Analysis using <fixed-case>BERT</fixed-case></title>
      <author><first>Mickel</first><last>Hoang</last></author>
      <author><first>Oskar Alija</first><last>Bihorac</last></author>
      <author><first>Jacobo</first><last>Rouces</last></author>
      <pages>187–196</pages>
      <abstract>Sentiment analysis has become very popular in both research and business due to the increasing amount of opinionated text from Internet users. Standard sentiment analysis deals with classifying the overall sentiment of a text, but this doesn’t include other important information such as towards which entity, topic or aspect within the text the sentiment is directed. Aspect-based sentiment analysis (ABSA) is a more complex task that consists in identifying both sentiments and aspects. This paper shows the potential of using the contextual word representations from the pre-trained language model BERT, together with a fine-tuning method with additional generated text, in order to solve out-of-domain ABSA and outperform previous state-of-the-art results on SemEval-2015 Task 12 subtask 2 and SemEval-2016 Task 5. To the best of our knowledge, no other existing work has been done on out-of-domain ABSA for aspect classification.</abstract>
      <url hash="daea37d8">W19-6120</url>
      <bibkey>hoang-etal-2019-aspect</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="21">
      <title>Political Stance in <fixed-case>D</fixed-case>anish</title>
      <author><first>Rasmus</first><last>Lehmann</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>197–207</pages>
      <abstract>The task of stance detection consists of classifying the opinion within a text towards some target. This paper seeks to generate a dataset of quotes from Danish politicians, label this dataset to allow the task of stance detection to be performed, and present annotation guidelines to allow further expansion of the generated dataset. Furthermore, three models based on an LSTM architecture are designed, implemented and optimized to perform the task of stance detection for the generated dataset. Experiments are performed using conditionality and bi-directionality for these models, and using either singular word embeddings or averaged word embeddings for an entire quote, to determine the optimal model design. The simplest model design, applying neither conditionality or bi-directionality, and averaged word embeddings across quotes, yields the strongest results. Furthermore, it was found that inclusion of the quotes politician, and the party affiliation of the quoted politician, greatly improved performance of the strongest model.</abstract>
      <url hash="65f1da0f">W19-6121</url>
      <bibkey>lehmann-derczynski-2019-political</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/polstance">polstance</pwcdataset>
    </paper>
    <paper id="22">
      <title>Joint Rumour Stance and Veracity Prediction</title>
      <author><first>Anders Edelbo</first><last>Lillie</last></author>
      <author><first>Emil Refsgaard</first><last>Middelboe</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>208–221</pages>
      <abstract>The net is rife with rumours that spread through microblogs and social media. Not all the claims in these can be verified. However, recent work has shown that the stances alone that commenters take toward claims can be sufficiently good indicators of claim veracity, using e.g. an HMM that takes conversational stance sequences as the only input. Existing results are monolingual (English) and mono-platform (Twitter). This paper introduces a stance-annotated Reddit dataset for the Danish language, and describes various implementations of stance classification models. Of these, a Linear SVM provides predicts stance best, with 0.76 accuracy / 0.42 macro F1. Stance labels are then used to predict veracity across platforms and also across languages, training on conversations held in one language and using the model on conversations held in another. In our experiments, monolinugal scores reach stance-based veracity accuracy of 0.83 (F1 0.68); applying the model across languages predicts veracity of claims with an accuracy of 0.82 (F1 0.67). This demonstrates the surprising and powerful viability of transferring stance-based veracity prediction across languages.</abstract>
      <url hash="71f29b16">W19-6122</url>
      <bibkey>lillie-etal-2019-joint</bibkey>
      <pwccode url="https://github.com/danish-stance-detectors/RumourResolution" additional="true">danish-stance-detectors/RumourResolution</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dast">DAST</pwcdataset>
    </paper>
    <paper id="23">
      <title>Named-Entity Recognition for <fixed-case>N</fixed-case>orwegian</title>
      <author><first>Bjarte</first><last>Johansen</last></author>
      <pages>222–231</pages>
      <abstract>NER is the task of recognizing and demarcating the segments of a document that are part of a name and which type of name it is. We use 4 different categories of names: Locations (LOC), miscellaneous (MISC), organizations (ORG), and persons (PER). Even though we employ state of the art methods—including sub-word embeddings—that work well for English, we are unable to reproduce the same success for the Norwegian written forms. However, our model performs better than any previous research on Norwegian text. The study also presents the first NER for Nynorsk. Lastly, we find that by combining Nynorsk and Bokmål into one training corpus we improve the performance of our model on both languages.</abstract>
      <url hash="f0e9a112">W19-6123</url>
      <bibkey>johansen-2019-named</bibkey>
      <pwccode url="https://github.com/ljos/navnkjenner" additional="false">ljos/navnkjenner</pwccode>
    </paper>
    <paper id="24">
      <title>Projecting named entity recognizers without annotated or parallel corpora</title>
      <author><first>Jue</first><last>Hou</last></author>
      <author><first>Maximilian</first><last>Koppatz</last></author>
      <author><first>José María Hoya</first><last>Quecedo</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>232–241</pages>
      <abstract>Named entity recognition (NER) is a well-researched task in the field of NLP, which typically requires large annotated corpora for training usable models. This is a problem for languages which lack large annotated corpora, such as Finnish. We propose an approach to create a named entity recognizer with no annotated or parallel documents, by leveraging strong NER models that exist for English. We automatically gather a large amount of <i>chronologically matched</i> data in two languages, then project named entity annotations from the English documents onto the Finnish ones, by resolving the matches with limited linguistic rules. We use this “artificially” annotated data to train a BiLSTM-CRF model. Our results show that this method can produce annotated instances with high precision, and the resulting model achieves state-of-the-art performance.</abstract>
      <url hash="7145045a">W19-6124</url>
      <bibkey>hou-etal-2019-projecting</bibkey>
    </paper>
    <paper id="25">
      <title>Template-free Data-to-Text Generation of <fixed-case>F</fixed-case>innish Sports News</title>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Riina</first><last>Kekki</last></author>
      <author><first>Tapio</first><last>Salakoski</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>242–252</pages>
      <abstract>News articles such as sports game reports are often thought to closely follow the underlying game statistics, but in practice they contain a notable amount of background knowledge, interpretation, insight into the game, and quotes that are not present in the official statistics. This poses a challenge for automated data-to-text news generation with real-world news corpora as training data. We report on the development of a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product. The new dataset and system source code are available for research purposes.</abstract>
      <url hash="b943c31f">W19-6125</url>
      <bibkey>kanerva-etal-2019-template</bibkey>
      <pwccode url="https://github.com/scoopmatic/finnish-hockey-news-generation-paper" additional="false">scoopmatic/finnish-hockey-news-generation-paper</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ice-hockey-news-dataset">Ice Hockey News Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
    </paper>
    <paper id="26">
      <title>Matching Keys and Encrypted Manuscripts</title>
      <author><first>Eva</first><last>Pettersson</last></author>
      <author><first>Beata</first><last>Megyesi</last></author>
      <pages>253–261</pages>
      <abstract>Historical cryptology is the study of historical encrypted messages aiming at their decryption by analyzing the mathematical, linguistic and other coding patterns and their historical context. In libraries and archives we can find quite a lot of ciphers, as well as keys describing the method used to transform the plaintext message into a ciphertext. In this paper, we present work on automatically mapping keys to ciphers to reconstruct the original plaintext message, and use language models generated from historical texts to guess the underlying plaintext language.</abstract>
      <url hash="2efb698e">W19-6126</url>
      <bibkey>pettersson-megyesi-2019-matching</bibkey>
    </paper>
    <paper id="27">
      <title>Perceptual and acoustic analysis of voice similarities between parents and young children</title>
      <author><first>Evgeniia</first><last>Rykova</last></author>
      <author><first>Stefan</first><last>Werner</last></author>
      <pages>262–271</pages>
      <abstract>Human voice provides the means for verbal communication and forms a part of personal identity. Due to genetic and environmental factors, a voice of a child should resemble the voice of her parent(s), but voice similarities between parents and young children are underresearched. Read-aloud speech of Finnish-speaking and Russian-speaking parent-child pairs was subject to perceptual and multi-step instrumental and statistical analysis. Finnish-speaking listeners could not discriminate family pairs auditorily in an XAB paradigm, but the Russian-speaking listeners’ mean accuracy of answers reached 72.5%. On average, in both language groups family-internal f0 similarities were stronger than family-external, with parents showing greater family-internal similarities than children. Auditory similarities did not reflect acoustic similarities in a straightforward way.</abstract>
      <url hash="2bbdbf52">W19-6127</url>
      <bibkey>rykova-werner-2019-perceptual</bibkey>
    </paper>
    <paper id="28">
      <title>Enhancing Natural Language Understanding through Cross-Modal Interaction: Meaning Recovery from Acoustically Noisy Speech</title>
      <author><first>Ozge</first><last>Alacam</last></author>
      <pages>272–280</pages>
      <abstract>Cross-modality between vision and language is a key component for effective and efficient communication, and human language processing mechanism successfully integrates information from various modalities to extract the intended meaning. However, incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges for a successful communication. In that case, an incompleteness in one channel can be compensated by information from another one. In this paper, by conducting visual-world paradigm, we investigated the dynamics between syntactically possible gap fillers and the visual arrangements in incomplete German sentences and their effect on overall sentence interpretation.</abstract>
      <url hash="e3c8bea5">W19-6128</url>
      <bibkey>alacam-2019-enhancing</bibkey>
    </paper>
    <paper id="29">
      <title>Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations</title>
      <author><first>Aarne</first><last>Talman</last></author>
      <author><first>Antti</first><last>Suni</last></author>
      <author><first>Hande</first><last>Celikkanat</last></author>
      <author><first>Sofoklis</first><last>Kakouros</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <author><first>Martti</first><last>Vainio</last></author>
      <pages>281–290</pages>
      <abstract>In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text. To our knowledge this will be the largest publicly available dataset with prosodic labels. We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature-based classifiers to neural network systems for the prediction of discretized prosodic prominence. We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10% of the training data. Finally we discuss the dataset in light of the results and point to future research and plans for further improving both the dataset and methods of predicting prosodic prominence from text. The dataset and the code for the models will be made publicly available.</abstract>
      <url hash="c7779ed6">W19-6129</url>
      <bibkey>talman-etal-2019-predicting</bibkey>
      <pwccode url="https://github.com/Helsinki-NLP/prosody" additional="false">Helsinki-NLP/prosody</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/helsinki-prosody-corpus">Helsinki Prosody Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/libritts">LibriTTS</pwcdataset>
    </paper>
    <paper id="30">
      <title>Toward Multilingual Identification of Online Registers</title>
      <author><first>Veronika</first><last>Laippala</last></author>
      <author><first>Roosa</first><last>Kyllönen</last></author>
      <author><first>Jesse</first><last>Egbert</last></author>
      <author><first>Douglas</first><last>Biber</last></author>
      <author><first>Sampo</first><last>Pyysalo</last></author>
      <pages>292–297</pages>
      <abstract>We consider cross- and multilingual text classification approaches to the identification of online registers (genres), i.e. text varieties with specific situational characteristics. Register is the most important predictor of linguistic variation, and register information could improve the potential of online data for many applications. We introduce the first manually annotated non-English corpus of online registers featuring the full range of linguistic variation found online. The data set consists of 2,237 Finnish documents and follows the register taxonomy developed for the Corpus of Online Registers of English (CORE). Using CORE and the newly introduced corpus, we demonstrate the feasibility of cross-lingual register identification using a simple approach based on convolutional neural networks and multilingual word embeddings. We further find that register identification results can be improved through multilingual training even when a substantial number of annotations is available in the target language.</abstract>
      <url hash="b14c6949">W19-6130</url>
      <bibkey>laippala-etal-2019-toward</bibkey>
    </paper>
    <paper id="31">
      <title>A Wide-Coverage Symbolic Natural Language Inference System</title>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <pages>298–303</pages>
      <abstract>We present a system for Natural Language Inference which uses a dynamic semantics converter from abstract syntax trees to Coq types. It combines the fine-grainedness of a dynamic semantics system with the powerfulness of a state-of-the-art proof assistant, like Coq. We evaluate the system on all sections of the FraCaS test suite, excluding section 6. This is the first system that does a complete run on the anaphora and ellipsis sections of the FraCaS. It has a better overall accuracy than any previous system.</abstract>
      <url hash="f206406a">W19-6131</url>
      <bibkey>chatzikyriakidis-bernardy-2019-wide</bibkey>
    </paper>
    <paper id="32">
      <title>Ensembles of Neural Morphological Inflection Models</title>
      <author><first>Ilmari</first><last>Kylliäinen</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <pages>304–309</pages>
      <abstract>We investigate different ensemble learning techniques for neural morphological inflection using bidirectional LSTM encoder-decoder models with attention. We experiment with weighted and unweighted majority voting and bagging. We find that all investigated ensemble methods lead to improved accuracy over a baseline of a single model. However, contrary to expectation based on earlier work by Najafi et al. (2018) and Silfverberg et al. (2017), weighting does not deliver clear benefits. Bagging was found to underperform plain voting ensembles in general.</abstract>
      <url hash="61ef40bf">W19-6132</url>
      <bibkey>kylliainen-silfverberg-2019-ensembles</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>N</fixed-case>efnir: A high accuracy lemmatizer for <fixed-case>I</fixed-case>celandic</title>
      <author><first>Svanhvít Lilja</first><last>Ingólfsdóttir</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <author><first>Jón Friðrik</first><last>Daðason</last></author>
      <author><first>Kristín</first><last>Bjarnadóttir</last></author>
      <pages>310–315</pages>
      <abstract>Lemmatization, finding the basic morphological form of a word in a corpus, is an important step in many natural language processing tasks when working with morphologically rich languages. We describe and evaluate Nefnir, a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules, derived from a large morphological database, to lemmatize tagged text. Evaluation shows that for correctly tagged text, Nefnir obtains an accuracy of 99.55%, and for text tagged with a PoS tagger, the accuracy obtained is 96.88%.</abstract>
      <url hash="c3e385eb">W19-6133</url>
      <bibkey>ingolfsdottir-etal-2019-nefnir</bibkey>
    </paper>
    <paper id="34">
      <title>Natural Language Processing in Policy Evaluation: Extracting Policy Conditions from <fixed-case>IMF</fixed-case> Loan Agreements</title>
      <author><first>Joakim</first><last>Åkerström</last></author>
      <author><first>Adel</first><last>Daoud</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <pages>316–320</pages>
      <abstract>Social science researchers often use text as the raw data in investigations: for instance, when investigating the effects of IMF policies on the development of countries under IMF programs, researchers typically encode structured descriptions of the programs using a time-consuming manual effort. Making this process automatic may open up new opportunities in scaling up such investigations. As a first step towards automatizing this coding process, we describe an experiment where we apply a sentence classifier that automatically detects mentions of policy conditions in IMF loan agreements and divides them into different types. The results show that the classifier is generally able to detect the policy conditions, although some types are hard to distinguish.</abstract>
      <url hash="faabc840">W19-6134</url>
      <bibkey>akerstrom-etal-2019-natural</bibkey>
    </paper>
    <paper id="35">
      <title>Interconnecting lexical resources and word alignment: How do learners get on with particle verbs?</title>
      <author><first>David</first><last>Alfter</last></author>
      <author><first>Johannes</first><last>Graën</last></author>
      <pages>321–326</pages>
      <abstract>In this paper, we present a prototype for an online exercise aimed at learners of English and Swedish that serves multiple purposes. The exercise allows learners of the aforementioned languages to train their knowledge of particle verbs receiving clues from the exercise application. The user themselves decide which clue to receive and pay in virtual currency for each, which provides us with valuable information about the utility of the clues that we provide as well as the learners willingness to trade virtual currency versus accuracy of their choice. As resources, we use list with annotated levels from the proficiency scale defined by the Common European Framework of Reference (CEFR) and a multilingual corpus with syntactic dependency relations and word annotation for all language pairs. From the latter resource, we extract translation equivalents for particle verb construction together with a list of parallel corpus examples that can be used as clues in the exercise.</abstract>
      <url hash="8d054628">W19-6135</url>
      <bibkey>alfter-graen-2019-interconnecting</bibkey>
    </paper>
    <paper id="36">
      <title>May <fixed-case>I</fixed-case> Check Again? — A simple but efficient way to generate and use contextual dictionaries for Named Entity Recognition. Application to <fixed-case>F</fixed-case>rench Legal Texts.</title>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>Amaury</first><last>Fouret</last></author>
      <pages>327–332</pages>
      <abstract>In this paper we present a new method to learn a model robust to typos for a Named Entity Recognition task. Our improvement over existing methods helps the model to take into account the context of the sentence inside a justice decision in order to recognize an entity with a typo. We used state-of-the-art models and enriched the last layer of the neural network with high-level information linked with the potential of the word to be a certain type of entity. More precisely, we utilized the similarities between the word and the potential entity candidates the tagged sentence context. The experiments on a dataset of french justice decisions show a reduction of the relative F1-score error of 32%, upgrading the score obtained with the most competitive fine-tuned state-of-the-art system from 94.85% to 96.52%.</abstract>
      <url hash="b0cd6f86">W19-6136</url>
      <bibkey>barriere-fouret-2019-may</bibkey>
    </paper>
    <paper id="37">
      <title>Predicates as Boxes in <fixed-case>B</fixed-case>ayesian Semantics for Natural Language</title>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Rasmus</first><last>Blanck</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <author><first>Shalom</first><last>Lappin</last></author>
      <author><first>Aleksandre</first><last>Maskharashvili</last></author>
      <pages>333–337</pages>
      <abstract>In this paper, we present a Bayesian approach to natural language semantics. Our main focus is on the inference task in an environment where judgments require probabilistic reasoning. We treat nouns, verbs, adjectives, etc. as unary predicates, and we model them as boxes in a bounded domain. We apply Bayesian learning to satisfy constraints expressed as premises. In this way we construct a model, by specifying boxes for the predicates. The probability of the hypothesis (the conclusion) is evaluated against the model that incorporates the premises as constraints.</abstract>
      <url hash="04990054">W19-6137</url>
      <bibkey>bernardy-etal-2019-predicates</bibkey>
    </paper>
    <paper id="38">
      <title>Bornholmsk Natural Language Processing: Resources and Tools</title>
      <author><first>Leon</first><last>Derczynski</last></author>
      <author><first>Alex Speed</first><last>Kjeldsen</last></author>
      <pages>338–344</pages>
      <abstract>This paper introduces language processing resources and tools for Bornholmsk, a language spoken on the island of Bornholm, with roots in Danish and closely related to Scanian. This presents an overview of the language and available data, and the first NLP models for this living, minority Nordic language. Sammenfattnijng på borrijnholmst: Dæjnna artikkelijn introduserer natursprågsresurser å varktoi for borrijnholmst, ed språg a dær snakkes på ön Borrijnholm me rødder i danst å i nær familia me skånst. Artikkelijn gjer ed âuersyn âuer språged å di datan som fijnnes, å di fosste NLP modællarna for dætta læwenes nordiska minnretâlsspråged.</abstract>
      <url hash="88e2c62f">W19-6138</url>
      <bibkey>derczynski-kjeldsen-2019-bornholmsk</bibkey>
      <pwccode url="https://github.com/StrombergNLP/bornholmsk" additional="false">StrombergNLP/bornholmsk</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bornholmsk-parallel">bornholmsk_parallel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="39">
      <title>Morphosyntactic Disambiguation in an Endangered Language Setting</title>
      <author><first>Jeff</first><last>Ens</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Philippe</first><last>Pasquier</last></author>
      <pages>345–349</pages>
      <abstract>Endangered Uralic languages present a high variety of inflectional forms in their morphology. This results in a high number of homonyms in inflections, which introduces a lot of morphological ambiguity in sentences. Previous research has employed constraint grammars to address this problem, however CGs are often unable to fully disambiguate a sentence, and their development is labour intensive. We present an LSTM based model for automatically ranking morphological readings of sentences based on their quality. This ranking can be used to evaluate the existing CG disambiguators or to directly morphologically disambiguate sentences. Our approach works on a morphological abstraction and it can be trained with a very small dataset.</abstract>
      <url hash="82aba012">W19-6139</url>
      <bibkey>ens-etal-2019-morphosyntactic</bibkey>
    </paper>
    <paper id="40">
      <title>Tagging a <fixed-case>N</fixed-case>orwegian Dialect Corpus</title>
      <author><first>Andre</first><last>Kåsen</last></author>
      <author><first>Anders</first><last>Nøklestad</last></author>
      <author><first>Kristin</first><last>Hagen</last></author>
      <author><first>Joel</first><last>Priestley</last></author>
      <pages>350–355</pages>
      <abstract>This paper describes an evaluation of five data-driven part-of-speech (PoS) taggers for spoken Norwegian. The taggers all rely on different machine learning mechanisms: decision trees, hidden Markov models (HMMs), conditional random fields (CRFs), long-short term memory networks (LSTMs), and convolutional neural networks (CNNs). We go into some of the challenges posed by the task of tagging spoken, as opposed to written, language, and in particular a wide range of dialects as is found in the recordings of the LIA (Language Infrastructure made Accessible) project. The results show that the taggers based on either conditional random fields or neural networks perform much better than the rest, with the LSTM tagger getting the highest score.</abstract>
      <url hash="64060e6d">W19-6140</url>
      <bibkey>kasen-etal-2019-tagging</bibkey>
    </paper>
    <paper id="41">
      <title>The Lacunae of <fixed-case>D</fixed-case>anish Natural Language Processing</title>
      <author><first>Andreas</first><last>Kirkedal</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <author><first>Natalie</first><last>Schluter</last></author>
      <pages>356–362</pages>
      <abstract>Danish is a North Germanic language spoken principally in Denmark, a country with a long tradition of technological and scientific innovation. However, the language has received relatively little attention from a technological perspective. In this paper, we review Natural Language Processing (NLP) research, digital resources and tools which have been developed for Danish. We find that availability of models and tools is limited, which calls for work that lifts Danish NLP a step closer to the privileged languages. Dansk abstrakt: Dansk er et nordgermansk sprog, talt primært i kongeriget Danmark, et land med stærk tradition for teknologisk og videnskabelig innovation. Det danske sprog har imidlertid været genstand for relativt begrænset opmærksomhed, teknologisk set. I denne artikel gennemgår vi sprogteknologi-forskning, -ressourcer og -værktøjer udviklet for dansk. Vi konkluderer at der eksisterer et fåtal af modeller og værktøjer, hvilket indbyder til forskning som løfter dansk sprogteknologi i niveau med mere priviligerede sprog.</abstract>
      <url hash="5c51235d">W19-6141</url>
      <bibkey>kirkedal-etal-2019-lacunae</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="42">
      <title>Towards High Accuracy Named Entity Recognition for <fixed-case>I</fixed-case>celandic</title>
      <author><first>Svanhvít Lilja</first><last>Ingólfsdóttir</last></author>
      <author><first>Sigurjón</first><last>Þorsteinsson</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>363–369</pages>
      <abstract>We report on work in progress which consists of annotating an Icelandic corpus for named entities (NEs) and using it for training a named entity recognizer based on a Bidirectional Long Short-Term Memory model. Currently, we have annotated 7,538 NEs appearing in the first 200,000 tokens of a 1 million token corpus, MIM-GOLD, originally developed for serving as a gold standard for part-of-speech tagging. Our best performing model, trained on this subset of MIM-GOLD, and enriched with external word embeddings, obtains an overall F1 score of 81.3% when categorizing NEs into the following four categories: persons, locations, organizations and miscellaneous. Our preliminary results are promising, especially given the fact that 80% of MIM-GOLD has not yet been used for training.</abstract>
      <url hash="c567bcb7">W19-6142</url>
      <bibkey>ingolfsdottir-etal-2019-towards</bibkey>
    </paper>
    <paper id="43">
      <title>Neural Cross-Lingual Transfer and Limited Annotated Data for Named Entity Recognition in <fixed-case>D</fixed-case>anish</title>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>370–375</pages>
      <abstract>Named Entity Recognition (NER) has greatly advanced by the introduction of deep neural architectures. However, the success of these methods depends on large amounts of training data. The scarcity of publicly-available human-labeled datasets has resulted in limited evaluation of existing NER systems, as is the case for Danish. This paper studies the effectiveness of cross-lingual transfer for Danish, evaluates its complementarity to limited gold data, and sheds light on performance of Danish NER.</abstract>
      <url hash="13236d45">W19-6143</url>
      <bibkey>plank-2019-neural</bibkey>
      <pwccode url="https://github.com/bplank/danish_ner_transfer" additional="false">bplank/danish_ner_transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="44">
      <title>The Seemingly (Un)systematic Linking Element in <fixed-case>D</fixed-case>anish</title>
      <author><first>Sidsel</first><last>Boldsen</last></author>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <pages>376–380</pages>
      <abstract>The use of a linking element between compound members is a common phenomenon in Germanic languages. Still, the exact use and conditioning of such elements is a disputed topic in linguistics. In this paper we address the issue of predicting the use of linking elements in Danish. Following previous research that shows how the choice of linking element might be conditioned by phonology, we frame the problem as a language modeling task: Considering the linking elements -s/-∅ the problem becomes predicting what is most probable to encounter next, a syllable boundary or the joining element, ‘s’. We show that training a language model on this task reaches an accuracy of 94 %, and in the case of an unsupervised model, the accuracy reaches 80%.</abstract>
      <url hash="3862130d">W19-6144</url>
      <bibkey>boldsen-agirrezabal-2019-seemingly</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>LEGATO</fixed-case>: A flexible lexicographic annotation tool</title>
      <author><first>David</first><last>Alfter</last></author>
      <author><first>Therese Lindström</first><last>Tiedemann</last></author>
      <author><first>Elena</first><last>Volodina</last></author>
      <pages>382–388</pages>
      <abstract>This article is a report from an ongoing project aiming at analyzing lexical and grammatical competences of Swedish as a Second language (L2). To facilitate lexical analysis, we need access to metalinguistic information about relevant vocabulary that L2 learners can use and understand. The focus of the current article is on the lexical annotation of the vocabulary scope for a range of lexicographical aspects, such as morphological analysis, valency, types of multi-word units, etc. We perform parts of the analysis automatically, and other parts manually. The rationale behind this is that where there is no possibility to add information automatically, manual effort needs to be added. To facilitate the latter, a tool LEGATO has been designed, implemented and currently put to active testing.</abstract>
      <url hash="0df83b0d">W19-6145</url>
      <bibkey>alfter-etal-2019-legato</bibkey>
    </paper>
    <paper id="46">
      <title>The <fixed-case>OPUS</fixed-case> Resource Repository: An Open Package for Creating Parallel Corpora and Machine Translation Services</title>
      <author><first>Mikko</first><last>Aulamo</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>389–394</pages>
      <abstract>This paper presents a flexible and powerful system for creating parallel corpora and for running neural machine translation services. Our package provides a scalable data repository backend that offers transparent data pre-processing pipelines and automatic alignment procedures that facilitate the compilation of extensive parallel data sets from a variety of sources. Moreover, we develop a web-based interface that constitutes an intuitive frontend for end-users of the platform. The whole system can easily be distributed over virtual machines and implements a sophisticated permission system with secure connections and a flexible database for storing arbitrary metadata. Furthermore, we also provide an interface for neural machine translation that can run as a service on virtual machines, which also incorporates a connection to the data repository software.</abstract>
      <url hash="0694d499">W19-6146</url>
      <bibkey>aulamo-tiedemann-2019-opus</bibkey>
    </paper>
    <paper id="47">
      <title>Garnishing a phonetic dictionary for <fixed-case>ASR</fixed-case> intake</title>
      <author><first>Iben Nyholm</first><last>Debess</last></author>
      <author><first>Sandra Saxov</first><last>Lamhauge</last></author>
      <author><first>Peter Juel</first><last>Henrichsen</last></author>
      <pages>395–399</pages>
      <abstract>We present a new method for preparing a lexical-phonetic database as a resource for acoustic model training. The research is an offshoot of the ongoing Project Ravnur (Speech Recognition for Faroese), but the method is language-independent. At NODALIDA 2019 we demonstrate the method (called SHARP) online, showing how a traditional lexical-phonetic dictionary (with a very rich phone inventory) is transformed into an ASR-friendly database (with reduced phonetics, preventing data sparseness). The mapping procedure is informed by a corpus of speech transcripts. We conclude with a discussion on the benefits of a well-thought-out BLARK design (Basic Language Resource Kit), making tools like SHARP possible.</abstract>
      <url hash="6ebc64ea">W19-6147</url>
      <bibkey>debess-etal-2019-garnishing</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>D</fixed-case>ocria: Processing and Storing Linguistic Data with <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Marcus</first><last>Klang</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <pages>400–405</pages>
      <abstract>The availability of user-generated content has increased significantly over time. Wikipedia is one example of a corpora which spans a huge range of topics and is freely available. Storing and processing these corpora requires flexible documents models as they may contain malicious and incorrect data. Docria is a library which attempts to address this issue by providing a solution which can be used with small to large corpora, from laptops using Python interactively in a Jupyter notebook to clusters running map-reduce frameworks with optimized compiled code. Docria is available as open-source code.</abstract>
      <url hash="e7fafc0b">W19-6148</url>
      <bibkey>klang-nugues-2019-docria</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>U</fixed-case>ni<fixed-case>P</fixed-case>arse: A universal graph-based parsing toolkit</title>
      <author><first>Daniel</first><last>Varab</last></author>
      <author><first>Natalie</first><last>Schluter</last></author>
      <pages>406–410</pages>
      <abstract>This paper describes the design and use of the graph-based parsing framework and toolkit UniParse, released as an open-source python software package. UniParse as a framework novelly streamlines research prototyping, development and evaluation of graph-based dependency parsing architectures. UniParse does this by enabling highly efficient, sufficiently independent, easily readable, and easily extensible implementations for all dependency parser components. We distribute the toolkit with ready-made configurations as re-implementations of all current state-of-the-art first-order graph-based parsers, including even more efficient Cython implementations of both encoders and decoders, as well as the required specialised loss functions.</abstract>
      <url hash="2c1e51dc">W19-6149</url>
      <bibkey>varab-schluter-2019-uniparse</bibkey>
      <pwccode url="https://github.com/ITUnlp/UniParse" additional="false">ITUnlp/UniParse</pwccode>
    </paper>
  </volume>
  <volume id="62" ingest-date="2019-09-30">
    <meta>
      <booktitle>Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing</booktitle>
      <url hash="20e12391">W19-62</url>
      <editor><first>Joakim</first><last>Nivre</last></editor>
      <editor><first>Leon</first><last>Derczynski</last></editor>
      <editor><first>Filip</first><last>Ginter</last></editor>
      <editor><first>Bjørn</first><last>Lindi</last></editor>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Anders</first><last>Søgaard</last></editor>
      <editor><first>Jörg</first><last>Tidemann</last></editor>
      <publisher>Linköping University Electronic Press</publisher>
      <address>Turku, Finland</address>
      <month>September</month>
      <year>2019</year>
      <venue>nodalida</venue>
    </meta>
    <frontmatter>
      <url hash="6632db61">W19-6200</url>
      <bibkey>ws-2019-nlpl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Mark my Word: A Sequence-to-Sequence Approach to Definition Modeling</title>
      <author><first>Timothee</first><last>Mickus</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <author><first>Matthieu</first><last>Constant</last></author>
      <pages>1–11</pages>
      <abstract>Defining words in a textual context is a useful task both for practical purposes and for gaining insight into distributed word representations. Building on the distributional hypothesis, we argue here that the most natural formalization of definition modeling is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a contextually appropriate definition for it. We implement this approach in a Transformer-based sequence-to-sequence model. Our proposal allows to train contextualization and definition generation in an end-to-end fashion, which is a conceptual improvement over earlier works. We achieve state-of-the-art results both in contextual and non-contextual definition modeling.</abstract>
      <url hash="717c40b9">W19-6201</url>
      <bibkey>mickus-etal-2019-mark</bibkey>
    </paper>
    <paper id="2">
      <title>Improving Semantic Dependency Parsing with Syntactic Features</title>
      <author><first>Robin</first><last>Kurtz</last></author>
      <author><first>Daniel</first><last>Roxbo</last></author>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <pages>12–21</pages>
      <abstract>We extend a state-of-the-art deep neural architecture for semantic dependency parsing with features defined over syntactic dependency trees. Our empirical results show that only gold-standard syntactic information leads to consistent improvements in semantic parsing accuracy, and that the magnitude of these improvements varies with the specific combination of the syntactic and the semantic representation used. In contrast, automatically predicted syntax does not seem to help semantic parsing. Our error analysis suggests that there is a significant overlap between syntactic and semantic representations.</abstract>
      <url hash="478d8cdc">W19-6202</url>
      <bibkey>kurtz-etal-2019-improving</bibkey>
    </paper>
    <paper id="3">
      <title>To Lemmatize or Not to Lemmatize: How Word Normalisation Affects <fixed-case>ELM</fixed-case>o Performance in Word Sense Disambiguation</title>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Elizaveta</first><last>Kuzmenko</last></author>
      <pages>22–28</pages>
      <abstract>In this paper, we critically evaluate the widespread assumption that deep learning NLP models do not require lemmatized input. To test this, we trained versions of contextualised word embedding ELMo models on raw tokenized corpora and on the corpora with word tokens replaced by their lemmas. Then, these models were evaluated on the word sense disambiguation task. This was done for the English and Russian languages. The experiments showed that while lemmatization is indeed not necessary for English, the situation is different for Russian. It seems that for rich-morphology languages, using lemmatized training and testing data yields small but consistent improvements: at least for word sense disambiguation. This means that the decisions about text pre-processing before training ELMo should consider the linguistic nature of the language in question.</abstract>
      <url hash="77b8ffa7">W19-6203</url>
      <bibkey>kutuzov-kuzmenko-2019-lemmatize</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/russe">RUSSE</pwcdataset>
    </paper>
    <paper id="4">
      <title>Is Multilingual <fixed-case>BERT</fixed-case> Fluent in Language Generation?</title>
      <author><first>Samuel</first><last>Rönnqvist</last></author>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Tapio</first><last>Salakoski</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>29–36</pages>
      <abstract>The multilingual BERT model is trained on 104 languages and meant to serve as a universal language model and tool for encoding sentences. We explore how well the model performs on several languages across several tasks: a diagnostic classification probing the embeddings for a particular syntactic property, a cloze task testing the language modelling ability to fill in gaps in a sentence, and a natural language generation task testing for the ability to produce coherent text fitting a given context. We find that the currently available multilingual BERT model is clearly inferior to the monolingual counterparts, and cannot in many cases serve as a substitute for a well-trained monolingual model. We find that the English and German models perform well at generation, whereas the multilingual model is lacking, in particular, for Nordic languages. The code of the experiments in the paper is available at: https://github.com/TurkuNLP/bert-eval</abstract>
      <url hash="3a09a851">W19-6204</url>
      <bibkey>ronnqvist-etal-2019-multilingual</bibkey>
      <pwccode url="https://github.com/TurkuNLP/bert-eval" additional="false">TurkuNLP/bert-eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="5">
      <title>Multilingual Probing of Deep Pre-Trained Contextual Encoders</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Memduh</first><last>Gökırmak</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>37–47</pages>
      <abstract>Encoders that generate representations based on context have, in recent years, benefited from adaptations that allow for pre-training on large text corpora. Earlier work on evaluating fixed-length sentence representations has included the use of ‘probing’ tasks, that use diagnostic classifiers to attempt to quantify the extent to which these encoders capture specific linguistic phenomena. The principle of probing has also resulted in extended evaluations that include relatively newer word-level pre-trained encoders. We build on probing tasks established in the literature and comprehensively evaluate and analyse – from a typological perspective amongst others – multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages. Specifically, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT’s cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM).</abstract>
      <url hash="29fa4b3b">W19-6205</url>
      <bibkey>ravishankar-etal-2019-multilingual</bibkey>
    </paper>
    <paper id="6">
      <title>Cross-Domain Sentiment Classification using Vector Embedded Domain Representations</title>
      <author><first>Nicolaj Filrup</first><last>Rasmussen</last></author>
      <author><first>Kristian Nørgaard</first><last>Jensen</last></author>
      <author><first>Marco</first><last>Placenti</last></author>
      <author><first>Thai</first><last>Wang</last></author>
      <pages>48–57</pages>
      <abstract>Due to the differences between reviews in different product categories, creating a general model for cross-domain sentiment classification can be a difficult task. This paper proposes an architecture that incorporates domain knowledge into a neural sentiment classification model. In addition to providing a cross-domain model, this also provides a quantifiable representation of the domains as numeric vectors. We show that it is possible to cluster the domain vectors and provide qualitative insights into the inter-domain relations. We also a) present a new data set for sentiment classification that includes a domain parameter and preprocessed data points, and b) perform an ablation study in order to determine whether some word groups impact performance.</abstract>
      <url hash="a424094d">W19-6206</url>
      <bibkey>rasmussen-etal-2019-cross</bibkey>
    </paper>
    <paper id="7">
      <title>Multiclass Text Classification on Unbalanced, Sparse and Noisy Data</title>
      <author><first>Tillmann</first><last>Dönicke</last></author>
      <author><first>Matthias</first><last>Damaschk</last></author>
      <author><first>Florian</first><last>Lux</last></author>
      <pages>58–65</pages>
      <abstract>This paper discusses methods to improve the performance of text classification on data that is difficult to classify due to a large number of unbalanced classes with noisy examples. A variety of features are tested, in combination with three different neural-network-based methods with increasing complexity. The classifiers are applied to a songtext–artist dataset which is large, unbalanced and noisy. We come to the conclusion that substantial improvement can be obtained by removing unbalancedness and sparsity from the data. This fulfils a classification task unsatisfactorily—however, with contemporary methods, it is a practical step towards fairly satisfactory results.</abstract>
      <url hash="41136359">W19-6207</url>
      <bibkey>damaschk-etal-2019-multiclass</bibkey>
    </paper>
  </volume>
  <volume id="63" ingest-date="2019-09-30">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on NLP for Computer Assisted Language Learning</booktitle>
      <url hash="ead33fc7">W19-63</url>
      <editor><first>David</first><last>Alfter</last></editor>
      <editor><first>Elena</first><last>Volodina</last></editor>
      <editor><first>Lars</first><last>Borin</last></editor>
      <editor><first>Ildikó</first><last>Pilan</last></editor>
      <editor><first>Herbert</first><last>Lange</last></editor>
      <publisher>LiU Electronic Press</publisher>
      <address>Turku, Finland</address>
      <month>September</month>
      <year>2019</year>
      <venue>nlp4call</venue>
    </meta>
    <frontmatter>
      <url hash="46fa56e1">W19-6300</url>
      <bibkey>ws-2019-nlp-computer</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Predicting learner knowledge of individual words using machine learning</title>
      <author><first>Drilon</first><last>Avdiu</last></author>
      <author><first>Vanessa</first><last>Bui</last></author>
      <author><first>Klára Ptačinová</first><last>Klimčíková</last></author>
      <pages>1–9</pages>
      <url hash="fa97a584">W19-6301</url>
      <bibkey>avdiu-etal-2019-predicting</bibkey>
    </paper>
    <paper id="2">
      <title>Automatic Generation and Semantic Grading of <fixed-case>E</fixed-case>speranto Sentences in a Teaching Context</title>
      <author><first>Eckhard</first><last>Bick</last></author>
      <pages>10–19</pages>
      <url hash="eb258bd1">W19-6302</url>
      <bibkey>bick-2019-automatic</bibkey>
    </paper>
    <paper id="3">
      <title>Toward automatic improvement of language produced by non-native language learners</title>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Eetu</first><last>Sjöblom</last></author>
      <pages>20–30</pages>
      <url hash="b8c047d0">W19-6303</url>
      <bibkey>creutz-sjoblom-2019-toward</bibkey>
    </paper>
    <paper id="4">
      <title>Linguistic features and proficiency classification in <fixed-case>L</fixed-case>2 <fixed-case>S</fixed-case>panish and <fixed-case>L</fixed-case>2<fixed-case>P</fixed-case>ortuguese.</title>
      <author><first>Iria</first><last>del Río</last></author>
      <pages>31–40</pages>
      <url hash="862f9967">W19-6304</url>
      <bibkey>del-rio-2019-linguistic</bibkey>
    </paper>
    <paper id="5">
      <title>Integrating large-scale web data and curated corpus data in a search engine supporting <fixed-case>G</fixed-case>erman literacy education</title>
      <author><first>Sabrina</first><last>Dittrich</last></author>
      <author><first>Zarah</first><last>Weiss</last></author>
      <author><first>Hannes</first><last>Schröter</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>41–56</pages>
      <url hash="59c18aed">W19-6305</url>
      <bibkey>dittrich-etal-2019-integrating</bibkey>
    </paper>
    <paper id="6">
      <title>Formalism for a language agnostic language learning game and productive grid generation</title>
      <author><first>Sylvain</first><last>Hatier</last></author>
      <author><first>Arnaud</first><last>Bey</last></author>
      <author><first>Mathieu</first><last>Loiseau</last></author>
      <pages>57–64</pages>
      <url hash="96c80743">W19-6306</url>
      <bibkey>hatier-etal-2019-formalism</bibkey>
    </paper>
    <paper id="7">
      <title>Understanding Vocabulary Growth Through An Adaptive Language Learning System</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Andreas</first><last>Burgdorf</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <author><first>Stefan</first><last>Meeger</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Christian</first><last>Kohlschein</last></author>
      <author><first>Tobias</first><last>Meisen</last></author>
      <pages>65–78</pages>
      <url hash="63c70796">W19-6307</url>
      <bibkey>kerz-etal-2019-understanding</bibkey>
    </paper>
    <paper id="8">
      <title>Summarization Evaluation meets Short-Answer Grading</title>
      <author><first>Margot</first><last>Mieskes</last></author>
      <author><first>Ulrike</first><last>Padó</last></author>
      <pages>79–85</pages>
      <url hash="e885fe08">W19-6308</url>
      <bibkey>mieskes-pado-2019-summarization</bibkey>
    </paper>
    <paper id="9">
      <title>Experiments on Non-native Speech Assessment and its Consistency</title>
      <author><first>Ziwei</first><last>Zhou</last></author>
      <author><first>Sowmya</first><last>Vajjala</last></author>
      <author><first>Seyed Vahid</first><last>Mirnezami</last></author>
      <pages>86–92</pages>
      <url hash="3998dcf0">W19-6309</url>
      <bibkey>zhou-etal-2019-experiments</bibkey>
    </paper>
    <paper id="10">
      <title>The Impact of Spelling Correction and Task Context on Short Answer Assessment for Intelligent Tutoring Systems</title>
      <author><first>Ramon</first><last>Ziai</last></author>
      <author><first>Florian</first><last>Nuxoll</last></author>
      <author><first>Kordula</first><last>De Kuthy</last></author>
      <author><first>Björn</first><last>Rudzewitz</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>93–99</pages>
      <url hash="819bb592">W19-6310</url>
      <bibkey>ziai-etal-2019-impact</bibkey>
    </paper>
  </volume>
  <volume id="64" ingest-date="2019-09-30">
    <meta>
      <booktitle>Proceedings of the Second Financial Narrative Processing Workshop (FNP 2019)</booktitle>
      <url hash="c9c922ad">W19-64</url>
      <editor><first>Mahmoud</first><last>El-Haj</last></editor>
      <editor><first>Paul</first><last>Rayson</last></editor>
      <editor><first>Steven</first><last>Young</last></editor>
      <editor><first>Houda</first><last>Bouamor</last></editor>
      <editor><first>Sira</first><last>Ferradans</last></editor>
      <publisher>Linköping University Electronic Press</publisher>
      <address>Turku, Finland</address>
      <month>September</month>
      <year>2019</year>
      <venue>fnp</venue>
    </meta>
    <frontmatter>
      <url hash="04c56194">W19-6400</url>
      <bibkey>ws-2019-financial-narrative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Finance document Extraction Using Data Augmentation and Attention</title>
      <author><first>Ke</first><last>Tian</last></author>
      <author><first>Zi Jun</first><last>Peng</last></author>
      <pages>1–4</pages>
      <url hash="f1adfb30">W19-6401</url>
      <bibkey>tian-peng-2019-finance</bibkey>
    </paper>
    <paper id="2">
      <title>Utilizing Pre-Trained Word Embeddings to Learn Classification Lexicons with Little Supervision</title>
      <author><first>Frederick</first><last>Blumenthal</last></author>
      <author><first>Ferdinand</first><last>Graf</last></author>
      <pages>5–15</pages>
      <url hash="89694492">W19-6402</url>
      <bibkey>blumenthal-graf-2019-utilizing</bibkey>
    </paper>
    <paper id="3">
      <title>Automated Stock Price Prediction Using Machine Learning</title>
      <author><first>Wassim El-Hajj</first><last>Mariam Mokalled</last></author>
      <author><first>Mohamad</first><last>Jaber</last></author>
      <pages>16–24</pages>
      <url hash="d8d8873d">W19-6403</url>
      <bibkey>mariam-mokalled-jaber-2019-automated</bibkey>
    </paper>
    <paper id="4">
      <title>Active Learning for Financial Investment Reports</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <author><first>Ted</first><last>Briscoe</last></author>
      <pages>25–32</pages>
      <url hash="fc54885d">W19-6404</url>
      <bibkey>gooding-briscoe-2019-active</bibkey>
    </paper>
    <paper id="5">
      <title>Towards Unlocking the Narrative of the <fixed-case>U</fixed-case>nited <fixed-case>S</fixed-case>tates Income Tax Forms</title>
      <author><first>Esme</first><last>Manandise</last></author>
      <pages>33–41</pages>
      <url hash="28958015">W19-6405</url>
      <bibkey>manandise-2019-towards</bibkey>
    </paper>
    <paper id="6">
      <title>Tone Analysis in <fixed-case>S</fixed-case>panish Financial Reporting Narratives</title>
      <author><first>Antonio</first><last>Moreno-Sandoval</last></author>
      <author><first>Pablo Alfonso Haya</first><last>Ana Gisbert</last></author>
      <author><first>Marta</first><last>Guerrero</last></author>
      <author><first>Helena</first><last>Montoro</last></author>
      <pages>42–50</pages>
      <url hash="2d99cc26">W19-6406</url>
      <bibkey>moreno-sandoval-etal-2019-tone</bibkey>
    </paper>
    <paper id="7">
      <title>The <fixed-case>F</fixed-case>in<fixed-case>TOC</fixed-case>-2019 Shared Task: Financial Document Structure Extraction</title>
      <author><first>Remi</first><last>Juge</last></author>
      <author><first>Imane</first><last>Bentabet</last></author>
      <author><first>Sira</first><last>Ferradans</last></author>
      <pages>51–57</pages>
      <url hash="11d71fa7">W19-6407</url>
      <bibkey>juge-etal-2019-fintoc</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>F</fixed-case>in<fixed-case>TOC</fixed-case>-2019 Shared Task: Finding Title in Text Blocks</title>
      <author><first>Hanna Abi</first><last>Akl</last></author>
      <author><first>Anubhav</first><last>Gupta</last></author>
      <author><first>Dominique</first><last>Mariko</last></author>
      <pages>58–62</pages>
      <url hash="0625e080">W19-6408</url>
      <bibkey>akl-etal-2019-fintoc</bibkey>
    </paper>
    <paper id="9">
      <title>Daniel@<fixed-case>F</fixed-case>in<fixed-case>TOC</fixed-case>-2019 Shared Task : <fixed-case>TOC</fixed-case> Extraction and Title Detection</title>
      <author><first>Emmanuel</first><last>Giguet</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>63–68</pages>
      <url hash="ee7e6daf">W19-6409</url>
      <bibkey>giguet-lejeune-2019-daniel</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>F</fixed-case>in<fixed-case>DSE</fixed-case>@<fixed-case>F</fixed-case>in<fixed-case>TOC</fixed-case>-2019 Shared Task</title>
      <author><first>Carla</first><last>Abreu</last></author>
      <author><first>Henrique</first><last>Cardoso</last></author>
      <author><first>Eugénio</first><last>Oliveira</last></author>
      <pages>69–73</pages>
      <url hash="1f913162">W19-6410</url>
      <bibkey>abreu-etal-2019-findse</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>UWB</fixed-case>@<fixed-case>F</fixed-case>in<fixed-case>TOC</fixed-case>-2019 Shared Task: Financial Document Title Detection</title>
      <author><first>Pavel Král</first><last>Tomas Hercig</last></author>
      <pages>74–78</pages>
      <url hash="1993842a">W19-6411</url>
      <bibkey>tomas-hercig-2019-uwb</bibkey>
    </paper>
  </volume>
  <volume id="65" ingest-date="2019-09-30">
    <meta>
      <booktitle>Proceedings of the Workshop on NLP and Pseudonymisation</booktitle>
      <url hash="44bf7ab2">W19-65</url>
      <editor><first>Lars</first><last>Ahrenberg</last></editor>
      <editor><first>Beata</first><last>Megyesi</last></editor>
      <publisher>Linköping Electronic Press</publisher>
      <address>Turku, Finland</address>
      <month>September</month>
      <year>2019</year>
      <venue>nodalida</venue>
    </meta>
    <frontmatter>
      <url hash="3308fe8d">W19-6500</url>
      <bibkey>ws-2019-nlp-pseudonymisation</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>nony<fixed-case>M</fixed-case>ate: A Toolkit for Anonymizing Unstructured Chat Data</title>
      <author><first>Allison</first><last>Adams</last></author>
      <author><first>Eric</first><last>Aili</last></author>
      <author><first>Daniel</first><last>Aioanei</last></author>
      <author><first>Rebecca</first><last>Jonsson</last></author>
      <author><first>Lina</first><last>Mickelsson</last></author>
      <author><first>Dagmar</first><last>Mikmekova</last></author>
      <author><first>Fred</first><last>Roberts</last></author>
      <author><first>Javier Fernandez</first><last>Valencia</last></author>
      <author><first>Roger</first><last>Wechsler</last></author>
      <pages>1–7</pages>
      <url hash="4d025e88">W19-6501</url>
      <bibkey>adams-etal-2019-anonymate</bibkey>
    </paper>
    <paper id="2">
      <title>Augmenting a De-identification System for <fixed-case>S</fixed-case>wedish Clinical Text Using Open Resources and Deep Learning</title>
      <author><first>Hanna</first><last>Berg</last></author>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>8–15</pages>
      <url hash="15667dc1">W19-6502</url>
      <bibkey>berg-dalianis-2019-augmenting</bibkey>
    </paper>
    <paper id="3">
      <title>Pseudonymisation of <fixed-case>S</fixed-case>wedish Electronic Patient Records Using a Rule-Based Approach</title>
      <author><first>Hercules</first><last>Dalianis</last></author>
      <pages>16–23</pages>
      <url hash="6f6a273e">W19-6503</url>
      <bibkey>dalianis-2019-pseudonymisation</bibkey>
    </paper>
  </volume>
  <volume id="66" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of Machine Translation Summit XVII: Research Track</booktitle>
      <url hash="3d4008aa">W19-66</url>
      <editor><first>Mikel</first><last>Forcada</last></editor>
      <editor><first>Andy</first><last>Way</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Rico</first><last>Sennrich</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>mtsummit</venue>
    </meta>
    <frontmatter>
      <url hash="9fc87448">W19-6600</url>
      <bibkey>ws-2019-machine-translation-summit</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Online Sentence Segmentation for Simultaneous Interpretation using Multi-Shifted Recurrent Neural Network</title>
      <author><first>Xiaolin</first><last>Wang</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>1–11</pages>
      <url hash="54a85e7d">W19-6601</url>
      <bibkey>wang-etal-2019-online</bibkey>
    </paper>
    <paper id="2">
      <title>Robust Document Representations for Cross-Lingual Information Retrieval in Low-Resource Settings</title>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Xutai</first><last>Ma</last></author>
      <author><first>Sorami</first><last>Hisamoto</last></author>
      <author><first>Muhammad</first><last>Rahman</last></author>
      <author><first>Yiming</first><last>Wang</last></author>
      <author><first>Hainan</first><last>Xu</last></author>
      <author><first>Daniel</first><last>Povey</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>12–20</pages>
      <url hash="d7c0f55a">W19-6602</url>
      <bibkey>yarmohammadi-etal-2019-robust</bibkey>
    </paper>
    <paper id="3">
      <title>Enhancing Transformer for End-to-end Speech-to-Text Translation</title>
      <author><first>Mattia Antonino</first><last>Di Gangi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Roldano</first><last>Cattoni</last></author>
      <author><first>Roberto</first><last>Dessi</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>21–31</pages>
      <url hash="1c630e13">W19-6603</url>
      <bibkey>di-gangi-etal-2019-enhancing</bibkey>
    </paper>
    <paper id="4">
      <title>Debiasing Word Embeddings Improves Multimodal Machine Translation</title>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>32–42</pages>
      <url hash="9aa72256">W19-6604</url>
      <bibkey>hirasawa-komachi-2019-debiasing</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>T</fixed-case>ranslator2<fixed-case>V</fixed-case>ec: Understanding and Representing Human Post-Editors</title>
      <author><first>António</first><last>Góis</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>43–54</pages>
      <url hash="bb41a8fa">W19-6605</url>
      <bibkey>gois-martins-2019-translator2vec</bibkey>
    </paper>
    <paper id="6">
      <title>Domain Adaptation for <fixed-case>MT</fixed-case>: A Study with Unknown and Out-of-Domain Tasks</title>
      <author><first>Hoang</first><last>Cuong</last></author>
      <pages>55–66</pages>
      <url hash="0204aa78">W19-6606</url>
      <bibkey>cuong-2019-domain</bibkey>
    </paper>
    <paper id="7">
      <title>What is the impact of raw <fixed-case>MT</fixed-case> on <fixed-case>J</fixed-case>apanese users of Word: preliminary results of a usability study using eye-tracking</title>
      <author><first>Ana Guerberof</first><last>Arenas</last></author>
      <author><first>Joss</first><last>Moorkens</last></author>
      <author><first>Sharon</first><last>O’Brien</last></author>
      <pages>67–77</pages>
      <url hash="ca13a5eb">W19-6607</url>
      <bibkey>arenas-etal-2019-impact</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>MAGMAT</fixed-case>ic: A Multi-domain Academic Gold Standard with Manual Annotation of Terminology for Machine Translation Evaluation</title>
      <author><first>Randy</first><last>Scansani</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Silvia</first><last>Bernardini</last></author>
      <author><first>Adriano</first><last>Ferraresi</last></author>
      <pages>78–86</pages>
      <url hash="59a1980a">W19-6608</url>
      <bibkey>scansani-etal-2019-magmatic</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic error classification with multiple error labels</title>
      <author><first>Maja</first><last>Popovic</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <pages>87–95</pages>
      <url hash="3bee11f3">W19-6609</url>
      <bibkey>popovic-vilar-2019-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>Interactive-Predictive Neural Machine Translation through Reinforcement and Imitation</title>
      <author><first>Tsz Kin</first><last>Lam</last></author>
      <author><first>Shigehiko</first><last>Schamoni</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <pages>96–106</pages>
      <url hash="38996299">W19-6610</url>
      <bibkey>lam-etal-2019-interactive</bibkey>
    </paper>
    <paper id="11">
      <title>An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation Architectures</title>
      <author><first>Hamidreza</first><last>Ghader</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <pages>107–117</pages>
      <url hash="2217c87b">W19-6611</url>
      <bibkey>ghader-monz-2019-intrinsic</bibkey>
    </paper>
    <paper id="12">
      <title>Improving Neural Machine Translation Using Noisy Parallel Data through Distillation</title>
      <author><first>Praveen</first><last>Dakwale</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <pages>118–127</pages>
      <url hash="024189b6">W19-6612</url>
      <bibkey>dakwale-monz-2019-improving</bibkey>
    </paper>
    <paper id="13">
      <title>Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation</title>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Kenji</first><last>Imamura</last></author>
      <pages>128–139</pages>
      <url hash="0edc2d83">W19-6613</url>
      <bibkey>imankulova-etal-2019-exploiting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/aspec">ASPEC</pwcdataset>
    </paper>
    <paper id="14">
      <title>Improving Anaphora Resolution in Neural Machine Translation Using Curriculum Learning</title>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>140–150</pages>
      <url hash="0b4df783">W19-6614</url>
      <bibkey>stojanovski-fraser-2019-improving</bibkey>
    </paper>
    <paper id="15">
      <title>Improving <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Recognition with Synthetic Data</title>
      <author><first>Jungi</first><last>Kim</last></author>
      <author><first>Patricia</first><last>O’Neill-Brown</last></author>
      <pages>151–161</pages>
      <url hash="e89c1d72">W19-6615</url>
      <bibkey>kim-oneill-brown-2019-improving</bibkey>
    </paper>
    <paper id="16">
      <title>Selecting Informative Context Sentence by Forced Back-Translation</title>
      <author><first>Ryuichiro</first><last>Kimura</last></author>
      <author><first>Shohei</first><last>Iida</last></author>
      <author><first>Hongyi</first><last>Cui</last></author>
      <author><first>Po-Hsuan</first><last>Hung</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>162–171</pages>
      <url hash="4e2d533c">W19-6616</url>
      <bibkey>kimura-etal-2019-selecting</bibkey>
    </paper>
    <paper id="17">
      <title>Memory-Augmented Neural Networks for Machine Translation</title>
      <author><first>Mark</first><last>Collier</last></author>
      <author><first>Joeran</first><last>Beel</last></author>
      <pages>172–181</pages>
      <url hash="0217c37e">W19-6617</url>
      <bibkey>collier-beel-2019-memory</bibkey>
      <pwccode url="https://github.com/MarkPKCollier/MANNs4NMT" additional="false">MarkPKCollier/MANNs4NMT</pwccode>
    </paper>
    <paper id="18">
      <title>An Exploration of Placeholding in Neural Machine Translation</title>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Marianna</first><last>Martindale</last></author>
      <author><first>Winston</first><last>Wu</last></author>
      <pages>182–192</pages>
      <url hash="915327db">W19-6618</url>
      <bibkey>post-etal-2019-exploration</bibkey>
    </paper>
    <paper id="19">
      <title>Controlling the Reading Level of Machine Translation Output</title>
      <author><first>Kelly</first><last>Marchisio</last></author>
      <author><first>Jialiang</first><last>Guo</last></author>
      <author><first>Cheng-I</first><last>Lai</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>193–203</pages>
      <url hash="3c44d287">W19-6619</url>
      <bibkey>marchisio-etal-2019-controlling</bibkey>
    </paper>
    <paper id="20">
      <title>A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation</title>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Adithya</first><last>Renduchintala</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>204–213</pages>
      <url hash="6b858502">W19-6620</url>
      <attachment type="poster" hash="823b4722">W19-6620.Poster.pdf</attachment>
      <bibkey>ding-etal-2019-call</bibkey>
    </paper>
    <paper id="21">
      <title>The Impact of Preprocessing on <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>nglish Statistical and Neural Machine Translation</title>
      <author><first>Mai</first><last>Oudah</last></author>
      <author><first>Amjad</first><last>Almahairi</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>214–221</pages>
      <url hash="3242935f">W19-6621</url>
      <bibkey>oudah-etal-2019-impact</bibkey>
    </paper>
    <paper id="22">
      <title>Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation</title>
      <author><first>Eva</first><last>Vanmassenhove</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>222–232</pages>
      <url hash="081e2b69">W19-6622</url>
      <bibkey>vanmassenhove-etal-2019-lost</bibkey>
    </paper>
    <paper id="23">
      <title>Identifying Fluently Inadequate Output in Neural and Statistical Machine Translation</title>
      <author><first>Marianna</first><last>Martindale</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Paul</first><last>McNamee</last></author>
      <pages>233–243</pages>
      <url hash="7ca52920">W19-6623</url>
      <bibkey>martindale-etal-2019-identifying</bibkey>
    </paper>
    <paper id="24">
      <title>Character-Aware Decoder for Translation into Morphologically Rich Languages</title>
      <author><first>Adithya</first><last>Renduchintala</last></author>
      <author><first>Pamela</first><last>Shapiro</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>244–255</pages>
      <url hash="1428a3be">W19-6624</url>
      <bibkey>renduchintala-etal-2019-character</bibkey>
    </paper>
    <paper id="25">
      <title>Improving Translations by Combining Fuzzy-Match Repair with Automatic Post-Editing</title>
      <author><first>John</first><last>Ortega</last></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <pages>256–266</pages>
      <url hash="8cf28d47">W19-6625</url>
      <bibkey>ortega-etal-2019-improving</bibkey>
    </paper>
    <paper id="26">
      <title>Post-editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain</title>
      <author><first>Samuel</first><last>Läubli</last></author>
      <author><first>Chantal</first><last>Amrhein</last></author>
      <author><first>Patrick</first><last>Düggelin</last></author>
      <author><first>Beatriz</first><last>Gonzalez</last></author>
      <author><first>Alena</first><last>Zwahlen</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>267–272</pages>
      <url hash="172ee046">W19-6626</url>
      <bibkey>laubli-etal-2019-post</bibkey>
    </paper>
    <paper id="27">
      <title>Post-editese: an Exacerbated Translationese</title>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>273–281</pages>
      <url hash="fe6f0999">W19-6627</url>
      <bibkey>toral-2019-post</bibkey>
      <pwccode url="https://github.com/antot/posteditese_mtsummit19" additional="false">antot/posteditese_mtsummit19</pwccode>
    </paper>
  </volume>
  <volume id="67" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks</booktitle>
      <url hash="2005f1d7">W19-67</url>
      <editor><first>Mikel</first><last>Forcada</last></editor>
      <editor><first>Andy</first><last>Way</last></editor>
      <editor><first>John</first><last>Tinsley</last></editor>
      <editor><first>Dimitar</first><last>Shterionov</last></editor>
      <editor><first>Celia</first><last>Rico</last></editor>
      <editor><first>Federico</first><last>Gaspari</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>mtsummit</venue>
    </meta>
    <frontmatter>
      <url hash="eb33468e">W19-6700</url>
      <bibkey>ws-2019-machine-translation-summit-xvii</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Competitiveness Analysis of the <fixed-case>E</fixed-case>uropean Machine Translation Market</title>
      <author><first>Andrejs</first><last>Vasiļjevs</last></author>
      <author><first>Inguna</first><last>Skadiņa</last></author>
      <author><first>Indra</first><last>Sāmīte</last></author>
      <author><first>Kaspars</first><last>Kauliņš</last></author>
      <author><first>Ēriks</first><last>Ajausks</last></author>
      <author><first>Jūlija</first><last>Meļņika</last></author>
      <author><first>Aivars</first><last>Bērziņš</last></author>
      <pages>1–7</pages>
      <url hash="c556bb14">W19-6701</url>
      <bibkey>vasiljevs-etal-2019-competitiveness</bibkey>
    </paper>
    <paper id="2">
      <title>Improving <fixed-case>CAT</fixed-case> Tools in the Translation Workflow: New Approaches and Evaluation</title>
      <author><first>Mihaela</first><last>Vela</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <author><first>Sudip</first><last>Naskar</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>8–15</pages>
      <url hash="63927fd1">W19-6702</url>
      <bibkey>vela-etal-2019-improving</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>H</fixed-case>ungarian translators’ perceptions of Neural Machine Translation in the <fixed-case>E</fixed-case>uropean Commission</title>
      <author><first>Ágnes</first><last>Lesznyák</last></author>
      <pages>16–22</pages>
      <url hash="81dc059e">W19-6703</url>
      <bibkey>lesznyak-2019-hungarian</bibkey>
    </paper>
    <paper id="4">
      <title>Applying Machine Translation to Psychology: Automatic Translation of Personality Adjectives</title>
      <author><first>Ritsuko</first><last>Iwai</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Takatsune</first><last>Kumada</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>23–29</pages>
      <url hash="07b1c621">W19-6704</url>
      <bibkey>iwai-etal-2019-applying</bibkey>
    </paper>
    <paper id="5">
      <title>Evaluating machine translation in a low-resource language combination: <fixed-case>S</fixed-case>panish-<fixed-case>G</fixed-case>alician.</title>
      <author><first>María Do Campo</first><last>Bayón</last></author>
      <author><first>Pilar</first><last>Sánchez-Gijón</last></author>
      <pages>30–35</pages>
      <url hash="60aa9655">W19-6705</url>
      <bibkey>bayon-sanchez-gijon-2019-evaluating</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>MTPE</fixed-case> in Patents: A Successful Business Story</title>
      <author><first>Valeria</first><last>Premoli</last></author>
      <author><first>Elena</first><last>Murgolo</last></author>
      <author><first>Diego</first><last>Cresceri</last></author>
      <pages>36–41</pages>
      <url hash="5c9e7b14">W19-6706</url>
      <bibkey>premoli-etal-2019-mtpe</bibkey>
    </paper>
    <paper id="7">
      <title>User expectations towards machine translation: A case study</title>
      <author><first>Barbara</first><last>Heinisch</last></author>
      <author><first>Vesna</first><last>Lušicky</last></author>
      <pages>42–48</pages>
      <url hash="e77f76f4">W19-6707</url>
      <bibkey>heinisch-lusicky-2019-user</bibkey>
    </paper>
    <paper id="8">
      <title>Does <fixed-case>NMT</fixed-case> make a difference when post-editing closely related languages? The case of <fixed-case>S</fixed-case>panish-<fixed-case>C</fixed-case>atalan</title>
      <author><first>Sergi</first><last>Alvarez</last></author>
      <author><first>Antoni</first><last>Oliver</last></author>
      <author><first>Toni</first><last>Badia</last></author>
      <pages>49–56</pages>
      <url hash="7fca7cbb">W19-6708</url>
      <bibkey>alvarez-etal-2019-nmt</bibkey>
    </paper>
    <paper id="9">
      <title>Machine Translation in the Financial Services Industry: A Case Study</title>
      <author><first>Mara</first><last>Nunziatini</last></author>
      <pages>57–63</pages>
      <url hash="f7f26235">W19-6709</url>
      <bibkey>nunziatini-2019-machine</bibkey>
    </paper>
    <paper id="10">
      <title>Pre-editing Plus Neural Machine Translation for Subtitling: Effective Pre-editing Rules for Subtitling of <fixed-case>TED</fixed-case> Talks</title>
      <author><first>Yusuke</first><last>Hiraoka</last></author>
      <author><first>Masaru</first><last>Yamada</last></author>
      <pages>64–72</pages>
      <url hash="d83cba74">W19-6710</url>
      <bibkey>hiraoka-yamada-2019-pre</bibkey>
    </paper>
    <paper id="11">
      <title>Do translator trainees trust machine translation? An experiment on post-editing and revision</title>
      <author><first>Randy</first><last>Scansani</last></author>
      <author><first>Silvia</first><last>Bernardini</last></author>
      <author><first>Adriano</first><last>Ferraresi</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <pages>73–79</pages>
      <url hash="0c4ee013">W19-6711</url>
      <bibkey>scansani-etal-2019-translator</bibkey>
    </paper>
    <paper id="12">
      <title>On reducing translation shifts in translations intended for <fixed-case>MT</fixed-case> evaluation</title>
      <author><first>Maja</first><last>Popovic</last></author>
      <pages>80–87</pages>
      <url hash="7c23ce00">W19-6712</url>
      <bibkey>popovic-2019-reducing</bibkey>
    </paper>
    <paper id="13">
      <title>Comparative Analysis of Errors in <fixed-case>MT</fixed-case> Output and Computer-assisted Translation: Effect of the Human Factor</title>
      <author><first>Irina</first><last>Ovchinnikova</last></author>
      <author><first>Daria</first><last>Morozova</last></author>
      <pages>88–94</pages>
      <url hash="2ca00a44">W19-6713</url>
      <bibkey>ovchinnikova-morozova-2019-comparative</bibkey>
    </paper>
    <paper id="14">
      <title>A Comparative Study of <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>hinese Translations of Court Texts by Machine and Human Translators and the <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec Based Similarity Measure’s Ability To Gauge Human Evaluation Biases</title>
      <author><first>Ming</first><last>Qian</last></author>
      <author><first>Jessie</first><last>Liu</last></author>
      <author><first>Chaofeng</first><last>Li</last></author>
      <author><first>Liming</first><last>Pals</last></author>
      <pages>95–100</pages>
      <url hash="eb546510">W19-6714</url>
      <bibkey>qian-etal-2019-comparative</bibkey>
    </paper>
    <paper id="15">
      <title>Translating Terminologies: A Comparative Examination of <fixed-case>NMT</fixed-case> and <fixed-case>PBSMT</fixed-case> Systems</title>
      <author><first>Long-Huei</first><last>Chen</last></author>
      <author><first>Kyo</first><last>Kageura</last></author>
      <pages>101–108</pages>
      <url hash="f1a38d7b">W19-6715</url>
      <bibkey>chen-kageura-2019-translating</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>NEC TM</fixed-case> Data Project</title>
      <author><first>Alexandre</first><last>Helle</last></author>
      <author><first>Manuel</first><last>Herranz</last></author>
      <pages>109–109</pages>
      <url hash="11c26f77">W19-6716</url>
      <bibkey>helle-herranz-2019-nec</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>APE</fixed-case>-<fixed-case>QUEST</fixed-case></title>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <author><first>Heidi</first><last>Depraetere</last></author>
      <author><first>Sara</first><last>Szoc</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Koen</first><last>Van Winckel</last></author>
      <author><first>Frederic</first><last>Everaert</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Maxim</first><last>Khalilov</last></author>
      <author><first>Christine</first><last>Maroti</last></author>
      <author><first>Eduardo</first><last>Farah</last></author>
      <author><first>Artur</first><last>Ventura</last></author>
      <pages>110–111</pages>
      <url hash="83bc1d97">W19-6717</url>
      <bibkey>van-den-bogaert-etal-2019-ape</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>PRINCIPLE</fixed-case>: Providing Resources in <fixed-case>I</fixed-case>rish, <fixed-case>N</fixed-case>orwegian, <fixed-case>C</fixed-case>roatian and <fixed-case>I</fixed-case>celandic for the Purposes of Language Engineering</title>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Federico</first><last>Gaspari</last></author>
      <pages>112–113</pages>
      <url hash="d8facab4">W19-6718</url>
      <bibkey>way-gaspari-2019-principle</bibkey>
    </paper>
    <paper id="19">
      <title>i<fixed-case>ADAATPA</fixed-case> Project: Pangeanic use cases</title>
      <author><first>Mercedes</first><last>García-Martínez</last></author>
      <author><first>Amando</first><last>Estela</last></author>
      <author><first>Laurent</first><last>Bié</last></author>
      <author><first>Alexandre</first><last>Helle</last></author>
      <author><first>Manuel</first><last>Herranz</last></author>
      <pages>114–115</pages>
      <url hash="1c596de6">W19-6719</url>
      <bibkey>garcia-martinez-etal-2019-iadaatpa</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>MICE</fixed-case></title>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <author><first>Heidi</first><last>Depraetere</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Frederic</first><last>Everaert</last></author>
      <author><first>Koen</first><last>Van Winckel</last></author>
      <author><first>Katri</first><last>Tammsaar</last></author>
      <author><first>Ingmar</first><last>Vali</last></author>
      <author><first>Tambet</first><last>Artma</last></author>
      <author><first>Piret</first><last>Saartee</last></author>
      <author><first>Laura Katariina</first><last>Teder</last></author>
      <author><first>Artūrs</first><last>Vasiļevskis</last></author>
      <author><first>Valters</first><last>Sics</last></author>
      <author><first>Johan</first><last>Haelterman</last></author>
      <author><first>David</first><last>Bienfait</last></author>
      <pages>116–117</pages>
      <url hash="18daa9c0">W19-6720</url>
      <bibkey>van-den-bogaert-etal-2019-mice</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>P</fixed-case>ara<fixed-case>C</fixed-case>rawl: Web-scale parallel corpora for the languages of the <fixed-case>EU</fixed-case></title>
      <author><first>Miquel</first><last>Esplà</last></author>
      <author><first>Mikel</first><last>Forcada</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Hieu</first><last>Hoang</last></author>
      <pages>118–119</pages>
      <url hash="efe5fb18">W19-6721</url>
      <bibkey>espla-etal-2019-paracrawl</bibkey>
    </paper>
    <paper id="22">
      <title>Pivot Machine Translation in <fixed-case>INTERACT</fixed-case> Project</title>
      <author><first>Chao-Hong</first><last>Liu</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Catarina</first><last>Silva</last></author>
      <author><first>André</first><last>Martins</last></author>
      <pages>120–121</pages>
      <url hash="7cc9875a">W19-6722</url>
      <bibkey>liu-etal-2019-pivot</bibkey>
    </paper>
    <paper id="23">
      <title>Global Under-Resourced Media Translation (<fixed-case>G</fixed-case>o<fixed-case>URMET</fixed-case>)</title>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Ivan</first><last>Tito</last></author>
      <author><first>Antonio Valerio Miceli</first><last>Barone</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
      <author><first>Mikel L.</first><last>Forcada</last></author>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Víctor</first><last>Sánchez-Cartagena</last></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <author><first>Andrew</first><last>Secker</last></author>
      <author><first>Peggy</first><last>van der Kreeft</last></author>
      <pages>122–122</pages>
      <url hash="1d38b16a">W19-6723</url>
      <bibkey>birch-etal-2019-global</bibkey>
    </paper>
    <paper id="24">
      <title>Neural machine translation system for the <fixed-case>K</fixed-case>azakh language</title>
      <author><first>Ualsher</first><last>Tukeyev</last></author>
      <author><first>Zhandos</first><last>Zhumanov</last></author>
      <pages>123–124</pages>
      <url hash="33769a76">W19-6724</url>
      <bibkey>tukeyev-zhumanov-2019-neural</bibkey>
    </paper>
    <paper id="25">
      <title>Leveraging Rule-Based Machine Translation Knowledge for Under-Resourced Neural Machine Translation Models</title>
      <author><first>Daniel</first><last>Torregrosa</last></author>
      <author><first>Nivranshu</first><last>Pasricha</last></author>
      <author><first>Maraim</first><last>Masoud</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Juan</first><last>Alonso</last></author>
      <author><first>Noe</first><last>Casas</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <pages>125–133</pages>
      <url hash="6d0c8d8d">W19-6725</url>
      <bibkey>torregrosa-etal-2019-leveraging</bibkey>
    </paper>
    <paper id="26">
      <title>Bootstrapping a Natural Language Interface to a Cyber Security Event Collection System using a Hybrid Translation Approach</title>
      <author><first>Johann</first><last>Roturier</last></author>
      <author><first>Brian</first><last>Schlatter</last></author>
      <author><first>David Silva</first><last>Schlatter</last></author>
      <pages>134–141</pages>
      <url hash="c5f421bb">W19-6726</url>
      <bibkey>roturier-etal-2019-bootstrapping</bibkey>
    </paper>
    <paper id="27">
      <title>Improving Robustness in Real-World Neural Machine Translation Engines</title>
      <author><first>Rohit</first><last>Gupta</last></author>
      <author><first>Patrik</first><last>Lambert</last></author>
      <author><first>Raj</first><last>Patel</last></author>
      <author><first>John</first><last>Tinsley</last></author>
      <pages>142–148</pages>
      <url hash="54c3d5b3">W19-6727</url>
      <bibkey>gupta-etal-2019-improving-robustness</bibkey>
    </paper>
    <paper id="28">
      <title>Surveying the potential of using speech technologies for post-editing purposes in the context of international organizations: What do professional translators think?</title>
      <author><first>Jeevanthi</first><last>Liyanapathirana</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Bartolomé</first><last>Mesa-Lao</last></author>
      <pages>149–158</pages>
      <url hash="a660ef27">W19-6728</url>
      <bibkey>liyanapathirana-etal-2019-surveying</bibkey>
    </paper>
    <paper id="29">
      <title>Automatic Translation for Software with Safe Velocity</title>
      <author><first>Dag</first><last>Schmidtke</last></author>
      <author><first>Declan</first><last>Groves</last></author>
      <pages>159–166</pages>
      <url hash="d4f50c26">W19-6729</url>
      <bibkey>schmidtke-groves-2019-automatic</bibkey>
    </paper>
    <paper id="30">
      <title>Application of Post-Edited Machine Translation in Fashion e<fixed-case>C</fixed-case>ommerce</title>
      <author><first>Kasia</first><last>Kosmaczewska</last></author>
      <author><first>Matt</first><last>Train</last></author>
      <pages>167–173</pages>
      <url hash="3f607404">W19-6730</url>
      <bibkey>kosmaczewska-train-2019-application</bibkey>
    </paper>
    <paper id="31">
      <title>Morphological Neural Pre- and Post-Processing for <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Giorgio</first><last>Bernardinello</last></author>
      <pages>174–178</pages>
      <url hash="0b3afc10">W19-6731</url>
      <bibkey>bernardinello-2019-morphological</bibkey>
    </paper>
    <paper id="32">
      <title>Large-scale Machine Translation Evaluation of the i<fixed-case>ADAATPA</fixed-case> Project</title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <author><first>Natália</first><last>Resende</last></author>
      <author><first>Federico</first><last>Gaspari</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Tony</first><last>O’Dowd</last></author>
      <author><first>Marek</first><last>Mazur</last></author>
      <author><first>Manuel</first><last>Herranz</last></author>
      <author><first>Alex</first><last>Helle</last></author>
      <author><first>Gema</first><last>Ramírez-Sánchez</last></author>
      <author><first>Víctor</first><last>Sánchez-Cartagena</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <author><first>Valters</first><last>Šics</last></author>
      <pages>179–185</pages>
      <url hash="95c43ae8">W19-6732</url>
      <bibkey>castilho-etal-2019-large</bibkey>
    </paper>
    <paper id="33">
      <title>Collecting domain specific data for <fixed-case>MT</fixed-case>: an evaluation of the <fixed-case>P</fixed-case>ara<fixed-case>C</fixed-case>rawlpipeline</title>
      <author><first>Arne</first><last>Defauw</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Sara</first><last>Szoc</last></author>
      <author><first>Frederic</first><last>Everaert</last></author>
      <author><first>Koen</first><last>Van Winckel</last></author>
      <author><first>Kim</first><last>Scholte</last></author>
      <author><first>Joris</first><last>Brabers</last></author>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <pages>186–195</pages>
      <url hash="e99638b6">W19-6733</url>
      <bibkey>defauw-etal-2019-collecting</bibkey>
    </paper>
    <paper id="34">
      <title>Monolingual backtranslation in a medical speech translation system for diagnostic interviews - a <fixed-case>NMT</fixed-case> approach</title>
      <author><first>Jonathan</first><last>Mutal</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Johanna</first><last>Gerlach</last></author>
      <author><first>Paula</first><last>Estrella</last></author>
      <author><first>Hervé</first><last>Spechbach</last></author>
      <pages>196–203</pages>
      <url hash="f667888b">W19-6734</url>
      <bibkey>mutal-etal-2019-monolingual</bibkey>
    </paper>
    <paper id="35">
      <title>Improving Domain Adaptation for Machine Translation with<fixed-case>T</fixed-case>ranslation Pieces</title>
      <author><first>Catarina</first><last>Silva</last></author>
      <pages>204–212</pages>
      <url hash="5ed30a2f">W19-6735</url>
      <bibkey>silva-2019-improving</bibkey>
    </paper>
    <paper id="36">
      <title>Raising the <fixed-case>TM</fixed-case> Threshold in Neural <fixed-case>MT</fixed-case> Post-Editing: a Case Study on<fixed-case>T</fixed-case>wo Datasets</title>
      <author><first>Anna</first><last>Zaretskaya</last></author>
      <pages>213–218</pages>
      <url hash="d98fcbd7">W19-6736</url>
      <bibkey>zaretskaya-2019-raising</bibkey>
    </paper>
    <paper id="37">
      <title>Incremental Adaptation of <fixed-case>NMT</fixed-case> for Professional Post-editors: A User Study</title>
      <author><first>Miguel</first><last>Domingo</last></author>
      <author><first>Mercedes</first><last>García-Martínez</last></author>
      <author><first>Álvaro</first><last>Peris</last></author>
      <author><first>Alexandre</first><last>Helle</last></author>
      <author><first>Amando</first><last>Estela</last></author>
      <author><first>Laurent</first><last>Bié</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <author><first>Manuel</first><last>Herranz</last></author>
      <pages>219–227</pages>
      <url hash="8accfa30">W19-6737</url>
      <bibkey>domingo-etal-2019-incremental</bibkey>
    </paper>
    <paper id="38">
      <title>When less is more in Neural Quality Estimation of Machine Translation. An industry case study</title>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <author><first>Félix Do</first><last>Carmo</last></author>
      <author><first>Joss</first><last>Moorkens</last></author>
      <author><first>Eric</first><last>Paquin</last></author>
      <author><first>Dag</first><last>Schmidtke</last></author>
      <author><first>Declan</first><last>Groves</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>228–235</pages>
      <url hash="f19abae3">W19-6738</url>
      <bibkey>shterionov-etal-2019-less</bibkey>
    </paper>
  </volume>
  <volume id="68" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages</booktitle>
      <url hash="8d3db337">W19-68</url>
      <editor><first>Alina</first><last>Karakanta</last></editor>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Chao-Hong</first><last>Liu</last></editor>
      <editor><first>Jonathan</first><last>Washington</last></editor>
      <editor><first>Nathaniel</first><last>Oco</last></editor>
      <editor><first>Surafel Melaku</first><last>Lakew</last></editor>
      <editor><first>Valentin</first><last>Malykh</last></editor>
      <editor><first>Xiaobing</first><last>Zhao</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="3f3f87a9">W19-6800</url>
      <bibkey>ws-2019-technologies</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Finite State Transducer based Morphology analysis for <fixed-case>M</fixed-case>alayalam Language</title>
      <author><first>Santhosh</first><last>Thottingal</last></author>
      <pages>1–5</pages>
      <url hash="dbf7d64e">W19-6801</url>
      <bibkey>thottingal-2019-finite</bibkey>
    </paper>
    <paper id="2">
      <title>A step towards <fixed-case>T</fixed-case>orwali machine translation: an analysis of morphosyntactic challenges in a low-resource language</title>
      <author><first>Naeem</first><last>Uddin</last></author>
      <author><first>Jalal</first><last>Uddin</last></author>
      <pages>6–10</pages>
      <url hash="4e9eb05e">W19-6802</url>
      <bibkey>uddin-uddin-2019-step</bibkey>
    </paper>
    <paper id="3">
      <title>Workflows for kickstarting <fixed-case>RBMT</fixed-case> in virtually No-Resource Situation</title>
      <author><first>Tommi A</first><last>Pirinen</last></author>
      <pages>11–16</pages>
      <url hash="0a0c5abe">W19-6803</url>
      <bibkey>pirinen-2019-workflows</bibkey>
    </paper>
    <paper id="4">
      <title>A Continuous Improvement Framework of Machine Translation for <fixed-case>S</fixed-case>hipibo-Konibo</title>
      <author><first>Héctor Erasmo Gómez</first><last>Montoya</last></author>
      <author><first>Kervy Dante Rivas</first><last>Rojas</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <pages>17–23</pages>
      <url hash="420188d3">W19-6804</url>
      <bibkey>montoya-etal-2019-continuous</bibkey>
    </paper>
    <paper id="5">
      <title>A free/open-source rule-based machine translation system for Crimean Tatar to Turkish</title>
      <author><first>Memduh</first><last>Gökırmak</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Jonathan</first><last>Washington</last></author>
      <pages>24–31</pages>
      <url hash="9892f661">W19-6805</url>
      <bibkey>gokirmak-etal-2019-machine</bibkey>
    </paper>
    <paper id="6">
      <title>Developing a Neural Machine Translation system for <fixed-case>I</fixed-case>rish</title>
      <author><first>Arne</first><last>Defauw</last></author>
      <author><first>Sara</first><last>Szoc</last></author>
      <author><first>Tom</first><last>Vanallemeersch</last></author>
      <author><first>Anna</first><last>Bardadym</last></author>
      <author><first>Joris</first><last>Brabers</last></author>
      <author><first>Frederic</first><last>Everaert</last></author>
      <author><first>Kim</first><last>Scholte</last></author>
      <author><first>Koen</first><last>Van Winckel</last></author>
      <author><first>Joachim</first><last>Van den Bogaert</last></author>
      <pages>32–38</pages>
      <url hash="34997e0e">W19-6806</url>
      <bibkey>defauw-etal-2019-developing</bibkey>
    </paper>
    <paper id="7">
      <title>Sentence-Level Adaptation for Low-Resource Neural Machine Translation</title>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Yash Kumar</first><last>Lal</last></author>
      <pages>39–47</pages>
      <url hash="a35485b3">W19-6807</url>
      <bibkey>mueller-lal-2019-sentence</bibkey>
    </paper>
    <paper id="8">
      <title>Corpus Building for Low Resource Languages in the <fixed-case>DARPA</fixed-case> <fixed-case>LORELEI</fixed-case> Program</title>
      <author><first>Jennifer</first><last>Tracey</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Zhiyi</first><last>Song</last></author>
      <author><first>Michael</first><last>Arrigo</last></author>
      <author><first>Kira</first><last>Griffitt</last></author>
      <author><first>Dana</first><last>Delgado</last></author>
      <author><first>Dave</first><last>Graff</last></author>
      <author><first>Seth</first><last>Kulick</last></author>
      <author><first>Justin</first><last>Mott</last></author>
      <author><first>Neil</first><last>Kuster</last></author>
      <pages>48–55</pages>
      <url hash="b789241b">W19-6808</url>
      <bibkey>tracey-etal-2019-corpus</bibkey>
    </paper>
    <paper id="9">
      <title>Multilingual Multimodal Machine Translation for <fixed-case>D</fixed-case>ravidian Languages utilizing Phonetic Transcription</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>Bernardo</first><last>Stearns</last></author>
      <author><first>Arun</first><last>Jayapal</last></author>
      <author><first>Sridevy</first><last>S</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>Manel</first><last>Zarrouk</last></author>
      <author><first>John P</first><last>McCrae</last></author>
      <pages>56–63</pages>
      <url hash="c1355cc1">W19-6809</url>
      <bibkey>chakravarthi-etal-2019-multilingual</bibkey>
    </paper>
    <paper id="10">
      <title>A3-108 Machine Translation System for <fixed-case>L</fixed-case>o<fixed-case>R</fixed-case>es<fixed-case>MT</fixed-case> 2019</title>
      <author><first>Saumitra</first><last>Yadav</last></author>
      <author><first>Vandan</first><last>Mujadia</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>64–67</pages>
      <url hash="f6d7f355">W19-6810</url>
      <bibkey>yadav-etal-2019-a3</bibkey>
    </paper>
    <paper id="11">
      <title>Factored Neural Machine Translation at <fixed-case>L</fixed-case>o<fixed-case>R</fixed-case>es<fixed-case>MT</fixed-case> 2019</title>
      <author><first>Saptarashmi</first><last>Bandyopadhyay</last></author>
      <pages>68–71</pages>
      <url hash="2f341431">W19-6811</url>
      <bibkey>bandyopadhyay-2019-factored</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>JHU</fixed-case> <fixed-case>L</fixed-case>o<fixed-case>R</fixed-case>es<fixed-case>MT</fixed-case> 2019 Shared Task System Description</title>
      <author><first>Paul</first><last>McNamee</last></author>
      <pages>72–75</pages>
      <url hash="732fe159">W19-6812</url>
      <bibkey>mcnamee-2019-jhu</bibkey>
    </paper>
  </volume>
  <volume id="69" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the Celtic Language Technology Workshop</booktitle>
      <url hash="ca14cb63">W19-69</url>
      <editor><first>Teresa</first><last>Lynn</last></editor>
      <editor><first>Delyth</first><last>Prys</last></editor>
      <editor><first>Colin</first><last>Batchelor</last></editor>
      <editor><first>Francis</first><last>Tyers</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>cltw</venue>
    </meta>
    <frontmatter>
      <url hash="f15f60dd">W19-6900</url>
      <bibkey>ws-2019-celtic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Unsupervised multi-word term recognition in <fixed-case>W</fixed-case>elsh</title>
      <author><first>Irena</first><last>Spasić</last></author>
      <author><first>David</first><last>Owen</last></author>
      <author><first>Dawn</first><last>Knight</last></author>
      <author><first>Andreas</first><last>Artemiou</last></author>
      <pages>1–6</pages>
      <url hash="2c6b0703">W19-6901</url>
      <bibkey>spasic-etal-2019-unsupervised</bibkey>
    </paper>
    <paper id="2">
      <title>Universal dependencies for <fixed-case>S</fixed-case>cottish <fixed-case>G</fixed-case>aelic: syntax</title>
      <author><first>Colin</first><last>Batchelor</last></author>
      <pages>7–15</pages>
      <url hash="98d8c482">W19-6902</url>
      <bibkey>batchelor-2019-universal</bibkey>
    </paper>
    <paper id="3">
      <title>Speech technology and Argentinean <fixed-case>W</fixed-case>elsh</title>
      <author><first>Elise</first><last>Bell</last></author>
      <pages>16–20</pages>
      <url hash="a614558e">W19-6903</url>
      <bibkey>bell-2019-speech</bibkey>
    </paper>
    <paper id="4">
      <title>Development of a <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies treebank for <fixed-case>W</fixed-case>elsh</title>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Francis M.</first><last>Tyers</last></author>
      <pages>21–31</pages>
      <url hash="ebd06954">W19-6904</url>
      <bibkey>heinecke-tyers-2019-development</bibkey>
    </paper>
    <paper id="5">
      <title>Code-switching in <fixed-case>I</fixed-case>rish tweets: A preliminary analysis</title>
      <author><first>Teresa</first><last>Lynn</last></author>
      <author><first>Kevin</first><last>Scannell</last></author>
      <pages>32–40</pages>
      <url hash="65f5d67d">W19-6905</url>
      <bibkey>lynn-scannell-2019-code</bibkey>
    </paper>
    <paper id="6">
      <title>Embedding <fixed-case>E</fixed-case>nglish to <fixed-case>W</fixed-case>elsh <fixed-case>MT</fixed-case> in a Private Company</title>
      <author><first>Myfyr</first><last>Prys</last></author>
      <author><first>Dewi Bryn</first><last>Jones</last></author>
      <pages>41–47</pages>
      <url hash="bf58a6d2">W19-6906</url>
      <bibkey>prys-jones-2019-embedding</bibkey>
    </paper>
    <paper id="7">
      <title>Adapting Term Recognition to an Under-Resourced Language: the Case of <fixed-case>I</fixed-case>rish</title>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Adrian</first><last>Doyle</last></author>
      <pages>48–57</pages>
      <url hash="80c75fd5">W19-6907</url>
      <bibkey>mccrae-doyle-2019-adapting</bibkey>
    </paper>
    <paper id="8">
      <title>Leveraging backtranslation to improve machine translation for <fixed-case>G</fixed-case>aelic languages</title>
      <author><first>Meghan</first><last>Dowling</last></author>
      <author><first>Teresa</first><last>Lynn</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>58–62</pages>
      <url hash="89071c40">W19-6908</url>
      <bibkey>dowling-etal-2019-leveraging</bibkey>
    </paper>
    <paper id="9">
      <title>Improving full-text search results on dúchas.ie using language technology</title>
      <author><first>Brian</first><last>Ó Raghallaigh</last></author>
      <author><first>Kevin</first><last>Scannell</last></author>
      <author><first>Meghan</first><last>Dowling</last></author>
      <pages>63–69</pages>
      <url hash="f20b1817">W19-6909</url>
      <bibkey>o-raghallaigh-etal-2019-improving</bibkey>
    </paper>
    <paper id="10">
      <title>A Character-Level <fixed-case>LSTM</fixed-case> Network Model for Tokenizing the <fixed-case>O</fixed-case>ld <fixed-case>I</fixed-case>rish text of the <fixed-case>W</fixed-case>ürzburg Glosses on the <fixed-case>P</fixed-case>auline Epistles</title>
      <author><first>Adrian</first><last>Doyle</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <author><first>Clodagh</first><last>Downey</last></author>
      <pages>70–79</pages>
      <url hash="2184b019">W19-6910</url>
      <bibkey>doyle-etal-2019-character</bibkey>
    </paper>
    <paper id="11">
      <title>A Green Approach for an <fixed-case>I</fixed-case>rish App (Refactor, reuse and keeping it real)</title>
      <author><first>Monica</first><last>Ward</last></author>
      <author><first>Maxim</first><last>Mozgovoy</last></author>
      <author><first>Marina</first><last>Purgina</last></author>
      <pages>80–88</pages>
      <url hash="389db1c0">W19-6911</url>
      <bibkey>ward-etal-2019-green</bibkey>
    </paper>
  </volume>
  <volume id="70" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production</booktitle>
      <url hash="a3dc8ceb">W19-70</url>
      <editor><first>Michael</first><last>Carl</last></editor>
      <editor><first>Silvia</first><last>Hansen-Schirra</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="eaa8167c">W19-7000</url>
      <bibkey>ws-2019-memento</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Edit distances do not describe editing, but they can be useful for translation process research</title>
      <author><first>Félix</first><last>do Carmo</last></author>
      <pages>1–2</pages>
      <url hash="affd70cc">W19-7001</url>
      <bibkey>do-carmo-2019-edit</bibkey>
    </paper>
    <paper id="2">
      <title>Modelling word translation entropy and syntactic equivalence with machine learning</title>
      <author><first>Bram</first><last>Vanroy</last></author>
      <author><first>Orphée</first><last>De Clercq</last></author>
      <author><first>Lieve</first><last>Macken</last></author>
      <pages>3–4</pages>
      <url hash="e48ef07d">W19-7002</url>
      <bibkey>vanroy-etal-2019-modelling</bibkey>
    </paper>
    <paper id="3">
      <title>Comparison of temporal, technical and cognitive dimension measurements for post-editing effort</title>
      <author><first>Cristina</first><last>Cumbreno</last></author>
      <author><first>Nora</first><last>Aranberri</last></author>
      <pages>5–6</pages>
      <url hash="d8c591ef">W19-7003</url>
      <bibkey>cumbreno-aranberri-2019-comparison</bibkey>
    </paper>
    <paper id="4">
      <title>Translation Quality and Effort Prediction in Professional Machine Translation Post-Editing</title>
      <author><first>Jennifer</first><last>Vardaro</last></author>
      <author><first>Moritz</first><last>Schaeffer</last></author>
      <author><first>Silvia</first><last>Hansen-Schirra</last></author>
      <pages>7–8</pages>
      <url hash="75ab2def">W19-7004</url>
      <bibkey>vardaro-etal-2019-translation</bibkey>
    </paper>
    <paper id="5">
      <title>With or without post-editing processes? Evidence for a gap in machine translation evaluation</title>
      <author><first>Caroline</first><last>Rossi</last></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last></author>
      <pages>9–10</pages>
      <url hash="0df0dc69">W19-7005</url>
      <bibkey>rossi-esperanca-rodier-2019-without</bibkey>
    </paper>
    <paper id="6">
      <title>Investigating Correlations Between Human Translation and <fixed-case>MT</fixed-case> Output</title>
      <author><first>Samar A.</first><last>Almazroei</last></author>
      <author><first>Haruka</first><last>Ogawa</last></author>
      <author><first>Devin</first><last>Gilbert</last></author>
      <pages>11–13</pages>
      <url hash="55dc496a">W19-7006</url>
      <bibkey>almazroei-etal-2019-investigating</bibkey>
    </paper>
    <paper id="7">
      <title>Lexical Representation &amp; Retrieval on Monolingual Interpretative text production</title>
      <author><first>Debasish</first><last>Sahoo</last></author>
      <author><first>Michael</first><last>Carl</last></author>
      <pages>14–16</pages>
      <url hash="4972d92a">W19-7007</url>
      <bibkey>sahoo-carl-2019-lexical</bibkey>
    </paper>
    <paper id="8">
      <title>Predicting Cognitive Effort in Translation Production</title>
      <author><first>Yuxiang</first><last>Wei</last></author>
      <pages>17–17</pages>
      <url hash="9a1066fd">W19-7008</url>
      <bibkey>wei-2019-predicting</bibkey>
    </paper>
    <paper id="9">
      <title>Computerized Note-taking in Consecutive Interpreting: A Pen-voice Integrated Approach towards Omissions, Additions and Reconstructions in Notes</title>
      <author><first>Huolingxiao</first><last>Kuang</last></author>
      <pages>18–18</pages>
      <url hash="5ff99e9d">W19-7009</url>
      <bibkey>kuang-2019-computerized</bibkey>
    </paper>
    <paper id="10">
      <title>Automatization of subprocesses in subtitling</title>
      <author><first>Anke</first><last>Tardel</last></author>
      <author><first>Silvia</first><last>Hansen-Schirra</last></author>
      <author><first>Silke</first><last>Gutermuth</last></author>
      <author><first>Moritz</first><last>Schaeffer</last></author>
      <pages>19–20</pages>
      <url hash="d490e839">W19-7010</url>
      <bibkey>tardel-etal-2019-automatization</bibkey>
    </paper>
    <paper id="11">
      <title>Correlating Metaphors to Behavioural Data: A <fixed-case>CRITT</fixed-case> <fixed-case>TPR</fixed-case>-<fixed-case>DB</fixed-case>-based Study</title>
      <author><first>Faustino</first><last>Dardi</last></author>
      <pages>21–22</pages>
      <url hash="37d5763c">W19-7011</url>
      <bibkey>dardi-2019-correlating</bibkey>
    </paper>
    <paper id="12">
      <title>Exploring Cognitive Effort in Written Translation of <fixed-case>C</fixed-case>hinese Neologisms: An Eye-tracking and Keylogging Study</title>
      <author><first>Jinjin</first><last>Chen</last></author>
      <author><first>Defeng</first><last>Li</last></author>
      <author><first>Victoria</first><last>Lei</last></author>
      <pages>23–23</pages>
      <url hash="d82e8661">W19-7012</url>
      <bibkey>chen-etal-2019-exploring</bibkey>
    </paper>
  </volume>
  <volume id="71" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Multilingualism at the Intersection of Knowledge Bases and Machine Translation</booktitle>
      <url hash="5db98a78">W19-71</url>
      <editor><first>Mihael</first><last>Arcan</last></editor>
      <editor><first>Marco</first><last>Turchi</last></editor>
      <editor><first>Jinhua</first><last>Du</last></editor>
      <editor><first>Dimitar</first><last>Shterionov</last></editor>
      <editor><first>Daniel</first><last>Torregrosa</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="80ba498a">W19-7100</url>
      <bibkey>ws-2019-multilingualism</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Gloss Translation for Under-resourced Languages using Multilingual Neural Machine Translation</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>1–7</pages>
      <url hash="cfd12103">W19-7101</url>
      <bibkey>chakravarthi-etal-2019-wordnet</bibkey>
    </paper>
    <paper id="2">
      <title>Leveraging <fixed-case>SNOMED</fixed-case> <fixed-case>CT</fixed-case> terms and relations for machine translation of clinical texts from <fixed-case>B</fixed-case>asque to <fixed-case>S</fixed-case>panish</title>
      <author><first>Xabier</first><last>Soto</last></author>
      <author><first>Olatz</first><last>Perez-De-Viñaspre</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <pages>8–18</pages>
      <url hash="a0bf8fc2">W19-7102</url>
      <bibkey>soto-etal-2019-leveraging</bibkey>
    </paper>
  </volume>
  <volume id="72" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Patent and Scientific Literature Translation</booktitle>
      <url hash="dd316611">W19-72</url>
      <editor><first>Takehito</first><last>Utsuro</last></editor>
      <editor><first>Katsuhito</first><last>Sudoh</last></editor>
      <editor><first>Takashi</first><last>Tsunakawa</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="6617f7e3">W19-7200</url>
      <bibkey>ws-2019-patent</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation</title>
      <author><first>Junya</first><last>Ono</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>4–12</pages>
      <url hash="1a84c146">W19-7201</url>
      <bibkey>ono-etal-2019-hybrid</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="2">
      <title>Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Gideon</first><last>Maillette de Buy Wenniger</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>13–23</pages>
      <url hash="fb5cd9e6">W19-7202</url>
      <bibkey>poncelas-etal-2019-transductive</bibkey>
    </paper>
    <paper id="3">
      <title>A Multi-Hop Attention for <fixed-case>RNN</fixed-case> based Neural Machine Translation</title>
      <author><first>Shohei</first><last>Iida</last></author>
      <author><first>Ryuichiro</first><last>Kimura</last></author>
      <author><first>Hongyi</first><last>Cui</last></author>
      <author><first>Po-Hsuan</first><last>Hung</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>24–31</pages>
      <url hash="4d80fe2c">W19-7203</url>
      <bibkey>iida-etal-2019-multi</bibkey>
    </paper>
    <paper id="4">
      <title>Decision-making, Risk, and Gist Machine Translation in the Work of Patent Professionals</title>
      <author><first>Mary</first><last>Nurminen</last></author>
      <pages>32–42</pages>
      <url hash="3a3eeb3f">W19-7204</url>
      <bibkey>nurminen-2019-decision</bibkey>
    </paper>
  </volume>
  <volume id="73" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the Qualities of Literary Machine Translation</booktitle>
      <url hash="87d46cfe">W19-73</url>
      <editor><first>James</first><last>Hadley</last></editor>
      <editor><first>Maja</first><last>Popović</last></editor>
      <editor><first>Haithem</first><last>Afli</last></editor>
      <editor><first>Andy</first><last>Way</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="721257c7">W19-7300</url>
      <bibkey>ws-2019-qualities</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Neural Machine Translation of Literary Texts from <fixed-case>E</fixed-case>nglish to <fixed-case>S</fixed-case>lovene</title>
      <author><first>Taja</first><last>Kuzman</last></author>
      <author><first>Špela</first><last>Vintar</last></author>
      <author><first>Mihael</first><last>Arčan</last></author>
      <pages>1–9</pages>
      <url hash="fbbba106">W19-7301</url>
      <bibkey>kuzman-etal-2019-neural</bibkey>
    </paper>
    <paper id="2">
      <title>The Challenges of Using Neural Machine Translation for Literature</title>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <pages>10–19</pages>
      <url hash="0a0cc1b4">W19-7302</url>
      <bibkey>matusov-2019-challenges</bibkey>
    </paper>
    <paper id="3">
      <title>Using Intergaelic to pre-translate and subsequently post-edit a sci-fi novel from <fixed-case>S</fixed-case>cottish <fixed-case>G</fixed-case>aelic to <fixed-case>I</fixed-case>rish</title>
      <author><first>Eoin P. Ó</first><last>Murchú</last></author>
      <pages>20–25</pages>
      <url hash="cfca6dfb">W19-7303</url>
      <bibkey>murchu-2019-using</bibkey>
    </paper>
    <paper id="4">
      <title>Would <fixed-case>MT</fixed-case> kill creativity in literary retranslation?</title>
      <author><first>Mehmet</first><last>Şahin</last></author>
      <author><first>Sabri</first><last>Gürses</last></author>
      <pages>26–34</pages>
      <url hash="1d7f8025">W19-7304</url>
      <bibkey>sahin-gurses-2019-mt</bibkey>
    </paper>
    <paper id="5">
      <title>Free indirect discourse: an insurmountable challenge for literary <fixed-case>MT</fixed-case> systems?</title>
      <author><first>Kristiina</first><last>Taivalkoski-Shilov</last></author>
      <pages>35–39</pages>
      <url hash="5069b875">W19-7305</url>
      <bibkey>taivalkoski-shilov-2019-free</bibkey>
    </paper>
    <paper id="6">
      <title>When a ‘sport’ is a person and other issues for <fixed-case>NMT</fixed-case> of novels</title>
      <author><first>Arda</first><last>Tezcan</last></author>
      <author><first>Joke</first><last>Daems</last></author>
      <author><first>Lieve</first><last>Macken</last></author>
      <pages>40–49</pages>
      <url hash="f5baf48f">W19-7306</url>
      <bibkey>tezcan-etal-2019-sport</bibkey>
    </paper>
  </volume>
  <volume id="74" ingest-date="2019-08-16">
    <meta>
      <booktitle>Proceedings of the 3rd International Conference on Natural Language and Speech Processing</booktitle>
      <url hash="0fd5a799">W19-74</url>
      <editor><first>Mourad</first><last>Abbas</last></editor>
      <editor><first>Abed Alhakim</first><last>Freihat</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Trento, Italy</address>
      <month>September</month>
      <year>2019</year>
      <venue>icnlsp</venue>
    </meta>
    <frontmatter>
      <url hash="4156107d">W19-7400</url>
      <bibkey>ws-2019-international-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>witter Bot Detection using Diversity Measures</title>
      <author><first>Dijana</first><last>Kosmajac</last></author>
      <author><first>Vlado</first><last>Keselj</last></author>
      <pages>1–8</pages>
      <url hash="6f2f1882">W19-7401</url>
      <bibkey>kosmajac-keselj-2019-twitter</bibkey>
    </paper>
    <paper id="2">
      <title>Aligning the <fixed-case>I</fixed-case>ndo<fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et with the <fixed-case>P</fixed-case>rinceton <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Nandu Chandran</first><last>Nair</last></author>
      <author><first>Rajendran Sankara</first><last>Velayuthan</last></author>
      <author><first>Khuyagbaatar</first><last>Batsuren</last></author>
      <pages>9–16</pages>
      <url hash="517c03d2">W19-7402</url>
      <bibkey>nair-etal-2019-aligning</bibkey>
    </paper>
    <paper id="3">
      <title>Automatic Data-Driven Approaches for Evaluating the Phonemic Verbal Fluency Task with Healthy Adults</title>
      <author><first>Hali</first><last>Lindsay</last></author>
      <author><first>Nicklas</first><last>Linz</last></author>
      <author><first>Johannes</first><last>Troeger</last></author>
      <author><first>Jan</first><last>Alexandersson</last></author>
      <pages>17–24</pages>
      <url hash="d7541202">W19-7403</url>
      <bibkey>lindsay-etal-2019-automatic</bibkey>
    </paper>
    <paper id="4">
      <title>Automatic Detection and Classification of Argument Components using Multi-task Deep Neural Network</title>
      <author><first>Jean-Christophe</first><last>Mensonides</last></author>
      <author><first>Sébastien</first><last>Harispe</last></author>
      <author><first>Jacky</first><last>Montmain</last></author>
      <author><first>Véronique</first><last>Thireau</last></author>
      <pages>25–33</pages>
      <url hash="1ff80091">W19-7404</url>
      <bibkey>mensonides-etal-2019-automatic</bibkey>
    </paper>
    <paper id="5">
      <title>Multi Sense Embeddings from Topic Models</title>
      <author><first>Shobhit</first><last>Jain</last></author>
      <author><first>Sravan Babu</first><last>Bodapati</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Anima</first><last>Anandkumar</last></author>
      <pages>34–41</pages>
      <url hash="6bcc8135">W19-7405</url>
      <bibkey>jain-etal-2019-multi</bibkey>
    </paper>
    <paper id="6">
      <title>Automatic <fixed-case>A</fixed-case>rabic Text Summarization Based on Fuzzy Logic</title>
      <author><first>Lamees Al</first><last>Qassem</last></author>
      <author><first>Di</first><last>Wang</last></author>
      <author><first>Hassan</first><last>Barada</last></author>
      <author><first>Ahmad</first><last>Al-Rubaie</last></author>
      <author><first>Nawaf</first><last>Almoosa</last></author>
      <pages>42–48</pages>
      <url hash="bd93d7d0">W19-7406</url>
      <bibkey>qassem-etal-2019-automatic</bibkey>
    </paper>
    <paper id="7">
      <title>An <fixed-case>A</fixed-case>rabic Multi-Domain Spoken Language Understanding System</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Rachida</first><last>Djeradi</last></author>
      <author><first>Amar</first><last>Djeradi</last></author>
      <pages>49–53</pages>
      <url hash="da47aea5">W19-7407</url>
      <bibkey>lichouri-etal-2019-arabic</bibkey>
    </paper>
    <paper id="8">
      <title>Building a Speech Corpus based on <fixed-case>A</fixed-case>rabic Podcasts for Language and Dialect Identification</title>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <pages>54–58</pages>
      <url hash="9098ed45">W19-7408</url>
      <bibkey>lounnas-etal-2019-building</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic Text Tagging of <fixed-case>A</fixed-case>rabic News Articles Using Ensemble Deep Learning Models</title>
      <author><first>Ashraf</first><last>Elnagar</last></author>
      <author><first>Omar</first><last>Einea</last></author>
      <author><first>Ridhwan</first><last>Al-Debsi</last></author>
      <pages>59–66</pages>
      <url hash="793ade76">W19-7409</url>
      <bibkey>elnagar-etal-2019-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>A Probabilistic Approach for Confidence Scoring in Speech Recognition</title>
      <author><first>Punnoose</first><last>Kuriakose</last></author>
      <pages>67–74</pages>
      <url hash="7691c86a">W19-7410</url>
      <bibkey>kuriakose-2019-probabilistic</bibkey>
    </paper>
    <paper id="11">
      <title>A Crowdsourcing-based Approach for Speech Corpus Transcription Case of <fixed-case>A</fixed-case>rabic <fixed-case>A</fixed-case>lgerian Dialects</title>
      <author><first>Ilyes</first><last>Zine</last></author>
      <author><first>Mohamed Cherif</first><last>Zeghad</last></author>
      <author><first>Soumia</first><last>Bougrine</last></author>
      <author><first>Hadda</first><last>Cherroun</last></author>
      <pages>75–83</pages>
      <url hash="d336d26f">W19-7411</url>
      <bibkey>zine-etal-2019-crowdsourcing</bibkey>
    </paper>
    <paper id="12">
      <title>Sample Size in <fixed-case>A</fixed-case>rabic Authorship Verification</title>
      <author><first>Hossam</first><last>Ahmed</last></author>
      <pages>84–91</pages>
      <url hash="8b1ef0c3">W19-7412</url>
      <bibkey>ahmed-2019-sample</bibkey>
    </paper>
    <paper id="13">
      <title>A folksonomy-based approach for profiling human perception on word similarity</title>
      <author><first>Guani</first><last>Wu</last></author>
      <author><first>Ker-Chau</first><last>Li</last></author>
      <pages>92–99</pages>
      <url hash="f1bca9ee">W19-7413</url>
      <bibkey>wu-li-2019-folksonomy</bibkey>
    </paper>
    <paper id="14">
      <title>Automatic Diacritization as Prerequisite Towards the Automatic Generation of <fixed-case>A</fixed-case>rabic Lexical Recognition Tests</title>
      <author><first>Osama</first><last>Hamed</last></author>
      <pages>100–106</pages>
      <url hash="2d7a4ecb">W19-7414</url>
      <bibkey>hamed-2019-automatic</bibkey>
    </paper>
    <paper id="15">
      <title>Expanding <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>hinese Dictionaries by <fixed-case>W</fixed-case>ikipedia Titles</title>
      <author><first>Wei-Ting</first><last>Chen</last></author>
      <author><first>Yu-Te</first><last>Wang</last></author>
      <author><first>Chuan-Jie</first><last>Lin</last></author>
      <pages>107–113</pages>
      <url hash="79895400">W19-7415</url>
      <bibkey>chen-etal-2019-expanding</bibkey>
    </paper>
    <paper id="16">
      <title>Production of Voicing Contrast in Children with Cochlear Implants</title>
      <author><first>Georgia</first><last>Koupka</last></author>
      <pages>114–119</pages>
      <url hash="ef6942ea">W19-7416</url>
      <bibkey>koupka-2019-production</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>um<fixed-case>SAT</fixed-case>: Hybrid <fixed-case>A</fixed-case>rabic Text Summarization based on symbolic and numerical Approaches</title>
      <author><first>Mohamed Amine</first><last>Cheragui</last></author>
      <author><first>Said Moulay</first><last>Lakhdar</last></author>
      <pages>120–127</pages>
      <url hash="ed02b0f4">W19-7417</url>
      <bibkey>cheragui-lakhdar-2019-sumsat</bibkey>
    </paper>
    <paper id="18">
      <title>Speech Coding Combining Chaos Encryption and Error Recovery for G.722.2 Codec</title>
      <author><first>Messaouda</first><last>Boumaraf</last></author>
      <author><first>Fatiha</first><last>Merazka</last></author>
      <pages>128–134</pages>
      <url hash="aa3cbfe7">W19-7418</url>
      <bibkey>boumaraf-merazka-2019-speech</bibkey>
    </paper>
    <paper id="19">
      <title>Compositional pre-training for neural semantic parsing</title>
      <author><first>Amir</first><last>Ziai</last></author>
      <pages>135–141</pages>
      <url hash="24313e62">W19-7419</url>
      <bibkey>ziai-2019-compositional</bibkey>
    </paper>
    <paper id="20">
      <title>Encoding Position Improves Recurrent Neural Text Summarizers</title>
      <author><first>Apostolos</first><last>Karanikolos</last></author>
      <author><first>Ioannis</first><last>Refanidis</last></author>
      <pages>142–150</pages>
      <url hash="95a775a3">W19-7420</url>
      <bibkey>karanikolos-refanidis-2019-encoding</bibkey>
    </paper>
  </volume>
  <volume id="75" ingest-date="2019-10-16">
    <meta>
      <booktitle>Proceedings of the 6th International Sanskrit Computational Linguistics Symposium</booktitle>
      <shortbooktitle>6th ISCLS</shortbooktitle>
      <url hash="48102019">W19-75</url>
      <editor><first>Pawan</first><last>Goyal</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>IIT Kharagpur, India</address>
      <month>October</month>
      <year>2019</year>
      <venue>iscls</venue>
    </meta>
    <frontmatter>
      <url hash="c2fb5a26">W19-7500</url>
      <bibkey>ws-2019-international-sanskrit</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>anskrit Sentence Generator</title>
      <author><first>Amba</first><last>Kulkarni</last></author>
      <author><first>Madhusoodana</first><last>Pai</last></author>
      <pages>1–13</pages>
      <url hash="9cbc04cf">W19-7501</url>
      <revision id="1" href="W19-7501v1" hash="3bbb4289"/>
      <revision id="2" href="W19-7501v2" hash="9cbc04cf">The authors have revised some of the explanatory linguistic aspects presented in the paper and produced a revised version.</revision>
      <bibkey>kulkarni-pai-2019-sanskrit</bibkey>
    </paper>
    <paper id="2">
      <title>Dependency Parser for <fixed-case>S</fixed-case>anskrit Verses</title>
      <author><first>Amba</first><last>Kulkarni</last></author>
      <author><first>Sanal</first><last>Vikram</last></author>
      <author><first>Sriram</first><last>K</last></author>
      <pages>14–27</pages>
      <url hash="a0bdaf7b">W19-7502</url>
      <revision id="1" href="W19-7502v1" hash="3626cd85"/>
      <revision id="2" href="W19-7502v2" hash="a0bdaf7b">The authors have provided a revised version of the draft, which adds more clarity to certain linguistic aspects to the paper.</revision>
      <bibkey>kulkarni-etal-2019-dependency</bibkey>
    </paper>
    <paper id="3">
      <title>Revisiting the Role of Feature Engineering for Compound Type Identification in <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Jivnesh</first><last>Sandhan</last></author>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <author><first>Laxmidhar</first><last>Behera</last></author>
      <pages>28–44</pages>
      <url hash="3e39d1a9">W19-7503</url>
      <bibkey>sandhan-etal-2019-revisiting</bibkey>
    </paper>
    <paper id="4">
      <title>A Machine Learning Approach for Identifying Compound Words from a <fixed-case>S</fixed-case>anskrit Text</title>
      <author><first>Premjith</first><last>B</last></author>
      <author><first>Chandni Chandran</first><last>V</last></author>
      <author><first>Shriganesh</first><last>Bhat</last></author>
      <author><first>Soman</first><last>Kp</last></author>
      <author><first>Prabaharan</first><last>P</last></author>
      <pages>45–51</pages>
      <url hash="52efd3c9">W19-7504</url>
      <bibkey>b-etal-2019-machine</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>LDA</fixed-case> Topic Modeling for pramāṇa Texts: A Case Study in <fixed-case>S</fixed-case>anskrit <fixed-case>NLP</fixed-case> Corpus Building</title>
      <author><first>Tyler</first><last>Neill</last></author>
      <pages>52–67</pages>
      <url hash="3b0f5101">W19-7505</url>
      <bibkey>neill-2019-lda</bibkey>
    </paper>
    <paper id="6">
      <title>A Platform for Community-sourced Indic Knowledge Processing at Scale</title>
      <author><first>Sai</first><last>Susarla</last></author>
      <author><first>Damodar Reddy</first><last>Challa</last></author>
      <pages>68–82</pages>
      <url hash="8418cbe3">W19-7506</url>
      <bibkey>susarla-challa-2019-platform</bibkey>
    </paper>
    <paper id="7">
      <title>On <fixed-case>S</fixed-case>anskrit and Information Retrieval</title>
      <author><first>Michaël</first><last>Meyer</last></author>
      <pages>83–96</pages>
      <url hash="e5946e4b">W19-7507</url>
      <bibkey>meyer-2019-sanskrit</bibkey>
    </paper>
    <paper id="8">
      <title>Framework for Question-Answering in <fixed-case>S</fixed-case>anskrit through Automated Construction of Knowledge Graphs</title>
      <author><first>Hrishikesh</first><last>Terdalkar</last></author>
      <author><first>Arnab</first><last>Bhattacharya</last></author>
      <pages>97–116</pages>
      <url hash="df205827">W19-7508</url>
      <bibkey>terdalkar-bhattacharya-2019-framework</bibkey>
    </paper>
    <paper id="9">
      <title>Introduction to <fixed-case>S</fixed-case>anskrit Shabdamitra: An Educational Application of <fixed-case>S</fixed-case>anskrit <fixed-case>W</fixed-case>ordnet</title>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <author><first>Nilesh</first><last>Joshi</last></author>
      <author><first>Sayali</first><last>Khare</last></author>
      <author><first>Hanumant</first><last>Redkar</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>117–133</pages>
      <url hash="b49448f5">W19-7509</url>
      <bibkey>kulkarni-etal-2019-introduction</bibkey>
    </paper>
    <paper id="10">
      <title>Vaijayantīkośa Knowledge-Net</title>
      <author><first>Aruna</first><last>Vayuvegula</last></author>
      <author><first>Satish</first><last>Kanugovi</last></author>
      <author><first>Sivaja S</first><last>Nair</last></author>
      <author><first>Shivani</first><last>V</last></author>
      <pages>134–151</pages>
      <url hash="993271a6">W19-7510</url>
      <bibkey>vayuvegula-etal-2019-vaijayantikosa</bibkey>
    </paper>
    <paper id="11">
      <title>Utilizing Word Embeddings based Features for Phylogenetic Tree Generation of <fixed-case>S</fixed-case>anskrit Texts</title>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Abhijeet</first><last>Dubey</last></author>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Gholemreza</first><last>Haffari</last></author>
      <pages>152–165</pages>
      <url hash="1cdfd59b">W19-7511</url>
      <bibkey>kanojia-etal-2019-utilizing-word</bibkey>
    </paper>
    <paper id="12">
      <title>An Introduction to the Textual History Tool</title>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Eivind</first><last>Kahrs</last></author>
      <pages>166–180</pages>
      <url hash="8e71171d">W19-7512</url>
      <bibkey>kanojia-etal-2019-introduction</bibkey>
    </paper>
    <paper id="13">
      <title>Pāli Sandhi – A computational approach</title>
      <author><first>Swati</first><last>Basapur</last></author>
      <author><first>Shivani</first><last>V</last></author>
      <author><first>Sivaja</first><last>Nair</last></author>
      <pages>181–192</pages>
      <url hash="9d659ea6">W19-7513</url>
      <bibkey>basapur-etal-2019-pali</bibkey>
    </paper>
  </volume>
  <volume id="76" ingest-date="2019-08-28">
    <meta>
      <booktitle>Proceedings of Machine Translation Summit XVII: Tutorial Abstracts</booktitle>
      <editor><first>Laura</first><last>Rossi</last></editor>
      <publisher>European Association for Machine Translation</publisher>
      <address>Dublin, Ireland</address>
      <month>August</month>
      <year>2019</year>
      <venue>mtsummit</venue>
    </meta>
    <frontmatter>
      <bibkey>ws-2019-machine-translation-summit-xvii-3</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The unreasonable effectiveness of Neural Models in Language Decoding</title>
      <author><first>Tony</first><last>O'Dowd</last></author>
      <abstract>This tutorial will provide an in-depth look at the experiments, jointly carried out by KantanMT and eBay during 2018, to determine which Neural Model delivers the best translation performance for eBay Customer Service content. It will lay out the timeline, process and mechanisms used to customise Neural MT models and how these were used in conjunction with Human Based evaluations to determine which approach to Neural MT provided the best translation outcomes.

The tutorial will cover the following topics and methods:
- Structural differences in Neural Networks and how they assist the language decoding process – RNN, CNN and TNN will be covered in detailed.
- Customisation of Neural MT using the KantanMT Platform
- Using MQM Framework for the evaluation and comparison of Translation Outputs and comparison to Human Translation
- Collation and analysis of experimental findings in reaching our decision to standardise on Transformer type networks.

Participants of the tutorial will get a clear understanding of Neural Model types and the differences, it will also cover how to customise these models and then how to set up a controlled experiment to determine translation performance.</abstract>
      <bibkey>o-dowd-2019-unreasonable</bibkey>
    </paper>
    <paper id="2">
      <title>Challenge Test Sets for <fixed-case>MT</fixed-case> Evaluation</title>
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Sheila</first><last>Castilho</last></author>
      <abstract>Most of the test sets used for the evaluation of MT systems reflect the frequency distribution of different phenomena found in naturally occurring data (”standard” or ”natural” test sets). However, to better understand particular strengths and weaknesses of MT systems, especially those based on neural networks, it is necessary to apply more focused evaluation procedures. Therefore, another type of test sets (”challenge” test sets, also called ”test suites”) is being increasingly employed in order to highlight points of difficulty which are relevant to model development, training, or using of the given system. This tutorial will be useful for anyone (researchers, developers, users, translators) interested in detailed evaluation and getting a better understanding of machine translation (MT) systems and models. The attendees will learn about the motivation and linguistic background of challenge test sets and a range of testing possibilities applied to the state-of-the-art MT systems, as well as a number of practical aspects and challenges.</abstract>
      <attachment type="presentation" hash="12760e39">W19-7602.Presentation.pdf</attachment>
      <bibkey>popovic-castilho-2019-challenge</bibkey>
    </paper>
    <paper id="3">
      <title>A Deep Learning Curve for Post-Editing 2</title>
      <author><first>Lena</first><last>Marg</last></author>
      <author><first>Alex&gt;</first><last>Yanishevsky</last></author>
      <abstract>In the last couple of years, machine translation technology has seen major changes with the breakthrough of neural machine translation (NMT), a growing number of providers and translation platforms. Machine Translation generally is experiencing a peak in demand from translation buyers, thanks to Machine Learning and AI being omnipresent in the media and at industry events. At the same time, new models for defining translation quality are becoming more widely adopted. These changes have profound implications for translators, LSPs and translation buyers: translators have to adjust their post-editing approaches, while LSPs and translation buyers are faced with decisions on selecting providers, best approaches for updating MT systems, financial investments, integrating tools, and getting the timing for implementation right for an optimum ROI.

In this tutorial on MT and post-editing we would like to continue sharing the latest trends in the field of MT technologies, and discuss their impact on post-editing practices as well as integrating MT on large, multi-language translation programs. We will look at tool compatibility, different use cases of MT and dynamic quality models, and share our experience of measuring performance.</abstract>
      <bibkey>marg-yanishevsky-2019-deep</bibkey>
    </paper>
    <paper id="4">
      <title>Practical Statistics for Research in Machine Translation and Translation Studies</title>
      <author><first>Antonio</first><last>Toral</last></author>
      <abstract>The tutorial will introduce a set of very useful statistical tests for conducting analyses in the research areas of Machine Translation (MT) and Translation Studies (TS). For each statistical test, the presenter will: 1) introduce it in the context of a common research example that pertains to the area of MT and/or TS 2) explain the technique behind the test and its assumptions 3) cover common pitfalls when the test is applied in research studies, and 4) conduct a hands-on activity so that attendees can put the knowledge acquired in practice straight-away. All examples and exercises will be in R. The following statistical tests will be covered: t-tests (both parametric and non-parametric), bootstrap resampling, Pearson and Spearman correlation coefficients, linear mixed-effects models.</abstract>
      <bibkey>toral-2019-practical</bibkey>
    </paper>
  </volume>
  <volume id="77" ingest-date="2019-10-02">
    <meta>
      <booktitle>Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)</booktitle>
      <url hash="0e85fbe0">W19-77</url>
      <editor><first>Kim</first><last>Gerdes</last></editor>
      <editor><first>Sylvain</first><last>Kahane</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Paris, France</address>
      <month>August</month>
      <year>2019</year>
      <venue>depling</venue>
      <venue>syntaxfest</venue>
    </meta>
    <frontmatter>
      <url hash="51282b95">W19-7700</url>
      <bibkey>ws-2019-international-dependency</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>yntax<fixed-case>F</fixed-case>est 2019 Invited talk - Inductive biases and language emergence in communicative agents</title>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <pages>1–1</pages>
      <url hash="f19be3bb">W19-7701</url>
      <doi>10.18653/v1/W19-7701</doi>
      <bibkey>dupoux-2019-syntaxfest</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>S</fixed-case>yntax<fixed-case>F</fixed-case>est 2019 Invited talk - Transferring <fixed-case>NLP</fixed-case> models across languages and domains</title>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>2–2</pages>
      <url hash="afc73caf">W19-7702</url>
      <doi>10.18653/v1/W19-7702</doi>
      <bibkey>plank-2019-syntaxfest</bibkey>
    </paper>
    <paper id="3">
      <title>Syntactic dependencies correspond to word pairs with high mutual information</title>
      <author><first>Richard</first><last>Futrell</last></author>
      <author><first>Peng</first><last>Qian</last></author>
      <author><first>Edward</first><last>Gibson</last></author>
      <author><first>Evelina</first><last>Fedorenko</last></author>
      <author><first>Idan</first><last>Blank</last></author>
      <pages>3–13</pages>
      <url hash="ae63f377">W19-7703</url>
      <doi>10.18653/v1/W19-7703</doi>
      <bibkey>futrell-etal-2019-syntactic</bibkey>
    </paper>
    <paper id="4">
      <title>Reflexives in <fixed-case>C</fixed-case>zech from a Dependency Perspective</title>
      <author><first>Vaclava</first><last>Kettnerova</last></author>
      <author><first>Marketa</first><last>Lopatkova</last></author>
      <pages>14–25</pages>
      <url hash="b855954d">W19-7704</url>
      <doi>10.18653/v1/W19-7704</doi>
      <bibkey>kettnerova-lopatkova-2019-reflexives</bibkey>
    </paper>
    <paper id="5">
      <title>Coordination of Unlike Grammatical Functions</title>
      <author><first>Agnieszka</first><last>Patejuk</last></author>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <pages>26–37</pages>
      <url hash="be1a3c2e">W19-7705</url>
      <doi>10.18653/v1/W19-7705</doi>
      <bibkey>patejuk-przepiorkowski-2019-coordination</bibkey>
    </paper>
    <paper id="6">
      <title>Predicate Catenae: A Dependency Grammar Analysis of It-Clefts</title>
      <author><first>Timothy</first><last>Osborne</last></author>
      <pages>38–48</pages>
      <url hash="f7966b14">W19-7706</url>
      <doi>10.18653/v1/W19-7706</doi>
      <bibkey>osborne-2019-predicate</bibkey>
    </paper>
    <paper id="7">
      <title>Noun Phrases Rooted by Adjectives: A Dependency Grammar Analysis of the Big Mess Construction</title>
      <author><first>Timothy</first><last>Osborne</last></author>
      <pages>49–59</pages>
      <url hash="9c8fb874">W19-7707</url>
      <doi>10.18653/v1/W19-7707</doi>
      <bibkey>osborne-2019-noun</bibkey>
    </paper>
    <paper id="8">
      <title>Cliticization of <fixed-case>S</fixed-case>erbian Personal Pronouns and Auxiliary Verbs. A Dependency-Based Account</title>
      <author><first>Jasmina</first><last>Milicevic</last></author>
      <pages>60–68</pages>
      <url hash="7bdc4519">W19-7708</url>
      <doi>10.18653/v1/W19-7708</doi>
      <bibkey>milicevic-2019-cliticization</bibkey>
    </paper>
    <paper id="9">
      <title>The evolution of spatial rationales in Tesnière’s stemmas</title>
      <author><first>Nicolas</first><last>Mazziotta</last></author>
      <pages>69–80</pages>
      <url hash="1d3679f1">W19-7709</url>
      <doi>10.18653/v1/W19-7709</doi>
      <bibkey>mazziotta-2019-evolution</bibkey>
    </paper>
    <paper id="10">
      <title>Toward a cognitive dependency grammar of <fixed-case>H</fixed-case>ungarian</title>
      <author><first>András</first><last>Imrényi</last></author>
      <pages>81–88</pages>
      <url hash="989c6530">W19-7710</url>
      <doi>10.18653/v1/W19-7710</doi>
      <bibkey>imrenyi-2019-toward</bibkey>
    </paper>
    <paper id="11">
      <title>Interpreting and defining connections in a dependency structure</title>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>89–99</pages>
      <url hash="5a2dcfe5">W19-7711</url>
      <doi>10.18653/v1/W19-7711</doi>
      <bibkey>kahane-2019-interpreting</bibkey>
    </paper>
    <paper id="12">
      <title>Identifying Grammar Rules for Language Education with Dependency Parsing in <fixed-case>G</fixed-case>erman</title>
      <author><first>Eleni</first><last>Metheniti</last></author>
      <author><first>Pomi</first><last>Park</last></author>
      <author><first>Kristina</first><last>Kolesova</last></author>
      <author><first>Günter</first><last>Neumann</last></author>
      <pages>100–111</pages>
      <url hash="89b03040">W19-7712</url>
      <doi>10.18653/v1/W19-7712</doi>
      <bibkey>metheniti-etal-2019-identifying</bibkey>
    </paper>
    <paper id="13">
      <title>How to Parse Low-Resource Languages: Cross-Lingual Parsing, Target Language Annotation, or Both?</title>
      <author><first>Ailsa</first><last>Meechan-Maddon</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <pages>112–120</pages>
      <url hash="d8fe7b57">W19-7713</url>
      <doi>10.18653/v1/W19-7713</doi>
      <bibkey>meechan-maddon-nivre-2019-parse</bibkey>
      <revision id="1" href="W19-7713v1" hash="d4f1e7f9"/>
      <revision id="2" href="W19-7713v2" hash="d8fe7b57" date="2022-02-11">Added missing acknowledgment.</revision>
    </paper>
    <paper id="14">
      <title>Word order variation in <fixed-case>M</fixed-case>byá <fixed-case>G</fixed-case>uaraní</title>
      <author><first>Angelika</first><last>Kiss</last></author>
      <author><first>Guillaume</first><last>Thomas</last></author>
      <pages>121–129</pages>
      <url hash="b380e95e">W19-7714</url>
      <doi>10.18653/v1/W19-7714</doi>
      <bibkey>kiss-thomas-2019-word</bibkey>
    </paper>
    <paper id="15">
      <title>Examining <fixed-case>MDD</fixed-case> and <fixed-case>MHD</fixed-case> as Syntactic Complexity Measures with Intermediate <fixed-case>J</fixed-case>apanese Learner Corpus Data</title>
      <author><first>Saeko</first><last>Komori</last></author>
      <author><first>Masatoshi</first><last>Sugiura</last></author>
      <author><first>Wenping</first><last>Li</last></author>
      <pages>130–135</pages>
      <url hash="d9ffb988">W19-7715</url>
      <doi>10.18653/v1/W19-7715</doi>
      <bibkey>komori-etal-2019-examining</bibkey>
    </paper>
    <paper id="16">
      <title>Dependency Parsing as Sequence Labeling with Head-Based Encoding and Multi-Task Learning</title>
      <author><first>Ophélie</first><last>Lacroix</last></author>
      <pages>136–143</pages>
      <url hash="6a6f74d6">W19-7716</url>
      <doi>10.18653/v1/W19-7716</doi>
      <bibkey>lacroix-2019-dependency</bibkey>
    </paper>
    <paper id="17">
      <title>Towards Deep <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Kira</first><last>Droganova</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>144–152</pages>
      <url hash="c6764aef">W19-7717</url>
      <doi>10.18653/v1/W19-7717</doi>
      <bibkey>droganova-zeman-2019-towards</bibkey>
    </paper>
    <paper id="18">
      <title>Delimiting Adverbial Meanings. A corpus-based comparative study on <fixed-case>C</fixed-case>zech spatial prepositions and their <fixed-case>E</fixed-case>nglish equivalents</title>
      <author><first>Marie</first><last>Mikulová</last></author>
      <author><first>Veronika</first><last>Kolářová</last></author>
      <author><first>Jarmila</first><last>Panevová</last></author>
      <author><first>Eva</first><last>Hajičová</last></author>
      <pages>153–159</pages>
      <url hash="69b60630">W19-7718</url>
      <doi>10.18653/v1/W19-7718</doi>
      <bibkey>mikulova-etal-2019-delimiting</bibkey>
    </paper>
    <paper id="19">
      <title>A <fixed-case>S</fixed-case>panish <fixed-case>E</fixed-case>-dictionary of Collocations</title>
      <author><first>Maria Auxiliadora</first><last>Barrios Rodriguez</last></author>
      <author><first>Igor</first><last>Boguslavsky</last></author>
      <pages>160–167</pages>
      <url hash="afb0a090">W19-7719</url>
      <doi>10.18653/v1/W19-7719</doi>
      <bibkey>barrios-rodriguez-boguslavsky-2019-spanish</bibkey>
    </paper>
    <paper id="20">
      <title>Exceptive constructions. A Dependency-based Analysis</title>
      <author><first>Mohamed</first><last>Galal</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Yomna</first><last>Safwat</last></author>
      <pages>168–174</pages>
      <url hash="432c1793">W19-7720</url>
      <doi>10.18653/v1/W19-7720</doi>
      <bibkey>galal-etal-2019-exceptive</bibkey>
    </paper>
    <paper id="21">
      <title>Quantitative Analysis on verb valence evolution of <fixed-case>C</fixed-case>hinese</title>
      <author><first>Bingli</first><last>Liu</last></author>
      <author><first>Chunshan</first><last>Xu</last></author>
      <pages>175–180</pages>
      <url hash="54e287f4">W19-7721</url>
      <doi>10.18653/v1/W19-7721</doi>
      <bibkey>liu-xu-2019-quantitative</bibkey>
    </paper>
    <paper id="22">
      <title>Association Metrics in Neural Transition-Based Dependency Parsing</title>
      <author><first>Patricia</first><last>Fischer</last></author>
      <author><first>Sebastian</first><last>Pütz</last></author>
      <author><first>Daniël</first><last>de Kok</last></author>
      <pages>181–189</pages>
      <url hash="30c3c801">W19-7722</url>
      <doi>10.18653/v1/W19-7722</doi>
      <bibkey>fischer-etal-2019-association</bibkey>
    </paper>
    <paper id="23">
      <title>Presenting <fixed-case>TWITTIRÒ</fixed-case>-<fixed-case>UD</fixed-case>: An <fixed-case>I</fixed-case>talian <fixed-case>T</fixed-case>witter Treebank in <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Alessandra Teresa</first><last>Cignarella</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>190–197</pages>
      <url hash="e57d084e">W19-7723</url>
      <doi>10.18653/v1/W19-7723</doi>
      <bibkey>cignarella-etal-2019-presenting</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>P</fixed-case>āṇinian Syntactico-Semantic Relation Labels</title>
      <author><first>Amba</first><last>Kulkarni</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>198–208</pages>
      <url hash="131c2b9a">W19-7724</url>
      <doi>10.18653/v1/W19-7724</doi>
      <bibkey>kulkarni-sharma-2019-paninian</bibkey>
    </paper>
    <paper id="25">
      <title>Experiments on human incremental parsing</title>
      <author><first>Leonid</first><last>Mityushin</last></author>
      <author><first>Leonid</first><last>Iomdin</last></author>
      <pages>209–215</pages>
      <url hash="ba946dcf">W19-7725</url>
      <doi>10.18653/v1/W19-7725</doi>
      <bibkey>mityushin-iomdin-2019-experiments</bibkey>
    </paper>
    <paper id="26">
      <title>Character-level Annotation for <fixed-case>C</fixed-case>hinese Surface-Syntactic <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Yixuan</first><last>Li</last></author>
      <author><first>Gerdes</first><last>Kim</last></author>
      <author><first>Dong</first><last>Chuanming</last></author>
      <pages>216–226</pages>
      <url hash="03a8daaf">W19-7726</url>
      <doi>10.18653/v1/W19-7726</doi>
      <bibkey>li-etal-2019-character</bibkey>
    </paper>
  </volume>
  <volume id="78" ingest-date="2019-10-02">
    <meta>
      <booktitle>Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)</booktitle>
      <url hash="e7045a46">W19-78</url>
      <editor><first>Marie</first><last>Candito</last></editor>
      <editor><first>Kilian</first><last>Evang</last></editor>
      <editor><first>Stephan</first><last>Oepen</last></editor>
      <editor><first>Djamé</first><last>Seddah</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Paris, France</address>
      <month>August</month>
      <year>2019</year>
      <venue>tlt</venue>
      <venue>syntaxfest</venue>
    </meta>
    <frontmatter>
      <url hash="8cdee6d4">W19-7800</url>
      <bibkey>ws-2019-international-treebanks</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>yntax<fixed-case>F</fixed-case>est 2019 Invited talk - Quantitative Computational Syntax: dependencies, intervention effects and word embeddings</title>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>1–1</pages>
      <url hash="009af682">W19-7801</url>
      <doi>10.18653/v1/W19-7801</doi>
      <bibkey>merlo-2019-syntaxfest</bibkey>
    </paper>
    <paper id="2">
      <title>Are formal restrictions on crossing dependencies epiphenominal?</title>
      <author><first>Himanshu</first><last>Yadav</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <pages>2–12</pages>
      <url hash="2e25eda6">W19-7802</url>
      <doi>10.18653/v1/W19-7802</doi>
      <bibkey>yadav-etal-2019-formal</bibkey>
    </paper>
    <paper id="3">
      <title>A Surface-Syntactic <fixed-case>UD</fixed-case> Treebank for <fixed-case>N</fixed-case>aija</title>
      <author><first>Bernard</first><last>Caron</last></author>
      <author><first>Marine</first><last>Courtin</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>13–24</pages>
      <url hash="18c66392">W19-7803</url>
      <doi>10.18653/v1/W19-7803</doi>
      <bibkey>caron-etal-2019-surface</bibkey>
    </paper>
    <paper id="4">
      <title>Can <fixed-case>G</fixed-case>reenbergian universals be induced from language networks?</title>
      <author><first>Kartik</first><last>Sharma</last></author>
      <author><first>Kaivalya</first><last>Swami</last></author>
      <author><first>Aditya</first><last>Shete</last></author>
      <author><first>Samar</first><last>Husain</last></author>
      <pages>25–37</pages>
      <url hash="d6705275">W19-7804</url>
      <doi>10.18653/v1/W19-7804</doi>
      <bibkey>sharma-etal-2019-greenbergian</bibkey>
    </paper>
    <paper id="5">
      <title>Parallel Dependency Treebank Annotated with Interlinked Verbal Synonym Classes and Roles</title>
      <author><first>Zdeňka</first><last>Urešová</last></author>
      <author><first>Eva</first><last>Fučíková</last></author>
      <author><first>Eva</first><last>Hajičová</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <pages>38–50</pages>
      <url hash="907e00ba">W19-7805</url>
      <doi>10.18653/v1/W19-7805</doi>
      <bibkey>uresova-etal-2019-parallel</bibkey>
    </paper>
    <paper id="6">
      <title>Ordering of Adverbials of Time and Place in Grammars and in an Annotated <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech Parallel Corpus</title>
      <author><first>Eva</first><last>Hajičová</last></author>
      <author><first>Jiří</first><last>Mírovský</last></author>
      <author><first>Kateřina</first><last>Rysová</last></author>
      <pages>51–60</pages>
      <url hash="e6fbc977">W19-7806</url>
      <doi>10.18653/v1/W19-7806</doi>
      <bibkey>hajicova-etal-2019-ordering</bibkey>
    </paper>
    <paper id="7">
      <title>Weighted posets: Learning surface order from dependency trees</title>
      <author><first>William</first><last>Dyer</last></author>
      <pages>61–73</pages>
      <url hash="7898666b">W19-7807</url>
      <doi>10.18653/v1/W19-7807</doi>
      <bibkey>dyer-2019-weighted</bibkey>
    </paper>
    <paper id="8">
      <title>Linked Open Treebanks. Interlinking Syntactically Annotated Corpora in the <fixed-case>LiLa</fixed-case> Knowledge Base of Linguistic Resources for <fixed-case>L</fixed-case>atin</title>
      <author><first>Francesco</first><last>Mambrini</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <pages>74–81</pages>
      <url hash="3114629a">W19-7808</url>
      <doi>10.18653/v1/W19-7808</doi>
      <bibkey>mambrini-passarotti-2019-linked</bibkey>
    </paper>
    <paper id="9">
      <title>Challenges of Annotating a Code-Switching Treebank</title>
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>82–90</pages>
      <url hash="286de035">W19-7809</url>
      <doi>10.18653/v1/W19-7809</doi>
      <bibkey>cetinoglu-coltekin-2019-challenges</bibkey>
    </paper>
    <paper id="10">
      <title>Dependency Parser for <fixed-case>B</fixed-case>engali-<fixed-case>E</fixed-case>nglish Code-Mixed Data enhanced with a Synthetic Treebank</title>
      <author><first>Urmi</first><last>Ghosh</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <author><first>Simran</first><last>Khanuja</last></author>
      <pages>91–99</pages>
      <url hash="0a198c21">W19-7810</url>
      <doi>10.18653/v1/W19-7810</doi>
      <bibkey>ghosh-etal-2019-dependency</bibkey>
    </paper>
    <paper id="11">
      <title>twee<fixed-case>D</fixed-case>e – A <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies treebank for <fixed-case>G</fixed-case>erman tweets</title>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Bich-Ngoc</first><last>Do</last></author>
      <pages>100–108</pages>
      <url hash="d59a817b">W19-7811</url>
      <doi>10.18653/v1/W19-7811</doi>
      <bibkey>rehbein-etal-2019-tweede</bibkey>
    </paper>
    <paper id="12">
      <title>Creating, Enriching and Valorizing Treebanks of <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek</title>
      <author><first>Alek</first><last>Keersmaekers</last></author>
      <author><first>Wouter</first><last>Mercelis</last></author>
      <author><first>Colin</first><last>Swaelens</last></author>
      <author><first>Toon</first><last>Van Hal</last></author>
      <pages>109–117</pages>
      <url hash="c9270e75">W19-7812</url>
      <doi>10.18653/v1/W19-7812</doi>
      <bibkey>keersmaekers-etal-2019-creating</bibkey>
    </paper>
    <paper id="13">
      <title>Syntax is clearer on the other side - Using parallel corpus to extract monolingual data</title>
      <author><first>Andrea</first><last>Dömötör</last></author>
      <pages>118–125</pages>
      <url hash="c8a65236">W19-7813</url>
      <doi>10.18653/v1/W19-7813</doi>
      <bibkey>domotor-2019-syntax</bibkey>
    </paper>
    <paper id="14">
      <title>Improving Surface-syntactic <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies (<fixed-case>SUD</fixed-case>): <fixed-case>MWE</fixed-case>s and deep syntactic features</title>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>126–132</pages>
      <url hash="318a3917">W19-7814</url>
      <doi>10.18653/v1/W19-7814</doi>
      <bibkey>gerdes-etal-2019-improving</bibkey>
    </paper>
    <paper id="15">
      <title>Artificially Evolved Chunks for Morphosyntactic Analysis</title>
      <author><first>Mark</first><last>Anderson</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <pages>133–143</pages>
      <url hash="9460bbcb">W19-7815</url>
      <doi>10.18653/v1/W19-7815</doi>
      <bibkey>anderson-etal-2019-artificially</bibkey>
    </paper>
    <paper id="16">
      <title>Challenges of language change and variation: towards an extended treebank of Medieval <fixed-case>F</fixed-case>rench</title>
      <author><first>Mathilde</first><last>Regnault</last></author>
      <author><first>Sophie</first><last>Prévost</last></author>
      <author><first>Eric</first><last>Villemonte de la Clergerie</last></author>
      <pages>144–150</pages>
      <url hash="d9487ab9">W19-7816</url>
      <doi>10.18653/v1/W19-7816</doi>
      <bibkey>regnault-etal-2019-challenges</bibkey>
    </paper>
  </volume>
  <volume id="79" ingest-date="2019-10-02">
    <meta>
      <booktitle>Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)</booktitle>
      <url hash="57a59bef">W19-79</url>
      <editor><first>Xinying</first><last>Chen</last></editor>
      <editor><first>Ramon</first><last>Ferrer-i-Cancho</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Paris, France</address>
      <month>August</month>
      <year>2019</year>
      <venue>quasy</venue>
      <venue>syntaxfest</venue>
    </meta>
    <frontmatter>
      <url hash="9fde9bd6">W19-7900</url>
      <bibkey>ws-2019-quantitative</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>yntax<fixed-case>F</fixed-case>est 2019 Invited talk - Dependency distance minimization: facts, theory and predictions</title>
      <author><first>Ramon</first><last>Ferrer-i-Cancho</last></author>
      <pages>1–1</pages>
      <url hash="a06ff559">W19-7901</url>
      <doi>10.18653/v1/W19-7901</doi>
      <bibkey>ferrer-i-cancho-2019-syntaxfest</bibkey>
    </paper>
    <paper id="2">
      <title>Information-theoretic locality properties of natural language</title>
      <author><first>Richard</first><last>Futrell</last></author>
      <pages>2–15</pages>
      <url hash="f98fd024">W19-7902</url>
      <doi>10.18653/v1/W19-7902</doi>
      <bibkey>futrell-2019-information</bibkey>
    </paper>
    <paper id="3">
      <title>Which annotation scheme is more expedient to measure syntactic difficulty and cognitive demand?</title>
      <author><first>Jianwei</first><last>Yan</last></author>
      <author><first>Haitao</first><last>Liu</last></author>
      <pages>16–24</pages>
      <url hash="f6973401">W19-7903</url>
      <doi>10.18653/v1/W19-7903</doi>
      <bibkey>yan-liu-2019-annotation</bibkey>
    </paper>
    <paper id="4">
      <title>A quantitative probe into the hierarchical structure of written <fixed-case>C</fixed-case>hinese</title>
      <author><first>Heng</first><last>Chen</last></author>
      <author><first>Haitao</first><last>Liu</last></author>
      <pages>25–32</pages>
      <url hash="5cec3191">W19-7904</url>
      <doi>10.18653/v1/W19-7904</doi>
      <bibkey>chen-liu-2019-quantitative</bibkey>
    </paper>
    <paper id="5">
      <title>A Comparative Corpus Analysis of <fixed-case>PP</fixed-case> Ordering in <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>hinese</title>
      <author><first>Zoey</first><last>Liu</last></author>
      <pages>33–45</pages>
      <url hash="9992f9da">W19-7905</url>
      <doi>10.18653/v1/W19-7905</doi>
      <bibkey>liu-2019-comparative</bibkey>
    </paper>
    <paper id="6">
      <title>Intervention effects in object relatives in <fixed-case>E</fixed-case>nglish and <fixed-case>I</fixed-case>talian: a study in quantitative computational syntax</title>
      <author><first>Giuseppe</first><last>Samo</last></author>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>46–56</pages>
      <url hash="a5ec7aff">W19-7906</url>
      <doi>10.18653/v1/W19-7906</doi>
      <bibkey>samo-merlo-2019-intervention</bibkey>
    </paper>
    <paper id="7">
      <title>An explanation of the decisive role of function words in driving syntactic development</title>
      <author><first>Anat</first><last>Ninio</last></author>
      <pages>57–67</pages>
      <url hash="ec344c01">W19-7907</url>
      <doi>10.18653/v1/W19-7907</doi>
      <bibkey>ninio-2019-explanation</bibkey>
    </paper>
    <paper id="8">
      <title>Extracting out of the subject in <fixed-case>F</fixed-case>rench: experimental evidence</title>
      <author><first>Anne</first><last>Abeillé</last></author>
      <author><first>Elodie</first><last>Winckel</last></author>
      <pages>68–74</pages>
      <url hash="ae65bb82">W19-7908</url>
      <doi>10.18653/v1/W19-7908</doi>
      <bibkey>abeille-winckel-2019-extracting</bibkey>
    </paper>
    <paper id="9">
      <title>The relation between dependency distance and frequency</title>
      <author><first>Xinying</first><last>Chen</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <pages>75–82</pages>
      <url hash="28817a5e">W19-7909</url>
      <doi>10.18653/v1/W19-7909</doi>
      <bibkey>chen-gerdes-2019-relation</bibkey>
    </paper>
    <paper id="10">
      <title>Full valency and the position of enclitics in the Old <fixed-case>C</fixed-case>zech</title>
      <author><first>Radek</first><last>Cech</last></author>
      <author><first>Pavel</first><last>Kosek</last></author>
      <author><first>Olga</first><last>Navratilova</last></author>
      <author><first>Jan</first><last>Macutek</last></author>
      <pages>83–88</pages>
      <url hash="0d16f04c">W19-7910</url>
      <doi>10.18653/v1/W19-7910</doi>
      <bibkey>cech-etal-2019-full</bibkey>
    </paper>
    <paper id="11">
      <title>Dependency Length Minimization vs. Word Order Constraints: An Empirical Study On 55 Treebanks</title>
      <author><first>Xiang</first><last>Yu</last></author>
      <author><first>Agnieszka</first><last>Falenska</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <pages>89–97</pages>
      <url hash="484cc200">W19-7911</url>
      <doi>10.18653/v1/W19-7911</doi>
      <bibkey>yu-etal-2019-dependency</bibkey>
    </paper>
    <paper id="12">
      <title>Advantages of the flux-based interpretation of dependency length minimization</title>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Chunxiao</first><last>Yan</last></author>
      <pages>98–109</pages>
      <url hash="b8fb5acf">W19-7912</url>
      <doi>10.18653/v1/W19-7912</doi>
      <bibkey>kahane-yan-2019-advantages</bibkey>
    </paper>
    <paper id="13">
      <title>Length of non-projective sentences: A pilot study using a <fixed-case>C</fixed-case>zech <fixed-case>UD</fixed-case> treebank</title>
      <author><first>Jan</first><last>Macutek</last></author>
      <author><first>Radek</first><last>Cech</last></author>
      <author><first>Jiri</first><last>Milicka</last></author>
      <pages>110–117</pages>
      <url hash="9f3e0af5">W19-7913</url>
      <doi>10.18653/v1/W19-7913</doi>
      <bibkey>macutek-etal-2019-length</bibkey>
    </paper>
    <paper id="14">
      <title>Gradient constraints on the use of <fixed-case>E</fixed-case>stonian possessive reflexives</title>
      <author><first>Suzanne</first><last>Lesage</last></author>
      <author><first>Olivier</first><last>Bonami</last></author>
      <pages>118–124</pages>
      <url hash="0ff895a0">W19-7914</url>
      <doi>10.18653/v1/W19-7914</doi>
      <bibkey>lesage-bonami-2019-gradient</bibkey>
    </paper>
    <paper id="15">
      <title>What can we learn from natural and artificial dependency trees</title>
      <author><first>Marine</first><last>Courtin</last></author>
      <author><first>Chunxiao</first><last>Yan</last></author>
      <pages>125–135</pages>
      <url hash="13a83663">W19-7915</url>
      <doi>10.18653/v1/W19-7915</doi>
      <bibkey>courtin-yan-2019-learn</bibkey>
    </paper>
  </volume>
  <volume id="80" ingest-date="2019-10-02">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)</booktitle>
      <url hash="bec2ab47">W19-80</url>
      <editor><first>Alexandre</first><last>Rademaker</last></editor>
      <editor><first>Francis</first><last>Tyers</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Paris, France</address>
      <month>August</month>
      <year>2019</year>
      <venue>udw</venue>
      <venue>syntaxfest</venue>
    </meta>
    <frontmatter>
      <url hash="5bfe9c21">W19-8000</url>
      <bibkey>ws-2019-universal</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>S</fixed-case>yntax<fixed-case>F</fixed-case>est 2019 Invited talk - Arguments and adjuncts</title>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <pages>1–1</pages>
      <url hash="78937d2b">W19-8001</url>
      <doi>10.18653/v1/W19-8001</doi>
      <bibkey>przepiorkowski-2019-syntaxfest</bibkey>
    </paper>
    <paper id="2">
      <title>Building a treebank for <fixed-case>O</fixed-case>ccitan: what use for <fixed-case>R</fixed-case>omance <fixed-case>UD</fixed-case> corpora?</title>
      <author><first>Aleksandra</first><last>Miletic</last></author>
      <author><first>Myriam</first><last>Bras</last></author>
      <author><first>Louise</first><last>Esher</last></author>
      <author><first>Jean</first><last>Sibille</last></author>
      <author><first>Marianne</first><last>Vergez-Couret</last></author>
      <pages>2–11</pages>
      <url hash="0a2aeac9">W19-8002</url>
      <doi>10.18653/v1/W19-8002</doi>
      <bibkey>miletic-etal-2019-building</bibkey>
    </paper>
    <paper id="3">
      <title>Developing <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for <fixed-case>W</fixed-case>olof</title>
      <author><first>Cheikh Bamba</first><last>Dione</last></author>
      <pages>12–23</pages>
      <url hash="4d8ded6e">W19-8003</url>
      <doi>10.18653/v1/W19-8003</doi>
      <bibkey>dione-2019-developing</bibkey>
    </paper>
    <paper id="4">
      <title>Improving <fixed-case>UD</fixed-case> processing via satellite resources for morphology</title>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>24–34</pages>
      <url hash="b2fefe52">W19-8004</url>
      <doi>10.18653/v1/W19-8004</doi>
      <bibkey>dobrovoljc-etal-2019-improving</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies in a galaxy far, far away... What makes Yoda’s <fixed-case>E</fixed-case>nglish truly alien</title>
      <author><first>Natalia</first><last>Levshina</last></author>
      <pages>35–45</pages>
      <url hash="a79668f1">W19-8005</url>
      <doi>10.18653/v1/W19-8005</doi>
      <bibkey>levshina-2019-universal</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>HDT</fixed-case>-<fixed-case>UD</fixed-case>: A very large <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank for <fixed-case>G</fixed-case>erman</title>
      <author><first>Emanuel</first><last>Borges Völker</last></author>
      <author><first>Maximilian</first><last>Wendt</last></author>
      <author><first>Felix</first><last>Hennig</last></author>
      <author><first>Arne</first><last>Köhn</last></author>
      <pages>46–57</pages>
      <url hash="a8a2858d">W19-8006</url>
      <doi>10.18653/v1/W19-8006</doi>
      <bibkey>borges-volker-etal-2019-hdt</bibkey>
    </paper>
    <paper id="7">
      <title>Nested Coordination in <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Adam</first><last>Przepiórkowski</last></author>
      <author><first>Agnieszka</first><last>Patejuk</last></author>
      <pages>58–69</pages>
      <url hash="e893ab80">W19-8007</url>
      <doi>10.18653/v1/W19-8007</doi>
      <bibkey>przepiorkowski-patejuk-2019-nested</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for <fixed-case>M</fixed-case>byá <fixed-case>G</fixed-case>uaraní</title>
      <author><first>Guillaume</first><last>Thomas</last></author>
      <pages>70–77</pages>
      <url hash="61322ea3">W19-8008</url>
      <doi>10.18653/v1/W19-8008</doi>
      <bibkey>thomas-2019-universal</bibkey>
    </paper>
    <paper id="9">
      <title>Survey of <fixed-case>U</fixed-case>ralic <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies development</title>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <pages>78–86</pages>
      <url hash="eb7b8cdf">W19-8009</url>
      <doi>10.18653/v1/W19-8009</doi>
      <bibkey>partanen-rueter-2019-survey</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>C</fixed-case>onllu<fixed-case>E</fixed-case>ditor: a fully graphical editor for Universal dependencies treebank files</title>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <pages>87–93</pages>
      <url hash="c0e718e0">W19-8010</url>
      <doi>10.18653/v1/W19-8010</doi>
      <bibkey>heinecke-2019-conllueditor</bibkey>
    </paper>
    <paper id="11">
      <title>Towards an adequate account of parataxis in <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Lars</first><last>Ahrenberg</last></author>
      <pages>94–100</pages>
      <url hash="db1939ed">W19-8011</url>
      <doi>10.18653/v1/W19-8011</doi>
      <bibkey>ahrenberg-2019-towards</bibkey>
    </paper>
    <paper id="12">
      <title>Recursive <fixed-case>LSTM</fixed-case> Tree Representation for Arc-Standard Transition-Based Dependency Parsing</title>
      <author><first>Mohab</first><last>Elkaref</last></author>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <pages>101–107</pages>
      <url hash="4e35ef9d">W19-8012</url>
      <doi>10.18653/v1/W19-8012</doi>
      <bibkey>elkaref-bohnet-2019-recursive</bibkey>
    </paper>
    <paper id="13">
      <title>Improving the Annotations in the <fixed-case>T</fixed-case>urkish <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Treebank</title>
      <author><first>Utku</first><last>Türk</last></author>
      <author><first>Furkan</first><last>Atmaca</last></author>
      <author><first>Şaziye</first><last>Betül Özateş</last></author>
      <author><first>Balkız</first><last>Öztürk Başaran</last></author>
      <author><first>Tunga</first><last>Güngör</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <pages>108–115</pages>
      <url hash="2141ec43">W19-8013</url>
      <doi>10.18653/v1/W19-8013</doi>
      <bibkey>turk-etal-2019-improving</bibkey>
    </paper>
    <paper id="14">
      <title>Towards transferring <fixed-case>B</fixed-case>ulgarian Sentences with Elliptical Elements to <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies: issues and strategies</title>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Kiril</first><last>Simov</last></author>
      <pages>116–123</pages>
      <url hash="c2325743">W19-8014</url>
      <doi>10.18653/v1/W19-8014</doi>
      <bibkey>osenova-simov-2019-towards</bibkey>
    </paper>
    <paper id="15">
      <title>Rediscovering Greenberg’s Word Order Universals in <fixed-case>UD</fixed-case></title>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <author><first>Xinying</first><last>Chen</last></author>
      <pages>124–131</pages>
      <url hash="0dcdd2ad">W19-8015</url>
      <doi>10.18653/v1/W19-8015</doi>
      <bibkey>gerdes-etal-2019-rediscovering</bibkey>
    </paper>
    <paper id="16">
      <title>Building minority dependency treebanks, dictionaries and computational grammars at the same time—an experiment in <fixed-case>K</fixed-case>arelian treebanking</title>
      <author><first>Tommi A</first><last>Pirinen</last></author>
      <pages>132–136</pages>
      <url hash="ac8710bb">W19-8016</url>
      <doi>10.18653/v1/W19-8016</doi>
      <bibkey>pirinen-2019-building</bibkey>
    </paper>
  </volume>
  <volume id="81" ingest-date="2019-12-20">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Discourse Structure in Neural NLG</booktitle>
      <url hash="037f6580">W19-81</url>
      <editor><first>Anusha</first><last>Balakrishnan</last></editor>
      <editor><first>Vera</first><last>Demberg</last></editor>
      <editor><first>Chandra</first><last>Khatri</last></editor>
      <editor><first>Abhinav</first><last>Rastogi</last></editor>
      <editor><first>Donia</first><last>Scott</last></editor>
      <editor><first>Marilyn</first><last>Walker</last></editor>
      <editor><first>Michael</first><last>White</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Tokyo, Japan</address>
      <month>November</month>
      <year>2019</year>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="dd0e92f1">W19-8100</url>
      <bibkey>ws-2019-discourse-structure</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Maximizing Stylistic Control and Semantic Accuracy in <fixed-case>NLG</fixed-case>: Personality Variation and Discourse Contrast</title>
      <author><first>Vrindavan</first><last>Harrison</last></author>
      <author><first>Lena</first><last>Reed</last></author>
      <author><first>Shereen</first><last>Oraby</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <pages>1–12</pages>
      <abstract>Neural generation methods for task-oriented dialogue typically generate from a meaning representation that is populated using a database of domain information, such as a table of data describing a restaurant. While earlier work focused solely on the semantic fidelity of outputs, recent work has started to explore methods for controlling the style of the generated text while simultaneously achieving semantic accuracy. Here we experiment with two stylistic benchmark tasks, generating language that exhibits variation in personality, and generating discourse contrast. We report a huge performance improvement in both stylistic control and semantic accuracy over the state of the art on both of these benchmarks. We test several different models and show that putting stylistic conditioning in the decoder and eliminating the semantic re-ranker used in earlier models results in more than 15 points higher BLEU for Personality, with a reduction of semantic error to near zero. We also report an improvement from .75 to .81 in controlling contrast and a reduction in semantic error from 16% to 2%.</abstract>
      <url hash="4ee39df4">W19-8101</url>
      <doi>10.18653/v1/W19-8101</doi>
      <bibkey>harrison-etal-2019-maximizing</bibkey>
    </paper>
    <paper id="2">
      <title>Incorporating Textual Evidence in Visual Storytelling</title>
      <author><first>Tianyi</first><last>Li</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>13–17</pages>
      <abstract>Previous work on visual storytelling mainly focused on exploring image sequence as evidence for storytelling and neglected textual evidence for guiding story generation. Motivated by human storytelling process which recalls stories for familiar images, we exploit textual evidence from similar images to help generate coherent and meaningful stories. To pick the images which may provide textual experience, we propose a two-step ranking method based on image object recognition techniques. To utilize textual information, we design an extended Seq2Seq model with two-channel encoder and attention. Experiments on the VIST dataset show that our method outperforms state-of-the-art baseline models without heavy engineering.</abstract>
      <url hash="2fb426ea">W19-8102</url>
      <doi>10.18653/v1/W19-8102</doi>
      <bibkey>li-li-2019-incorporating</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/vist">VIST</pwcdataset>
    </paper>
    <paper id="3">
      <title>Fine-Grained Control of Sentence Segmentation and Entity Positioning in Neural <fixed-case>NLG</fixed-case></title>
      <author><first>Kritika</first><last>Mehta</last></author>
      <author><first>Raheel</first><last>Qader</last></author>
      <author><first>Cyril</first><last>Labbe</last></author>
      <author><first>François</first><last>Portet</last></author>
      <pages>18–23</pages>
      <abstract>The move from pipeline Natural Language Generation (NLG) approaches to neural end-to-end approaches led to a loss of control in sentence planning operations owing to the conflation of intermediary micro-planning stages into a single model. Such control is highly necessary when the text should be tailored to respect some constraints such as which entity to be mentioned first, the entity position, the complexity of sentences, etc. In this paper, we introduce fine-grained control of sentence planning in neural data-to-text generation models at two levels - realization of input entities in desired sentences and realization of the input entities in the desired position among individual sentences. We show that by augmenting the input with explicit position identifiers, the neural model can achieve a great control over the output structure while keeping the naturalness of the generated text intact. Since sentence level metrics are not entirely suitable to evaluate this task, we used a metric specific to our task that accounts for the model’s ability to achieve control. The results demonstrate that the position identifiers do constraint the neural model to respect the intended output structure which can be useful in a variety of domains that require the generated text to be in a certain structure.</abstract>
      <url hash="41e69108">W19-8103</url>
      <doi>10.18653/v1/W19-8103</doi>
      <bibkey>mehta-etal-2019-fine</bibkey>
    </paper>
    <paper id="4">
      <title>Zero-shot <fixed-case>C</fixed-case>hinese Discourse Dependency Parsing via Cross-lingual Mapping</title>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>24–29</pages>
      <abstract>Due to the absence of labeled data, discourse parsing still remains challenging in some languages. In this paper, we present a simple and efficient method to conduct zero-shot Chinese text-level dependency parsing by leveraging English discourse labeled data and parsing techniques. We first construct the Chinese-English mapping from the level of sentence and elementary discourse unit (EDU), and then exploit the parsing results of the corresponding English translations to obtain the discourse trees for the Chinese text. This method can automatically conduct Chinese discourse parsing, with no need of a large scale of Chinese labeled data.</abstract>
      <url hash="0fb8104f">W19-8104</url>
      <doi>10.18653/v1/W19-8104</doi>
      <bibkey>cheng-li-2019-zero</bibkey>
    </paper>
  </volume>
  <volume id="83" ingest-date="2019-12-20">
    <meta>
      <booktitle>Proceedings of the 1st International Workshop of <fixed-case>AI</fixed-case> Werewolf and Dialog System (<fixed-case>AIW</fixed-case>olf<fixed-case>D</fixed-case>ial2019)</booktitle>
      <url hash="262d21e5">W19-83</url>
      <editor><first>Yoshinobu</first><last>Kano</last></editor>
      <editor><first>Claus</first><last>Aranha</last></editor>
      <editor><first>Michimasa</first><last>Inaba</last></editor>
      <editor><first>Fujio</first><last>Toriumi</last></editor>
      <editor><first>Hirotaka</first><last>Osawa</last></editor>
      <editor><first>Daisuke</first><last>Katagami</last></editor>
      <editor><first>Takashi</first><last>Otsuki</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Tokyo, Japan</address>
      <month>October</month>
      <year>2019</year>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="262d21e5">W19-8300</url>
      <bibkey>ws-2019-international-ai</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Overview of <fixed-case>AIW</fixed-case>olf<fixed-case>D</fixed-case>ial 2019 Shared Task: Contest of Automatic Dialog Agents to Play the Werewolf Game through Conversations</title>
      <author><first>Yoshinobu</first><last>Kano</last></author>
      <author><first>Claus</first><last>Aranha</last></author>
      <author><first>Michimasa</first><last>Inaba</last></author>
      <author><first>Fujio</first><last>Toriumi</last></author>
      <author><first>Hirotaka</first><last>Osawa</last></author>
      <author><first>Daisuke</first><last>Katagami</last></author>
      <author><first>Takashi</first><last>Otsuki</last></author>
      <author><first>Issei</first><last>Tsunoda</last></author>
      <author><first>Shoji</first><last>Nagayama</last></author>
      <author><first>Dolça</first><last>Tellols</last></author>
      <author><first>Yu</first><last>Sugawara</last></author>
      <author><first>Yohei</first><last>Nakata</last></author>
      <pages>1–6</pages>
      <url hash="99cd54fc">W19-8301</url>
      <doi>10.18653/v1/W19-8301</doi>
      <bibkey>kano-etal-2019-overview</bibkey>
    </paper>
    <paper id="2">
      <title>Are Talkative <fixed-case>AI</fixed-case> Agents More Likely to Win the Werewolf Game?</title>
      <author><first>Dolça</first><last>Tellols</last></author>
      <pages>11–14</pages>
      <url hash="8c3f8c8f">W19-8302</url>
      <doi>10.18653/v1/W19-8302</doi>
      <bibkey>tellols-2019-talkative</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>AI</fixed-case> Werewolf Agent with Reasoning Using Role Patterns and Heuristics</title>
      <author><first>Issei</first><last>Tsunoda</last></author>
      <author><first>Yoshinobu</first><last>Kano</last></author>
      <pages>15–19</pages>
      <url hash="d5887d14">W19-8303</url>
      <doi>10.18653/v1/W19-8303</doi>
      <bibkey>tsunoda-kano-2019-ai</bibkey>
    </paper>
    <paper id="4">
      <title>Data Augmentation Based on Distributed Expressions in Text Classification Tasks</title>
      <author><first>Yu</first><last>Sugawara</last></author>
      <pages>7–10</pages>
      <url hash="9e3f08a5">W19-8304</url>
      <doi>10.18653/v1/W19-8304</doi>
      <bibkey>sugawara-2019-data</bibkey>
    </paper>
    <paper id="5">
      <title>Strategies for an Autonomous Agent Playing the “Werewolf game” as a Stealth Werewolf</title>
      <author><first>Shoji</first><last>Nagayama</last></author>
      <author><first>Jotaro</first><last>Abe</last></author>
      <author><first>Kosuke</first><last>Oya</last></author>
      <author><first>Kotaro</first><last>Sakamoto</last></author>
      <author><first>Hideyuki</first><last>Shibuki</last></author>
      <author><first>Tatsunori</first><last>Mori</last></author>
      <author><first>Noriko</first><last>Kando</last></author>
      <pages>20–24</pages>
      <url hash="a8301f3c">W19-8305</url>
      <doi>10.18653/v1/W19-8305</doi>
      <bibkey>nagayama-etal-2019-strategies</bibkey>
    </paper>
  </volume>
  <volume id="84" ingest-date="2019-12-20">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)</booktitle>
      <url hash="523120b0">W19-84</url>
      <editor><first>Jose M.</first><last>Alonso</last></editor>
      <editor><first>Alejandro</first><last>Catala</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <year>2019</year>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="ddaf1192">W19-8400</url>
      <bibkey>ws-2019-interactive</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Explainable Artificial Intelligence and its potential within Industry</title>
      <author><first>Saad</first><last>Mahamood</last></author>
      <pages>1–2</pages>
      <url hash="b60fbc57">W19-8401</url>
      <doi>10.18653/v1/W19-8401</doi>
      <bibkey>mahamood-2019-explainable</bibkey>
    </paper>
    <paper id="2">
      <title>Natural Language Generation Challenges for Explainable <fixed-case>AI</fixed-case></title>
      <author><first>Ehud</first><last>Reiter</last></author>
      <pages>3–7</pages>
      <url hash="307bc7c8">W19-8402</url>
      <doi>10.18653/v1/W19-8402</doi>
      <bibkey>reiter-2019-natural</bibkey>
    </paper>
    <paper id="3">
      <title>A Survey of Explainable <fixed-case>AI</fixed-case> Terminology</title>
      <author><first>Miruna-Adriana</first><last>Clinciu</last></author>
      <author><first>Helen</first><last>Hastie</last></author>
      <pages>8–13</pages>
      <url hash="12323ee7">W19-8403</url>
      <doi>10.18653/v1/W19-8403</doi>
      <bibkey>clinciu-hastie-2019-survey</bibkey>
    </paper>
    <paper id="4">
      <title>Some Insights Towards a Unified Semantic Representation of Explanation for e<fixed-case>X</fixed-case>plainable Artificial Intelligence</title>
      <author><first>Ismaïl</first><last>Baaj</last></author>
      <author><first>Jean-Philippe</first><last>Poli</last></author>
      <author><first>Wassila</first><last>Ouerdane</last></author>
      <pages>14–19</pages>
      <url hash="c8a418c2">W19-8404</url>
      <doi>10.18653/v1/W19-8404</doi>
      <bibkey>baaj-etal-2019-insights</bibkey>
    </paper>
    <paper id="5">
      <title>Paving the way towards counterfactual generation in argumentative conversational agents</title>
      <author><first>Ilia</first><last>Stepin</last></author>
      <author><first>Alejandro</first><last>Catala</last></author>
      <author><first>Martin</first><last>Pereira-Fariña</last></author>
      <author><first>Jose M.</first><last>Alonso</last></author>
      <pages>20–25</pages>
      <url hash="0dd7d3e9">W19-8405</url>
      <doi>10.18653/v1/W19-8405</doi>
      <bibkey>stepin-etal-2019-paving</bibkey>
    </paper>
    <paper id="6">
      <title>Engaging in Dialogue about an Agent’s Norms and Behaviors</title>
      <author><first>Daniel</first><last>Kasenberg</last></author>
      <author><first>Antonio</first><last>Roque</last></author>
      <author><first>Ravenna</first><last>Thielstrom</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <pages>26–28</pages>
      <url hash="1a598e7e">W19-8406</url>
      <doi>10.18653/v1/W19-8406</doi>
      <bibkey>kasenberg-etal-2019-engaging</bibkey>
    </paper>
    <paper id="7">
      <title>An Approach to Summarize Concordancers’ Lists Visually to Support Language Learners in <fixed-case>U</fixed-case>nderstanding<fixed-case>W</fixed-case>ord Usages</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>29–31</pages>
      <url hash="2a90deb4">W19-8407</url>
      <doi>10.18653/v1/W19-8407</doi>
      <bibkey>ehara-2019-approach</bibkey>
    </paper>
  </volume>
  <volume id="85" ingest-date="2019-11-06">
    <meta>
      <booktitle>Proceedings of the Second International Workshop on Resources and Tools for Derivational Morphology</booktitle>
      <url hash="3827a648">W19-85</url>
      <editor><first>Magda</first><last>Ševčíková</last></editor>
      <editor><first>Zdeněk</first><last>Žabokrtský</last></editor>
      <editor><first>Eleonora</first><last>Litta</last></editor>
      <editor><first>Marco</first><last>Passarotti</last></editor>
      <publisher>Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics</publisher>
      <address>Prague, Czechia</address>
      <month>September</month>
      <year>2019</year>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="57ff6674">W19-8500</url>
      <bibkey>ws-2019-international-resources</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Cross-linguistic research into derivational networks</title>
      <author><first>Lívia</first><last>Kőrtvélyessy</last></author>
      <pages>1--4</pages>
      <url hash="d101aafa">W19-8501</url>
      <bibkey>kortvelyessy-2019-cross</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>P</fixed-case>ara<fixed-case>D</fixed-case>is and Démonette: From Theory to Resources for Derivational Paradigms</title>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>5--14</pages>
      <url hash="8aacd0b4">W19-8502</url>
      <bibkey>namer-hathout-2019-paradis</bibkey>
    </paper>
    <paper id="3">
      <title>Semantic descriptions of <fixed-case>F</fixed-case>rench derivational relations in a families-and-paradigms framework</title>
      <author><first>Daniele</first><last>Sanacore</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <pages>15--24</pages>
      <url hash="5114adc1">W19-8503</url>
      <bibkey>sanacore-etal-2019-semantic</bibkey>
    </paper>
    <paper id="4">
      <title>Correlation between the gradability of <fixed-case>L</fixed-case>atin adjectives and the ability to form qualitative abstract nouns</title>
      <author><first>Lucie</first><last>Pultrová</last></author>
      <pages>25--34</pages>
      <url hash="99487789">W19-8504</url>
      <bibkey>pultrova-2019-correlation</bibkey>
    </paper>
    <paper id="5">
      <title>The Treatment of Word Formation in the <fixed-case>L</fixed-case>i<fixed-case>L</fixed-case>a Knowledge Base of Linguistic Resources for <fixed-case>L</fixed-case>atin</title>
      <author><first>Eleonora</first><last>Litta</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Francesco</first><last>Mambrini</last></author>
      <pages>35--43</pages>
      <url hash="3663ca63">W19-8505</url>
      <bibkey>litta-etal-2019-treatment</bibkey>
    </paper>
    <paper id="6">
      <title>Combining Data-Intense and Compute-Intense Methods for Fine-Grained Morphological Analyses</title>
      <author><first>Petra</first><last>Steiner</last></author>
      <pages>45--54</pages>
      <url hash="861f9cd1">W19-8506</url>
      <bibkey>steiner-2019-combining</bibkey>
    </paper>
    <paper id="7">
      <title>The Tagged Corpus (<fixed-case>SYN</fixed-case>2010) as a Help and a Pitfall in the Word-formation Research</title>
      <author><first>Klára</first><last>Osolsobě</last></author>
      <pages>55--59</pages>
      <url hash="337ed96f">W19-8507</url>
      <bibkey>osolsobe-2019-tagged</bibkey>
    </paper>
    <paper id="8">
      <title>Attempting to separate inflection and derivation using vector space representations</title>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <pages>61--70</pages>
      <url hash="8891b6ef">W19-8508</url>
      <bibkey>rosa-zabokrtsky-2019-attempting</bibkey>
    </paper>
    <paper id="9">
      <title>Redesign of the <fixed-case>C</fixed-case>roatian derivational lexicon</title>
      <author><first>Matea</first><last>Filko</last></author>
      <author><first>Krešimir</first><last>Šojat</last></author>
      <author><first>Vanja</first><last>Štefanec</last></author>
      <pages>71--80</pages>
      <url hash="d9d338c1">W19-8509</url>
      <bibkey>filko-etal-2019-redesign</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>D</fixed-case>eri<fixed-case>N</fixed-case>et 2.0: Towards an All-in-One Word-Formation Resource</title>
      <author><first>Jonáš</first><last>Vidra</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Magda</first><last>Ševčíková</last></author>
      <author><first>Lukáš</first><last>Kyjánek</last></author>
      <pages>81--89</pages>
      <url hash="9258f9bd">W19-8510</url>
      <bibkey>vidra-etal-2019-derinet</bibkey>
    </paper>
    <paper id="11">
      <title>Building a Morphological Network for <fixed-case>P</fixed-case>ersian on Top of a Morpheme-Segmented Lexicon</title>
      <author><first>Hamid</first><last>Haghdoost</last></author>
      <author><first>Ebrahim</first><last>Ansari</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Mahshid</first><last>Nikravesh</last></author>
      <pages>91--100</pages>
      <url hash="60eadf6e">W19-8511</url>
      <bibkey>haghdoost-etal-2019-building</bibkey>
    </paper>
    <paper id="12">
      <title>Universal Derivations Kickoff: A Collection of Harmonized Derivational Resources for Eleven Languages</title>
      <author><first>Lukáš</first><last>Kyjánek</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Magda</first><last>Ševčíková</last></author>
      <author><first>Jonáš</first><last>Vidra</last></author>
      <pages>101--110</pages>
      <url hash="1d4ced75">W19-8512</url>
      <bibkey>kyjanek-etal-2019-universal</bibkey>
    </paper>
    <paper id="13">
      <title>A Parametric Approach to Implemented Analyses: Valence-changing Morphology in the <fixed-case>L</fixed-case>in<fixed-case>GO</fixed-case> Grammar Matrix</title>
      <author><first>Christian</first><last>Curtis</last></author>
      <pages>111--120</pages>
      <url hash="b0b1792f">W19-8513</url>
      <bibkey>curtis-2019-parametric</bibkey>
    </paper>
    <paper id="14">
      <title>Grammaticalization in Derivational Morphology: Verification of the Process by Innovative Derivatives</title>
      <author><first>Junya</first><last>Morita</last></author>
      <pages>121--130</pages>
      <url hash="ca98b4e8">W19-8514</url>
      <bibkey>morita-2019-grammaticalization</bibkey>
    </paper>
  </volume>
  <volume id="86" ingest-date="2019-11-30">
    <meta>
      <booktitle>Proceedings of the 12th International Conference on Natural Language Generation</booktitle>
      <editor><first>Kees</first><last>van Deemter</last></editor>
      <editor><first>Chenghua</first><last>Lin</last></editor>
      <editor><first>Hiroya</first><last>Takamura</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Tokyo, Japan</address>
      <month>October–November</month>
      <year>2019</year>
      <venue>inlg</venue>
    </meta>
    <frontmatter>
      <url hash="70d782a4">W19-8600</url>
      <bibkey>ws-2019-international-natural-language</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Talking about what is not there: Generating indefinite referring expressions in <fixed-case>M</fixed-case>inecraft</title>
      <author><first>Arne</first><last>Köhn</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <pages>1–10</pages>
      <abstract>When generating technical instructions, it is often necessary to describe an object that does not exist yet. For example, an NLG system which explains how to build a house needs to generate sentences like “build *a wall of height five to your left*” and “now build *a wall on the other side*.” Generating (indefinite) referring expressions to objects that do not exist yet is fundamentally different from generating the usual definite referring expressions, because the new object must be distinguished from an infinite set of possible alternatives. We formalize this problem and present an algorithm for generating such expressions, in the context of generating building instructions within the Minecraft video game.</abstract>
      <url hash="1c6f8de1">W19-8601</url>
      <doi>10.18653/v1/W19-8601</doi>
      <bibkey>kohn-koller-2019-talking</bibkey>
    </paper>
    <paper id="2">
      <title>Generating Quantified Referring Expressions with Perceptual Cost Pruning</title>
      <author><first>Gordon</first><last>Briggs</last></author>
      <author><first>Hillary</first><last>Harner</last></author>
      <pages>11–18</pages>
      <abstract>We model the production of quantified referring expressions (QREs) that identify collections of visual items. To address this task, we propose a method of perceptual cost pruning, which consists of two steps: (1) determine what subset of quantity information can be perceived given a time limit t, and (2) apply a preference order based REG algorithm (e.g., the Incremental Algorithm) to this reduced set of information. We demonstrate that this method successfully improves the human-likeness of the IA in the QRE generation task and successfully models human-generated language in most cases.</abstract>
      <url hash="b588657d">W19-8602</url>
      <doi>10.18653/v1/W19-8602</doi>
      <bibkey>briggs-harner-2019-generating</bibkey>
    </paper>
    <paper id="3">
      <title>A case study on context-bound referring expression generation</title>
      <author><first>Maurice</first><last>Langner</last></author>
      <pages>19–23</pages>
      <abstract>In recent years, Bayesian models of referring expression generation have gained prominence in order to produce situationally more adequate referring expressions. Basically, these models enable the integration of different parameters into the decision process for using a specific referring expression like the cardinality of the object set, the configuration and complexity of the visual field, and the discriminatory power of available attributes that need to be combined with visual salience and personal preference. This paper describes and discusses the results of an empirical study on the production of referring expressions in visual fields with different object configurations of varying complexity and different contextual premises for using a referring expression. The visual fields are set up using data from the TUNA experiment with plain random or pragmatically enriched configurations which allow for target inference. Different categories of the situational contexts, in which the referring expressions are produced, provide different degrees of cooperativeness, so that generation quality and its relations to contextual user intention can be observed. The results of the study suggest that Bayesian approaches must integrate individual generation preference and the cooperativeness of the situational task in order to model the broad variance between speakers more adequately.</abstract>
      <url hash="e95df3ff">W19-8603</url>
      <doi>10.18653/v1/W19-8603</doi>
      <bibkey>langner-2019-case</bibkey>
    </paper>
    <paper id="4">
      <title>Rethinking Text Attribute Transfer: A Lexical Analysis</title>
      <author><first>Yao</first><last>Fu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Jiaze</first><last>Chen</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>24–33</pages>
      <abstract>Text attribute transfer is modifying certain linguistic attributes (e.g. sentiment, style, author-ship, etc.) of a sentence and transforming them from one type to another. In this paper, we aim to analyze and interpret what is changed during the transfer process. We start from the observation that in many existing models and datasets, certain words within a sentence play important roles in determining the sentence attribute class. These words are referred as the Pivot Words. Based on these pivot words, we propose a lexical analysis framework, the Pivot Analysis, to quantitatively analyze the effects of these words in text attribute classification and transfer. We apply this framework to existing datasets and models and show that: (1) the pivot words are strong features for the classification of sentence attributes; (2) to change the attribute of a sentence, many datasets only requires to change certain pivot words; (3) consequently, many transfer models only perform the lexical-level modification,while leaving higher-level sentence structures unchanged. Our work provides an in-depth understanding of linguistic attribute transfer and further identifies the future requirements and challenges of this task</abstract>
      <url hash="496b647a">W19-8604</url>
      <doi>10.18653/v1/W19-8604</doi>
      <bibkey>fu-etal-2019-rethinking</bibkey>
      <pwccode url="https://github.com/FranxYao/pivot_analysis" additional="false">FranxYao/pivot_analysis</pwccode>
    </paper>
    <paper id="5">
      <title>Choosing between Long and Short Word Forms in <fixed-case>M</fixed-case>andarin</title>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>Kees</first><last>van Deemter</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <author><first>Jingyu</first><last>Fan</last></author>
      <pages>34–39</pages>
      <abstract>Between 80% and 90% of all Chinese words have long and short form such as 老虎/虎 (lao-hu/hu , tiger) (Duanmu:2013). Consequently, the choice between long and short forms is a key problem for lexical choice across NLP and NLG. Following an earlier work on abbreviations in English (Mahowald et al, 2013), we bring a probabilistic perspective to these questions, using both a behavioral and a corpus-based approach. We hypothesized that there is a higher probability of choosing short form in supportive context than in neutral context in Mandarin. Consistent with our prediction, our findings revealed that predictability of contexts makes effect on speakers’ long and short form choice.</abstract>
      <url hash="9c86aeed">W19-8605</url>
      <doi>10.18653/v1/W19-8605</doi>
      <bibkey>li-etal-2019-choosing</bibkey>
    </paper>
    <paper id="6">
      <title>Diamonds in the Rough: Generating Fluent Sentences from Early-Stage Drafts for Academic Writing Assistance</title>
      <author><first>Takumi</first><last>Ito</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last></author>
      <author><first>Hayato</first><last>Kobayashi</last></author>
      <author><first>Ana</first><last>Brassard</last></author>
      <author><first>Masato</first><last>Hagiwara</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>40–53</pages>
      <abstract>The writing process consists of several stages such as drafting, revising, editing, and proofreading. Studies on writing assistance, such as grammatical error correction (GEC), have mainly focused on sentence editing and proofreading, where surface-level issues such as typographical errors, spelling errors, or grammatical errors should be corrected. We broaden this focus to include the earlier revising stage, where sentences require adjustment to the information included or major rewriting and propose Sentence-level Revision (SentRev) as a new writing assistance task. Well-performing systems in this task can help inexperienced authors by producing fluent, complete sentences given their rough, incomplete drafts. We build a new freely available crowdsourced evaluation dataset consisting of incomplete sentences authored by non-native writers paired with their final versions extracted from published academic papers for developing and evaluating SentRev models. We also establish baseline performance on SentRev using our newly built evaluation dataset.</abstract>
      <url hash="8a3d858c">W19-8606</url>
      <doi>10.18653/v1/W19-8606</doi>
      <bibkey>ito-etal-2019-diamonds</bibkey>
      <pwccode url="https://github.com/taku-ito/INLG2019_SentRev" additional="false">taku-ito/INLG2019_SentRev</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="7">
      <title>Computational Argumentation Synthesis as a Language Modeling Task</title>
      <author><first>Roxanne</first><last>El Baff</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>54–64</pages>
      <abstract>Synthesis approaches in computational argumentation so far are restricted to generating claim-like argument units or short summaries of debates. Ultimately, however, we expect computers to generate whole new arguments for a given stance towards some topic, backing up claims following argumentative and rhetorical considerations. In this paper, we approach such an argumentation synthesis as a language modeling task. In our language model, argumentative discourse units are the “words”, and arguments represent the “sentences”. Given a pool of units for any unseen topic-stance pair, the model selects a set of unit types according to a basic rhetorical strategy (logos vs. pathos), arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments.</abstract>
      <url hash="1f24f3d2">W19-8607</url>
      <doi>10.18653/v1/W19-8607</doi>
      <bibkey>el-baff-etal-2019-computational</bibkey>
    </paper>
    <paper id="8">
      <title>Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators</title>
      <author><first>Sanghyun</first><last>Yi</last></author>
      <author><first>Rahul</first><last>Goel</last></author>
      <author><first>Chandra</first><last>Khatri</last></author>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>Tagyoung</first><last>Chung</last></author>
      <author><first>Behnam</first><last>Hedayatnia</last></author>
      <author><first>Anu</first><last>Venkatesh</last></author>
      <author><first>Raefer</first><last>Gabriel</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>65–75</pages>
      <abstract>Encoder-decoder based neural architectures serve as the basis of state-of-the-art approaches in end-to-end open domain dialog systems. Since most of such systems are trained with a maximum likelihood (MLE) objective they suffer from issues such as lack of generalizability and the generic response problem, i.e., a system response that can be an answer to a large number of user utterances, e.g., “Maybe, I don’t know.” Having explicit feedback on the relevance and interestingness of a system response at each turn can be a useful signal for mitigating such issues and improving system quality by selecting responses from different approaches. Towards this goal, we present a system that evaluates chatbot responses at each dialog turn for coherence and engagement. Our system provides explicit turn-level dialog quality feedback, which we show to be highly correlated with human evaluation. To show that incorporating this feedback in the neural response generation models improves dialog quality, we present two different and complementary mechanisms to incorporate explicit feedback into a neural response generation model: reranking and direct modification of the loss function during training. Our studies show that a response generation model that incorporates these combined feedback mechanisms produce more engaging and coherent responses in an open-domain spoken dialog setting, significantly improving the response quality using both automatic and human evaluation.</abstract>
      <url hash="50fc1a18">W19-8608</url>
      <doi>10.18653/v1/W19-8608</doi>
      <bibkey>yi-etal-2019-towards</bibkey>
    </paper>
    <paper id="9">
      <title>Importance of Search and Evaluation Strategies in Neural Dialogue Modeling</title>
      <author><first>Ilia</first><last>Kulikov</last></author>
      <author><first>Alexander</first><last>Miller</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>76–87</pages>
      <abstract>We investigate the impact of search strategies in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a model-based Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics: log-probabilities assigned by the model and utterance diversity. Our experiments reveal that better search algorithms lead to higher rated conversations. However, finding the optimal selection mechanism to choose from a more diverse set of candidates is still an open question.</abstract>
      <url hash="1c5a19e3">W19-8609</url>
      <doi>10.18653/v1/W19-8609</doi>
      <bibkey>kulikov-etal-2019-importance</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
    </paper>
    <paper id="10">
      <title>Towards Best Experiment Design for Evaluating Dialogue System Output</title>
      <author><first>Sashank</first><last>Santhanam</last></author>
      <author><first>Samira</first><last>Shaikh</last></author>
      <pages>88–94</pages>
      <abstract>To overcome the limitations of automated metrics (e.g. BLEU, METEOR) for evaluating dialogue systems, researchers typically use human judgments to provide convergent evidence. While it has been demonstrated that human judgments can suffer from the inconsistency of ratings, extant research has also found that the design of the evaluation task affects the consistency and quality of human judgments. We conduct a between-subjects study to understand the impact of four experiment conditions on human ratings of dialogue system output. In addition to discrete and continuous scale ratings, we also experiment with a novel application of Best-Worst scaling to dialogue evaluation. Through our systematic study with 40 crowdsourced workers in each task, we find that using continuous scales achieves more consistent ratings than Likert scale or ranking-based experiment design. Additionally, we find that factors such as time taken to complete the task and no prior experience of participating in similar studies of rating dialogue system output positively impact consistency and agreement amongst raters.</abstract>
      <url hash="99411864">W19-8610</url>
      <doi>10.18653/v1/W19-8610</doi>
      <bibkey>santhanam-shaikh-2019-towards</bibkey>
      <pwccode url="https://github.com/sashank06/INLG_eval" additional="false">sashank06/INLG_eval</pwccode>
    </paper>
    <paper id="11">
      <title>A Tree-to-Sequence Model for Neural <fixed-case>NLG</fixed-case> in Task-Oriented Dialog</title>
      <author><first>Jinfeng</first><last>Rao</last></author>
      <author><first>Kartikeya</first><last>Upasani</last></author>
      <author><first>Anusha</first><last>Balakrishnan</last></author>
      <author><first>Michael</first><last>White</last></author>
      <author><first>Anuj</first><last>Kumar</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <pages>95–100</pages>
      <abstract>Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Sequence-to-sequence models on flat meaning representations (MR) have been dominant in this task, for example in the E2E NLG Challenge. Previous work has shown that a tree-structured MR can improve the model for better discourse-level structuring and sentence-level planning. In this work, we propose a tree-to-sequence model that uses a tree-LSTM encoder to leverage the tree structures in the input MR, and further enhance the decoding by a structure-enhanced attention mechanism. In addition, we explore combining these enhancements with constrained decoding to improve semantic correctness. Our experiments not only show significant improvements over standard seq2seq baselines, but also is more data-efficient and generalizes better to hard scenarios.</abstract>
      <url hash="1184f35b">W19-8611</url>
      <doi>10.18653/v1/W19-8611</doi>
      <bibkey>rao-etal-2019-tree</bibkey>
    </paper>
    <paper id="12">
      <title>Multiple News Headlines Generation using Page Metadata</title>
      <author><first>Kango</first><last>Iwama</last></author>
      <author><first>Yoshinobu</first><last>Kano</last></author>
      <pages>101–105</pages>
      <abstract>Multiple headlines of a newspaper article have an important role to express the content of the article accurately and concisely. A headline depends on the content and intent of their article. While a single headline expresses the whole corresponding article, each of multiple headlines expresses different information individually. We suggest automatic generation method of such a diverse multiple headlines in a newspaper. Our generation method is based on the Pointer-Generator Network, using page metadata on a newspaper which can change headline generation behavior. This page metadata includes headline location, headline size, article page number, etc. In a previous related work, ensemble of three different generation models was performed to obtain a single headline, where each generation model generates a single headline candidate. In contrast, we use a single model to generate multiple headlines. We conducted automatic evaluations for generated headlines. The results show that our method improved ROUGE-1 score by 4.32 points higher than baseline. These results suggest that our model using page metadata can generate various multiple headlines for an article In better performance.</abstract>
      <url hash="5584a66d">W19-8612</url>
      <doi>10.18653/v1/W19-8612</doi>
      <bibkey>iwama-kano-2019-multiple</bibkey>
    </paper>
    <paper id="13">
      <title>Neural Question Generation using Interrogative Phrases</title>
      <author><first>Yuichi</first><last>Sasazawa</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>106–111</pages>
      <abstract>Question Generation (QG) is the task of generating questions from a given passage. One of the key requirements of QG is to generate a question such that it results in a target answer. Previous works used a target answer to obtain a desired question. However, we also want to specify how to ask questions and improve the quality of generated questions. In this study, we explore the use of interrogative phrases as additional sources to control QG. By providing interrogative phrases, we expect that QG can generate a more reliable sequence of words subsequent to an interrogative phrase. We present a baseline sequence-to-sequence model with the attention, copy, and coverage mechanisms, and show that the simple baseline achieves state-of-the-art performance. The experiments demonstrate that interrogative phrases contribute to improving the performance of QG. In addition, we report the superiority of using interrogative phrases in human evaluation. Finally, we show that a question answering system can provide target answers more correctly when the questions are generated with interrogative phrases.</abstract>
      <url hash="169a0911">W19-8613</url>
      <doi>10.18653/v1/W19-8613</doi>
      <bibkey>sasazawa-etal-2019-neural</bibkey>
    </paper>
    <paper id="14">
      <title>Generating Text from Anonymised Structures</title>
      <author><first>Emilie</first><last>Colin</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>112–117</pages>
      <abstract>Surface realisation (SR) consists in generating a text from a meaning representations (MR). In this paper, we introduce a new parallel dataset of deep meaning representations (MR) and French sentences and we present a novel method for MR-to-text generation which seeks to generalise by abstracting away from lexical content. Most current work on natural language generation focuses on generating text that matches a reference using BLEU as evaluation criteria. In this paper, we additionally consider the model’s ability to reintroduce the function words that are absent from the deep input meaning representations. We show that our approach increases both BLEU score and the scores used to assess function words generation.</abstract>
      <url hash="66d78e0e">W19-8614</url>
      <attachment type="supplementary-attachment" hash="7a0d016d">W19-8614.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8614</doi>
      <bibkey>colin-gardent-2019-generating</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>M</fixed-case>in<fixed-case>W</fixed-case>iki<fixed-case>S</fixed-case>plit: A Sentence Splitting Corpus with Minimal Propositions</title>
      <author><first>Christina</first><last>Niklaus</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <pages>118–123</pages>
      <abstract>We compiled a new sentence splitting corpus that is composed of 203K pairs of aligned complex source and simplified target sentences. Contrary to previously proposed text simplification corpora, which contain only a small number of split examples, we present a dataset where each input sentence is broken down into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions. This corpus is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance.</abstract>
      <url hash="4cec696f">W19-8615</url>
      <doi>10.18653/v1/W19-8615</doi>
      <bibkey>niklaus-etal-2019-minwikisplit</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>QTUNA</fixed-case>: A Corpus for Understanding How Speakers Use Quantification</title>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Kees</first><last>van Deemter</last></author>
      <author><first>Silvia</first><last>Pagliaro</last></author>
      <author><first>Louk</first><last>Smalbil</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>124–129</pages>
      <abstract>A prominent strand of work in formal semantics investigates the ways in which human languages quantify over the elements of a set, as when we say “<i>All <i>A</i> are <i>B</i>
        </i>”, “<i>All except two <i>A</i> are <i>B</i>
        </i>”, “<i>Only a few of the <i>A</i> are <i>B</i>
        </i>” and so on. Our aim is to build Natural Language Generation algorithms that mimic humans’ use of quantified expressions. To inform these algorithms, we conducted on a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions. We discuss how these experiments were conducted and what corpora they gave rise to. We conduct an informal analysis of the corpora, and offer an initial assessment of the challenges that these corpora pose for Natural Language Generation. The dataset is available at: <url>https://github.com/a-quei/qtuna</url>.</abstract>
      <url hash="4782e67a">W19-8616</url>
      <attachment type="supplementary-attachment" hash="7f9670cd">W19-8616.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8616</doi>
      <bibkey>chen-etal-2019-qtuna</bibkey>
      <pwccode url="https://github.com/a-quei/qtuna" additional="false">a-quei/qtuna</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qtuna">QTuna</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>KPT</fixed-case>imes: A Large-Scale Dataset for Keyphrase Generation on News Documents</title>
      <author><first>Ygor</first><last>Gallina</last></author>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Beatrice</first><last>Daille</last></author>
      <pages>130–135</pages>
      <abstract>Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:// github.com/ygorg/KPTimes.</abstract>
      <url hash="dd8565e2">W19-8617</url>
      <doi>10.18653/v1/W19-8617</doi>
      <bibkey>gallina-etal-2019-kptimes</bibkey>
      <pwccode url="https://github.com/ygorg/KPTimes" additional="false">ygorg/KPTimes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kptimes">KPTimes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="18">
      <title>Sketch Me if You Can: Towards Generating Detailed Descriptions of Object Shape by Grounding in Images and Drawings</title>
      <author><first>Ting</first><last>Han</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>136–140</pages>
      <abstract>A lot of recent work in Language &amp; Vision has looked at generating descriptions or referring expressions for objects in scenes of real-world images, though focusing mostly on relatively simple language like object names, color and location attributes (e.g., brown chair on the left). This paper presents work on Draw-and-Tell, a dataset of detailed descriptions for common objects in images where annotators have produced fine-grained attribute-centric expressions distinguishing a target object from a range of similar objects. Additionally, the dataset comes with hand-drawn sketches for each object. As Draw-and-Tell is medium-sized and contains a rich vocabulary, it constitutes an interesting challenge for CNN-LSTM architectures used in state-of-the-art image captioning models. We explore whether the additional modality given through sketches can help such a model to learn to accurately ground detailed language referring expressions to object shapes. Our results are encouraging.</abstract>
      <url hash="0764949b">W19-8618</url>
      <doi>10.18653/v1/W19-8618</doi>
      <bibkey>han-zarriess-2019-sketch</bibkey>
    </paper>
    <paper id="19">
      <title>An Encoder with non-Sequential Dependency for Neural Data-to-Text Generation</title>
      <author><first>Feng</first><last>Nie</last></author>
      <author><first>Jinpeng</first><last>Wang</last></author>
      <author><first>Rong</first><last>Pan</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>141–146</pages>
      <abstract>Data-to-text generation aims to generate descriptions given a structured input data (i.e., a table with multiple records). Existing neural methods for encoding input data can be divided into two categories: a) pooling based encoders which ignore dependencies between input records or b) recurrent encoders which model only sequential dependencies between input records. In our investigation, although the recurrent encoder generally outperforms the pooling based encoder by learning the sequential dependencies, it is sensitive to the order of the input records (i.e., performance decreases when injecting the random shuffling noise over input data). To overcome this problem, we propose to adopt the self-attention mechanism to learn dependencies between arbitrary input records. Experimental results show the proposed method achieves comparable results and remains stable under random shuffling over input data.</abstract>
      <url hash="84813a34">W19-8619</url>
      <doi>10.18653/v1/W19-8619</doi>
      <bibkey>nie-etal-2019-encoder</bibkey>
    </paper>
    <paper id="20">
      <title>On Leveraging the Visual Modality for Neural Machine Translation</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Sang Keun</first><last>Choe</last></author>
      <author><first>Quanyang</first><last>Lu</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <pages>147–151</pages>
      <abstract>Leveraging the visual modality effectively for Neural Machine Translation (NMT) remains an open problem in computational linguistics. Recently, Caglayan et al. posit that the observed gains are limited mainly due to the very simple, short, repetitive sentences of the Multi30k dataset (the only multimodal MT dataset available at the time), which renders the source text sufficient for context. In this work, we further investigate this hypothesis on a new large scale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times longer mean sentence length than Multi30k and no repetition. We propose and evaluate three novel fusion techniques, each of which is designed to ensure the utilization of visual context at different stages of the Sequence-to-Sequence transduction pipeline, even under full linguistic context. However, we still obtain only marginal gains under full linguistic context and posit that visual embeddings extracted from deep vision models (ResNet for Multi30k, ResNext for How2) do not lend themselves to increasing the discriminativeness between the vocabulary elements at token level prediction in NMT. We demonstrate this qualitatively by analyzing attention distribution and quantitatively through Principal Component Analysis, arriving at the conclusion that it is the quality of the visual embeddings rather than the length of sentences, which need to be improved in existing MMT datasets.</abstract>
      <url hash="c86d0c17">W19-8620</url>
      <doi>10.18653/v1/W19-8620</doi>
      <bibkey>raunak-etal-2019-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="21">
      <title>Tell Me More: A Dataset of Visual Scene Description Sequences</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>152–157</pages>
      <abstract>We present a dataset consisting of what we call image description sequences, which are multi-sentence descriptions of the contents of an image. These descriptions were collected in a pseudo-interactive setting, where the describer was told to describe the given image to a listener who needs to identify the image within a set of images, and who successively asks for more information. As we show, this setup produced nicely structured data that, we think, will be useful for learning models capable of planning and realising such description discourses.</abstract>
      <url hash="17699348">W19-8621</url>
      <doi>10.18653/v1/W19-8621</doi>
      <bibkey>ilinykh-etal-2019-tell</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/image-description-sequences">Image Description Sequences</pwcdataset>
    </paper>
    <paper id="22">
      <title>A Closer Look at Recent Results of Verb Selection for Data-to-Text <fixed-case>NLG</fixed-case></title>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Jin-Ge</first><last>Yao</last></author>
      <pages>158–163</pages>
      <abstract>Automatic natural language generation systems need to use the contextually-appropriate verbs when describing different kinds of facts or events, which has triggered research interest on verb selection for data-to-text generation. In this paper, we discuss a few limitations of the current task settings and the evaluation metrics. We also provide two simple, efficient, interpretable baseline approaches for statistical selection of trend verbs, which give a strong performance on both previously used evaluation metrics and our new evaluation.</abstract>
      <url hash="9a8a92bc">W19-8622</url>
      <attachment type="supplementary-attachment" hash="aa37004d">W19-8622.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8622</doi>
      <bibkey>chen-yao-2019-closer</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>V</fixed-case>i<fixed-case>GGO</fixed-case>: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation</title>
      <author><first>Juraj</first><last>Juraska</last></author>
      <author><first>Kevin</first><last>Bowden</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <pages>164–172</pages>
      <abstract>The uptake of deep learning in natural language generation (NLG) led to the release of both small and relatively large parallel corpora for training neural models. The existing data-to-text datasets are, however, aimed at task-oriented dialogue systems, and often thus limited in diversity and versatility. They are typically crowdsourced, with much of the noise left in them. Moreover, current neural NLG models do not take full advantage of large training data, and due to their strong generalizing properties produce sentences that look template-like regardless. We therefore present a new corpus of 7K samples, which (1) is clean despite being crowdsourced, (2) has utterances of 9 generalizable and conversational dialogue act types, making it more suitable for open-domain dialogue systems, and (3) explores the domain of video games, which is new to dialogue systems despite having excellent potential for supporting rich conversations.</abstract>
      <url hash="1be0b7bc">W19-8623</url>
      <doi>10.18653/v1/W19-8623</doi>
      <bibkey>juraska-etal-2019-viggo</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/viggo">ViGGO</pwcdataset>
    </paper>
    <paper id="24">
      <title><fixed-case>BERT</fixed-case> for Question Generation</title>
      <author><first>Ying-Hong</first><last>Chan</last></author>
      <author><first>Yao-Chung</first><last>Fan</last></author>
      <pages>173–177</pages>
      <abstract>In this study, we investigate the employment of the pre-trained BERT language model to tackle question generation tasks. We introduce two neural architectures built on top of BERT for question generation tasks. The first one is a straightforward BERT employment, which reveals the defects of directly using BERT for text generation. And, the second one remedies the first one by restructuring the BERT employment into a sequential manner for taking information from previous decoded results. Our models are trained and evaluated on the question-answering dataset SQuAD. Experiment results show that our best model yields state-of-the-art performance which advances the BLEU4 score of existing best models from 16.85 to 18.91.</abstract>
      <url hash="7f01394b">W19-8624</url>
      <doi>10.18653/v1/W19-8624</doi>
      <bibkey>chan-fan-2019-bert</bibkey>
    </paper>
    <paper id="25">
      <title>Visually grounded generation of entailments from premises</title>
      <author><first>Somayeh</first><last>Jafaritazehjani</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author><first>Marc</first><last>Tanti</last></author>
      <pages>178–188</pages>
      <abstract>Natural Language Inference (NLI) is the task of determining the semantic relationship between a premise and a hypothesis. In this paper, we focus on the generation of hypotheses from premises in a multimodal setting, to generate a sentence (hypothesis) given an image and/or its description (premise) as the input. The main goals of this paper are (a) to investigate whether it is reasonable to frame NLI as a generation task; and (b) to consider the degree to which grounding textual premises in visual information is beneficial to generation. We compare different neural architectures, showing through automatic and human evaluation that entailments can indeed be generated successfully. We also show that multimodal models outperform unimodal models in this task, albeit marginally</abstract>
      <url hash="ad3eee4d">W19-8625</url>
      <doi>10.18653/v1/W19-8625</doi>
      <bibkey>jafaritazehjani-etal-2019-visually</bibkey>
    </paper>
    <paper id="26">
      <title>Detecting Machine-Translated Text using Back Translation</title>
      <author><first>Hoang-Quoc</first><last>Nguyen-Son</last></author>
      <author><first>Thao</first><last>Tran Phuong</last></author>
      <author><first>Seira</first><last>Hidano</last></author>
      <author><first>Shinsaku</first><last>Kiyomoto</last></author>
      <pages>189–197</pages>
      <abstract>Machine-translated text plays a crucial role in the communication of people using different languages. However, adversaries can use such text for malicious purposes such as plagiarism and fake review. The existing methods detected a machine-translated text only using the text’s intrinsic content, but they are unsuitable for classifying the machine-translated and human-written texts with the same meanings. We have proposed a method to extract features used to distinguish machine/human text based on the similarity between the intrinsic text and its back-translation. The evaluation of detecting translated sentences with French shows that our method achieves 75.0% of both accuracy and F-score. It outperforms the existing methods whose the best accuracy is 62.8% and the F-score is 62.7%. The proposed method even detects more efficiently the back-translated text with 83.4% of accuracy, which is higher than 66.7% of the best previous accuracy. We also achieve similar results not only with F-score but also with similar experiments related to Japanese. Moreover, we prove that our detector can recognize both machine-translated and machine-back-translated texts without the language information which is used to generate these machine texts. It demonstrates the persistence of our method in various applications in both low- and rich-resource languages.</abstract>
      <url hash="3be328b8">W19-8626</url>
      <doi>10.18653/v1/W19-8626</doi>
      <bibkey>nguyen-son-etal-2019-detecting</bibkey>
    </paper>
    <paper id="27">
      <title>Neural Conversation Model Controllable by Given Dialogue Act Based on Adversarial Learning and Label-aware Objective</title>
      <author><first>Seiya</first><last>Kawano</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>198–207</pages>
      <abstract>Building a controllable neural conversation model (NCM) is an important task. In this paper, we focus on controlling the responses of NCMs by using dialogue act labels of responses as conditions. We introduce an adversarial learning framework for the task of generating conditional responses with a new objective to a discriminator, which explicitly distinguishes sentences by using labels. This change strongly encourages the generation of label-conditioned sentences. We compared the proposed method with some existing methods for generating conditional responses. The experimental results show that our proposed method has higher controllability for dialogue acts even though it has higher or comparable naturalness to existing methods.</abstract>
      <url hash="0d07517f">W19-8627</url>
      <doi>10.18653/v1/W19-8627</doi>
      <bibkey>kawano-etal-2019-neural</bibkey>
    </paper>
    <paper id="28">
      <title>Low Level Linguistic Controls for Style Transfer and Content Preservation</title>
      <author><first>Katy</first><last>Gero</last></author>
      <author><first>Chris</first><last>Kedzie</last></author>
      <author><first>Jonathan</first><last>Reeve</last></author>
      <author><first>Lydia</first><last>Chilton</last></author>
      <pages>208–218</pages>
      <abstract>Despite the success of style transfer in image processing, it has seen limited progress in natural language generation. Part of the problem is that content is not as easily decoupled from style in the text domain. Curiously, in the field of stylometry, content does not figure prominently in practical methods of discriminating stylistic elements, such as authorship and genre. Rather, syntax and function words are the most salient features. Drawing on this work, we model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions. We train a neural encoder-decoder model to reconstruct reference sentences given only content words and the setting of the controls. We perform style transfer by keeping the content words fixed while adjusting the controls to be indicative of another style. In experiments, we show that the model reliably responds to the linguistic controls and perform both automatic and manual evaluations on style transfer. We find we can fool a style classifier 84% of the time, and that our model produces highly diverse and stylistically distinctive outputs. This work introduces a formal, extendable model of style that can add control to any neural text generation system.</abstract>
      <url hash="05761dae">W19-8628</url>
      <doi>10.18653/v1/W19-8628</doi>
      <bibkey>gero-etal-2019-low</bibkey>
    </paper>
    <paper id="29">
      <title>Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>219–228</pages>
      <abstract>Neural Machine Translation (NMT) models tend to achieve the best performances when larger sets of parallel sentences are provided for training. For this reason, augmenting the training set with artificially-generated sentence pair can boost the performance. Nonetheless, the performance can also be improved with a small number of sentences if they are in the same domain as the test set. Accordingly, we want to explore the use of artificially-generated sentence along with data-selection algorithms to improve NMT models trained solely with authentic data. In this work, we show how artificially-generated sentences can be more beneficial than authentic pairs and what are their advantages when used in combination with data-selection algorithms.</abstract>
      <url hash="44fb4d31">W19-8629</url>
      <doi>10.18653/v1/W19-8629</doi>
      <bibkey>poncelas-way-2019-selecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2015">WMT 2015</pwcdataset>
    </paper>
    <paper id="30">
      <title>Efficiency Metrics for Data-Driven Models: A Text Summarization Case Study</title>
      <author><first>Erion</first><last>Çano</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>229–239</pages>
      <abstract>Using data-driven models for solving text summarization or similar tasks has become very common in the last years. Yet most of the studies report basic accuracy scores only, and nothing is known about the ability of the proposed models to improve when trained on more data. In this paper, we define and propose three data efficiency metrics: data score efficiency, data time deficiency and overall data efficiency. We also propose a simple scheme that uses those metrics and apply it for a more comprehensive evaluation of popular methods on text summarization and title generation tasks. For the latter task, we process and release a huge collection of 35 million abstract-title pairs from scientific articles. Our results reveal that among the tested models, the Transformer is the most efficient on both tasks.</abstract>
      <url hash="73d1b8d4">W19-8630</url>
      <doi>10.18653/v1/W19-8630</doi>
      <bibkey>cano-bojar-2019-efficiency</bibkey>
    </paper>
    <paper id="31">
      <title>An <fixed-case>NLG</fixed-case> System for Constituent Correspondence: Personality, Affect, and Alignment</title>
      <author><first>William</first><last>Kolkey</last></author>
      <author><first>Jian</first><last>Dong</last></author>
      <author><first>Greg</first><last>Bybee</last></author>
      <pages>240–243</pages>
      <abstract>Roughly 30% of congressional staffers in the United States report spending a “great deal” of time writing responses to constituent letters. Letters often solicit an update on the status of legislation and a description of a congressman’s vote record or vote intention — structurable data that can be leveraged by a natural language generation (NLG) system to create a coherent letter response. This paper describes how PoliScribe, a pipeline-architectured NLG platform, constructs personalized responses to constituents inquiring about legislation. Emphasis will be placed on adapting NLG methodologies to the political domain, which entails special attention to affect, discursive variety, and rhetorical strategies that align a speaker with their interlocutor, even in cases of policy disagreement.</abstract>
      <url hash="d55cebf0">W19-8631</url>
      <doi>10.18653/v1/W19-8631</doi>
      <bibkey>kolkey-etal-2019-nlg</bibkey>
    </paper>
    <paper id="32">
      <title>Margin Call: an Accessible Web-based Text Viewer with Generated Paragraph Summaries in the Margin</title>
      <author><first>Naba</first><last>Rizvi</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Lidan</first><last>Wang</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <pages>244–246</pages>
      <abstract>We present Margin Call, a web-based text viewer that automatically generates short summaries for each paragraph of the text and displays the summaries in the margin of the text next to the corresponding paragraph. On the back-end, the summarizer first identifies the most important sentence for each paragraph in the text file uploaded by the user. The selected sentence is then automatically compressed to produce the short summary. The resulting summary is a few words long. The displayed summaries can help the user understand and retrieve information faster from the text, while increasing the retention of information.</abstract>
      <url hash="71554cce">W19-8632</url>
      <doi>10.18653/v1/W19-8632</doi>
      <bibkey>rizvi-etal-2019-margin</bibkey>
    </paper>
    <paper id="33">
      <title>Privacy-Aware Text Rewriting</title>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Chenchen</first><last>Xu</last></author>
      <author><first>Ran</first><last>Cui</last></author>
      <pages>247–257</pages>
      <abstract>Biased decisions made by automatic systems have led to growing concerns in research communities. Recent work from the NLP community focuses on building systems that make fair decisions based on text. Instead of relying on unknown decision systems or human decision-makers, we argue that a better way to protect data providers is to remove the trails of sensitive information before publishing the data. In light of this, we propose a new privacy-aware text rewriting task and explore two privacy-aware back-translation methods for the task, based on adversarial training and approximate fairness risk. Our extensive experiments on three real-world datasets with varying demographical attributes show that our methods are effective in obfuscating sensitive attributes. We have also observed that the fairness risk method retains better semantics and fluency, while the adversarial training method tends to leak less sensitive information.</abstract>
      <url hash="12bfa3ea">W19-8633</url>
      <attachment type="supplementary-attachment" hash="2cf3dd8b">W19-8633.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8633</doi>
      <bibkey>xu-etal-2019-privacy</bibkey>
    </paper>
    <paper id="34">
      <title>Personalized Substitution Ranking for Lexical Simplification</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Chak Yan</first><last>Yeung</last></author>
      <pages>258–267</pages>
      <abstract>A lexical simplification (LS) system substitutes difficult words in a text with simpler ones to make it easier for the user to understand. In the typical LS pipeline, the Substitution Ranking step determines the best substitution out of a set of candidates. Most current systems do not consider the user’s vocabulary proficiency, and always aim for the simplest candidate. This approach may overlook less-simple candidates that the user can understand, and that are semantically closer to the original word. We propose a personalized approach for Substitution Ranking to identify the candidate that is the closest synonym and is non-complex for the user. In experiments on learners of English at different proficiency levels, we show that this approach enhances the semantic faithfulness of the output, at the cost of a relatively small increase in the number of complex words.</abstract>
      <url hash="6376adf8">W19-8634</url>
      <doi>10.18653/v1/W19-8634</doi>
      <bibkey>lee-yeung-2019-personalized</bibkey>
    </paper>
    <paper id="35">
      <title>Revisiting the Binary Linearization Technique for Surface Realization</title>
      <author><first>Yevgeniy</first><last>Puzikov</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>268–278</pages>
      <abstract>End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying decision-making process, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the system. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the system which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others.</abstract>
      <url hash="f9292ada">W19-8635</url>
      <doi>10.18653/v1/W19-8635</doi>
      <bibkey>puzikov-etal-2019-revisiting</bibkey>
    </paper>
    <paper id="36">
      <title>Head-First Linearization with Tree-Structured Representation</title>
      <author><first>Xiang</first><last>Yu</last></author>
      <author><first>Agnieszka</first><last>Falenska</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Jonas</first><last>Kuhn</last></author>
      <pages>279–289</pages>
      <abstract>We present a dependency tree linearization model with two novel components: (1) a tree-structured encoder based on bidirectional Tree-LSTM that propagates information first bottom-up then top-down, which allows each token to access information from the entire tree; and (2) a linguistically motivated head-first decoder that emphasizes the central role of the head and linearizes the subtree by incrementally attaching the dependents on both sides of the head. With the new encoder and decoder, we reach state-of-the-art performance on the Surface Realization Shared Task 2018 dataset, outperforming not only the shared tasks participants, but also previous state-of-the-art systems (Bohnet et al., 2011; Puduppully et al., 2016). Furthermore, we analyze the power of the tree-structured encoder with a probing task and show that it is able to recognize the topological relation between any pair of tokens in a tree.</abstract>
      <url hash="9208016c">W19-8636</url>
      <doi>10.18653/v1/W19-8636</doi>
      <bibkey>yu-etal-2019-head</bibkey>
    </paper>
    <paper id="37">
      <title>Let’s <fixed-case>FACE</fixed-case> it. <fixed-case>F</fixed-case>innish Poetry Generation with Aesthetics and Framing</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <pages>290–300</pages>
      <abstract>We present a creative poem generator for the morphologically rich Finnish language. Our method falls into the master-apprentice paradigm, where a computationally creative genetic algorithm teaches a BRNN model to generate poetry. We model several parts of poetic aesthetics in the fitness function of the genetic algorithm, such as sonic features, semantic coherence, imagery and metaphor. Furthermore, we justify the creativity of our method based on the FACE theory on computational creativity and take additional care in evaluating our system by automatic metrics for concepts together with human evaluation for aesthetics, framing and expressions.</abstract>
      <url hash="d0cc7455">W19-8637</url>
      <doi>10.18653/v1/W19-8637</doi>
      <bibkey>hamalainen-alnajjar-2019-lets</bibkey>
      <pwccode url="https://github.com/mikahama/finmeter" additional="false">mikahama/finmeter</pwccode>
    </paper>
    <paper id="38">
      <title>Generation of Hip-Hop Lyrics with Hierarchical Modeling and Conditional Templates</title>
      <author><first>Enrique</first><last>Manjavacas</last></author>
      <author><first>Mike</first><last>Kestemont</last></author>
      <author><first>Folgert</first><last>Karsdorp</last></author>
      <pages>301–310</pages>
      <abstract>This paper addresses Hip-Hop lyric generation with conditional Neural Language Models. We develop a simple yet effective mechanism to extract and apply conditional templates from text snippets, and show—on the basis of a large-scale crowd-sourced manual evaluation—that these templates significantly improve the quality and realism of the generated snippets. Importantly, the proposed approach enables end-to-end training, targeting formal properties of text such as rhythm and rhyme, which are central characteristics of rap texts. Additionally, we explore how generating text at different scales (e.g. character-level or word-level) affects the quality of the output. We find that a hybrid form—a hierarchical model that aims to integrate Language Modeling at both word and character-level scales—yields significant improvements in text quality, yet surprisingly, cannot exploit conditional templates to their fullest extent. Our findings highlight that text generation models based on Recurrent Neural Networks (RNN) are sensitive to the modeling scale and call for further research on the observed differences in effectiveness of the conditioning mechanism at different scales.</abstract>
      <url hash="b592c8be">W19-8638</url>
      <doi>10.18653/v1/W19-8638</doi>
      <bibkey>manjavacas-etal-2019-generation</bibkey>
    </paper>
    <paper id="39">
      <title>Revisiting Challenges in Data-to-Text Generation with Fact Grounding</title>
      <author><first>Hongmin</first><last>Wang</last></author>
      <pages>311–322</pages>
      <abstract>Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.</abstract>
      <url hash="cae33f68">W19-8639</url>
      <doi>10.18653/v1/W19-8639</doi>
      <bibkey>wang-2019-revisiting</bibkey>
      <pwccode url="https://github.com/wanghm92/rw_fg" additional="false">wanghm92/rw_fg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
    </paper>
    <paper id="40">
      <title>Controlling Contents in Data-to-Document Generation with Human-Designed Topic Labels</title>
      <author><first>Kasumi</first><last>Aoki</last></author>
      <author><first>Akira</first><last>Miyazawa</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Tatsuya</first><last>Aoki</last></author>
      <author><first>Hiroshi</first><last>Noji</last></author>
      <author><first>Keiichi</first><last>Goshima</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <pages>323–332</pages>
      <abstract>We propose a data-to-document generator that can easily control the contents of output texts based on a neural language model. Conventional data-to-text model is useful when a reader seeks a global summary of data because it has only to describe an important part that has been extracted beforehand. However, because depending on users, it differs what they are interested in, so it is necessary to develop a method to generate various summaries according to users’ interests. We develop a model to generate various summaries and to control their contents by providing the explicit targets for a reference to the model as controllable factors. In the experiments, we used five-minute or one-hour charts of 9 indicators (e.g., Nikkei225), as time-series data, and daily summaries of Nikkei Quick News as textual data. We conducted comparative experiments using two pieces of information: human-designed topic labels indicating the contents of a sentence and automatically extracted keywords as the referential information for generation.</abstract>
      <url hash="5303c2e5">W19-8640</url>
      <doi>10.18653/v1/W19-8640</doi>
      <bibkey>aoki-etal-2019-controlling</bibkey>
    </paper>
    <paper id="41">
      <title>A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation</title>
      <author><first>Yuta</first><last>Hitomi</last></author>
      <author><first>Yuya</first><last>Taguchi</last></author>
      <author><first>Hideaki</first><last>Tamori</last></author>
      <author><first>Ko</first><last>Kikuta</last></author>
      <author><first>Jiro</first><last>Nishitoba</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>333–343</pages>
      <abstract>Browsing news articles on multiple devices is now possible. The lengths of news article headlines have precise upper bounds, dictated by the size of the display of the relevant device or interface. Therefore, controlling the length of headlines is essential when applying the task of headline generation to news production. However, because there is no corpus of headlines of multiple lengths for a given article, previous research on controlling output length in headline generation has not discussed whether the system outputs could be adequately evaluated without multiple references of different lengths. In this paper, we introduce two corpora, which are Japanese News Corpus (JNC) and JApanese MUlti-Length Headline Corpus (JAMUL), to confirm the validity of previous evaluation settings. The JNC provides common supervision data for headline generation. The JAMUL is a large-scale evaluation dataset for headlines of three different lengths composed by professional editors. We report new findings on these corpora; for example, although the longest length reference summary can appropriately evaluate the existing methods controlling output length, this evaluation setting has several problems.</abstract>
      <url hash="b8c8f388">W19-8641</url>
      <doi>10.18653/v1/W19-8641</doi>
      <bibkey>hitomi-etal-2019-large</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/jamul">JAMUL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jnc">JNC</pwcdataset>
    </paper>
    <paper id="42">
      <title>Agreement is overrated: A plea for correlation to assess human evaluation reliability</title>
      <author><first>Jacopo</first><last>Amidei</last></author>
      <author><first>Paul</first><last>Piwek</last></author>
      <author><first>Alistair</first><last>Willis</last></author>
      <pages>344–354</pages>
      <abstract>Inter-Annotator Agreement (IAA) is used as a means of assessing the quality of NLG evaluation data, in particular, its reliability. According to existing scales of IAA interpretation – see, for example, Lommel et al. (2014), Liu et al. (2016), Sedoc et al. (2018) and Amidei et al. (2018a) – most data collected for NLG evaluation fail the reliability test. We confirmed this trend by analysing papers published over the last 10 years in NLG-specific conferences (in total 135 papers that included some sort of human evaluation study). Following Sampson and Babarczy (2008), Lommel et al. (2014), Joshi et al. (2016) and Amidei et al. (2018b), such phenomena can be explained in terms of irreducible human language variability. Using three case studies, we show the limits of considering IAA as the only criterion for checking evaluation reliability. Given human language variability, we propose that for human evaluation of NLG, correlation coefficients and agreement coefficients should be used together to obtain a better assessment of the evaluation data reliability. This is illustrated using the three case studies.</abstract>
      <url hash="cba32c51">W19-8642</url>
      <doi>10.18653/v1/W19-8642</doi>
      <bibkey>amidei-etal-2019-agreement</bibkey>
    </paper>
    <paper id="43">
      <title>Best practices for the human evaluation of automatically generated text</title>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Sander</first><last>Wubben</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>355–368</pages>
      <abstract>Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated. While there is some agreement regarding automatic metrics, there is a high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how human evaluation is currently conducted, and presents a set of best practices, grounded in the literature. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.</abstract>
      <url hash="8a96b50d">W19-8643</url>
      <attachment type="supplementary-attachment" hash="ba358947">W19-8643.Supplementary_Attachment.xlsx</attachment>
      <doi>10.18653/v1/W19-8643</doi>
      <bibkey>van-der-lee-etal-2019-best</bibkey>
    </paper>
    <paper id="44">
      <title>Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)</title>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Karin</first><last>Sevegnani</last></author>
      <author><first>Ioannis</first><last>Konstas</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>369–376</pages>
      <abstract>We present a recurrent neural network based system for automatic quality estimation of natural language generation (NLG) outputs, which jointly learns to assign numerical ratings to individual outputs and to provide pairwise rankings of two different outputs. The latter is trained using pairwise hinge loss over scores from two copies of the rating network. We use learning to rank and synthetic data to improve the quality of ratings assigned by our system: We synthesise training pairs of distorted system outputs and train the system to rank the less distorted one higher. This leads to a 12% increase in correlation with human ratings over the previous benchmark. We also establish the state of the art on the dataset of relative rankings from the E2E NLG Challenge (Dusek et al., 2019), where synthetic data lead to a 4% accuracy increase over the base model.</abstract>
      <url hash="6cff5405">W19-8644</url>
      <attachment type="supplementary-attachment" hash="cbddf56a">W19-8644.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8644</doi>
      <bibkey>dusek-etal-2019-automatic</bibkey>
      <pwccode url="https://github.com/tuetschek/ratpred" additional="false">tuetschek/ratpred</pwccode>
    </paper>
    <paper id="45">
      <title>Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation</title>
      <author><first>Amit</first><last>Moryossef</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>377–382</pages>
      <abstract>We follow the step-by-step approach to neural data-to-text generation proposed by Moryossef et al (2019), in which the generation process is divided into a text planning stage followed by a plan realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner; (2) we incorporate typing hints that improve the model’s ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate.</abstract>
      <url hash="8dcf5834">W19-8645</url>
      <attachment type="supplementary-attachment" hash="a7bc5e8c">W19-8645.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8645</doi>
      <bibkey>moryossef-etal-2019-improving</bibkey>
    </paper>
    <paper id="46">
      <title>Toward a Better Story End: Collecting Human Evaluation with Reasons</title>
      <author><first>Yusuke</first><last>Mori</last></author>
      <author><first>Hiroaki</first><last>Yamane</last></author>
      <author><first>Yusuke</first><last>Mukuta</last></author>
      <author><first>Tatsuya</first><last>Harada</last></author>
      <pages>383–390</pages>
      <abstract>Creativity is an essential element of human nature used for many activities, such as telling a story. Based on human creativity, researchers have attempted to teach a computer to generate stories automatically or support this creative process. In this study, we undertake the task of story ending generation. This is a relatively new task, in which the last sentence of a given incomplete story is automatically generated. This is challenging because, in order to predict an appropriate ending, the generation method should comprehend the context of events. Despite the importance of this task, no clear evaluation metric has been established thus far; hence, it has remained an open problem. Therefore, we study the various elements involved in evaluating an automatic method for generating story endings. First, we introduce a baseline hierarchical sequence-to-sequence method for story ending generation. Then, we conduct a pairwise comparison against human-written endings, in which annotators choose the preferable ending. In addition to a quantitative evaluation, we conduct a qualitative evaluation by asking annotators to specify the reason for their choice. From the collected reasons, we discuss what elements the evaluation should focus on, to thereby propose effective metrics for the task.</abstract>
      <url hash="085e7935">W19-8646</url>
      <attachment type="supplementary-attachment" hash="576ea1be">W19-8646.Supplementary_Attachment.zip</attachment>
      <doi>10.18653/v1/W19-8646</doi>
      <revision id="1" href="W19-8646v1" hash="5fa8db2d"/>
      <revision id="2" href="W19-8646v2" hash="085e7935" date="2020-07-07">Corrected values in Table 2 and its discussion.</revision>
      <bibkey>mori-etal-2019-toward</bibkey>
    </paper>
    <paper id="47">
      <title>Hotel Scribe: Generating High Variation Hotel Descriptions</title>
      <author><first>Saad</first><last>Mahamood</last></author>
      <author><first>Maciej</first><last>Zembrzuski</last></author>
      <pages>391–396</pages>
      <abstract>This paper describes the implementation of the Hotel Scribe system. A commercial Natural Language Generation (NLG) system which generates descriptions of hotels from accommodation metadata with a high level of content and linguistic variation in English. It has been deployed live by *Anonymised Company Name* for the purpose of improving coverage of accommodation descriptions and for Search Engine Optimisation (SEO). In this paper, we describe the motivation for building this system, the challenges faced when dealing with limited metadata, and the implementation used to generate the highly variate accommodation descriptions. Additionally, we evaluate the uniqueness of the texts generated by our system against comparable human written accommodation description texts.</abstract>
      <url hash="745ab3d0">W19-8647</url>
      <doi>10.18653/v1/W19-8647</doi>
      <bibkey>mahamood-zembrzuski-2019-hotel</bibkey>
    </paper>
    <paper id="48">
      <title>The use of rating and <fixed-case>L</fixed-case>ikert scales in Natural Language Generation human evaluation tasks: A review and some recommendations</title>
      <author><first>Jacopo</first><last>Amidei</last></author>
      <author><first>Paul</first><last>Piwek</last></author>
      <author><first>Alistair</first><last>Willis</last></author>
      <pages>397–402</pages>
      <abstract>Rating and Likert scales are widely used in evaluation experiments to measure the quality of Natural Language Generation (NLG) systems. We review the use of rating and Likert scales for NLG evaluation tasks published in NLG specialized conferences over the last ten years (135 papers in total). Our analysis brings to light a number of deviations from good practice in their use. We conclude with some recommendations about the use of such scales. Our aim is to encourage the appropriate use of evaluation methodologies in the NLG community.</abstract>
      <url hash="23d07764">W19-8648</url>
      <doi>10.18653/v1/W19-8648</doi>
      <bibkey>amidei-etal-2019-use</bibkey>
    </paper>
    <paper id="49">
      <title>On task effects in <fixed-case>NLG</fixed-case> corpus elicitation: a replication study using mixed effects modeling</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Merel</first><last>van de Kerkhof</last></author>
      <author><first>Ruud</first><last>Koolen</last></author>
      <author><first>Martijn</first><last>Goudbeek</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>403–408</pages>
      <abstract>Task effects in NLG corpus elicitation recently started to receive more attention, but are usually not modeled statistically. We present a controlled replication of the study by Van Miltenburg et al. (2018b), contrasting spoken with written descriptions. We collected additional written Dutch descriptions to supplement the spoken data from the DIDEC corpus, and analyzed the descriptions using mixed effects modeling to account for variation between participants and items. Our results show that the effects of modality largely disappear in a controlled setting.</abstract>
      <url hash="f751c808">W19-8649</url>
      <attachment type="supplementary-attachment" hash="41f3a884">W19-8649.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8649</doi>
      <bibkey>van-miltenburg-etal-2019-task</bibkey>
    </paper>
    <paper id="50">
      <title>Procedural Text Generation from a Photo Sequence</title>
      <author><first>Taichi</first><last>Nishimura</last></author>
      <author><first>Atsushi</first><last>Hashimoto</last></author>
      <author><first>Shinsuke</first><last>Mori</last></author>
      <pages>409–414</pages>
      <abstract>Multimedia procedural texts, such as instructions and manuals with pictures, support people to share how-to knowledge. In this paper, we propose a method for generating a procedural text given a photo sequence allowing users to obtain a multimedia procedural text. We propose a single embedding space both for image and text enabling to interconnect them and to select appropriate words to describe a photo. We implemented our method and tested it on cooking instructions, i.e., recipes. Various experimental results showed that our method outperforms standard baselines.</abstract>
      <url hash="e607d414">W19-8650</url>
      <doi>10.18653/v1/W19-8650</doi>
      <bibkey>nishimura-etal-2019-procedural</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>S</fixed-case>imple<fixed-case>NLG</fixed-case>-<fixed-case>DE</fixed-case>: Adapting <fixed-case>S</fixed-case>imple<fixed-case>NLG</fixed-case> 4 to <fixed-case>G</fixed-case>erman</title>
      <author><first>Daniel</first><last>Braun</last></author>
      <author><first>Kira</first><last>Klimt</last></author>
      <author><first>Daniela</first><last>Schneider</last></author>
      <author><first>Florian</first><last>Matthes</last></author>
      <pages>415–420</pages>
      <abstract>SimpleNLG is a popular open source surface realiser for the English language. For German, however, the availability of open source and non-domain specific realisers is sparse, partly due to the complexity of the German language. In this paper, we present SimpleNLG-DE, an adaption of SimpleNLG to German. We discuss which parts of the German language have been implemented and how we evaluated our implementation using the TIGER Corpus and newly created data-sets.</abstract>
      <url hash="a91347d5">W19-8651</url>
      <doi>10.18653/v1/W19-8651</doi>
      <bibkey>braun-etal-2019-simplenlg</bibkey>
      <pwccode url="https://github.com/sebischair/SimpleNLG-DE" additional="false">sebischair/SimpleNLG-DE</pwccode>
    </paper>
    <paper id="52">
      <title>Semantic Noise Matters for Neural Natural Language Generation</title>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>David M.</first><last>Howcroft</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>421–426</pages>
      <abstract>Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.</abstract>
      <url hash="181aa57e">W19-8652</url>
      <attachment type="supplementary-attachment" hash="28c7e8cf">W19-8652.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8652</doi>
      <bibkey>dusek-etal-2019-semantic</bibkey>
      <pwccode url="https://github.com/tuetschek/e2e-cleaning" additional="false">tuetschek/e2e-cleaning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
    </paper>
    <paper id="53">
      <title>Can Neural Image Captioning be Controlled via Forced Attention?</title>
      <author><first>Philipp</first><last>Sadler</last></author>
      <author><first>Tatjana</first><last>Scheffler</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>427–431</pages>
      <abstract>Learned dynamic weighting of the conditioning signal (attention) has been shown to improve neural language generation in a variety of settings. The weights applied when generating a particular output sequence have also been viewed as providing a potentially explanatory insight in the internal workings of the generator. In this paper, we reverse the direction of this connection and ask whether through the control of the attention of the model we can control its output. Specifically, we take a standard neural image captioning model that uses attention, and fix the attention to predetermined areas in the image. We evaluate whether the resulting output is more likely to mention the class of the object in that area than the normally generated caption. We introduce three effective methods to control the attention and find that these are producing expected results in up to 27.43% of the cases.</abstract>
      <url hash="38242aab">W19-8653</url>
      <attachment type="supplementary-attachment" hash="fa29e90e">W19-8653.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8653</doi>
      <bibkey>sadler-etal-2019-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="54">
      <title>Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement</title>
      <author><first>Jan Milan</first><last>Deriu</last></author>
      <author><first>Mark</first><last>Cieliebak</last></author>
      <pages>432–437</pages>
      <abstract>We present “AutoJudge”, an automated evaluation method for conversational dialogue systems. The method works by first generating dialogues based on self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings on these dialogues to train an automated judgement model. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing systems. This works well for re-ranking a set of candidate utterances. However, our experiments show that AutoJudge cannot be applied as reward for reinforcement learning, although the metric can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.</abstract>
      <url hash="e6951a43">W19-8654</url>
      <attachment type="supplementary-attachment" hash="90e2d3f4">W19-8654.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8654</doi>
      <bibkey>deriu-cieliebak-2019-towards</bibkey>
    </paper>
    <paper id="55">
      <title>Generating Paraphrases with Lean Vocabulary</title>
      <author><first>Tadashi</first><last>Nomoto</last></author>
      <pages>438–442</pages>
      <abstract>In this work, we examine whether it is possible to achieve the state of the art performance in paraphrase generation with reduced vocabulary. Our approach consists of building a convolution to sequence model (Conv2Seq) partially guided by the reinforcement learning, and training it on the subword representation of the input. The experiment on the Quora dataset, which contains over 140,000 pairs of sentences and corresponding paraphrases, found that with less than 1,000 token types, we were able to achieve performance which exceeded that of the current state of the art.</abstract>
      <url hash="6061097e">W19-8655</url>
      <doi>10.18653/v1/W19-8655</doi>
      <bibkey>nomoto-2019-generating</bibkey>
    </paper>
    <paper id="56">
      <title>A Personalized Data-to-Text Support Tool for Cancer Patients</title>
      <author><first>Saar</first><last>Hommes</last></author>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Felix</first><last>Clouth</last></author>
      <author><first>Jeroen</first><last>Vermunt</last></author>
      <author><first>Xander</first><last>Verbeek</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>443–452</pages>
      <abstract>In this paper, we present a novel data-to-text system for cancer patients, providing information on quality of life implications after treatment, which can be embedded in the context of shared decision making. Currently, information on quality of life implications is often not discussed, partly because (until recently) data has been lacking. In our work, we rely on a newly developed prediction model, which assigns patients to scenarios. Furthermore, we use data-to-text techniques to explain these scenario-based predictions in personalized and understandable language. We highlight the possibilities of NLG for personalization, discuss ethical implications and also present the outcomes of a first evaluation with clinicians.</abstract>
      <url hash="b7cae015">W19-8656</url>
      <doi>10.18653/v1/W19-8656</doi>
      <bibkey>hommes-etal-2019-personalized</bibkey>
    </paper>
    <paper id="57">
      <title>Natural Language Generation at Scale: A Case Study for Open Domain Question Answering</title>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>Chandra</first><last>Khatri</last></author>
      <author><first>Rahul</first><last>Goel</last></author>
      <author><first>Behnam</first><last>Hedayatnia</last></author>
      <author><first>Anu</first><last>Venkatesh</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Raefer</first><last>Gabriel</last></author>
      <pages>453–462</pages>
      <abstract>Current approaches to Natural Language Generation (NLG) for dialog mainly focus on domain-specific, task-oriented applications (e.g. restaurant booking) using limited ontologies (up to 20 slot types), usually without considering the previous conversation context. Furthermore, these approaches require large amounts of data for each domain, and do not benefit from examples that may be available for other domains. This work explores the feasibility of applying statistical NLG to scenarios requiring larger ontologies, such as multi-domain dialog applications or open-domain question answering (QA) based on knowledge graphs. We model NLG through an Encoder-Decoder framework using a large dataset of interactions between real-world users and a conversational agent for open-domain QA. First, we investigate the impact of increasing the number of slot types on the generation quality and experiment with different partitions of the QA data with progressively larger ontologies (up to 369 slot types). Second, we perform multi-task learning experiments between open-domain QA and task-oriented dialog, and benchmark our model on a popular NLG dataset. Moreover, we experiment with using the conversational context as an additional input to improve response generation quality. Our experiments show the feasibility of learning statistical NLG models for open-domain QA with larger ontologies.</abstract>
      <url hash="636ff989">W19-8657</url>
      <doi>10.18653/v1/W19-8657</doi>
      <bibkey>cervone-etal-2019-natural</bibkey>
    </paper>
    <paper id="58">
      <title>Using <fixed-case>NLG</fixed-case> for speech synthesis of mathematical sentences</title>
      <author><first>Alessandro</first><last>Mazzei</last></author>
      <author><first>Michele</first><last>Monticone</last></author>
      <author><first>Cristian</first><last>Bernareggi</last></author>
      <pages>463–472</pages>
      <abstract>People with sight impairments can access to a mathematical expression by using its LaTeX source. However, this mechanisms have several drawbacks: (1) it assumes the knowledge of the LaTeX, (2) it is slow, since LaTeX is verbose and (3) it is error-prone since LATEX is a typographical language. In this paper we study the design of a natural language generation system for producing a mathematical sentence, i.e. a natural language sentence expressing the semantics of a mathematical expression. Moreover, we describe the main results of a first human based evaluation experiment of the system for Italian language.</abstract>
      <url hash="747f9958">W19-8658</url>
      <doi>10.18653/v1/W19-8658</doi>
      <bibkey>mazzei-etal-2019-using</bibkey>
    </paper>
    <paper id="59">
      <title>Teaching <fixed-case>FORG</fixed-case>e to Verbalize <fixed-case>DB</fixed-case>pedia Properties in <fixed-case>S</fixed-case>panish</title>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Stamatia</first><last>Dasiopoulou</last></author>
      <author><first>Beatriz</first><last>Fisas</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>473–483</pages>
      <abstract>Statistical generators increasingly dominate the research in NLG. However, grammar-based generators that are grounded in a solid linguistic framework remain very competitive, especially for generation from deep knowledge structures. Furthermore, if built modularly, they can be ported to other genres and languages with a limited amount of work, without the need of the annotation of a considerable amount of training data. One of these generators is FORGe, which is based on the Meaning-Text Model. In the recent WebNLG challenge (the first comprehensive task addressing the mapping of RDF triples to text) FORGe ranked first with respect to the overall quality in human evaluation. We extend the coverage of FORGE’s open source grammatical and lexical resources for English, so as to further improve the English texts, and port them to Spanish, to achieve a comparable quality. This confirms that, as already observed in the case of SimpleNLG, a robust universal grammar-driven framework and a systematic organization of the linguistic resources can be an adequate choice for NLG applications.</abstract>
      <url hash="205dafd4">W19-8659</url>
      <doi>10.18653/v1/W19-8659</doi>
      <bibkey>mille-etal-2019-teaching</bibkey>
    </paper>
    <paper id="60">
      <title>Generating justifications for norm-related agent decisions</title>
      <author><first>Daniel</first><last>Kasenberg</last></author>
      <author><first>Antonio</first><last>Roque</last></author>
      <author><first>Ravenna</first><last>Thielstrom</last></author>
      <author><first>Meia</first><last>Chita-Tegmark</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <pages>484–493</pages>
      <abstract>We present an approach to generating natural language justifications of decisions derived from norm-based reasoning. Assuming an agent which maximally satisfies a set of rules specified in an object-oriented temporal logic, the user can ask factual questions (about the agent’s rules, actions, and the extent to which the agent violated the rules) as well as “why” questions that require the agent comparing actual behavior to counterfactual trajectories with respect to these rules. To produce natural-sounding explanations, we focus on the subproblem of producing natural language clauses from statements in a fragment of temporal logic, and then describe how to embed these clauses into explanatory sentences. We use a human judgment evaluation on a testbed task to compare our approach to variants in terms of intelligibility, mental model and perceived trust.</abstract>
      <url hash="038a0ced">W19-8660</url>
      <doi>10.18653/v1/W19-8660</doi>
      <bibkey>kasenberg-etal-2019-generating</bibkey>
    </paper>
    <paper id="61">
      <title>Towards Generating Math Word Problems from Equations and Topics</title>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Danqing</first><last>Huang</last></author>
      <pages>494–503</pages>
      <abstract>A math word problem is a narrative with a specific topic that provides clues to the correct equation with numerical quantities and variables therein. In this paper, we focus on the task of generating math word problems. Previous works are mainly template-based with pre-defined rules. We propose a novel neural network model to generate math word problems from the given equations and topics. First, we design a fusion mechanism to incorporate the information of both equations and topics. Second, an entity-enforced loss is introduced to ensure the relevance between the generated math problem and the equation. Automatic evaluation results show that the proposed model significantly outperforms the baseline models. In human evaluations, the math word problems generated by our model are rated as being more relevant (in terms of solvability of the given equations and relevance to topics) and natural (i.e., grammaticality, fluency) than the baseline models.</abstract>
      <url hash="bf94fde4">W19-8661</url>
      <attachment type="supplementary-attachment" hash="092dd71d">W19-8661.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8661</doi>
      <bibkey>zhou-huang-2019-towards</bibkey>
    </paper>
    <paper id="62">
      <title><fixed-case>D</fixed-case>is<fixed-case>S</fixed-case>im: A Discourse-Aware Syntactic Text Simplification Framework for <fixed-case>E</fixed-case>nglish and <fixed-case>G</fixed-case>erman</title>
      <author><first>Christina</first><last>Niklaus</last></author>
      <author><first>Matthias</first><last>Cetto</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <pages>504–507</pages>
      <abstract>We introduce DisSim, a discourse-aware sentence splitting framework for English and German whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks.</abstract>
      <url hash="91cbc337">W19-8662</url>
      <doi>10.18653/v1/W19-8662</doi>
      <bibkey>niklaus-etal-2019-dissim</bibkey>
    </paper>
    <paper id="63">
      <title>Real World Voice Assistant System for Cooking</title>
      <author><first>Takahiko</first><last>Ito</last></author>
      <author><first>Shintaro</first><last>Inuzuka</last></author>
      <author><first>Yoshiaki</first><last>Yamada</last></author>
      <author><first>Jun</first><last>Harashima</last></author>
      <pages>508–509</pages>
      <abstract>This study presents a voice assistant system to support cooking by utilizing smart speakers in Japan. This system not only speaks the procedures written in recipes point by point but also answers the common questions from users for the specified recipes. The system applies machine comprehension techniques to millions of recipes for answering the common questions in cooking such as “人参はどうしたらよいですか (How should I cook carrots?)”. Furthermore, numerous machine-learning techniques are applied to generate better responses to users.</abstract>
      <url hash="0ce98014">W19-8663</url>
      <doi>10.18653/v1/W19-8663</doi>
      <bibkey>ito-etal-2019-real</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>VAE</fixed-case>-<fixed-case>PGN</fixed-case> based Abstractive Model in Multi-stage Architecture for Text Summarization</title>
      <author><first>Hyungtak</first><last>Choi</last></author>
      <author><first>Lohith</first><last>Ravuru</last></author>
      <author><first>Tomasz</first><last>Dryjański</last></author>
      <author><first>Sunghan</first><last>Rye</last></author>
      <author><first>Donghyun</first><last>Lee</last></author>
      <author><first>Hojung</first><last>Lee</last></author>
      <author><first>Inchul</first><last>Hwang</last></author>
      <pages>510–515</pages>
      <abstract>This paper describes our submission to the TL;DR challenge. Neural abstractive summarization models have been successful in generating fluent and consistent summaries with advancements like the copy (Pointer-generator) and coverage mechanisms. However, these models suffer from their extractive nature as they learn to copy words from the source text. In this paper, we propose a novel abstractive model based on Variational Autoencoder (VAE) to address this issue. We also propose a Unified Summarization Framework for the generation of summaries. Our model eliminates non-critical information at a sentence-level with an extractive summarization module and generates the summary word by word using an abstractive summarization module. To implement our framework, we combine submodules with state-of-the-art techniques including Pointer-Generator Network (PGN) and BERT while also using our new VAE-PGN abstractive model. We evaluate our model on the benchmark Reddit corpus as part of the TL;DR challenge and show that our model outperforms the baseline in ROUGE score while generating diverse summaries.</abstract>
      <url hash="5e3e5f7a">W19-8664</url>
      <doi>10.18653/v1/W19-8664</doi>
      <bibkey>choi-etal-2019-vae</bibkey>
    </paper>
    <paper id="65">
      <title>Generating Abstractive Summaries with Finetuned Language Models</title>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Zachary</first><last>Ziegler</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>516–522</pages>
      <abstract>Neural abstractive document summarization is commonly approached by models that exhibit a mostly extractive behavior. This behavior is facilitated by a copy-attention which allows models to copy words from a source document. While models in the mostly extractive news summarization domain benefit from this inductive bias, they commonly fail to paraphrase or compress information from the source document. Recent advances in transfer-learning from large pretrained language models give rise to alternative approaches that do not rely on copy-attention and instead learn to generate concise and abstractive summaries. In this paper, as part of the TL;DR challenge, we compare the abstractiveness of summaries from different summarization approaches and show that transfer-learning can be efficiently utilized without any changes to the model architecture. We demonstrate that the approach leads to a higher level of abstraction for a similar performance on the TL;DR challenge tasks, enabling true natural language compression.</abstract>
      <url hash="d4e50260">W19-8665</url>
      <doi>10.18653/v1/W19-8665</doi>
      <bibkey>gehrmann-etal-2019-generating</bibkey>
    </paper>
    <paper id="66">
      <title>Towards Summarization for Social Media - Results of the <fixed-case>TL</fixed-case>;<fixed-case>DR</fixed-case> Challenge</title>
      <author><first>Shahbaz</first><last>Syed</last></author>
      <author><first>Michael</first><last>Völske</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>523–528</pages>
      <abstract>In this paper, we report on the results of the TL;DR challenge, discussing an extensive manual evaluation of the expected properties of a good summary based on analyzing the comments provided by human annotators.</abstract>
      <url hash="f7c3ed31">W19-8666</url>
      <doi>10.18653/v1/W19-8666</doi>
      <bibkey>syed-etal-2019-towards</bibkey>
    </paper>
    <paper id="67">
      <title>Generating Quantified Descriptions of Abstract Visual Scenes</title>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Kees</first><last>van Deemter</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>529–539</pages>
      <abstract>Quantified expressions have always taken up a central position in formal theories of meaning and language use. Yet quantified expressions have so far attracted far less attention from the Natural Language Generation community than, for example, referring expressions. In an attempt to start redressing the balance, we investigate a recently developed corpus in which quantified expressions play a crucial role; the corpus is the result of a carefully controlled elicitation experiment, in which human participants were asked to describe visually presented scenes. Informed by an analysis of this corpus, we propose algorithms that produce computer-generated descriptions of a wider class of visual scenes, and we evaluate the descriptions generated by these algorithms in terms of their correctness, completeness, and human-likeness. We discuss what this exercise can teach us about the nature of quantification and about the challenges posed by the generation of quantified expressions.</abstract>
      <url hash="659b5e10">W19-8667</url>
      <attachment type="supplementary-attachment" hash="ae67e5d7">W19-8667.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8667</doi>
      <bibkey>chen-etal-2019-generating</bibkey>
    </paper>
    <paper id="68">
      <title>What goes into a word: generating image descriptions with top-down spatial knowledge</title>
      <author><first>Mehdi</first><last>Ghanimifard</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>540–551</pages>
      <abstract>Generating grounded image descriptions requires associating linguistic units with their corresponding visual clues. A common method is to train a decoder language model with attention mechanism over convolutional visual features. Attention weights align the stratified visual features arranged by their location with tokens, most commonly words, in the target description. However, words such as spatial relations (e.g. <i>next to</i> and <i>under</i>) are not directly referring to geometric arrangements of pixels but to complex geometric and conceptual representations. The aim of this paper is to evaluate what representations facilitate generating image descriptions with spatial relations and lead to better grounded language generation. In particular, we investigate the contribution of three different representational modalities in generating relational referring expressions: (i) pre-trained convolutional visual features, (ii) different top-down geometric relational knowledge between objects, and (iii) world knowledge captured by contextual embeddings in language models.</abstract>
      <url hash="2c39afbe">W19-8668</url>
      <attachment type="supplementary-attachment" hash="a450646b">W19-8668.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8668</doi>
      <bibkey>ghanimifard-dobnik-2019-goes</bibkey>
    </paper>
    <paper id="69">
      <title>Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models</title>
      <author><first>Raheel</first><last>Qader</last></author>
      <author><first>François</first><last>Portet</last></author>
      <author><first>Cyril</first><last>Labbé</last></author>
      <pages>552–562</pages>
      <abstract>In Natural Language Generation (NLG), End-to-End (E2E) systems trained through deep learning have recently gained a strong interest. Such deep models need a large amount of carefully annotated data to reach satisfactory performance. However, acquiring such datasets for every new NLG application is a tedious and time-consuming task. In this paper, we propose a semi-supervised deep learning scheme that can learn from non-annotated data and annotated data when available. It uses a NLG and a Natural Language Understanding (NLU) sequence-to-sequence models which are learned jointly to compensate for the lack of annotation. Experiments on two benchmark datasets show that, with limited amount of annotated data, the method can achieve very competitive results while not using any pre-processing or re-scoring tricks. These findings open the way to the exploitation of non-annotated datasets which is the current bottleneck for the E2E NLG system development to new applications.</abstract>
      <url hash="82f93c73">W19-8669</url>
      <doi>10.18653/v1/W19-8669</doi>
      <bibkey>qader-etal-2019-semi</bibkey>
    </paper>
    <paper id="70">
      <title>Neural Generation for <fixed-case>C</fixed-case>zech: Data and Baselines</title>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Filip</first><last>Jurčíček</last></author>
      <pages>563–574</pages>
      <abstract>We present the first dataset targeted at end-to-end NLG in Czech in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach. While non-English NLG is under-explored in general, Czech, as a morphologically rich language, makes the task even harder: Since Czech requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial. In our experiments, we present two different approaches to this this problem: (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup: our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.</abstract>
      <url hash="2806eeca">W19-8670</url>
      <doi>10.18653/v1/W19-8670</doi>
      <bibkey>dusek-jurcicek-2019-neural</bibkey>
    </paper>
    <paper id="71">
      <title>Modeling Confidence in Sequence-to-Sequence Models</title>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Ngoc-Quan</first><last>Pham</last></author>
      <pages>575–583</pages>
      <abstract>Recently, significant improvements have been achieved in various natural language processing tasks using neural sequence-to-sequence models. While aiming for the best generation quality is important, ultimately it is also necessary to develop models that can assess the quality of their output. In this work, we propose to use the similarity between training and test conditions as a measure for models’ confidence. We investigate methods solely using the similarity as well as methods combining it with the posterior probability. While traditionally only target tokens are annotated with confidence measures, we also investigate methods to annotate source tokens with confidence. By learning an internal alignment model, we can significantly improve confidence projection over using state-of-the-art external alignment tools. We evaluate the proposed methods on downstream confidence estimation for machine translation (MT). We show improvements on segment-level confidence estimation as well as on confidence estimation for source tokens. In addition, we show that the same methods can also be applied to other tasks using sequence-to-sequence models. On the automatic speech recognition (ASR) task, we are able to find 60% of the errors by looking at 20% of the data.</abstract>
      <url hash="dd96805c">W19-8671</url>
      <doi>10.18653/v1/W19-8671</doi>
      <bibkey>niehues-pham-2019-modeling</bibkey>
    </paper>
    <paper id="72">
      <title>A Good Sample is Hard to Find: Noise Injection Sampling and Self-Training for Neural Language Generation Models</title>
      <author><first>Chris</first><last>Kedzie</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>584–593</pages>
      <abstract>Deep neural networks (DNN) are quickly becoming the de facto standard modeling method for many natural language generation (NLG) tasks. In order for such models to truly be useful, they must be capable of correctly generating utterances for novel meaning representations (MRs) at test time. In practice, even sophisticated DNNs with various forms of semantic control frequently fail to generate utterances faithful to the input MR. In this paper, we propose an architecture agnostic self-training method to sample novel MR/text utterance pairs to augment the original training data. Remarkably, after training on the augmented data, even simple encoder-decoder models with greedy decoding are capable of generating semantically correct utterances that are as good as state-of-the-art outputs in both automatic and human evaluations of quality.</abstract>
      <url hash="1244d742">W19-8672</url>
      <attachment type="supplementary-attachment" hash="e1ffa534">W19-8672.Supplementary_Attachment.pdf</attachment>
      <doi>10.18653/v1/W19-8672</doi>
      <bibkey>kedzie-mckeown-2019-good</bibkey>
      <pwccode url="https://github.com/kedz/noiseylg" additional="false">kedz/noiseylg</pwccode>
    </paper>
    <paper id="73">
      <title>A Stable Variational Autoencoder for Text Modelling</title>
      <author><first>Ruizhe</first><last>Li</last></author>
      <author><first>Xiao</first><last>Li</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Matthew</first><last>Collinson</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <pages>594–599</pages>
      <abstract>Variational Autoencoder (VAE) is a powerful method for learning representations of high-dimensional data. However, VAEs can suffer from an issue known as latent variable collapse (or KL term vanishing), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks. Such an issue is particularly prevalent when employing VAE-RNN architectures for text modelling (Bowman et al., 2016; Yang et al., 2017). In this paper, we present a new architecture called Full-Sampling-VAE-RNN, which can effectively avoid latent variable collapse. Compared to the general VAE-RNN architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality.</abstract>
      <url hash="5439569b">W19-8673</url>
      <doi>10.18653/v1/W19-8673</doi>
      <bibkey>li-etal-2019-stable</bibkey>
      <pwccode url="https://github.com/ruizheliUOA/HR-VAE" additional="false">ruizheliUOA/HR-VAE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
  </volume>
  <volume id="87" ingest-date="2020-01-15">
    <meta>
      <booktitle>Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)</booktitle>
      <url hash="9951dd38">W19-87</url>
      <publisher>Incoma Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="f466c5d7">W19-8700</url>
      <bibkey>ws-2019-human</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Comparison between Automatic and Human Subtitling: A Case Study with Game of Thrones</title>
      <author><first>Sabrina</first><last>Baldo de Brébisson</last></author>
      <pages>1–10</pages>
      <abstract>In this submission, I would like to share my experiences with the software DeepL and the comparison analysis I have made with human subtitling offered by the DVD version of the corpus I have chosen as the topic of my study – the eight Seasons of Game of Thrones. The idea is to study if the version proposed by an automatic translation program could be used as a first draft for the professional subtitler. It is expected that the latter would work on the form of the subtitles, that is to say mainly on their length, in a second step.</abstract>
      <url hash="3839f18a">W19-8701</url>
      <doi>10.26615/issn.2683-0078.2019_001</doi>
      <bibkey>baldo-de-brebisson-2019-comparison</bibkey>
    </paper>
    <paper id="2">
      <title>Parallel Corpus of <fixed-case>C</fixed-case>roatian-<fixed-case>I</fixed-case>talian Administrative Texts</title>
      <author><first>Marija</first><last>Brkic Bakaric</last></author>
      <author><first>Ivana</first><last>Lalli Pacelat</last></author>
      <pages>11–18</pages>
      <abstract>Parallel corpora constitute a unique re-source for providing assistance to human translators. The selection and preparation of the parallel corpora also conditions the quality of the resulting MT engine. Since Croatian is a national language and Italian is officially recognized as a minority lan-guage in seven cities and twelve munici-palities of Istria County, a large amount of parallel texts is produced on a daily basis. However, there have been no attempts in using these texts for compiling a parallel corpus. A domain-specific sentence-aligned parallel Croatian-Italian corpus of administrative texts would be of high value in creating different language tools and resources. The aim of this paper is, therefore, to explore the value of parallel documents which are publicly available mostly in pdf format and to investigate the use of automatically-built dictionaries in corpus compilation. The effects that a document format and, consequently sentence splitting, and the dictionary input have on the sentence alignment process are manually evaluated.</abstract>
      <url hash="f4c24647">W19-8702</url>
      <doi>10.26615/issn.2683-0078.2019_002</doi>
      <bibkey>brkic-bakaric-lalli-pacelat-2019-parallel</bibkey>
    </paper>
    <paper id="3">
      <title>What Influences the Features of Post-editese? A Preliminary Study</title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <author><first>Natália</first><last>Resende</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>19–27</pages>
      <abstract>While a number of studies have shown evidence of translationese phenomena, that is, statistical differences between original texts and translated texts (Gellerstam, 1986), results of studies searching for translationese features in postedited texts (what has been called ”posteditese” (Daems et al., 2017)) have presented mixed results. This paper reports a preliminary study aimed at identifying the presence of post-editese features in machine-translated post-edited texts and at understanding how they differ from translationese features. We test the influence of factors such as post-editing (PE) levels (full vs. light), translation proficiency (professionals vs. students) and text domain (news vs. literary). Results show evidence of post-editese features, especially in light PE texts and in certain domains.</abstract>
      <url hash="f873968e">W19-8703</url>
      <doi>10.26615/issn.2683-0078.2019_003</doi>
      <bibkey>castilho-etal-2019-influences</bibkey>
    </paper>
    <paper id="4">
      <title>Designing a Frame-Semantic Machine Translation Evaluation Metric</title>
      <author><first>Oliver</first><last>Czulo</last></author>
      <author><first>Tiago Timponi</first><last>Torrent</last></author>
      <author><first>Ely Edison da Silva</first><last>Matos</last></author>
      <author><first>Alexandre</first><last>Diniz da Costa</last></author>
      <author><first>Debanjana</first><last>Kar</last></author>
      <pages>28–35</pages>
      <abstract>We propose a metric for machine translation evaluation based on frame semantics which does not require the use of reference translations or human corrections, but is aimed at comparing original and translated output directly. The metrics is described on the basis of an existing manual frame-semantic annotation of a parallel corpus with an English original and a Brazilian Portuguese and a German translation. We discuss implications of our metrics design, including the potential of scaling it for multiple languages.</abstract>
      <url hash="d13687ea">W19-8704</url>
      <doi>10.26615/issn.2683-0078.2019_004</doi>
      <bibkey>czulo-etal-2019-designing</bibkey>
    </paper>
    <paper id="5">
      <title>Human Evaluation of Neural Machine Translation: The Case of Deep Learning</title>
      <author><first>Marie</first><last>Escribe</last></author>
      <pages>36–46</pages>
      <abstract>Recent advances in artificial neural networks now have a great impact on translation technology. A considerable achievement was reached in this field with the publication of L’Apprentissage Profond. This book, originally written in English (Deep Learning), was entirely machine-translated into French and post-edited by several experts. In this context, it appears essential to have a clear vision of the performance of MT tools. Providing an evaluation of NMT is precisely the aim of the present research paper. To accomplish this objective, a framework for error categorisation was built and a comparative analysis of the raw translation output and the post-edited version was performed with the purpose of identifying recurring patterns of errors. The findings showed that even though some grammatical errors were spotted, the output was generally correct from a linguistic point of view. The most recurring errors are linked to the specialised terminology employed in this book. Further errors include parts of text that were not translated as well as edits based on stylistic preferences. The major part of the output was not acceptable as such and required several edits per segment, but some sentences were of publishable quality and were therefore left untouched in the final version.</abstract>
      <url hash="14e4e8f2">W19-8705</url>
      <doi>10.26615/issn.2683-0078.2019_005</doi>
      <bibkey>escribe-2019-human</bibkey>
    </paper>
    <paper id="6">
      <title>Translationese Features as Indicators of Quality in <fixed-case>E</fixed-case>nglish-<fixed-case>R</fixed-case>ussian Human Translation</title>
      <author><first>Maria</first><last>Kunilovskaya</last></author>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <pages>47–56</pages>
      <abstract>We use a range of morpho-syntactic features inspired by research in register studies (e.g. Biber, 1995; Neumann, 2013) and translation studies (e.g. Ilisei et al., 2010; Zanettin, 2013; Kunilovskaya and Kutuzov, 2018) to reveal the association between translationese and human translation quality. Translationese is understood as any statistical deviations of translations from non-translations (Baker, 1993) and is assumed to affect the fluency of translations, rendering them foreign-sounding and clumsy of wording and structure. This connection is often posited or implied in the studies of translationese or translational varieties (De Sutter et al., 2017), but is rarely directly tested. Our 45 features include frequencies of selected morphological forms and categories, some types of syntactic structures and relations, as well as several overall text measures extracted from Universal Dependencies annotation. The research corpora include English-to-Russian professional and student translations of informational or argumentative newspaper texts and a comparable corpus of non-translated Russian. Our results indicate lack of direct association between translationese and quality in our data: while our features distinguish translations and non-translations with the near perfect accuracy, the performance of the same algorithm on the quality classes barely exceeds the chance level.</abstract>
      <url hash="795bdd3f">W19-8706</url>
      <doi>10.26615/issn.2683-0078.2019_006</doi>
      <bibkey>kunilovskaya-lapshinova-koltunski-2019-translationese</bibkey>
    </paper>
    <paper id="7">
      <title>The Punster’s Amanuensis: The Proper Place of Humans and Machines in the Translation of Wordplay</title>
      <author><first>Tristan</first><last>Miller</last></author>
      <pages>57–65</pages>
      <abstract>The translation of wordplay is one of the most extensively researched problems in translation studies, but it has attracted little attention in the fields of natural language processing and machine translation. This is because today’s language technologies treat anomalies and ambiguities in the input as things that must be resolved in favour of a single “correct” interpretation, rather than preserved and interpreted in their own right. But if computers cannot yet process such creative language on their own, can they at least provide specialized support to translation professionals? In this paper, I survey the state of the art relevant to computational processing of humorous wordplay and put forth a vision of how existing theories, resources, and technologies could be adapted and extended to support interactive, computer-assisted translation.</abstract>
      <url hash="7b7ec52e">W19-8707</url>
      <doi>10.26615/issn.2683-0078.2019_007</doi>
      <bibkey>miller-2019-punsters</bibkey>
    </paper>
    <paper id="8">
      <title>Comparing a Hand-crafted to an Automatically Generated Feature Set for Deep Learning: Pairwise Translation Evaluation</title>
      <author><first>Despoina</first><last>Mouratidis</last></author>
      <author><first>Katia Lida</first><last>Kermanidis</last></author>
      <pages>66–74</pages>
      <abstract>The automatic evaluation of machine translation (MT) has proven to be a very significant research topic. Most automatic evaluation methods focus on the evaluation of the output of MT as they compute similarity scores that represent translation quality. This work targets on the performance of MT evaluation. We present a general scheme for learning to classify parallel translations, using linguistic information, of two MT model outputs and one human (reference) translation. We present three experiments to this scheme using neural networks (NN). One using string based hand-crafted features (Exp1), the second using automatically trained embeddings from the reference and the two MT outputs (one from a statistical machine translation (SMT) model and the other from a neural ma-chine translation (NMT) model), which are learned using NN (Exp2), and the third experiment (Exp3) that combines information from the other two experiments. The languages involved are English (EN), Greek (GR) and Italian (IT) segments are educational in domain. The proposed language-independent learning scheme which combines information from the two experiments (experiment 3) achieves higher classification accuracy compared with models using BLEU score information as well as other classification approaches, such as Random Forest (RF) and Support Vector Machine (SVM).</abstract>
      <url hash="de59dc0b">W19-8708</url>
      <doi>10.26615/issn.2683-0078.2019_008</doi>
      <bibkey>mouratidis-kermanidis-2019-comparing</bibkey>
    </paper>
    <paper id="9">
      <title>Differences between <fixed-case>SMT</fixed-case> and <fixed-case>NMT</fixed-case> Output - a Translators’ Point of View</title>
      <author><first>Jonathan</first><last>Mutal</last></author>
      <author><first>Lise</first><last>Volkart</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Sabrina</first><last>Girletti</last></author>
      <author><first>Paula</first><last>Estrella</last></author>
      <pages>75–81</pages>
      <abstract>In this study, we compare the output quality of two MT systems, a statistical (SMT) and a neural (NMT) engine, customised for Swiss Post’s Language Service using the same training data. We focus on the point of view of professional translators and investigate how they perceive the differences between the MT output and a human reference (namely deletions, substitutions, insertions and word order). Our findings show that translators more frequently consider these differences to be errors in SMT than NMT, and that deletions are the most serious errors in both architectures. We also observe lower agreement on differences to be corrected in NMT than in SMT, suggesting that errors are easier to identify in SMT. These findings confirm the ability of NMT to produce correct paraphrases, which could also explain why BLEU is often considered as an inadequate metric to evaluate the performance of NMT systems.</abstract>
      <url hash="9d11d718">W19-8709</url>
      <doi>10.26615/issn.2683-0078.2019_009</doi>
      <bibkey>mutal-etal-2019-differences</bibkey>
    </paper>
    <paper id="10">
      <title>The <fixed-case>C</fixed-case>hinese/<fixed-case>E</fixed-case>nglish Political Interpreting Corpus (<fixed-case>CEPIC</fixed-case>): A New Electronic Resource for Translators and Interpreters</title>
      <author><first>Jun</first><last>Pan</last></author>
      <pages>82–88</pages>
      <abstract>The Chinese/English Political Interpreting Corpus (CEPIC) is a new electronic and open access resource developed for translators and interpreters, especially those working with political text types. Over 6 million word tokens in size, the online corpus consists of transcripts of Chinese (Cantonese &amp; Putonghua) / English political speeches and their translated and interpreted texts. It includes rich meta-data and is POS-tagged and annotated with prosodic and paralinguistic features that are of concern to spoken language and interpreting. The online platform of the CEPIC features main functions including Keyword Search, Word Collocation and Expanded Keyword in Context, which are illustrated in the paper. The CEPIC can shed light on online translation and interpreting corpora development in the future.</abstract>
      <url hash="4d81348c">W19-8710</url>
      <doi>10.26615/issn.2683-0078.2019_010</doi>
      <bibkey>pan-2019-chinese</bibkey>
    </paper>
    <paper id="11">
      <title>Translation Quality Assessment Tools and Processes in Relation to <fixed-case>CAT</fixed-case> Tools</title>
      <author><first>Viktoriya</first><last>Petrova</last></author>
      <pages>89–97</pages>
      <abstract>Modern translation QA tools are the latest attempt to overcome the inevitable subjective component of human revisers. This paper analyzes the current situation in the translation industry in respect to those tools and their relationship with CAT tools. The adoption of international standards has set the basic frame that defines “quality”. Because of the clear impossibility to develop a universal QA tool, all of the existing ones have in common a wide variety of settings for the user to choose from. A brief comparison is made between most popular standalone QA tools. In order to verify their results in practice, QA outputs from two of those tools have been compared. Polls that cover a period of 12 years have been collected. Their participants explained what practices they adopted in order to guarantee quality.</abstract>
      <url hash="03203014">W19-8711</url>
      <doi>10.26615/issn.2683-0078.2019_011</doi>
      <bibkey>petrova-2019-translation</bibkey>
    </paper>
    <paper id="12">
      <title>Corpus Linguistics, Translation and Error Analysis</title>
      <author><first>Maria</first><last>Stambolieva</last></author>
      <pages>98–104</pages>
      <abstract>The paper presents a study of the French Imparfait and its functional equivalents in Bulgarian and English in view of applications in machine translation and error analysis. The aims of the study are: 1/ based on the analysis of a corpus of text, to validate/revise earlier research on the values of the French Imparfait, 2/ to define the contextual factors pointing to the realisation of one or another value of the forms, 3/ based on the analysis of aligned translations, to identify the translation equivalents of these values, 4/ to formulate translation rules, 5/ based on the analysis of the translation rules, to refine the annotation modules of the environment used – the NBU e-Platform for language teaching and research.</abstract>
      <url hash="773c20ec">W19-8712</url>
      <doi>10.26615/issn.2683-0078.2019_012</doi>
      <bibkey>stambolieva-2019-corpus</bibkey>
    </paper>
    <paper id="13">
      <title>Human-Informed Speakers and Interpreters Analysis in the <fixed-case>WAW</fixed-case> Corpus and an Automatic Method for Calculating Interpreters’ Décalage</title>
      <author><first>Irina</first><last>Temnikova</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Souhila</first><last>Djabri</last></author>
      <author><first>Samy</first><last>Hedaya</last></author>
      <pages>105–115</pages>
      <abstract>This article presents a multi-faceted analysis of a subset of interpreted conference speeches from the WAW corpus for the English-Arabic language pair. We analyze several speakers and interpreters variables via manual annotation and automatic methods. We propose a new automatic method for calculating interpreters’ décalage based on Automatic Speech Recognition (ASR) and automatic alignment of named entities and content words between speaker and interpreter. The method is evaluated by two human annotators who have expertise in interpreting and Interpreting Studies and shows highly satisfactory results, accompanied with a high inter-annotator agreement. We provide insights about the relations of speakers’ variables, interpreters’ variables and décalage and discuss them from Interpreting Studies and interpreting practice point of view. We had interesting findings about interpreters behavior which need to be extended to a large number of conference sessions in our future research.</abstract>
      <url hash="c6818e83">W19-8713</url>
      <doi>10.26615/issn.2683-0078.2019_013</doi>
      <bibkey>temnikova-etal-2019-human</bibkey>
    </paper>
    <paper id="14">
      <title>Towards a Proactive <fixed-case>MWE</fixed-case> Terminological Platform for Cross-Lingual Mediation in the Age of Big Data</title>
      <author><first>Benjamin K.</first><last>Tsou</last></author>
      <author><first>Kapo</first><last>Chow</last></author>
      <author><first>Junru</first><last>Nie</last></author>
      <author><first>Yuan</first><last>Yuan</last></author>
      <pages>116–121</pages>
      <abstract>The emergence of China as a global economic power in the 21st Century has brought about surging needs for cross-lingual and cross-cultural mediation, typically performed by translators. Advances in Artificial Intelligence and Language Engineering have been bolstered by Machine learning and suitable Big Data cultivation. They have helped to meet some of the translator’s needs, though the technical specialists have not kept pace with the practical and expanding requirements in language mediation. One major technical and linguistic hurdle involves words outside the vocabulary of the translator or the lexical database he/she consults, especially Multi-Word Expressions (Compound Words) in technical subjects. A further problem is in the multiplicity of renditions of a term in the target language. This paper discusses a proactive approach following the successful extraction and application of sizable bilingual Multi-Word Expressions (Compound Words) for language mediation in technical subjects, which do not fall within the expertise of typical translators, who have inadequate appreciation of the range of new technical tools available to help him/her. Our approach draws on the personal reflections of translators and teachers of translation and is based on the prior R&amp;D efforts relating to 300,000 comparable Chinese-English patents. The subsequent protocol we have developed aims to be proactive in meeting four identified practical challenges in technical translation (e.g. patents). It has broader economic implication in the Age of Big Data (Tsou et al, 2015) and Trade War, as the workload, if not, the challenges, increasingly cannot be met by currently available front-line translators. We shall demonstrate how new tools can be harnessed to spearhead the application of language technology not only in language mediation but also in the “teaching” and “learning” of translation. It shows how a better appreciation of their needs may enhance the contributions of the technical specialists, and thus enhance the resultant synergetic benefits.</abstract>
      <url hash="ac34f265">W19-8714</url>
      <doi>10.26615/issn.2683-0078.2019_014</doi>
      <bibkey>tsou-etal-2019-towards</bibkey>
    </paper>
    <paper id="15">
      <title>Exploring Adequacy Errors in Neural Machine Translation with the Help of Cross-Language Aligned Word Embeddings</title>
      <author><first>Michael</first><last>Ustaszewski</last></author>
      <pages>122–128</pages>
      <abstract>Neural machine translation (NMT) was shown to produce more fluent output than phrase-based statistical (PBMT) and rule-based machine translation (RBMT). However, improved fluency makes it more difficult for post editors to identify and correct adequacy errors, because unlike RBMT and SMT, in NMT adequacy errors are frequently not anticipated by fluency errors. Omissions and additions of content in otherwise flawlessly fluent NMT output are the most prominent types of such adequacy errors, which can only be detected with reference to source texts. This contribution explores the degree of semantic similarity between source texts, NMT output and post edited output. In this way, computational semantic similarity scores (cosine similarity) are related to human quality judgments. The analyses are based on publicly available NMT post editing data annotated for errors in three language pairs (EN-DE, EN-LV, EN-HR) with the Multidimensional Quality Metrics (MQM). Methodologically, this contribution tests whether cross-language aligned word embeddings as the sole source of semantic information mirror human error annotation.</abstract>
      <url hash="82778361">W19-8715</url>
      <doi>10.26615/issn.2683-0078.2019_015</doi>
      <bibkey>ustaszewski-2019-exploring</bibkey>
    </paper>
    <paper id="16">
      <title>The Success Story of Mitra Translations</title>
      <author><first>Mina</first><last>Ilieva</last></author>
      <author><first>Mariya</first><last>Kancheva</last></author>
      <pages>129–133</pages>
      <abstract>Technologies and their constant updates and innovative nature drastically and irreversibly transformed this small business into a leading brand on the translation market, along with just few other LSPs integrating translation software solutions. Now, we are constantly following the new developments in software updates and online platforms and we are successfully keeping up with any new trend in the field of translation, localization, transcreation, revision, post-editing, etc. Ultimately, we are positive that proper implementation of technology (with focus on quality, cost and time) and hard work are the stepping stones in the way to become a trusted translation services provider.</abstract>
      <url hash="3601e252">W19-8716</url>
      <doi>10.26615/issn.2683-0078.2019_016</doi>
      <bibkey>ilieva-kancheva-2019-success</bibkey>
    </paper>
    <paper id="17">
      <title>The Four Stages of Machine Translation Acceptance in a Freelancer’s Life</title>
      <author><first>Maria</first><last>Sgourou</last></author>
      <pages>134–135</pages>
      <abstract>Technology is a big challenge and raises many questions and issues when it comes to its application in the translation process, but translation’s biggest problem is not technology; it is rather how technology is perceived by translators. MT developers and researchers should take into account this perception and move towards a more democratized approach to include the base of the translation industry and perhaps its more valuable asset, the translators.</abstract>
      <url hash="67d39698">W19-8717</url>
      <doi>10.26615/issn.2683-0078.2019_017</doi>
      <bibkey>sgourou-2019-four</bibkey>
    </paper>
    <paper id="18">
      <title>Optimising the Machine Translation Post-editing Workflow</title>
      <author><first>Anna</first><last>Zaretskaya</last></author>
      <pages>136–139</pages>
      <abstract>In this article, we describe how machine translation is used for post-editing at TransPerfect and the ways in which we optimise the workflow. This includes MT evaluation, MT engine customisation, leveraging MT suggestions compared to TM matches, and the lessons learnt from implementing MT at a large scale.</abstract>
      <url hash="045082d9">W19-8718</url>
      <doi>10.26615/issn.2683-0078.2019_018</doi>
      <bibkey>zaretskaya-2019-optimising</bibkey>
    </paper>
  </volume>
  <volume id="89" ingest-date="2020-01-15">
    <meta>
      <booktitle>Proceedings of the Workshop MultiLing 2019: Summarization Across Languages, Genres and Sources</booktitle>
      <url hash="8eac9beb">W19-89</url>
      <editor><first>George</first><last>Giannakopoulos</last></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="7ccb3e20">W19-8900</url>
      <bibkey>ws-2019-multiling</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>RANLP</fixed-case> 2019 Multilingual Headline Generation Task Overview</title>
      <author><first>Marina</first><last>Litvak</last></author>
      <author><first>John M.</first><last>Conroy</last></author>
      <author><first>Peter A.</first><last>Rankel</last></author>
      <pages>1–5</pages>
      <abstract>The objective of the 2019 RANLP Multilingual Headline Generation (HG) Task is to explore some of the challenges highlighted by current state of the art approaches on creating informative headlines to news articles: non-descriptive headlines, out-of-domain training data, generating headlines from long documents which are not well represented by the head heuristic, and dealing with multilingual domain. This tasks makes available a large set of training data for headline generation and provides an evaluation methods for the task. Our data sets are drawn from Wikinews as well as Wikipedia. Participants were required to generate headlines for at least 3 languages, which were evaluated via automatic methods. A key aspect of the task is multilinguality. The task measures the performance of multilingual headline generation systems using the Wikipedia and Wikinews articles in multiple languages. The objective is to assess the performance of automatic headline generation techniques on text documents covering a diverse range of languages and topics outside the news domain.</abstract>
      <url hash="f619fdd7">W19-8901</url>
      <doi>10.26615/978-954-452-058-8_001</doi>
      <bibkey>litvak-etal-2019-ranlp</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>ing 2019: Financial Narrative Summarisation</title>
      <author><first>Mahmoud</first><last>El-Haj</last></author>
      <pages>6–10</pages>
      <abstract>The Financial Narrative Summarisation task at MultiLing 2019 aims to demonstrate the value and challenges of applying automatic text summarisation to financial text written in English, usually referred to as financial narrative disclosures. The task dataset has been extracted from UK annual reports published in PDF file format. The participants were asked to provide structured summaries, based on real-world, publicly available financial annual reports of UK firms by extracting information from different key sections. Participants were asked to generate summaries that reflects the analysis and assessment of the financial trend of the business over the past year, as provided by annual reports. The evaluation of the summaries was performed using AutoSummENG and Rouge automatic metrics. This paper focuses mainly on the data creation process.</abstract>
      <url hash="19200b25">W19-8902</url>
      <doi>10.26615/978-954-452-058-8_002</doi>
      <bibkey>el-haj-2019-multiling</bibkey>
    </paper>
    <paper id="3">
      <title>The Summary Evaluation Task in the <fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>ing - <fixed-case>RANLP</fixed-case> 2019 Workshop</title>
      <author><first>George</first><last>Giannakopoulos</last></author>
      <author><first>Nikiforos</first><last>Pittaras</last></author>
      <pages>11–16</pages>
      <abstract>This report covers the summarization evaluation task, proposed to the summarization community via the MultiLing 2019 Workshop of the RANLP 2019 conference. The task aims to encourage the development of automatic summarization evaluation methods closely aligned with manual, human-authored summary grades and judgements. A multilingual setting is adopted, building upon a corpus of Wikinews articles across 6 languages (English, Arabic, Romanian, Greek, Spanish and Czech). The evaluation utilizes human (golden) and machine-generated (peer) summaries, which have been assigned human evaluation scores from previous MultiLing tasks. Using these resources, the original corpus is augmented with synthetic data, combining summary texts under three different strategies (reorder, merge and replace), each engineered to introduce noise in the summary in a controlled and quantifiable way. We estimate that the utilization of such data can extract and highlight useful attributes of summary quality estimation, aiding the creation of data-driven automatic methods with an increased correlation to human summary evaluations across domains and languages. This paper provides a brief description of the summary evaluation task, the data generation protocol and the resources made available by the MultiLing community, towards improving automatic summarization evaluation.</abstract>
      <url hash="71963d11">W19-8903</url>
      <doi>10.26615/978-954-452-058-8_003</doi>
      <bibkey>giannakopoulos-pittaras-2019-summary</bibkey>
    </paper>
    <paper id="4">
      <title>Multi-lingual <fixed-case>W</fixed-case>ikipedia Summarization and Title Generation On Low Resource Corpus</title>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Zuying</first><last>Huang</last></author>
      <author><first>Yinan</first><last>Liu</last></author>
      <pages>17–25</pages>
      <abstract>MultiLing 2019 Headline Generation Task on Wikipedia Corpus raised a critical and practical problem: multilingual task on low resource corpus. In this paper we proposed QDAS extractive summarization model enhanced by sentence2vec and try to apply transfer learning based on large multilingual pre-trained language model for Wikipedia Headline Generation task. We treat it as sequence labeling task and develop two schemes to handle with it. Experimental results have shown that large pre-trained model can effectively utilize learned knowledge to extract certain phrase using low resource supervised data.</abstract>
      <url hash="2c755b3a">W19-8904</url>
      <doi>10.26615/978-954-452-058-8_004</doi>
      <bibkey>liu-etal-2019-multi-lingual</bibkey>
    </paper>
    <paper id="5">
      <title>A topic-based sentence representation for extractive text summarization</title>
      <author><first>Nikolaos</first><last>Gialitsis</last></author>
      <author><first>Nikiforos</first><last>Pittaras</last></author>
      <author><first>Panagiotis</first><last>Stamatopoulos</last></author>
      <pages>26–34</pages>
      <abstract>In this study, we examine the effect of probabilistic topic model-based word representations, on sentence-based extractive summarization. We formulate the task of summary extraction as a binary classification problem, and we test a variety of machine learning algorithms, exploring a range of different settings. An wide experimental evaluation on the MultiLing 2015 MSS dataset illustrates that topic-based representations can prove beneficial to the extractive summarization process in terms of F1, ROUGE-L and ROUGE-W scores, compared to a TF-IDF baseline, with QDA-based analysis providing the best results.</abstract>
      <url hash="8a711cdb">W19-8905</url>
      <doi>10.26615/978-954-452-058-8_005</doi>
      <bibkey>gialitsis-etal-2019-topic</bibkey>
    </paper>
    <paper id="6">
      <title>A Study on Game Review Summarization</title>
      <author><first>George</first><last>Panagiotopoulos</last></author>
      <author><first>George</first><last>Giannakopoulos</last></author>
      <author><first>Antonios</first><last>Liapis</last></author>
      <pages>35–43</pages>
      <abstract>Game reviews have constituted a unique means of interaction between players and companies for many years. The dynamics appearing through online publishing have significantly grown the number of comments per game, giving rise to very interesting communities. The growth has, in turn, led to a difficulty in dealing with the volume and varying quality of the comments as a source of information. This work studies whether and how game reviews can be summarized, based on the notions pre-existing in aspect-based summarization and sentiment analysis. The work provides suggested pipeline of analysis, also offering preliminary findings on whether aspects detected in a set of comments can be consistently evaluated by human users.</abstract>
      <url hash="cdc594f2">W19-8906</url>
      <doi>10.26615/978-954-452-058-8_006</doi>
      <bibkey>panagiotopoulos-etal-2019-study</bibkey>
    </paper>
    <paper id="7">
      <title>Social Web Observatory: An entity-driven, holistic information summarization platform across sources</title>
      <author><first>Leonidas</first><last>Tsekouras</last></author>
      <author><first>Georgios</first><last>Petasis</last></author>
      <author><first>Aris</first><last>Kosmopoulos</last></author>
      <pages>44–52</pages>
      <abstract>The Social Web Observatory is an entity-driven, sentiment-aware, event summarization web platform, combining various methods and tools to overview trends across social media and news sources in Greek. SWO crawls, clusters and summarizes information following an entity-centric view of text streams, allowing to monitor the public sentiment towards a specific person, organization or other entity. In this paper, we overview the platform, outline the analysis pipeline and describe a user study aimed to quantify the usefulness of the system and especially the meaningfulness and coherence of discovered events.</abstract>
      <url hash="f395e6e2">W19-8907</url>
      <doi>10.26615/978-954-452-058-8_007</doi>
      <bibkey>tsekouras-etal-2019-social</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>EASY</fixed-case>-<fixed-case>M</fixed-case>: Evaluation System for Multilingual Summarizers</title>
      <pages>53–62</pages>
      <abstract>Automatic text summarization aims at producing a shorter version of a document (or a document set). Evaluation of summarization quality is a challenging task. Because human evaluations are expensive and evaluators often disagree between themselves, many researchers prefer to evaluate their systems automatically, with help of software tools. Such a tool usually requires a point of reference in the form of one or more human-written summaries for each text in the corpus. Then, a system-generated summary is compared to one or more human-written summaries, according to selected metrics. However, a single metric cannot reflect all quality-related aspects of a summary. In this paper we present the EvAluation SYstem for Multilingual Summarization (EASY-M), which enables the evaluation of system-generated summaries in 17 different languages with several quality measures, based on comparison with their human-generated counterparts. The system also provides comparative results with two built-in baselines. The source code and both online and offline versions of EASY-M is freely available for the NLP community.</abstract>
      <url hash="0682a761">W19-8908</url>
      <doi>10.26615/978-954-452-058-8_008</doi>
      <bibkey>nn-2019-easy</bibkey>
    </paper>
    <paper id="9">
      <title>A study of semantic augmentation of word embeddings for extractive summarization</title>
      <author><first>Nikiforos</first><last>Pittaras</last></author>
      <author><first>Vangelis</first><last>Karkaletsis</last></author>
      <pages>63–72</pages>
      <abstract>In this study we examine the effect of semantic augmentation approaches on extractive text summarization. Wordnet hypernym relations are used to extract term-frequency concept information, subsequently concatenated to sentence-level representations produced by aggregated deep neural word embeddings. Multiple dimensionality reduction techniques and combination strategies are examined via feature transformation and clustering methods. An experimental evaluation on the MultiLing 2015 MSS dataset illustrates that semantic information can introduce benefits to the extractive summarization process in terms of F1, ROUGE-1 and ROUGE-2 scores, with LSA-based post-processing introducing the largest improvements.</abstract>
      <url hash="b45f6e1c">W19-8909</url>
      <doi>10.26615/978-954-452-058-8_009</doi>
      <bibkey>pittaras-karkaletsis-2019-study</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>HE</fixed-case>v<fixed-case>AS</fixed-case>: Headline Evaluation and Analysis System</title>
      <author><first>Marina</first><last>Litvak</last></author>
      <author><first>Natalia</first><last>Vanetik</last></author>
      <author><first>Itzhak</first><last>Eretz Kdosha</last></author>
      <pages>73–80</pages>
      <abstract>Automatic headline generation is a subtask of one-line summarization with many reported applications. Evaluation of systems generating headlines is a very challenging and undeveloped area. We introduce the Headline Evaluation and Analysis System (HEvAS) that performs automatic evaluation of systems in terms of a quality of the generated headlines. HEvAS provides two types of metrics– one which measures the informativeness of a headline, and another that measures its readability. The results of evaluation can be compared to the results of baseline methods which are implemented in HEvAS. The system also performs the statistical analysis of the evaluation results and provides different visualization charts. This paper describes all evaluation metrics, baselines, analysis, and architecture, utilized by our system.</abstract>
      <url hash="bdde845a">W19-8910</url>
      <doi>10.26615/978-954-452-058-8_010</doi>
      <bibkey>litvak-etal-2019-hevas</bibkey>
    </paper>
  </volume>
  <volume id="90" ingest-date="2020-01-15">
    <meta>
      <booktitle>Proceedings of the Workshop on Language Technology for Digital Historical Archives</booktitle>
      <url hash="cc3caaca">W19-90</url>
      <editor><first>Cristina</first><last>Vertan</last><affiliation>University of Hamburg</affiliation></editor>
      <editor><first>Petya</first><last>Osenova</last><affiliation>Bulgarian Academy of Sciences and St. Kliment Ohridski University of Sofia</affiliation></editor>
      <editor><first>Dimitar</first><last>Iliev</last><affiliation>St. Kliment Ohridski University of Sofia</affiliation></editor>
      <publisher>INCOMA Ltd.</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2019</year>
      <venue>ranlp</venue>
    </meta>
    <frontmatter>
      <url hash="f01ef581">W19-9000</url>
      <bibkey>ws-2019-language</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Graphemic ambiguous queries on <fixed-case>A</fixed-case>rabic-scripted historical corpora</title>
      <author><first>Alicia</first><last>González Martínez</last></author>
      <pages>1–2</pages>
      <abstract/>
      <url hash="9f258738">W19-9001</url>
      <doi>10.26615/978-954-452-059-5_001</doi>
      <bibkey>gonzalez-martinez-2019-graphemic</bibkey>
    </paper>
    <paper id="2">
      <title>Word Clustering for Historical Newspapers Analysis</title>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Jani</first><last>Marjanen</last></author>
      <pages>3–10</pages>
      <abstract>This paper is a part of a collaboration between computer scientists and historians aimed at development of novel tools and methods to improve analysis of historical newspapers. We present a case study of ideological terms ending with -ism suffix in nineteenth century Finnish newspapers. We propose a two-step procedure to trace differences in word usages over time: training of diachronic embeddings on several time slices and when clustering embeddings of selected words together with their neighbours to obtain historical context. The obtained clusters turn out to be useful for historical studies. The paper also discuss specific difficulties related to development historian-oriented tools.</abstract>
      <url hash="705c5e66">W19-9002</url>
      <doi>10.26615/978-954-452-059-5_002</doi>
      <bibkey>pivovarova-etal-2019-word</bibkey>
    </paper>
    <paper id="3">
      <title>Geotagging a Diachronic Corpus of Alpine Texts: Comparing Distinct Approaches to Toponym Recognition</title>
      <author><first>Tannon</first><last>Kew</last></author>
      <author><first>Anastassia</first><last>Shaitarova</last></author>
      <author><first>Isabel</first><last>Meraner</last></author>
      <author><first>Janis</first><last>Goldzycher</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>11–18</pages>
      <abstract>Geotagging historic and cultural texts provides valuable access to heritage data, enabling location-based searching and new geographically related discoveries. In this paper, we describe two distinct approaches to geotagging a variety of fine-grained toponyms in a diachronic corpus of alpine texts. By applying a traditional gazetteer-based approach, aided by a few simple heuristics, we attain strong high-precision annotations. Using the output of this earlier system, we adopt a state-of-the-art neural approach in order to facilitate the detection of new toponyms on the basis of context. Additionally, we present the results of preliminary experiments on integrating a small amount of crowdsourced annotations to improve overall performance of toponym recognition in our heritage corpus.</abstract>
      <url hash="3a4cb241">W19-9003</url>
      <doi>10.26615/978-954-452-059-5_003</doi>
      <bibkey>kew-etal-2019-geotagging</bibkey>
    </paper>
    <paper id="4">
      <title>Controlled Semi-automatic Annotation of Classical Ethiopic</title>
      <author><first>Cristina</first><last>Vertan</last></author>
      <pages>19–23</pages>
      <abstract>Preservation of the cultural heritage by means of digital methods became extremely popular during last years. After intensive digitization campaigns the focus moves slowly from the genuine preservation (i.e digital archiving together with standard search mechanisms) to research-oriented usage of materials available electronically. This usage is intended to go far beyond simple reading of digitized materials; researchers should be able to gain new insigts in materials, discover new facts by means of tools relying on innovative algorithms. In this article we will describe the workflow necessary for the annotation of a dichronic corpus of classical Ethiopic, language of essential importance for the study of Early Christianity</abstract>
      <url hash="e82471e4">W19-9004</url>
      <doi>10.26615/978-954-452-059-5_004</doi>
      <bibkey>vertan-2019-controlled</bibkey>
    </paper>
    <paper id="5">
      <title>Implementing an archival, multilingual and Semantic Web-compliant taxonomy by means of <fixed-case>SKOS</fixed-case> (Simple Knowledge Organization System)</title>
      <author><first>Francesco</first><last>Gelati</last></author>
      <pages>24–27</pages>
      <abstract>The paper shows how a multilingual hierarchical thesaurus, or taxonomy, can be created and implemented in compliance with Semantic Web requirements by means of the data model SKOS (Simple Knowledge Organization System). It takes the EHRI (European Holocaust Research Infrastructure) portal as an example, and shows how open-source software like SKOS Play! can facilitate the task.</abstract>
      <url hash="b4b4f367">W19-9005</url>
      <doi>10.26615/978-954-452-059-5_005</doi>
      <bibkey>gelati-2019-implementing</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>EU</fixed-case> 4 <fixed-case>U</fixed-case>: An educational platform for the cultural heritage of the <fixed-case>EU</fixed-case></title>
      <author><first>Maria</first><last>Stambolieva</last></author>
      <pages>28–33</pages>
      <abstract>The paper presents an ongoing project of the NBU Laboratory for Language Technology aiming to create a multilingual, CEFR-graded electronic didactic resource for online learning, centered on the history and cultural heritage of the EU (e-EULearn). The resource is developed within the e-Platform of the NBU Laboratory for Language Technology and re-uses the rich corpus of educational material created at the Laboratory for the needs of NBU program modules, distance and blended learning language courses and other projects. Focus being not just on foreign language tuition, but above all on people, places and events in the history and culture of the EU member states, the annotation modules of the e-Platform have been accordingly extended. Current and upcoming activities are directed at: 1/ enriching the English corpus of didactic materials on EU history and culture, 2/ translating the texts into (the) other official EU languages and aligning the translations with the English texts; 3/ developing new test modules. In the process of developing this resource, a database on important people, places, objects and events in the cultural history of the EU will be created.</abstract>
      <url hash="aebd339b">W19-9006</url>
      <doi>10.26615/978-954-452-059-5_006</doi>
      <bibkey>stambolieva-2019-eu</bibkey>
    </paper>
    <paper id="7">
      <title>Modelling linguistic vagueness and uncertainty in historical texts</title>
      <author><first>Cristina</first><last>Vertan</last></author>
      <pages>34–38</pages>
      <abstract>Many applications in Digital Humanities (DH) rely on annotations of the raw material. These annotations (inferred automatically or done manually) assume that labelled facts are either true or false, thus all inferences started on such annotations us boolean logic. This contradicts hermeneutic principles used by humanites in which most part of the knowledge has a degree of truth which varies depending on the experience and the world knowledge of the interpreter. In this paper we will show how uncertainty and vagueness, two main features of any historical text can be encoded in annotations and thus be considered by DH applications.</abstract>
      <url hash="fecaaa61">W19-9007</url>
      <doi>10.26615/978-954-452-059-5_007</doi>
      <bibkey>vertan-2019-modelling</bibkey>
    </paper>
  </volume>
</collection>
