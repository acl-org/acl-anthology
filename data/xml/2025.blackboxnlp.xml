<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.blackboxnlp">
  <volume id="1" ingest-date="2025-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 8th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP</booktitle>
      <editor><first>Yonatan</first><last>Belinkov</last></editor>
      <editor><first>Aaron</first><last>Mueller</last></editor>
      <editor><first>Najoung</first><last>Kim</last></editor>
      <editor><first>Hosein</first><last>Mohebbi</last></editor>
      <editor><first>Hanjie</first><last>Chen</last></editor>
      <editor><first>Dana</first><last>Arad</last></editor>
      <editor><first>Gabriele</first><last>Sarti</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>November</month>
      <year>2025</year>
      <url hash="8d562767">2025.blackboxnlp-1</url>
      <venue>blackboxnlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-346-3</isbn>
    </meta>
    <frontmatter>
      <url hash="a140525a">2025.blackboxnlp-1.0</url>
      <bibkey>blackboxnlp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>CE</fixed-case>-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders</title>
      <author orcid="0009-0005-1549-7130"><first>Alex</first><last>Gulko</last></author>
      <author><first>Yusen</first><last>Peng</last></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>1-15</pages>
      <abstract>Sparse autoencoders (SAEs) are a promising approach for uncovering interpretable features in large language models (LLMs). While several automated evaluation methods exist for SAEs, most rely on external LLMs. In this work, we introduce CE-Bench, a novel and lightweight contrastive evaluation benchmark for sparse autoencoders, built on a curated dataset of contrastive story pairs. We conduct comprehensive evaluation studies to validate the effectiveness of our approach. Our results show that CE-Bench reliably measures the interpretability of sparse autoencoders and aligns well with existing benchmarks without requiring an external LLM judge, achieving over 70% Spearman correlation with results in SAEBench. The official implementation and evaluation dataset are open-sourced and publicly available.</abstract>
      <url hash="5b03ab07">2025.blackboxnlp-1.1</url>
      <bibkey>gulko-etal-2025-ce</bibkey>
    </paper>
    <paper id="2">
      <title>Char-mander Use m<fixed-case>B</fixed-case>ackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author orcid="0000-0001-6389-576X"><first>Himanshu</first><last>Beniwal</last></author>
      <author><first>Sailesh</first><last>Panda</last></author>
      <author><first>Birudugadda</first><last>Srivibhav</last></author>
      <author orcid="0000-0001-8757-2623" id="mayank-singh"><first>Mayank</first><last>Singh</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <pages>16-47</pages>
      <abstract>We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare and high-occurring tokens serving as specific, effective triggers. Our findings reveal a critical vulnerability that affects the model’s architecture, leading to a concealed backdoor effect during the information flow. Our code and data are publicly available at https://github.com/himanshubeniwal/X-BAT.</abstract>
      <url hash="3fbbb8b6">2025.blackboxnlp-1.2</url>
      <bibkey>beniwal-etal-2025-char</bibkey>
    </paper>
    <paper id="3">
      <title>Evil twins are not that evil: Qualitative insights into machine-generated prompts</title>
      <author orcid="0000-0003-0181-2644"><first>Nathanaël Carraz</first><last>Rakotonirina</last></author>
      <author><first>Corentin</first><last>Kervadec</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author orcid="0000-0003-0503-2792"><first>Francesca</first><last>Franzon</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Marco</first><last>Baroni</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <pages>48-68</pages>
      <abstract>It has been widely observed that language models (LMs) respond in predictable ways to algorithmically generated prompts that are seemingly unintelligible. This is both a sign that we lack a full understanding of how LMs work, and a practical challenge, because opaqueness can be exploited for harmful uses of LMs, such as jailbreaking. We present the first thorough analysis of opaque machine-generated prompts, or autoprompts, pertaining to 6 LMs of different sizes and families. We find that machine-generated prompts are characterized by a last token that is often intelligible and strongly affects the generation. A small but consistent proportion of the previous tokens are prunable, probably appearing in the prompt as a by-product of the fact that the optimization process fixes the number of tokens. The remaining tokens fall into two categories: filler tokens, which can be replaced with semantically unrelated substitutes, and keywords, that tend to have at least a loose semantic relation with the generation, although they do not engage in well-formed syntactic relations with it. Additionally, human experts can reliably identify the most influential tokens in an autoprompt a posteriori, suggesting these prompts are not entirely opaque. Finally, some of the ablations we applied to autoprompts yield similar effects in natural language inputs, suggesting that autoprompts emerge naturally from the way LMs process linguistic inputs in general.</abstract>
      <url hash="2fbd6b50">2025.blackboxnlp-1.3</url>
      <bibkey>rakotonirina-etal-2025-evil</bibkey>
    </paper>
    <paper id="4">
      <title>Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2</title>
      <author><first>Stefan</first><last>Arnold</last></author>
      <author><first>Rene</first><last>Gröbner</last></author>
      <pages>69-78</pages>
      <abstract>Language Models, when generating prepositional phrases, must often decide for whether their complements functions as an instrumental adjunct (describing the verb adverbially) or an attributive modifier (enriching the noun adjectivally), yet the internal mechanisms that resolve this split decision remain poorly understood. In this study, we conduct a targeted investigation into Gemma-2 to uncover and control the generation of prepositional complements. We assemble a prompt suite containing with-headed prepositional phrases whose contexts equally accommodate either an instrumental or attributive continuation, revealing a strong preference for an instrumental reading at a ratio of 3:4. To pinpoint individual attention heads that favor instrumental over attributive complements, we project activations into the vocabulary space. By scaling the value vector of a single attention head, we can shift the distribution of functional roles of complements, attenuating instruments to 33% while elevating attributes to 36%.</abstract>
      <url hash="b02db2fe">2025.blackboxnlp-1.4</url>
      <bibkey>arnold-grobner-2025-steering</bibkey>
    </paper>
    <paper id="5">
      <title>The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of <fixed-case>LLM</fixed-case> Evaluators</title>
      <author><first>Hawon</first><last>Jeong</last></author>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Jimin</first><last>Hong</last><affiliation>Krafton.Inc and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Hojoon</first><last>Lee</last></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>79-108</pages>
      <abstract>As large language models (LLMs) are increasingly used as evaluators for natural language generation tasks, ensuring unbiased assessments is essential. However, LLM evaluators often display biased preferences, such as favoring verbosity and authoritative tones.Our empirical analysis reveals that these biases are exacerbated in pairwise evaluation, where LLMs directly compare two outputs and easily prioritize superficial attributes. In contrast, pointwise evaluation, which assesses outputs independently, is less susceptible to such bias because each output is judged in isolation. To address the limitations of the pairwise evaluation, we introduce a novel evaluation method, PRePair, which integrates pointwise reasoning within a pairwise framework. PRePair effectively alleviates biased preference, improving performance on the adversarial benchmark (LLMBar) while outperforming pointwise evaluation on the standard benchmark (MT-Bench).</abstract>
      <url hash="20781741">2025.blackboxnlp-1.5</url>
      <bibkey>jeong-etal-2025-comparative</bibkey>
    </paper>
    <paper id="6">
      <title>Not a nuisance but a useful heuristic: Outlier dimensions favor frequent tokens in language models</title>
      <author orcid="0000-0001-5701-7725"><first>Iuri</first><last>Macocco</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Nora</first><last>Graichen</last></author>
      <author orcid="0000-0001-6140-7080"><first>Gemma</first><last>Boleda</last><affiliation>ICREA and Universitat Pompeu Fabra</affiliation></author>
      <author><first>Marco</first><last>Baroni</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <pages>109-136</pages>
      <abstract>We study last-layer outlier dimensions, i.e. dimensions that display extreme activations for the majority of inputs. We show that outlier dimensions arise in many different modern language models, and trace their function back to the heuristic of constantly predicting frequent words. We further show how a model can block this heuristic when it is not contextually appropriate, by assigning a counterbalancing weight mass to the remaining dimensions, and we investigate which model parameters boost outlier dimensions and when they arise during training. We conclude that outlier dimensions are a specialized mechanism discovered by many distinct models to implement a useful token prediction heuristic.</abstract>
      <url hash="4e53f490">2025.blackboxnlp-1.6</url>
      <bibkey>macocco-etal-2025-nuisance</bibkey>
    </paper>
    <paper id="7">
      <title>Language Dominance in Multilingual Large Language Models</title>
      <author><first>Nadav</first><last>Shani</last></author>
      <author orcid="0000-0002-4718-886X"><first>Ali</first><last>Basirat</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>137-148</pages>
      <abstract>This paper investigates the language dominance hypothesis in multilingual large language models (LLMs), which posits that cross-lingual understanding is facilitated by an implicit translation into a dominant language seen more frequently during pretraining. We propose a novel approach to quantify how languages influence one another in a language model. By analyzing the hidden states across intermediate layers of language models, we model interactions between language-specific embedding spaces using Gaussian Mixture Models. Our results reveal only weak signs of language dominance in middle layers, affecting only a fraction of tokens. Our findings suggest that multilingual processing in LLMs is better explained by language-specific and shared representational spaces rather than internal translation into a single dominant language.</abstract>
      <url hash="bd454644">2025.blackboxnlp-1.7</url>
      <bibkey>shani-basirat-2025-language</bibkey>
    </paper>
    <paper id="8">
      <title>Interpreting Language Models Through Concept Descriptions: A Survey</title>
      <author orcid="0009-0008-7408-7483"><first>Nils</first><last>Feldhus</last></author>
      <author orcid="0009-0005-4280-3818"><first>Laura</first><last>Kopf</last><affiliation>Technische Universität Berlin</affiliation></author>
      <pages>149-162</pages>
      <abstract>Understanding the decision-making processes of neural networks is a central goal of mechanistic interpretability. In the context of Large Language Models (LLMs), this involves uncovering the underlying mechanisms and identifying the roles of individual model components such as neurons and attention heads, as well as model abstractions such as the learned sparse features extracted by Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this challenge by using powerful generator models to produce open-vocabulary, natural language concept descriptions for these components. In this paper, we provide the first survey of the emerging field of concept descriptions for model components and abstractions. We chart the key methods for generating these descriptions, the evolving landscape of automated and human metrics for evaluating them, and the datasets that underpin this research. Our synthesis reveals a growing demand for more rigorous, causal evaluation. By outlining the state of the art and identifying key challenges, this survey provides a roadmap for future research toward making models more transparent.</abstract>
      <url hash="cffb5bd1">2025.blackboxnlp-1.8</url>
      <bibkey>feldhus-kopf-2025-interpreting</bibkey>
    </paper>
    <paper id="9">
      <title>Investigating <fixed-case>R</fixed-case>e<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Effects on the Learning Dynamics of Small Language Models</title>
      <author><first>Yuval</first><last>Weiss</last><affiliation>University of Cambridge</affiliation></author>
      <author orcid="0009-0005-3543-8891"><first>David Demitri</first><last>Africa</last><affiliation>UK AI Security Institute</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Richard</first><last>Diehl Martinez</last></author>
      <pages>163-175</pages>
      <abstract>Parameter-efficient methods like LoRA have revolutionised large language model (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly merging and reinitialising low-rank adapters, increasing cumulative rank while keeping updates cheap. This aligns well with observations that high-capacity models learn through locally low-rank trajectories that expand over time. By contrast, recent work suggests that small language models (SLMs) exhibit rank deficiencies and under-utilise their available dimensionality. This raises a natural question: can ReLoRA’s rank-expanding update rule steer SLMs toward healthier learning dynamics, mitigating rank bottlenecks in a capacity-constrained regime? We argue SLMs are an ideal testbed: they train quickly, enable controlled ablations, and make rank phenomena more measurable. We present the first systematic study of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and learning dynamics. Across loss, Paloma perplexity, and BLiMP, we find that ReLoRA underperforms full-rank training, with gaps widening at larger scales. Analysis of proportional effective rank and condition numbers shows that ReLoRA amplifies existing rank deficiencies and induces ill-conditioned updates early in training. Our results suggest that while ReLoRA’s merge-and-restart strategy can expand ranks in larger models, it does not straightforwardly translate to capacity-limited SLMs, motivating adaptive-rank or hybrid-rank approaches for low-compute pretraining.</abstract>
      <url hash="8ecdd853">2025.blackboxnlp-1.9</url>
      <bibkey>weiss-etal-2025-investigating</bibkey>
    </paper>
    <paper id="10">
      <title>When <fixed-case>LRP</fixed-case> Diverges from Leave-One-Out in Transformers</title>
      <author><first>Weiqiu</first><last>You</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author orcid="0009-0008-2042-0754"><first>Siqi</first><last>Zeng</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Yao-Hung Hubert</first><last>Tsai</last><affiliation>Apple</affiliation></author>
      <author><first>Makoto</first><last>Yamada</last><affiliation>Okinawa Institute of Science and Technology (OIST)</affiliation></author>
      <author orcid="0000-0002-8579-1600"><first>Han</first><last>Zhao</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>176-188</pages>
      <abstract>Leave-One-Out (LOO) provides an intuitive measure of feature importance but is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP) offers a potentially efficient alternative, its axiomatic soundness in modern Transformers remains under-examined. In this work, we first show that the bilinear propagation rules used in recent advances of AttnLRP violate implementation invariance. We prove this analytically and confirm it empirically in linear attention layers. Second, we also revisit CP-LRP as a diagnostic baseline and find that bypassing relevance propagation through the softmax layer—back-propagating relevance only through the value matrices—significantly improves alignment with LOO, particularly in the middle-to-late Transformer layers. Overall, our results suggest that (i) bilinear factorization sensitivity and (ii) softmax propagation error potentially jointly undermine LRP’s ability to approximate LOO in Transformers.</abstract>
      <url hash="a7c9b098">2025.blackboxnlp-1.10</url>
      <bibkey>you-etal-2025-lrp</bibkey>
    </paper>
    <paper id="11">
      <title>Understanding the Side Effects of Rank-One Knowledge Editing</title>
      <author id="ryosuke-takahashi"><first>Ryosuke</first><last>Takahashi</last><affiliation>Tohoku University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Go</first><last>Kamoda</last><affiliation>Graduate University for Advanced Studies and National Institute for Japanese Language and Linguistics</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author orcid="0000-0001-6510-604X"><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>189-205</pages>
      <abstract>This study conducts a detailed analysis of the side effects of rank-one knowledge editing using language models with controlled knowledge. The analysis focuses on each element of knowledge triples (subject, relation, object) and examines two aspects: “knowledge that causes large side effects when edited” and “knowledge that is affected by the side effects.” Our findings suggest that editing knowledge with subjects that have relationships with numerous objects or are robustly embedded within the LM may trigger extensive side effects. Furthermore, we demonstrate that the similarity between relation vectors, the density of object vectors, and the distortion of knowledge representations are closely related to how susceptible knowledge is to editing influences. The findings of this research provide new insights into the mechanisms of side effects in LM knowledge editing and indicate specific directions for developing more effective and reliable knowledge editing methods.</abstract>
      <url hash="fe2ba8f6">2025.blackboxnlp-1.11</url>
      <bibkey>takahashi-etal-2025-understanding</bibkey>
    </paper>
    <paper id="12">
      <title>Emergent Convergence in Multi-Agent <fixed-case>LLM</fixed-case> Annotation</title>
      <author orcid="0000-0002-5245-5571"><first>Angelina</first><last>Parfenova</last></author>
      <author><first>Alexander</first><last>Denzler</last><affiliation>HSLU - Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author orcid="0000-0002-1677-150X"><first>Jürgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <pages>206-225</pages>
      <abstract>Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7,500 multi-agent, multi-round discussions in an inductive coding task, generating over 125,000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics—code stability, semantic self-consistency, and lexical confidence—alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.</abstract>
      <url hash="db7a0097">2025.blackboxnlp-1.12</url>
      <bibkey>parfenova-etal-2025-emergent</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>P</fixed-case>rivacy<fixed-case>S</fixed-case>calpel: Enhancing <fixed-case>LLM</fixed-case> Privacy via Interpretable Feature Intervention with Sparse Autoencoders</title>
      <author><first>Ahmed</first><last>Frikha</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Muhammad Reza Ar</first><last>Razi</last></author>
      <author><first>Krishna Kanth</first><last>Nakka</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ricardo</first><last>Mendes</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xue</first><last>Jiang</last></author>
      <author><first>Xuebing</first><last>Zhou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>226-238</pages>
      <abstract>Large Language Models (LLMs) achieve impressive natural language processing performance but can memorize and leak Personally Identifiable Information (PII), posing serious privacy risks. Existing mitigation strategies—such as differential privacy and neuron-level interventions—often degrade utility or fail to reliably prevent leakage. We present PrivacyScalpel, a privacy-preserving framework that leverages LLM interpretability to identify and suppress PII leakage while preserving performance. PrivacyScalpel operates in three stages: (1) Feature Probing to locate model layers encoding PII-rich representations; (2) Sparse Autoencoding using a k-Sparse Autoencoder (k-SAE) to disentangle and isolate privacy-sensitive features; and (3) Feature-Level Interventions via targeted ablation and vector steering to reduce leakage. Experiments on Gemma2-2B and Llama2-7B fine-tuned with the Enron dataset show that PrivacyScalpel reduces email leakage from 5.15% to 0.0% while retaining over 99.4% of the original utility. Compared to neuron-level methods, our approach achieves a superior privacy–utility trade-off, highlighting the effectiveness of targeting sparse, monosemantic features over polysemantic neurons. Beyond privacy gains, PrivacyScalpel offers interpretability insights into PII memorization mechanisms, contributing to safer and more transparent LLM deployment.</abstract>
      <url hash="beb41bc9">2025.blackboxnlp-1.13</url>
      <bibkey>frikha-etal-2025-privacyscalpel</bibkey>
    </paper>
    <paper id="14">
      <title>Circuit-Tracer: A New Library for Finding Feature Circuits</title>
      <author><first>Michael</first><last>Hanna</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mateusz</first><last>Piotrowski</last></author>
      <author><first>Jack</first><last>Lindsey</last></author>
      <author><first>Emmanuel</first><last>Ameisen</last><affiliation>Anthropic</affiliation></author>
      <pages>239-249</pages>
      <abstract>Feature circuits aim to shed light on LLM behavior by identifying the features that are causally responsible for a given LLM output, and connecting them into a directed graph, or *circuit*, that explains how both each feature and each output arose. However, performing circuit analysis is challenging: the tools for finding, visualizing, and verifying feature circuits are complex and spread across multiple libraries.To facilitate feature-circuit finding, we introduce ‘circuit-tracer‘, an open-source library for efficient identification of feature circuits. ‘circuit-tracer‘ provides an integrated pipeline for finding, visualizing, annotating, and performing interventions on such feature circuits, tested with various model sizes, up to 14B parameters. We make ‘circuit-tracer‘ available to both developers and end users, via integration with tools such as Neuronpedia, which provides a user-friendly interface.</abstract>
      <url hash="27a57a60">2025.blackboxnlp-1.14</url>
      <bibkey>hanna-etal-2025-circuit</bibkey>
    </paper>
    <paper id="15">
      <title>The Lookahead Limitation: Why Multi-Operand Addition is Hard for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Tanja</first><last>Baeumel</last></author>
      <author><first>Josef Van</first><last>Genabith</last></author>
      <author orcid="0000-0002-0899-0657"><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>250-262</pages>
      <abstract>Autoregressive large language models (LLMs) exhibit impressive performance across various tasks but struggle with simple arithmetic, such as additions of two or more operands. We show that this struggle arises from LLMs’ use of a simple one-digit lookahead heuristic, which forms an upper bound for LLM performance and accounts for characteristic error patterns in two-operand addition and failure in multi-operand addition, where the carry-over logic is more complex. Our probing experiments and digit-wise accuracy evaluation show that the evaluated LLMs fail precisely where a one-digit lookahead is insufficient to account for cascading carries. We analyze the impact of tokenization strategies on arithmetic performance and show that all investigated models, regardless of tokenization and size, are inherently limited in the addition of multiple operands due to their reliance on a one-digit lookahead heuristic. Our findings reveal limitations that prevent LLMs from generalizing to more complex numerical reasoning.</abstract>
      <url hash="317f4b12">2025.blackboxnlp-1.15</url>
      <bibkey>baeumel-etal-2025-lookahead</bibkey>
    </paper>
    <paper id="16">
      <title>Can <fixed-case>LLM</fixed-case>s Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference</title>
      <author><first>Dang Thi Thao</first><last>Anh</last></author>
      <author><first>Rick</first><last>Nouwen</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>263-275</pages>
      <abstract>Our goal is to study how LLMs represent and interpret plural reference in ambiguous and unambiguous contexts. We ask the following research questions: (1) Do LLMs exhibit human-like preferences in representing plural reference? and (2) Are LLMs able to detect ambiguity in plural anaphoric expressions and identify possible referents? To address these questions, we design a set of experiments, examining pronoun production using next-token prediction tasks, pronoun interpretation, and ambiguity detection using different prompting strategies. We then assess how comparable LLMs are to humans in formulating and interpreting plural reference. We find that LLMs are sometimes aware of possible referents of ambiguous pronouns. However, they do not always follow human reference when choosing between interpretations, especially when the possible interpretation is not explicitly mentioned. In addition, they struggle to identify ambiguity without direct instruction. Our findings also reveal inconsistencies in the results across different types of experiments.</abstract>
      <url hash="8f537ecd">2025.blackboxnlp-1.16</url>
      <bibkey>anh-etal-2025-llms</bibkey>
    </paper>
    <paper id="17">
      <title>Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives</title>
      <author><first>Kentaro</first><last>Ozeki</last><affiliation>The University of Tokyo and Keio University</affiliation></author>
      <author orcid="0009-0001-1517-2572"><first>Risako</first><last>Ando</last><affiliation>Keio University</affiliation></author>
      <author><first>Takanobu</first><last>Morishita</last><affiliation>Keio University</affiliation></author>
      <author orcid="0009-0008-5415-3367"><first>Hirohiko</first><last>Abe</last><affiliation>Keio University</affiliation></author>
      <author orcid="0000-0003-4033-3216"><first>Koji</first><last>Mineshima</last><affiliation>Keio University</affiliation></author>
      <author><first>Mitsuhiro</first><last>Okada</last></author>
      <pages>276-294</pages>
      <abstract>Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs’ reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs’ normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.</abstract>
      <url hash="cd8277b5">2025.blackboxnlp-1.17</url>
      <bibkey>ozeki-etal-2025-normative</bibkey>
    </paper>
    <paper id="18">
      <title>A Theorem-Proving-Based Evaluation of Neural Semantic Parsing</title>
      <author><first>Hayate</first><last>Funakura</last></author>
      <author><first>Hyunsoo</first><last>Kim</last></author>
      <author orcid="0000-0003-4033-3216"><first>Koji</first><last>Mineshima</last><affiliation>Keio University</affiliation></author>
      <pages>295-306</pages>
      <abstract>Graph-matching metrics such as Smatch are the de facto standard for evaluating neural semantic parsers, yet they capture surface overlap rather than logical equivalence. We reassess evaluation by pairing graph-matching with automated theorem proving. We compare two approaches to building parsers: supervised fine-tuning (T5-Small/Base) and few-shot in-context learning (GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs using graph-matching, bidirectional entailment between source and target formulas with a first-order logic theorem prover, and well-formedness. Across settings, we find that models performing well on graph-matching often fail to produce logically equivalent formulas. Normalization reduces incidental target variability, improves well-formedness, and strengthens logical adequacy. Error analysis shows performance degrades with increasing formula complexity and with coordination, prepositional phrases, and passive voice; the dominant failures involve variable binding and indexing, and predicate naming. These findings highlight limits of graph-based metrics for reasoning-oriented applications and motivate logic-sensitive evaluation and training objectives together with simplified, normalized target representations. All data and code will be publicly released.</abstract>
      <url hash="4e7d54b7">2025.blackboxnlp-1.18</url>
      <bibkey>funakura-etal-2025-theorem</bibkey>
    </paper>
    <paper id="19">
      <title>A Pipeline to Assess Merging Methods via Behavior and Internals</title>
      <author><first>Yutaro</first><last>Sigrist</last></author>
      <author><first>Andreas</first><last>Waldis</last><affiliation>Technische Universität Darmstadt and Lucerne University of Applied Sciences and Arts</affiliation></author>
      <pages>307-316</pages>
      <abstract>Merging methods combine the weights of multiple language models (LMs) to leverage their capacities, such as for domain adaptation. While existing studies investigate merged models from a solely behavioral perspective, we offer the first comprehensive view by assessing and connecting their behavior and internals. We present a novel evaluation pipeline that first merges multiple parent LMs, and then evaluates the merged models in comparison to the initial ones based on their behavior on downstream tasks, like MMLU, and the internal encoded linguistic competence.We showcase this pipeline by assessing the merging of instruction fine-tuned with math- and code-adapted LMs from the Qwen2.5 family. Our results show that merging methods impacts behavior and internals differently. While the performance of merged models is typically between that of the two parent models, their encoded information about linguistic phenomena – particularly in morphology and syntax – can surpass the parent models.Moreover, we find weak ranking correlation between this behavior and internal evaluation. With our pipeline and initial results, we emphasize the need for more comprehensive evaluations of model merging methods to gain a faithful understanding of their capabilities and reliability, beyond potential superficial behavioral advances.</abstract>
      <url hash="ff69281f">2025.blackboxnlp-1.19</url>
      <bibkey>sigrist-waldis-2025-pipeline</bibkey>
    </paper>
    <paper id="20">
      <title>From <fixed-case>BERT</fixed-case> to <fixed-case>LLM</fixed-case>s: Comparing and Understanding <fixed-case>C</fixed-case>hinese Classifier Prediction in Language Models</title>
      <author><first>Ziqi</first><last>Zhang</last></author>
      <author orcid="0009-0003-4114-3884"><first>Jianfei</first><last>Ma</last></author>
      <author orcid="0000-0001-8742-0451"><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>You</first><last>Jieshun</last></author>
      <author><first>Zhaoxin</first><last>Feng</last></author>
      <pages>317-329</pages>
      <abstract>Classifiers are an important and defining feature of the Chinese language, and their correct prediction is key to numerous educational applications. Yet, whether the most popular Large Language Models (LLMs) possess proper knowledge the Chinese classifiers is an issue that has largely remain unexplored in the Natural Language Processing (NLP) literature.To address such a question, we employ various masking strategies to evaluate the LLMs’ intrinsic ability, the contribution of different sentence elements, and the working of the attention mechanisms during prediction. Besides, we explore fine-tuning for LLMs to enhance the classifier performance.Our findings reveal that LLMs perform worse than BERT, even with fine-tuning. The prediction, as expected, greatly benefits from the information about the following noun, which also explains the advantage of models with a bidirectional attention mechanism such as BERT.</abstract>
      <url hash="fe4d9c8f">2025.blackboxnlp-1.20</url>
      <bibkey>zhang-etal-2025-bert</bibkey>
    </paper>
    <paper id="21">
      <title>Mechanistic Fine-tuning for In-context Learning</title>
      <author orcid="0000-0002-7127-1954"><first>Hakaze</first><last>Cho</last></author>
      <author><first>Peng</first><last>Luo</last></author>
      <author><first>Mariko</first><last>Kato</last></author>
      <author><first>Rin</first><last>Kaenbyou</last></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>Ichikara, RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <pages>330-357</pages>
      <abstract>In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.</abstract>
      <url hash="d3f57164">2025.blackboxnlp-1.21</url>
      <bibkey>cho-etal-2025-mechanistic</bibkey>
    </paper>
    <paper id="22">
      <title>Understanding How <fixed-case>C</fixed-case>ode<fixed-case>LLM</fixed-case>s (Mis)Predict Types with Activation Steering</title>
      <author orcid="0009-0002-5837-6097"><first>Francesca</first><last>Lucchetti</last></author>
      <author orcid="0000-0002-7493-3271"><first>Arjun</first><last>Guha</last><affiliation>Northeastern University</affiliation></author>
      <pages>358-397</pages>
      <abstract>Large Language Models (LLMs) are widely used by software engineers for programming tasks. However, research shows that LLMs often lack a deep understanding of program semantics. Even minor changes to syntax, such as renaming variables, can significantly degrade performance across various tasks. In this work, we examine the task of *type prediction*: given a partially typed program, can a model predict a missing type annotations such that the resulting program is more typed? We construct a dataset of adversarial examples where models initially predict the correct types, but begin to fail after semantically irrelevant edits. This is problematic, as models should ideally generalize across different syntactic forms of semantically equivalent code. This lack of robustness suggests that models may have a shallow understanding of code semantics.Despite this, we provide evidence that LLMs do, in fact, learn robust mechanisms for type prediction—though these mechanisms often fail to activate in adversarial scenarios. By using *activation steering*, a method that manipulates a model’s internal activations to guide it toward using latent knowledge, we restore accurate predictions on adversarial inputs. We show that steering successfully activates a type prediction mechanism that is shared by both Python and TypeScript, and is more effective than prompting with in-context examples. Across five different models, our comprehensive evaluation demonstrates that LLMs can learn generalizable representations of code semantics that transfer across programming languages.</abstract>
      <url hash="ba739e23">2025.blackboxnlp-1.22</url>
      <bibkey>lucchetti-guha-2025-understanding</bibkey>
    </paper>
    <paper id="23">
      <title>The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models</title>
      <author orcid="0009-0002-9312-7360"><first>Lina</first><last>Conti</last><affiliation>University of Trento and Fondazione Bruno Kessler</affiliation></author>
      <author orcid="0000-0002-0940-5595"><first>Dennis</first><last>Fucci</last></author>
      <author orcid="0000-0003-4217-1396"><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author orcid="0000-0002-8811-4330"><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author orcid="0000-0002-4445-080X"><first>Guillaume</first><last>Wisniewski</last><affiliation>LLF / Université Paris Cité</affiliation></author>
      <author orcid="0000-0001-7480-2231"><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>398-414</pages>
      <abstract>Contrastive explanations, which indicate why an AI system produced one output (the target) instead of another (the foil), are widely recognized in explainable AI as more informative and interpretable than standard explanations. However, obtaining such explanations for speech-to-text (S2T) generative models remains an open challenge. Adopting a feature attribution framework, we propose the first method to obtain contrastive explanations in S2T by analyzing how specific regions of the input spectrogram influence the choice between alternative outputs. Through a case study on gender translation in speech translation, we show that our method accurately identifies the audio features that drive the selection of one gender over another.</abstract>
      <url hash="55d72b2f">2025.blackboxnlp-1.23</url>
      <bibkey>conti-etal-2025-unheard</bibkey>
    </paper>
    <paper id="24">
      <title>Exploring Large Language Models’ World Perception: A Multi-Dimensional Evaluation through Data Distribution</title>
      <author orcid="0000-0003-2447-9960"><first>Zhi</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author orcid="0000-0002-5918-2991"><first>Jing</first><last>Yang</last></author>
      <author><first>Ying</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>415-432</pages>
      <abstract>In recent years, large language models (LLMs) have achieved remarkable success across diverse natural language processing tasks. Nevertheless, their capacity to process and reflect core human experiences remains underexplored. Current benchmarks for LLM evaluation typically focus on a single aspect of linguistic understanding, thus failing to capture the full breadth of its abstract reasoning about the world. To address this gap, we propose a multidimensional paradigm to investigate the capacity of LLMs to perceive the world through temporal, spatial, sentimental, and causal aspects. We conduct extensive experiments by partitioning datasets according to different distributions and employing various prompting strategies. Our findings reveal significant differences and shortcomings in how LLMs handle temporal granularity, multi-hop spatial reasoning, subtle sentiments, and implicit causal relationships. While sophisticated prompting approaches can mitigate some of these limitations, substantial challenges persist in effectively capturing abstract human perception. We aspire that this work, which assesses LLMs from multiple perspectives of human understanding of the world, will guide more instructive research on the LLMs’ perception or cognition.</abstract>
      <url hash="f8306a40">2025.blackboxnlp-1.24</url>
      <bibkey>li-etal-2025-exploring-large</bibkey>
    </paper>
    <paper id="25">
      <title>On the Representations of Entities in Auto-regressive Large Language Models</title>
      <author orcid="0009-0001-5312-2998"><first>Victor</first><last>Morand</last></author>
      <author orcid="0000-0001-9273-2193"><first>Josiane</first><last>Mothe</last><affiliation>Université de Toulouse-le-Mirail, UT2J</affiliation></author>
      <author orcid="0000-0001-6792-3262"><first>Benjamin</first><last>Piwowarski</last><affiliation>CNRS / ISIR, Sorbonne Université and CNRS</affiliation></author>
      <pages>433-451</pages>
      <abstract>Named entities are fundamental building blocks of knowledge in text, grounding factual information and structuring relationships within language. Despite their importance, it remains unclear how Large Language Models (LLMs) internally represent entities. Prior research has primarily examined explicit relationships, but little is known about entity representations themselves. We introduce entity mention reconstruction as a novel framework for studying how LLMs encode and manipulate entities. We investigate whether entity mentions can be generated from internal representations, how multi-token entities are encoded beyond last-token embeddings, and whether these representations capture relational knowledge. Our proposed method, leveraging task vectors, allows to consistently generate multi-token mentions from various entity representations derived from the LLMs hidden states. We thus introduce the Entity Lens, extending the logit-lens to predict multi-token mentions. Our results bring new evidence that LLMs develop entity-specific mechanisms to represent and manipulate any multi-token entities, including those unseen during training.</abstract>
      <url hash="40479376">2025.blackboxnlp-1.25</url>
      <bibkey>morand-etal-2025-representations</bibkey>
    </paper>
    <paper id="26">
      <title>Can Language Neuron Intervention Reduce Non-Target Language Output?</title>
      <author><first>Suchun</first><last>Xie</last></author>
      <author><first>Hwichan</first><last>Kim</last><affiliation>SB Intuitions</affiliation></author>
      <author><first>Shota</first><last>Sasaki</last><affiliation>Cyberagent, Inc.</affiliation></author>
      <author><first>Kosuke</first><last>Yamada</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author orcid="0000-0003-2108-1340"><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <pages>452-466</pages>
      <abstract>Large language models (LLMs) often fail togenerate text in the intended target language,particularly in non-English interactions. Con-currently, recent work has explored LanguageNeuron Intervention (LNI) as a promising tech-nique for steering output language. In thispaper, we re-evaluate LNI in more practicalscenarios—specifically with instruction-tunedmodels and prompts that explicitly specify thetarget language. Our experiments show thatwhile LNI also shows potential in such practi-cal scenarios, its average effect is limited andunstable across models and tasks, with a 0.83%reduction in undesired language output and a0.1% improvement in performance. Our furtheranalysis identifies two key factors for LNI’slimitation: (1) LNI affects both the output lan-guage and the content semantics, making ithard to control one without affecting the other,which explains the weak performance gains. (2)LNI increases the target language token proba-bilities, but they often remain below the top-1generation threshold, resulting in failure to gen-erate the target language in most cases. Ourresults highlight both the potential and limi-tations of LNI, paving the way for future im-provements</abstract>
      <url hash="23fa1ca0">2025.blackboxnlp-1.26</url>
      <bibkey>xie-etal-2025-language</bibkey>
    </paper>
    <paper id="27">
      <title>Fine-Grained Manipulation of Arithmetic Neurons</title>
      <author><first>Wenyu</first><last>Du</last></author>
      <author orcid="0009-0002-3414-1146"><first>Rui</first><last>Zheng</last></author>
      <author><first>Tongxu</first><last>Luo</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Stephen</first><last>Chung</last><affiliation>University of Cambridge</affiliation></author>
      <author orcid="0000-0002-4494-843X"><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>467-479</pages>
      <abstract>It is a longstanding challenge to understand how neural models perform mathematical reasoning. Recent mechanistic interpretability work indicates that large language models (LLMs) use a “bag of heuristics” in middle to late-layer MLP neurons for arithmetic, where each heuristic promotes logits for specific numerical patterns. Building on this, we aim for fine-grained manipulation of these heuristic neurons to causally steer model predictions towards specific arithmetic outcomes, moving beyond simply disrupting accuracy. This paper presents a methodology that enables the systematic identification and causal manipulation of heuristic neurons, which is applied to the addition task in this study. We train a linear classifier to predict heuristics based on activation values, achieving over 90% classification accuracy. The trained classifier also allows us to rank neurons by their importance to a given heuristic. By targeting a small set of top-ranked neurons (K=50), we demonstrate high success rates—over 80% for the ones place and nearly 70% for the tens place—in controlling addition outcomes. This manipulation is achieved by transforming the activation of identified neurons into specific target heuristics by zeroing out source-heuristic neurons and adjusting target-heuristic neurons towards their class activation centroids. We explain these results by hypothesizing that high-ranking neurons possess ‘cleaner channels’ for their heuristics, supported by Signal-to-Noise Ratio (SNR) analysis where these neurons show higher SNR scores. Our work offers a robust approach to dissect, causally test, and precisely influence LLM arithmetic, advancing understanding of their internal mechanisms.</abstract>
      <url hash="192344c5">2025.blackboxnlp-1.27</url>
      <bibkey>du-etal-2025-fine</bibkey>
    </paper>
    <paper id="28">
      <title>What Features in Prompts Jailbreak <fixed-case>LLM</fixed-case>s? Investigating the Mechanisms Behind Attacks</title>
      <author><first>Nathalie Maria</first><last>Kirch</last></author>
      <author><first>Constantin Niko</first><last>Weisser</last></author>
      <author><first>Severin</first><last>Field</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last><affiliation>King’s College London, University of London</affiliation></author>
      <author orcid="0000-0003-0084-1937"><first>Stephen</first><last>Casper</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>480-520</pages>
      <abstract>Jailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train linear and non-linear probes on hidden states of open-weight LLMs to predict jailbreak success. Probes achieve strong in-distribution accuracy but transfer is attack-family-specific, revealing that different jailbreaks are supported by distinct internal mechanisms rather than a single universal direction. To establish causal relevance, we construct probe-guided latent interventions that systematically shift compliance in the predicted direction. Interventions derived from non-linear probes produce larger and more reliable effects than those from linear probes, indicating that features linked to jailbreak success are encoded non-linearly in prompt representations. Overall, the results surface heterogeneous, non-linear structure in jailbreak mechanisms and provide a prompt-side methodology for recovering and testing the features that drive jailbreak outcomes.</abstract>
      <url hash="37fc6e06">2025.blackboxnlp-1.28</url>
      <bibkey>kirch-etal-2025-features</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>B</fixed-case>lackbox<fixed-case>NLP</fixed-case>-2025 <fixed-case>MIB</fixed-case> Shared Task: Improving Circuit Faithfulness via Better Edge Selection</title>
      <author orcid="0000-0003-3425-2540"><first>Yaniv</first><last>Nikankin</last></author>
      <author><first>Dana</first><last>Arad</last></author>
      <author><first>Itay</first><last>Itzhak</last></author>
      <author><first>Anja</first><last>Reusch</last></author>
      <author><first>Adi</first><last>Simhi</last></author>
      <author><first>Gal</first><last>Kesten</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>521-527</pages>
      <abstract>One of the main challenges in mechanistic interpretability is circuit discovery – determining which parts of a model perform a given task. We build on the Mechanistic Interpretability Benchmark (MIB) and propose three key improvements to circuit discovery. First, we use bootstrapping to identify edges with consistent attribution scores. Second, we introduce a simple ratio-based selection strategy to prioritize strong positive-scoring edges, balancing performance and faithfulness. Third, we replace the standard greedy selection with an integer linear programming formulation. Our methods yield more faithful circuits and outperform prior approaches across multiple MIB tasks and models.</abstract>
      <url hash="164273e7">2025.blackboxnlp-1.29</url>
      <bibkey>nikankin-etal-2025-blackboxnlp</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>B</fixed-case>lackbox<fixed-case>NLP</fixed-case>-2025 <fixed-case>MIB</fixed-case> Shared Task: <fixed-case>IPE</fixed-case>: Isolating Path Effects for Improving Latent Circuit Identification</title>
      <author orcid="0009-0001-8498-7134"><first>Nicolò</first><last>Brunello</last></author>
      <author><first>Andrea</first><last>Cerutti</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <author><first>Andrea</first><last>Sassella</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <author><first>Mark James</first><last>Carman</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <pages>528-536</pages>
      <abstract>Understanding why large language models (LLMs) exhibit certain behaviors is the goal of mechanistic interpretability. One of the major tools employed by mechanistic interpretability is circuit discovery, i.e., identifying a subset of the model’s components responsible for a given task. We present a novel circuit discovery technique called IPE (Isolating Path Effects) that, unlike traditional edge-centric approaches, aims to identify entire computational paths (from input embeddings to output logits) responsible for certain model behaviors. Our method modifies the messages passed between nodes along a given path in such a way as to either precisely remove the effects of the entire path (i.e., ablate it) or to replace the path’s effects with those that would have been generated by a counterfactual input. IPE is different from current path-patching or edge activation-patching techniques since they are not ablating single paths, but rather a set of paths sharing certain edges, preventing more precise tracing of information flow. We apply our method to the well-known Indirect Object Identification (IOI) task, recovering the canonical circuit reported in prior work. On the MIB workshop leaderboard, we tested IOI and MCQA tasks on GPT2-small and Qwen2.5. For GPT2, path counterfactual replacement outperformed path ablation as expected and led to top-ranking results, while for Qwen, no significant differences were observed, indicating a need for larger experiments to distinguish the two approaches.</abstract>
      <url hash="f823c52b">2025.blackboxnlp-1.30</url>
      <bibkey>brunello-etal-2025-blackboxnlp</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>B</fixed-case>lackbox<fixed-case>NLP</fixed-case>-2025 <fixed-case>MIB</fixed-case> Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods</title>
      <author orcid="0009-0008-6446-4687"><first>Philipp</first><last>Mondorf</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Sebastian</first><last>Gerstner</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Ahmad Dawar</first><last>Hakimi</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author orcid="0000-0003-1073-0958"><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Leonor</first><last>Veloso</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Shijia</first><last>Zhou</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>537-542</pages>
      <abstract>The Circuit Localization track of the Mechanistic Interpretability Benchmark (MIB) evaluates methods for localizing circuits within large language models (LLMs), i.e., subnetworks responsible for specific task behaviors. In this work, we investigate whether ensembling two or more circuit localization methods can improve performance. We explore two variants: parallel and sequential ensembling. In parallel ensembling, we combine attribution scores assigned to each edge by different methods—e.g., by averaging or taking the minimum or maximum value. In the sequential ensemble, we use edge attribution scores obtained via EAP-IG as a warm start for a more expensive but more precise circuit identification method, namely edge pruning. We observe that both approaches yield notable gains on the benchmark metrics, leading to a more precise circuit identification approach. Finally, we find that taking a parallel ensemble over various methods, including the sequential ensemble, achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB Shared Task, comparing ensemble scores to official baselines across multiple model–task combinations.</abstract>
      <url hash="99e7ca19">2025.blackboxnlp-1.31</url>
      <bibkey>mondorf-etal-2025-blackboxnlp</bibkey>
    </paper>
    <paper id="32">
      <title>Findings of the <fixed-case>B</fixed-case>lackbox<fixed-case>NLP</fixed-case> 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models</title>
      <author><first>Dana</first><last>Arad</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <author><first>Najoung</first><last>Kim</last><affiliation>Boston University</affiliation></author>
      <author orcid="0000-0001-8184-7825"><first>Hosein</first><last>Mohebbi</last></author>
      <author><first>Aaron</first><last>Mueller</last><affiliation>Northeastern University and Technion - Israel Institute of Technology</affiliation></author>
      <author orcid="0000-0001-8715-2987"><first>Gabriele</first><last>Sarti</last></author>
      <author><first>Martin</first><last>Tutek</last></author>
      <pages>543-552</pages>
      <abstract>Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.</abstract>
      <url hash="0e27c035">2025.blackboxnlp-1.32</url>
      <bibkey>arad-etal-2025-findings</bibkey>
    </paper>
  </volume>
</collection>
