<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.case">
  <volume id="1" ingest-date="2026-01-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 8th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Texts</booktitle>
      <editor><first>Ali</first><last>Hürriyetoğlu</last></editor>
      <editor><first>Hristo</first><last>Tanev</last></editor>
      <editor><first>Surendrabikram</first><last>Thapa</last></editor>
      <editor><first>Surabhi</first><last>Adhikari</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2025</year>
      <url hash="582a2fd1">2025.case-1</url>
      <venue>case</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="7ba722d1">2025.case-1.0</url>
      <bibkey>case-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings and Insights from the 8th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text</title>
      <author><first>Ali</first><last>Hurriyetoglu</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Hristo</first><last>Tanev</last></author>
      <author><first>Surabhi</first><last>Adhikari</last></author>
      <pages>1–5</pages>
      <abstract>This paper presents an overview of the 8th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE), held in conjunction with RANLP 2025. The workshop featured a range of contributions, including regular research papers, system descriptions from shared task participants, and an overview paper on shared task outcomes. Continuing its tradition, CASE brings together researchers from computational and social sciences to explore the evolving landscape of event extraction. With the rapid advancement of large language models (LLMs), this year’s edition placed particular emphasis on their application to socio-political event extraction. Alongside text-based approaches, the workshop also highlighted the growing interest in multimodal event extraction, addressing complex real-world scenarios across diverse modalities.</abstract>
      <url hash="d8b45cc9">2025.case-1.1</url>
      <bibkey>hurriyetoglu-etal-2025-findings</bibkey>
    </paper>
    <paper id="2">
      <title>Challenges and Applications of Automated Extraction of Socio-political Events at the age of Large Language Models</title>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Surabhi</first><last>Adhikari</last></author>
      <author><first>Hristo</first><last>Tanev</last></author>
      <author><first>Ali</first><last>Hurriyetoglu</last></author>
      <pages>6–19</pages>
      <abstract>Socio-political event extraction (SPE) enables automated identification of critical events such as protests, conflicts, and policy shifts from unstructured text. As a foundational tool for journalism, social science research, and crisis response, SPE plays a key role in understanding complex global dynamics. The emergence of large language models (LLMs) like GPT-4 and LLaMA offers new opportunities for flexible, multilingual, and zero-shot SPE. However, applying LLMs to this domain introduces significant risks, including hallucinated outputs, lack of transparency, geopolitical bias, and potential misuse in surveillance or censorship. This position paper critically examines the promises and pitfalls of LLM-driven SPE, drawing on recent datasets and benchmarks. We argue that SPE is a high-stakes application requiring rigorous ethical scrutiny, interdisciplinary collaboration, and transparent design practices. We propose a research agenda focused on reproducibility, participatory development, and building systems that align with democratic values and the rights of affected communities.</abstract>
      <url hash="d619f3a9">2025.case-1.2</url>
      <bibkey>thapa-etal-2025-challenges</bibkey>
    </paper>
    <paper id="3">
      <title>Multimodal Hate, Humor, and Stance Event Detection in Marginalized Sociopolitical Movements</title>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Siddhant Bikram</first><last>Shah</last></author>
      <author><first>Kritesh</first><last>Rauniyar</last></author>
      <author><first>Shuvam</first><last>Shiwakoti</last></author>
      <author><first>Surabhi</first><last>Adhikari</last></author>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Kristina T.</first><last>Johnson</last></author>
      <author><first>Ali</first><last>Hurriyetoglu</last></author>
      <author><first>Hristo</first><last>Tanev</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>20–31</pages>
      <abstract>This paper presents the Shared Task on Multimodal Detection of Hate Speech, Humor, and Stance in Marginalized Socio-Political Movement Discourse, hosted at CASE 2025. The task is built on the PrideMM dataset, a curated collection of 5,063 text-embedded images related to the LGBTQ+ pride movement, annotated for four interrelated subtasks: (A) Hate Speech Detection, (B) Hate Target Classification, (C) Topical Stance Classification, and (D) Intended Humor Detection. Eighty-nine teams registered, with competitive submissions across all subtasks. The results show that multimodal approaches consistently outperform unimodal baselines, particularly for hate speech detection, while fine-grained tasks such as target identification and stance classification remain challenging due to label imbalance, multimodal ambiguity, and implicit or culturally specific content. CLIP-based models and parameter-efficient fusion architectures achieved strong performance, showing promising directions for low-resource and efficient multimodal systems.</abstract>
      <url hash="f0da9a76">2025.case-1.3</url>
      <bibkey>thapa-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="4">
      <title>Natural Language Processing vs Large Language Models: this is the end of the world as we know it, and <fixed-case>I</fixed-case> feel fine</title>
      <author><first>Bertrand</first><last>De Longueville</last></author>
      <pages>32–37</pages>
      <abstract>As practitioners in the field of Natural Language Processing (NLP), we have had the unique vantage point of witnessing the evolutionary strides leading to the emergence of Large Language Models (LLMs) over the past decades. This perspective allows us to contextualise the current enthusiasm surrounding LLMs, especially following the introduction of “General Purpose” Language Models and the widespread adoption of conversational chatbots built on their frameworks. At the same time, we have observed the remarkable capabilities of zeroshot systems powered by LLMs in extracting structured information from text, outperforming previous iterations of language models. In this paper, we contend that that the hype around “conversational AI” is both a revolution and an epiphenomenon for NLP, particularly in the domain of information extraction from text. By adopting a measured approach to the recent technological advancements in Artificial Intelligence that are reshaping NLP, and by utilising Automated Socio-Political Event Extraction from text as a case study, this commentary seeks to offer insights into the ongoing trends and future directions in the field.</abstract>
      <url hash="0402afbd">2025.case-1.4</url>
      <bibkey>de-longueville-2025-natural</bibkey>
    </paper>
    <paper id="5">
      <title>Machine Translation in the <fixed-case>AI</fixed-case> Era: Comparing previous methods of machine translation with large language models</title>
      <author><first>William Jock</first><last>Boyd</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <pages>38–51</pages>
      <abstract>The aim of this paper is to compare the efficacy of multiple different methods of machine translation in the French-English language pair. There is a particular focus on Large Language Models given they are an emerging technology that could have a profound effect on the field of machine translation. This study used the European Parliament’s parallel French-English corpus, testing each method on the same section of data, with multiple different Neural Translation, Large Language Model and Rule-Based solutions being used. The translations were then evaluated using BLEU and METEOR scores to gain an accurate understanding of both precision and semantic accuracy of translation. Statistical analysis was then performed to ensure the results validity and statistical significance. This study found that Neural Translation was the best translation technology overall, with Large Language Models coming second and Rule-Based translation coming last by a significant margin. It was also discovered that within Large Language Model implementations that specifically trained translation capabilities outperformed emergent translation capabilities.</abstract>
      <url hash="1312b17d">2025.case-1.5</url>
      <bibkey>boyd-mitkov-2025-machine</bibkey>
    </paper>
    <paper id="6">
      <title>Steering Towards Fairness: Mitigating Political Stance Bias in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Afrozah</first><last>Nadeem</last></author>
      <author><first>Mark</first><last>Dras</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>52–61</pages>
      <abstract>Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases along political and economic dimensions. In this paper, we employ a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), this method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.</abstract>
      <url hash="49b7467a">2025.case-1.6</url>
      <bibkey>nadeem-etal-2025-steering</bibkey>
    </paper>
    <paper id="7">
      <title>wangkongqiang@<fixed-case>CASE</fixed-case> 2025: Detection and Classifying Language and Targets of Hate Speech using Auxiliary Text Supervised Learning</title>
      <author><first>Wang</first><last>Kongqiang</last></author>
      <author><first>Zhang</first><last>Peng</last></author>
      <pages>62–70</pages>
      <abstract>Our team was interested in content classification and labeling from multimodal detection of Hate speech, Humor, and Stance in marginalized socio-political movement discourse. We joined the task: Subtask A-Detection of Hate Speech and Subtask B-Classifying the Targets of Hate Speech. In this two task, our goal is to assign a content classification label to multimodal Hate Speech. Detection of Hate Speech: The aim is to detect the presence of hate speech in the images. The dataset for this task will have binary labels: No Hate and Hate. Classifying the Targets of Hate Speech: Given that an image is hateful, the goal here is to identify the targets of hate speech. The dataset here will have four labels: Undirected, Individual, Community, and Organization. Our group used a supervised learning method and a text prediction model. The best result on the test set for Subtask-A and Subtask-B were F1 score of 0.6209 and 0.3453, ranking twentieth and thirteenth among all teams.</abstract>
      <url hash="b625f68b">2025.case-1.7</url>
      <bibkey>kongqiang-peng-2025-wangkongqiang</bibkey>
    </paper>
    <paper id="8">
      <title>Luminaries@<fixed-case>CASE</fixed-case> 2025: Multimodal Hate Speech, Target, Stance and Humor Detection using <fixed-case>ALBERT</fixed-case> and Classical Models</title>
      <author><first>Akshay</first><last>Esackimuthu</last></author>
      <pages>71–75</pages>
      <abstract>In recent years, the detection of harmful and socially impactful content in multimodal online data has emerged as a critical area of research, driven by the increasing prevalence of text-embedded images and memes on social media platforms. These multimodal artifacts serve as powerful vehicles for expressing solidarity, resistance, humor, and sometimes hate, especially within the context of marginalized socio-political movements. To address these challenges, this shared task introduces a comprehensive, fine-grained classification framework consisting of four subtasks: (A) detection of hate speech, (B) identification of hate speech targets, (C) classification of topical stance toward marginalized movements, and (D) detection of intended humor. By focusing on the nuanced interplay between text and image modalities, this task aims to push the boundaries of automated socio-political event understanding and moderation. Using state-of-the-art deep learning and multimodal modeling approaches, this work seeks to enable a more effective detection of complex online phenomena, thus contributing to safer and more inclusive digital environments</abstract>
      <url hash="1baa2801">2025.case-1.8</url>
      <bibkey>esackimuthu-2025-luminaries</bibkey>
    </paper>
    <paper id="9">
      <title>Overfitters@<fixed-case>CASE</fixed-case>2025: Multimodal Hate Speech Analysis Using <fixed-case>BERT</fixed-case> and <fixed-case>RESNET</fixed-case></title>
      <author><first>Bidhan Chandra</first><last>Bhattarai</last></author>
      <author><first>Dipshan</first><last>Pokhrel</last></author>
      <author><first>Ishan</first><last>Maharjan</last></author>
      <author><first>Rabin</first><last>Thapa</last></author>
      <pages>76–82</pages>
      <abstract>Marginalized socio-political movements have become focal points of online discourse, polarizing public opinion and attracting attention through controversial or humorous content. Memes, play a powerful role in shaping this discourse both as tools of empowerment, and as vessels for ridicule or hate. The ambiguous and highly contextual nature of these memes presents a unique challenge for computational systems. In this work we try to identify these trends. Our approach leverages the BERT+ResNet(BERTRES) model to classify the multimodal content into different categories based on different tasks for the Shared Task on Multimodal Detection of Hate Speech, Humor, and Stance in Marginalized SocioPolitical Movement Discourse at CASE 2025. The task is divided into four sub-tasks: subtask A focuses on detection of hate speech, subtask B focuses on classifying the targets of hate speech, subtask C focuses on classification of topical stance and subtask D focuses on detection of intended humor. Our approach obtained a 0.73 F1 score in subtask A, 0.56 F1 score in subtask B, 0.6 F1 score in subtask C, 0.65 F1 score in subtask D.</abstract>
      <url hash="d9b51643">2025.case-1.9</url>
      <bibkey>bhattarai-etal-2025-overfitters</bibkey>
    </paper>
    <paper id="10">
      <title>Silver@<fixed-case>CASE</fixed-case>2025: Detection of Hate Speech, Targets, Humor, and Stance in Marginalized Movement</title>
      <author><first>Rohan</first><last>Mainali</last></author>
      <author><first>Neha</first><last>Aryal</last></author>
      <author><first>Sweta</first><last>Poudel</last></author>
      <author><first>Anupraj</first><last>Acharya</last></author>
      <author><first>Rabin</first><last>Thapa</last></author>
      <pages>83–90</pages>
      <abstract>Memes, a multimodal form of communication, have emerged as a popular mode of expression in online discourse, particularly among marginalized groups. With multiple meanings, memes often combine satire, irony, and nuanced language, presenting particular challenges to machines in detecting hate speech, humor, stance, and the target of hostility. This paper presents a comparison of unimodal and multimodal solutions to address all four subtasks of the CASE 2025 Shared Task on Multimodal Hate, Humor, and Stance Detection. We compare transformer-based text models (BERT, RoBERTa) with CNN-based vision models (DenseNet, EfficientNet), and multimodal fusion methods, such as CLIP. We find that multimodal systems consistently outperform the unimodal baseline, with CLIP performing the best on all subtasks with a macro F1 score of 78% in sub-task A, 56% in sub-task B, 59% in sub-task C, and 72% in sub-task D.</abstract>
      <url hash="e638bd56">2025.case-1.10</url>
      <bibkey>mainali-etal-2025-silver</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>MLI</fixed-case>nitiative at <fixed-case>CASE</fixed-case> 2025: Multimodal Detection of Hate Speech, Humor,and Stance using Transformers</title>
      <author><first>Ashish</first><last>Acharya</last></author>
      <author><first>Ankit</first><last>Bk</last></author>
      <author><first>Bikram</first><last>K.c.</last></author>
      <author><first>Surabhi</first><last>Adhikari</last></author>
      <author><first>Rabin</first><last>Thapa</last></author>
      <author><first>Sandesh</first><last>Shrestha</last></author>
      <author><first>Tina</first><last>Lama</last></author>
      <pages>91–97</pages>
      <abstract>In recent years, memes have developed as popular forms of online satire and critique, artfully merging entertainment, social critique, and political discourse. On the other side, memes have also become a medium for the spread of hate speech, misinformation, and bigotry, especially towards marginalized communities, including the LGBTQ+ population. Solving this problem calls for the development of advanced multimodal systems that analyze the complex interplay between text and visuals in memes. This paper describes our work in the CASE@RANLP 2025 shared task. As a part of that task, we developed systems for hate speech detection, target identification, stance classification, and humor recognition within the text of memes. We investigate two multimodal transformer-based systems, ResNet-18 with BERT and SigLIP2, for these sub-tasks. Our results show that SigLIP-2 consistently outperforms the baseline, achieving an F1 score of 79.27 in hate speech detection, 72.88 in humor classification, and competitive performance in stance 60.59 and target detection 54.86. Through this study, we aim to contribute to the development of ethically grounded, inclusive NLP systems capable of interpreting complex sociolinguistic narratives in multi-modal content.</abstract>
      <url hash="a492da89">2025.case-1.11</url>
      <bibkey>acharya-etal-2025-mlinitiative</bibkey>
    </paper>
    <paper id="12">
      <title>Multimodal Deep Learning for Detection of Hate, Humor, and Stance in Social Discourse on Marginalized Communities</title>
      <author><first>Durgesh</first><last>Verma</last></author>
      <author><first>Abhinav</first><last>Kumar</last></author>
      <pages>98–106</pages>
      <abstract>Internet memes serve as powerful vehicles of expression across platforms like Instagram, Twitter, and WhatsApp. However, they often carry implicit messages such as humor, sarcasm, or offense especially in the context of marginalized communities. Understanding such intent is crucial for effective moderation and content filtering. This paper introduces a deep learning-based multimodal framework developed for the CASE 2025 Shared Task on detecting hate, humor, and stance in memes related to marginalized movements. The study explores three architectures combining textual models (BERT, XLM-RoBERTa) with visual encoders (ViT, CLIP), enhanced through cross-modal attention and Transformer-based fusion. Evaluated on four subtasks, the models effectively classify meme content—such as satire and offense—demonstrating the value of attention-driven multimodal integration in interpreting nuanced social media expressions</abstract>
      <url hash="fd643b94">2025.case-1.12</url>
      <bibkey>verma-kumar-2025-multimodal</bibkey>
    </paper>
    <paper id="13">
      <title>Multimodal Kathmandu@<fixed-case>CASE</fixed-case> 2025: Task-Specific Adaptation of Multimodal Transformers for Hate, Stance, and Humor Detection</title>
      <author><first>Sujal</first><last>Maharjan</last></author>
      <author><first>Astha</first><last>Shrestha</last></author>
      <author><first>Shuvam</first><last>Thakur</last></author>
      <author><first>Rabin</first><last>Thapa</last></author>
      <pages>107–114</pages>
      <abstract>The multimodal ambiguity of text-embedded images (memes), particularly those pertaining to marginalized communities, presents a significant challenge for natural language and vision processing. The subtle interaction between text, image, and cultural context makes it challenging to develop robust moderation tools. This paper tackles this challenge across four key tasks: (A) Hate Speech Detection, (B) Hate Target Classification, (C) Topical Stance Classification, and (D) Intended Humor Detection. We demonstrate that the nuances of these tasks demand a departure from a ‘onesize-fits-all’ approach. Our central contribution is a task-specific methodology, where we align model architecture with the specific challenges of each task, all built upon a common CLIP-ViT backbone. Our results illustrate the strong performance of this task-specific approach, with multiple architectures excelling at each task. For Hate Speech Detection (Task A), the Co-Attention Ensemble model achieved a top F1-score of 0.7929; for Hate Target Classification (Task B), our Hierarchical CrossAttention Transformer achieved an F1-score of 0.5777; and for Stance (Task C) and Humor Detection (Task D), our Two-Stage Multiplicative Fusion Framework yielded leading F1-scores of 0.6070 and 0.7529, respectively. Beyond raw results, we also provide detailed error analyses, including confusion matrices, to reveal weaknesses driven by multimodal ambiguity and class imbalance. Ultimately, this work provides a blueprint for the community, establishing that optimal performance in multimodal analysis is achieved not by a single superior model, but through the customized design of specialized solutions, supported by empirical validation of key methodological choices.</abstract>
      <url hash="dcee0c80">2025.case-1.13</url>
      <bibkey>maharjan-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>MMF</fixed-case>usion@<fixed-case>CASE</fixed-case> 2025: Attention-Based Multimodal Learning for Text-Image Content Analysis</title>
      <author><first>Prerana</first><last>Rane</last></author>
      <pages>115–122</pages>
      <abstract>Text-embedded images, such as memes, are now increasingly common in social media discourse. These images combine visual and textual elements to convey complex attitudes and emotions. Deciphering the intent of these images is challenging due to their multimodal and context-dependent nature. This paper presents our approach to the Shared Task on Multimodal Hate, Humor, and Stance Detection in Marginalized Movement at CASE 2025. The shared task focuses on four key aspects of multimodal content analysis for text-embedded images: hate speech detection, target identification, stance classification, and humor recognition. We propose a multimodal learning framework that uses both textual and visual representations, along with cross-modal attention mechanisms, to classify content across all tasks effectively.</abstract>
      <url hash="627bc821">2025.case-1.14</url>
      <bibkey>rane-2025-mmfusion</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>TSR</fixed-case>@<fixed-case>CASE</fixed-case> 2025: Low Dimensional Multimodal Fusion Using Multiplicative Fine Tuning Modules</title>
      <author><first>Sushant Kr.</first><last>Ray</last></author>
      <author><first>Rafiq</first><last>Ali</last></author>
      <author><first>Abdullah</first><last>Mohammad</last></author>
      <author><first>Ebad</first><last>Shabbir</last></author>
      <author><first>Samar</first><last>Wazir</last></author>
      <pages>123–132</pages>
      <abstract>This study describes our submission to the CASE 2025 shared task on multimodal hate event detection, which focuses on hate detection, hate target identification, stance determination, and humour detection on text embedded images as classification challenges. Our submission contains entries in all of the subtasks. We propose FIMIF, a lightweight and efficient classification model that leverages frozen CLIP encoders. We utilise a feature interaction module that allows the model to exploit multiplicative interactions between features without any manual engineering. Our results demonstrate that the model achieves comparable or superior performance to larger models, despite having a significantly smaller parameter count</abstract>
      <url hash="ffd7758d">2025.case-1.15</url>
      <bibkey>ray-etal-2025-tsr</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>P</fixed-case>hantom<fixed-case>T</fixed-case>roupe@<fixed-case>CASE</fixed-case> 2025: Multimodal Hate Speech Detection in Text-Embedded Memes using Instruction-Tuned <fixed-case>LLM</fixed-case>s</title>
      <author><first>Farhan</first><last>Amin</last></author>
      <author><first>Muhammad</first><last>Abu Horaira</last></author>
      <author><first>Md. Tanvir Ahammed</first><last>Shawon</last></author>
      <author><first>Md. Ayon</first><last>Mia</last></author>
      <author><first>Muhammad Ibrahim</first><last>Khan</last></author>
      <pages>133–138</pages>
      <abstract>Memes and other text-embedded images are powerful tools for expressing opinions and identities, especially within marginalized socio-political movements. Detecting hate speech in this type of multimodal content is challenging because of the subtle ways text and visuals interact. In this paper, we describe our approach for Subtask A of the Shared Task on Multimodal Hate Detection in Marginalized Movement@CASE 2025, which focuses on classifying memes as either Hate or No Hate. We tested both unimodal and multimodal setups, using models like DistilBERT, HateBERT, Vision Transformer, and Swin Transformer. Our best system is the large multimodal model Qwen2.5-VL-7B-Instruct-bnb-4bit, fine-tuned with 4-bit quantization and instruction prompts. While we also tried late fusion with multiple transformers, Qwen performed better at capturing text-image interactions in memes. This LLM-based approach reached the highest F1-score of 0.8086 on the test set, ranking our team 5th overall in the task. These results show the value of late fusion and instruction-tuned LLMs for tackling complex hate speech in socio-political memes.</abstract>
      <url hash="c5358212">2025.case-1.16</url>
      <bibkey>amin-etal-2025-phantomtroupe</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>ID</fixed-case>4<fixed-case>F</fixed-case>usion@<fixed-case>CASE</fixed-case> 2025: A Multimodal Approach to Hate Speech Detection in Text-Embedded Memes Using ensemble Transformer based approach</title>
      <author><first>Tabassum Basher</first><last>Rashfi</last></author>
      <author><first>Md. Tanvir Ahammed</first><last>Shawon</last></author>
      <author><first>Md. Ayon</first><last>Mia</last></author>
      <author><first>Muhammad Ibrahim</first><last>Khan</last></author>
      <pages>139–145</pages>
      <abstract>Identification of hate speech in images with text is a complicated task in the scope of online content moderation, especially when such talk penetrates into the spheres of humor and critical societal topics. This paper deals with Subtask A of the Shared Task on Multimodal Hate, Humor, and Stance Detection in Marginalized Movement@CASE2025. This task is binary classification over whether or not hate speech exists in image contents, and it advances as Hate versus No Hate. To meet this goal, we present a new multimodal architecture that blends the textual and visual features to reach effective classification. In the textual aspect, we have fine-tuned two state-of-the-art transformer models, which are RoBERTa and HateBERT, to extract linguistic clues of hate speech. The image encoder contains both the EfficientNetB7 and a Vision Transformer (ViT) model, which were found to work well in retrieving image-related details. The predictions made by each modality are then merged through an ensemble mechanism, with the last estimate being a weighted average of the text- and image-based scores. The resulting model produces a desirable F1- score metric of 0.7868, which is ranked 10 among the total number of systems, thus becoming a clear indicator of the success of multimodal combination in addressing the complex issue of self-identifying the hate speech in text-embedded images.</abstract>
      <url hash="6beb5ec4">2025.case-1.17</url>
      <bibkey>rashfi-etal-2025-id4fusion</bibkey>
    </paper>
    <paper id="18">
      <title>Team <fixed-case>M</fixed-case>eme<fixed-case>M</fixed-case>asters@<fixed-case>CASE</fixed-case> 2025: Adapting Vision-Language Models for Understanding Hate Speech in Multimodal Content</title>
      <author><first>Shruti</first><last>Gurung</last></author>
      <author><first>Shubham</first><last>Shakya</last></author>
      <pages>146–151</pages>
      <abstract>Social media memes have become a powerful form of digital communication, combining images and text to convey humor, social commentary, and sometimes harmful content. This paper presents a multimodal approach using a fine-tuned CLIP model to analyze textembedded images in the CASE 2025 Shared Task. We address four subtasks: Hate Speech Detection, Target Classification, Stance Detection, and Humor Detection. Our method effectively captures visual and textual signals, achieving strong performance with precision of 80% for the detection of hate speech and 76% for the detection of humor, while stance and target classification achieved a precision of 60% and 54%, respectively. Detailed evaluations with classification reports and confusion matrices highlight the ability of the model to handle complex multimodal signals in social media content, demonstrating the potential of vision-language models for computational social science applications.</abstract>
      <url hash="4304bd2e">2025.case-1.18</url>
      <bibkey>gurung-shakya-2025-team</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>CUET</fixed-case> <fixed-case>NOOB</fixed-case>@<fixed-case>CASE</fixed-case>2025: <fixed-case>M</fixed-case>ultimodal<fixed-case>H</fixed-case>ate Speech Detection in Text-Embedded Memes using Late Fusion with Attention Mechanism</title>
      <author><first>Tomal Paul</first><last>Joy</last></author>
      <author><first>Aminul</first><last>Islam</last></author>
      <author><first>Saimum</first><last>Islam</last></author>
      <author><first>Md. Tanvir Ahammed</first><last>Shawon</last></author>
      <author><first>Md. Ayon</first><last>Mia</last></author>
      <author><first>Mohammad Ibrahim</first><last>Khan</last></author>
      <pages>152–158</pages>
      <abstract>Memes and text-embedded images have rapidly become compelling cultural artifacts that both facilitate expressive communication and serve as conduits for spreading hate speech against marginalized communities. Detecting hate speech within such multimodal content poses significant challenges due to the complex and subtle interplay between textual and visual elements. This paper presents our approach for Subtask A of the Shared Task on Multimodal Hate Detection in Marginalized Movement@CASE 2025, focusing on the binary classification of memes into Hate or No Hate categories. We propose a novel multimodal architecture that integrates DistilBERT for textual encoding with Vision Transformer (ViT) for image representation, combined through an advanced late fusion mechanism leveraging multi-head attention. Our method utilizes attention-based feature alignment to capture nuanced cross-modal interactions within memes. The proposed system achieved an F1-score of 0.7416 on the test set, securing the 13th position in the competition. These results underscore the value of sophisticated fusion strategies and attention mechanisms in comprehending and detecting complex socio-political content embedded in memes.</abstract>
      <url hash="609a1789">2025.case-1.19</url>
      <bibkey>joy-etal-2025-cuet</bibkey>
    </paper>
  </volume>
</collection>
