<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.ldl">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020)</booktitle>
      <editor><first>Maxim</first><last>Ionov</last></editor>
      <editor><first>John P.</first><last>McCrae</last></editor>
      <editor><first>Christian</first><last>Chiarcos</last></editor>
      <editor><first>Thierry</first><last>Declerck</last></editor>
      <editor><first>Julia</first><last>Bosque-Gil</last></editor>
      <editor><first>Jorge</first><last>Gracia</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-36-8</isbn>
    </meta>
    <frontmatter>
      <url hash="f15e978d">2020.ldl-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Towards an Ontology Based on Hallig-Wartburg’s Begriffssystem for Historical Linguistic Linked Data</title>
      <author><first>Sabine</first><last>Tittel</last></author>
      <author><first>Frances</first><last>Gillis-Webber</last></author>
      <author><first>Alessandro A.</first><last>Nannini</last></author>
      <pages>1–10</pages>
      <abstract>To empower end users in searching for historical linguistic content with a performance that far exceeds the research functions offered by websites of, e.g., historical dictionaries, is undoubtedly a major advantage of (Linguistic) Linked Open Data ([L]LOD). An important aim of lexicography is to enable a language-independent, onomasiological approach, and the modelling of linguistic resources following the LOD paradigm facilitates the semantic mapping to ontologies making this approach possible. Hallig-Wartburg’s Begriffssystem (HW) is a well-known extra-linguistic conceptual system used as an onomasiological framework by many historical lexicographical and lexicological works. Published in 1952, HW has meanwhile been digitised. With proprietary XML data as the starting point, our goal is the transformation of HW into Linked Open Data in order to facilitate its use by linguistic resources modelled as LOD. In this paper, we describe the particularities of the HW conceptual model and the method of converting HW: We discuss two approaches, (i) the representation of HW in RDF using SKOS, the SKOS thesaurus extension, and XKOS, and (ii) the creation of a lightweight ontology expressed in OWL, based on the RDF/SKOS model. The outcome is illustrated with use cases of medieval Gascon, and Italian.</abstract>
      <url hash="bf57f384">2020.ldl-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title>Transforming the Cologne Digital <fixed-case>S</fixed-case>anskrit Dictionaries into <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-Lemon</title>
      <author><first>Francisco</first><last>Mondaca</last></author>
      <author><first>Felix</first><last>Rau</last></author>
      <pages>11–14</pages>
      <abstract>The Cologne Digital Sanskrit Dictionaries (CDSD) is a large collection of complex digitized Sanskrit dictionaries, consisting of over thirty-five works, and is the most prominent collection of Sanskrit dictionaries worldwide. In this paper we evaluate two methods for transforming the CDSD into Ontolex-Lemon based on a modelling exercise. The first method that we evaluate consists of applying RDFa to the existent TEI-P5 files. The second method consists of transforming the TEI-encoded dictionaries into new files containing RDF triples modelled in OntoLex-Lemon. As a result of the modelling exercise we choose the second method: to transform TEI-encoded lexical data into Ontolex-Lemon by creating new files containing exclusively RDF triples.</abstract>
      <url hash="7bb4408c">2020.ldl-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>Representing Temporal Information in Lexical Linked Data Resources</title>
      <author><first>Fahad</first><last>Khan</last></author>
      <pages>15–22</pages>
      <abstract>The increasing recognition of the utility of Linked Data as a means of publishing lexical resource has helped to underline the need for RDF based data models which have the flexibility and expressivity to be able to represent the most salient kinds of information contained in such resources as structured data, including, notably, information relating to time and the temporal dimension. In this article we describe a perdurantist approach to modelling diachronic lexical information which builds upon work which we have previously presented and which is based on the ontolex-lemon vocabulary. We present two extended examples, one taken from the Oxford English Dictionary, the other from a work on etymology, to show how our approach can handle different kinds of temporal information often found in lexical resources.</abstract>
      <url hash="3f30f4f1">2020.ldl-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>From Linguistic Descriptions to Language Profiles</title>
      <author><first>Shafqat Mumtaz</first><last>Virk</last></author>
      <author><first>Harald</first><last>Hammarström</last></author>
      <author><first>Lars</first><last>Borin</last></author>
      <author><first>Markus</first><last>Forsberg</last></author>
      <author><first>Søren</first><last>Wichmann</last></author>
      <pages>23–27</pages>
      <abstract>Language catalogues and typological databases are two important types of resources containing different types of knowledge about the world’s natural languages. The former provide metadata such as number of speakers, location (in prose descriptions and/or GPS coordinates), language code, literacy, etc., while the latter contain information about a set of structural and functional attributes of languages. Given that both types of resources are developed and later maintained manually, there are practical limits as to the number of languages and the number of features that can be surveyed. We introduce the concept of a language profile, which is intended to be a structured representation of various types of knowledge about a natural language extracted semi-automatically from descriptive documents and stored at a central location. It has three major parts: (1) an introductory; (2) an attributive; and (3) a reference part, each containing different types of knowledge about a given natural language. As a case study, we develop and present a language profile of an example language. At this stage, a language profile is an independent entity, but in the future it is envisioned to become part of a network of language profiles connected to each other via various types of relations. Such a representation is expected to be suitable both for humans and machines to read and process for further deeper linguistic analyses and/or comparisons.</abstract>
      <url hash="69baadad">2020.ldl-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>Terme-à-<fixed-case>LLOD</fixed-case>: Simplifying the Conversion and Hosting of Terminological Resources as Linked Data</title>
      <author><first>Maria Pia</first><last>di Buono</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>Mohammad Fazleh</first><last>Elahi</last></author>
      <author><first>Frank</first><last>Grimm</last></author>
      <pages>28–35</pages>
      <abstract>In recent years, there has been increasing interest in publishing lexicographic and terminological resources as linked data. The benefit of using linked data technologies to publish terminologies is that terminologies can be linked to each other, thus creating a cloud of linked terminologies that cross domains, languages and that support advanced applications that do not work with single terminologies but can exploit multiple terminologies seamlessly. We present Terme-‘a-LLOD (TAL), a new paradigm for transforming and publishing terminologies as linked data which relies on a virtualization approach. The approach rests on a preconfigured virtual image of a server that can be downloaded and installed. We describe our approach to simplifying the transformation and hosting of terminological resources in the remainder of this paper. We provide a proof-of-concept for this paradigm showing how to apply it to the conversion of the well-known IATE terminology as well as to various smaller terminologies. Further, we discuss how the implementation of our paradigm can be integrated into existing NLP service infrastructures that rely on virtualization technology. While we apply this paradigm to the transformation and hosting of terminologies as linked data, the paradigm can be applied to any other resource format as well.</abstract>
      <url hash="da08bb87">2020.ldl-1.5</url>
      <language>eng</language>
    </paper>
    <paper id="6">
      <title>Annohub – Annotation Metadata for Linked Data Applications</title>
      <author><first>Frank</first><last>Abromeit</last></author>
      <author><first>Christian</first><last>Fäth</last></author>
      <author><first>Luis</first><last>Glaser</last></author>
      <pages>36–44</pages>
      <abstract>We introduce a new dataset for the Linguistic Linked Open Data (LLOD) cloud that will provide metadata about annotation and language information harvested from annotated language resources like corpora freely available on the internet. To our knowledge annotation metadata is not provided by any metadata provider, e.g. linghub, datahub or CLARIN so far. On the other hand, language metadata that is found on such portals is rarely provided in machine-readable form, especially as Linked Data. In this paper, we describe the harvesting process, content and structure of the new dataset and its application in the Lin|gu|is|tik portal, a research platform for linguists. Aside from that, we introduce tools for the conversion of XML encoded language resources to the CoNLL format. The generated RDF data as well as the XML-converter application are made public under an open license.</abstract>
      <url hash="73193626">2020.ldl-1.6</url>
      <language>eng</language>
    </paper>
    <paper id="7">
      <title>Challenges of Word Sense Alignment: <fixed-case>P</fixed-case>ortuguese Language Resources</title>
      <author><first>Ana</first><last>Salgado</last></author>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <author><first>Alberto</first><last>Simões</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <author><first>Rute</first><last>Costa</last></author>
      <pages>45–51</pages>
      <abstract>This paper reports on an ongoing task of monolingual word sense alignment in which a comparative study between the Portuguese Academy of Sciences Dictionary and the Dicionário Aberto is carried out in the context of the ELEXIS (European Lexicographic Infrastructure) project. Word sense alignment involves searching for matching senses within dictionary entries of different lexical resources and linking them, which poses significant challenges. The lexicographic criteria are not always entirely consistent within individual dictionaries and even less so across different projects where different options may have been assumed in terms of structure and especially wording techniques of lexicographic glosses. This hinders the task of matching senses. We aim to present our annotation workflow in Portuguese using the Semantic Web technologies. The results obtained are useful for the discussion within the community.</abstract>
      <url hash="71ecc019">2020.ldl-1.7</url>
      <language>eng</language>
    </paper>
    <paper id="8">
      <title>A Lime-Flavored <fixed-case>REST</fixed-case> <fixed-case>API</fixed-case> for Alignment Services</title>
      <author><first>Manuel</first><last>Fiorelli</last></author>
      <author><first>Armando</first><last>Stellato</last></author>
      <pages>52–60</pages>
      <abstract>A practical alignment service should be flexible enough to handle the varied alignment scenarios that arise in the real world, while minimizing the need for manual configuration. MAPLE, an orchestration framework for ontology alignment, supports this goal by coordinating a few loosely coupled actors, which communicate and cooperate to solve a matching task using explicit metadata about the input ontologies, other available resources and the task itself. The alignment task is thus summarized by a report listing its characteristics and suggesting alignment strategies. The schema of the report is based on several metadata vocabularies, among which the Lime module of the OntoLex-Lemon model is particularly important, summarizing the lexical content of the input ontologies and describing external language resources that may be exploited for performing the alignment. In this paper, we propose a REST API that enables the participation of downstream alignment services in the process orchestrated by MAPLE, helping them self-adapt in order to handle heterogeneous alignment tasks and scenarios. The realization of this alignment orchestration effort has been performed through two main phases: we first described its API as an OpenAPI specification (a la API-first), which we then exploited to generate server stubs and compliant client libraries. Finally, we switched our focus to the integration of existing alignment systems, with one fully integrated system and an additional one being worked on, in the effort to propose the API as a valuable addendum to any system being developed.</abstract>
      <url hash="5adf957e">2020.ldl-1.8</url>
      <language>eng</language>
    </paper>
    <paper id="9">
      <title>Using <fixed-case>O</fixed-case>nto<fixed-case>L</fixed-case>ex-Lemon for Representing and Interlinking Lexicographic Collections of <fixed-case>B</fixed-case>avarian Dialects</title>
      <author><first>Yalemisew</first><last>Abgaz</last></author>
      <pages>61–69</pages>
      <abstract>This paper describes the ongoing work in converting the lexicographic collection of a non-standard German language dataset (Bavarian Dialects) into a Linguistic Linked Open Data (LLOD) format. The collection is divided into two: questionnaire dataset (DBÖ) which contains details of the questionnaires, questions, collectors, paper slips etc., and the lexical dataset (WBÖ) which contains lexical entries (answers) collected in response to the questions. In its current form, the lexical dataset is available in a TEI/XML format separately from the questionnaire dataset. This paper presents the mapping of the lexical entries in the TEI/XML format into LLOD format using the Ontolex-Lemon model. The paper shows how the data in TEI/XML format is transformed into LLOD and produces a lexicon for Bavarian Dialects. It also presents the approach used to interlink the original questions with the lexical entries. The resulting lexicon complements the questionnaire dataset, which is already in a LLOD format, by semantically interlinking the original questions with the answers and vice-versa.</abstract>
      <url hash="43e801f4">2020.ldl-1.9</url>
      <language>eng</language>
    </paper>
    <paper id="10">
      <title>Involving Lexicographers in the <fixed-case>LLOD</fixed-case> Cloud with <fixed-case>L</fixed-case>ex<fixed-case>O</fixed-case>, an Easy-to-use Editor of Lemon Lexical Resources</title>
      <author><first>Andrea</first><last>Bellandi</last></author>
      <author><first>Emiliano</first><last>Giovannetti</last></author>
      <pages>70–74</pages>
      <abstract>In this contribution, we show LexO, a user-friendly web collaborative editor of lexical resources based on the lemon model. LexO has been developed in the context of Digital Humanities projects, in which a key point in the design of an editor was the ease of use by lexicographers with no skill in Linked Data or Semantic Web technologies. Though the tool already allows creating a lemon lexicon from scratch and lets a team of users work on it collaboratively, many developments are possible. The involvement of the LLOD community appears now crucial both to find new users and application fields where to test it, and, even more importantly, to understand in which way it should evolve.</abstract>
      <url hash="1de0e9bc">2020.ldl-1.10</url>
      <language>eng</language>
    </paper>
    <paper id="11">
      <title>Supervised Hypernymy Detection in <fixed-case>S</fixed-case>panish through Order Embeddings</title>
      <author><first>Gun Woo</first><last>Lee</last></author>
      <author><first>Mathias</first><last>Etcheverry</last></author>
      <author><first>Daniel</first><last>Fernandez Sanchez</last></author>
      <author><first>Dina</first><last>Wonsever</last></author>
      <pages>75–81</pages>
      <abstract>This paper addresses the task of supervised hypernymy detection in Spanish through an order embedding and using pretrained word vectors as input. Although the task has been widely addressed in English, there is not much work in Spanish, and according to our knowledge there is not any available dataset for supervised hypernymy detection in Spanish. We built a supervised hypernymy dataset for Spanish from WordNet and corpus statistics information, with different versions according to the lexical intersection between its partitions: random and lexical split. We show the results of using the resulting dataset within an order embedding consuming pretrained word vectors as input. We show the ability of pretrained word vectors to transfer learning to unseen lexical units according to the results in the lexical split dataset. To finish, we study the results of giving additional information in training time, such as, cohyponym links and instances extracted through patterns.</abstract>
      <url hash="54bb839b">2020.ldl-1.11</url>
      <language>eng</language>
    </paper>
    <paper id="12">
      <title>Lexemes in <fixed-case>W</fixed-case>ikidata: 2020 status</title>
      <author><first>Finn</first><last>Nielsen</last></author>
      <pages>82–86</pages>
      <abstract>Wikidata now records data about lexemes, senses and lexical forms and exposes them as Linguistic Linked Open Data. Since lexemes in Wikidata was first established in 2018, this data has grown considerable in size. Links between lexemes in different languages can be made, e.g., through a derivation property or senses. We present some descriptive statistics about the lexemes of Wikidata, focusing on the multilingual aspects and show that there are still relatively few multilingual links.</abstract>
      <url hash="0d0fdd66">2020.ldl-1.12</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
