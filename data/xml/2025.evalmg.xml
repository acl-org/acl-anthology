<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.evalmg">
  <volume id="1" ingest-date="2025-01-24" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop of Evaluation of Multi-Modal Generation</booktitle>
      <editor><first>Wei Emma</first><last>Zhang</last></editor>
      <editor><first>Xiang</first><last>Dai</last></editor>
      <editor><first>Desmond</first><last>Elliot</last></editor>
      <editor><first>Byron</first><last>Fang</last></editor>
      <editor><first>Mongyuan</first><last>Sim</last></editor>
      <editor><first>Haojie</first><last>Zhuang</last></editor>
      <editor><first>Weitong</first><last>Chen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, UAE</address>
      <month>Jan</month>
      <year>2025</year>
      <url hash="62b231ec">2025.evalmg-1</url>
      <venue>evalmg</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="4e40753b">2025.evalmg-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>A Dataset for Programming-based Instructional Video Classification and Question Answering</title>
      <author><first>Sana Javaid</first><last>Raja</last></author>
      <author><first>Adeel</first><last>Zafar</last></author>
      <author><first>Aqsa</first><last>Shoaib</last></author>
      <pages>1–9</pages>
      <abstract>This work aims to develop an understanding of the rapidly emerging field of VideoQA, particularly in the context of instructional programming videos. It also encourages designing of system that can produce visual answer to programming based natural language questions. We introduce two datasets: CodeVidQA, with 2,104 question-answer pair links with timestamps taken from programming videos of Stack Overflow for Programming Visual Answer Localization task, and CodeVidCL with 4,331 videos (1,751 programming ,2580 non-programming) for Programming Video Classification task. In addition, we proposed a framework that adapts BigBird and SVM for video classification techniques. The proposed approach achieves a significantly high accuracy of 99.61% for video classification.</abstract>
      <url hash="78dae3aa">2025.evalmg-1.1</url>
    </paper>
    <paper id="2">
      <title><fixed-case>CVT</fixed-case>5: Using Compressed Video Encoder and <fixed-case>UMT</fixed-case>5 for Dense Video Captioning</title>
      <author><first>Mohammad Javad</first><last>Pirhadi</last></author>
      <author><first>Motahhare</first><last>Mirzaei</last></author>
      <author><first>Sauleh</first><last>Eetemadi</last></author>
      <pages>10–23</pages>
      <abstract>The dense video captioning task aims to detect all events occurring in a video and describe each event using natural language. Unlike most other video processing tasks, where it is typically assumed that videos contain only a single main event, this task deals with long, untrimmed videos. Consequently, the speed of processing videos in dense video captioning is a critical aspect of the system. To the best of our knowledge, all published work on this task uses RGB frames to encode input videos. In this work, we introduce the use of compressed videos for the first time in this task. Our experiments on the SoccerNet challenge demonstrate significant improvements in both processing speed and GPU memory footprint while achieving competitive results. Additionally, we leverage multilingual transcripts, which seems to be effective. The encoder in our proposed method achieves approximately 5.4× higher speed and 5.1× lower GPU memory usage during training, and 4.7× higher speed and 7.8× lower GPU memory usage during inference, compared to its RGB-based counterpart. The code is publicly available at https://github.com/mohammadjavadpirhadi/CVT5.</abstract>
      <url hash="7686f5a5">2025.evalmg-1.2</url>
    </paper>
    <paper id="3">
      <title>If <fixed-case>I</fixed-case> feel smart, <fixed-case>I</fixed-case> will do the right thing: Combining Complementary Multimodal Information in Visual Language Models</title>
      <author><first>Yuyu</first><last>Bai</last></author>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <pages>24–39</pages>
      <abstract>Generative visual language models (VLMs) have recently shown potential across various downstream language-and-vision tasks. At the same time, it is still an open question whether, and to what extent, these models can properly understand a multimodal context where language and vision provide complementary information—a mechanism routinely in place in human language communication. In this work, we test various VLMs on the task of generating action descriptions consistent with both an image’s visual content and an intention or attitude (not visually grounded) conveyed by a textual prompt. Our results show that BLIP-2 is not far from human performance when the task is framed as a generative multiple-choice problem, while other models struggle. Furthermore, the actions generated by BLIP-2 in an open-ended generative setting are better than those by the competitors; indeed, human annotators judge most of them as plausible continuations for the multimodal context. Our study reveals substantial variability among VLMs in integrating complementary multimodal information, yet BLIP-2 demonstrates promising trends across most evaluations, paving the way for seamless human-computer interaction.</abstract>
      <url hash="383df8ea">2025.evalmg-1.3</url>
    </paper>
    <paper id="4">
      <title><fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case>-<fixed-case>RE</fixed-case>: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</title>
      <author><first>Tao</first><last>Sun</last></author>
      <author><first>Oliver</first><last>Liu</last></author>
      <author><first>JinJin</first><last>Li</last></author>
      <author><first>Lan</first><last>Ma</last></author>
      <pages>40–51</pages>
      <abstract>Multimodal generative AI usually involves generating image or text responses given inputs in another modality. The evaluation of image-text relevancy is essential for measuring the response quality or ranking candidate responses. In particular, binary relevancy evaluation, i.e., “Relevant” vs. “Not Relevant”, is a fundamental problem. However, this is a challenging task considering that texts have diverse formats and the definition of relevancy varies in different scenarios. We find that Multimodal Large Language Models (MLLMs) are an ideal choice to build such evaluators, as they can flexibly handle complex text formats and take in additional task information. In this paper, we present LLaVA-RE, a first attempt for binary image-text relevancy evaluation with MLLM. It follows the LLaVA architecture and adopts detailed task instructions and multimodal in-context samples. Further, we propose a novel binary relevancy dataset covering diverse tasks. Experimental results validate the effectiveness of our framework.</abstract>
      <url hash="0fcafeab">2025.evalmg-1.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>P</fixed-case>ersian in a Court: Benchmarking <fixed-case>VLM</fixed-case>s In <fixed-case>P</fixed-case>ersian Multi-Modal Tasks</title>
      <author><first>Farhan</first><last>Farsi</last></author>
      <author><first>Shahriar</first><last>Shariati Motlagh</last></author>
      <author><first>Shayan</first><last>Bali</last></author>
      <author><first>Sadra</first><last>Sabouri</last></author>
      <author><first>Saeedeh</first><last>Momtazi</last></author>
      <pages>52–56</pages>
      <abstract>This study introduces a novel framework for evaluating Large Language Models (LLMs) and Vision-Language Models (VLMs) in Persian, a low-resource language. We develop comprehensive datasets to assess reasoning, linguistic understanding, and multimodal capabilities. Our datasets include Persian-OCR-QA for optical character recognition, Persian-VQA for visual question answering, Persian world-image puzzle for multimodal integration, Visual-Abstraction-Reasoning for abstract reasoning, and Iran-places for visual knowledge of Iranian figures and locations. We evaluate models like GPT-4o, Claude 3.5 Sonnet, and Llama 3.2 90B Vision, revealing their strengths and weaknesses in processing Persian. This research contributes to inclusive language processing by addressing the unique challenges of low-resource language evaluation.</abstract>
      <url hash="4c02d993">2025.evalmg-1.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>T</fixed-case>aiwan<fixed-case>VQA</fixed-case>: A Benchmark for Visual Question Answering for <fixed-case>T</fixed-case>aiwanese Daily Life</title>
      <author><first>Hsin-Yi</first><last>Hsieh</last></author>
      <author><first>Shang Wei</first><last>Liu</last></author>
      <author><first>Chang Chih</first><last>Meng</last></author>
      <author><first>Shuo-Yueh</first><last>Lin</last></author>
      <author><first>Chen</first><last>Chien-Hua</last></author>
      <author><first>Hung-Ju</first><last>Lin</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>I-Chen</first><last>Wu</last></author>
      <pages>57–75</pages>
      <abstract>We introduce TaiwanVQA, a novel visual question answering benchmark designed to evaluate vision language models’ (VLMs) ability to recognize and reason about Taiwan-specific multimodal content.TaiwanVQA comprises 2,000 image-question pairs covering diverse topics relevant to Taiwanese culture and daily life. We categorize the questions into recognition and reasoning tasks, further sub-classifying reasoning questions based on the level of external knowledge required. We conduct extensive experiments on state-of-the-art VLMs, including GPT-4o, Llama-3.2, LLaVA, Qwen2-VL, and InternVL2 models. Our findings reveal significant limitations in current VLMs when handling culturally specific content. The performance gap widens between recognition tasks (top score 73.60%) and reasoning tasks (top score 49.80%), indicating challenges in cultural inference and contextual understanding.These results highlight the need for more culturally diverse training data and improved model architectures that can better integrate visual and textual information within specific cultural contexts. By providing TaiwanVQA, we aim to contribute to the development of more inclusive and culturally aware AI models, facilitating their deployment in diverse real-world settings. TaiwanVQA can be accessed on our GitHub page.</abstract>
      <url hash="f30e7eb6">2025.evalmg-1.6</url>
    </paper>
    <paper id="7">
      <title>Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types</title>
      <author><first>Neelabh</first><last>Sinha</last></author>
      <author><first>Vinija</first><last>Jain</last></author>
      <author><first>Aman</first><last>Chadha</last></author>
      <pages>76–94</pages>
      <abstract>Visual Question-Answering (VQA) has become key to user experience, particularly after improved generalization capabilities of Vision-Language Models (VLMs). But evaluating VLMs for an application requirement using a standardized framework in practical settings is still challenging. This paper aims to solve that using an end-to-end framework. We present VQA360 - a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, for a comprehensive evaluation. We also introduce GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments. Our experiments with state-of-the-art VLMs reveal that no single model excels universally, thus, making a right choice a key design decision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source models like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive strengths, while providing additional advantages. Our framework can also be extended to other tasks.</abstract>
      <url hash="bdabbf16">2025.evalmg-1.7</url>
    </paper>
  </volume>
</collection>
