<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.loresmt">
  <volume id="1" ingest-date="2022-10-06" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)</booktitle>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Chao-Hong</first><last>Liu</last></editor>
      <editor><first>Ekaterina</first><last>Vylomova</last></editor>
      <editor><first>Jade</first><last>Abbott</last></editor>
      <editor><first>Jonathan</first><last>Washington</last></editor>
      <editor><first>Nathaniel</first><last>Oco</last></editor>
      <editor><first>Tommi A</first><last>Pirinen</last></editor>
      <editor><first>Valentin</first><last>Malykh</last></editor>
      <editor><first>Varvara</first><last>Logacheva</last></editor>
      <editor><first>Xiaobing</first><last>Zhao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="4dd1c2e3">2022.loresmt-1</url>
      <venue>loresmt</venue>
    </meta>
    <frontmatter>
      <url hash="a3e428f8">2022.loresmt-1.0</url>
      <bibkey>loresmt-2022-technologies</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Very Low Resource Sentence Alignment: Luhya and <fixed-case>S</fixed-case>wahili</title>
      <author><first>Everlyn</first><last>Chimoto</last></author>
      <author><first>Bruce</first><last>Bassett</last></author>
      <pages>1–8</pages>
      <abstract>Language-agnostic sentence embeddings generated by pre-trained models such as LASER and LaBSE are attractive options for mining large datasets to produce parallel corpora for low-resource machine translation. We test LASER and LaBSE in extracting bitext for two related low-resource African languages: Luhya and Swahili. For this work, we created a new parallel set of nearly 8000 Luhya-English sentences which allows a new zero-shot test of LASER and LaBSE. We find that LaBSE significantly outperforms LASER on both languages. Both LASER and LaBSE however perform poorly at zero-shot alignment on Luhya, achieving just 1.5% and 22.0% successful alignments respectively (P@1 score). We fine-tune the embeddings on a small set of parallel Luhya sentences and show significant gains, improving the LaBSE alignment accuracy to 53.3%. Further, restricting the dataset to sentence embedding pairs with cosine similarity above 0.7 yielded alignments with over 85% accuracy.</abstract>
      <url hash="f4f6510b">2022.loresmt-1.1</url>
      <bibkey>chimoto-bassett-2022-low</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/bucc">BUCC</pwcdataset>
    </paper>
    <paper id="2">
      <title>Multiple Pivot Languages and Strategic Decoder Initialization Helps Neural Machine Translation</title>
      <author><first>Shivam</first><last>Mhaskar</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>9–14</pages>
      <abstract>In machine translation, a pivot language can be used to assist the source to target translation model. In pivot-based transfer learning, the source to pivot and the pivot to target models are used to improve the performance of the source to target model. This technique works best when both source-pivot and pivot-target are high resource language pairs and the source-target is a low resource language pair. But in some cases, such as Indic languages, the pivot to target language pair is not a high resource one. To overcome this limitation, we use multiple related languages as pivot languages to assist the source to target model. We show that using multiple pivot languages gives 2.03 BLEU and 3.05 chrF score improvement over the baseline model. We show that strategic decoder initialization while performing pivot-based transfer learning with multiple pivot languages gives a 3.67 BLEU and 5.94 chrF score improvement over the baseline model.</abstract>
      <url hash="dd56a049">2022.loresmt-1.2</url>
      <bibkey>mhaskar-bhattacharyya-2022-multiple</bibkey>
    </paper>
    <paper id="3">
      <title>Known Words Will Do: Unknown Concept Translation via Lexical Relations</title>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <pages>15–22</pages>
      <abstract>Translating into low-resource languages is challenging due to the scarcity of training data. In this paper, we propose a probabilistic lexical translation method that bridges through lexical relations including synonyms, hypernyms, hyponyms, and co-hyponyms. This method, which only requires a dictionary like Wiktionary and a lexical database like WordNet, enables the translation of unknown vocabulary into low-resource languages for which we may only know the translation of a related concept. Experiments on translating a core vocabulary set into 472 languages, most of them low-resource, show the effectiveness of our approach.</abstract>
      <url hash="a2f7ca37">2022.loresmt-1.3</url>
      <bibkey>wu-yarowsky-2022-known</bibkey>
    </paper>
    <paper id="4">
      <title>The Only Chance to Understand: Machine Translation of the Severely Endangered Low-resource Languages of Eurasia</title>
      <author><first>Anna</first><last>Mosolova</last></author>
      <author><first>Kamel</first><last>Smaili</last></author>
      <pages>23–34</pages>
      <abstract>Numerous machine translation systems have been proposed since the appearance of this task. Nowadays, new large language model-based algorithms show results that sometimes overcome human ones on the rich-resource languages. Nevertheless, it is still not the case for the low-resource languages, for which all these algorithms did not show equally impressive results. In this work, we want to compare 3 generations of machine translation models on 7 low-resource languages and make a step further by proposing a new way of automatic parallel data augmentation using the state-of-the-art generative model.</abstract>
      <url hash="bee778c2">2022.loresmt-1.4</url>
      <bibkey>mosolova-smaili-2022-chance</bibkey>
    </paper>
    <paper id="5">
      <title>Data-adaptive Transfer Learning for Translation: A Case Study in <fixed-case>H</fixed-case>aitian and Jamaican</title>
      <author><first>Nathaniel</first><last>Robinson</last></author>
      <author><first>Cameron</first><last>Hogan</last></author>
      <author><first>Nancy</first><last>Fulda</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <pages>35–42</pages>
      <abstract>Multilingual transfer techniques often improve low-resource machine translation (MT). Many of these techniques are applied without considering data characteristics. We show in the context of Haitian-to-English translation that transfer effectiveness is correlated with amount of training data and relationships between knowledge-sharing languages. Our experiments suggest that for some languages beyond a threshold of authentic data, back-translation augmentation methods are counterproductive, while cross-lingual transfer from a sufficiently related language is preferred. We complement this finding by contributing a rule-based French-Haitian orthographic and syntactic engine and a novel method for phonological embedding. When used with multilingual techniques, orthographic transformation makes statistically significant improvements over conventional methods. And in very low-resource Jamaican MT, code-switching with a transfer language for orthographic resemblance yields a 6.63 BLEU point advantage.</abstract>
      <url hash="1138bc6b">2022.loresmt-1.5</url>
      <bibkey>robinson-etal-2022-data</bibkey>
    </paper>
    <paper id="6">
      <title>Augmented Bio-<fixed-case>SBERT</fixed-case>: Improving Performance for Pairwise Sentence Tasks in Bio-medical Domain</title>
      <author><first>Sonam</first><last>Pankaj</last></author>
      <author><first>Amit</first><last>Gautam</last></author>
      <pages>43–47</pages>
      <abstract>One of the modern challenges in AI is the access to high-quality and annotated data, especially in NLP; that is why augmentation is gaining importance. In computer vision, where image data augmentation is standard, text data augmentation in NLP is complex due to the high complexity of language. Moreover, we have seen the advantages of augmentation where there are fewer data available, which can significantly improve the model’s accuracy and performance. We have implemented Augmentation in Pairwise sentence scoring in the biomedical domain. By experimenting with our approach to downstream tasks on biomedical data, we have looked into the solution to improve Bi-encoders’ sentence transformer performance using an augmented dataset generated by cross-encoders fine-tuned on Biosses and MedNLI on the pre-trained Bio-BERT model. It has significantly improved the results with respect to the model only trained on Gold data for the respective tasks.</abstract>
      <url hash="ddbad567">2022.loresmt-1.6</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ed69adc3">2022.loresmt-1.6.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>pankaj-gautam-2022-augmented</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/biosses">BIOSSES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
    </paper>
    <paper id="7">
      <title>Machine Translation for a Very Low-Resource Language - Layer Freezing Approach on Transfer Learning</title>
      <author><first>Amartya</first><last>Chowdhury</last></author>
      <author><first>Deepak</first><last>K. T.</last></author>
      <author><first>Samudra Vijaya</first><last>K</last></author>
      <author><first>S. R. Mahadeva</first><last>Prasanna</last></author>
      <pages>48–55</pages>
      <abstract>This paper presents the implementation of Machine Translation (MT) between Lambani, a low-resource Indian tribal language, and English, a high-resource universal language. Lambani is spoken by nomadic tribes of the Indian state of Karnataka and there are similarities between Lambani and various other Indian languages. To implement the English-Lambani MT system, we followed the transfer learning approach with English-Kannada as the parent MT model. The implementation and performance of the English-Lambani MT system are discussed in this paper. Since Lambani has been influenced by various other languages, we explored the possibility of getting better MT performance by using parent models associated with related Indian languages. Specifically, we experimented with English-Gujarati and English-Marathi as additional parent models. We compare the performance of three different English-Lambani MT systems derived from three parent language models, and the observations are presented in the paper. Additionally, we will also explore the effect of freezing the encoder layer and decoder layer and the change in performance from both of them.</abstract>
      <url hash="7310cc2c">2022.loresmt-1.7</url>
      <bibkey>chowdhury-etal-2022-machine</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>HFT</fixed-case>: High Frequency Tokens for Low-Resource <fixed-case>NMT</fixed-case></title>
      <author><first>Edoardo</first><last>Signoroni</last></author>
      <author><first>Pavel</first><last>Rychlý</last></author>
      <pages>56–63</pages>
      <abstract>Tokenization has been shown to impact the quality of downstream tasks, such as Neural Machine Translation (NMT), which is susceptible to out-of-vocabulary words and low frequency training data. Current state-of-the-art algorithms have been helpful in addressing the issues of out-of-vocabulary words, bigger vocabulary sizes and token frequency by implementing subword segmentation. We argue, however, that there is still room for improvement, in particular regarding low-frequency tokens in the training data. In this paper, we present “High Frequency Tokenizer”, or HFT, a new language-independent subword segmentation algorithm that addresses this issue. We also propose a new metric to measure the frequency coverage of a tokenizer’s vocabulary, based on a frequency rank weighted average of the frequency values of its items. We experiment with a diverse set of language corpora, vocabulary sizes, and writing systems and report improvements on both frequency statistics and on the average length of the output. We also observe a positive impact on downstream NMT.</abstract>
      <url hash="8ffc3343">2022.loresmt-1.8</url>
      <bibkey>signoroni-rychly-2022-hft</bibkey>
      <pwccode url="https://github.com/edoardosignoroni/hftoks-eval" additional="false">edoardosignoroni/hftoks-eval</pwccode>
    </paper>
    <paper id="9">
      <title><fixed-case>R</fixed-case>omanian Language Translation in the <fixed-case>RELATE</fixed-case> Platform</title>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Maria</first><last>Mitrofan</last></author>
      <author><first>Andrei-Marius</first><last>Avram</last></author>
      <pages>64–74</pages>
      <abstract>This paper presents the usage of the RELATE platform for translation tasks involving the Romanian language. Using this platform, it is possible to perform text and speech data translations, either for single documents or for entire corpora. Furthermore, the platform was successfully used in international projects to create new resources useful for Romanian language translation.</abstract>
      <url hash="9c51b831">2022.loresmt-1.9</url>
      <bibkey>pais-etal-2022-romanian-language</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rtasc">RTASC</pwcdataset>
    </paper>
    <paper id="10">
      <title>Translating <fixed-case>S</fixed-case>panish into <fixed-case>S</fixed-case>panish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage: Combining Rules and Data-driven Approaches</title>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <author><first>Euan</first><last>McGill</last></author>
      <author><first>Santiago</first><last>Egea-Gómez</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>75–83</pages>
      <abstract>This paper presents a series of experiments on translating between spoken Spanish and Spanish Sign Language glosses (LSE), including enriching Neural Machine Translation (NMT) systems with linguistic features, and creating synthetic data to pretrain and later on finetune a neural translation model. We found evidence that pretraining over a large corpus of LSE synthetic data aligned to Spanish sentences could markedly improve the performance of the translation models.</abstract>
      <url hash="9ef11686">2022.loresmt-1.10</url>
      <bibkey>chiruzzo-etal-2022-translating</bibkey>
    </paper>
    <paper id="11">
      <title>Benefiting from Language Similarity in the Multilingual <fixed-case>MT</fixed-case> Training: Case Study of <fixed-case>I</fixed-case>ndonesian and <fixed-case>M</fixed-case>alaysian</title>
      <author><first>Alberto</first><last>Poncelas</last></author>
      <author><first>Johanes</first><last>Effendi</last></author>
      <pages>84–92</pages>
      <abstract>The development of machine translation (MT) has been successful in breaking the language barrier of the world’s top 10-20 languages. However, for the rest of it, delivering an acceptable translation quality is still a challenge due to the limited resource. To tackle this problem, most studies focus on augmenting data while overlooking the fact that we can borrow high-quality natural data from the closely-related language. In this work, we propose an MT model training strategy by increasing the language directions as a means of augmentation in a multilingual setting. Our experiment result using Indonesian and Malaysian on the state-of-the-art MT model showcases the effectiveness and robustness of our method.</abstract>
      <url hash="e559b7c4">2022.loresmt-1.11</url>
      <bibkey>poncelas-effendi-2022-benefiting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccmatrix">CCMatrix</pwcdataset>
    </paper>
    <paper id="12">
      <title>A Preordered <fixed-case>RNN</fixed-case> Layer Boosts Neural Machine Translation in Low Resource Settings</title>
      <author><first>Mohaddeseh</first><last>Bastan</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <pages>93–98</pages>
      <abstract>Neural Machine Translation (NMT) models are strong enough to convey semantic and syntactic information from the source language to the target language. However, these models are suffering from the need for a large amount of data to learn the parameters. As a result, for languages with scarce data, these models are at risk of underperforming. We propose to augment attention based neural network with reordering information to alleviate the lack of data. This augmentation improves the translation quality for both English to Persian and Persian to English by up to 6% BLEU absolute over the baseline models.</abstract>
      <url hash="66c6850c">2022.loresmt-1.12</url>
      <bibkey>bastan-khadivi-2022-preordered</bibkey>
    </paper>
    <paper id="13">
      <title>Exploring Word Alignment towards an Efficient Sentence Aligner for <fixed-case>F</fixed-case>ilipino and <fixed-case>C</fixed-case>ebuano Languages</title>
      <author><first>Jenn Leana</first><last>Fernandez</last></author>
      <author><first>Kristine Mae M.</first><last>Adlaon</last></author>
      <pages>99–106</pages>
      <abstract>Building a robust machine translation (MT) system requires a large amount of parallel corpus which is an expensive resource for low-resourced languages. The two major languages being spoken in the Philippines which are Filipino and Cebuano have an abundance in monolingual data that this study took advantage of attempting to find the best way to automatically generate parallel corpus out from monolingual corpora through the use of bitext alignment. Byte-pair encoding was applied in an attempt to optimize the alignment of the source and target texts. Results have shown that alignment was best achieved without segmenting the tokens. Itermax alignment score is best for short-length sentences and match or argmax alignment score are best for long-length sentences.</abstract>
      <url hash="e436249e">2022.loresmt-1.13</url>
      <bibkey>fernandez-adlaon-2022-exploring</bibkey>
    </paper>
    <paper id="14">
      <title>Aligning Word Vectors on Low-Resource Languages with <fixed-case>W</fixed-case>iktionary</title>
      <author><first>Mike</first><last>Izbicki</last></author>
      <pages>107–117</pages>
      <abstract>Aligned word embeddings have become a popular technique for low-resource natural language processing. Most existing evaluation datasets are generated automatically from machine translations systems, so they have many errors and exist only for high-resource languages. We introduce the Wiktionary bilingual lexicon collection, which provides high-quality human annotated translations for words in 298 languages to English. We use these lexicons to train and evaluate the largest published collection of aligned word embeddings on 157 different languages. All of our code and data is publicly available at <url>https://github.com/mikeizbicki/wiktionary_bli</url>.</abstract>
      <url hash="a24ede4d">2022.loresmt-1.14</url>
      <bibkey>izbicki-2022-aligning</bibkey>
      <pwccode url="https://github.com/mikeizbicki/wiktionary_bli" additional="false">mikeizbicki/wiktionary_bli</pwccode>
    </paper>
  </volume>
</collection>
