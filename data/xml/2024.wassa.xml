<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.wassa">
  <volume id="1" ingest-date="2024-07-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis</booktitle>
      <editor><first>Orphée</first><last>De Clercq</last></editor>
      <editor><first>Valentin</first><last>Barriere</last></editor>
      <editor><first>Jeremy</first><last>Barnes</last></editor>
      <editor><first>Roman</first><last>Klinger</last></editor>
      <editor><first>João</first><last>Sedoc</last></editor>
      <editor><first>Shabnam</first><last>Tafreshi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="9f3b506e">2024.wassa-1</url>
      <venue>wassa</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="a1b3c826">2024.wassa-1.0</url>
      <bibkey>wassa-2024-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Enhanced Financial Sentiment Analysis and Trading Strategy Development Using Large Language Models</title>
      <author><first>Kemal</first><last>Kirtac</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Guido</first><last>Germano</last><affiliation>University College London, University of London</affiliation></author>
      <pages>1-10</pages>
      <abstract>This study examines a novel methodology for enhanced financial sentiment analysis and trading strategy development using large language models (LLMs) such as OPT, BERT, FinBERT, LLAMA 3, and RoBERTa. Utilizing a dataset of 965,375 U.S. financial news articles from 2010 to 2023, our research demonstrates that the GPT-3-based OPT significantly outperforms other models, achieving a prediction accuracy of 74.4% for stock market returns. Our findings reveal that the advanced capabilities of LLMs, particularly OPT, surpass traditional sentiment analysis methods such as the Loughran-McDonald dictionary model in predicting and explaining stock returns. For instance, a self-financing strategy based on OPT scores achieves a Sharpe ratio of 3.05 over our sample period, compared to a Sharpe ratio of 1.23 for the strategy based on the dictionary model. This study highlights the superior performance of LLMs in financial sentiment analysis, encouraging further research into integrating artificial intelligence and LLMs in financial markets.</abstract>
      <url hash="1b893b00">2024.wassa-1.1</url>
      <bibkey>kirtac-germano-2024-enhanced</bibkey>
      <doi>10.18653/v1/2024.wassa-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>SEC</fixed-case>: Context-Aware Metric Learning for Efficient Emotion Recognition in Conversation</title>
      <author><first>Barbara</first><last>Gendron</last><affiliation>University of Lorraine</affiliation></author>
      <author><first>Gaël</first><last>Guibon</last><affiliation>University of Lorraine</affiliation></author>
      <pages>11-22</pages>
      <abstract>The advent of deep learning models has made a considerable contribution to the achievement of Emotion Recognition in Conversation (ERC). However, this task still remains an important challenge due to the plurality and subjectivity of human emotions. Previous work on ERC provides predictive models using mostly graph-based conversation representations. In this work, we propose a way to model the conversational context that we incorporate into a metric learning training strategy, with a two-step process. This allows us to perform ERC in a flexible classification scenario and end up with a lightweight yet efficient model. Using metric learning through a Siamese Network architecture, we achieve 57.71 in macro F1 score for emotion classification in conversation on DailyDialog dataset, which outperforms the related work. This state-of-the-art result is promising in terms of the use of metric learning for emotion recognition, yet perfectible compared to the micro F1 score obtained.</abstract>
      <url hash="a4798b58">2024.wassa-1.2</url>
      <bibkey>gendron-guibon-2024-wassa</bibkey>
      <doi>10.18653/v1/2024.wassa-1.2</doi>
    </paper>
    <paper id="3">
      <title>Modeling Complex Interactions in Long Documents for Aspect-Based Sentiment Analysis</title>
      <author><first>Zehong</first><last>Yan</last></author>
      <author><first>Wynne</first><last>Hsu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Mong-Li</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>David</first><last>Bartram-Shaw</last></author>
      <pages>23-34</pages>
      <abstract>The growing number of online articles and reviews necessitates innovative techniques for document-level aspect-based sentiment analysis. Capturing the context in which an aspect is mentioned is crucial. Existing models have focused on relatively short reviews and may fail to consider distant contextual information. This is especially so in longer documents where an aspect may be referred to in multiple ways across dispersed sentences. This work introduces a hierarchical Transformer-based architecture that encodes information at different level of granularities with attention aggregation mechanisms to learn the local and global aspect-specific document representations. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues. Experimental results show that the proposed architecture outperforms state-of-the-art methods for document-level aspect-based sentiment classification. We also demonstrate the potential applicability of our approach for long document trust prediction.</abstract>
      <url hash="b0af216c">2024.wassa-1.3</url>
      <bibkey>yan-etal-2024-modeling</bibkey>
      <doi>10.18653/v1/2024.wassa-1.3</doi>
    </paper>
    <paper id="4">
      <title>Hierarchical Adversarial Correction to Mitigate Identity Term Bias in Toxicity Detection</title>
      <author><first>Johannes</first><last>Schäfer</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Ulrich</first><last>Heid</last><affiliation>Universität Hildesheim and Universität Stuttgart</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>35-51</pages>
      <abstract>Corpora that are the fundament for toxicity detection contain such expressions typically directed against a target individual or group, e.g., people of a specific gender or ethnicity. Prior work has shown that the target identity mention can constitute a confounding variable. As an example, a model might learn that Christians are always mentioned in the context of hate speech. This misguided focus can lead to a limited generalization to newly emerging targets that are not found in the training data. In this paper, we hypothesize and subsequently show that this issue can be mitigated by considering targets on different levels of specificity. We distinguish levels of (1) the existence of a target, (2) a class (e.g., that the target is a religious group), or (3) a specific target group (e.g., Christians or Muslims). We define a target label hierarchy based on these three levels and then exploit this hierarchy in an adversarial correction for the lowest level (i.e. (3)) while maintaining some basic target features. This approach does not lower the toxicity detection performance but increases the generalization to targets not being available at training time.</abstract>
      <url hash="f87655b7">2024.wassa-1.4</url>
      <bibkey>schafer-etal-2024-hierarchical</bibkey>
      <doi>10.18653/v1/2024.wassa-1.4</doi>
    </paper>
    <paper id="5">
      <title>A Systematic Analysis on the Temporal Generalization of Language Models in Social Media</title>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <pages>52-62</pages>
      <abstract>In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time, and they can become obsolete due to the dynamism and evolving nature of online content. This paper focuses on temporal shifts in social media and, in particular, Twitter. We propose a unified evaluation scheme to assess the performance of language models (LMs) under temporal shift on standard social media tasks. LMs are tested on five diverse social media NLP tasks under different temporal settings, which revealed two important findings: (i) the decrease in performance under temporal shift is consistent across different models for entity-focused tasks such as named entity recognition or disambiguation, and hate speech detection, but not significant in the other tasks analysed (i.e., topic and sentiment classification); and (ii) continuous pre-training on the test period does not improve the temporal adaptability of LMs.</abstract>
      <url hash="a2a38e46">2024.wassa-1.5</url>
      <bibkey>ushio-camacho-collados-2024-systematic</bibkey>
      <doi>10.18653/v1/2024.wassa-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>-Based Models for Aspect-Based Sentiment Analysis</title>
      <author><first>Jakub</first><last>Šmíd</last><affiliation>University of West Bohemia</affiliation></author>
      <author><first>Pavel</first><last>Priban</last><affiliation>University of West Bohemia</affiliation></author>
      <author><first>Pavel</first><last>Kral</last><affiliation>University of West Bohemia</affiliation></author>
      <pages>63-70</pages>
      <abstract>While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca 2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models.</abstract>
      <url hash="8cd2e8b1">2024.wassa-1.6</url>
      <bibkey>smid-etal-2024-llama</bibkey>
      <doi>10.18653/v1/2024.wassa-1.6</doi>
    </paper>
    <paper id="7">
      <title>A Multi-Faceted <fixed-case>NLP</fixed-case> Analysis of Misinformation Spreaders in <fixed-case>T</fixed-case>witter</title>
      <author><first>Dimosthenis</first><last>Antypas</last></author>
      <author><first>Alun</first><last>Preece</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <pages>71-83</pages>
      <abstract>Social media is an integral part of the daily life of an increasingly large number of people worldwide. Used for entertainment, communication and news updates, it constitutes a source of information that has been extensively used to study human behaviour. Unfortunately, the open nature of social media platforms along with the difficult task of supervising their content has led to a proliferation of misinformation posts. In this paper, we aim to identify the textual differences between the profiles of user that share misinformation from questionable sources and those that do not. Our goal is to better understand user behaviour in order to be better equipped to combat this issue. To this end, we identify Twitter (X) accounts of potential misinformation spreaders and apply transformer models specialised in social media to extract characteristics such as sentiment, emotion, topic and presence of hate speech. Our results indicate that, while there may be some differences between the behaviour of users that share misinformation and those that do not, there are no large differences when it comes to the type of content shared.</abstract>
      <url hash="ad769db3">2024.wassa-1.7</url>
      <bibkey>antypas-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.wassa-1.7</doi>
    </paper>
    <paper id="8">
      <title>Entity-Level Sentiment: More than the Sum of Its Parts</title>
      <author><first>Egil</first><last>Rønningstad</last></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept. of Informatics, University of Oslo</affiliation></author>
      <author><first>Erik</first><last>Velldal</last><affiliation>University of Oslo</affiliation></author>
      <pages>84-96</pages>
      <abstract>In sentiment analysis of longer texts, there may be a variety of topics discussed, of entities mentioned, and of sentiments expressed regarding each entity. We find a lack of studies exploring how such texts express their sentiment towards each entity of interest, and how these sentiments can be modelled. In order to better understand how sentiment regarding persons and organizations (each entity in our scope) is expressed in longer texts, we have collected a dataset of expert annotations where the overall sentiment regarding each entity is identified, together with the sentence-level sentiment for these entities separately. We show that the reader’s perceived sentiment regarding an entity often differs from an arithmetic aggregation of sentiments at the sentence level. Only 70% of the positive and 55% of the negative entities receive a correct overall sentiment label when we aggregate the (human-annotated) sentiment labels for the sentences where the entity is mentioned. Our dataset reveals the complexity of entity-specific sentiment in longer texts, and allows for more precise modelling and evaluation of such sentiment expressions.</abstract>
      <url hash="6cd290db">2024.wassa-1.8</url>
      <bibkey>ronningstad-etal-2024-entity</bibkey>
      <doi>10.18653/v1/2024.wassa-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>MBIAS</fixed-case>: Mitigating Bias in Large Language Models While Retaining Context</title>
      <author><first>Shaina</first><last>Raza</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Ananya</first><last>Raval</last><affiliation>Vector Institute</affiliation></author>
      <author><first>Veronica</first><last>Chatrath</last></author>
      <pages>97-111</pages>
      <abstract>The deployment of Large Language Models (LLMs) in diverse applications necessitates an assurance of safety without compromising the contextual integrity of the generated content. Traditional approaches, including safety-specific fine-tuning or adversarial testing, often yield safe outputs at the expense of contextual meaning. This can result in a diminished capacity to handle nuanced aspects of bias and toxicity, such as underrepresentation or negative portrayals across various demographics. To address these challenges, we introduce MBIAS, an LLM framework carefully instruction fine-tuned on a custom dataset designed specifically for safety interventions. MBIAS is designed to significantly reduce biases and toxic elements in LLM outputs while preserving the main information. This work also details our further use of LLMs: as annotator under human supervision and as evaluator of generated content. Empirical analysis reveals that MBIAS achieves a reduction in bias and toxicity by over 30% in standard evaluations, and by more than 90% in diverse demographic tests, highlighting the robustness of our approach. We make the dataset and the fine-tuned MBIAS model available to the research community for further investigation and to ensure reproducibility. The code for this project can be accessed here https://github.com/shainarazavi/MBIAS.</abstract>
      <url hash="4615c9e2">2024.wassa-1.9</url>
      <bibkey>raza-etal-2024-mbias</bibkey>
      <doi>10.18653/v1/2024.wassa-1.9</doi>
    </paper>
    <paper id="10">
      <title>Polarization of Autonomous Generative <fixed-case>AI</fixed-case> Agents Under Echo Chambers</title>
      <author><first>Masaya</first><last>Ohagi</last><affiliation>LINE Yahoo Corporation</affiliation></author>
      <pages>112-124</pages>
      <abstract>Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs.An echo chamber often generates polarization, leading to conflicts between people with radical opinions.The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group’s opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is caused by ChatGPT’s high prompt understanding ability to update its opinion by considering its own and surrounding agents’ opinions. We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize. As a result, we identified factors that influence polarization, such as the agent’s persona.</abstract>
      <url hash="a43347ed">2024.wassa-1.10</url>
      <bibkey>ohagi-2024-polarization</bibkey>
      <doi>10.18653/v1/2024.wassa-1.10</doi>
    </paper>
    <paper id="11">
      <title>Know Thine Enemy: Adaptive Attacks on Misinformation Detection Using Reinforcement Learning</title>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <author><first>Euan</first><last>McGill</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>Horacio</first><last>Saggion</last><affiliation>Universitat Pompeu Fabra and Universitat Pompeu Fabra</affiliation></author>
      <pages>125-140</pages>
      <abstract>We present XARELLO: a generator of adversarial examples for testing the robustness of text classifiers based on reinforcement learning. Our solution is adaptive, it learns from previous successes and failures in order to better adjust to the vulnerabilities of the attacked model. This reflects the behaviour of a persistent and experienced attacker, which are common in the misinformation-spreading environment. We evaluate our approach using several victim classifiers and credibility-assessment tasks, showing it generates better-quality examples with less queries, and is especially effective against the modern LLMs. We also perform a qualitative analysis to understand the language patterns in the misinformation text that play a role in the attacks.</abstract>
      <url hash="4895ae7d">2024.wassa-1.11</url>
      <bibkey>przybyla-etal-2024-know</bibkey>
      <doi>10.18653/v1/2024.wassa-1.11</doi>
    </paper>
    <paper id="12">
      <title>The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models</title>
      <author><first>Xiliang</first><last>Zhu</last><affiliation>Dialpad Inc.</affiliation></author>
      <author><first>Shayna</first><last>Gardiner</last></author>
      <author><first>Tere</first><last>Roldán</last></author>
      <author><first>David</first><last>Rossouw</last></author>
      <pages>141-152</pages>
      <abstract>Sentiment analysis serves as a pivotal component in Natural Language Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R and mT5 have contributed to the increasing interest in cross-lingual sentiment analysis. The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied. This work undertakes an empirical analysis to compare the cross-lingual transfer capability of public Small Multilingual Language Models (SMLM) like XLM-R, against English-centric LLMs such as Llama-3, in the context of sentiment analysis across English, Spanish, French and Chinese. Our findings reveal that among public models, SMLMs exhibit superior zero-shot cross-lingual performance relative to LLMs. However, in few-shot cross-lingual settings, public LLMs demonstrate an enhanced adaptive potential. In addition, we observe that proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but are outpaced by public models in few-shot scenarios.</abstract>
      <url hash="1df586eb">2024.wassa-1.12</url>
      <bibkey>zhu-etal-2024-model</bibkey>
      <doi>10.18653/v1/2024.wassa-1.12</doi>
    </paper>
    <paper id="13">
      <title>Guiding Sentiment Analysis with Hierarchical Text Clustering: Analyzing the <fixed-case>G</fixed-case>erman <fixed-case>X</fixed-case>/<fixed-case>T</fixed-case>witter Discourse on Face Masks in the 2020 <fixed-case>COVID</fixed-case>-19 Pandemic</title>
      <author><first>Silvan</first><last>Wehrli</last></author>
      <author><first>Chisom</first><last>Ezekannagha</last><affiliation>Robert Koch Institute</affiliation></author>
      <author><first>Georges</first><last>Hattab</last><affiliation>Centre for Artificial Intelligence in Public Health Research (ZKI-PH), Robert Koch-Institute</affiliation></author>
      <author><first>Tamara</first><last>Boender</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Bert</first><last>Arnrich</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Christopher</first><last>Irrgang</last><affiliation>Robert Koch Institute</affiliation></author>
      <pages>153-167</pages>
      <abstract>Social media are a critical component of the information ecosystem during public health crises. Understanding the public discourse is essential for effective communication and misinformation mitigation. Computational methods can aid these efforts through online social listening. We combined hierarchical text clustering and sentiment analysis to examine the face mask-wearing discourse in Germany during the COVID-19 pandemic using a dataset of 353,420 German X (formerly Twitter) posts from 2020. For sentiment analysis, we annotated a subsample of the data to train a neural network for classifying the sentiments of posts (neutral, negative, or positive). In combination with clustering, this approach uncovered sentiment patterns of different topics and their subtopics, reflecting the online public response to mask mandates in Germany. We show that our approach can be used to examine long-term narratives and sentiment dynamics and to identify specific topics that explain peaks of interest in the social media discourse.</abstract>
      <url hash="6d7c53cf">2024.wassa-1.13</url>
      <bibkey>wehrli-etal-2024-guiding</bibkey>
      <doi>10.18653/v1/2024.wassa-1.13</doi>
    </paper>
    <paper id="14">
      <title>Emotion Identification for <fixed-case>F</fixed-case>rench in Written Texts: Considering Modes of Emotion Expression as a Step Towards Text Complexity Analysis</title>
      <author><first>Aline</first><last>Étienne</last></author>
      <author><first>Delphine</first><last>Battistelli</last><affiliation>Université Paris Nanterre</affiliation></author>
      <author><first>Gwénolé</first><last>Lecorvé</last><affiliation>Orange</affiliation></author>
      <pages>168-185</pages>
      <abstract>The objective of this paper is to predict (A) whether a sentence in a written text expresses an emotion, (B) the mode(s) in which the emotion is expressed, (C) whether it is basic or complex, and (D) its emotional category.One of our major contributions, in addition to a dataset and a model, is to integrate the fact that an emotion can be expressed in different modes: from a direct mode, essentially lexicalized, to a more indirect mode, where emotions will only be suggested, a mode that NLP approaches generally don’t take into account. The scope is on written texts, i.e. it does not focus on conversational or multi-modal data. In this context, modes of expression are seen as a factor towards the automatic analysis of complexity in texts.Experiments on French texts show acceptable results compared to the human annotators’ agreement to predict the mode and category, and outperforming results compared to using a large language model with in-context learning (i.e. no fine-tuning) on all tasks.Dataset and model can be downloaded on HuggingFace: https://huggingface.co/TextToKids .</abstract>
      <url hash="aa7a467c">2024.wassa-1.14</url>
      <bibkey>etienne-etal-2024-emotion</bibkey>
      <doi>10.18653/v1/2024.wassa-1.14</doi>
    </paper>
    <paper id="15">
      <title>Comparing Tools for Sentiment Analysis of <fixed-case>D</fixed-case>anish Literature from Hymns to Fairy Tales: Low-Resource Language and Domain Challenges</title>
      <author><first>Pascale</first><last>Feldkamp</last></author>
      <author><first>Jan</first><last>Kostkan</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Ea</first><last>Overgaard</last></author>
      <author><first>Mia</first><last>Jacobsen</last></author>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <pages>186-199</pages>
      <abstract>While Sentiment Analysis has become increasingly central in computational approaches to literary texts, the literary domain still poses important challenges for the detection of textual sentiment due to its highly complex use of language and devices - from subtle humor to poetic imagery. Furthermore these challenges are only further amplified in low-resource language and domain settings. In this paper we investigate the application and efficacy of different Sentiment Analysis tools on Danish literary texts, using historical fairy tales and religious hymns as our datasets. The scarcity of linguistic resources for Danish and the historical context of the data further compounds the challenges for the tools. We compare human annotations to the continuous valence scores of both transformer- and dictionary-based Sentiment Analysis methods to assess their performance, seeking to understand how distinct methods handle the language of Danish prose and poetry.</abstract>
      <url hash="16543f96">2024.wassa-1.15</url>
      <bibkey>feldkamp-etal-2024-comparing</bibkey>
      <doi>10.18653/v1/2024.wassa-1.15</doi>
    </paper>
    <paper id="16">
      <title>Multi-Target User Stance Discovery on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Benjamin</first><last>Steel</last><affiliation>McGill University, McGill University</affiliation></author>
      <author><first>Derek</first><last>Ruths</last><affiliation>McGill University</affiliation></author>
      <pages>200-214</pages>
      <abstract>We consider how to credibly and reliably assess the opinions of individuals using their social media posts. To this end, this paper makes three contributions. First, we assemble a workflow and approach to applying modern natural language processing (NLP) methods to multi-target user stance detection in the wild. Second, we establish why the multi-target modeling of user stance is qualitatively more complicated than uni-target user-stance detection. Finally, we validate our method by showing how multi-dimensional measurement of user opinions not only reproduces known opinion polling results, but also enables the study of opinion dynamics at high levels of temporal and semantic resolution.</abstract>
      <url hash="a99a60cc">2024.wassa-1.16</url>
      <bibkey>steel-ruths-2024-multi</bibkey>
      <doi>10.18653/v1/2024.wassa-1.16</doi>
    </paper>
    <paper id="17">
      <title>Subjectivity Detection in <fixed-case>E</fixed-case>nglish News using Large Language Models</title>
      <author><first>Mohammad</first><last>Shokri</last></author>
      <author><first>Vivek</first><last>Sharma</last><affiliation>CUNY John Jay College of Criminal Justice and The Graduate Center, CUNY</affiliation></author>
      <author><first>Elena</first><last>Filatova</last><affiliation>CUNY City Tech</affiliation></author>
      <author><first>Shweta</first><last>Jain</last><affiliation>CUNY John Jay College of Criminal Justice</affiliation></author>
      <author><first>Sarah</first><last>Levitan</last><affiliation>CUNY Hunter College</affiliation></author>
      <pages>215-226</pages>
      <abstract>Trust in media has reached a historical low as consumers increasingly doubt the credibility of the news they encounter. This growing skepticism is exacerbated by the prevalence of opinion-driven articles, which can influence readers’ beliefs to align with the authors’ viewpoints. In response to this trend, this study examines the expression of opinions in news by detecting subjective and objective language. We conduct an analysis of the subjectivity present in various news datasets and evaluate how different language models detect subjectivity and generalize to out-of-distribution data. We also investigate the use of in-context learning (ICL) within large language models (LLMs) and propose a straightforward prompting method that outperforms standard ICL and chain-of-thought (CoT) prompts.</abstract>
      <url hash="812c07e4">2024.wassa-1.17</url>
      <bibkey>shokri-etal-2024-subjectivity</bibkey>
      <doi>10.18653/v1/2024.wassa-1.17</doi>
    </paper>
    <paper id="18">
      <title>Monitoring Depression Severity and Symptoms in User-Generated Content: An Annotation Scheme and Guidelines</title>
      <author><first>Falwah</first><last>Alhamed</last></author>
      <author><first>Rebecca</first><last>Bendayan</last></author>
      <author><first>Julia</first><last>Ive</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Lucia</first><last>Specia</last><affiliation>Imperial College London</affiliation></author>
      <pages>227-233</pages>
      <abstract>Depression is a highly prevalent condition recognized by the World Health Organization as a leading contributor to global disability. Many people suffering from depression express their thoughts and feelings using social media, which thus becomes a source of data for research in this domain. However, existing annotation schemes tailored to studying depression symptoms in social media data remain limited. Reliable and valid annotation guidelines are crucial for accurately measuring mental health conditions for those studies. This paper addresses this gap by presenting a novel depression annotation scheme and guidelines for detecting depression symptoms and their severity in social media text. Our approach leverages validated depression questionnaires and incorporates the expertise of psychologists and psychiatrists during scheme refinement. The resulting annotation scheme achieves high inter-rater agreement, demonstrating its potential for suitable depression assessment in social media contexts.</abstract>
      <url hash="558de98f">2024.wassa-1.18</url>
      <bibkey>alhamed-etal-2024-monitoring</bibkey>
      <doi>10.18653/v1/2024.wassa-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>R</fixed-case>ide<fixed-case>KE</fixed-case>: Leveraging Low-resource <fixed-case>T</fixed-case>witter User-generated Content for Sentiment and Emotion Detection on Code-switched <fixed-case>RHS</fixed-case> Dataset.</title>
      <author><first>Naome</first><last>Etori</last></author>
      <author><first>Maria</first><last>Gini</last><affiliation>University of Minnesota , Twin Ciities</affiliation></author>
      <pages>234-249</pages>
      <abstract>Social media has become a crucial open-access platform enabling individuals to freely express opinions and share experiences. These platforms contain user-generated content facilitating instantaneous communication and feedback. However, leveraging low-resource language data from Twitter can be challenging due to the scarcity and poor quality of content with significant variations in language use, such as slang and code-switching. Automatically identifying tweets in low-resource languages can also be challenging because Twitter primarily supports high-resource languages; low-resource languages often lack robust linguistic and contextual support. This paper analyzes Kenyan code-switched data from Twitter using four transformer-based pretrained models for sentiment and emotion classification tasks using supervised and semi-supervised methods. We detail the methodology behind data collection, the annotation procedure, and the challenges encountered during the data curation phase. Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2%) and F1 score (66.1%), XLM-R semi-supervised (67.2% accuracy, 64.1% F1 score). In emotion analysis, DistilBERT supervised leads in accuracy (59.8%) and F1 score (31%), mBERT semi-supervised (accuracy (59% and F1 score 26.5%). AfriBERTa models show the lowest accuracy and F1 scores. This indicates that the semi-supervised method’s performance is constrained by the small labeled dataset.</abstract>
      <url hash="0ff94ac1">2024.wassa-1.19</url>
      <bibkey>etori-gini-2024-rideke</bibkey>
      <doi>10.18653/v1/2024.wassa-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>POL</fixed-case>ygraph: <fixed-case>P</fixed-case>olish Fake News Dataset</title>
      <author><first>Daniel</first><last>Dzienisiewicz</last><affiliation>Adam Mickiewicz University of Poznan</affiliation></author>
      <author><first>Filip</first><last>Graliński</last><affiliation>Adam Mickiewicz University, Adam Mickiewicz University, Applica.ai and Applica.ai</affiliation></author>
      <author><first>Piotr</first><last>Jabłoński</last></author>
      <author><first>Marek</first><last>Kubis</last><affiliation>Adam Mickiewicz University of Poznan</affiliation></author>
      <author><first>Paweł</first><last>Skórzewski</last><affiliation>Adam Mickiewicz University of Poznan</affiliation></author>
      <author><first>Piotr</first><last>Wierzchon</last><affiliation>Adam mickiewicz University</affiliation></author>
      <pages>250-263</pages>
      <abstract>This paper presents the POLygraph dataset, a unique resource for fake news detection in Polish. The dataset, created by an interdisciplinary team, is composed of two parts: the “fake-or-not” dataset with 11,360 pairs of news articles (identified by their URLs) and corresponding labels, and the “fake-they-say” dataset with 5,082 news articles (identified by their URLs) and tweets commenting on them. Unlike existing datasets, POLygraph encompasses a variety of approaches from source literature, providing a comprehensive resource for fake news detection. The data was collected through manual annotation by expert and non-expert annotators. The project also developed a software tool that uses advanced machine learning techniques to analyze the data and determine content authenticity. The tool and dataset are expected to benefit various entities, from public sector institutions to publishers and fact-checking organizations. Further dataset exploration will foster fake news detection and potentially stimulate the implementation of similar models in other languages. The paper focuses on the creation and composition of the dataset, so it does not include a detailed evaluation of the software tool for content authenticity analysis, which is planned at a later stage of the project.</abstract>
      <url hash="aece5141">2024.wassa-1.20</url>
      <bibkey>dzienisiewicz-etal-2024-polygraph</bibkey>
      <doi>10.18653/v1/2024.wassa-1.20</doi>
    </paper>
    <paper id="21">
      <title>Exploring Language Models to Analyze Market Demand Sentiments from News</title>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <author><first>Manjira</first><last>Sinha</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>264-272</pages>
      <abstract>Obtaining demand trends for products is an essential aspect of supply chain planning. It helps in generating scenarios for simulation before actual demands start pouring in. Presently, experts obtain this number manually from different News sources. In this paper, we have presented methods that can automate the information acquisition process. We have presented a joint framework that performs information extraction and sentiment analysis to acquire demand related information from business text documents. The proposed system leverages a TwinBERT-based deep neural network model to first extract product information for which demand is associated and then identify the respective sentiment polarity. The articles are also subjected to causal analytics, that, together yield rich contextual information about reasons for rise or fall of demand of various products. The enriched information is targeted for the decision-makers, analysts and knowledge workers. We have exhaustively evaluated our proposed models with datasets curated and annotated for two different domains namely, automobile sector and housing. The proposed model outperforms the existing baseline systems.</abstract>
      <url hash="18e6e5b8">2024.wassa-1.21</url>
      <bibkey>dasgupta-sinha-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.wassa-1.21</doi>
    </paper>
    <paper id="22">
      <title>Impact of Decoding Methods on Human Alignment of Conversational <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shaz</first><last>Furniturewala</last></author>
      <author><first>Kokil</first><last>Jaidka</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yashvardhan</first><last>Sharma</last><affiliation>BITS Pilani</affiliation></author>
      <pages>273-279</pages>
      <abstract>To be included into chatbot systems, Large language models (LLMs) must be aligned with human conversational conventions. However, being trained mainly on web-scraped data gives existing LLMs a voice closer to informational text than actual human speech. In this paper, we examine the effect of decoding methods on the alignment between LLM-generated and human conversations, including Beam Search, Top K Sampling, and Nucleus Sampling. We present new measures of alignment in substance, style, and psychometric orientation, and experiment with two conversation datasets. Our results provide subtle insights: better alignment is attributed to fewer beams in Beam Search and lower values of P in Nucleus Sampling. We also find that task-oriented and open-ended datasets perform differently in terms of alignment, indicating the significance of taking into account the context of the interaction.</abstract>
      <url hash="666d4f20">2024.wassa-1.22</url>
      <bibkey>furniturewala-etal-2024-impact</bibkey>
      <doi>10.18653/v1/2024.wassa-1.22</doi>
    </paper>
    <paper id="23">
      <title>Loneliness Episodes: A <fixed-case>J</fixed-case>apanese Dataset for Loneliness Detection and Analysis</title>
      <author><first>Naoya</first><last>Fujikawa</last></author>
      <author><first>Nguyen</first><last>Toan</last></author>
      <author><first>Kazuhiro</first><last>Ito</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>280-293</pages>
      <abstract>Loneliness, a significant public health concern, is closely connected to both physical and mental well-being. Hence, detection and intervention for individuals experiencing loneliness are crucial. Identifying loneliness in text is straightforward when it is explicitly stated but challenging when it is implicit. Detecting implicit loneliness requires a manually annotated dataset because whereas explicit loneliness can be detected using keywords, implicit loneliness cannot be. However, there are no freely available datasets with clear annotation guidelines for implicit loneliness. In this study, we construct a freely accessible Japanese loneliness dataset with annotation guidelines grounded in the psychological definition of loneliness. This dataset covers loneliness intensity and the contributing factors of loneliness. We train two models to classify whether loneliness is expressed and the intensity of loneliness. The model classifying loneliness versus non-loneliness achieves an F1-score of 0.833, but the model for identifying the intensity of loneliness has a low F1-score of 0.400, which is likely due to label imbalance and a shortage of a certain label in the dataset. We validate performance in another domain, specifically X (formerly Twitter), and observe a decrease. In addition, we propose improvement suggestions for domain adaptation.</abstract>
      <url hash="c25ad59a">2024.wassa-1.23</url>
      <bibkey>fujikawa-etal-2024-loneliness</bibkey>
      <doi>10.18653/v1/2024.wassa-1.23</doi>
    </paper>
    <paper id="24">
      <title>Estimation of Happiness Changes through Longitudinal Analysis of Employees’ Texts</title>
      <author><first>Junko</first><last>Hayashi</last></author>
      <author><first>Kazuhiro</first><last>Ito</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Masae</first><last>Manabe</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yasushi</first><last>Watanabe</last><affiliation>Kyoto University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Masataka</first><last>Nakayama</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yukiko</first><last>Uchida</last></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>294-304</pages>
      <abstract>Measuring happiness as a determinant of well-being is increasingly recognized as crucial. While previous studies have utilized free-text descriptions to estimate happiness on a broad scale, limited research has focused on tracking individual fluctuations in happiness over time owing to the challenges associated with longitudinal data collection. This study addresses this issue by obtaining longitudinal data from two workplaces over two and six months respectively.Subsequently, the data is used to construct a happiness estimation model and assess individual happiness levels.Evaluation of the model performance using correlation coefficients shows variability in the correlation values among individuals.Notably, the model performs satisfactorily in estimating 9 of the 11 users’ happiness scores, with a correlation coefficient of 0.4 or higher. To investigate the factors affecting the model performance, we examine the relationship between the model performance and variables such as sentence length, lexical diversity, and personality traits. Correlations are observed between these features and model performance.</abstract>
      <url hash="362c4427">2024.wassa-1.24</url>
      <bibkey>hayashi-etal-2024-estimation</bibkey>
      <doi>10.18653/v1/2024.wassa-1.24</doi>
    </paper>
    <paper id="25">
      <title>Subjectivity Theory vs. Speaker Intuitions: Explaining the Results of a Subjectivity Regressor Trained on Native Speaker Judgements</title>
      <author><first>Elena</first><last>Savinova</last><affiliation>Radboud University</affiliation></author>
      <author><first>Jet</first><last>Hoek</last></author>
      <pages>305-315</pages>
      <abstract>In this paper, we address the issue of explainability in a transformer-based subjectivity regressor trained on native English speakers’ judgements. The main goal of this work is to test how the regressor’s predictions, and therefore native speakers’ intuitions, relate to theoretical accounts of subjectivity. We approach this goal using two methods: a top-down manual selection of theoretically defined subjectivity features and a bottom-up extraction of top subjective and objective features using the LIME explanation method. The explainability of the subjectivity regressor is evaluated on a British news dataset containing sentences taken from social media news posts and from articles on the websites of the same news outlets. Both methods provide converging evidence that theoretically defined subjectivity features, such as emoji, evaluative adjectives, exclamations, questions, intensifiers, and first person pronouns, are prominent predictors of subjectivity scores. Thus, our findings show that the predictions of the regressor, and therefore native speakers’ perceptions of subjectivity, align with subjectivity theory. However, an additional comparison of the effects of different subjectivity features in author text and the text of cited sources reveals that the distinction between author and source subjectivity might not be as salient for naïve speakers as it is in the theory.</abstract>
      <url hash="782828b1">2024.wassa-1.25</url>
      <bibkey>savinova-hoek-2024-subjectivity</bibkey>
      <doi>10.18653/v1/2024.wassa-1.25</doi>
    </paper>
    <paper id="26">
      <title>Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?</title>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>H. Andrew</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>316-328</pages>
      <abstract>Pre-trained language models consider the context of neighboring words and documents but lack any author context of the human generating the text. However, language depends on the author’s states, traits, social, situational, and environmental attributes, collectively referred to as human context (Soni et al., 2024). Human-centered natural language processing requires incorporating human context into language models. Currently, two methods exist: pre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2) individual traits. Group attributes are simple but coarse — not all 45-year-olds write the same way — while individual traits allow for more personalized representations, but require more complex modeling and data. It is unclear which approach benefits what tasks. We compare pre-training models with human context via 1) group attributes, 2) individual users, and 3) a combined approach on five user- and document-level tasks. Our results show that there is no best approach, but that human-centered language modeling holds avenues for different methods.</abstract>
      <url hash="e7a95994">2024.wassa-1.26</url>
      <bibkey>soni-etal-2024-comparing</bibkey>
      <doi>10.18653/v1/2024.wassa-1.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>LLM</fixed-case>s for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive Dilemma</title>
      <author><first>Jana</first><last>Juroš</last></author>
      <author><first>Laura</first><last>Majer</last></author>
      <author><first>Jan</first><last>Snajder</last><affiliation>UniZg-FER, University of Zagreb</affiliation></author>
      <pages>329-343</pages>
      <abstract>News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Due to its subjectivity, creating TSA datasets can involve various annotation paradigms, from descriptive to prescriptive, either encouraging or limiting subjectivity. LLMs are a good fit for TSA due to their broad linguistic and world knowledge and in-context learning abilities, yet their performance depends on prompt design. In this paper, we compare the accuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news headlines using descriptive and prescriptive datasets across several languages. Exploring the descriptive–prescriptive continuum, we analyze how performance is affected by prompt prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts. Finally, we evaluate the ability of LLMs to quantify uncertainty via calibration error and comparison to human label variation. We find that LLMs outperform fine-tuned encoders on descriptive datasets, while calibration and F1-score generally improve with increased prescriptiveness, yet the optimal level varies.</abstract>
      <url hash="234ea30a">2024.wassa-1.27</url>
      <bibkey>juros-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.wassa-1.27</doi>
    </paper>
    <paper id="28">
      <title>Context is Important in Depressive Language: A Study of the Interaction Between the Sentiments and Linguistic Markers in <fixed-case>R</fixed-case>eddit Discussions</title>
      <author><first>Neha</first><last>Sharma</last></author>
      <author><first>Kairit</first><last>Sirts</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <pages>344-361</pages>
      <abstract>Research exploring linguistic markers in individuals with depression has demonstrated that language usage can serve as an indicator of mental health. This study investigates the impact of discussion topic as context on linguistic markers and emotional expression in depression, using a Reddit dataset to explore interaction effects. Contrary to common findings, our sentiment analysis revealed a broader range of emotional intensity in depressed individuals, with both higher negative and positive sentiments than controls. This pattern was driven by posts containing no emotion words, revealing the limitations of the lexicon based approaches in capturing the full emotional context. We observed several interesting results demonstrating the importance of contextual analyses. For instance, the use of 1st person singular pronouns and words related to anger and sadness correlated with increased positive sentiments, whereas a higher rate of present-focused words was associated with more negative sentiments. Our findings highlight the importance of discussion contexts while interpreting the language used in depression, revealing that the emotional intensity and meaning of linguistic markers can vary based on the topic of discussion.</abstract>
      <url hash="72624ec8">2024.wassa-1.28</url>
      <bibkey>sharma-sirts-2024-context</bibkey>
      <doi>10.18653/v1/2024.wassa-1.28</doi>
    </paper>
    <paper id="29">
      <title>To Aggregate or Not to Aggregate. That is the Question: A Case Study on Annotation Subjectivity in Span Prediction</title>
      <author><first>Kemal</first><last>Kurniawan</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Meladel</first><last>Mistica</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>362-368</pages>
      <abstract>This paper explores the task of automatic prediction of text spans in a legal problem description that support a legal area label. We use a corpus of problem descriptions written by laypeople in English that is annotated by practising lawyers. Inherent subjectivity exists in our task because legal area categorisation is a complex task, and lawyers often have different views on a problem. Experiments show that training on majority-voted spans outperforms training on disaggregated ones.</abstract>
      <url hash="116ac031">2024.wassa-1.29</url>
      <bibkey>kurniawan-etal-2024-aggregate</bibkey>
      <doi>10.18653/v1/2024.wassa-1.29</doi>
    </paper>
    <paper id="30">
      <title>Findings of <fixed-case>WASSA</fixed-case> 2024 Shared Task on Empathy and Personality Detection in Interactions</title>
      <author><first>Salvatore</first><last>Giorgi</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>Shabnam</first><last>Tafreshi</last></author>
      <pages>369-379</pages>
      <abstract>This paper presents the results of the WASSA 2024 shared task on predicting empathy, emotion, and personality in conversations and reactions to news articles. Participating teams were given access to a new, unpublished extension of the WASSA 2023 shared task dataset. This task is both multi-level and multi-modal: data is available at the person, essay, dialog, and dialog-turn levels and includes formal (news articles) and informal text (essays and dialogs), self-report data (personality and distress), and third-party annotations (empathy and emotion). The shared task included a new focus on conversations between humans and LLM-based virtual agents which occur immediately after reading and reacting to the news articles. Participants were encouraged to explore the multi-level and multi-modal nature of this data. Participation was encouraged in four tracks: (i) predicting the perceived empathy at the dialog level, (ii) predicting turn-level empathy, emotion polarity, and emotion intensity in conversations, (iii) predicting state empathy and distress scores, and (iv) predicting personality. In total, 14 teams participated in the shared task. We summarize the methods and resources used by the participating teams.</abstract>
      <url hash="fb908c38">2024.wassa-1.30</url>
      <bibkey>giorgi-etal-2024-findings</bibkey>
      <doi>10.18653/v1/2024.wassa-1.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>RU</fixed-case> at <fixed-case>WASSA</fixed-case> 2024 Shared Task: Task-Aligned Prompt for Predicting Empathy and Distress</title>
      <author><first>Haein</first><last>Kong</last></author>
      <author><first>Seonghyeon</first><last>Moon</last><affiliation>Brookhaven National Laboratory</affiliation></author>
      <pages>380-384</pages>
      <abstract>This paper describes our approach for the WASSA 2024 Shared Task on Empathy Detection and Emotion Classification and Personality Detection in Interactions at ACL 2024. We focused on Track 3: Empathy Prediction (EMP) which aims to predict the empathy and distress of writers based on their essays. Recently, LLMs have been used to detect the psychological status of the writers based on the texts. Previous studies showed that the performance of LLMs can be improved by designing prompts properly. While diverse approaches have been made, we focus on the fact that LLMs can have different nuances for psychological constructs such as empathy or distress to the specific task. In addition, people can express their empathy or distress differently according to the context. Thus, we tried to enhance the prediction performance of LLMs by proposing a new prompting strategy: Task-Aligned Prompt (TAP). This prompt consists of aligned definitions for empathy and distress to the original paper and the contextual information about the dataset. Our proposed prompt was tested using ChatGPT and GPT4o with zero-shot and few-shot settings and the performance was compared to the plain prompts. The results showed that the TAP-ChatGPT-zero-shot achieved the highest average Pearson correlation of empathy and distress on the EMP track.</abstract>
      <url hash="e4fa180e">2024.wassa-1.31</url>
      <bibkey>kong-moon-2024-ru</bibkey>
      <doi>10.18653/v1/2024.wassa-1.31</doi>
    </paper>
    <paper id="32">
      <title>Chinchunmei at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Boosting <fixed-case>LLM</fixed-case>’s Prediction with Role-play Augmentation and Contrastive Reasoning Calibration</title>
      <author><first>Tian</first><last>Li</last></author>
      <author><first>Nicolay</first><last>Rusnachenko</last></author>
      <author><first>Huizhi</first><last>Liang</last><affiliation>Newcastle University, UK</affiliation></author>
      <pages>385-392</pages>
      <abstract>This paper presents the Chinchunmei team’s contributions to the WASSA2024 Shared-Task 1: Empathy Detection and Emotion Classification. We participated in Tracks 1, 2, and 3 to predict empathetic scores based on dialogue, article, and essay content. We choose Llama3-8b-instruct as our base model. We developed three supervised fine-tuning schemes: standard prediction, role-play, and contrastive prediction, along with an innovative scoring calibration method called Contrastive Reasoning Calibration during inference. Pearson Correlation was used as the evaluation metric across all tracks. For Track 1, we achieved 0.43 on the devset and 0.17 on the testset. For Track 2 emotion, empathy, and polarity labels, we obtained 0.64, 0.66, and 0.79 on the devset and 0.61, 0.68, and 0.58 on the testset. For Track 3 empathy and distress labels, we got 0.64 and 0.56 on the devset and 0.33 and 0.35 on the testset.</abstract>
      <url hash="2858e73a">2024.wassa-1.32</url>
      <bibkey>li-etal-2024-chinchunmei</bibkey>
      <doi>10.18653/v1/2024.wassa-1.32</doi>
    </paper>
    <paper id="33">
      <title>Empathify at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Contextualizing Empathy with a <fixed-case>BERT</fixed-case>-Based Context-Aware Approach for Empathy Detection</title>
      <author><first>Arda</first><last>Numanoğlu</last></author>
      <author><first>Süleyman</first><last>Ateş</last></author>
      <author><first>Nihan</first><last>Cicekli</last><affiliation>Middle East Technical University</affiliation></author>
      <author><first>Dilek</first><last>Küçük</last><affiliation>METU, Middle East Technical University</affiliation></author>
      <pages>393-398</pages>
      <abstract>Empathy detection from textual data is a complex task that requires an understanding of both the content and context of the text. This study presents a BERT-based context-aware approach to enhance empathy detection in conversations and essays. We participated in the WASSA 2024 Shared Task, focusing on two tracks: empathy and emotion prediction in conversations (CONV-turn) and empathy and distress prediction in essays (EMP). Our approach leverages contextual information by incorporating related articles and emotional characteristics as additional inputs, using BERT-based Siamese (parallel) architecture. Our experiments demonstrated that using article summaries as context significantly improves performance, with the parallel BERT approach outperforming the traditional method of concatenating inputs with the ‘[SEP]‘ token. These findings highlight the importance of context-awareness in empathy detection and pave the way for future improvements in the sensitivity and accuracy of such systems.</abstract>
      <url hash="0b7b1620">2024.wassa-1.33</url>
      <bibkey>numanoglu-etal-2024-empathify</bibkey>
      <doi>10.18653/v1/2024.wassa-1.33</doi>
    </paper>
    <paper id="34">
      <title>Zhenmei at <fixed-case>WASSA</fixed-case>-2024 Empathy and Personality Shared Track 2 Incorporating <fixed-case>P</fixed-case>earson Correlation Coefficient as a Regularization Term for Enhanced Empathy and Emotion Prediction in Conversational Turns</title>
      <author><first>Liting</first><last>Huang</last></author>
      <author><first>Huizhi</first><last>Liang</last><affiliation>Newcastle University, UK</affiliation></author>
      <pages>399-403</pages>
      <abstract>In the realm of conversational empathy and emotion prediction, emotions are frequently categorized into multiple levels. This study seeks to enhance the performance of emotion prediction models by incorporating the Pearson correlation coefficient as a regularization term within the loss function. This regularization approach ensures closer alignment between predicted and actual emotion levels, mitigating extreme predictions and resulting in smoother and more consistent outputs. Such outputs are essential for capturing the subtle transitions between continuous emotion levels. Through experimental comparisons between models with and without Pearson regularization, our findings demonstrate that integrating the Pearson correlation coefficient significantly boosts model performance, yielding higher correlation scores and more accurate predictions. Our system officially ranked 9th at the Track 2: CONV-turn. The code for our model can be found at Link .</abstract>
      <url hash="20c28555">2024.wassa-1.34</url>
      <bibkey>huang-liang-2024-zhenmei</bibkey>
      <doi>10.18653/v1/2024.wassa-1.34</doi>
    </paper>
    <paper id="35">
      <title>Empaths at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Turn-Level Empathy Prediction Using Psychological Indicators</title>
      <author><first>Shaz</first><last>Furniturewala</last></author>
      <author><first>Kokil</first><last>Jaidka</last><affiliation>National University of Singapore</affiliation></author>
      <pages>404-411</pages>
      <abstract>For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose a novel turn-level empathy detection method that decomposes empathy into six psychological indicators: Emotional Language, Perspective-Taking, Sympathy and Compassion, Extroversion, Openness, and Agreeableness. A pipeline of text enrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning demonstrates a significant improvement in the Pearson Correlation Coefficient and F1 scores for empathy detection, highlighting the effectiveness of our approach. Our system officially ranked 7th at the CONV-turn track.</abstract>
      <url hash="52d05449">2024.wassa-1.35</url>
      <bibkey>furniturewala-jaidka-2024-empaths</bibkey>
      <doi>10.18653/v1/2024.wassa-1.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>NU</fixed-case> at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Enhancing Personality Predictions with Knowledge Graphs; A Graphical Neural Network and <fixed-case>L</fixed-case>ight<fixed-case>GBM</fixed-case> Ensemble Approach</title>
      <author><first>Emmanuel</first><last>Osei-Brefo</last><affiliation>Newcastle University, UK</affiliation></author>
      <author><first>Huizhi</first><last>Liang</last><affiliation>Newcastle University, UK</affiliation></author>
      <pages>412-419</pages>
      <abstract>This paper proposes a novel ensemble approach that combines Graph Neural Networks (GNNs) and LightGBM to enhance personality prediction based on the personality Big 5 model. By integrating BERT embeddings from user essays with knowledge graph-derived embeddings, our method accurately captures rich semantic and relational information. Additionally, a special loss function that combines Mean Squared Error (MSE), Pearson correlation loss, and contrastive loss to improve model performance is introduced. The proposed ensemble model, made of Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and LightGBM, demonstrates superior performance over other models, with significant improvements in prediction accuracy for the Big Five personality traits achieved. Our system officially ranked <tex-math>2^{nd}</tex-math> at the Track 4: PER track.</abstract>
      <url hash="2cb19a77">2024.wassa-1.36</url>
      <bibkey>osei-brefo-liang-2024-nu</bibkey>
      <doi>10.18653/v1/2024.wassa-1.36</doi>
    </paper>
    <paper id="37">
      <title>Daisy at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: A Quick Exploration on Emotional Pattern of Empathy and Distress</title>
      <author><first>Rendi</first><last>Chevi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <pages>420-424</pages>
      <abstract>When we encountered upsetting or tragic situations involving other people, we might feel certain emotions that are congruent, though not necessarily identical, to what that person might went through. These kind of vicarious emotions are what defined empathy and distress, they can be seen as a form of emotional response to other people in need. In this paper, we describe our participation in WASSA 2024 Shared Task 3 in predicting writer’s level of empathy and distress from their personal essays. We approach this task by assuming one’s level of empathy and distress can be revealed from the emotional patterns within their essay. By extracting the emotional patterns from essays via an emotion classifier, we regress the empathy and distress levels from these patterns. Through correlation and model explainability analysis, we found that there are similar set of emotions, such as sadness or disappointment, and distinct set of emotions, such as anger or approval, that might describe the writer’s level of empathy and distress. We hope that our approach and findings could serve as a basis for future work that try to model and explain empathy and distress from emotional patterns.</abstract>
      <url hash="20b8c60b">2024.wassa-1.37</url>
      <bibkey>chevi-aji-2024-daisy</bibkey>
      <doi>10.18653/v1/2024.wassa-1.37</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>WASSA</fixed-case> 2024 Shared Task: Enhancing Emotional Intelligence with Prompts</title>
      <author><first>Svetlana</first><last>Churina</last></author>
      <author><first>Preetika</first><last>Verma</last></author>
      <author><first>Suchismita</first><last>Tripathy</last></author>
      <pages>425-429</pages>
      <abstract>This paper describes the system for the last-min-submittion team in WASSA-2024 Shared Task 1:Empathy Detection and Emotion Classification. This task aims at developing models which can predict the empathy, emotion, and emotional polarity. This system achieved relatively goodresults on the competition’s official leaderboard.The code of this system is available here.</abstract>
      <url hash="f792865b">2024.wassa-1.38</url>
      <bibkey>churina-etal-2024-wassa</bibkey>
      <doi>10.18653/v1/2024.wassa-1.38</doi>
    </paper>
    <paper id="39">
      <title>hyy33 at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Using the <fixed-case>C</fixed-case>ombined<fixed-case>L</fixed-case>oss and <fixed-case>FGM</fixed-case> for Enhancing <fixed-case>BERT</fixed-case>-based Models in Emotion and Empathy Prediction from Conversation Turns</title>
      <author><first>Huiyu</first><last>Yang</last></author>
      <author><first>Liting</first><last>Huang</last></author>
      <author><first>Tian</first><last>Li</last></author>
      <author><first>Nicolay</first><last>Rusnachenko</last></author>
      <author><first>Huizhi</first><last>Liang</last><affiliation>Newcastle University, UK</affiliation></author>
      <pages>430-434</pages>
      <abstract>This paper presents our participation to the WASSA 2024 Shared Task on Empathy Detection and Emotion Classification and Personality Detection in Interactions. We focus on Track 2: Empathy and Emotion Prediction in Conversations Turns (CONV-turn), which consists of predicting the perceived empathy, emotion polarity and emotion intensity at turn level in a conversation. In the method, we conduct BERT and DeBERTa based finetuning, implement the CombinedLoss which consists of a structured contrastive loss and Pearson loss, adopt adversarial training using Fast Gradient Method (FGM). This method achieved Pearson correlation of 0.581 for Emotion,0.644 for Emotional Polarity and 0.544 for Empathy on the test set, with the average value of 0.590 which ranked 4th among all teams. After submission to WASSA 2024 competition, we further introduced the segmented mix-up for data augmentation, boosting for ensemble and regression experiments, which yield even better results: 0.6521 for Emotion, 0.7376 for EmotionalPolarity, 0.6326 for Empathy in Pearson correlation on the development set. The implementation and fine-tuned models are publicly-available at https://github.com/hyy-33/hyy33-WASSA-2024-Track-2.</abstract>
      <url hash="966b1b9f">2024.wassa-1.39</url>
      <bibkey>yang-etal-2024-hyy33</bibkey>
      <doi>10.18653/v1/2024.wassa-1.39</doi>
    </paper>
    <paper id="40">
      <title>Fraunhofer <fixed-case>SIT</fixed-case> at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Use of Sentiment Transformers and Data Augmentation With Fuzzy Labels to Predict Emotional Reactions in Conversations and Essays</title>
      <author><first>Raphael</first><last>Frick</last><affiliation>Fraunhofer SIT</affiliation></author>
      <author><first>Martin</first><last>Steinebach</last><affiliation>Fraunhofer SIT</affiliation></author>
      <pages>435-440</pages>
      <abstract>Predicting emotions and emotional reactions during conversations and within texts poses challenges, even for advanced AI systems. The second iteration of the WASSA Empathy and Personality Shared Task focuses on creating innovative models that can anticipate emotional responses to news articles containing harmful content across four tasks.In this paper, we introduce our Fraunhofer SIT team’s solutions for the three tasks: Task 1 (CONVD), Task 2 (CONVT), and Task 3 (EMP).It involves combining LLM-driven data augmentation with fuzzy labels and fine-tuning RoBERTa models pre-trained on sentiment classification tasks to solve the regression problems. In the competition, our solutions achieved first place in Task 1, X in Task 2, and third place in Task 3.</abstract>
      <url hash="6dea361b">2024.wassa-1.40</url>
      <bibkey>frick-steinebach-2024-fraunhofer</bibkey>
      <doi>10.18653/v1/2024.wassa-1.40</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>E</fixed-case>mpathetic<fixed-case>FIG</fixed-case> at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: Predicting Empathy and Emotion in Conversations with Figurative Language</title>
      <author><first>Gyeongeun</first><last>Lee</last></author>
      <author><first>Zhu</first><last>Wang</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Sathya N.</first><last>Ravi</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>441-447</pages>
      <abstract>Recent research highlights the importance of figurative language as a tool for amplifying emotional impact. In this paper, we dive deeper into this phenomenon and outline our methods for <i>Track 1, Empathy Prediction in Conversations</i> (CONV-dialog) and <i>Track 2, Empathy and Emotion Prediction in Conversation Turns</i> (CONV-turn) of the WASSA 2024 shared task. We leveraged transformer-based large language models augmented with figurative language prompts, specifically idioms, metaphors and hyperbole, that were selected and trained for each track to optimize system performance. For Track 1, we observed that a fine-tuned BERT with metaphor and hyperbole features outperformed other models on the development set. For Track 2, DeBERTa, with different combinations of figurative language prompts, performed well for different prediction tasks. Our method provides a novel framework for understanding how figurative language influences emotional perception in conversational contexts. Our system officially ranked 4th in the 1st track and 3rd in the 2nd track.</abstract>
      <url hash="00fc7b05">2024.wassa-1.41</url>
      <bibkey>lee-etal-2024-empatheticfig</bibkey>
      <doi>10.18653/v1/2024.wassa-1.41</doi>
    </paper>
    <paper id="42">
      <title><fixed-case>C</fixed-case>on<fixed-case>T</fixed-case>ext at <fixed-case>WASSA</fixed-case> 2024 Empathy and Personality Shared Task: History-Dependent Embedding Utterance Representations for Empathy and Emotion Prediction in Conversations</title>
      <author><first>Patrícia</first><last>Pereira</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Helena</first><last>Moniz</last><affiliation>Universidade de Lisboa</affiliation></author>
      <author><first>Joao Paulo</first><last>Carvalho</last><affiliation>Instituto Superior Técnico and INESC-ID</affiliation></author>
      <pages>448-453</pages>
      <abstract>Empathy and emotion prediction are key components in the development of effective and empathetic agents, amongst several other applications. The WASSA shared task on empathy empathy and emotion prediction in interactions presents an opportunity to benchmark approaches to these tasks.Appropriately selecting and representing the historical context is crucial in the modelling of empathy and emotion in conversations. In our submissions, we model empathy, emotion polarity and emotion intensity of each utterance in a conversation by feeding the utterance to be classified together with its conversational context, i.e., a certain number of previous conversational turns, as input to an encoder Pre-trained Language Model (PLM), to which we append a regression head for prediction. We also model perceived counterparty empathy of each interlocutor by feeding all utterances from the conversation and a token identifying the interlocutor for which we are predicting the empathy. Our system officially ranked 1st at the CONV-turn track and 2nd at the CONV-dialog track.</abstract>
      <url hash="53abc7a0">2024.wassa-1.42</url>
      <bibkey>pereira-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.wassa-1.42</doi>
    </paper>
    <paper id="43">
      <title>Findings of the <fixed-case>WASSA</fixed-case> 2024 <fixed-case>EXALT</fixed-case> shared task on Explainability for Cross-Lingual Emotion in Tweets</title>
      <author><first>Aaron</first><last>Maladry</last><affiliation>LT3, Ghent University</affiliation></author>
      <author><first>Pranaydeep</first><last>Singh</last><affiliation>LT3, Ghent University</affiliation></author>
      <author><first>Els</first><last>Lefever</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>454-463</pages>
      <abstract>This paper presents a detailed description and results of the first shared task on explainability for cross-lingual emotion in tweets. Given a tweet in one of the five target languages (Dutch, Russian, Spanish, English, and French), systems should predict the correct emotion label (Task 1), as well as the words triggering the predicted emotion label (Task 2). The tweets were collected based on a list of stop words to prevent topical or emotional bias and were subsequently manually annotated. For both tasks, only a training corpus for English was provided, obliging participating systems to design cross-lingual approaches. Our shared task received submissions from 14 teams for the emotion detection task and from 6 teams for the trigger word detection task. The highest macro F1-scores obtained for both tasks are respectively 0.629 and 0.616, demonstrating that cross-lingual emotion detection is still a challenging task.</abstract>
      <url hash="9006d4da">2024.wassa-1.43</url>
      <bibkey>maladry-etal-2024-findings</bibkey>
      <doi>10.18653/v1/2024.wassa-1.43</doi>
    </paper>
    <paper id="44">
      <title>Cross-lingual Emotion Detection through Large Language Models</title>
      <author><first>Ram Mohan Rao</first><last>Kadiyala</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>464-469</pages>
      <abstract>This paper presents a detailed system description of our entry which finished 1st with a large lead at WASSA 2024 Task 2, focused on cross-lingual emotion detection. We utilized a combination of large language models (LLMs) and their ensembles to effectively understand and categorize emotions across different languages. Our approach not only outperformed other submissions with a large margin, but also demonstrated the strength of integrating multiple models to enhance performance. Additionally, We conducted a thorough comparison of the benefits and limitations of each model used. An error analysis is included along with suggested areas for future improvement. This paper aims to offer a clear and comprehensive understanding of advanced techniques in emotion detection, making it accessible even to those new to the field.</abstract>
      <url hash="8fcdb1b5">2024.wassa-1.44</url>
      <bibkey>kadiyala-2024-cross</bibkey>
      <doi>10.18653/v1/2024.wassa-1.44</doi>
    </paper>
    <paper id="45">
      <title>Knowledge Distillation from Monolingual to Multilingual Models for Intelligent and Interpretable Multilingual Emotion Detection</title>
      <author><first>Yuqi</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Zimu</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Nijia</first><last>Han</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Qi</first><last>Chen</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Haiyang</first><last>Zhang</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Yushan</first><last>Pan</last><affiliation>Xi’an Jiaotong Liverpool University</affiliation></author>
      <author><first>Anh</first><last>Nguyen</last><affiliation>University of Liverpool</affiliation></author>
      <pages>470-475</pages>
      <abstract>Emotion detection from text is a crucial task in understanding natural language with wide-ranging applications. Existing approaches for multilingual emotion detection from text face challenges with data scarcity across many languages and a lack of interpretability. We propose a novel method that leverages both monolingual and multilingual pre-trained language models to improve performance and interpretability. Our approach involves 1) training a high-performing English monolingual model in parallel with a multilingual model and 2) using knowledge distillation to transfer the emotion detection capabilities from the monolingual teacher to the multilingual student model. Experiments on a multilingual dataset demonstrate significant performance gains for refined multilingual models like XLM-RoBERTa and E5 after distillation. Furthermore, our approach enhances interpretability by enabling better identification of emotion-trigger words. Our work presents a promising direction for building accurate, robust and explainable multilingual emotion detection systems.</abstract>
      <url hash="f436d2b5">2024.wassa-1.45</url>
      <bibkey>wang-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.wassa-1.45</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>HITSZ</fixed-case>-<fixed-case>HLT</fixed-case> at <fixed-case>WASSA</fixed-case>-2024 Shared Task 2: Language-agnostic Multi-task Learning for Explainability of Cross-lingual Emotion Detection</title>
      <author><first>Feng</first><last>Xiong</last><affiliation>Harbin Institute of Technology, Shenzhen, China</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>Harbin Institute of Technology, Shenzhen, China</affiliation></author>
      <author><first>Geng</first><last>Tu</last><affiliation>Harbin Institute of Technology, Shenzhen, China</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology, Shenzhen, China</affiliation></author>
      <pages>476-482</pages>
      <abstract>This paper describes the system developed by the HITSZ-HLT team for WASSA-2024 Shared Task 2, which addresses two closely linked sub-tasks: Cross-lingual Emotion Detection and Binary Trigger Word Detection in tweets. The main goal of Shared Task 2 is to simultaneously identify the emotions expressed and detect the trigger words across multiple languages. To achieve this, we introduce a Language-agnostic Multi Task Learning (LaMTL) framework that integrates emotion prediction and emotion trigger word detection tasks. By fostering synergistic interactions between task-specific and task-agnostic representations, the LaMTL aims to mutually enhance emotional cues, ultimately improving the performance of both tasks. Additionally, we leverage large-scale language models to translate the training dataset into multiple languages, thereby fostering the formation of language-agnostic representations within the model, significantly enhancing the model’s ability to transfer and perform well across multilingual data. Experimental results demonstrate the effectiveness of our framework across both tasks, with a particular highlight on its success in achieving second place in sub-task 2.</abstract>
      <url hash="85df0937">2024.wassa-1.46</url>
      <bibkey>xiong-etal-2024-hitsz</bibkey>
      <doi>10.18653/v1/2024.wassa-1.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>UWB</fixed-case> at <fixed-case>WASSA</fixed-case>-2024 Shared Task 2: Cross-lingual Emotion Detection</title>
      <author><first>Jakub</first><last>Šmíd</last><affiliation>University of West Bohemia, Faculty of Applied Sciences, Department of Computer Science and Engineering</affiliation></author>
      <author><first>Pavel</first><last>Přibáň</last><affiliation>University of West Bohemia, Faculty of Applied Sciences, Department of Computer Science and Engineering</affiliation></author>
      <author><first>Pavel</first><last>Král</last><affiliation>University of West Bohemia, Faculty of Applied Sciences, Department of Computer Science and Engineering</affiliation></author>
      <pages>483-489</pages>
      <abstract>This paper presents our system built for the WASSA-2024 Cross-lingual Emotion Detection Shared Task. The task consists of two subtasks: first, to assess an emotion label from six possible classes for a given tweet in one of five languages, and second, to predict words triggering the detected emotions in binary and numerical formats. Our proposed approach revolves around fine-tuning quantized large language models, specifically Orca 2, with low-rank adapters (LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We enhance performance through machine translation for both subtasks and trigger word switching for the second subtask. The system achieves excellent performance, ranking 1st in numerical trigger words detection, 3rd in binary trigger words detection, and 7th in emotion detection.</abstract>
      <url hash="e70b3c18">2024.wassa-1.47</url>
      <bibkey>smid-etal-2024-uwb</bibkey>
      <doi>10.18653/v1/2024.wassa-1.47</doi>
    </paper>
    <paper id="48">
      <title><fixed-case>PCICUNAM</fixed-case> at <fixed-case>WASSA</fixed-case> 2024: Cross-lingual Emotion Detection Task with Hierarchical Classification and Weighted Loss Functions</title>
      <author><first>Jesús</first><last>Vázquez-Osorio</last><affiliation>Universidad Nacional Autónoma de México, Posgrado en Ciencia e Ingeniería de la Computación</affiliation></author>
      <author><first>Gerardo</first><last>Sierra</last><affiliation>Universidad Nacional Autónoma de México, Instituto de Ingeniería</affiliation></author>
      <author><first>Helena</first><last>Gómez-Adorno</last><affiliation>Universidad Nacional Autónoma de México, Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas</affiliation></author>
      <author><first>Gemma</first><last>Bel-Enguix</last><affiliation>Universidad Nacional Autónoma de México, Instituto de Ingeniería</affiliation></author>
      <pages>490-494</pages>
      <abstract>This paper addresses the shared task of multi-lingual emotion detection in tweets, presented at the Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media Analysis (WASSA) co-located with the ACL 2024 conference. The task involves predicting emotions from six classes in tweets from five different languages using only English for model training. Our approach focuses on addressing class imbalance through data augmentation, hierarchical classification, and the application of focal loss and weighted cross-entropy loss functions. These methods enhance our transformer-based model’s ability to transfer emotion detection capabilities across languages, resulting in improved performance despite the constraints of limited computational resources.</abstract>
      <url hash="9a075abd">2024.wassa-1.48</url>
      <bibkey>vazquez-osorio-etal-2024-pcicunam</bibkey>
      <doi>10.18653/v1/2024.wassa-1.48</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>TEII</fixed-case>: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection</title>
      <author><first>Long</first><last>Cheng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Qihao</first><last>Shao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Christine</first><last>Zhao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Sheng</first><last>Bi</last><affiliation>University of Washington</affiliation></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington</affiliation></author>
      <pages>495-504</pages>
      <abstract>Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques. Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.</abstract>
      <url hash="b3d2204e">2024.wassa-1.49</url>
      <bibkey>cheng-etal-2024-teii</bibkey>
      <doi>10.18653/v1/2024.wassa-1.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>NYCU</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>EXALT</fixed-case> 2024: Assembling Large Language Models for Cross-Lingual Emotion and Trigger Detection</title>
      <author><first>Tzu-Mi</first><last>Lin</last><affiliation>Institute of Artificial Intelligence Innovation, National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Zhe-Yu</first><last>Xu</last><affiliation>Institute of Artificial Intelligence Innovation, National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Jian-Yu</first><last>Zhou</last><affiliation>Institute of Artificial Intelligence Innovation, National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Lung-Hao</first><last>Lee</last><affiliation>Institute of Artificial Intelligence Innovation, National Yang Ming Chiao Tung University</affiliation></author>
      <pages>505-510</pages>
      <abstract>This study describes the model design of the NYCU-NLP system for the EXALT shared task at the WASSA 2024 workshop. We instruction-tune several large language models and then assemble various model combinations as our main system architecture for cross-lingual emotion and trigger detection in tweets. Experimental results showed that our best performing submission is an assembly of the Starling (7B) and Llama 3 (8B) models. Our submission was ranked sixth of 17 participating systems for the emotion detection subtask, and fifth of 7 systems for the binary trigger detection subtask.</abstract>
      <url hash="6e0e9fbf">2024.wassa-1.50</url>
      <bibkey>lin-etal-2024-nycu</bibkey>
      <doi>10.18653/v1/2024.wassa-1.50</doi>
    </paper>
    <paper id="51">
      <title>Effectiveness of Scalable Monolingual Data and Trigger Words Prompting on Cross-Lingual Emotion Detection Task</title>
      <author><first>Yao-Fei</first><last>Cheng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jeongyeob</first><last>Hong</last><affiliation>University of Washington</affiliation></author>
      <author><first>Andrew</first><last>Wang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Anita</first><last>Silva</last><affiliation>University of Washington</affiliation></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington</affiliation></author>
      <pages>511-522</pages>
      <abstract>This paper introduces our submitted systems for WASSA 2024 Shared Task 2: Cross-Lingual Emotion Detection. We implemented a BERT-based classifier and an in-context learning-based system. Our best-performing model, using English Chain of Thought prompts with trigger words, reached 3rd overall with an F1 score of 0.6015. Following the motivation of the shared task, we further analyzed the scalability and transferability of the monolingual English dataset on cross-lingual tasks. Our analysis demonstrates the importance of data quality over quantity. We also found that augmented multilingual data does not necessarily perform better than English monolingual data in cross-lingual tasks. We open-sourced the augmented data and source code of our system for future research.</abstract>
      <url hash="e6b11985">2024.wassa-1.51</url>
      <bibkey>cheng-etal-2024-effectiveness</bibkey>
      <doi>10.18653/v1/2024.wassa-1.51</doi>
    </paper>
    <paper id="52">
      <title><fixed-case>WU</fixed-case>_<fixed-case>TLAXE</fixed-case> at <fixed-case>WASSA</fixed-case> 2024 Explainability for Cross-Lingual Emotion in Tweets Shared Task 1: Emotion through Translation using <fixed-case>T</fixed-case>w<fixed-case>HIN</fixed-case>-<fixed-case>BERT</fixed-case> and <fixed-case>GPT</fixed-case></title>
      <author><first>Jon</first><last>Davenport</last><affiliation>University of Washington</affiliation></author>
      <author><first>Keren</first><last>Ruditsky</last><affiliation>University of Washington</affiliation></author>
      <author><first>Anna</first><last>Batra</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yulha</first><last>Lhawa</last><affiliation>University of Washington</affiliation></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington</affiliation></author>
      <pages>523-527</pages>
      <abstract>This paper describes our task 1 submission for the WASSA 2024 shared task on Explainability for Cross-lingual Emotion in Tweets. Our task is to predict the correct emotion label (Anger, Sadness, Fear, Joy, Love, and Neutral) for a dataset of English, Dutch, French, Spanish, and Russian tweets, while training exclusively on English emotion labeled data, to reveal what kind of emotion detection information is transferable cross-language (Maladry et al., 2024). To that end, we used an ensemble of models with a GPT-4 decider. Our ensemble consisted of a few-shot GPT-4 prompt system and a TwHIN-BERT system fine-tuned on the EXALT and additional English data. We ranked 8th place under the name WU_TLAXE with an F1 Macro score of 0.573 on the test set. We also experimented with an English-only TwHIN-BERT model by translating the other languages into English for inference, which proved to be worse than the other models.</abstract>
      <url hash="2880f66f">2024.wassa-1.52</url>
      <bibkey>davenport-etal-2024-wu</bibkey>
      <doi>10.18653/v1/2024.wassa-1.52</doi>
    </paper>
    <paper id="53">
      <title>Enhancing Cross-Lingual Emotion Detection with Data Augmentation and Token-Label Mapping</title>
      <author><first>Jinghui</first><last>Zhang</last><affiliation>China Telecom Cloud Technology Co., Ltd</affiliation></author>
      <author><first>Yuan</first><last>Zhao</last><affiliation>China Telecom Cloud Technology Co., Ltd</affiliation></author>
      <author><first>Siqin</first><last>Zhang</last><affiliation>China Telecom Cloud Technology Co., Ltd</affiliation></author>
      <author><first>Ruijing</first><last>Zhao</last><affiliation>China Telecom Cloud Technology Co., Ltd</affiliation></author>
      <author><first>Siyu</first><last>Bao</last><affiliation>China Telecom Cloud Technology Co., Ltd</affiliation></author>
      <pages>528-533</pages>
      <abstract>Cross-lingual emotion detection faces challenges such as imbalanced label distribution, data scarcity, cultural and linguistic differences, figurative language, and the opaqueness of pre-trained language models. This paper presents our approach to the EXALT shared task at WASSA 2024, focusing on emotion transferability across languages and trigger word identification. We employ data augmentation techniques, including back-translation and synonym replacement, to address data scarcity and imbalance issues in the emotion detection sub-task. For the emotion trigger identification sub-task, we utilize token and label mapping to capture emotional information at the subword level. Our system achieves competitive performance, ranking 13th, 1st, and 2nd in the Emotion Detection, Binary Trigger Word Detection, and Numerical Trigger Word Detection tasks.</abstract>
      <url hash="6efae64e">2024.wassa-1.53</url>
      <bibkey>zhang-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.wassa-1.53</doi>
    </paper>
  </volume>
</collection>
