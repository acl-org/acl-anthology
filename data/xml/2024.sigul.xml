<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.sigul">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Annual Meeting of the Special Interest Group on Under-resourced Languages @ LREC-COLING 2024</booktitle>
      <editor><first>Maite</first><last>Melero</last></editor>
      <editor><first>Sakriani</first><last>Sakti</last></editor>
      <editor><first>Claudia</first><last>Soria</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="ec23435b">2024.sigul-1</url>
      <venue>sigul</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="9a503a91">2024.sigul-1.0</url>
      <bibkey>sigul-2024-special</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Bit of a Problem: Measurement Disparities in Dataset Sizes across Languages</title>
      <author><first>Catherine</first><last>Arnett</last></author>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin</first><last>Bergen</last></author>
      <pages>1–9</pages>
      <abstract>How should text dataset sizes be compared across languages? Even for content-matched (parallel) corpora, UTF-8 encoded text can require a dramatically different number of bytes for different languages. In our work, we define the byte premium between two languages as the ratio of bytes used to encode content-matched text in those languages. We compute byte premiums for 1155 languages, and we use linear regressions to estimate byte premiums for other languages. We release a tool to obtain byte premiums for any two languages, enabling comparisons of dataset sizes across languages for more equitable multilingual model development and data practices.</abstract>
      <url hash="9b308c45">2024.sigul-1.1</url>
      <bibkey>arnett-etal-2024-bit</bibkey>
    </paper>
    <paper id="2">
      <title>A Novel Corpus for Automated Sexism Identification on Social Media</title>
      <author><first>Lutfiye Seda</first><last>Mut Altin</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>10–15</pages>
      <abstract>In this paper, we present a novel dataset for the study of automated sexism identification and categorization on social media in Turkish. For this purpose, we have collected, following a well established methodology, a set of Tweets and YouTube comments. Relying on expert organizations in the area of gender equality, each text has been annotated based on a two-level labelling schema derived from previous research. Our resulting dataset consists of around 7,000 annotated instances useful for the study of expressions of sexism and misogyny on the Web. To the best of our knowledge, this is the first two-level manually annotated comprehensive Turkish dataset for sexism identification. In order to fuel research in this relevant area, we also present the result of our benchmarking experiments in the area of sexism identification in Turkish.</abstract>
      <url hash="4ff1c10b">2024.sigul-1.2</url>
      <bibkey>mut-altin-saggion-2024-novel</bibkey>
    </paper>
    <paper id="3">
      <title>Advancing Generative <fixed-case>AI</fixed-case> for <fixed-case>P</fixed-case>ortuguese with Open Decoder Gervásio <fixed-case>PT</fixed-case>*</title>
      <author><first>Rodrigo</first><last>Santos</last></author>
      <author><first>João Ricardo</first><last>Silva</last></author>
      <author><first>Luís</first><last>Gomes</last></author>
      <author><first>João</first><last>Rodrigues</last></author>
      <author><first>António</first><last>Branco</last></author>
      <pages>16–26</pages>
      <abstract>To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gervásio PT*, a strong LLaMA 2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gervásio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.</abstract>
      <url hash="0c7d90f9">2024.sigul-1.3</url>
      <bibkey>santos-etal-2024-advancing</bibkey>
    </paper>
    <paper id="4">
      <title>Assessing Pre-Built Speaker Recognition Models for Endangered Language Data</title>
      <author><first>Gina-Anne</first><last>Levow</last></author>
      <pages>27–32</pages>
      <abstract>Significant research has focused on speaker recognition, determining which speaker is speaking in a segment of audio. However, few experiments have investigated speaker recognition for very low-resource or endangered languages. Furthermore, speaker recognition has the potential to support language documentation and revitalization efforts, making recordings more accessible to researchers and communities. Since endangered language datasets are too small to build competitive speaker representations from scratch, we investigate the application of large-scale pre-built speaker recognition models to bridge this gap. This paper compares four speaker recognition models on six diverse endangered language data sets. Comparisons contrast three recent neural network-based x-vector models and an earlier baseline i-vector model. Experiments demonstrate significantly stronger performance for some of the studied models. Further analysis highlights differences in effectiveness tied to the lengths of test audio segments and amount of data used for speaker modeling.</abstract>
      <url hash="6d73a56f">2024.sigul-1.4</url>
      <bibkey>levow-2024-assessing</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>BERT</fixed-case>bek: A Pretrained Language Model for <fixed-case>U</fixed-case>zbek</title>
      <author><first>Elmurod</first><last>Kuriyozov</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <pages>33–44</pages>
      <abstract>Recent advances in neural networks based language representation made it possible for pretrained language models to outperform previous models in many downstream natural language processing (NLP) tasks. These pretrained language models have also shown that if large enough, they exhibit good few-shot abilities, which is especially beneficial for low-resource scenarios. In this respect, although there are some large-scale multilingual pretrained language models available, language-specific pretrained models have demonstrated to be more accurate for monolingual evaluation setups. In this work, we present BERTbek - pretrained language models based on the BERT (Bidirectional Encoder Representations from Transformers) architecture for the low-resource Uzbek language. We also provide a comprehensive evaluation of the models on a number of NLP tasks: sentiment analysis, multi-label topic classification, and named entity recognition, comparing the models with various machine learning methods as well as multilingual BERT (mBERT). Experimental results indicate that our models outperform mBERT and other task-specific baseline models in all three tasks. Additionally, we also show the impact of training data size and quality on the downstream performance of BERT models, by training three different models with different text sources and corpus sizes.</abstract>
      <url hash="423adf8b">2024.sigul-1.5</url>
      <bibkey>kuriyozov-etal-2024-bertbek</bibkey>
    </paper>
    <paper id="6">
      <title>Beyond Error Categories: A Contextual Approach of Evaluating Emerging Spell and Grammar Checkers</title>
      <author><first>Þórunn</first><last>Arnardóttir</last></author>
      <author><first>Svanhvít Lilja</first><last>Ingólfsdóttir</last></author>
      <author><first>Haukur Barri</first><last>Símonarson</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <author><first>Vilhjálmur</first><last>Þorsteinsson</last></author>
      <pages>45–52</pages>
      <abstract>Automatic spell and grammar checking can be done using various system architectures, and large language models have recently been used to solve the task with promising results. Here we describe a new method of creating test data to measure the performance of spell and grammar checkers, including large language models. Three types of test data represent different approaches to evaluation, from basic error detection to error correction with natural language explanations of the corrections made and error severity scores, which is the main novelty of this approach. These additions are especially useful when evaluating large language models. We present a spell and grammar checking test set for Icelandic in which the described approach is applied. The data consists of whole texts instead of discrete sentences, which facilitates evaluating context awareness of models. The resulting test set can be used to compare different spell and grammar checkers and is published under permissive licenses.</abstract>
      <url hash="bc92c494">2024.sigul-1.6</url>
      <bibkey>arnardottir-etal-2024-beyond</bibkey>
    </paper>
    <paper id="7">
      <title>Bidirectional <fixed-case>E</fixed-case>nglish-<fixed-case>N</fixed-case>epali Machine Translation(<fixed-case>MT</fixed-case>) System for Legal Domain</title>
      <author><first>Shabdapurush</first><last>Poudel</last></author>
      <author><first>Bal Krishna</first><last>Bal</last></author>
      <author><first>Praveen</first><last>Acharya</last></author>
      <pages>53–58</pages>
      <abstract>Nepali, a low-resource language belonging to the Indo-Aryan language family and spoken in Nepal, India, Sikkim, and Burma has comparatively very little digital content and resources, more particularly in the legal domain. However, the need to translate legal documents is ever-increasing in the context of growing volumes of legal cases and a large population seeking to go abroad for higher education or employment. This underscores the need for developing an English-Nepali Machine Translation for the legal domain. We attempt to address this problem by utilizing a Neural Machine Translation (NMT) System with an encoder-decoder architecture, specifically designed for legal Nepali-English translation. Leveraging a custom-built legal corpus of 125,000 parallel sentences, our system achieves encouraging BLEU scores of 7.98 in (Nepali → English) and 6.63 (English → Nepali) direction</abstract>
      <url hash="d7dbbafb">2024.sigul-1.7</url>
      <bibkey>poudel-etal-2024-bidirectional</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>BK</fixed-case>3<fixed-case>AT</fixed-case>: Bangsamoro K-3 Children’s Speech Corpus for Developing Assessment Tools in the Bangsamoro Languages</title>
      <author><first>Kiel D.</first><last>Gonzales</last></author>
      <author><first>Jazzmin R.</first><last>Maranan</last></author>
      <author><first>Francis Paolo D.</first><last>Santelices</last></author>
      <author><first>Edsel Jedd M.</first><last>Renovalles</last></author>
      <author><first>Nissan D.</first><last>Macale</last></author>
      <author><first>Nicole Anne A.</first><last>Palafox</last></author>
      <author><first>Jose Marie A.</first><last>Mendoza</last></author>
      <pages>59–65</pages>
      <abstract>Bangsamoro languages are among the under-resourced languages in the Mindanao region in the Philippines. Moreover, there is no currently publicly available data for children’s speech on most of these languages. BK3AT children’s speech corpus is a corpus designed for creating speech technologies that could help facilitators and teachers in K-3 education. The corpus consists of 122 hours of children speech data across 10 languages: Bahasa Sug, Chavacano, English, Filipino, Iranun, Maguindanaon, Meranaw, Sinama, Teduray, and Yakan. Preliminary experiments using Wav2Vec-XLSR architecture have been done in fine-tuning the Tagalog and L2 English corpus subsets to develop automatic speech recognition backend for literacy assessment. Results from the experiments show low word error rates (WERs) for small-vocabulary and targeted domains.</abstract>
      <url hash="dc3d9424">2024.sigul-1.8</url>
      <bibkey>gonzales-etal-2024-bk3at</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>orpus<fixed-case>A</fixed-case>rièja: Building an Annotated Corpus with Variation in <fixed-case>O</fixed-case>ccitan</title>
      <author><first>Clamenca</first><last>Poujade</last></author>
      <author><first>Myriam</first><last>Bras</last></author>
      <author><first>Assaf</first><last>Urieli</last></author>
      <pages>66–71</pages>
      <abstract>The Occitan language is a less resourced language and is classified as ‘in danger’ by the UNESCO. Thereby, it is important to build resources and tools that can help to safeguard and develop the digitisation of the language. CorpusArièja is a collection of 72 texts (just over 41,000 tokens) in the Occitan language of the French department of Ariège. The majority of the texts needed to be digitised and pass within an Optical Character Recognition. This corpus contains dialectal and spelling variation, but is limited to prose, without diachronic variation or genre variation. It is an annotated corpus with two levels of lemmatisation, POS tags and verbal inflection. One of the main aims of the corpus is to enable the conception of tools that can automatically annotate all Occitan texts, regardless of the dialect or spelling used. The Ariège territory is interesting because it includes the two variations that we focus on, dialectal and spelling. It has plenty of authors that write in their native language, their variety of Occitan.</abstract>
      <url hash="e8d16220">2024.sigul-1.9</url>
      <bibkey>poujade-etal-2024-corpusarieja</bibkey>
    </paper>
    <paper id="10">
      <title>Developing Infrastructure for Low-Resource Language Corpus Building</title>
      <author><first>Hedwig G.</first><last>Sekeres</last></author>
      <author><first>Wilbert</first><last>Heeringa</last></author>
      <author><first>Wietse</first><last>de Vries</last></author>
      <author><first>Oscar Yde</first><last>Zwagers</last></author>
      <author><first>Martijn</first><last>Wieling</last></author>
      <author><first>Goffe Th.</first><last>Jensma</last></author>
      <pages>72–78</pages>
      <abstract>For many of the world’s small languages, few resources are available. In this project, a written online accessible corpus was created for the minority language variant Gronings, which serves both researchers interested in language change and variation and a general audience of (new) speakers interested in finding real-life examples of language use. The corpus was created using a combination of volunteer work and automation, which together formed an efficient pipeline for converting printed text to Key Words in Context (KWICs), annotated with lemmas and part-of-speech tags. In the creation of the corpus, we have taken into account several of the challenges that can occur when creating resources for minority languages, such as a lack of standardisation and limited (financial) resources. As the solutions we offer are applicable to other small languages as well, each step of the corpus creation process is discussed and resources will be made available benefiting future projects on other low-resource languages.</abstract>
      <url hash="54a5e6b9">2024.sigul-1.10</url>
      <bibkey>sekeres-etal-2024-developing</bibkey>
    </paper>
    <paper id="11">
      <title>Evaluating <fixed-case>I</fixed-case>celandic Sentiment Analysis Models Trained on Translated Data</title>
      <author><first>Ólafur A.</first><last>Jóhannsson</last></author>
      <author><first>Birkir H.</first><last>Arndal</last></author>
      <author><first>Eysteinn Ö.</first><last>Jónsson</last></author>
      <author><first>Stefan</first><last>Olafsson</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>79–89</pages>
      <abstract>We experiment with sentiment classification models for Icelandic that leverage machine-translated data for training. Since no large sentiment dataset exists for Icelandic, we translate 50,000 English IMDb reviews, classified either as positive or negative, into Icelandic using two services: Google Translate and GreynirTranslate. After machine translation, we assess whether the sentiment of the source language text is retained in the target language. Moreover, we evaluate the accuracy of the sentiment classifiers on non-translated Icelandic text.The performance of three types of baseline classifiers is compared, i.e., Support Vector Machines, Logistic Regression and Naive Bayes, when trained on translated data generated by either translation service. Furthermore, we fine-tune and evaluate three pre-trained transformer-based models, RoBERTa, IceBERT and ELECTRA, on both the original English texts and the translated texts. Our results indicate that the transformer models perform better than the baseline classifiers on all datasets. Moreover, our evaluation shows that the transformer models trained on data translated from English reviews can be used to effectively classify sentiment on non-translated Icelandic movie reviews.</abstract>
      <url hash="7f6b4472">2024.sigul-1.11</url>
      <bibkey>johannsson-etal-2024-evaluating</bibkey>
    </paper>
    <paper id="12">
      <title>Exploring Text Classification for Enhancing Digital Game-Based Language Learning for <fixed-case>I</fixed-case>rish</title>
      <author><first>Leona</first><last>Mc Cahill</last></author>
      <author><first>Thomas</first><last>Baltazar</last></author>
      <author><first>Sally</first><last>Bruen</last></author>
      <author><first>Liang</first><last>Xu</last></author>
      <author><first>Monica</first><last>Ward</last></author>
      <author><first>Elaine</first><last>Uí Dhonnchadha</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>90–96</pages>
      <abstract>Digital game-based language learning (DGBLL) can help with the language learning process. DGBLL applications can make learning more enjoyable and engaging, but they are difficult to develop. A DBGLL app that relies on target language texts obviously needs to be able to use texts of the appropriate level for the individual learners. This implies that text classification tools should be available to DGBLL developers, who may not be familiar with the target language, in order to incorporate suitable texts into their games. While text difficulty classifiers exist for many of the most commonly spoken languages, this is not the case for under-resourced languages, such as Irish. In this paper, we explore approaches to the development of text classifiers for Irish. In the first approach to text analysis and grading, we apply linguistic analysis to assess text complexity. Features from this approach are then used in machine learning-based text classification, which explores the application of a number of machine learning algorithms to the problem. Although the development of these text classifiers is at an early stage, they show promise, particularly in a low-resourced scenario.</abstract>
      <url hash="e15ac382">2024.sigul-1.12</url>
      <bibkey>mc-cahill-etal-2024-exploring</bibkey>
    </paper>
    <paper id="13">
      <title>Forget <fixed-case>NLI</fixed-case>, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to <fixed-case>L</fixed-case>uxembourgish</title>
      <author><first>Fred</first><last>Philippy</last></author>
      <author><first>Shohreh</first><last>Haddadan</last></author>
      <author><first>Siwen</first><last>Guo</last></author>
      <pages>97–104</pages>
      <abstract>In NLP, zero-shot classification (ZSC) is the task of assigning labels to textual data without any labeled examples for the target classes. A common method for ZSC is to fine-tune a language model on a Natural Language Inference (NLI) dataset and then use it to infer the entailment between the input document and the target labels. However, this approach faces certain challenges, particularly for languages with limited resources. In this paper, we propose an alternative solution that leverages dictionaries as a source of data for ZSC. We focus on Luxembourgish, a low-resource language spoken in Luxembourg, and construct two new topic relevance classification datasets based on a dictionary that provides various synonyms, word translations and example sentences. We evaluate the usability of our dataset and compare it with the NLI-based approach on two topic classification tasks in a zero-shot manner. Our results show that by using the dictionary-based dataset, the trained models outperform the ones following the NLI-based approach for ZSC. While we focus on a single low-resource language in this study, we believe that the efficacy of our approach can also transfer to other languages where such a dictionary is available.</abstract>
      <url hash="ef76b2f2">2024.sigul-1.13</url>
      <bibkey>philippy-etal-2024-forget</bibkey>
    </paper>
    <paper id="14">
      <title>Fostering the Ecosystem of Open Neural Encoders for <fixed-case>P</fixed-case>ortuguese with Albertina <fixed-case>PT</fixed-case>* Family</title>
      <author><first>Rodrigo</first><last>Santos</last></author>
      <author><first>João</first><last>Rodrigues</last></author>
      <author><first>Luís</first><last>Gomes</last></author>
      <author><first>João Ricardo</first><last>Silva</last></author>
      <author><first>António</first><last>Branco</last></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last></author>
      <author><first>Tomás Freitas</first><last>Osório</last></author>
      <author><first>Bernardo</first><last>Leite</last></author>
      <pages>105–114</pages>
      <abstract>To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are relevant for this ecosystem were obtained as well, namely new datasets for Portuguese based on the SuperGLUE benchmark, which we also distribute openly.</abstract>
      <url hash="4c83a3e6">2024.sigul-1.14</url>
      <bibkey>santos-etal-2024-fostering</bibkey>
    </paper>
    <paper id="15">
      <title>Improving Language Coverage on <fixed-case>H</fixed-case>e<fixed-case>LI</fixed-case>-<fixed-case>OTS</fixed-case></title>
      <author><first>Tommi</first><last>Jauhiainen</last></author>
      <author><first>Krister</first><last>Lindén</last></author>
      <pages>115–125</pages>
      <abstract>In this paper, we add under-resourced languages into the language repertoire of an existing off-the-shelf language identifier, HeLI-OTS. Adding more languages to a language identifier often comes with the drawback of lessened accuracy for the languages already part of the repertoire. We aim to minimize this effect. As sources for training and development data in the new languages, we use the OpenLID and FLORES-200 datasets. They are openly available high-quality datasets that are especially well-suited for language identifier development. By carefully inspecting the effect of each added language and the quality of their training and development data, we managed to add support for 20 new under-resourced languages to HeLI-OTS without affecting the performance of any existing languages to a noticeable extent.</abstract>
      <url hash="73881a20">2024.sigul-1.15</url>
      <bibkey>jauhiainen-linden-2024-improving</bibkey>
    </paper>
    <paper id="16">
      <title>Improving Legal Judgement Prediction in <fixed-case>R</fixed-case>omanian with Long Text Encoders</title>
      <author><first>Mihai</first><last>Masala</last></author>
      <author><first>Traian</first><last>Rebedea</last></author>
      <author><first>Horia</first><last>Velicu</last></author>
      <pages>126–132</pages>
      <abstract>In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized models and handling long texts are critical for a good performance.</abstract>
      <url hash="a93b72b5">2024.sigul-1.16</url>
      <bibkey>masala-etal-2024-improving</bibkey>
    </paper>
    <paper id="17">
      <title>Improving Noisy Student Training for Low-resource Languages in End-to-End <fixed-case>ASR</fixed-case> Using <fixed-case>C</fixed-case>ycle<fixed-case>GAN</fixed-case> and Inter-domain Losses</title>
      <author><first>Chia-Yu</first><last>Li</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>133–142</pages>
      <abstract>Training a semi-supervised end-to-end speech recognition system using noisy student training has significantly improved performance. However, this approach requires a substantial amount of paired speech-text and unlabeled speech, which is costly for low-resource languages. Therefore, this paper considers a more extreme case of semi-supervised end-to-end automatic speech recognition where there are limited paired speech-text, unlabeled speech (less than five hours), and abundant external text. Firstly, we observe improved performance by training the model using our previous work on semi-supervised learning “CycleGAN and inter-domain losses” solely with external text. Secondly, we enhance “CycleGAN and inter-domain losses” by incorporating automatic hyperparameter tuning, calling “enhanced CycleGAN inter-domain losses.” Thirdly, we integrate it into the noisy student training approach pipeline for low-resource scenarios. Our experimental results, conducted on six non-English languages from Voxforge and Common Voice, show a 20% word error rate reduction compared to the baseline teacher model and a 10% word error rate reduction compared to the baseline best student model, highlighting the significant improvements achieved through our proposed method.</abstract>
      <url hash="9852525a">2024.sigul-1.17</url>
      <bibkey>li-vu-2024-improving</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>I</fixed-case>ndonesian-<fixed-case>E</fixed-case>nglish Code-Switching Speech Recognition Using the Machine Speech Chain Based Semi-Supervised Learning</title>
      <author><first>Rais Vaza Man</first><last>Tazakka</last></author>
      <author><first>Dessi</first><last>Lestari</last></author>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <author><first>Dipta</first><last>Tanaya</last></author>
      <author><first>Kurniawati</first><last>Azizah</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <pages>143–148</pages>
      <abstract>Indonesia is home to a diverse linguistic landscape, where individuals seamlessly transition between Indonesian, English, and local dialects in their everyday conversations—a phenomenon known as code-switching. Understanding and accommodating this linguistic fluidity is essential, particularly in the development of accurate speech recognition systems. However, tackling code-switching in Indonesian poses a challenge due to the scarcity of paired code-switching data. Thus, this study endeavors to address Indonesian-English code-switching in speech recognition, leveraging unlabeled data and employing a semi-supervised technique known as the machine speech chain. Our findings demonstrate that the machine speech chain method effectively enhances Automatic Speech Recognition (ASR) performance in recognizing code-switching between Indonesian and English, utilizing previously untapped resources of unlabeled data.</abstract>
      <url hash="af232b9f">2024.sigul-1.18</url>
      <bibkey>tazakka-etal-2024-indonesian</bibkey>
    </paper>
    <paper id="19">
      <title>Inter-language Transfer Learning for Visual Speech Recognition toward Under-resourced Environments</title>
      <author><first>Fumiya</first><last>Kondo</last></author>
      <author><first>Satoshi</first><last>Tamura</last></author>
      <pages>149–154</pages>
      <abstract>In this study, we introduce a method of inter-language transfer learning for under-resourced visual speech recognition. Deploying speech-related technology to all languages is a quite important activity. However, applying state-of-the-art deep-learning techniques requires huge-size labeled corpora, which makes it hard for under-resourced languages. Our approach leverages a small amount of labeled video data of the target language, and employs inter-language transfer learning using a pre-trained English lip-reading model. By applying the proposed scheme, we build a Japanese lip-reading model, using the ROHAN corpus, the size of which is about one 450th of the size of English datasets. The front-end encoder part of the pre-trained model is fine-tuned to improve the acquisition of pronunciation and lip movement patterns unique to Japanese. On the other hand, the back-end encoder and the decoder are built using the Japanese dataset. Although English and Japanese have different language structures, evaluation experiments show that it is possible to build the Japanese lip-reading model efficiently. Comparison with competitive schemes demonstrates the effectiveness of our method.</abstract>
      <url hash="b5a9dae8">2024.sigul-1.19</url>
      <bibkey>kondo-tamura-2024-inter</bibkey>
    </paper>
    <paper id="20">
      <title>Investigating Neural Machine Translation for Low-Resource Languages: Using <fixed-case>B</fixed-case>avarian as a Case Study</title>
      <author><first>Wan-hua</first><last>Her</last></author>
      <author><first>Udo</first><last>Kruschwitz</last></author>
      <pages>155–167</pages>
      <abstract>Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using combined metrics: BLEU, chrF and TER. Statistical significance results with Bonferroni correction show surprisingly high baseline systems, and that Back-translation leads to significant improvement. Furthermore, we present a qualitative analysis of translation errors and system limitations.</abstract>
      <url hash="ab3e85ae">2024.sigul-1.20</url>
      <bibkey>her-kruschwitz-2024-investigating</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>I</fixed-case>talian-<fixed-case>L</fixed-case>igurian Machine Translation in Its Cultural Context</title>
      <author><first>Christopher R.</first><last>Haberland</last></author>
      <author><first>Jean</first><last>Maillard</last></author>
      <author><first>Stefano</first><last>Lusito</last></author>
      <pages>168–176</pages>
      <abstract>Large multilingual machine translation efforts are driving improved access and performance for under-resourced languages, but often fail to translate culturally specific and local concepts. Additionally, translation from practically relevant input languages may flag behind those that are comparatively over-represented in the training dataset. In this work, we release a new corpus, ZenaMT, containing 7,561 parallel Ligurian-Italian sentences, nearly a fifth of which are also translated in English. This corpus spans five domains: local and international news, Ligurian literature, Genoese Ligurian linguistics concepts, traditional card game rules, and Ligurian geographic expressions. We find that a translation model augmented with ZenaMT improves a baseline by 20%, and by over 25% (BLEU) compared to NLLB-3.3B, which is over 50 times the size. Our results demonstrate the utility of creating data sets for MT that are specifically tailored for the cultural context of Ligurian speakers. We freely release ZenaMT and expect to periodically update the corpus to improve MT performance and domain coverage.</abstract>
      <url hash="7859c188">2024.sigul-1.21</url>
      <bibkey>haberland-etal-2024-italian</bibkey>
    </paper>
    <paper id="22">
      <title>Labadain-30k+: A Monolingual Tetun Document-Level Audited Dataset</title>
      <author><first>Gabriel</first><last>de Jesus</last></author>
      <author><first>Sérgio</first><last>Nunes</last></author>
      <pages>177–188</pages>
      <abstract>This paper introduces Labadain-30k+, a monolingual dataset comprising 33.6k documents in Tetun, a low-resource language spoken in Timor-Leste. The dataset was acquired through web crawling and augmented with Wikipedia documents released by Wikimedia. Both sets of documents underwent thorough manual audits at the document level by native Tetun speakers, resulting in the construction of a Tetun text dataset well-suited for a variety of natural language processing and information retrieval tasks. This dataset was employed to conduct a comprehensive content analysis aimed at providing a nuanced understanding of document composition and the evolution of Tetun documents on the web. The analysis revealed that news articles constitute the predominant documents within the dataset, accounting for 89.87% of the total, followed by Wikipedia documents at 4.34%, and legal and governmental documents at 3.65%, among others. Notably, there was a substantial increase in the number of documents in 2020, indicating 11.75 percentage points rise in document quantity, compared to an average of 4.76 percentage points per year from 2001 to 2023. Moreover, the year 2017, marked by the increased popularity of online news in Tetun, served as a threshold for analyzing the evolution of document writing on the web pre- and post-2017, specifically regarding vocabulary usage. Surprisingly, this analysis showed a significant increase of 6.12 percentage points in the Tetun written adhering to the Tetun official standard. Additionally, the persistence of Portuguese loanwords in that trajectory remained evident, reflecting an increase of 5.09 percentage points.</abstract>
      <url hash="5933e8e5">2024.sigul-1.22</url>
      <bibkey>de-jesus-nunes-2024-labadain</bibkey>
    </paper>
    <paper id="23">
      <title>Language Models on a Diet: Cost-Efficient Development of Encoders for Closely-Related Languages via Additional Pretraining</title>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Vít</first><last>Suchomel</last></author>
      <author><first>Peter</first><last>Rupnik</last></author>
      <author><first>Taja</first><last>Kuzman</last></author>
      <author><first>Rik</first><last>van Noord</last></author>
      <pages>189–203</pages>
      <abstract>The world of language models is going through turbulent times, better and ever larger models are coming out at an unprecedented speed. However, we argue that, especially for the scientific community, encoder models of up to 1 billion parameters are still very much needed, their primary usage being in enriching large collections of data with metadata necessary for downstream research. We investigate the best way to ensure the existence of such encoder models on the set of very closely related languages - Croatian, Serbian, Bosnian and Montenegrin, by setting up a diverse benchmark for these languages, and comparing the trained-from-scratch models with the new models constructed via additional pretraining of existing multilingual models. We show that comparable performance to dedicated from-scratch models can be obtained by additionally pretraining available multilingual models even with a limited amount of computation. We also show that neighboring languages, in our case Slovenian, can be included in the additional pretraining with little to no loss in the performance of the final model.</abstract>
      <url hash="7b4d106e">2024.sigul-1.23</url>
      <bibkey>ljubesic-etal-2024-language</bibkey>
    </paper>
    <paper id="24">
      <title>Man or Machine: Evaluating Spelling Error Detection in <fixed-case>D</fixed-case>anish Newspaper Corpora</title>
      <author><first>Eckhard</first><last>Bick</last></author>
      <author><first>Jonas Nygaard</first><last>Blom</last></author>
      <author><first>Marianne</first><last>Rathje</last></author>
      <author><first>Jørgen</first><last>Schack</last></author>
      <pages>204–211</pages>
      <abstract>This paper evaluates frequency and detection performance for both spelling and grammatical errors in a corpus of published Danish newspaper texts, comparing the results of three human proofreaders with those of an automatic system, DanProof. Adopting the error categorization scheme of the latter, we look at the accuracy of individual error types and their relative distribution over time, as well as the adequacy of suggested corrections. Finally, we discuss so-called artefact errors introduced by corpus processing, and the potential of DanProof as a corpus cleaning tool for identifying and correcting format conversion, OCR or other compilation errors. In the evaluation, with balanced F1-scores of 77.6 and 67.6 for 1999 texts and 2019 texts, respectively, DanProof achieved a higher recall and accuracy than the individual human annotators, and contributed the largest share of errors not detected by others (16.4% for 1999 and 23.6% for 2019). However, the human annotators had a significantly higher precision. Not counting artifacts, the overall error frequency in the corpus was low ( 0.5%), and less than half in the newer texts compared to the older ones, a change that mostly concerned orthographical errors, with a correspondingly higher relative share of grammatical errors.</abstract>
      <url hash="10701e7d">2024.sigul-1.24</url>
      <bibkey>bick-etal-2024-man</bibkey>
    </paper>
    <paper id="25">
      <title>Managing Fine-grained Metadata for Text Bases in Extremely Low Resource Languages: The Cases of Two Regional Languages of <fixed-case>F</fixed-case>rance</title>
      <author><first>Marianne</first><last>Vergez-Couret</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Michael</first><last>Nauge</last></author>
      <author><first>Myriam</first><last>Bras</last></author>
      <author><first>Pablo</first><last>Ruiz Fabo</last></author>
      <author><first>Carole</first><last>Werner</last></author>
      <pages>212–221</pages>
      <abstract>Metadata are key components of language resources and facilitate their exploitation and re-use. Their creation is a labour intensive process and requires a modeling step, which identifies resource-specific information as well as standards and controlled vocabularies that can be reused. In this article, we focus on metadata for documenting text bases for regional languages of France characterised by several levels of variation (space, time, usage, social status), based on a survey of existing metadata schema. Moreover, we implement our metadata model as a database structure for the Heurist data management system, which combines both the ease of use of spreadsheets and the ability to model complex relationships between entities of relational databases. The Heurist template is made freely available and was used to describe metadata for text bases in Alsatian and Poitevin-Santongeais. We also propose tools to automatically generate XML metadata headers files from the database.</abstract>
      <url hash="05af90d1">2024.sigul-1.25</url>
      <bibkey>vergez-couret-etal-2024-managing</bibkey>
    </paper>
    <paper id="26">
      <title>Mixat: A Data Set of Bilingual Emirati-<fixed-case>E</fixed-case>nglish Speech</title>
      <author><first>Maryam Khalifa</first><last>Al Ali</last></author>
      <author><first>Hanan</first><last>Aldarmaki</last></author>
      <pages>222–226</pages>
      <abstract>This paper introduces Mixat: a dataset of Emirati speech code-mixed with English. Mixat was developed to address the shortcomings of current speech recognition resources when applied to Emirati speech, and in particular, to bilignual Emirati speakers who often mix and switch between their local dialect and English. The data set consists of 15 hours of speech derived from two public podcasts featuring native Emirati speakers, one of which is in the form of conversations between the host and a guest. Therefore, the collection contains examples of Emirati-English code-switching in both formal and natural conversational contexts. In this paper, we describe the process of data collection and annotation, and describe some of the features and statistics of the resulting data set. In addition, we evaluate the performance of pre-trained Arabic and multi-lingual ASR systems on our dataset, demonstrating the shortcomings of existing models on this low-resource dialectal Arabic, and the additional challenge of recognizing code-switching in ASR. The dataset will be made publicly available for research use.</abstract>
      <url hash="bf23554e">2024.sigul-1.26</url>
      <bibkey>al-ali-aldarmaki-2024-mixat</bibkey>
    </paper>
    <paper id="27">
      <title>Bi-dialectal <fixed-case>ASR</fixed-case> of <fixed-case>A</fixed-case>rmenian from Naturalistic and Read Speech</title>
      <author><first>Malajyan</first><last>Arthur</last></author>
      <author><first>Victoria</first><last>Khurshudyan</last></author>
      <author><first>Karen</first><last>Avetisyan</last></author>
      <author><first>Hossep</first><last>Dolatian</last></author>
      <author><first>Damien</first><last>Nouvel</last></author>
      <pages>227–236</pages>
      <abstract>The paper explores the development of Automatic Speech Recognition (ASR) models for Armenian, by using data from two standard dialects (Eastern Armenian and Western Armenian). The goal is to develop a joint bi-variational model. We achieve state-of-the-art results. Results from our ASR experiments demonstrate the impact of dataset selection and data volume on model performance. The study reveals limited transferability between dialects, although integrating datasets from both dialects enhances overall performance. The paper underscores the importance of dataset diversity and volume in ASR model training for under-resourced languages like Armenian.</abstract>
      <url hash="8999af79">2024.sigul-1.27</url>
      <bibkey>arthur-etal-2024-multi</bibkey>
    </paper>
    <paper id="28">
      <title>Multilingual Self-supervised Visually Grounded Speech Models</title>
      <author><first>Huynh Phuong Thanh</first><last>Nguyen</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <pages>237–243</pages>
      <abstract>Developing a multilingual speech-to-speech translation system poses challenges due to the scarcity of paired speech data in various languages, particularly when dealing with unknown and untranscribed languages. However, the shared semantic representation across multiple languages presents an opportunity to build a translation system based on images. Recently, researchers have explored methods for aligning bilingual speech as a novel approach to discovering speech pairs using semantic images from unknown and untranscribed speech. These aligned speech pairs can then be utilized to train speech-to-speech translation systems. Our research builds upon these approaches by expanding into multiple languages and focusing on achieving multimodal multilingual pairs alignment, with a key component being multilingual visually grounded speech models. The objectives of our research are twofold: (1) to create visually grounded speech datasets for English, Japanese, Indonesian, and Vietnamese, and (2) to develop self-supervised visually grounded speech models for these languages. Our experiments have demonstrated the feasibility of this approach, showcasing the ability to retrieve associations between speeches and images. The results indicate that our multilingual visually grounded speech models yield promising outcomes in representing speeches using semantic images across multiple languages.</abstract>
      <url hash="5ac8fb9b">2024.sigul-1.28</url>
      <bibkey>nguyen-sakti-2024-multilingual</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>N</fixed-case>epal Script Text Recognition Using <fixed-case>CRNN</fixed-case> <fixed-case>CTC</fixed-case> Architecture</title>
      <author><first>Swornim</first><last>Nakarmi</last></author>
      <author><first>Sarin</first><last>Sthapit</last></author>
      <author><first>Arya</first><last>Shakya</last></author>
      <author><first>Rajani</first><last>Chulyadyo</last></author>
      <author><first>Bal Krishna</first><last>Bal</last></author>
      <pages>244–251</pages>
      <abstract>Nepal Script (also known as Prachalit Script) is the widely used script of Nepal Bhasa, the native language of the Kathmandu Valley in Nepal. Derived from the Brahmi Script, the Nepal Script was developed in the 9th century and was extensively used till the 20th century, before being replaced by the Devanagari script. Numerous ancient manuscripts, inscriptions, and documents written in the Nepal Script are still available containing immense knowledge on architecture, arts, astrology, ayurveda, literature, music, tantrism, etc. To preserve and revive Nepal Bhasa, digitizing such documents plays a crucial role. This paper presents our work on text recognition for the Nepal Script. The implementation includes the Nepal Script text recognizer based on CRNN CTC architecture aided by line and word segmentations. Leveraging a carefully curated dataset that encompasses handwritten and printed texts in the Nepal Script, our work has achieved CER of 6.65% and WER of 13.11%. The dataset used for this work is available as Nepal Script Text Dataset on Kaggle. The paper further explores the associated challenges due to the complex nature of the script such as conjuncts, modifiers and variations; and the current state of the script.</abstract>
      <url hash="1fefecd3">2024.sigul-1.29</url>
      <bibkey>nakarmi-etal-2024-nepal</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>NLP</fixed-case> for Arbëresh: How an Endangered Language Learns to Write in the 21st Century</title>
      <author><first>Giulio</first><last>Cusenza</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>252–256</pages>
      <abstract>Societies are becoming more and more connected, and minority languages often find themselves helpless against the advent of the digital age, with their speakers having to regularly turn to other languages for written communication. This work introduces the case of Arbëresh, a southern Italian language related to Albanian. It presents the very first machine-readable Arbëresh data, collected through a web campaign, and describes a set of tools developed to enable the Arbëresh people to learn how to write their language, including a spellchecker, a conjugator, a numeral generator, and an interactive platform to learn Arbëresh spelling. A comprehensive web application was set up to make these tools available to the public, as well as to collect further data through them. This method can be replicated to help revive other minority languages in a situation similar to Arbëresh’s. The main challenges of the process were the extremely low-resource setting and the variability of Arbëresh dialects.</abstract>
      <url hash="e1ec79f3">2024.sigul-1.30</url>
      <bibkey>cusenza-coltekin-2024-nlp</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>P</fixed-case>ersian<fixed-case>E</fixed-case>mo: Enhancing <fixed-case>F</fixed-case>arsi-<fixed-case>D</fixed-case>ari Emotion Analysis with a Hybrid Transformer and Recurrent Neural Network Model</title>
      <author><first>Mohammad Ali</first><last>Hussiny</last></author>
      <author><first>Mohammad Arif</first><last>Payenda</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <pages>257–263</pages>
      <abstract>Emotion analysis is a critical research domain within the field of natural language processing (NLP). While substantial progress has been made in this area for the Persian language, there is still a need for more precise models and larger datasets specifically focusing on the Farsi and Dari dialects. In this research, we introduce “LearnArmanEmo” as a new dataset and a superior ensemble approach for Persian text emotion classification. Our proposed model, which combines XLM-RoBERTa-large and BiGRU, undergoes evaluation on LetHerLearn for the Dari dialect, ARMANEMO for the Farsi dialect, and LearnArmanEmo for both Dari and Farsi dialects. The empirical results substantiate the efficacy of our approach with the combined model demonstrating superior performance. Specifically, our model achieves an F1 score of 72.9% on LetHerLearn, an F1 score of 77.1% on ARMANEMO, and an F1 score of 78.8% on the LearnArmanEmo dataset, establishing it as a better ensemble model for these datasets. These findings underscore the potential of this hybrid model as a useful tool for enhancing the performance of emotion analysis in Persian language processing.</abstract>
      <url hash="babd90a8">2024.sigul-1.31</url>
      <bibkey>hussiny-etal-2024-persianemo</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>P</fixed-case>hilippine Languages Database: A Multilingual Speech Corpora for Developing Systems for Low-Resource Languages</title>
      <author><first>Rowena Cristina L.</first><last>Guevara</last></author>
      <author><first>Rhandley D.</first><last>Cajote</last></author>
      <author><first>Michael Gringo Angelo R.</first><last>Bayona</last></author>
      <author><first>Crisron Rudolf G.</first><last>Lucas</last></author>
      <pages>264–271</pages>
      <abstract>Previous efforts to collect Filipino speech were done in the development of Filipino-Speech Corpus, TAGCO, and Filipino-Bisaya speech corpus. These corpora, however, are either domain-specific, non-parallel, non-multilingual or relatively insufficient for the development of state-of-the-art Automatic Speech Recognizers (ASR) and Text-To-Speech Systems (TTS) which usually requires hundreds of hours of speech data. This paper presents a multilingual corpora for the Philippine languages namely: Filipino, English, Cebuano, Kapampangan, Hiligaynon, Ilokano, Bikolano, Waray, and Tausug. PLD includes over 454 hours of recordings from speakers of the ten languages, covering multiple domains in news, medical, education, tourism and spontaneous speech. The applicability of the corpus has also been demonstrated in adult and children ASR, phoneme transcriber, voice conversion, and TTS applications.</abstract>
      <url hash="44ff74f0">2024.sigul-1.32</url>
      <bibkey>guevara-etal-2024-philippine</bibkey>
    </paper>
    <paper id="33">
      <title>Prompting towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with <fixed-case>GPT</fixed-case> as a Pivot</title>
      <author><first>Michelle</first><last>Terblanche</last></author>
      <author><first>Kayode</first><last>Olaleye</last></author>
      <author><first>Vukosi</first><last>Marivate</last></author>
      <pages>272–282</pages>
      <abstract>Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans–English and Yoruba–English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans–English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.</abstract>
      <url hash="d0de42d2">2024.sigul-1.33</url>
      <bibkey>terblanche-etal-2024-prompting</bibkey>
    </paper>
    <paper id="34">
      <title>Quantifying the Ethical Dilemma of Using Culturally Toxic Training Data in <fixed-case>AI</fixed-case> Tools for Indigenous Languages</title>
      <author><first>Pedro Henrique</first><last>Domingues</last></author>
      <author><first>Claudio Santos</first><last>Pinhanez</last></author>
      <author><first>Paulo</first><last>Cavalin</last></author>
      <author><first>Julio</first><last>Nogima</last></author>
      <pages>283–293</pages>
      <abstract>This paper tries to quantify the ethical dilemma of using culturally toxic training data to improve the performance of AI tools for ultra low-resource languages such as Indigenous languages. Our case study explores the use of Bible data which is both a commonly available source of training pairs for translators of Indigenous languages and a text which has a trail of physical and cultural violence for many Indigenous communities. In the context of fine-tuning a WMT19 German-to-English model into a Guarani Mbya-to-English translator, we first show, with two commonly-used Machine Translation metrics, that using only Bible data is not enough to create successful translators for everyday sentences gathered from a dictionary. Indeed, even fine-tuning with only 3,000 pairs of data from the dictionary produces significant increases in accuracy compared to Bible-only models. We then show that simultaneously fine-tuning with dictionary and Bible data achieves a substantial increase over the accuracy of a dictionary-only trained translator, and similarly happens when using two-step methods of fine-tuning. However, we also observed some, measurable, contaminated text from the Bible into the outputs of the best translator, creating concerns about its release to an Indigenous community. We end by discussing mechanisms to mitigate the negative impacts of this contamination.</abstract>
      <url hash="96f2a92a">2024.sigul-1.34</url>
      <bibkey>domingues-etal-2024-quantifying</bibkey>
    </paper>
    <paper id="35">
      <title>Residual Dropout: A Simple Approach to Improve Transformer’s Data Efficiency</title>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Francesca</first><last>De Luca Fornaciari</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <pages>294–299</pages>
      <abstract>Transformer models often demand a vast amount of training data to achieve the desired level of performance. However, this data requirement poses a major challenge for low-resource languages seeking access to high-quality systems, particularly in tasks like Machine Translation. To address this issue, we propose adding Dropout to Transformer’s Residual Connections. Our experimental results demonstrate that this modification effectively mitigates overfitting during training, resulting in substantial performance gains of over 4 BLEU points on a dataset consisting of merely 10 thousand examples.</abstract>
      <url hash="ccc871a5">2024.sigul-1.35</url>
      <bibkey>escolano-etal-2024-residual</bibkey>
    </paper>
    <paper id="36">
      <title>Resource Acquisition for Understudied Languages: Extracting Wordlists from Dictionaries for Computer-assisted Language Comparison</title>
      <author><first>Frederic</first><last>Blum</last></author>
      <author><first>Johannes</first><last>Englisch</last></author>
      <author><first>Alba</first><last>Hermida Rodriguez</last></author>
      <author><first>Rik</first><last>van Gijn</last></author>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <pages>300–306</pages>
      <abstract>Comparative wordlists play a crucial role for historical language comparison. They are regularly used for the identification of related words and languages, or for the reconstruction of language phylogenies and proto-languages. While automated solutions exist for the majority of methods used for this purpose, no standardized computational or computer-assisted approaches for the compilation of comparative wordlists have been proposed so far. Up to today, scholars compile wordlists by sifting manually through dictionaries or similar language resources and typing them into spreadsheets. In this study we present a semi-automatic approach to extract wordlists from machine-readable dictionaries. The transparent workflow allows to build user-defined wordlists for individual languages in a standardized format. By automating the search for translation equivalents in dictionaries, our approach greatly facilitates the aggregation of individual resources into multilingual comparative wordlists that can be used for a variety of purposes.</abstract>
      <url hash="4329cf61">2024.sigul-1.36</url>
      <bibkey>blum-etal-2024-resource</bibkey>
    </paper>
    <paper id="37">
      <title>Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation</title>
      <author><first>Seunghyun</first><last>Ji</last></author>
      <author><first>Hagai Raja</first><last>Sinulingga</last></author>
      <author><first>Darongsae</first><last>Kwon</last></author>
      <pages>307–317</pages>
      <abstract>Low-resourced data presents a significant challenge for neural machine translation. In most cases, the low-resourced environment is caused by high costs due to the need for domain experts or the lack of language experts. Therefore, identifying the most training-efficient data within an unsupervised setting emerges as a practical strategy. Recent research suggests that such effective data can be identified by selecting ‘appropriately complex data’ based on its volume, providing strong intuition for unsupervised data selection. However, we have discovered that establishing criteria for unsupervised data selection remains a challenge, as the ‘appropriate level of difficulty’ may vary depending on the data domain. We introduce a novel unsupervised data selection method named ‘Capturing Perplexing Named Entities,’ which leverages the maximum inference entropy in translated named entities as a metric for selection. When tested with the ‘Korean-English Parallel Corpus of Specialized Domains,’ our method served as robust guidance for identifying training-efficient data across different domains, in contrast to existing methods.</abstract>
      <url hash="7b41737a">2024.sigul-1.37</url>
      <bibkey>ji-etal-2024-robust</bibkey>
    </paper>
    <paper id="38">
      <title>Seeding Alignment between Language Technology and Indigenous Methodologies: A Decolonizing Framework for Endangered Language Revitalization</title>
      <author><first>Craig John</first><last>Carpenter</last></author>
      <author><first>John</first><last>Lyon</last></author>
      <author><first>Miles</first><last>Thorogood</last></author>
      <author><first>Jeannette C.</first><last>Armstrong</last></author>
      <pages>318–324</pages>
      <abstract>The integration of a speech technology into a digital edition to support the acquisition of a critically endangered Indigenous language is a complex task. More than simply consisting of technical challenges of working with an under-resourced language, researchers face the potential of re-enacting causes of language endangerment without rigorous adherence to qualitative methodologies. Based on reflections throughout the development process of a speech technology, this paper proposes a cross-disciplinary decolonizing framework for researchers working in the field of computational linguistics for Indigenous Language Revitalization (ILR). The authors propose a series of qualitative methodologies to ensure alignment with the language community which the technology is intended to benefit. The proposed relational framework is designed to sustain the integrity of the Four Rs: a series of principles first presented by Verna J. Kirkness and Ray Barnhardt in their 1991 article, “First Nations and Higher Education: The Four R’s - Respect, Relevance, Reciprocity, Responsibility”.</abstract>
      <url hash="77f8dcd7">2024.sigul-1.38</url>
      <bibkey>carpenter-etal-2024-seeding</bibkey>
      <erratum id="1" hash="0faad5e6" date="2024-06-19">2024.sigul-1.38e1</erratum>
    </paper>
    <paper id="39">
      <title>Solving Failure Modes in the Creation of Trustworthy Language Technologies</title>
      <author><first>Gianna</first><last>Leoni</last></author>
      <author><first>Lee</first><last>Steven</last></author>
      <author><first>Tūreiti</first><last>Keith</last></author>
      <author><first>Keoni</first><last>Mahelona</last></author>
      <author><first>Peter-Lucas</first><last>Jones</last></author>
      <author><first>Suzanne</first><last>Duncan</last></author>
      <pages>325–330</pages>
      <abstract>To produce high-quality Natural Language Processing (NLP) technologies for low-resource languages, authentic leadership and participation from the low-resource language community is crucial. This reduces chances of bias, surveillance and the inclusion of inaccurate data that can negatively impact output in language technologies. It also ensures that decision-making throughout the pipeline of work centres on the language community rather than only prioritising metrics. The NLP building process involves a range of steps and decisions to ensure the production of successful models and outputs. Rarely does a model perform as expected or desired the first time it is deployed for testing, resulting in the need for re-assessment and re-deployment. This paper discusses the process involved in solving failure modes for a Māori language automatic speech recognition (ASR) model. It explains how the data is curated and how language and data specialists offer unparalleled insight into the debugging process because of their knowledge of the data. This expertise has a significant influence on decision-making to ensure the entire pipeline is embedded in ethical practice and the work is culturally appropriate for the Māori language community thus creating trustworthy language technology.</abstract>
      <url hash="dc69ef85">2024.sigul-1.39</url>
      <bibkey>leoni-etal-2024-solving</bibkey>
    </paper>
    <paper id="40">
      <title>Tandem Long-Short Duration-based Modeling for Automatic Speech Recognition</title>
      <author><first>Dalai</first><last>Mengke</last></author>
      <author><first>Yan</first><last>Meng</last></author>
      <author><first>Peter</first><last>Mihajlik</last></author>
      <pages>331–336</pages>
      <abstract>This study outlines our duration-dependent modeling experiments on limited-resource Hungarian speech recognition tasks. As it is well known, very short utterances pose significant challenges in automatic speech recognition due to the lack of context and other phenomena. In particular, we found that that the exclusion of shorter speech samples from fine-tuning for longer duration test data significantly improves the recognition rate measured on public Hungarian datasets, BEA-Base and CommonVoice (CV). Therefore we apply a tandem modeling approach, separate models are used for short and long duration test data. Our strategy improved the ability to recognize short utterances while maintaining recognition of long utterances efficiently, which led to a significant increase in overall recognition accuracy.</abstract>
      <url hash="2310d0b9">2024.sigul-1.40</url>
      <bibkey>mengke-etal-2024-tandem</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>TELP</fixed-case> – Text Extraction with Linguistic Patterns</title>
      <author><first>João</first><last>Cordeiro</last></author>
      <author><first>Purificação Moura</first><last>Silvano</last></author>
      <author><first>António</first><last>Leal</last></author>
      <author><first>Sebastião</first><last>Pais</last></author>
      <pages>337–344</pages>
      <abstract>Linguistic studies in under-resourced languages pose additional challenges at various levels, including the automatic collection of examples, cases, and corpora construction. Several sophisticated applications, such as GATE (Cunningham, 2002), can be configured/adjusted/programmed by experts to automatically collect examples from the Web in any language. However, these applications are too complex and intricate to be operated, requiring, in some cases, skills in computer science. In this work, we present TELP, a tool that allows for the simplified expression of linguistic patterns to extract case studies automatically from World Wide Web sites. It is a straightforward application with an intuitive GUI and a quick learning curve, facilitating its broad use by researchers from different domains. In this paper, we describe the operational and technical aspects of TELP and some relatively recent and relevant use cases in the field of linguistic studies.</abstract>
      <url hash="914dd83e">2024.sigul-1.41</url>
      <bibkey>cordeiro-etal-2024-telp</bibkey>
    </paper>
    <paper id="42">
      <title>The First Parallel Corpus and Neural Machine Translation Model of <fixed-case>W</fixed-case>estern <fixed-case>A</fixed-case>rmenian and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Ari Nubar</first><last>Boyacıoğlu</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>345–356</pages>
      <abstract>Western Armenian is a low-resource language spoken by the Armenian Diaspora residing in various places of the world. Although having content on the internet as well as a relatively rich literary heritage for a minority language, there is no data for the machine translation task and only a very limited amount of labeled data for other NLP tasks. In this work, we build the first machine translation system between Western Armenian and English. We explore different techniques for data collection and evaluate their impact in this very low-resource scenario. Then, we build the machine translation system while focusing on the possibilities of performing knowledge transfer from Eastern Armenian. The system is finetuned with the data collected for the first Western Armenian-English parallel corpus, which contains a total of approximately 147k sentence pairs, whose shareable part of 52k examples was made open-source. The best system through the experiments performs with a BLEU score of 29.8 while translating into English and 17 into Western Armenian.</abstract>
      <url hash="4fe93435">2024.sigul-1.42</url>
      <bibkey>boyacioglu-niehues-2024-first</bibkey>
    </paper>
    <paper id="43">
      <title>Tracing Linguistic Heritage: Constructing a <fixed-case>S</fixed-case>omali-<fixed-case>I</fixed-case>talian Terminological Resource through Explorers’ Notebooks and Contemporary Corpus Analysis</title>
      <author><first>Silvia</first><last>Piccini</last></author>
      <author><first>Giuliana Elizabeth</first><last>Vilela Ruiz</last></author>
      <author><first>Andrea</first><last>Bellandi</last></author>
      <author><first>Enrico</first><last>Carniani</last></author>
      <pages>357–362</pages>
      <abstract>The aim of this contribution is to introduce the initial phases of constructing a Somali-Italian terminological resource that dates back to Italy’s colonial expansion into Africa. Specifically, the terminological data was extracted from the notebooks authored by the Italian explorer Ugo Ferrandi (1852 - 1928) and published by the Società Geografica in 1903 under the title “Lugh. Emporio Commerciale sul Giuba”. In order to develop Ferrandi’s terminological resource, we have employed Semantic Web technologies (RDF, OWL, and SPARQL) and embraced the Linked Open Data paradigm. This ensures the FAIRness of the data and enables the publication and sharing of our terminological resource within an open interconnected Web of Data, thus contributing to addressing the absence of Somali in the Linguistic Linked Data cloud. Whenever feasible, Ferrandi’s lexicon entries have been linked and enriched with information derived from a Somali lexicon included in a contemporary Somali Corpus. This approach allows the synchronic corpus-related Somali lexicon to acquire historical depth, thereby illuminating the linguistic dynamics that have transpired over time and would otherwise have remained obscure.</abstract>
      <url hash="35c24c0d">2024.sigul-1.43</url>
      <bibkey>piccini-etal-2024-tracing</bibkey>
    </paper>
    <paper id="44">
      <title>Uncovering Social Changes of the <fixed-case>B</fixed-case>asque Speaking <fixed-case>T</fixed-case>witter Community During <fixed-case>COVID</fixed-case>-19 Pandemic</title>
      <author><first>Joseba</first><last>Fernandez de Landa</last></author>
      <author><first>Iker</first><last>García-Ferrero</last></author>
      <author><first>Ander</first><last>Salaberria</last></author>
      <author><first>Jon Ander</first><last>Campos</last></author>
      <pages>363–371</pages>
      <abstract>The aim of this work is to study the impact of the COVID-19 pandemic on the Basque speaking Twitter community by applying Natural Language Processing unsupervised techniques. In order to carry out this study, we collected and publicly released the biggest dataset of Basque tweets containing up to 8M tweets from September 2019 to February 2021. To analyze the impact of the pandemic, the variability of the content over time was studied through quantitative and qualitative analysis of words and emojis. For the quantitative analysis, the shift at the frequency of the terms was calculated using linear regression over frequencies. On the other hand, for the qualitative analysis, word embeddings were used to study the changes in the meaning of the most significant words and emojis at different periods of the pandemic. Through this multifaceted approach, we discovered noteworthy alterations in the political inclinations exhibited by Basque users throughout the course of the pandemic.</abstract>
      <url hash="cc81d3b9">2024.sigul-1.44</url>
      <bibkey>fernandez-de-landa-etal-2024-uncovering</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>U</fixed-case>ni<fixed-case>D</fixed-case>ive: A <fixed-case>COST</fixed-case> Action on Universality, Diversity and Idiosyncrasy in Language Technology</title>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Anabela</first><last>Barreiro</last></author>
      <author><first>Olesea</first><last>Caftanatov</last></author>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Kaja</first><last>Dobrovoljc</last></author>
      <author><first>Gülşen</first><last>Eryiğit</last></author>
      <author><first>Voula</first><last>Giouli</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <author><first>Nurit</first><last>Melnik</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Abigail</first><last>Walsh</last></author>
      <author><first>Beata</first><last>Wójtowicz</last></author>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <pages>372–382</pages>
      <abstract>This paper presents the objectives, organization and activities of the UniDive COST Action, a scientific network dedicated to universality, diversity and idiosyncrasy in language technology. We describe the objectives and organization of this initiative, the people involved, the working groups and the ongoing tasks and activities. This paper is also an pen call for participation towards new members and countries.</abstract>
      <url hash="ae16b627">2024.sigul-1.45</url>
      <bibkey>savary-etal-2024-unidive</bibkey>
      <revision id="1" href="2024.sigul-1.45v1" hash="1f9c2d06"/>
      <revision id="2" href="2024.sigul-1.45v2" hash="ae16b627" date="2025-07-25">Added a sponsor.</revision>
    </paper>
    <paper id="46">
      <title>Unsupervised Outlier Detection for Language-Independent Text Quality Filtering</title>
      <author><first>Jón</first><last>Daðason</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <pages>383–393</pages>
      <abstract>Web-crawled corpora offer an abundant source of training data for language models. However, they are generally noisy and are typically filtered using heuristic rules or classifiers. These methods require careful tuning or labeling by fluent speakers. In this paper, we assess the effectiveness of commonly applied rules on TQ-IS, a manually labeled text quality dataset for Icelandic. Additionally, we advocate for the utilization of unsupervised clustering and outlier detection algorithms for filtering. These algorithms are language-independent, computationally efficient and do not require language expertise. Using grid search, we find the optimal configuration for every combination of rules, optimizing for F1 score on TQ-IS. For a rule-based approach, we discover that optimal results can be achieved with only a small subset of the full ruleset. Using five rules, we obtain an F1 score of 98.2%. We then evaluate three unsupervised algorithms, i.e., Gaussian Mixture Models (GMMs), Isolation Forests and One-Class SVMs. Our findings reveal that unsupervised algorithms perform well on the TQ-IS dataset, with GMMs obtaining the best results, comparable to those obtained with the rule-based approach. Finally, we show that unsupervised methods appear to be equally suitable for languages other than Icelandic, including Estonian and Basque.</abstract>
      <url hash="97694b18">2024.sigul-1.46</url>
      <bibkey>dadason-loftsson-2024-unsupervised</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>U</fixed-case>z<fixed-case>ABSA</fixed-case>: Aspect-Based Sentiment Analysis for the <fixed-case>U</fixed-case>zbek Language</title>
      <author><first>Sanatbek Gayratovich</first><last>Matlatipov</last></author>
      <author><first>Jaloliddin</first><last>Rajabov</last></author>
      <author><first>Elmurod</first><last>Kuriyozov</last></author>
      <author><first>Mersaid</first><last>Aripov</last></author>
      <pages>394–403</pages>
      <abstract>The objective of enhancing the availability of natural language processing technologies for low-resource languages has significant importance in facilitating technological accessibility within the populations of speakers of these languages. Our current grasping shows that there are no established linguistic resources available open source to develop aspect-based sentiment analysis (ABSA) tools tailored to the Uzbek language. This work aims to address the aforementioned gap by presenting the first high-quality annotated ABSA dataset - UzABSA. The data used in this study was obtained from a compilation of online reviews of Uzbek restaurants. Consequently, the constructed dataset has a length of 3500 reviews at the document level and 6100+ sentences at the sentence level. The popular approach to language resources of this kind explores four distinctive characteristics, namely Aspect Terms, Aspect Term Polarities, Aspect Category Terms, as well as Aspect Category Polarities. To the best of our knowledge, it is the first and the largest ABSA dataset for the Uzbek language. To evaluate the annotation process of our dataset, we used established statistical techniques such as Cohen’s kappa coefficient and Krippendorff’s <tex-math>\alpha</tex-math> to assess agreement between annotators. Subsequently, a classification model, namely K-Nearest Neighbour (KNN), was used to evaluate the performance of the created dataset. Both sets of evaluation techniques demonstrate comparable levels of accuracy. The first findings across the various tasks showed promising outcomes, with accuracy rates ranging from 72% to 88%. This study not only highlights the significance of our acquired dataset but also plays a valuable tool for scholars interested in furthering sentiment analysis in the Uzbek language.</abstract>
      <url hash="799cd9e6">2024.sigul-1.47</url>
      <bibkey>matlatipov-etal-2024-uzabsa</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>V</fixed-case>i<fixed-case>H</fixed-case>ealth<fixed-case>NLI</fixed-case>: A Dataset for <fixed-case>V</fixed-case>ietnamese Natural Language Inference in Healthcare</title>
      <author><first>Huyen</first><last>Nguyen</last></author>
      <author><first>Quyen The</first><last>Ngo</last></author>
      <author><first>Thanh-Ha</first><last>Do</last></author>
      <author><first>Tuan-Anh</first><last>Hoang</last></author>
      <pages>404–409</pages>
      <abstract>This paper introduces ViHealthNLI, a large dataset for the natural language inference problem for Vietnamese. Unlike the similar Vietnamese datasets, ours is specific to the healthcare domain. We conducted an exploratory analysis to characterize the dataset and evaluated the state-of-the-art methods on the dataset. Our findings indicate that the dataset poses significant challenges while also holding promise for further advanced research and the creation of practical applications.</abstract>
      <url hash="ec120a97">2024.sigul-1.48</url>
      <bibkey>nguyen-etal-2024-vihealthnli</bibkey>
    </paper>
    <paper id="49">
      <title>Why the Unexpected? Dissecting the Political and Economic Bias in <fixed-case>P</fixed-case>ersian Small and Large Language Models</title>
      <author><first>Ehsan</first><last>Barkhordar</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Ashwarya</first><last>Maratha</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>410–420</pages>
      <abstract>Recently, language models (LMs) like BERT and large language models (LLMs) like GPT-4 have demonstrated potential in various linguistic tasks such as text generation, translation, and sentiment analysis. However, these abilities come with a cost of a risk of perpetuating biases from their training data. Political and economic inclinations play a significant role in shaping these biases. Thus, this research aims to understand political and economic biases in Persian LMs and LLMs, addressing a significant gap in AI ethics and fairness research. Focusing on the Persian language, our research employs a two-step methodology. First, we utilize the political compass test adapted to Persian. Second, we analyze biases present in these models. Our findings indicate the presence of nuanced biases, underscoring the importance of ethical considerations in AI deployments within Persian-speaking contexts.</abstract>
      <url hash="aff90479">2024.sigul-1.49</url>
      <bibkey>barkhordar-etal-2024-unexpected</bibkey>
    </paper>
    <paper id="50">
      <title>Work in Progress: Text-to-speech on Edge Devices for Te Reo <fixed-case>M</fixed-case>āori and ‘Ōlelo Hawaiʻi</title>
      <author><first>Tūreiti</first><last>Keith</last></author>
      <pages>421–426</pages>
      <abstract>Existing popular text-to-speech technologies focus on large models requiring a large corpus of recorded speech to train. The resulting models are typically run on high-resource servers where users synthesise speech from a client device requiring constant connectivity. For speakers of low-resource languages living in remote areas, this approach does not work. Corpora are typically small and synthesis needs to run on an unconnected, battery or solar-powered edge device. In this paper, we demonstrate how knowledge transfer and adversarial training can be used to create efficient models capable of running on edge devices using a corpus of only several hours. We apply these concepts to create a voice synthesiser for te reo Māori (the indigenous language of Aotearoa New Zealand) for a non-speaking user and ‘ōlelo Hawaiʻi (the indigenous language of Hawaiʻi) for a legally blind user, thus creating the first high-quality text-to-speech tools for these endangered, central-eastern Polynesian languages capable of running on a low powered edge device.</abstract>
      <url hash="a391ab39">2024.sigul-1.50</url>
      <bibkey>keith-2024-work</bibkey>
    </paper>
  </volume>
</collection>
