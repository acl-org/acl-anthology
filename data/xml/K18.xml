<?xml version='1.0' encoding='UTF-8'?>
<collection id="K18">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 22nd Conference on Computational Natural Language Learning</booktitle>
      <url hash="6bce8718">K18-1</url>
      <editor><first>Anna</first><last>Korhonen</last></editor>
      <editor><first>Ivan</first><last>Titov</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Brussels, Belgium</address>
      <month>October</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="9b858de5">K18-1000</url>
    </frontmatter>
    <paper id="1">
      <title>Embedded-State Latent Conditional Random Fields for Sequence Labeling</title>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Sree Harsha</first><last>Ramesh</last></author>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Luke</first><last>Vilnis</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>1–10</pages>
      <url hash="1bfb5f25">K18-1001</url>
      <abstract>Complex textual information extraction tasks are often posed as sequence labeling or <i>shallow parsing</i>, where fields are extracted using local labels made consistent through probabilistic inference in a graphical model with constrained transitions. Recently, it has become common to locally parametrize these models using rich features extracted by recurrent neural networks (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing Markovian dependencies between successive labels. However, the simple graphical model structure belies the often complex non-local constraints between output labels. For example, many fields, such as a first name, can only occur a fixed number of times, or in the presence of other fields. While RNNs have provided increasingly powerful context-aware local features for sequence tagging, they have yet to be integrated with a global graphical model of similar expressivity in the output distribution. Our model goes beyond the linear chain CRF to incorporate multiple hidden states per output label, but parametrizes them parsimoniously with low-rank log-potential scoring matrices, effectively learning an embedding space for hidden states. This augmented latent space of inference variables complements the rich feature representation of the RNN, and allows exact global inference obeying complex, learned non-local output constraints. We experiment with several datasets and show that the model outperforms baseline CRF+RNN models when global output constraints are necessary at inference-time, and explore the interpretable latent structure.</abstract>
      <doi>10.18653/v1/K18-1001</doi>
    </paper>
    <paper id="2">
      <title>Continuous Word Embedding Fusion via Spectral Decomposition</title>
      <author><first>Tianfan</first><last>Fu</last></author>
      <author><first>Cheng</first><last>Zhang</last></author>
      <author><first>Stephan</first><last>Mandt</last></author>
      <pages>11–20</pages>
      <url hash="3446ee86">K18-1002</url>
      <abstract>Word embeddings have become a mainstream tool in statistical natural language processing. Practitioners often use pre-trained word vectors, which were trained on large generic text corpora, and which are readily available on the web. However, pre-trained word vectors oftentimes lack important words from specific domains. It is therefore often desirable to extend the vocabulary and embed new words into a set of pre-trained word vectors. In this paper, we present an efficient method for including new words from a specialized corpus, containing new words, into pre-trained generic word embeddings. We build on the established view of word embeddings as matrix factorizations to present a spectral algorithm for this task. Experiments on several domain-specific corpora with specialized vocabularies demonstrate that our method is able to embed the new words efficiently into the original embedding space. Compared to competing methods, our method is faster, parameter-free, and deterministic.</abstract>
      <doi>10.18653/v1/K18-1002</doi>
    </paper>
    <paper id="3">
      <title>Dual Latent Variable Model for Low-Resource Natural Language Generation in Dialogue Systems</title>
      <author><first>Van-Khanh</first><last>Tran</last></author>
      <author><first>Le-Minh</first><last>Nguyen</last></author>
      <pages>21–30</pages>
      <url hash="ee8332eb">K18-1003</url>
      <abstract>Recent deep learning models have shown improving results to natural language generation (NLG) irrespective of providing sufficient annotated data. However, a modest training data may harm such models’ performance. Thus, how to build a generator that can utilize as much of knowledge from a low-resource setting data is a crucial issue in NLG. This paper presents a variational neural-based generation model to tackle the NLG problem of having limited labeled dataset, in which we integrate a variational inference into an encoder-decoder generator and introduce a novel auxiliary auto-encoding with an effective training procedure. Experiments showed that the proposed methods not only outperform the previous models when having sufficient training dataset but also demonstrate strong ability to work acceptably well when the training data is scarce.</abstract>
      <doi>10.18653/v1/K18-1003</doi>
    </paper>
    <paper id="4">
      <title>A Trio Neural Model for Dynamic Entity Relatedness Ranking</title>
      <author><first>Tu</first><last>Nguyen</last></author>
      <author><first>Tuan</first><last>Tran</last></author>
      <author><first>Wolfgang</first><last>Nejdl</last></author>
      <pages>31–41</pages>
      <url hash="9015a14e">K18-1004</url>
      <abstract>Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in a static setting and unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity relations are very dynamic over time. In this work, we propose a neural network-based approach that leverages public attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.</abstract>
      <doi>10.18653/v1/K18-1004</doi>
    </paper>
    <paper id="5">
      <title>A Unified Neural Network Model for Geolocating <fixed-case>T</fixed-case>witter Users</title>
      <author><first>Mohammad</first><last>Ebrahimi</last></author>
      <author><first>Elaheh</first><last>ShafieiBavani</last></author>
      <author><first>Raymond</first><last>Wong</last></author>
      <author><first>Fang</first><last>Chen</last></author>
      <pages>42–53</pages>
      <url hash="453ebfae">K18-1005</url>
      <abstract>Locations of social media users are important to many applications such as rapid disaster response, targeted advertisement, and news recommendation. However, many users do not share their exact geographical coordinates due to reasons such as privacy concerns. The lack of explicit location information has motivated a growing body of research in recent years looking at different automatic ways of determining the user’s primary location. In this paper, we propose a unified user geolocation method which relies on a fusion of neural networks. Our joint model incorporates different types of available information including tweet text, user network, and metadata to predict users’ locations. Moreover, we utilize a bidirectional LSTM network augmented with an attention mechanism to identify the most location indicative words in textual content of tweets. The experiments demonstrate that our approach achieves state-of-the-art performance over two Twitter benchmark geolocation datasets. We also conduct an ablation study to evaluate the contribution of each type of information in user geolocation performance.</abstract>
      <doi>10.18653/v1/K18-1005</doi>
    </paper>
    <paper id="6">
      <title>Corpus-Driven Thematic Hierarchy Induction</title>
      <author><first>Ilia</first><last>Kuznetsov</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>54–64</pages>
      <url hash="6a19bb8e">K18-1006</url>
      <abstract>Thematic role hierarchy is a widely used linguistic tool to describe interactions between semantic roles and their syntactic realizations. Despite decades of dedicated research and numerous thematic hierarchy suggestions in the literature, this concept has not been used in NLP so far due to incompatibility and limited scope of existing hierarchies. We introduce an empirical framework for thematic hierarchy induction and evaluate several role ranking strategies on English and German full-text corpus data. We hypothesize that global thematic hierarchy induction is feasible, that a hierarchy can be induced from just fractions of training data and that resulting hierarchies apply cross-lingually. We evaluate these assumptions empirically.</abstract>
      <doi>10.18653/v1/K18-1006</doi>
    </paper>
    <paper id="7">
      <title>Adversarially Regularising Neural <fixed-case>NLI</fixed-case> Models to Integrate Logical Background Knowledge</title>
      <author><first>Pasquale</first><last>Minervini</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>65–74</pages>
      <url hash="470459df">K18-1007</url>
      <abstract>Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation. In NLP, however, most example generation strategies produce input text by using known, pre-specified semantic transformations, requiring significant manual effort and in-depth understanding of the problem and domain. In this paper, we investigate the problem of automatically generating adversarial examples that violate a set of given First-Order Logic constraints in Natural Language Inference (NLI). We reduce the problem of identifying such adversarial examples to a combinatorial optimisation problem, by maximising a quantity measuring the degree of violation of such constraints and by using a language model for generating linguistically-plausible examples. Furthermore, we propose a method for adversarially regularising neural NLI models for incorporating background knowledge. Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it significantly and consistently increases the predictive accuracy on adversarially-crafted datasets – up to a 79.6% relative improvement – while drastically reducing the number of background knowledge violations. Furthermore, we show that adversarial examples transfer among model architectures, and that the proposed adversarial training procedure improves the robustness of NLI models to adversarial examples.</abstract>
      <doi>10.18653/v1/K18-1007</doi>
    </paper>
    <paper id="8">
      <title>From Strings to Other Things: Linking the Neighborhood and Transposition Effects in Word Reading</title>
      <author><first>Stéphan</first><last>Tulkens</last></author>
      <author><first>Dominiek</first><last>Sandra</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>75–85</pages>
      <url hash="433baa6e">K18-1008</url>
      <abstract>We investigate the relation between the transposition and deletion effects in word reading, i.e., the finding that readers can successfully read “SLAT” as “SALT”, or “WRK” as “WORK”, and the neighborhood effect. In particular, we investigate whether lexical orthographic neighborhoods take into account transposition and deletion in determining neighbors. If this is the case, it is more likely that the neighborhood effect takes place early during processing, and does not solely rely on similarity of internal representations. We introduce a new neighborhood measure, rd20, which can be used to quantify neighborhood effects over arbitrary feature spaces. We calculate the rd20 over large sets of words in three languages using various feature sets and show that feature sets that do not allow for transposition or deletion explain more variance in Reaction Time (RT) measurements. We also show that the rd20 can be calculated using the hidden state representations of an Multi-Layer Perceptron, and show that these explain less variance than the raw features. We conclude that the neighborhood effect is unlikely to have a perceptual basis, but is more likely to be the result of items co-activating after recognition. All code is available at: <url>www.github.com/clips/conll2018</url>
      </abstract>
      <doi>10.18653/v1/K18-1008</doi>
    </paper>
    <paper id="9">
      <title>Global Attention for Name Tagging</title>
      <author><first>Boliang</first><last>Zhang</last></author>
      <author><first>Spencer</first><last>Whitehead</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>86–96</pages>
      <url hash="763a42dd">K18-1009</url>
      <abstract>Many name tagging approaches use local contextual information with much success, but can fail when the local context is ambiguous or limited. We present a new framework to improve name tagging by utilizing local, document-level, and corpus-level contextual information. For each word, we retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other documents. We propose a model that learns to incorporate document-level and corpus-level contextual information alongside local contextual information via document-level and corpus-level attentions, which dynamically weight their respective contextual information and determines the influence of this information through gating mechanisms. Experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL-2003 datasets. We will make our code and pre-trained models publicly available for research purposes.</abstract>
      <doi>10.18653/v1/K18-1009</doi>
    </paper>
    <paper id="10">
      <title>Pervasive Attention: 2<fixed-case>D</fixed-case> Convolutional Neural Networks for Sequence-to-Sequence Prediction</title>
      <author><first>Maha</first><last>Elbayad</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Jakob</first><last>Verbeek</last></author>
      <pages>97–107</pages>
      <url hash="956499b1">K18-1010</url>
      <abstract>Current state-of-the-art machine translation systems are based on encoder-decoder architectures, that first encode the input sequence, and then generate an output sequence based on the input encoding. Both are interfaced with an attention mechanism that recombines a fixed encoding of the source tokens based on the decoder state. We propose an alternative approach which instead relies on a single 2D convolutional neural network across both sequences. Each layer of our network re-codes source tokens on the basis of the output sequence produced so far. Attention-like properties are therefore pervasive throughout the network. Our model yields excellent results, outperforming state-of-the-art encoder-decoder systems, while being conceptually simpler and having fewer parameters.</abstract>
      <doi>10.18653/v1/K18-1010</doi>
    </paper>
    <paper id="11">
      <title>Comparing Attention-Based Convolutional and Recurrent Neural Networks: Success and Limitations in Machine Reading Comprehension</title>
      <author><first>Matthias</first><last>Blohm</last></author>
      <author><first>Glorianna</first><last>Jagfeld</last></author>
      <author><first>Ekta</first><last>Sood</last></author>
      <author><first>Xiang</first><last>Yu</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>108–118</pages>
      <url hash="4eb10243">K18-1011</url>
      <abstract>We propose a machine reading comprehension model based on the compare-aggregate framework with two-staged attention that achieves state-of-the-art results on the MovieQA question answering dataset. To investigate the limitations of our model as well as the behavioral difference between convolutional and recurrent neural networks, we generate adversarial examples to confuse the model and compare to human performance. Furthermore, we assess the generalizability of our model by analyzing its differences to human inference, drawing upon insights from cognitive science.</abstract>
      <doi>10.18653/v1/K18-1011</doi>
    </paper>
    <paper id="12">
      <title>Uncovering Code-Mixed Challenges: A Framework for Linguistically Driven Question Generation and Neural Based Question Answering</title>
      <author><first>Deepak</first><last>Gupta</last></author>
      <author><first>Pabitra</first><last>Lenka</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>119–130</pages>
      <url hash="67b55f94">K18-1012</url>
      <abstract>Existing research on question answering (QA) and comprehension reading (RC) are mainly focused on the resource-rich language like English. In recent times, the rapid growth of multi-lingual web content has posed several challenges to the existing QA systems. Code-mixing is one such challenge that makes the task more complex. In this paper, we propose a linguistically motivated technique for code-mixed question generation (CMQG) and a neural network based architecture for code-mixed question answering (CMQA). For evaluation, we manually create the code-mixed questions for Hindi-English language pair. In order to show the effectiveness of our neural network based CMQA technique, we utilize two benchmark datasets, SQuAD and MMQA. Experiments show that our proposed model achieves encouraging performance on CMQG and CMQA.</abstract>
      <doi>10.18653/v1/K18-1012</doi>
    </paper>
    <paper id="13">
      <title>Learning to Embed Semantic Correspondence for Natural Language Understanding</title>
      <author><first>Sangkeun</first><last>Jung</last></author>
      <author><first>Jinsik</first><last>Lee</last></author>
      <author><first>Jiwon</first><last>Kim</last></author>
      <pages>131–140</pages>
      <url hash="f140b007">K18-1013</url>
      <abstract>While learning embedding models has yielded fruitful results in several NLP subfields, most notably Word2Vec, embedding correspondence has relatively not been well explored especially in the context of natural language understanding (NLU), a task that typically extracts structured semantic knowledge from a text. A NLU embedding model can facilitate analyzing and understanding relationships between unstructured texts and their corresponding structured semantic knowledge, essential for both researchers and practitioners of NLU. Toward this end, we propose a framework that learns to embed semantic correspondence between text and its extracted semantic knowledge, called semantic frame. One key contributed technique is semantic frame reconstruction used to derive a one-to-one mapping between embedded vectors and their corresponding semantic frames. Embedding into semantically meaningful vectors and computing their distances in vector space provides a simple, but effective way to measure semantic similarities. With the proposed framework, we demonstrate three key areas where the embedding model can be effective: visualization, semantic search and re-ranking.</abstract>
      <doi>10.18653/v1/K18-1013</doi>
    </paper>
    <paper id="14">
      <title>Commonsense Knowledge Base Completion and Generation</title>
      <author><first>Itsumi</first><last>Saito</last></author>
      <author><first>Kyosuke</first><last>Nishida</last></author>
      <author><first>Hisako</first><last>Asano</last></author>
      <author><first>Junji</first><last>Tomita</last></author>
      <pages>141–150</pages>
      <url hash="346da5a4">K18-1014</url>
      <abstract>This study focuses on acquisition of commonsense knowledge. A previous study proposed a commonsense knowledge base completion (CKB completion) method that predicts a confidence score of for triplet-style knowledge for improving the coverage of CKBs. To improve the accuracy of CKB completion and expand the size of CKBs, we formulate a new commonsense knowledge base generation task (CKB generation) and propose a joint learning method that incorporates both CKB completion and CKB generation. Experimental results show that the joint learning method improved completion accuracy and the generation model created reasonable knowledge. Our generation model could also be used to augment data and improve the accuracy of completion.</abstract>
      <doi>10.18653/v1/K18-1014</doi>
    </paper>
    <paper id="15">
      <title>Active Learning for Interactive Neural Machine Translation of Data Streams</title>
      <author><first>Álvaro</first><last>Peris</last></author>
      <author><first>Francisco</first><last>Casacuberta</last></author>
      <pages>151–160</pages>
      <url hash="5230d0b7">K18-1015</url>
      <abstract>We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation. The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent. The user will interactively translate those samples. Once validated, these data is useful for adapting the neural machine translation model. We propose two novel methods for selecting the samples to be validated. We exploit the information from the attention mechanism of a neural machine translation system. Our experiments show that the inclusion of active learning techniques into this pipeline allows to reduce the effort required during the process, while increasing the quality of the translation system. Moreover, it enables to balance the human effort required for achieving a certain translation quality. Moreover, our neural system outperforms classical approaches by a large margin.</abstract>
      <doi>10.18653/v1/K18-1015</doi>
    </paper>
    <paper id="16">
      <title>Churn Intent Detection in Multilingual Chatbot Conversations and Social Media</title>
      <author><first>Christian</first><last>Abbet</last></author>
      <author><first>Meryem</first><last>M’hamdi</last></author>
      <author><first>Athanasios</first><last>Giannakopoulos</last></author>
      <author><first>Robert</first><last>West</last></author>
      <author><first>Andreea</first><last>Hossmann</last></author>
      <author><first>Michael</first><last>Baeriswyl</last></author>
      <author><first>Claudiu</first><last>Musat</last></author>
      <pages>161–170</pages>
      <url hash="7c4dd91f">K18-1016</url>
      <abstract>We propose a new method to detect when users express the intent to leave a service, also known as churn. While previous work focuses solely on social media, we show that this intent can be detected in chatbot conversations. As companies increasingly rely on chatbots they need an overview of potentially churny users. To this end, we crowdsource and publish a dataset of churn intent expressions in chatbot interactions in German and English. We show that classifiers trained on social media data can detect the same intent in the context of chatbots. We introduce a classification architecture that outperforms existing work on churn intent detection in social media. Moreover, we show that, using bilingual word embeddings, a system trained on combined English and German data outperforms monolingual approaches. As the only existing dataset is in English, we crowdsource and publish a novel dataset of German tweets. We thus underline the universal aspect of the problem, as examples of churn intent in English help us identify churn in German tweets and chatbot conversations.</abstract>
      <doi>10.18653/v1/K18-1016</doi>
    </paper>
    <paper id="17">
      <title>Learning Text Representations for 500<fixed-case>K</fixed-case> Classification Tasks on Named Entity Disambiguation</title>
      <author><first>Ander</first><last>Barrena</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>171–180</pages>
      <url hash="e075e22d">K18-1017</url>
      <abstract>Named Entity Disambiguation algorithms typically learn a single model for all target entities. In this paper we present a word expert model and train separate deep learning models for each target entity string, yielding 500K classification tasks. This gives us the opportunity to benchmark popular text representation alternatives on this massive dataset. In order to face scarce training data we propose a simple data-augmentation technique and transfer-learning. We show that bag-of-word-embeddings are better than LSTMs for tasks with scarce training data, while the situation is reversed when having larger amounts. Transferring a LSTM which is learned on all datasets is the most effective context representation option for the word experts in all frequency bands. The experiments show that our system trained on out-of-domain Wikipedia data surpass comparable NED systems which have been trained on in-domain training data.</abstract>
      <doi>10.18653/v1/K18-1017</doi>
    </paper>
    <paper id="18">
      <title>Hierarchical Attention Based Position-Aware Network for Aspect-Level Sentiment Analysis</title>
      <author><first>Lishuang</first><last>Li</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>AnQiao</first><last>Zhou</last></author>
      <pages>181–189</pages>
      <url hash="193f8068">K18-1018</url>
      <abstract>Aspect-level sentiment analysis aims to identify the sentiment of a specific target in its context. Previous works have proved that the interactions between aspects and the contexts are important. On this basis, we also propose a succinct hierarchical attention based mechanism to fuse the information of targets and the contextual words. In addition, most existing methods ignore the position information of the aspect when encoding the sentence. In this paper, we argue that the position-aware representations are beneficial to this task. Therefore, we propose a hierarchical attention based position-aware network (HAPN), which introduces position embeddings to learn the position-aware representations of sentences and further generate the target-specific representations of contextual words. The experimental results on SemEval 2014 dataset show that our approach outperforms the state-of-the-art methods.</abstract>
      <doi>10.18653/v1/K18-1018</doi>
    </paper>
    <paper id="19">
      <title>Bidirectional Generative Adversarial Networks for Neural Machine Translation</title>
      <author><first>Zhirui</first><last>Zhang</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>190–199</pages>
      <url hash="5ccdece0">K18-1019</url>
      <abstract>Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.</abstract>
      <doi>10.18653/v1/K18-1019</doi>
    </paper>
    <paper id="20">
      <title>Latent Entities Extraction: How to Extract Entities that Do Not Appear in the Text?</title>
      <author><first>Eylon</first><last>Shoshan</last></author>
      <author><first>Kira</first><last>Radinsky</last></author>
      <pages>200–210</pages>
      <url hash="218b9c44">K18-1020</url>
      <abstract>Named-entity Recognition (NER) is an important task in the NLP field , and is widely used to solve many challenges. However, in many scenarios, not all of the entities are explicitly mentioned in the text. Sometimes they could be inferred from the context or from other indicative words. Consider the following sentence: “CMA can easily hydrolyze into free acetic acid.” Although water is not mentioned explicitly, one can infer that H2O is an entity involved in the process. In this work, we present the problem of Latent Entities Extraction (LEE). We present several methods for determining whether entities are discussed in a text, even though, potentially, they are not explicitly written. Specifically, we design a neural model that handles extraction of multiple entities jointly. We show that our model, along with multi-task learning approach and a novel task grouping algorithm, reaches high performance in identifying latent entities. Our experiments are conducted on a large biological dataset from the biochemical field. The dataset contains text descriptions of biological processes, and for each process, all of the involved entities in the process are labeled, including implicitly mentioned ones. We believe LEE is a task that will significantly improve many NER and subsequent applications and improve text understanding and inference.</abstract>
      <doi>10.18653/v1/K18-1020</doi>
    </paper>
    <paper id="21">
      <title>Generalizing <fixed-case>P</fixed-case>rocrustes Analysis for Better Bilingual Dictionary Induction</title>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>211–220</pages>
      <url hash="03dd4757">K18-1021</url>
      <abstract>Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings.</abstract>
      <doi>10.18653/v1/K18-1021</doi>
    </paper>
    <paper id="22">
      <title>Simple Unsupervised Keyphrase Extraction using Sentence Embeddings</title>
      <author><first>Kamil</first><last>Bennani-Smires</last></author>
      <author><first>Claudiu</first><last>Musat</last></author>
      <author><first>Andreea</first><last>Hossmann</last></author>
      <author><first>Michael</first><last>Baeriswyl</last></author>
      <author><first>Martin</first><last>Jaggi</last></author>
      <pages>221–229</pages>
      <url hash="c83d592a">K18-1022</url>
      <abstract>Keyphrase extraction is the task of automatically selecting a small set of phrases that best describe a given free text document. Supervised keyphrase extraction requires large amounts of labeled training data and generalizes very poorly outside the domain of the training data. At the same time, unsupervised systems have poor accuracy, and often do not generalize well, as they require the input document to belong to a larger corpus also given as input. Addressing these drawbacks, in this paper, we tackle keyphrase extraction from single documents with EmbedRank: a novel unsupervised method, that leverages sentence embeddings. EmbedRank achieves higher F-scores than graph-based state of the art systems on standard datasets and is suitable for real-time processing of large amounts of Web data. With EmbedRank, we also explicitly increase coverage and diversity among the selected keyphrases by introducing an embedding-based maximal marginal relevance (MMR) for new phrases. A user study including over 200 votes showed that, although reducing the phrases’ semantic overlap leads to no gains in F-score, our high diversity selection is preferred by humans.</abstract>
      <doi>10.18653/v1/K18-1022</doi>
    </paper>
    <paper id="23">
      <title>A Temporally Sensitive Submodularity Framework for Timeline Summarization</title>
      <author><first>Sebastian</first><last>Martschat</last></author>
      <author><first>Katja</first><last>Markert</last></author>
      <pages>230–240</pages>
      <url hash="cf429007">K18-1023</url>
      <abstract>Timeline summarization (TLS) creates an overview of long-running events via dated daily summaries for the most important dates. TLS differs from standard multi-document summarization (MDS) in the importance of date selection, interdependencies between summaries of different dates and by having very short summaries compared to the number of corpus documents. However, we show that MDS optimization models using submodular functions can be adapted to yield well-performing TLS models by designing objective functions and constraints that model the temporal dimension inherent in TLS. Importantly, these adaptations retain the elegance and advantages of the original MDS models (clear separation of features and inference, performance guarantees and scalability, little need for supervision) that current TLS-specific models lack.</abstract>
      <doi>10.18653/v1/K18-1023</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>C</fixed-case>hinese Poetry Generation with a Salient-Clue Mechanism</title>
      <author><first>Xiaoyuan</first><last>Yi</last></author>
      <author><first>Ruoyu</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>241–250</pages>
      <url hash="581170df">K18-1024</url>
      <abstract>As a precious part of the human cultural heritage, Chinese poetry has influenced people for generations. Automatic poetry composition is a challenge for AI. In recent years, significant progress has been made in this area benefiting from the development of neural networks. However, the coherence in meaning, theme or even artistic conception for a generated poem as a whole still remains a big problem. In this paper, we propose a novel Salient-Clue mechanism for Chinese poetry generation. Different from previous work which tried to exploit all the context information, our model selects the most salient characters automatically from each so-far generated line to gradually form a salient clue, which is utilized to guide successive poem generation process so as to eliminate interruptions and improve coherence. Besides, our model can be flexibly extended to control the generated poem in different aspects, for example, poetry style, which further enhances the coherence. Experimental results show that our model is very effective, outperforming three strong baselines.</abstract>
      <doi>10.18653/v1/K18-1024</doi>
    </paper>
    <paper id="25">
      <title>Multi-Modal Sequence Fusion via Recursive Attention for Emotion Recognition</title>
      <author><first>Rory</first><last>Beard</last></author>
      <author><first>Ritwik</first><last>Das</last></author>
      <author><first>Raymond W. M.</first><last>Ng</last></author>
      <author><first>P. G. Keerthana</first><last>Gopalakrishnan</last></author>
      <author><first>Luka</first><last>Eerens</last></author>
      <author><first>Pawel</first><last>Swietojanski</last></author>
      <author><first>Ondrej</first><last>Miksik</last></author>
      <pages>251–259</pages>
      <url hash="191fe90f">K18-1025</url>
      <abstract>Natural human communication is nuanced and inherently multi-modal. Humans possess specialised sensoria for processing vocal, visual, and linguistic, and para-linguistic information, but form an intricately fused percept of the multi-modal data stream to provide a holistic representation. Analysis of emotional content in face-to-face communication is a cognitive task to which humans are particularly attuned, given its sociological importance, and poses a difficult challenge for machine emulation due to the subtlety and expressive variability of cross-modal cues. Inspired by the empirical success of recent so-called End-To-End Memory Networks and related works, we propose an approach based on recursive multi-attention with a shared external memory updated over multiple gated iterations of analysis. We evaluate our model across several large multi-modal datasets and show that global contextualised memory with gated memory update can effectively achieve emotion recognition.</abstract>
      <doi>10.18653/v1/K18-1025</doi>
    </paper>
    <paper id="26">
      <title>Using Sparse Semantic Embeddings Learned from Multimodal Text and Image Data to Model Human Conceptual Knowledge</title>
      <author><first>Steven</first><last>Derby</last></author>
      <author><first>Paul</first><last>Miller</last></author>
      <author><first>Brian</first><last>Murphy</last></author>
      <author><first>Barry</first><last>Devereux</last></author>
      <pages>260–270</pages>
      <url hash="39353751">K18-1026</url>
      <abstract>Distributional models provide a convenient way to model semantics using dense embedding spaces derived from unsupervised learning algorithms. However, the dimensions of dense embedding spaces are not designed to resemble human semantic knowledge. Moreover, embeddings are often built from a single source of information (typically text data), even though neurocognitive research suggests that semantics is deeply linked to both language and perception. In this paper, we combine multimodal information from both text and image-based representations derived from state-of-the-art distributional models to produce sparse, interpretable vectors using Joint Non-Negative Sparse Embedding. Through in-depth analyses comparing these sparse models to human-derived behavioural and neuroimaging data, we demonstrate their ability to predict interpretable linguistic descriptions of human ground-truth semantic knowledge.</abstract>
      <doi>10.18653/v1/K18-1026</doi>
    </paper>
    <paper id="27">
      <title>Similarity Dependent <fixed-case>C</fixed-case>hinese Restaurant Process for Cognate Identification in Multilingual Wordlists</title>
      <author><first>Taraka</first><last>Rama</last></author>
      <pages>271–281</pages>
      <url hash="79abdab9">K18-1027</url>
      <abstract>We present and evaluate two similarity dependent Chinese Restaurant Process (sd-CRP) algorithms at the task of automated cognate detection. The sd-CRP clustering algorithms do not require any predefined threshold for detecting cognate sets in a multilingual word list. We evaluate the performance of the algorithms on six language families (more than 750 languages) and find that both the sd-CRP variants performs as well as InfoMap and better than UPGMA at the task of inferring cognate clusters. The algorithms presented in this paper are family agnostic and can be applied to any linguistically under-studied language family.</abstract>
      <doi>10.18653/v1/K18-1027</doi>
    </paper>
    <paper id="28">
      <title>Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation</title>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Iñigo</first><last>Lopez-Gazpio</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>282–291</pages>
      <url hash="56280bed">K18-1028</url>
      <abstract>Following the recent success of word embeddings, it has been argued that there is no such thing as an ideal representation for words, as different models tend to capture divergent and often mutually incompatible aspects like semantics/syntax and similarity/relatedness. In this paper, we show that each embedding model captures more information than directly apparent. A linear transformation that adjusts the similarity order of the model without any external resource can tailor it to achieve better results in those aspects, providing a new perspective on how embeddings encode divergent linguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones.</abstract>
      <doi>10.18653/v1/K18-1028</doi>
    </paper>
    <paper id="29">
      <title>Comparing Models of Associative Meaning: An Empirical Investigation of Reference in Simple Language Games</title>
      <author><first>Judy Hanwen</first><last>Shen</last></author>
      <author><first>Matthias</first><last>Hofer</last></author>
      <author><first>Bjarke</first><last>Felbo</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>292–301</pages>
      <url hash="29b3d1cc">K18-1029</url>
      <abstract>Simple reference games are of central theoretical and empirical importance in the study of situated language use. Although language provides rich, compositional truth-conditional semantics to facilitate reference, speakers and listeners may sometimes lack the overall lexical and cognitive resources to guarantee successful reference through these means alone. However, language also has rich associational structures that can serve as a further resource for achieving successful reference. Here we investigate this use of associational information in a setting where only associational information is available: a simplified version of the popular game Codenames. Using optimal experiment design techniques, we compare a range of models varying in the type of associative information deployed and in level of pragmatic sophistication against human behavior. In this setting we find that listeners’ behavior reflects direct bigram collocational associations more strongly than word-embedding or semantic knowledge graph-based associations and that there is little evidence for pragmatically sophisticated behavior on the part of either speakers or listeners. More generally, we demonstrate the effective use of simple tasks to derive insights into the nature of complex linguistic phenomena.</abstract>
      <doi>10.18653/v1/K18-1029</doi>
    </paper>
    <paper id="30">
      <title>Sequence Classification with Human Attention</title>
      <author><first>Maria</first><last>Barrett</last></author>
      <author><first>Joachim</first><last>Bingel</last></author>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>302–312</pages>
      <url hash="94a3105d">K18-1030</url>
      <abstract>Learning attention functions requires large volumes of data, but many NLP tasks simulate human behavior, and in this paper, we show that human attention really does provide a good inductive bias on many attention functions in NLP. Specifically, we use estimated human attention derived from eye-tracking corpora to regularize attention functions in recurrent neural networks. We show substantial improvements across a range of tasks, including sentiment analysis, grammatical error detection, and detection of abusive language.</abstract>
      <doi>10.18653/v1/K18-1030</doi>
    </paper>
    <paper id="31">
      <title>Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Sascha</first><last>Rothe</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>313–323</pages>
      <url hash="8061003f">K18-1031</url>
      <abstract>Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact language model. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.</abstract>
      <doi>10.18653/v1/K18-1031</doi>
    </paper>
    <paper id="32">
      <title>Predefined Sparseness in Recurrent Sequence Models</title>
      <author><first>Thomas</first><last>Demeester</last></author>
      <author><first>Johannes</first><last>Deleu</last></author>
      <author><first>Fréderic</first><last>Godin</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <pages>324–333</pages>
      <url hash="090da4a4">K18-1032</url>
      <abstract>Inducing sparseness while training neural networks has been shown to yield models with a lower memory footprint but similar effectiveness to dense models. However, sparseness is typically induced starting from a dense model, and thus this advantage does not hold during training. We propose techniques to enforce sparseness upfront in recurrent sequence models for NLP applications, to also benefit training. First, in language modeling, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters, leading to more expressive models. Second, for sequence labeling, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.</abstract>
      <doi>10.18653/v1/K18-1032</doi>
    </paper>
    <paper id="33">
      <title>Learning to Actively Learn Neural Machine Translation</title>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>334–344</pages>
      <url hash="bf76014c">K18-1033</url>
      <abstract>Traditional active learning (AL) methods for machine translation (MT) rely on heuristics. However, these heuristics are limited when the characteristics of the MT problem change due to e.g. the language pair or the amount of the initial bitext. In this paper, we present a framework to learn sentence selection strategies for neural MT. We train the AL query strategy using a high-resource language-pair based on AL simulations, and then transfer it to the low-resource language-pair of interest. The learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget. Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions.</abstract>
      <doi>10.18653/v1/K18-1033</doi>
    </paper>
    <paper id="34">
      <title>Upcycle Your <fixed-case>OCR</fixed-case>: Reusing <fixed-case>OCR</fixed-case>s for Post-<fixed-case>OCR</fixed-case> Text Correction in <fixed-case>R</fixed-case>omanised <fixed-case>S</fixed-case>anskrit</title>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Bodhisattwa P.</first><last>Majumder</last></author>
      <author><first>Rajesh</first><last>Bhat</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>345–355</pages>
      <url hash="62970ccd">K18-1034</url>
      <abstract>We propose a post-OCR text correction approach for digitising texts in Romanised Sanskrit. Owing to the lack of resources our approach uses OCR models trained for other languages written in Roman. Currently, there exists no dataset available for Romanised Sanskrit OCR. So, we bootstrap a dataset of 430 images, scanned in two different settings and their corresponding ground truth. For training, we synthetically generate training images for both the settings. We find that the use of copying mechanism (Gu et al., 2016) yields a percentage increase of 7.69 in Character Recognition Rate (CRR) than the current state of the art model in solving monotone sequence-to-sequence tasks (Schnober et al., 2016). We find that our system is robust in combating OCR-prone errors, as it obtains a CRR of 87.01% from an OCR output with CRR of 35.76% for one of the dataset settings. A human judgement survey performed on the models shows that our proposed model results in predictions which are faster to comprehend and faster to improve for a human than the other systems.</abstract>
      <doi>10.18653/v1/K18-1034</doi>
    </paper>
    <paper id="35">
      <title>Weakly-Supervised Neural Semantic Parsing with a Generative Ranker</title>
      <author><first>Jianpeng</first><last>Cheng</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>356–367</pages>
      <url hash="1690ce9c">K18-1035</url>
      <abstract>Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent. The task is challenging due to the large search space and spuriousness of logical forms. In this paper we introduce a neural parser-ranker system for weakly-supervised semantic parsing. The parser generates candidate tree-structured logical forms from utterances using clues of denotations. These candidates are then ranked based on two criterion: their likelihood of executing to the correct denotation, and their agreement with the utterance semantics. We present a scheduled training procedure to balance the contribution of the two objectives. Furthermore, we propose to use a neurally encoded lexicon to inject prior domain knowledge to the model. Experiments on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range.</abstract>
      <doi>10.18653/v1/K18-1035</doi>
    </paper>
    <paper id="36">
      <title>Modeling Composite Labels for Neural Morphological Tagging</title>
      <author><first>Alexander</first><last>Tkachenko</last></author>
      <author><first>Kairit</first><last>Sirts</last></author>
      <pages>368–379</pages>
      <url hash="3a6b9632">K18-1036</url>
      <abstract>Neural morphological tagging has been regarded as an extension to POS tagging task, treating each morphological tag as a monolithic label and ignoring its internal structure. We propose to view morphological tags as composite labels and explicitly model their internal structure in a neural sequence tagger. For this, we explore three different neural architectures and compare their performance with both CRF and simple neural multiclass baselines. We evaluate our models on 49 languages and show that the neural architecture that models the morphological labels as sequences of morphological category values performs significantly better than both baselines establishing state-of-the-art results in morphological tagging for most languages.</abstract>
      <doi>10.18653/v1/K18-1036</doi>
    </paper>
    <paper id="37">
      <title>Evolutionary Data Measures: Understanding the Difficulty of Text Classification Tasks</title>
      <author><first>Edward</first><last>Collins</last></author>
      <author><first>Nikolai</first><last>Rozanov</last></author>
      <author><first>Bingbing</first><last>Zhang</last></author>
      <pages>380–391</pages>
      <url hash="999e8937">K18-1037</url>
      <abstract>Classification tasks are usually analysed and improved through new model architectures or hyperparameter optimisation but the underlying properties of datasets are discovered on an ad-hoc basis as errors occur. However, understanding the properties of the data is crucial in perfecting models. In this paper we analyse exactly which characteristics of a dataset best determine how difficult that dataset is for the task of text classification. We then propose an intuitive measure of difficulty for text classification datasets which is simple and fast to calculate. We empirically prove that this measure generalises to unseen data by comparing it to state-of-the-art datasets and results. This measure can be used to analyse the precise source of errors in a dataset and allows fast estimation of how difficult a dataset is to learn. We searched for this measure by training 12 classical and neural network based models on 78 real-world datasets, then use a genetic algorithm to discover the best measure of difficulty. Our difficulty-calculating code and datasets are publicly available.</abstract>
      <doi>10.18653/v1/K18-1037</doi>
    </paper>
    <paper id="38">
      <title>Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention Similarity</title>
      <author><first>Paola</first><last>Merlo</last></author>
      <author><first>Francesco</first><last>Ackermann</last></author>
      <pages>392–401</pages>
      <url hash="2fa94ece">K18-1038</url>
      <abstract>Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. In a vein similar to recent pieces of work investigating the linguistic abilities of these representations, we study another core, defining property of language: the property of long-distance dependencies. Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent. This ability is blocked if a similar, but extraneous, element intervenes between the discontinuous components. We present results that show, under exhaustive and precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the properties of intervention similarity in long-distance dependencies, and that therefore they fail to represent this core linguistic notion.</abstract>
      <doi>10.18653/v1/K18-1038</doi>
    </paper>
    <paper id="39">
      <title>Lessons Learned in Multilingual Grounded Language Learning</title>
      <author><first>Ákos</first><last>Kádár</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <author><first>Marc-Alexandre</first><last>Côté</last></author>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <pages>402–412</pages>
      <url hash="0a3ec32f">K18-1039</url>
      <abstract>Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language. Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model. We show that multilingual training improves over bilingual training, and that low-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective.</abstract>
      <doi>10.18653/v1/K18-1039</doi>
    </paper>
    <paper id="40">
      <title>Unsupervised Sentence Compression using Denoising Auto-Encoders</title>
      <author><first>Thibault</first><last>Févry</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <pages>413–422</pages>
      <url hash="468d9551">K18-1040</url>
      <abstract>In sentence compression, the task of shortening sentences while retaining the original meaning, models tend to be trained on large corpora containing pairs of verbose and compressed sentences. To remove the need for paired corpora, we emulate a summarization task and add noise to extend sentences and train a denoising auto-encoder to recover the original, constructing an end-to-end training regime without the need for any examples of compressed sentences. We conduct a human evaluation of our model on a standard text summarization dataset and show that it performs comparably to a supervised baseline based on grammatical correctness and retention of meaning. Despite being exposed to no target data, our unsupervised models learn to generate imperfect but reasonably readable sentence summaries. Although we underperform supervised models based on ROUGE scores, our models are competitive with a supervised baseline based on human evaluation for grammatical correctness and retention of meaning.</abstract>
      <doi>10.18653/v1/K18-1040</doi>
    </paper>
    <paper id="41">
      <title>Resources to Examine the Quality of Word Embedding Models Trained on n-Gram Data</title>
      <author><first>Ábel</first><last>Elekes</last></author>
      <author><first>Adrian</first><last>Englhardt</last></author>
      <author><first>Martin</first><last>Schäler</last></author>
      <author><first>Klemens</first><last>Böhm</last></author>
      <pages>423–432</pages>
      <url hash="5a8d79bc">K18-1041</url>
      <abstract>Word embeddings are powerful tools that facilitate better analysis of natural language. However, their quality highly depends on the resource used for training. There are various approaches relying on n-gram corpora, such as the Google n-gram corpus. However, n-gram corpora only offer a small window into the full text – 5 words for the Google corpus at best. This gives way to the concern whether the extracted word semantics are of high quality. In this paper, we address this concern with two contributions. First, we provide a resource containing 120 word-embedding models – one of the largest collection of embedding models. Furthermore, the resource contains the n-gramed versions of all used corpora, as well as our scripts used for corpus generation, model generation and evaluation. Second, we define a set of meaningful experiments allowing to evaluate the aforementioned quality differences. We conduct these experiments using our resource to show its usage and significance. The evaluation results confirm that one generally can expect high quality for n-grams with n &gt; 3.</abstract>
      <doi>10.18653/v1/K18-1041</doi>
    </paper>
    <paper id="42">
      <title>Linguistically-Based Deep Unstructured Question Answering</title>
      <author><first>Ahmad</first><last>Aghaebrahimian</last></author>
      <pages>433–443</pages>
      <url hash="49cb0d42">K18-1042</url>
      <abstract>In this paper, we propose a new linguistically-based approach to answering non-factoid open-domain questions from unstructured data. First, we elaborate on an architecture for textual encoding based on which we introduce a deep end-to-end neural model. This architecture benefits from a bilateral attention mechanism which helps the model to focus on a question and the answer sentence at the same time for phrasal answer extraction. Second, we feed the output of a constituency parser into the model directly and integrate linguistic constituents into the network to help it concentrate on chunks of an answer rather than on its single words for generating more natural output. By optimizing this architecture, we managed to obtain near-to-human-performance results and competitive to a state-of-the-art system on SQuAD and MS-MARCO datasets respectively.</abstract>
      <doi>10.18653/v1/K18-1042</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>DIMSIM</fixed-case>: An Accurate <fixed-case>C</fixed-case>hinese Phonetic Similarity Algorithm Based on Learned High Dimensional Encoding</title>
      <author><first>Min</first><last>Li</last></author>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Sara</first><last>Noeman</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <pages>444–453</pages>
      <url hash="c66637ef">K18-1043</url>
      <abstract>Phonetic similarity algorithms identify words and phrases with similar pronunciation which are used in many natural language processing tasks. However, existing approaches are designed mainly for Indo-European languages and fail to capture the unique properties of Chinese pronunciation. In this paper, we propose a high dimensional encoded phonetic similarity algorithm for Chinese, DIMSIM. The encodings are learned from annotated data to separately map initial and final phonemes into n-dimensional coordinates. Pinyin phonetic similarities are then calculated by aggregating the similarities of initial, final and tone. DIMSIM demonstrates a 7.5X improvement on mean reciprocal rank over the state-of-the-art phonetic similarity approaches.</abstract>
      <doi>10.18653/v1/K18-1043</doi>
    </paper>
    <paper id="44">
      <title>Challenge or Empower: Revisiting Argumentation Quality in a News Editorial Corpus</title>
      <author><first>Roxanne</first><last>El Baff</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Khalid</first><last>Al-Khatib</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>454–464</pages>
      <url hash="367a3f61">K18-1044</url>
      <abstract>News editorials are said to shape public opinion, which makes them a powerful tool and an important source of political argumentation. However, rarely do editorials change anyone’s stance on an issue completely, nor do they tend to argue explicitly (but rather follow a subtle rhetorical strategy). So, what does argumentation quality mean for editorials then? We develop the notion that an effective editorial challenges readers with opposing stance, and at the same time empowers the arguing skills of readers that share the editorial’s stance — or even challenges both sides. To study argumentation quality based on this notion, we introduce a new corpus with 1000 editorials from the New York Times, annotated for their perceived effect along with the annotators’ political orientations. Analyzing the corpus, we find that annotators with different orientation disagree on the effect significantly. While only 1% of all editorials changed anyone’s stance, more than 5% meet our notion. We conclude that our corpus serves as a suitable resource for studying the argumentation quality of news editorials.</abstract>
      <doi>10.18653/v1/K18-1044</doi>
    </paper>
    <paper id="45">
      <title>Bringing Order to Neural Word Embeddings with Embeddings Augmented by Random Permutations (<fixed-case>EARP</fixed-case>)</title>
      <author><first>Trevor</first><last>Cohen</last></author>
      <author><first>Dominic</first><last>Widdows</last></author>
      <pages>465–475</pages>
      <url hash="aa404145">K18-1045</url>
      <abstract>Word order is clearly a vital part of human language, but it has been used comparatively lightly in distributional vector models. This paper presents a new method for incorporating word order information into word vector embedding models by combining the benefits of permutation-based order encoding with the more recent method of skip-gram with negative sampling. The new method introduced here is called Embeddings Augmented by Random Permutations (EARP). It operates by applying permutations to the coordinates of context vector representations during the process of training. Results show an 8% improvement in accuracy on the challenging Bigger Analogy Test Set, and smaller but consistent improvements on other analogy reference sets. These findings demonstrate the importance of order-based information in analogical retrieval tasks, and the utility of random permutations as a means to augment neural embeddings.</abstract>
      <doi>10.18653/v1/K18-1045</doi>
    </paper>
    <paper id="46">
      <title>Aggregated Semantic Matching for Short Text Entity Linking</title>
      <author><first>Feng</first><last>Nie</last></author>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Jinpeng</first><last>Wang</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <author><first>Rong</first><last>Pan</last></author>
      <pages>476–485</pages>
      <url hash="41264888">K18-1046</url>
      <abstract>The task of entity linking aims to identify concepts mentioned in a text fragments and link them to a reference knowledge base. Entity linking in long text has been well studied in previous work. However, short text entity linking is more challenging since the text are noisy and less coherent. To better utilize the local information provided in short texts, we propose a novel neural network framework, Aggregated Semantic Matching (ASM), in which two different aspects of semantic information between the local context and the candidate entity are captured via representation-based and interaction-based neural semantic matching models, and then two matching signals work jointly for disambiguation with a rank aggregation mechanism. Our evaluation shows that the proposed model outperforms the state-of-the-arts on public tweet datasets.</abstract>
      <doi>10.18653/v1/K18-1046</doi>
    </paper>
    <paper id="47">
      <title>Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>486–496</pages>
      <url hash="c83d1998">K18-1047</url>
      <abstract>We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs. Moreover, training on all strategies combined achieves further improvements, achieving a new state-of-the-art performance on the original task (also verified via human evaluation). In addition to adversarial training, we also address the robustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive, requires only 1/4 of the original vocabulary size, and is robust to one of the adversarial strategies (to which the original model is vulnerable) even without adversarial training.</abstract>
      <doi>10.18653/v1/K18-1047</doi>
    </paper>
    <paper id="48">
      <title>Improving Response Selection in Multi-Turn Dialogue Systems by Incorporating Domain Knowledge</title>
      <author><first>Debanjan</first><last>Chaudhuri</last></author>
      <author><first>Agustinus</first><last>Kristiadi</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Asja</first><last>Fischer</last></author>
      <pages>497–507</pages>
      <url hash="4ff7c395">K18-1048</url>
      <abstract>Building systems that can communicate with humans is a core problem in Artificial Intelligence. This work proposes a novel neural network architecture for response selection in an end-to-end multi-turn conversational dialogue setting. The architecture applies context level attention and incorporates additional external knowledge provided by descriptions of domain-specific words. It uses a bi-directional Gated Recurrent Unit (GRU) for encoding context and responses and learns to attend over the context words given the latent response representation and vice versa. In addition, it incorporates external domain specific information using another GRU for encoding the domain keyword descriptions. This allows better representation of domain-specific keywords in responses and hence improves the overall performance. Experimental results show that our model outperforms all other state-of-the-art methods for response selection in multi-turn conversations.</abstract>
      <doi>10.18653/v1/K18-1048</doi>
    </paper>
    <paper id="49">
      <title>The Lifted Matrix-Space Model for Semantic Composition</title>
      <author><first>WooJin</first><last>Chung</last></author>
      <author><first>Sheng-Fu</first><last>Wang</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <pages>508–518</pages>
      <url hash="a7ad069c">K18-1049</url>
      <abstract>Tree-structured neural network architectures for sentence encoding draw inspiration from the approach to semantic composition generally seen in formal linguistics, and have shown empirical improvements over comparable sequence models by doing so. Moreover, adding multiplicative interaction terms to the composition functions in these models can yield significant further improvements. However, existing compositional approaches that adopt such a powerful composition function scale poorly, with parameter counts exploding as model dimension or vocabulary size grows. We introduce the Lifted Matrix-Space model, which uses a global transformation to map vector word embeddings to matrices, which can then be composed via an operation based on matrix-matrix multiplication. Its composition function effectively transmits a larger number of activations across layers with relatively few model parameters. We evaluate our model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the Stanford Sentiment Treebank and find that it consistently outperforms TreeLSTM (Tai et al., 2015), the previous best known composition function for tree-structured models.</abstract>
      <doi>10.18653/v1/K18-1049</doi>
    </paper>
    <paper id="50">
      <title>End-to-End Neural Entity Linking</title>
      <author><first>Nikolaos</first><last>Kolitsas</last></author>
      <author><first>Octavian-Eugen</first><last>Ganea</last></author>
      <author><first>Thomas</first><last>Hofmann</last></author>
      <pages>519–529</pages>
      <url hash="d1a230b0">K18-1050</url>
      <abstract>Entity Linking (EL) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose the first neural end-to-end EL system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both MD and ED decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention - entity map, without demanding other engineered features. Empirically, we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our ED model coupled with a traditional NER system offers the best or second best EL accuracy.</abstract>
      <doi>10.18653/v1/K18-1050</doi>
    </paper>
    <paper id="51">
      <title>Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces</title>
      <author><first>Thomas</first><last>Ager</last></author>
      <author><first>Ondřej</first><last>Kuželka</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>530–540</pages>
      <url hash="ba2d7362">K18-1051</url>
      <abstract>In this paper we consider semantic spaces consisting of objects from some particular domain (e.g. IMDB movie reviews). Various authors have observed that such semantic spaces often model salient features (e.g. how scary a movie is) as directions. These feature directions allow us to rank objects according to how much they have the corresponding feature, and can thus play an important role in interpretable classifiers, recommendation systems, or entity-oriented search engines, among others. Methods for learning semantic spaces, however, are mostly aimed at modelling similarity. In this paper, we argue that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. Following this observation, we propose a simple method to fine-tune existing semantic spaces, with the aim of improving the quality of their feature directions. Crucially, our method is fully unsupervised, requiring only a bag-of-words representation of the objects as input.</abstract>
      <doi>10.18653/v1/K18-1051</doi>
    </paper>
    <paper id="52">
      <title>Model Transfer with Explicit Knowledge of the Relation between Class Definitions</title>
      <author><first>Hiyori</first><last>Yoshikawa</last></author>
      <author><first>Tomoya</first><last>Iwakura</last></author>
      <pages>541–550</pages>
      <url hash="efefb5e0">K18-1052</url>
      <abstract>This paper investigates learning methods for multi-class classification using labeled data for the target classification scheme and another labeled data for a similar but different classification scheme (support scheme). We show that if we have prior knowledge about the relation between support and target classification schemes in the form of a class correspondence table, we can use it to improve the model performance further than the simple multi-task learning approach. Instead of learning the individual classification layers for the support and target schemes, the proposed method converts the class label of each example on the support scheme into a set of candidate class labels on the target scheme via the class correspondence table, and then uses the candidate labels to learn the classification layer for the target scheme. We evaluate the proposed method on two tasks in NLP. The experimental results show that our method effectively learns the target schemes especially for the classes that have a tight connection to certain support classes.</abstract>
      <doi>10.18653/v1/K18-1052</doi>
    </paper>
    <paper id="53">
      <title>Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue Partner</title>
      <author><first>Yury</first><last>Zemlyanskiy</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <pages>551–561</pages>
      <url hash="17406da9">K18-1053</url>
      <abstract>There have been several attempts to define a plausible motivation for a chit-chat dialogue agent that can lead to engaging conversations. In this work, we explore a new direction where the agent specifically focuses on discovering information about its interlocutor. We formalize this approach by defining a quantitative metric. We propose an algorithm for the agent to maximize it. We validate the idea with human evaluation where our system outperforms various baselines. We demonstrate that the metric indeed correlates with the human judgments of engagingness.</abstract>
      <doi>10.18653/v1/K18-1053</doi>
    </paper>
    <paper id="54">
      <title>Neural Maximum Subgraph Parsing for Cross-Domain Semantic Dependency Analysis</title>
      <author><first>Yufei</first><last>Chen</last></author>
      <author><first>Sheng</first><last>Huang</last></author>
      <author><first>Fang</first><last>Wang</last></author>
      <author><first>Junjie</first><last>Cao</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>562–572</pages>
      <url hash="51182282">K18-1054</url>
      <abstract>We present experiments for cross-domain semantic dependency analysis with a neural Maximum Subgraph parser. Our parser targets 1-endpoint-crossing, pagenumber-2 graphs which are a good fit to semantic dependency graphs, and utilizes an efficient dynamic programming algorithm for decoding. For disambiguation, the parser associates words with BiLSTM vectors and utilizes these vectors to assign scores to candidate dependencies. We conduct experiments on the data sets from SemEval 2015 as well as Chinese CCGBank. Our parser achieves very competitive results for both English and Chinese. To improve the parsing performance on cross-domain texts, we propose a data-oriented method to explore the linguistic generality encoded in English Resource Grammar, which is a precisionoriented, hand-crafted HPSG grammar, in an implicit way. Experiments demonstrate the effectiveness of our data-oriented method across a wide range of conditions.</abstract>
      <doi>10.18653/v1/K18-1054</doi>
    </paper>
    <paper id="55">
      <title>From Random to Supervised: A Novel Dropout Mechanism Integrated with Global Information</title>
      <author><first>Hengru</first><last>Xu</last></author>
      <author><first>Shen</first><last>Li</last></author>
      <author><first>Renfen</first><last>Hu</last></author>
      <author><first>Si</first><last>Li</last></author>
      <author><first>Sheng</first><last>Gao</last></author>
      <pages>573–582</pages>
      <url hash="0e8c1948">K18-1055</url>
      <abstract>Dropout is used to avoid overfitting by randomly dropping units from the neural networks during training. Inspired by dropout, this paper presents GI-Dropout, a novel dropout method integrating with global information to improve neural networks for text classification. Unlike the traditional dropout method in which the units are dropped randomly according to the same probability, we aim to use explicit instructions based on global information of the dataset to guide the training process. With GI-Dropout, the model is supposed to pay more attention to inapparent features or patterns. Experiments demonstrate the effectiveness of the dropout with global information on seven text classification tasks, including sentiment analysis and topic classification.</abstract>
      <doi>10.18653/v1/K18-1055</doi>
    </paper>
    <paper id="56">
      <title>Sequence to Sequence Mixture Model for Diverse Machine Translation</title>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Mohammad</first><last>Norouzi</last></author>
      <pages>583–592</pages>
      <url hash="43d8e10a">K18-1056</url>
      <abstract>Sequence to sequence (SEQ2SEQ) models lack diversity in their generated translations. This can be attributed to their limitations in capturing lexical and syntactic variations in parallel corpora, due to different styles, genres, topics, or ambiguity of human translation process. In this paper, we develop a novel sequence to sequence mixture (S2SMIX) model that improves both translation diversity and quality by adopting a committee of specialized translation models rather than a single translation model. Each mixture component selects its own training dataset via optimization of the marginal log-likelihood, which leads to a soft clustering of the parallel corpus. Experiments on four language pairs demonstrate the superiority of our mixture model compared to SEQ2SEQ model with the standard and diversity encouraged beam search. Our mixture model incurs negligible additional parameters and no extra computation in the decoding time.</abstract>
      <doi>10.18653/v1/K18-1056</doi>
    </paper>
  </volume>
  <volume id="2">
    <meta>
      <booktitle>Proceedings of the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</booktitle>
      <url hash="f401e9c4">K18-2</url>
      <editor><first>Daniel</first><last>Zeman</last></editor>
      <editor><first>Jan</first><last>Hajič</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Brussels, Belgium</address>
      <month>October</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="af8fbbf9">K18-2000</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task: Multilingual Parsing from Raw Text to <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies</title>
      <author><first>Daniel</first><last>Zeman</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Slav</first><last>Petrov</last></author>
      <pages>1–21</pages>
      <url hash="8bf6876e">K18-2001</url>
      <abstract>Every year, the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2018, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on test input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. This shared task constitutes a 2nd edition—the first one took place in 2017 (Zeman et al., 2017); the main metric from 2017 has been kept, allowing for easy comparison, also in 2018, and two new main metrics have been used. New datasets added to the Universal Dependencies collection between mid-2017 and the spring of 2018 have contributed to increased difficulty of the task this year. In this overview paper, we define the task and the updated evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.</abstract>
      <doi>10.18653/v1/K18-2001</doi>
    </paper>
    <paper id="2">
      <title>The 2018 Shared Task on Extrinsic Parser Evaluation: On the Downstream Utility of <fixed-case>E</fixed-case>nglish <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsers</title>
      <author><first>Murhaf</first><last>Fares</last></author>
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Jari</first><last>Björne</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <pages>22–33</pages>
      <url hash="7070093f">K18-2002</url>
      <abstract>We summarize empirical results and tentative conclusions from the Second Extrinsic Parser Evaluation Initiative (EPE 2018). We review the basic task setup, downstream applications involved, and end-to-end results for seventeen participating teams. Based on in-depth quantitative and qualitative analysis, we correlate intrinsic evaluation results at different layers of morph-syntactic analysis with observed downstream behavior.</abstract>
      <doi>10.18653/v1/K18-2002</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>CEA</fixed-case> <fixed-case>LIST</fixed-case>: Processing Low-Resource Languages for <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018</title>
      <author><first>Elie</first><last>Duthoo</last></author>
      <author><first>Olivier</first><last>Mesnard</last></author>
      <pages>34–44</pages>
      <url hash="1657896f">K18-2003</url>
      <abstract>In this paper, we describe the system used for our first participation at the CoNLL 2018 shared task. The submitted system largely reused the state of the art parser from CoNLL 2017 (<url>https://github.com/tdozat/Parser-v2</url>). We enhanced this system for morphological features predictions, and we used all available resources to provide accurate models for low-resource languages. We ranked 5th of 27 participants in MLAS for building morphology aware dependency trees, 2nd for morphological features only, and 3rd for tagging (UPOS) and parsing (LAS) low-resource languages.</abstract>
      <doi>10.18653/v1/K18-2003</doi>
    </paper>
    <paper id="4">
      <title>Semi-Supervised Neural System for Tagging, Parsing and Lematization</title>
      <author><first>Piotr</first><last>Rybak</last></author>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <pages>45–54</pages>
      <url hash="554fa07a">K18-2004</url>
      <abstract>This paper describes the ICS PAS system which took part in CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. The system consists of jointly trained tagger, lemmatizer, and dependency parser which are based on features extracted by a biLSTM network. The system uses both fully connected and dilated convolutional neural architectures. The novelty of our approach is the use of an additional loss function, which reduces the number of cycles in the predicted dependency graphs, and the use of self-training to increase the system performance. The proposed system, i.e. ICS PAS (Warszawa), ranked 3th/4th in the official evaluation obtaining the following overall results: 73.02 (LAS), 60.25 (MLAS) and 64.44 (BLEX).</abstract>
      <doi>10.18653/v1/K18-2004</doi>
    </paper>
    <paper id="5">
      <title>Towards Better <fixed-case>UD</fixed-case> Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation</title>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Yijia</first><last>Liu</last></author>
      <author><first>Yuxuan</first><last>Wang</last></author>
      <author><first>Bo</first><last>Zheng</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>55–64</pages>
      <url hash="dcdcddb9">K18-2005</url>
      <abstract>This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford’s winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating treebanks for further improvements. Experimental results on the development data show the effectiveness of our methods. In the final evaluation, our system was ranked first according to LAS (75.84%) and outperformed the other systems by a large margin.</abstract>
      <doi>10.18653/v1/K18-2005</doi>
    </paper>
    <paper id="6">
      <title>Joint Learning of <fixed-case>POS</fixed-case> and Dependencies for Multilingual <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Shexia</first><last>He</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>65–73</pages>
      <url hash="f1c59122">K18-2006</url>
      <abstract>This paper describes the system of team LeisureX in the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system predicts the part-of-speech tag and dependency tree jointly. For the basic tasks, including tokenization, lemmatization and morphology prediction, we employ the official baseline model (UDPipe). To train the low-resource languages, we adopt a sampling method based on other richresource languages. Our system achieves a macro-average of 68.31% LAS F1 score, with an improvement of 2.51% compared with the UDPipe.</abstract>
      <doi>10.18653/v1/K18-2006</doi>
    </paper>
    <paper id="7">
      <title>Multilingual <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing from Raw Text with Low-Resource Language Enhancement</title>
      <author><first>Yingting</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Jia-Jun</first><last>Tong</last></author>
      <pages>74–80</pages>
      <url hash="40e513cb">K18-2007</url>
      <abstract>This paper describes the system of our team Phoenix for participating CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Given the annotated gold standard data in CoNLL-U format, we train the tokenizer, tagger and parser separately for each treebank based on an open source pipeline tool UDPipe. Our system reads the plain texts for input, performs the pre-processing steps (tokenization, lemmas, morphology) and finally outputs the syntactic dependencies. For the low-resource languages with no training data, we use cross-lingual techniques to build models with some close languages instead. In the official evaluation, our system achieves the macro-averaged scores of 65.61%, 52.26%, 55.71% for LAS, MLAS and BLEX respectively.</abstract>
      <doi>10.18653/v1/K18-2007</doi>
    </paper>
    <paper id="8">
      <title>An Improved Neural Network Model for Joint <fixed-case>POS</fixed-case> Tagging and Dependency Parsing</title>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>81–91</pages>
      <url hash="f803f174">K18-2008</url>
      <abstract>We propose a novel neural network model for joint part-of-speech (POS) tagging and dependency parsing. Our model extends the well-known BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating a BiLSTM-based tagging component to produce automatically predicted POS tags for the parser. On the benchmark English Penn treebank, our model obtains strong UAS and LAS scores at 94.51% and 92.87%, respectively, producing 1.5+% absolute improvements to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging accuracy at 97.97%. Furthermore, experimental results on parsing 61 “big” Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe (Straka and Strakova, 2017) with 0.8% higher average POS tagging score and 3.6% higher average LAS score. In addition, with our model, we also obtain state-of-the-art downstream task scores for biomedical event extraction and opinion analysis applications. Our code is available together with all pre-trained models at: <url>https://github.com/datquocnguyen/jPTDP</url>
      </abstract>
      <doi>10.18653/v1/K18-2008</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>IBM</fixed-case> Research at the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task on Multilingual Parsing</title>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Vittorio</first><last>Castelli</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <pages>92–102</pages>
      <url hash="742eee1c">K18-2009</url>
      <abstract>This paper presents the IBM Research AI submission to the CoNLL 2018 Shared Task on Parsing Universal Dependencies. Our system implements a new joint transition-based parser, based on the Stack-LSTM framework and the Arc-Standard algorithm, that handles tokenization, part-of-speech tagging, morphological tagging and dependency parsing in one single model. By leveraging a combination of character-based modeling of words and recursive composition of partially built linguistic structures we qualified 13th overall and 7th in low resource. We also present a new sentence segmentation neural architecture based on Stack-LSTMs that was the 4th best overall.</abstract>
      <doi>10.18653/v1/K18-2009</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing with a General Transition-Based <fixed-case>DAG</fixed-case> Parser</title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Ari</first><last>Rappoport</last></author>
      <pages>103–112</pages>
      <url hash="ac0cbdcb">K18-2010</url>
      <attachment type="poster" hash="28b6a4c4">K18-2010.Poster.pdf</attachment>
      <abstract>This paper presents our experiments with applying TUPA to the CoNLL 2018 UD shared task. TUPA is a general neural transition-based DAG parser, which we use to present the first experiments on recovering enhanced dependencies as part of the general parsing task. TUPA was designed for parsing UCCA, a cross-linguistic semantic annotation scheme, exhibiting reentrancy, discontinuity and non-terminal nodes. By converting UD trees and graphs to a UCCA-like DAG format, we train TUPA almost without modification on the UD parsing task. The generic nature of our approach lends itself naturally to multitask learning.</abstract>
      <doi>10.18653/v1/K18-2010</doi>
    </paper>
    <paper id="11">
      <title>82 Treebanks, 34 Models: <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing with Multi-Treebank Models</title>
      <author><first>Aaron</first><last>Smith</last></author>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Yan</first><last>Shao</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>113–123</pages>
      <url hash="1497f080">K18-2011</url>
      <abstract>We present the Uppsala system for the CoNLL 2018 Shared Task on universal dependency parsing. Our system is a pipeline consisting of three components: the first performs joint word and sentence segmentation; the second predicts part-of-speech tags and morphological features; the third predicts dependency trees from words and tags. Instead of training a single parsing model for each treebank, we trained models with multiple treebanks for one language or closely related languages, greatly reducing the number of models. On the official test run, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system obtained the best scores overall for word segmentation, universal POS tagging, and morphological features.</abstract>
      <doi>10.18653/v1/K18-2011</doi>
    </paper>
    <paper id="12">
      <title>Tree-Stack <fixed-case>LSTM</fixed-case> in Transition Based Dependency Parsing</title>
      <author><first>Ömer</first><last>Kırnap</last></author>
      <author><first>Erenay</first><last>Dayanık</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>124–132</pages>
      <url hash="8272a882">K18-2012</url>
      <abstract>We introduce tree-stack LSTM to model state of a transition based parser with recurrent neural networks. Tree-stack LSTM does not use any parse tree based or hand-crafted features, yet performs better than models with these features. We also develop new set of embeddings from raw features to enhance the performance. There are 4 main components of this model: stack’s σ-LSTM, buffer’s β-LSTM, actions’ LSTM and tree-RNN. All LSTMs use continuous dense feature vectors (embeddings) as an input. Tree-RNN updates these embeddings based on transitions. We show that our model improves performance with low resource languages compared with its predecessors. We participate in CoNLL 2018 UD Shared Task as the “KParse” team and ranked 16th in LAS, 15th in BLAS and BLEX metrics, of 27 participants parsing 82 test sets from 57 languages.</abstract>
      <doi>10.18653/v1/K18-2012</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>T</fixed-case>urku Neural Parser Pipeline: An End-to-End System for the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task</title>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Niko</first><last>Miekka</last></author>
      <author><first>Akseli</first><last>Leino</last></author>
      <author><first>Tapio</first><last>Salakoski</last></author>
      <pages>133–142</pages>
      <url hash="742ce1d0">K18-2013</url>
      <abstract>In this paper we describe the TurkuNLP entry at the CoNLL 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies. Compared to the last year, this year the shared task includes two new main metrics to measure the morphological tagging and lemmatization accuracies in addition to syntactic trees. Basing our motivation into these new metrics, we developed an end-to-end parsing pipeline especially focusing on developing a novel and state-of-the-art component for lemmatization. Our system reached the highest aggregate ranking on three main metrics out of 26 teams by achieving 1st place on metric involving lemmatization, and 2nd on both morphological tagging and parsing.</abstract>
      <doi>10.18653/v1/K18-2013</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>SE</fixed-case>x <fixed-case>B</fixed-case>i<fixed-case>ST</fixed-case>: A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations</title>
      <author><first>KyungTae</first><last>Lim</last></author>
      <author><first>Cheoneum</first><last>Park</last></author>
      <author><first>Changki</first><last>Lee</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>143–152</pages>
      <url hash="962b375f">K18-2014</url>
      <abstract>We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies). The main characteristic of our work is the encoding of three different modes of contextual information for parsing: (i) Treebank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via unsupervised learning from external resources. Our parser performed well in the official end-to-end evaluation (73.02 LAS – 4th/26 teams, and 78.72 UAS – 2nd/26); remarkably, we achieved the best UAS scores on all the English corpora by applying the three suggested feature representations. Finally, we were also ranked 1st at the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign.</abstract>
      <doi>10.18653/v1/K18-2014</doi>
    </paper>
    <paper id="15">
      <title>The <fixed-case>SLT</fixed-case>-Interactions Parsing System at the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task</title>
      <author><first>Riyaz A.</first><last>Bhat</last></author>
      <author><first>Irshad</first><last>Bhat</last></author>
      <author><first>Srinivas</first><last>Bangalore</last></author>
      <pages>153–159</pages>
      <url hash="1401d018">K18-2015</url>
      <abstract>This paper describes our system (SLT-Interactions) for the CoNLL 2018 shared task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system performs three main tasks: word segmentation (only for few treebanks), POS tagging and parsing. While segmentation is learned separately, we use neural stacking for joint learning of POS tagging and parsing tasks. For all the tasks, we employ simple neural network architectures that rely on long short-term memory (LSTM) networks for learning task-dependent features. At the basis of our parser, we use an arc-standard algorithm with Swap action for general non-projective parsing. Additionally, we use neural stacking as a knowledge transfer mechanism for cross-domain parsing of low resource domains. Our system shows substantial gains against the UDPipe baseline, with an average improvement of 4.18% in LAS across all languages. Overall, we are placed at the 12th position on the official test sets.</abstract>
      <doi>10.18653/v1/K18-2015</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing from Scratch</title>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>Timothy</first><last>Dozat</last></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <pages>160–170</pages>
      <url hash="e6afe36d">K18-2016</url>
      <abstract>This paper describes Stanford’s system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2nd, 1st, and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on low-resource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.</abstract>
      <doi>10.18653/v1/K18-2016</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>NLP</fixed-case>-Cube: End-to-End Raw Text Processing With Neural Networks</title>
      <author><first>Tiberiu</first><last>Boros</last></author>
      <author><first>Stefan Daniel</first><last>Dumitrescu</last></author>
      <author><first>Ruxandra</first><last>Burtica</last></author>
      <pages>171–179</pages>
      <url hash="5d0ef661">K18-2017</url>
      <abstract>We introduce NLP-Cube: an end-to-end Natural Language Processing framework, evaluated in CoNLL’s “Multilingual Parsing from Raw Text to Universal Dependencies 2018” Shared Task. It performs sentence splitting, tokenization, compound word expansion, lemmatization, tagging and parsing. Based entirely on recurrent neural networks, written in Python, this ready-to-use open source system is freely available on GitHub. For each task we describe and discuss its specific network architecture, closing with an overview on the results obtained in the competition.</abstract>
      <doi>10.18653/v1/K18-2017</doi>
    </paper>
    <paper id="18">
      <title>Towards <fixed-case>J</fixed-case>oint<fixed-case>UD</fixed-case>: Part-of-speech Tagging and Lemmatization using Recurrent Neural Networks</title>
      <author><first>Gor</first><last>Arakelyan</last></author>
      <author><first>Karen</first><last>Hambardzumyan</last></author>
      <author><first>Hrant</first><last>Khachatrian</last></author>
      <pages>180–186</pages>
      <url hash="eef7c8ff">K18-2018</url>
      <abstract>This paper describes our submission to CoNLL UD Shared Task 2018. We have extended an LSTM-based neural network designed for sequence tagging to additionally generate character-level sequences. The network was jointly trained to produce lemmas, part-of-speech tags and morphological features. Sentence segmentation, tokenization and dependency parsing were handled by UDPipe 1.2 baseline. The results demonstrate the viability of the proposed multitask architecture, although its performance still remains far from state-of-the-art.</abstract>
      <doi>10.18653/v1/K18-2018</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>CUNI</fixed-case> x-ling: Parsing Under-Resourced Languages in <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 <fixed-case>UD</fixed-case> Shared Task</title>
      <author><first>Rudolf</first><last>Rosa</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>187–196</pages>
      <url hash="98516e2e">K18-2019</url>
      <abstract>This is a system description paper for the CUNI x-ling submission to the CoNLL 2018 UD Shared Task. We focused on parsing under-resourced languages, with no or little training data available. We employed a wide range of approaches, including simple word-based treebank translation, combination of delexicalized parsers, and exploitation of available morphological dictionaries, with a dedicated setup tailored to each of the languages. In the official evaluation, our submission was identified as the clear winner of the Low-resource languages category.</abstract>
      <doi>10.18653/v1/K18-2019</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>UDP</fixed-case>ipe 2.0 Prototype at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 <fixed-case>UD</fixed-case> Shared Task</title>
      <author><first>Milan</first><last>Straka</last></author>
      <pages>197–207</pages>
      <url hash="d7922bfe">K18-2020</url>
      <abstract>UDPipe is a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing. We present a prototype for UDPipe 2.0 and evaluate it in the CoNLL 2018 UD Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, which employs three metrics for submission ranking. Out of 26 participants, the prototype placed first in the MLAS ranking, third in the LAS ranking and third in the BLEX ranking. In extrinsic parser evaluation EPE 2018, the system ranked first in the overall score.</abstract>
      <doi>10.18653/v1/K18-2020</doi>
    </paper>
    <paper id="21">
      <title>Universal Morpho-Syntactic Parsing and the Contribution of Lexica: Analyzing the <fixed-case>ONLP</fixed-case> Lab Submission to the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task</title>
      <author><first>Amit</first><last>Seker</last></author>
      <author><first>Amir</first><last>More</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>208–215</pages>
      <url hash="be8ab8c2">K18-2021</url>
      <abstract>We present the contribution of the ONLP lab at the Open University of Israel to the UD shared task on multilingual parsing from raw text to Universal Dependencies. Our contribution is based on a transition-based parser called ‘yap – yet another parser’, which includes a standalone morphological model, a standalone dependency model, and a joint morphosyntactic model. In the task we used <i>yap</i>‘s standalone dependency parser to parse input morphologically disambiguated by UDPipe, and obtained the official score of 58.35 LAS. In our follow up investigation we use yap to show how the incorporation of morphological and lexical resources may improve the performance of end-to-end raw-to-dependencies parsing in the case of a <i>morphologically-rich</i> and <i>low-resource</i> language, Modern Hebrew. Our results on Hebrew underscore the importance of CoNLL-UL, a UD-compatible standard for accessing external lexical resources, for enhancing end-to-end UD parsing, in particular for morphologically rich and low-resource languages. We thus encourage the community to create, convert, or make available more such lexica in future tasks.</abstract>
      <doi>10.18653/v1/K18-2021</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>SP</fixed-case>arse: <fixed-case>K</fixed-case>oç <fixed-case>U</fixed-case>niversity Graph-Based Parsing System for the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task</title>
      <author><first>Berkay</first><last>Önder</last></author>
      <author><first>Can</first><last>Gümeli</last></author>
      <author><first>Deniz</first><last>Yuret</last></author>
      <pages>216–222</pages>
      <url hash="d0a61944">K18-2022</url>
      <abstract>We present SParse, our Graph-Based Parsing model submitted for the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2018). Our model extends the state-of-the-art biaffine parser (Dozat and Manning, 2016) with a structural meta-learning module, SMeta, that combines local and global label predictions. Our parser has been trained and run on Universal Dependencies datasets (Nivre et al., 2016, 2018) and has 87.48% LAS, 78.63% MLAS, 78.69% BLEX and 81.76% CLAS (Nivre and Fang, 2017) score on the Italian-ISDT dataset and has 72.78% LAS, 59.10% MLAS, 61.38% BLEX and 61.72% CLAS score on the Japanese-GSD dataset in our official submission. All other corpora are evaluated after the submission deadline, for whom we present our unofficial test results.</abstract>
      <doi>10.18653/v1/K18-2022</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>ELM</fixed-case>o<fixed-case>L</fixed-case>ex: Connecting <fixed-case>ELM</fixed-case>o and Lexicon Features for Dependency Parsing</title>
      <author><first>Ganesh</first><last>Jawahar</last></author>
      <author><first>Benjamin</first><last>Muller</last></author>
      <author><first>Amal</first><last>Fethi</last></author>
      <author><first>Louis</first><last>Martin</last></author>
      <author><first>Éric</first><last>Villemonte de la Clergerie</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>223–237</pages>
      <url hash="01a41bae">K18-2023</url>
      <abstract>In this paper, we present the details of the neural dependency parser and the neural tagger submitted by our team ‘ParisNLP’ to the CoNLL 2018 Shared Task on parsing from raw text to Universal Dependencies. We augment the deep Biaffine (BiAF) parser (Dozat and Manning, 2016) with novel features to perform competitively: we utilize an indomain version of ELMo features (Peters et al., 2018) which provide context-dependent word representations; we utilize disambiguated, embedded, morphosyntactic features from lexicons (Sagot, 2018), which complements the existing feature set. Henceforth, we call our system ‘ELMoLex’. In addition to incorporating character embeddings, ELMoLex benefits from pre-trained word vectors, ELMo and morphosyntactic features (whenever available) to correctly handle rare or unknown words which are prevalent in languages with complex morphology. ELMoLex ranked 11th by Labeled Attachment Score metric (70.64%), Morphology-aware LAS metric (55.74%) and ranked 9th by Bilexical dependency metric (60.70%).</abstract>
      <doi>10.18653/v1/K18-2023</doi>
    </paper>
    <paper id="24">
      <title>A Morphology-Based Representation Model for <fixed-case>LSTM</fixed-case>-Based Dependency Parsing of Agglutinative Languages</title>
      <author><first>Şaziye Betül</first><last>Özateş</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <author><first>Tunga</first><last>Güngör</last></author>
      <author><first>Balkız</first><last>Öztürk</last></author>
      <pages>238–247</pages>
      <url hash="20c3eed2">K18-2024</url>
      <abstract>We propose two word representation models for agglutinative languages that better capture the similarities between words which have similar tasks in sentences. Our models highlight the morphological features in words and embed morphological information into their dense representations. We have tested our models on an LSTM-based dependency parser with character-based word embeddings proposed by Ballesteros et al. (2015). We participated in the CoNLL 2018 Shared Task on multilingual parsing from raw text to universal dependencies as the BOUN team. We show that our morphology-based embedding models improve the parsing performance for most of the agglutinative languages.</abstract>
      <doi>10.18653/v1/K18-2024</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>A</fixed-case>nt<fixed-case>NLP</fixed-case> at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case> 2018 Shared Task: A Graph-Based Parser for <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing</title>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Yufang</first><last>Liu</last></author>
      <author><first>Yijun</first><last>Wang</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>248–255</pages>
      <url hash="d120dc33">K18-2025</url>
      <abstract>We describe the graph-based dependency parser in our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task. We use bidirectional lstm to get the word representation, then a bi-affine pointer networks to compute scores of candidate dependency edges and the MST algorithm to get the final dependency tree. From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX (8/26).</abstract>
      <doi>10.18653/v1/K18-2025</doi>
    </paper>
    <paper id="26">
      <title>A Simple yet Effective Joint Training Method for Cross-Lingual <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Parsing</title>
      <author><first>Danlu</first><last>Chen</last></author>
      <author><first>Mengxiao</first><last>Lin</last></author>
      <author><first>Zhifeng</first><last>Hu</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>256–263</pages>
      <url hash="422b0caa">K18-2026</url>
      <abstract>This paper describes Fudan’s submission to CoNLL 2018’s shared task Universal Dependency Parsing. We jointly train models when two languages are similar according to linguistic typology and then ensemble the models using a simple re-parse algorithm. We outperform the baseline method by 4.4% (2.1%) on average on development (test) set in CoNLL 2018 UD Shared Task.</abstract>
      <doi>10.18653/v1/K18-2026</doi>
    </paper>
  </volume>
  <volume id="3">
    <meta>
      <booktitle>Proceedings of the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task: Universal Morphological Reinflection</booktitle>
      <url hash="af74742d">K18-3</url>
      <editor><first>Mans</first><last>Hulden</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Brussels</address>
      <month>October</month>
      <year>2018</year>
    </meta>
    <frontmatter>
      <url hash="482bf89a">K18-3000</url>
    </frontmatter>
    <paper id="1">
      <title>The <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task: Universal Morphological Reinflection</title>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Christo</first><last>Kirov</last></author>
      <author><first>John</first><last>Sylak-Glassman</last></author>
      <author><first>Géraldine</first><last>Walther</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Sabrina J.</first><last>Mielke</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>David</first><last>Yarowsky</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>1–27</pages>
      <url hash="967c01e5">K18-3001</url>
      <doi>10.18653/v1/K18-3001</doi>
      <revision id="1" href="K18-3001v1" hash="78390ce4"/>
      <revision id="2" href="K18-3001v2" hash="967c01e5" date="2020-09-01">Name change for one of the authors.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>KU</fixed-case>-<fixed-case>CST</fixed-case> at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task: a Tridirectional Model</title>
      <author><first>Manex</first><last>Agirrezabal</last></author>
      <pages>28–32</pages>
      <url hash="5ce8a43d">K18-3002</url>
      <doi>10.18653/v1/K18-3002</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>IPS</fixed-case>-<fixed-case>WASEDA</fixed-case> system at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task on morphological inflection</title>
      <author><first>Rashel</first><last>Fam</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>33–42</pages>
      <url hash="883f31ad">K18-3003</url>
      <doi>10.18653/v1/K18-3003</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>AX</fixed-case> Semantics’ Submission to the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task</title>
      <author><first>Andreas</first><last>Madsack</last></author>
      <author><first>Alessia</first><last>Cavallo</last></author>
      <author><first>Johanna</first><last>Heininger</last></author>
      <author><first>Robert</first><last>Weißgraeber</last></author>
      <pages>43–47</pages>
      <url hash="2341e285">K18-3004</url>
      <doi>10.18653/v1/K18-3004</doi>
    </paper>
    <paper id="5">
      <title>Experiments on Morphological Reinflection: <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>-2018 Shared Task</title>
      <author><first>Rishabh</first><last>Jain</last></author>
      <author><first>Anil Kumar</first><last>Singh</last></author>
      <pages>48–57</pages>
      <url hash="7ca6bc23">K18-3005</url>
      <doi>10.18653/v1/K18-3005</doi>
    </paper>
    <paper id="6">
      <title>The <fixed-case>NYU</fixed-case> System for the <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task on Universal Morphological Reinflection</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Stanislas</first><last>Lauly</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>58–63</pages>
      <url hash="5952ba02">K18-3006</url>
      <doi>10.18653/v1/K18-3006</doi>
    </paper>
    <paper id="7">
      <title>Attention-free encoder decoder for morphological processing</title>
      <author><first>Stefan Daniel</first><last>Dumitrescu</last></author>
      <author><first>Tiberiu</first><last>Boros</last></author>
      <pages>64–68</pages>
      <url hash="7faa1930">K18-3007</url>
      <doi>10.18653/v1/K18-3007</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>UZH</fixed-case> at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task on Universal Morphological Reinflection</title>
      <author><first>Peter</first><last>Makarov</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <pages>69–75</pages>
      <url hash="f685b76a">K18-3008</url>
      <doi>10.18653/v1/K18-3008</doi>
    </paper>
    <paper id="9">
      <title>Finding the way from ä to a: Sub-character morphological inflection for the <fixed-case>SIGMORPHON</fixed-case> 2018 shared task</title>
      <author><first>Fynn</first><last>Schröder</last></author>
      <author><first>Marcel</first><last>Kamlot</last></author>
      <author><first>Gregor</first><last>Billing</last></author>
      <author><first>Arne</first><last>Köhn</last></author>
      <pages>76–85</pages>
      <url hash="bd7d4a33">K18-3009</url>
      <doi>10.18653/v1/K18-3009</doi>
    </paper>
    <paper id="10">
      <title>Morphological Reinflection in Context: <fixed-case>CU</fixed-case> Boulder’s Submission to <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task</title>
      <author><first>Ling</first><last>Liu</last></author>
      <author><first>Ilamvazhuthy</first><last>Subbiah</last></author>
      <author><first>Adam</first><last>Wiemerslage</last></author>
      <author><first>Jonathan</first><last>Lilley</last></author>
      <author><first>Sarah</first><last>Moeller</last></author>
      <pages>86–92</pages>
      <url hash="03edb49b">K18-3010</url>
      <doi>10.18653/v1/K18-3010</doi>
    </paper>
    <paper id="11">
      <title>Copenhagen at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding</title>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>93–98</pages>
      <url hash="d4eb9587">K18-3011</url>
      <doi>10.18653/v1/K18-3011</doi>
    </paper>
    <paper id="12">
      <title>What can we gain from language models for morphological inflection?</title>
      <author><first>Alexey</first><last>Sorokin</last></author>
      <pages>99–104</pages>
      <url hash="16bdf442">K18-3012</url>
      <doi>10.18653/v1/K18-3012</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>IIT</fixed-case>(<fixed-case>BHU</fixed-case>)–<fixed-case>IIITH</fixed-case> at <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task on Universal Morphological Reinflection</title>
      <author><first>Abhishek</first><last>Sharma</last></author>
      <author><first>Ganesh</first><last>Katrapati</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <pages>105–111</pages>
      <url hash="cac6be99">K18-3013</url>
      <doi>10.18653/v1/K18-3013</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>T</fixed-case>übingen-<fixed-case>O</fixed-case>slo system at <fixed-case>SIGMORPHON</fixed-case> shared task on morphological inflection. A multi-tasking multilingual sequence to sequence model.</title>
      <author><first>Taraka</first><last>Rama</last></author>
      <author><first>Çağrı</first><last>Çöltekin</last></author>
      <pages>112–115</pages>
      <url hash="57a91ca7">K18-3014</url>
      <doi>10.18653/v1/K18-3014</doi>
    </paper>
    <paper id="15">
      <title>Combining Neural and Non-Neural Methods for Low-Resource Morphological Reinflection</title>
      <author><first>Saeed</first><last>Najafi</last></author>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Rashed Rubby</first><last>Riyadh</last></author>
      <author><first>Leyuan</first><last>Yu</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>116–120</pages>
      <url hash="ba7de7c5">K18-3015</url>
      <doi>10.18653/v1/K18-3015</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>BME</fixed-case>-<fixed-case>HAS</fixed-case> System for <fixed-case>C</fixed-case>o<fixed-case>NLL</fixed-case>–<fixed-case>SIGMORPHON</fixed-case> 2018 Shared Task: Universal Morphological Reinflection</title>
      <author><first>Judit</first><last>Ács</last></author>
      <pages>121–126</pages>
      <url hash="5784508d">K18-3016</url>
      <doi>10.18653/v1/K18-3016</doi>
    </paper>
  </volume>
</collection>
